<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41598-018-36885-0">Original</a>
    <h1>Human mind control of rat cyborg&#39;s movement via brain-to-brain interface (2019)</h1>
    
    <div id="readability-page-1" class="page"><div>
                <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>Direct communication between brains has long been a dream for people, especially for those with difficulty in verbal or physical language. Brain-machine interfaces (BMIs) provide a promising information channel between the brain and external devices. As a potential human mind reading technology, many previous BMI studies have successfully decoded brain activity to control either virtual objects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Royer, A. S., Doud, A. J., Rose, M. L. &amp; He, B. EEG control of a virtual helicopter in 3-dimensional space using intelligent control strategies. Ieee T Neur Sys Reh 18, 581 (2010)." href="#ref-CR1" id="ref-link-section-d107484919e465">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Xia, B. et al. A combination strategy based brain–computer interface for two-dimensional movement control. J Neural Eng 12, 46021 (2015)." href="#ref-CR2" id="ref-link-section-d107484919e465_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Wolpaw, J. R. &amp; McFarland, D. J. Control of a two-dimensional movement signal by a noninvasive brain-computer interface in humans. P Natl Acad Sci Usa 101, 17849 (2004)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR3" id="ref-link-section-d107484919e468">3</a></sup> or real devices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Carlson, T. &amp; Millan, J. D. R. Brain-controlled wheelchairs: a robotic architecture. Ieee Robot Autom Mag 20, 65 (2013)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR4" id="ref-link-section-d107484919e472">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Meng, J. et al. Noninvasive electroencephalogram based control of a robotic arm for reach and grasp tasks. Sci Rep-Uk 6, 38565 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR5" id="ref-link-section-d107484919e475">5</a></sup>. On the other hand, BMIs can also be established in an inverse direction of information flow, where computer-generated information can be used to modulate the function of a specific brain region<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hoy, K. E. &amp; Fitzgerald, P. B. Brain stimulation in psychiatry and its effects on cognition. Nat Rev Neurol 6, 267 (2010)." href="#ref-CR6" id="ref-link-section-d107484919e479">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen, R. et al. Depression of motor cortex excitability by low‐frequency transcranial magnetic stimulation. Neurology 48, 1398 (1997)." href="#ref-CR7" id="ref-link-section-d107484919e479_1">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Nguyen, J. et al. Repetitive transcranial magnetic stimulation combined with cognitive training for the treatment of Alzheimer’s disease. Neurophysiologie Clinique/Clinical Neurophysiology 47, 47 (2017)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR8" id="ref-link-section-d107484919e482">8</a></sup> or import tactile information back to the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="O Doherty, J. E. et al. Active tactile exploration using a brain–machine–brain interface. Nature 479, 228 (2011)." href="#ref-CR9" id="ref-link-section-d107484919e486">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Flesher, S. N. et al. Intracortical microstimulation of human somatosensory cortex. Sci Transl Med 8, 141r (2016)." href="#ref-CR10" id="ref-link-section-d107484919e486_1">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Romo, R., Hernández, A., Zainos, A. &amp; Salinas, E. Somatosensory discrimination based on cortical microstimulation. Nature 392, 387 (1998)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR11" id="ref-link-section-d107484919e489">11</a></sup>. The combination of different types of BMI systems can thus help to realize direct information exchange between two brains to form a new brain-brain interface (BBI). However, very few previous studies have explored BBIs across different brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Min, B., Marzelli, M. J. &amp; Yoo, S. Neuroimaging-based approaches in the brain-computer interface. Trends Biotechnol 28, 552 (2010)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR12" id="ref-link-section-d107484919e493">12</a></sup>. Miguel Pais-Vieira <i>et al</i>. established a BBI to realize the real-time transfer of behaviorally meaningful sensorimotor information between the brains of two rats<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Pais-Vieira, M., Lebedev, M., Kunicki, C., Wang, J. &amp; Nicolelis, M. A. A brain-to-brain interface for real-time sharing of sensorimotor information. Sci Rep-Uk 3, 1319 (2013)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR13" id="ref-link-section-d107484919e501">13</a></sup>. While an encoder rat performed a sensorimotor task, samples of its cortical activity were transmitted to matching cortical areas of a “decoder” rat using intracortical micro-electrical stimulation (ICMS) on its somatosensory cortex. Guided solely by the information provided by the encoder rat’s brain, the decoder rat learned to make similar behavioral selections. BBIs between humans have also been preliminary explored. One example of a BBI between humans detected motor intention with EEG signals recorded from one volunteer and transmitted this information over the internet to the motor cortex region of another volunteer by transcranial magnetic stimulation, which resulted in the direct information transmission from one human brain to another using noninvasive means<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Rao, R. P. et al. A direct brain-to-brain interface in humans. Plos One 9, e111332 (2014)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR14" id="ref-link-section-d107484919e505">14</a></sup>. In addition to information transfer between two brains of the same type of organism, the BBI enables information to be transferred from a human brain to another organism’s brain. Yoo <i>et al</i>. used steady-state visual evoked potential (SSVEP)-based BMI to extract human intention and sent it to an anesthetized rat using transcranial focused ultrasound stimulation on its brain, thereby controlling the tail movement of the anesthetized rat by the human brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Yoo, S., Kim, H., Filandrianos, E., Taghados, S. J. &amp; Park, S. Non-invasive brain-to-brain interface (BBI): establishing functional links between two brains. Plos One 8, e60410 (2013)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR15" id="ref-link-section-d107484919e512">15</a></sup>. In a very recent work, a BBI was developed to implement motion control of a cyborg cockroach by combining a human’s SSVEP BMI and electrical nerve stimulation on the cockroach’s antennas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Li, G. &amp; Zhang, D. Brain-computer interface controlled cyborg: establishing a functional information transfer pathway from human brain to cockroach brain. Plos One 11, e150667 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR16" id="ref-link-section-d107484919e516">16</a></sup>. The cyborg cockroach could then be navigated by the human brain to complete walking along an S-shaped track.</p><p>Although the feasibility of BBIs has been preliminarily proven, it is still a big challenge to build an efficient BBI for the multidegree control for the continuous locomotion of a mammal in a complex environment. In the current study, we present a wireless brain-to-brain interface, through which a human can mind control a live rat’s continuous locomotion. Different from the control of lifeless devices, it is critical to have highly demanding instantaneity in the control of a living creature in real time due to its agility and self-consciousness. For this purpose, the BBI system requires timely reactions and a high level of accuracy in terms of information decoding and importing, as well as real-time visual feedback of the rat’s movement. The SSVEP-based BMI, as used for brain intention decoding in previous BBI works that have depended on visual stimulation, may distract the manipulator from reacting promptly to real-time visual feedback. As an alternative solution, motor imagery-based BMI has the advantages of rapid response and a low level of distraction from the visual feedback. Therefore, the BBI system established in the current study integrates control instructions decoded by noninvasive motor imagery with neural feedback, and the instructions are sent back to the rat’s brain by ICMS in real time. We also proposed and compared two different control models for our BBI system, the thresholding model (TREM) and the gradient model (GRAM), to provide a more natural and easier process for the manipulator during steering control. With this interface, our manipulators were able to mind control a rat cyborg to smoothly complete maze navigation tasks.</p></div></div></section><section data-title="Results"><div id="Sec2-section"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Set up of BBI system and task design</h3><p>The BBI system in the current study consisted of two parts: a noninvasive EEG-based BMI and a rat cyborg system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Feng, Z. et al. A remote control training system for rat navigation in complicated environment. J Zhejiang Univ-Sc A 8, 323 (2007)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR17" id="ref-link-section-d107484919e535">17</a></sup> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig1">1(a)</a>). The EEG-based BMI decoded the motor intent of left and right arm movement, which corresponded to the generation of instruction <i>Left</i> and <i>Right</i> turning, respectively. In the current study, the average EEG signal control accuracy of all 6 manipulators was 77.86 ± 12.4% over all the experiments conducted. The eye blink signals in the EEG were used to elicit the instruction <i>Forward/Reward</i>, which was detected by the amplitude of EEG signal in the frontopolar channel. The rat cyborgs were prepared based on previous works<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Feng, Z. et al. A remote control training system for rat navigation in complicated environment. J Zhejiang Univ-Sc A 8, 323 (2007)." href="#ref-CR17" id="ref-link-section-d107484919e552">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Talwar, S. K. et al. Behavioural neuroscience: Rat navigation guided by remote control. Nature 417, 37 (2002)." href="#ref-CR18" id="ref-link-section-d107484919e552_1">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang, Y. et al. Visual cue-guided rat cyborg for automatic navigation [research frontier]. IEEE Comput Intell M 10, 42 (2015)." href="#ref-CR19" id="ref-link-section-d107484919e552_2">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Yu, Y. et al. Intelligence-augmented rat cyborgs in maze solving. Plos One 11, e147754 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR20" id="ref-link-section-d107484919e555">20</a></sup> and were well-trained before experiments were conducted in this study (see Methods for more details). Two parts of the system were connected through an integration platform, sending decoded instructions from motor intent to the rat cyborgs, and providing visual information feedback in real time. An overview of the BBI system is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig1">1</a>.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Figure 1"><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Figure 1</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="528"/></picture></a></div><p>Experiment setup. (<b>a</b>) Overview of the BBI system. In the brain control sessions, EEG signal was acquired and sent to the host computer where the motor intent was decoded. The decoding results were then transferred into control instructions and sent to the stimulator on the back of the rat cyborg with preset parameters. The rat cyborg would then respond to the instructions and finish the task. For the eight-arm maze, the width of each arm was 12 cm and the height of the edge was 5 cm. The rat cyborg was located in the end of either arm at the beginning of each run. And preset turning directions were informed vocally by another participant when a new trial started. (<b>b</b>) Flowchart of the proposed brain-to-brain interface.</p></div></figure></div><p>The control effect of the rat cyborgs was evaluated by a turning task on an eight-arm maze. A complete run of the turning task contained a total of 16 turning trials, with eight left turnings and eight right turnings. To avoid the influence of the memory and training experience of the rats, the turning direction sequence was randomly generated by computer before each task run. The targeted turning direction of each trial was informed vocally by other experimenters at the start of each trial during the turning control experiments. For each run, the rats were placed at the end of one of the eight arms as a starting point. The rat was then driven towards the center of the maze and guided to turn into one of the adjacent arms. A trial was regarded as successful when the rat performed a correct turning and reached the end of the target arm. A new trial would then start when the rat reached the end of one arm and turned its head back towards the center of the maze. If the rat failed to complete one turning trial, the same turning direction trail was repeated until the rat succeeded. The total time from the start to the end of completing 16 correct trials was recorded as the completion time (CPT) of each run. The turning accuracy (TA) was then calculated as the ratio of the number of correct turns to the total number of turns performed.</p><p>The entire experiment contained three stages, one manual control stage and two brain control stages, with each stage containing 5 sessions and being performed on five consecutive days. Each session consisted of 3 independent runs, with an interval break time between each run of at least eight minutes. The entire procedure was video recorded, and the mouse clicking sequences during manual control stage were recorded for further analysis. In the second and third stages, two different control models (GRAM and TREM, see details in the Methods) were applied. To further test the applicability of brain control, the rat cyborgs were controlled to complete a navigation task in a more complicated maze.</p><h3 id="Sec4">Manual control of rat cyborg</h3><p>During the manual control stage, the rat cyborgs were controlled by experienced operators. We found that the turning accuracy of a well-trained rat cyborg could achieve an exceptionally high rate of nearly 100%. As displayed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig2">2(a)</a>, the average CPT of all rat cyborgs at the first session of manual control was 190.03 ± 75.41 s and decreased to 132.56 ± 12.39 s at the fifth session. Most of the rats showed an obvious learning curve through the manual control stage. The CPT of each rat cyborg became very close at the end of the manual control stage, indicating that they were becoming familiar with the task environment and the control instructions delivered into their brains. There was no significant difference (paired T-test, p &gt; 0.05) between the average CPT of the last two sessions of the manual control stage for each rat cyborg, which indicated that the rat cyborgs were in a steady state.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Figure 2"><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Figure 2</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="709"/></picture></a></div><p>(<b>a</b>) Performance of manual control stage. The mean CPT of each rat cyborg for manual control across all sessions. (Note: For display, only positive standard deviations are presented as error bars). (<b>b</b>) Different areas assigned in the investigation for the optimal area. The simplified plus-maze was modified from the original eight-arm maze by blocking four crossing arms. (<b>c</b>) The averaged success rate (mean ± SD) of each area for the rat cyborgs to receive instructions with manual control.</p></div></figure></div><p>During the manual control sessions, we noticed that the successful turning behavior of a rat cyborg was highly dependent on the timing of the turning instructions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig2">2(b)</a>). To optimize the instruction timing, an additional experiment was conducted. In this experiment, the rats were placed at the end of the plus-maze, which was modified from the original eight-arm maze, to wait for instructions to turn left or right. By delivering turning instructions while the rats’ bodies were located in different sections along the straight arm, the instruction timing could be evaluated by the turning success of the rats. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig2">2(c)</a> shows the overall performance of the turning success rate at five equally divided sections of the maze. According to the success rate of this plus-maze test, the best location for the rat cyborg to receive turning instructions was the area near the intersection (areas C and D in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig2">2(b)</a>). When considering brain control conditions, motor imagery should be initiated slightly before the optimal point for manual control because the decoding process and instruction generation take a short period of time. Thus, in our study, the manipulators were asked to start motor imagery when the rats arrived at areas D and E.</p><h3 id="Sec5">BBI evaluation</h3><p>After stage 1 of manual control, two further brain control stages were performed by several brain control manipulators. In the two brain control stages, the manipulators controlled the rat cyborgs with a BBI (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig1">1(a)</a>) based on one of the two proposed control models. During the first brain control stage (stage 2), the gradient model (GRAM) was applied, and in the second brain control stage, the thresholding model (TREM) was applied. The two control models were based on different threshold calculating strategies. The thresholds were used to differentiate the decoding results attributed to real intention or noise (see Methods for a detailed explanation of thresholds). The results of the two control models are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig3">3</a>. The overall CPT value remained stable in both brain control stages, with no significant difference between the two sessions inside each stage (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig3">3(a)</a>, paired T-test for the average CPT, p &gt; 0.05). However, a comparison between the two brain control stages showed that a longer time was taken to complete the same maze tasks with the TREM-based BBI system. The average CPT of all rat cyborgs across the GRAM-based stage 2 was shorter than the TREM-based stage 3 (243.41 ± 12.73 s <i>vs</i>. 275.05 ± 14.47 s, paired T-test, p &lt; 0.05), demonstrating that the GRAM model was better than the TREM model for the proposed BBI system.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Figure 3"><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Figure 3</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="796"/></picture></a></div><p>(<b>a</b>) Average CPT across all rat cyborgs for the three consecutive stages. (<b>b</b>) Average turning accuracy across all rat cyborgs for the three consecutive stages. Error bars indicate the standard deviation. *Indicates p &lt; 0.05.</p></div></figure></div><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig3">3(b)</a>, the average turning accuracy of all rat cyborgs dropped approximately 15% at the first session of brain control stage 2 compared to that in the manual control stage. The turning accuracy then gradually increased back to 98.08 ± 2.31% at the last session in stage 2, indicating that the rat cyborgs could quickly be accustomed to the transition of different control styles. The drop of the fourth session was most likely due to the poor performance (81.67 ± 5.44%) of one rat cyborg. When the brain control model changed from GRAM at stage 2 to TREM at stage 3, the turning accuracy slightly dropped to 90.35 ± 5.03% in the first session of stage 3 and then generally increased across the remainder of the last stage. The group level of turning accuracy on average for stage 2 and 3 was 91.75 ± 3.85% and 93.32 ± 1.73%, respectively (stage 2 <i>vs</i>. stage 3, paired T-test, p &gt; 0.05). Overall, the turning accuracy of stage 2 and stage 3 demonstrated stable behavior results of brain control on rat cyborgs at the group level.</p><p>We further analyzed the sending number of different instructions among the three stages. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig4">4(a)</a> shows the average number of <i>Left</i> and <i>Right</i> turning instructions to complete an experimental run across sessions of all the rat cyborgs tested. Theoretically, the minimum number of turning instructions given in a 100% accuracy run is 16, which can hardly be reached even by experienced manual control. Compared with the GRAM-based and the TREM-based brain control stages, the group-level number of turning instructions were 60.15 ± 7.33 and 87.98 ± 56.30 (stage 2 <i>vs</i>. stage 3, paired T-test, p &lt; 0.01), respectively. Thus, more turning instructions were needed to steer the rat cyborg with TREM-based brain control. Since the number of turning instructions was largely affected by the accuracy of the instructions, the extra instructions in TREM were most likely used to compensate the effect of wrong turning behavior. As we mentioned above, instructions given with a proper timing contributed to fewer mistakes; therefore, the lower number of turning instructions in the GRAM-based brain control stage demonstrated that there was less error turning correction in GRAM-based stage 2 than in TREM-based stage 3.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Figure 4"><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Figure 4</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="827"/></picture></a></div><p>(<b>a</b>) Average number of turning instructions for all the rat cyborgs across all the sessions and a comparison of the group-level number of turning instructions between different stages. (<b>b</b>) Average number of Forward instructions for all the rat cyborgs across all sessions and a comparison of the group-level number of Forward instruction between different stages. ***indicates p &lt; 0.01, *indicates p &lt; 0.05, paired T-test.</p></div></figure></div><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig4">4(b)</a>, the group level average of <i>Forward</i> instructions across the sessions of GRAM-based and TREM-based brain control was 228.14 ± 44.44 and 286.70 ± 13.57, respectively. The statistical analysis indicated that the sending number of <i>Forward</i> instructions had no significant difference (stage 2 <i>vs</i>. stage 3, paired T-test, p = 0.09) between the two brain control stages. This may be due to the large fluctuation in the first two sessions of stage 2, which might have been caused by the transition from manual control to brain control. On one hand, the brain-control manipulators needed to gain experiences in controlling rats. On the other hand, the rat cyborgs also needed time to get adapted to new controlling strategy, especially the different stimulation timing and frequency from manual control. When only the later three sessions of stage 2 and stage 3 were compared, the sending <i>Forward</i> instruction did show a significant difference (later three sessions, stage 2 <i>vs</i>. stage 3, paired T-test, p = 0.03). This result demonstrated that the TREM-based brain control model requires more <i>Forward</i> instructions for the rat cyborgs to complete the same turning tasks. The reason for more <i>Forward</i> instructions with the TREM-based brain control model was the rat cyborgs had a worse performance with the TREM model and required more turning and forward instructions to correct the wrong behavior.</p><p>To explain the different performances of GRAM- and TREM- based brain control strategies, we also calculated the short delays occurred between decoding result output from EEG device and instructions generated by two different control models. Our results showed a nearly 70% reduction of instruction generation delay with GRAM (155.01 ± 3.10 ms) compared to TREM (494.70 ± 47.22 ms) (Shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig5">5</a>). Turning instructions were thus generated and sent much quicker after the motor imagery with the GRAM model, which ensured less wrong turning behavior of the rat cyborgs and better turning performance.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Figure 5"><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Figure 5</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="543"/></picture></a></div><p>The delay between the start of decoding result output and the instruction generation refers to the thresholds for GRAM and TREM. ***indicates p &lt; 0.01, T-test.</p></div></figure></div><p>The BBI system was further tested in a maze of higher complexity to test its applicability and stability. The rats were asked to complete a series of preset navigation tasks such as climbing and descending steps, turning left or right, and going through a tunnel in a three-dimensional maze under control of the BBI system. When the rat went into a wrong direction or turned into an unexpected route, the manipulator needed to guide the rat back to the correct route (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig6">6</a>, see more details in Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41598-018-36885-0#MOESM1">1</a>). 5 minutes completion time for each run was limited as the criterion for evaluating success rate. A successful run was defined as the rat cyborgs finish all of preset navigation tasks following the route within the limited time. All rats participated in turning tasks were tested with the optimized GRAM-based brain control model in the maze task. The rats all performed well with high success rate in 10 consecutive tests (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Tab1">1</a>).</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Figure 6"><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Figure 6</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig6_HTML.png?as=webp"/><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="472"/></picture></a></div><p>The rat cyborg was navigated by human brain control in a more complex maze (see more details in Supplementary Video <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41598-018-36885-0#MOESM1">1</a>). The three-dimensional maze was more complicated, consisting of a start point and an end point, slops and stairs for climbing and descending, a raised platform with a height of half a meter, pillars to be avoided and a tunnel to be passed through. The rat cyborgs were asked to complete the navigation task along the preset route (red arrowed) within 5 minutes.</p></div></figure></div><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Table 1 Success rate of brain control in the complex maze.</b></figcaption></figure></div></div></div></section><section data-title="Discussion"><div id="Sec6-section"><h2 id="Sec6">Discussion</h2><div id="Sec6-content"><p>Our study demonstrated the feasibility of cultivating an information pathway between a human brain and a rat brain. With our BBI system, a rat cyborg could accurately complete turning and forward behavior under the control of a human mind, and could perform navigation tasks in a complicated maze. Our work extended and explored the further possibility of functional information transmission from brain to brain. Unlike mechanical robots, the rat cyborgs have self-consciousness and flexible motor ability, which means the rat cyborgs will have unexpected movements depending on their own will during the control period. The BBI system should thus be designed with high instantaneity and real-time feedback for a better control effect. Previous brain-to-brain systems have mainly been based on the SSVEP paradigm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Yoo, S., Kim, H., Filandrianos, E., Taghados, S. J. &amp; Park, S. Non-invasive brain-to-brain interface (BBI): establishing functional links between two brains. Plos One 8, e60410 (2013)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR15" id="ref-link-section-d107484919e1059">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Li, G. &amp; Zhang, D. Brain-computer interface controlled cyborg: establishing a functional information transfer pathway from human brain to cockroach brain. Plos One 11, e150667 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR16" id="ref-link-section-d107484919e1062">16</a></sup>. In the SSVEP paradigm, the manipulators must switch their attention between the feedback screen and the flickers. However, rat cyborgs move quickly and require a minimum frequency of <i>Forward</i> instructions above 3 Hz. It is thus difficult for the human manipulator to send a high frequency of <i>Forward</i> instructions and simultaneously watch the locomotion of the rat cyborgs in the feedback screen. Compared with previous works<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Yoo, S., Kim, H., Filandrianos, E., Taghados, S. J. &amp; Park, S. Non-invasive brain-to-brain interface (BBI): establishing functional links between two brains. Plos One 8, e60410 (2013)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR15" id="ref-link-section-d107484919e1072">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Li, G. &amp; Zhang, D. Brain-computer interface controlled cyborg: establishing a functional information transfer pathway from human brain to cockroach brain. Plos One 11, e150667 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR16" id="ref-link-section-d107484919e1075">16</a></sup>, we used motor imagery and eye blink as manipulative protocols and provided real-time visual feedback of the rat cyborg, which is comparably more viable and avoids the visual fatigue of the manipulators. In addition, during the rat cyborgs brain control experiments, the overall performance was influenced by several major factors:</p><ol>
                  <li>
                    <span>(1)</span>
                    
                      <p>The accuracy of instructions. The decoding correctness of motor imagery and the appropriate timing of control instructions influence the control performance the most. Furthermore, the instruction should be sent with high instantaneity, especially when an unexpected mistake occurs. In our brain control sessions, the correctness mainly depended on the threshold value and the timeliness of triggering instruction determined by the control models. The better performance (less CPT and number of turning and forward instructions) for GRAM-based BBI is most likely due to less delay between the start of the decoding results and the release of instructions. Comparatively, the longer delay occurred in TREM may probably contribute to a longer CPT, which in turn resulted in greater amount of instructions needed to complete the task. Besides, the longer delay in the TREM model also leads to obstruction of motor imagery. The manipulators reported that the delay of instruction release during TREM brain control could not readily be perceived. Although the manipulators tried to begin imagery in advance, it was difficult to decide the concrete timing and difficult to operate when instructions were needed to be released over a short period. In contrast, with the short response duration in GRAM, the manipulators were able to start motor imagery at the optimal instruction-receiving time, and switching between <i>Left</i> and <i>Right</i> instructions was much easier.</p>
                    
                  </li>
                  <li>
                    <span>(2)</span>
                    
                      <p>Adaption of the manipulators to brain control task. The mental status of a manipulator can be influenced by disturbance, such as environmental noise, and fatigue caused by long-duration imagery. The ability to overcome these could be improved after several practice sessions. The noninvasive EEG-based BMI used in this study translates the sensorimotor rhythms detected in the bilateral motor areas to the control signal for the rat cyborg. This is not intuitive to the manipulators at the beginning of the experiment, but becomes more intuitive as the experiment goes on. The manipulators gradually learn what instruction should be sent and when their imagery should begin according to the movements and locations of the rat cyborg, thereby cultivating a tacit understanding between the human and the rat cyborg. The stable level of performance seen in the latter stage 2 and stage 3 indicates this mutual adaption.</p>
                    
                  </li>
                  <li>
                    <span>(3)</span>
                    
                      <p>The inherent adaptive ability of rat cyborgs. Rat cyborgs possess an inherent adaptive ability to their environment and the control method. The overall decrease of average CPT in the manual control stage indicates the adaption of rat cyborgs to the control instructions. The variation trend of each line indicates the various adaption abilities among rat cyborgs. Intriguingly, the final CPT of each rat cyborg reached a similar level. It is likely that all of the rat cyborgs adapted to the same control pattern of the operator. In addition, the rat cyborgs can also adapt to the changes of instruction release due to their excellent learning ability. The results showed that the performance was adversely affected by changes in the control mode (stage 1, session 5 <i>vs</i>. stage 2, session 1 and stage 2, session 5 <i>vs</i>. stage 3, session 1 in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig3">3(a,b)</a>) but subsequently stabilized. The decrease in the turning accuracy from stage 1 to stage 2 was much sharper than the change from stage 2 to stage 3. This may be because the control pattern is more distinct between manual control and brain control. While between different brain control stages, the manipulator’s control pattern was not likely to dramatically alter.</p>
                    
                  </li>
                  <li>
                    <span>(4)</span>
                    
                      <p>In conclusion, our findings suggest that computer-assisted BBI that transmits information between two entities is intriguingly possible. The control model proposed here could transfer the decoding results of motor imagery-based EEG-BMI to other external devices with remarkable instantaneity. In the future, error-related potentials (ErrPs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Chavarriaga, R., Sobolewski, A. &amp; Millán, J. D. R. Errare machinale est: the use of error-related potentials in brain-machine interfaces. Front Neurosci-Switz 8, 208 (2014)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR21" id="ref-link-section-d107484919e1137">21</a></sup> could be used to detect false generated instructions, thereby eliminating the wrong instructions before sending them to the rat cyborgs. Furthermore, information flow will be made bidirectional and communicative between two human individuals.</p>
                    
                  </li>
                </ol></div></div></section><section data-title="Methods"><div id="Sec7-section"><h2 id="Sec7">Methods</h2><div id="Sec7-content"><h3 id="Sec8">Participants and ethics statement</h3><p>Six rats were engaged in this study. All methods were carried out in accordance with the National Research Council’s Guide for the Care and Use of Laboratory Animals. All experimental protocols were approved by the Ethics Committee of Zhejiang University, China. Informed consent was obtained from all manipulators.</p><h3 id="Sec9">Rat cyborg preparation</h3><p>The rat cyborg system had long been developed in our previous research work. Briefly, bipolar stimulating electrodes were made from pairs of insulated nichrome wires (65 μm in diameter), with a 0.5 mm vertical tip separation. Microelectrodes were implanted into the rat’s brain for the control of their locomotion. Two pairs of electrodes were implanted in the bilateral medial forebrain bundle (MFB)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Hermer-Vazquez, L. et al. Rapid learning and flexible memory in “habit” tasks in rats trained with brain stimulation reward. Physiol Behav 84, 753 (2005)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR22" id="ref-link-section-d107484919e1164">22</a></sup> for virtual reward stimulation and instruction of forward moving. The other two pairs of electrodes were implanted symmetrically in both sides of the whisker barrel fields of somatosensory cortices (SIBF)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Paxinos, G. &amp; Watson, C. The rat brain in stereotaxic coordinates. (Elsevier, Academic Press, Amsterdam, 2014)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR23" id="ref-link-section-d107484919e1168">23</a></sup> for turning cue stimulation. The rats were allowed to recover from the surgery for one week before the experiments. Once recovered, the rat cyborgs were first trained to correlate the stimulations with the corresponding locomotion behaviors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Feng, Z. et al. A remote control training system for rat navigation in complicated environment. J Zhejiang Univ-Sc A 8, 323 (2007)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR17" id="ref-link-section-d107484919e1172">17</a></sup>. The parameters of the electrical stimulation that were sent into the rat’s brain were based on our previous works<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Xu, K., Zhang, J., Zhou, H., Lee, J. C. T. &amp; Zheng, X. A novel turning behavior control method for rat-robot through the stimulation of ventral posteromedial thalamic nucleus. Behav Brain Res 298, 150 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR24" id="ref-link-section-d107484919e1176">24</a></sup>, which can activate appropriate behavior but avoid seizures even after a long duration of stimulation. During the training and control sessions, electrical stimulations were delivered through a wireless microstimulator mounted on the rat’s back. Control instructions were given by operators with a computer program wirelessly connected to the microstimulator through Bluetooth.</p><h3 id="Sec10">Decoding in the BBI</h3><p>A commercial EEG device, Emotiv EPOC (Emotiv Inc., USA)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Martinez-Leon, J., Cano-Izquierdo, J. &amp; Ibarrola, J. Are low cost Brain Computer Interface headsets ready for motor imagery applications? Expert Syst Appl 49, 136 (2016)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR25" id="ref-link-section-d107484919e1188">25</a></sup> was used in this study for EEG data recording. EEG data were acquired with a 14-channel neuroheadset, with all electrode impedances kept below 10 kΩ. During the brain control experiments, the EEG signals were sampled at the rate of 256 Hz. The recorded data were then wirelessly transmitted to a host computer through Bluetooth and further processed with the help of Emotiv SDK. Through trained imagination, the manipulators learned to modulate their sensorimotor rhythm amplitude in the upper mu (10–14 Hz) frequency band<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Pfurtscheller, G., Neuper, C., Flotzinger, D. &amp; Pregenzer, M. EEG-based discrimination between imagination of right and left hand movement. Electroencephalography and clinical Neurophysiology 103, 642 (1997)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR26" id="ref-link-section-d107484919e1192">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Wolpaw, J. R., Birbaumer, N., McFarland, D. J., Pfurtscheller, G. &amp; Vaughan, T. M. Brain–computer interfaces for communication and control. Clin Neurophysiol 113, 767 (2002)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR27" id="ref-link-section-d107484919e1195">27</a></sup>. The power spectrum of left and right composition was then obtained as the intensity of motor imagery by common spatial pattern (CSP)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Kumar, S., Mamun, K. &amp; Sharma, A. CSP-TSM: Optimizing the performance of Riemannian tangent space mapping using common spatial pattern for MI-BCI. Comput Biol Med 91, 231 (2017)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR28" id="ref-link-section-d107484919e1199">28</a></sup>, i.e., <i>x</i><sub><i>L</i></sub>(<i>t</i>) and <i>x</i><sub><i>R</i></sub>(<i>t</i>), respectively. Details of the common spatial pattern filter are described as follows:</p><p>Let X<sub><i>R</i></sub> and X<sub><i>L</i></sub> denote the preprocessed EEG during right- or left-hand movements with dimensions <i>N</i> × <i>T</i>, where N is the number of channels and T is the number of samples per channel. The common spatial pattern filter is acquired as follows:</p><ol>
                    <li>
                      <span>(1)</span>
                      
                        <p>Calculate the normalized channel covariance of X<sub><i>R</i></sub> and X<sub><i>L</i></sub> as:</p><div id="Equ1"><p><span>$${C}_{L}=\frac{{\rm{cov}}({{\rm{X}}}_{L})}{{\rm{trace}}({{\rm{X}}}_{{L}}{{\rm{X}}}_{L}^{T})}$$</span></p><p>
                    (1)
                </p></div><div id="Equ2"><p><span>$${C}_{R}=\frac{{\rm{cov}}({{\rm{X}}}_{R})}{{\rm{trace}}({{\rm{X}}}_{R}{{\rm{X}}}_{R}^{T})}$$</span></p><p>
                    (2)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(2)</span>
                      
                        <p>Average the <i>C</i><sub><i>L</i></sub> and <i>C</i><sub><i>R</i></sub> on all of the left- and right-hand movement EEG trials; the composite spatial covariance is:</p><div id="Equ3"><p><span>$$C=\overline{{C}_{L}}+\overline{{C}_{R}}$$</span></p><p>
                    (3)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(3)</span>
                      
                        <p>Perform eigenvalue decomposition on the composite spatial covariance, where:</p><div id="Equ4"><p><span>$$C={U}_{0}{\rm{\Sigma }}{U}_{0}^{T}$$</span></p><p>
                    (4)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(4)</span>
                      
                        <p>Perform whitening transform on <span>\(\overline{{C}_{L}}\)</span> and <span>\(\overline{{C}_{R}}\)</span>, and the transformed spatial covariance matrixes are:</p><div id="Equ5"><p><span>$${S}_{L}=P\overline{{C}_{L}}{P}^{T}$$</span></p><p>
                    (5)
                </p></div><div id="Equ6"><p><span>$${S}_{R}=P\overline{{C}_{R}}{P}^{T}$$</span></p><p>
                    (6)
                </p></div><p>where,</p><div id="Equ7"><p><span>$$P={{\rm{\Sigma }}}^{-1/2}{U}_{0}^{T}$$</span></p><p>
                    (7)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(5)</span>
                      
                        <p>Perform eigenvalue decomposition on the transformed spatial covariance matrix, where:</p><div id="Equ8"><p><span>$${S}_{L}={U}_{L}{{\rm{\Sigma }}}_{L}{U}_{L}^{T}$$</span></p><p>
                    (8)
                </p></div><div id="Equ9"><p><span>$${S}_{R}={U}_{R}{{\rm{\Sigma }}}_{R}{U}_{R}^{T}$$</span></p><p>
                    (9)
                </p></div>
                        <p>(Note that Σ<sub><i>L</i></sub> + Σ<sub><i>R</i></sub> must be an identity matrix);</p>
                      
                    </li>
                    <li>
                      <span>(6)</span>
                      
                        <p>The eigenvectors corresponding to the largest eigenvalue in Σ<sub><i>L</i></sub> and Σ<sub><i>R</i></sub> are chosen to calculate the common spatial pattern filters for right- and left-hand movements, which can be written as:</p><div id="Equ10"><p><span>$$S{F}_{L}={U}_{L}(i|{{\rm{argmax}}}_{i}\,{{\rm{\Sigma }}}_{L}(i))P$$</span></p><p>
                    (10)
                </p></div><div id="Equ11"><p><span>$$S{F}_{R}={U}_{R}(j|{{\rm{argmax}}}_{j}\,{{\rm{\Sigma }}}_{R}(j))P$$</span></p><p>
                    (11)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(7)</span>
                      
                        <p>Let <i>x</i>(<i>t</i>) be the preprocessed EEG signal recorded in movement imaginary application, the intensity of left- and right-hand movement imagery can be given as:</p><div id="Equ12"><p><span>$${x}_{L}(t)=S{F}_{L}x(t)$$</span></p><p>
                    (12)
                </p></div><div id="Equ13"><p><span>$${x}_{R}(t)=S{F}_{R}x(t)$$</span></p><p>
                    (13)
                </p></div>
                      
                    </li>
                    <li>
                      <span>(8)</span>
                      
                        <p>Finally, calculate the power spectral density of <i>x</i><sub><i>L</i></sub>(<i>t</i>) and <i>x</i><sub><i>R</i></sub>(<i>t</i>), and aggregate the band power within the overlapping window length of k.</p>
                      
                    </li>
                  </ol><div id="Equ14"><p><span>$${{\rm{B}}}_{L}(t)=\sum _{t-k}^{t}P({x}_{L}(t))$$</span></p><p>
                    (14)
                </p></div><div id="Equ15"><p><span>$${{\rm{B}}}_{R}(t)=\sum _{t-k}^{t}P({x}_{R}(t))$$</span></p><p>
                    (15)
                </p></div><p>where <span>\(P(x(t))\)</span> indicates the power spectral density of <i>x</i>(<i>t</i>). The intensity of motor intent was then mapped to a value ranged from 0 to 1, and the normalized <i>B</i>(<i>t</i>) was used as the input of the control model.</p><h3 id="Sec11">Set up of BBI system</h3><p>As the BBI system consisted of a noninvasive EEG-based BMI and a rat cyborg system, a controlling program written in Visual C++ was applied to acquire EEG raw data from Emotiv SDK, generate instructions with the control models and trigger the release of instructions to the rat cyborg. The locomotion and location of the rat cyborg in the entire experimental scene was video captured by a top-viewed camera and visually delivered back to the manipulators on an LCD screen in real time. The decoding results of motor imaginary were relayed using a flashing instruction feedback panel that was integrated in the bottom of the LCD by an OpenCV (Open Source Computer Vision Library, <a href="http://opencv.org">http://opencv.org</a>)-based self-written program. The EEG decoding results and motor control instructions were recorded with a J2EE (Java 2 Platform Enterprise Edition)-based program for further analysis.</p><h3 id="Sec12">Control models for BBI</h3><p>The inputs of the control model included the decoding results of <i>Left</i> or <i>Right</i> motor imagery and eye blink detection. The collected EEG signals were projected by a common spatial pattern (CSP) spatial filter. Next, the power spectrum of left and right composition was obtained as the intensity of motor imagery, i.e., <i>x</i><sub><i>L</i></sub>(<i>t</i>) and <i>x</i><sub><i>R</i></sub>(<i>t</i>), respectively. Eye blink, <i>x</i><sub><i>F</i></sub>(<i>t</i>), was detected when the EEG signal <span>\(\overrightarrow{(E(t))}\)</span> amplitude of channels near the eyes exceeds a threshold <span>\(\overrightarrow{{\theta }_{EOG}}\)</span>.</p><div id="Equ16"><p><span>$${x}_{F}(t)=\{\begin{array}{ll}1, &amp; \overrightarrow{E(t)}\ge \overrightarrow{{\theta }_{EOG}}\\ 0, &amp; Otherwise\end{array}$$</span></p><p>
                    (16)
                </p></div><p>The output of the control model was a control signal for the microelectrical stimulations. <i>x</i><sub><i>L</i></sub>(<i>t</i>), <i>Y</i><sub><i>R</i></sub>(<i>t</i>) and <i>Y</i><sub><i>F</i></sub>(<i>t</i>) represent the <i>Left</i>, <i>Right</i> and <i>Forward</i> instructions, respectively.</p><p>For the safety of the rat cyborgs, instructions should be sent under the following rule: If two instructions were presented continuously, the latter instruction would only be sent when the time interval was larger than a predefined threshold Δ<i>T</i>. Adjacent instructions were defined as tuples <c1, c2="">, C1, C2 <span>∈</span>{</c1,><i>Left</i>, <i>Right</i>, <i>Forward</i>}. Five out of nine types of tuples were restricted, namely, Δ<i>T</i><sub>&lt;<i>F</i>,<i>F</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>L</i>,<i>L</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>R</i>,<i>R</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>L</i>,<i>F</i>&gt;</sub> and Δ<i>T</i><sub>&lt;<i>R</i>,<i>F</i>&gt;</sub>. These five were determined by the number distribution of the interval for each tuple based on the manual control sequence record. To guarantee the proper reaction, the level of excitement and the safety of the rat cyborgs, the intervals of Δ<i>T</i><sub>&lt;<i>F</i>,<i>F</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>L</i>,<i>L</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>R</i>,<i>R</i>&gt;</sub>, Δ<i>T</i><sub>&lt;<i>L</i>,<i>F</i>&gt;</sub> and Δ<i>T</i><sub>&lt;<i>R</i>,<i>F</i>&gt;</sub> for brain control were set to be 200 ms, 500 ms, 500 ms, 350 ms and 350 ms, respectively, according to our previous work<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Feng, Z. et al. A remote control training system for rat navigation in complicated environment. J Zhejiang Univ-Sc A 8, 323 (2007)." href="https://www.nature.com/articles/s41598-018-36885-0#ref-CR17" id="ref-link-section-d107484919e3264">17</a></sup>. The minimum time interval was not restricted for F-L, F-R, R-L and L-R because the manipulator needed to send the first turning command as quickly as possible.</p><p>We defined <i>n</i> = 0, 1, … as the n-th generation of instruction, and <i>t</i><sub><i>L</i></sub>(<i>n</i>), <i>t</i><sub><i>R</i></sub>(<i>n</i>) and <i>t</i><sub><i>F</i></sub>(<i>n</i>) were the times that an instruction occurred. Initially, <i>t</i><sub><i>L</i></sub>(<i>n</i>), <i>t</i><sub><i>R</i></sub>(<i>n</i>) and <i>t</i><sub><i>F</i></sub>(<i>n</i>) were equal to 0 (n = 0). The generation of <i>Forward</i> was the same for the two models, as described below:</p><div id="Equ17"><p><span>$${Y}_{F}(t)=\{\begin{array}{ll}1, &amp; t\in \{{{\rm{t}}}_{F}|\begin{array}{l}{t}_{F}(n)-{t}_{F}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; F,F &gt; },\\ {t}_{F}(n)-{t}_{L}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; L,F &gt; },\\ \begin{array}{c}{t}_{F}(n)-{t}_{R}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; R,F &gt; }\\ and\,{x}_{F}({{\rm{t}}}_{F})=1\end{array}\end{array}\}\\ 0, &amp; otherwise\end{array}$$</span></p><p>
                    (17)
                </p></div><p>Two models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig1">1(b)</a>) for generating <i>Left</i> and <i>Right</i> instructions were proposed. One was called the thresholding model (TREM), in which the instructions were generated when the decoding results exceeded a threshold (<i>θ</i>). The other model was the gradient model (GRAM), in which the instructions were generated when the gradient value between two decoding results transcended a threshold (<i>θ</i>′). The thresholds were used to differentiate the decoding results attributed to real intention or noise. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-018-36885-0#Fig7">7</a> demonstrates typical decoding results of a left and right imagery and their corresponding gradients.</p><div data-test="figure" data-container-section="figure" id="figure-7" data-title="Figure 7"><figure><figcaption><b id="Fig7" data-test="figure-caption-text">Figure 7</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-018-36885-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig7_HTML.png?as=webp"/><img aria-describedby="Fig7" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-018-36885-0/MediaObjects/41598_2018_36885_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="229"/></picture></a></div><p>Samples of decoding results and their corresponding gradients of motor imagery in a preliminary experiment. The blue curve is the result of right imagery (Right) and the orange curve is the result of left imagery (Left). During the right turning period shown in the figure, only right imagery occurred, while in the left turning period, both left and right results appeared. The right decoding results were deemed to be caused by noise. In addition, the left decoding results appearing in the blank period (no imagination) are regarded as noise as well. The grad<sub>L</sub> (yellow) and grad<sub><i>R</i></sub> (light blue) curves represent the left and right gradient of corresponding decoding results, respectively. <i>θ</i><sub><i>L</i></sub> and <i>θ</i><sub><i>R</i></sub> are the optimal thresholds for left and right motor imagery in TREM. For GRAM, the optimal thresholds are <i>θ</i><sub><i>L</i></sub> and <i>θ</i><sub><i>R</i></sub>.</p></div></figure></div><h4 id="Sec13">Thresholding Control Model</h4><p>For TREM, controlling impulses were generated when the intensity of left or right exceeded a threshold <i>θ</i>. A turning instruction was generated if <i>x</i><sub><i>L</i></sub>(t) &gt; <i>θ</i><sub><i>L</i></sub> or <i>x</i><sub><i>R</i></sub>(t) &gt; <i>θ</i><sub><i>R</i></sub>. Therefore, the function of TREM is described as follows:</p><div id="Equ18"><p><span>$${Y}_{L}(t)=\{\begin{array}{ll}1, &amp; t\in \{{t}_{L}|{t}_{L}(n)-{t}_{L}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; L,L &gt; }\,and\,\begin{array}{c}{x}_{L}({t}_{L})\end{array}\ge {\theta }_{L}\}\\ 0, &amp; otherwise\end{array}$$</span></p><p>
                    (18)
                </p></div><div id="Equ19"><p><span>$${Y}_{R}(t)=\{\begin{array}{ll}1, &amp; t\in \{{t}_{R}|{t}_{R}(n)-{t}_{R}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; R,R &gt; }\,and\,\begin{array}{c}{x}_{R}({t}_{R})\end{array}\ge {\theta }_{R}\}\\ 0, &amp; otherwise\end{array}$$</span></p><p>
                    (19)
                </p></div><h4 id="Sec14">Gradient Control Model</h4><p>Although the threshold in TREM could differentiate the decoding results attributed to real intention or floating background noise, the delay between the start of the decoding results and the generation of instruction was too long. We proposed an improved model, GRAM, that outperformed in both differentiation and instantaneity. For GRAM, instructions were generated when the gradient value between two decoding windows transcended a threshold <i>θ</i>′. The gradient value was calculated as follows:</p><div id="Equ20"><p><span>$$Grad\begin{array}{c}x(t)\end{array}=x(t)-x(t-1)$$</span></p><p>
                    (20)
                </p></div><p>A turning instruction was generated if Grad <i>x</i>(<i>t</i>) &gt; <i>θ</i>′. Accordingly, the function of GRAM is described as follows:</p><div id="Equ21"><p><span>$${Y}_{L}(t)=\{\begin{array}{ll}1, &amp; t\in \{{t}_{L}|{t}_{L}(n)-{t}_{L}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; L,L &gt; }\,and\,Grad\begin{array}{c}{x}_{L}({t}_{L})\end{array}\ge {\theta ^{\prime} }_{L}\}\\ 0, &amp; otherwise\end{array}$$</span></p><p>
                    (21)
                </p></div><div id="Equ22"><p><span>$${Y}_{R}(t)=\{\begin{array}{ll}1, &amp; t\in \{{t}_{R}|{t}_{R}(n)-{t}_{R}(n-1)\ge {\rm{\Delta }}{T}_{ &lt; R,R &gt; }\,and\,Grad\begin{array}{c}{x}_{R}({t}_{R})\end{array}\ge {\theta ^{\prime} }_{R}\}\\ 0, &amp; otherwise\end{array}$$</span></p><p>
                    (22)
                </p></div><p>The thresholds <i>θ</i> and <i>θ</i>′ were decided prior to the implementation of brain control. To ascertain the optimal threshold, a preliminary experiment was conducted. The manipulators were asked to complete three rounds of eight motor imagery tasks. Intents were decoded in real time, and the decoding results were recorded. The best threshold was determined with a receiver operating characteristic (ROC) curve.</p></div></div></section>
                </div><div>
            <section data-title="Data Availability"><div id="data-availability-section"><h2 id="data-availability">Data Availability</h2><p>The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol data-track-component="outbound reference"><li data-counter="1."><p id="ref-CR1">Royer, A. S., Doud, A. J., Rose, M. L. &amp; He, B. EEG control of a virtual helicopter in 3-dimensional space using intelligent control strategies. <i>Ieee T Neur Sys Reh</i> <b>18</b>, 581 (2010).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TNSRE.2010.2077654" data-track-action="article reference" href="https://doi.org/10.1109%2FTNSRE.2010.2077654" aria-label="Article reference 1" data-doi="10.1109/TNSRE.2010.2077654">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=EEG%20control%20of%20a%20virtual%20helicopter%20in%203-dimensional%20space%20using%20intelligent%20control%20strategies&amp;journal=Ieee%20T%20Neur%20Sys%20Reh&amp;doi=10.1109%2FTNSRE.2010.2077654&amp;volume=18&amp;publication_year=2010&amp;author=Royer%2CAS&amp;author=Doud%2CAJ&amp;author=Rose%2CML&amp;author=He%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">Xia, B. <i>et al</i>. A combination strategy based brain–computer interface for two-dimensional movement control. <i>J Neural Eng</i> <b>12</b>, 46021 (2015).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1088/1741-2560/12/4/046021" data-track-action="article reference" href="https://doi.org/10.1088%2F1741-2560%2F12%2F4%2F046021" aria-label="Article reference 2" data-doi="10.1088/1741-2560/12/4/046021">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20combination%20strategy%20based%20brain%E2%80%93computer%20interface%20for%20two-dimensional%20movement%20control&amp;journal=J%20Neural%20Eng&amp;doi=10.1088%2F1741-2560%2F12%2F4%2F046021&amp;volume=12&amp;publication_year=2015&amp;author=Xia%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="3."><p id="ref-CR3">Wolpaw, J. R. &amp; McFarland, D. J. Control of a two-dimensional movement signal by a noninvasive brain-computer interface in humans. <i>P Natl Acad Sci Usa</i> <b>101</b>, 17849 (2004).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1073/pnas.0403504101" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.0403504101" aria-label="Article reference 3" data-doi="10.1073/pnas.0403504101">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXjtVSmtA%3D%3D" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2004PNAS..10117849W" aria-label="ADS reference 3">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Control%20of%20a%20two-dimensional%20movement%20signal%20by%20a%20noninvasive%20brain-computer%20interface%20in%20humans&amp;journal=P%20Natl%20Acad%20Sci%20Usa&amp;doi=10.1073%2Fpnas.0403504101&amp;volume=101&amp;publication_year=2004&amp;author=Wolpaw%2CJR&amp;author=McFarland%2CDJ">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">Carlson, T. &amp; Millan, J. D. R. Brain-controlled wheelchairs: a robotic architecture. <i>Ieee Robot Autom Mag</i> <b>20</b>, 65 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/MRA.2012.2229936" data-track-action="article reference" href="https://doi.org/10.1109%2FMRA.2012.2229936" aria-label="Article reference 4" data-doi="10.1109/MRA.2012.2229936">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain-controlled%20wheelchairs%3A%20a%20robotic%20architecture&amp;journal=Ieee%20Robot%20Autom%20Mag&amp;doi=10.1109%2FMRA.2012.2229936&amp;volume=20&amp;publication_year=2013&amp;author=Carlson%2CT&amp;author=Millan%2CJDR">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">Meng, J. <i>et al</i>. Noninvasive electroencephalogram based control of a robotic arm for reach and grasp tasks. <i>Sci Rep-Uk</i> <b>6</b>, 38565 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/srep38565" data-track-action="article reference" href="https://doi.org/10.1038%2Fsrep38565" aria-label="Article reference 5" data-doi="10.1038/srep38565">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XitFGjsrvF" aria-label="CAS reference 5">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016NatSR...638565M" aria-label="ADS reference 5">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Noninvasive%20electroencephalogram%20based%20control%20of%20a%20robotic%20arm%20for%20reach%20and%20grasp%20tasks&amp;journal=Sci%20Rep-Uk&amp;doi=10.1038%2Fsrep38565&amp;volume=6&amp;publication_year=2016&amp;author=Meng%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="6."><p id="ref-CR6">Hoy, K. E. &amp; Fitzgerald, P. B. Brain stimulation in psychiatry and its effects on cognition. <i>Nat Rev Neurol</i> <b>6</b>, 267 (2010).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nrneurol.2010.30" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrneurol.2010.30" aria-label="Article reference 6" data-doi="10.1038/nrneurol.2010.30">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain%20stimulation%20in%20psychiatry%20and%20its%20effects%20on%20cognition&amp;journal=Nat%20Rev%20Neurol&amp;doi=10.1038%2Fnrneurol.2010.30&amp;volume=6&amp;publication_year=2010&amp;author=Hoy%2CKE&amp;author=Fitzgerald%2CPB">
                    Google Scholar</a> 
                </p></li><li data-counter="7."><p id="ref-CR7">Chen, R. <i>et al</i>. Depression of motor cortex excitability by low‐frequency transcranial magnetic stimulation. <i>Neurology</i> <b>48</b>, 1398 (1997).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1212/WNL.48.5.1398" data-track-action="article reference" href="https://doi.org/10.1212%2FWNL.48.5.1398" aria-label="Article reference 7" data-doi="10.1212/WNL.48.5.1398">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2s3pslynug%3D%3D" aria-label="CAS reference 7">CAS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Depression%20of%20motor%20cortex%20excitability%20by%20low%E2%80%90frequency%20transcranial%20magnetic%20stimulation&amp;journal=Neurology&amp;doi=10.1212%2FWNL.48.5.1398&amp;volume=48&amp;publication_year=1997&amp;author=Chen%2CR">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">Nguyen, J. <i>et al</i>. Repetitive transcranial magnetic stimulation combined with cognitive training for the treatment of Alzheimer’s disease. <i>Neurophysiologie Clinique/Clinical Neurophysiology</i> <b>47</b>, 47 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.neucli.2017.01.001" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neucli.2017.01.001" aria-label="Article reference 8" data-doi="10.1016/j.neucli.2017.01.001">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Repetitive%20transcranial%20magnetic%20stimulation%20combined%20with%20cognitive%20training%20for%20the%20treatment%20of%20Alzheimer%E2%80%99s%20disease&amp;journal=Neurophysiologie%20Clinique%2FClinical%20Neurophysiology&amp;doi=10.1016%2Fj.neucli.2017.01.001&amp;volume=47&amp;publication_year=2017&amp;author=Nguyen%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="9."><p id="ref-CR9">O Doherty, J. E. <i>et al</i>. Active tactile exploration using a brain–machine–brain interface. <i>Nature</i> <b>479</b>, 228 (2011).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/nature10489" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature10489" aria-label="Article reference 9" data-doi="10.1038/nature10489">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXht1ylt73N" aria-label="CAS reference 9">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2011Natur.479..228O" aria-label="ADS reference 9">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20tactile%20exploration%20using%20a%20brain%E2%80%93machine%E2%80%93brain%20interface&amp;journal=Nature&amp;doi=10.1038%2Fnature10489&amp;volume=479&amp;publication_year=2011&amp;author=O%20Doherty%2CJE">
                    Google Scholar</a> 
                </p></li><li data-counter="10."><p id="ref-CR10">Flesher, S. N. <i>et al</i>. Intracortical microstimulation of human somatosensory cortex. <i>Sci Transl Med</i> <b>8</b>, 141r (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1126/scitranslmed.aaf8083" data-track-action="article reference" href="https://doi.org/10.1126%2Fscitranslmed.aaf8083" aria-label="Article reference 10" data-doi="10.1126/scitranslmed.aaf8083">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Intracortical%20microstimulation%20of%20human%20somatosensory%20cortex&amp;journal=Sci%20Transl%20Med&amp;doi=10.1126%2Fscitranslmed.aaf8083&amp;volume=8&amp;publication_year=2016&amp;author=Flesher%2CSN">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Romo, R., Hernández, A., Zainos, A. &amp; Salinas, E. Somatosensory discrimination based on cortical microstimulation. <i>Nature</i> <b>392</b>, 387 (1998).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/32891" data-track-action="article reference" href="https://doi.org/10.1038%2F32891" aria-label="Article reference 11" data-doi="10.1038/32891">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK1cXitlSiurw%3D" aria-label="CAS reference 11">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1998Natur.392..387R" aria-label="ADS reference 11">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Somatosensory%20discrimination%20based%20on%20cortical%20microstimulation&amp;journal=Nature&amp;doi=10.1038%2F32891&amp;volume=392&amp;publication_year=1998&amp;author=Romo%2CR&amp;author=Hern%C3%A1ndez%2CA&amp;author=Zainos%2CA&amp;author=Salinas%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="12."><p id="ref-CR12">Min, B., Marzelli, M. J. &amp; Yoo, S. Neuroimaging-based approaches in the brain-computer interface. <i>Trends Biotechnol</i> <b>28</b>, 552 (2010).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.tibtech.2010.08.002" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tibtech.2010.08.002" aria-label="Article reference 12" data-doi="10.1016/j.tibtech.2010.08.002">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXht12qurvP" aria-label="CAS reference 12">CAS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuroimaging-based%20approaches%20in%20the%20brain-computer%20interface&amp;journal=Trends%20Biotechnol&amp;doi=10.1016%2Fj.tibtech.2010.08.002&amp;volume=28&amp;publication_year=2010&amp;author=Min%2CB&amp;author=Marzelli%2CMJ&amp;author=Yoo%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="13."><p id="ref-CR13">Pais-Vieira, M., Lebedev, M., Kunicki, C., Wang, J. &amp; Nicolelis, M. A. A brain-to-brain interface for real-time sharing of sensorimotor information. <i>Sci Rep-Uk</i> <b>3</b>, 1319 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/srep01319" data-track-action="article reference" href="https://doi.org/10.1038%2Fsrep01319" aria-label="Article reference 13" data-doi="10.1038/srep01319">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3sXpvFWjtr4%3D" aria-label="CAS reference 13">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2013NatSR...3E1319P" aria-label="ADS reference 13">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20brain-to-brain%20interface%20for%20real-time%20sharing%20of%20sensorimotor%20information&amp;journal=Sci%20Rep-Uk&amp;doi=10.1038%2Fsrep01319&amp;volume=3&amp;publication_year=2013&amp;author=Pais-Vieira%2CM&amp;author=Lebedev%2CM&amp;author=Kunicki%2CC&amp;author=Wang%2CJ&amp;author=Nicolelis%2CMA">
                    Google Scholar</a> 
                </p></li><li data-counter="14."><p id="ref-CR14">Rao, R. P. <i>et al</i>. A direct brain-to-brain interface in humans. <i>Plos One</i> <b>9</b>, e111332 (2014).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pone.0111332" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pone.0111332" aria-label="Article reference 14" data-doi="10.1371/journal.pone.0111332">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2014PLoSO...9k1332R" aria-label="ADS reference 14">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20direct%20brain-to-brain%20interface%20in%20humans&amp;journal=Plos%20One&amp;doi=10.1371%2Fjournal.pone.0111332&amp;volume=9&amp;publication_year=2014&amp;author=Rao%2CRP">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">Yoo, S., Kim, H., Filandrianos, E., Taghados, S. J. &amp; Park, S. Non-invasive brain-to-brain interface (BBI): establishing functional links between two brains. <i>Plos One</i> <b>8</b>, e60410 (2013).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1371/journal.pone.0060410" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pone.0060410" aria-label="Article reference 15" data-doi="10.1371/journal.pone.0060410">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3sXmtFWqsL8%3D" aria-label="CAS reference 15">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2013PLoSO...860410Y" aria-label="ADS reference 15">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-invasive%20brain-to-brain%20interface%20%28BBI%29%3A%20establishing%20functional%20links%20between%20two%20brains&amp;journal=Plos%20One&amp;doi=10.1371%2Fjournal.pone.0060410&amp;volume=8&amp;publication_year=2013&amp;author=Yoo%2CS&amp;author=Kim%2CH&amp;author=Filandrianos%2CE&amp;author=Taghados%2CSJ&amp;author=Park%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="16."><p id="ref-CR16">Li, G. &amp; Zhang, D. Brain-computer interface controlled cyborg: establishing a functional information transfer pathway from human brain to cockroach brain. <i>Plos One</i> <b>11</b>, e150667 (2016).</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain-computer%20interface%20controlled%20cyborg%3A%20establishing%20a%20functional%20information%20transfer%20pathway%20from%20human%20brain%20to%20cockroach%20brain&amp;journal=Plos%20One&amp;volume=11&amp;publication_year=2016&amp;author=Li%2CG&amp;author=Zhang%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="17."><p id="ref-CR17">Feng, Z. <i>et al</i>. A remote control training system for rat navigation in complicated environment. <i>J Zhejiang Univ-Sc A</i> <b>8</b>, 323 (2007).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1631/jzus.2007.A0323" data-track-action="article reference" href="https://doi.org/10.1631%2Fjzus.2007.A0323" aria-label="Article reference 17" data-doi="10.1631/jzus.2007.A0323">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20remote%20control%20training%20system%20for%20rat%20navigation%20in%20complicated%20environment&amp;journal=J%20Zhejiang%20Univ-Sc%20A&amp;doi=10.1631%2Fjzus.2007.A0323&amp;volume=8&amp;publication_year=2007&amp;author=Feng%2CZ">
                    Google Scholar</a> 
                </p></li><li data-counter="18."><p id="ref-CR18">Talwar, S. K. <i>et al</i>. Behavioural neuroscience: Rat navigation guided by remote control. <i>Nature</i> <b>417</b>, 37 (2002).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1038/417037a" data-track-action="article reference" href="https://doi.org/10.1038%2F417037a" aria-label="Article reference 18" data-doi="10.1038/417037a">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD38XjsFOgtLs%3D" aria-label="CAS reference 18">CAS</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2002Natur.417...37T" aria-label="ADS reference 18">ADS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Behavioural%20neuroscience%3A%20Rat%20navigation%20guided%20by%20remote%20control&amp;journal=Nature&amp;doi=10.1038%2F417037a&amp;volume=417&amp;publication_year=2002&amp;author=Talwar%2CSK">
                    Google Scholar</a> 
                </p></li><li data-counter="19."><p id="ref-CR19">Wang, Y. <i>et al</i>. Visual cue-guided rat cyborg for automatic navigation [research frontier]. <i>IEEE Comput Intell M</i> <b>10</b>, 42 (2015).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/MCI.2015.2405318" data-track-action="article reference" href="https://doi.org/10.1109%2FMCI.2015.2405318" aria-label="Article reference 19" data-doi="10.1109/MCI.2015.2405318">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20cue-guided%20rat%20cyborg%20for%20automatic%20navigation%20%5Bresearch%20frontier%5D&amp;journal=IEEE%20Comput%20Intell%20M&amp;doi=10.1109%2FMCI.2015.2405318&amp;volume=10&amp;publication_year=2015&amp;author=Wang%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="20."><p id="ref-CR20">Yu, Y. <i>et al</i>. Intelligence-augmented rat cyborgs in maze solving. <i>Plos One</i> <b>11</b>, e147754 (2016).</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Intelligence-augmented%20rat%20cyborgs%20in%20maze%20solving&amp;journal=Plos%20One&amp;volume=11&amp;publication_year=2016&amp;author=Yu%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="21."><p id="ref-CR21">Chavarriaga, R., Sobolewski, A. &amp; Millán, J. D. R. Errare machinale est: the use of error-related potentials in brain-machine interfaces. <i>Front Neurosci-Switz</i> <b>8</b>, 208 (2014).</p><p><a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Errare%20machinale%20est%3A%20the%20use%20of%20error-related%20potentials%20in%20brain-machine%20interfaces&amp;journal=Front%20Neurosci-Switz&amp;volume=8&amp;publication_year=2014&amp;author=Chavarriaga%2CR&amp;author=Sobolewski%2CA&amp;author=Mill%C3%A1n%2CJDR">
                    Google Scholar</a> 
                </p></li><li data-counter="22."><p id="ref-CR22">Hermer-Vazquez, L. <i>et al</i>. Rapid learning and flexible memory in “habit” tasks in rats trained with brain stimulation reward. <i>Physiol Behav</i> <b>84</b>, 753 (2005).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.physbeh.2005.03.007" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.physbeh.2005.03.007" aria-label="Article reference 22" data-doi="10.1016/j.physbeh.2005.03.007">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXktVOhurY%3D" aria-label="CAS reference 22">CAS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Rapid%20learning%20and%20flexible%20memory%20in%20%E2%80%9Chabit%E2%80%9D%20tasks%20in%20rats%20trained%20with%20brain%20stimulation%20reward&amp;journal=Physiol%20Behav&amp;doi=10.1016%2Fj.physbeh.2005.03.007&amp;volume=84&amp;publication_year=2005&amp;author=Hermer-Vazquez%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="23."><p id="ref-CR23">Paxinos, G. &amp; Watson, C. <i>The rat brain in stereotaxic coordinates</i>. (Elsevier, Academic Press, Amsterdam, 2014).</p></li><li data-counter="24."><p id="ref-CR24">Xu, K., Zhang, J., Zhou, H., Lee, J. C. T. &amp; Zheng, X. A novel turning behavior control method for rat-robot through the stimulation of ventral posteromedial thalamic nucleus. <i>Behav Brain Res</i> <b>298</b>, 150 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.bbr.2015.11.002" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.bbr.2015.11.002" aria-label="Article reference 24" data-doi="10.1016/j.bbr.2015.11.002">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20novel%20turning%20behavior%20control%20method%20for%20rat-robot%20through%20the%20stimulation%20of%20ventral%20posteromedial%20thalamic%20nucleus&amp;journal=Behav%20Brain%20Res&amp;doi=10.1016%2Fj.bbr.2015.11.002&amp;volume=298&amp;publication_year=2016&amp;author=Xu%2CK&amp;author=Zhang%2CJ&amp;author=Zhou%2CH&amp;author=Lee%2CJCT&amp;author=Zheng%2CX">
                    Google Scholar</a> 
                </p></li><li data-counter="25."><p id="ref-CR25">Martinez-Leon, J., Cano-Izquierdo, J. &amp; Ibarrola, J. Are low cost Brain Computer Interface headsets ready for motor imagery applications? <i>Expert Syst Appl</i> <b>49</b>, 136 (2016).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.eswa.2015.11.015" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.eswa.2015.11.015" aria-label="Article reference 25" data-doi="10.1016/j.eswa.2015.11.015">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20low%20cost%20Brain%20Computer%20Interface%20headsets%20ready%20for%20motor%20imagery%20applications%3F&amp;journal=Expert%20Syst%20Appl&amp;doi=10.1016%2Fj.eswa.2015.11.015&amp;volume=49&amp;publication_year=2016&amp;author=Martinez-Leon%2CJ&amp;author=Cano-Izquierdo%2CJ&amp;author=Ibarrola%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="26."><p id="ref-CR26">Pfurtscheller, G., Neuper, C., Flotzinger, D. &amp; Pregenzer, M. EEG-based discrimination between imagination of right and left hand movement. <i>Electroencephalography and clinical Neurophysiology</i> <b>103</b>, 642 (1997).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S0013-4694(97)00080-1" data-track-action="article reference" href="https://doi.org/10.1016%2FS0013-4694%2897%2900080-1" aria-label="Article reference 26" data-doi="10.1016/S0013-4694(97)00080-1">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK1c3gs1Glug%3D%3D" aria-label="CAS reference 26">CAS</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=EEG-based%20discrimination%20between%20imagination%20of%20right%20and%20left%20hand%20movement&amp;journal=Electroencephalography%20and%20clinical%20Neurophysiology&amp;doi=10.1016%2FS0013-4694%2897%2900080-1&amp;volume=103&amp;publication_year=1997&amp;author=Pfurtscheller%2CG&amp;author=Neuper%2CC&amp;author=Flotzinger%2CD&amp;author=Pregenzer%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="27."><p id="ref-CR27">Wolpaw, J. R., Birbaumer, N., McFarland, D. J., Pfurtscheller, G. &amp; Vaughan, T. M. Brain–computer interfaces for communication and control. <i>Clin Neurophysiol</i> <b>113</b>, 767 (2002).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/S1388-2457(02)00057-3" data-track-action="article reference" href="https://doi.org/10.1016%2FS1388-2457%2802%2900057-3" aria-label="Article reference 27" data-doi="10.1016/S1388-2457(02)00057-3">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Brain%E2%80%93computer%20interfaces%20for%20communication%20and%20control&amp;journal=Clin%20Neurophysiol&amp;doi=10.1016%2FS1388-2457%2802%2900057-3&amp;volume=113&amp;publication_year=2002&amp;author=Wolpaw%2CJR&amp;author=Birbaumer%2CN&amp;author=McFarland%2CDJ&amp;author=Pfurtscheller%2CG&amp;author=Vaughan%2CTM">
                    Google Scholar</a> 
                </p></li><li data-counter="28."><p id="ref-CR28">Kumar, S., Mamun, K. &amp; Sharma, A. CSP-TSM: Optimizing the performance of Riemannian tangent space mapping using common spatial pattern for MI-BCI. <i>Comput Biol Med</i> <b>91</b>, 231 (2017).</p><p><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.compbiomed.2017.10.025" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.compbiomed.2017.10.025" aria-label="Article reference 28" data-doi="10.1016/j.compbiomed.2017.10.025">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=CSP-TSM%3A%20Optimizing%20the%20performance%20of%20Riemannian%20tangent%20space%20mapping%20using%20common%20spatial%20pattern%20for%20MI-BCI&amp;journal=Comput%20Biol%20Med&amp;doi=10.1016%2Fj.compbiomed.2017.10.025&amp;volume=91&amp;publication_year=2017&amp;author=Kumar%2CS&amp;author=Mamun%2CK&amp;author=Sharma%2CA">
                    Google Scholar</a> 
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-018-36885-0?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>This work was supported by the National Key Research and Development Program of China (grant number 2017YFC1308501, 2017YFB1002503); the National Natural Science Foundation of China (grant numbers 31627802); the Public Projects of Zhejiang province (Grant No. 2016C33059); Zhejiang Provincial Natural Science Foundation of China (LR15F020001); and the Fundamental Research Funds for the Central Universities (grant number 2016XZZX001-10).</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>Sheng Yuan and Lipeng Huang contributed equally.</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Qiushi Academy for Advanced Studies (QAAS), Zhejiang University, Hangzhou, China</p><p>Shaomin Zhang, Sheng Yuan, Xiaoxiang Zheng &amp; Kedi Xu</p></li><li id="Aff2"><p>Department of Computer Science, Zhejiang University, Hangzhou, China</p><p>Lipeng Huang, Zhaohui Wu &amp; Gang Pan</p></li><li id="Aff3"><p>Department of Biomedical Engineering, Key Laboratory of Biomedical Engineering of Education Ministry, Zhejiang University, Hangzhou, China</p><p>Shaomin Zhang, Sheng Yuan, Xiaoxiang Zheng &amp; Kedi Xu</p></li><li id="Aff4"><p>Zhejiang Provincial Key Laboratory of Cardio-Cerebral Vascular Detection Technology and Medicinal Effectiveness Appraisal, Zhejiang University, Hangzhou, China</p><p>Shaomin Zhang, Sheng Yuan, Xiaoxiang Zheng &amp; Kedi Xu</p></li></ol><h3 id="contributions">Contributions</h3><p>S.Z. conceived the study, designed the BBI system, performed the experiment and wrote the manuscript. S.Y. designed the BBI system, performed the experiment, analyzed the data and wrote the manuscript. L.H. designed the BBI system, performed the experiment, analyzed the data and wrote the manuscript. K.X. conceived and designed the study, analyzed the data and wrote the manuscript. G.P. conceived and designed the study. Chaonan Yu prepared and trained the animals. All authors reviewed the manuscript.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:xukd@zju.edu.cn">Kedi Xu</a> or <a id="corresp-c2" href="mailto:gpan@zju.edu.cn">Gang Pan</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar1">Competing Interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note:</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Supplementary information"><div id="Sec15-section"><h2 id="Sec15">Supplementary information</h2></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Human%20Mind%20Control%20of%20Rat%20Cyborg%E2%80%99s%20Continuous%20Locomotion%20with%20Wireless%20Brain-to-Brain%20Interface&amp;author=Shaomin%20Zhang%20et%20al&amp;contentID=10.1038%2Fs41598-018-36885-0&amp;copyright=The%20Author%28s%29&amp;publication=2045-2322&amp;publicationDate=2019-02-04&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41598-018-36885-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41598-018-36885-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Zhang, S., Yuan, S., Huang, L. <i>et al.</i> Human Mind Control of Rat Cyborg’s Continuous Locomotion with Wireless Brain-to-Brain Interface.
                    <i>Sci Rep</i> <b>9</b>, 1321 (2019). https://doi.org/10.1038/s41598-018-36885-0</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-018-36885-0?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2018-04-03">03 April 2018</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2018-11-16">16 November 2018</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2019-02-04">04 February 2019</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41598-018-36885-0</span></p></li></ul></div></div></div></div></section>
            </div></div>
  </body>
</html>
