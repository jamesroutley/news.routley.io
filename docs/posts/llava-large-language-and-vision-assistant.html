<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://llava-vl.github.io/">Original</a>
    <h1>LLaVA: Large Language and Vision Assistant</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            <h3>LLaVA: Large Language and Vision Assistant</h3>
            

            </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div>
      <p>
        <h4>
          LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna
          for general-purpose visual and language understanding,
          achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.
        </h4>
      </p>
    </div>
  </section>

  <section>
    
  </section>

  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Abstract</h2>
          <div>
            <p>
              Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.
              </p><ol type="1">
                <li><b>Multimodal Instruct Data</b>. <span>We present the first attempt to use <a href="https://openai.com/research/gpt-4">language-only GPT-4</a> to generate multimodal language-image instruction-following data. </span></li>
                <li><b>LLaVA Model</b>. <span>We introduce <it><b>LLaVA</b> (<b>L</b>arge <b>L</b>anguage-<b>a</b>nd-<b>V</b>ision <b>A</b>ssistant)</it>, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.</span></li>
                <li><b>Performance</b>. <span>Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset.
                  When fine-tuned on <a href="https://scienceqa.github.io/">Science QA</a>, the synergy of LLaVA and GPT-4  achieves a new state-of-the-art accuracy of 92.53%.</span></li>
                <li><b>Open-source</b>. <span>We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.</span></li>
              </ol>  
           
  
          </div>
        </div>
      </div>
        
    </div>
  </section>


  
<section>
  <!-- Results. -->
  <div>
    <div>
      <h2><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"/> Multimodal Instrucion-Following Data</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div>

  <div>
    <div>
      <p>
          Based on the COCO dataset, we interact with langauge-only GPT-4, and collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. Please check out ``LLaVA-Instruct-150K&#39;&#39;&#39; on 
          <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K">[HuggingFace Dataset]</a>.
        
        </p>              
    </div>
  </div>


</div></section>
 

<section>
  <!-- Results. -->
  <div>
    <div>
      <h2><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"/> LLaVA: Large Language-and-Vision Assistant</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div>

  <div>
    <div>
      <div> 
        <p>
          LLaVa connects pre-trained <a href="https://openai.com/research/clip">CLIP ViT-L/14</a> visual encoder and large language model <a href="https://github.com/facebookresearch/llama">LLaMA</a>, using a simple projection matrix.   We consider a two-stage instruction-tuning procedure:
          </p><ul type="1">
            <li><b>Stage 1: Pre-training for Feature Alignment</b>. <span>Only the projection matrix is updated, based on a subset of CC3M.</span></li>
            <li><b>Stage 2: Fine-tuning End-to-End.</b>. <span>Both the projection matrix and LLM are updated for two different use senarios: 
              <ul type="1">
                <li> <b>Visual Chat</b>: LLaVA is fine-tuned on our generated multimodal instruction-following data for daily user-oriented applications. 
                </li><li> <b>Science QA</b>: LLaVA is fine-tuned on this multimodal reasonsing dataset for the science domain.</li>
              </ul>  
          </span></li></ul><p>  
          Please check out ``LLaVA-13b-delta-v0&#39;&#39; model checkpoint on 
          <a href="https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0">[HuggingFace Models]</a>.
        </p>
      </div>
      <centering>
        <p><img id="teaser" width="70%" src="https://llava-vl.github.io/images/llava_arch.png"/>     
        </p>

      
      </centering>           
    </div>
  </div>


</div></section>
  


<section>
  <!-- Results. -->
  <div>
    <div>
      <h2><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"/> Performance</h2>
    </div>
  </div>



  <!-- </div> -->
  <!--/ Results. -->    
<div>


  <!-- Grounedtext2img. -->
  <div>
    <div>
      <h2><img id="painting_icon" width="4%" src="https://cdn-icons-png.flaticon.com/512/1698/1698535.png"/> <span>Visual Chat:</span> Towards building multimodal GPT-4 level chatbot  </h2>
      
      <p><a href="https://plotly.com/~lichunyuan24/5/?share_key=d78QObaCAYCIy8PJpe3gd1" target="_blank" title="llava_gpt4_pie">  <img id="painting_icon" width="90%" src="https://llava-vl.github.io/images/pie_llava_gpt4.png"/> </a>

    </p>

    <p><b>An evaluation dataset with 30 unseen images is constructed: each image is assocaited with three types of instructions: conversation, detailed description and complex reasoning. This leads to 90 new language-image instructions, on which we test LLaVA and GPT-4, and use GPT-4 to rate their responses from score 1 to 10. The summed score and relative score per type is reported. Overall, LLaVA achieves 85.1% relative score compared with GPT-4, indicating the effectinvess of the proposed self-instruct method in multimodal settings</b>               
    </p></div>
  </div>

  <!-- Grounedtext2img. -->
  <div>
    <div>
      <h2> <img id="painting_icon" width="3%" src="https://scienceqa.github.io/img/logo.png"/><span> Science QA:</span> New SoTA with the synergy of LLaVA with GPT-4</h2>
      
      <p><a href="https://plotly.com/~lichunyuan24/1/?share_key=v4opE3TJpxqQ08RYsDD4iv" target="_blank" title="Plot 1"><img id="painting_icon" width="65%" src="https://llava-vl.github.io/images/bar_llava_gpt4_scienceqa.png"/></a>
        
    </p>
        <p><b>LLaVA alones achieve 90.92%. We use the text-only GPT-4 as the juedge, to predict the final answer based on its own previous answers and the LLaVA answers. This ``GPT-4 as juedge&#39;&#39; scheme yields a new SOTA 92.53%.</b>
              
    </p></div>
  </div>
</div></section>




<section>

  <div>
    <p>
      <h2> Examples on Visual Instruction Following</h2>
    </p>
  </div>

      

    <div>
    <p><img id="teaser" width="35%" src="https://llava-vl.github.io/images/cmp_ironing.png"/>
      <img id="teaser" width="38%" src="https://llava-vl.github.io/images/cmp_chicken_nugget.png"/>
    </p>
    </div>  

  

    <div>
      <p>
         <h2>Optical character recognition (OCR)</h2>
      </p>
      </div>  

      

  

  


  
</section>

  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>
  @article{liu2023llava,
    author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title       = {Visual Instruction Tuning},
    publisher   = {arXiv:2304.08485},
    year        = {2023}
  }
  </code></pre>
    </div>
  </section>
  
  <section id="Acknowledgement">
    <div>
      <h2>Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We thank the LLaMA team for giving us access to their models, and open-source projects, including Alpaca and Vicuna.
      </p>

      <p>
<b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
</p>

      <p>
      <a href="https://github.com/Computer-Vision-in-the-Wild/"><img id="painting_icon" width="10%" src="https://avatars.githubusercontent.com/u/97258247?s=200&amp;v=4"/></a> 
      Related Links: 
      <a href="https://react-vl.github.io/">[REACT]</a>  
      <a href="https://gligen.github.io/">[GLIGEN]</a> 
      <a href="https://github.com/Computer-Vision-in-the-Wild/">[Computer Vision in the Wild (CVinW)]</a> 
      <a href="https://instruction-tuning-with-gpt-4.github.io/">[Insutrction Tuning with GPT-4]</a>      
      </p>    
    </div>
  </section>

  



</div>
  </body>
</html>
