<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://questdb.com/blog/async-profiler-kernel-bug/">Original</a>
    <h1>A kernel bug froze my machine: Debugging an async-profiler deadlock</h1>
    
    <div id="readability-page-1" class="page"><div><p>
  QuestDB is the open-source time-series database for demanding workloads—from trading floors to mission control.
  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.
  Native support for Parquet and SQL keeps your data portable, AI-ready—no vendor lock-in.</p>
<hr/>
<p>I&#39;ve been a Linux user since the late 90s, starting with <a href="http://www.slackware.com/">Slackware</a> on an underpowered AMD K6.
Over the years I&#39;ve hit plenty of bugs, but the last decade has been remarkably stable - until a kernel bug started
freezing my machine whenever I used <a href="https://github.com/async-profiler/async-profiler/">async-profiler</a>.</p>
<p>I&#39;m not a kernel developer, but I found myself poking around kernel source code to understand the problem better and
figure out what was going on under the hood.</p>
<h2 id="the-problem"><a href="#the-problem">The problem</a></h2>
<p>I was about to start an investigation of latency spikes in QuestDB reported by a user. To do that, I wanted to use the async-profiler to capture <a href="https://github.com/async-profiler/async-profiler/blob/3a493bedc4c6463ba19970018c50e0c9d7dcbfda/docs/Heatmap.md">CPU heatmaps</a>.</p>
<figure><div><p><img alt="Screenshot of async-profiler heatmap" src="https://jamiepalatnik.com/images/blog/2025-12-11/heatmap.webp" loading="lazy"/></p><figcaption>Async-profiler heatmap example</figcaption></div></figure>
<p>However, when I tried to attach the profiler, my machine froze completely. It did not respond to any
keys, it was impossible to switch to a terminal, it did not respond to SSH. The only way to recover was to hard
reboot it. I tried to start QuestDB with the profiler already configured to start at launch - the same result, a frozen machine almost immediately after the launch.</p>
<p>I thought that was weird, this had not happened to me in years. It was already late in the evening, I felt tired anyway so I decided to call it a day. There was a tiny chance I was hallucinating
and the problem would go away by itself overnight. A drowning man will clutch at a straw after all.</p>
<p>The next day, I tried to attach the profiler again - same result, frozen machine. Async-profiler integration in QuestDB is a relatively new feature, so I thought
there might be a bug in the integration code, perhaps a regression in the recent QuestDB release. So I built an older QuestDB version: The same result, frozen machine. This was puzzling - I positively
knew this worked before. How do I know? Because I worked on the <a href="https://github.com/questdb/questdb/pull/6150">integration code</a> not too long ago, and I tested the hell out of it.</p>
<p>This was a strong hint that the problem was not in QuestDB, but rather in the environment. I&#39;ve gotten lazy since my
Slackware days and I have been using Ubuntu for years now and I realized
that I had recently updated Ubuntu to the latest version: 25.10. Could it be that the problem is in the new Ubuntu
version?</p>
<p>At this point I started Googling around and I found a <a href="https://github.com/async-profiler/async-profiler/issues/1578">report</a> created by a fellow
performance aficionado, <a href="https://x.com/forked_franz">Francesco Nigro</a>, describing exactly the same problem: machine
freeze when using async-profiler. This was the final
confirmation I was not hallucinating! Except Francesco is using Fedora, not Ubuntu. However, his Fedora uses the same kernel version as my Ubuntu: 6.17.
I booted a machine with an older Ubuntu, started QuestDB and attached the profiler and it worked like a charm. This
was yet another indication that the problem was in the system, possibly even in the kernel. This allowed me to
narrow down my Google keywords and find this <a href="https://patchew.org/linux/176216460800.2601451.733142302683512228.tip-bot2@tip-bot2/">kernel patch</a> which talks about the very same problem!</p>
<p>I found it quite interesting: A kernel bug triggered by async-profiler causing machine freezes on recent mainstream distributions. After some poking I found a workaround:
Start the profiler with <code>-e ctimer</code> option to avoid using the problematic kernel feature. I tried the workaround and indeed, with this option, the profiler worked fine and my machine did not freeze.</p>
<p>Normally I&#39;d move on, but I was curious. What exactly is going on under the hood? Why is it freezing? What is this
<code>ctimer</code> thing? What exactly is the bug and how does the patch work? So I decided to dig deeper.</p>
<h2 id="how-async-profiler-works"><a href="#how-async-profiler-works">How Async-profiler Works</a></h2>
<p>Async-profiler is a sampling profiler. It periodically interrupts threads in the profiled application and collects their stack traces. The collected stack traces are then aggregated and visualized in
various ways (flame graphs are one of the most popular visualizations). It has multiple ways to interrupt the profiled application, the most common one is using <a href="https://man7.org/linux/man-pages/man2/perf_event_open.2.html"><code>perf_events</code></a> kernel feature. This is how it works by default on Linux
assuming <a href="https://www.kernel.org/doc/html/latest/admin-guide/perf-security.html">kernel paranoia settings</a> allow it.</p>
<h3 id="perf_events-under-the-hood"><a href="#perf_events-under-the-hood">perf_events Under the Hood</a></h3>
<p>The <code>perf_events</code> subsystem is a powerful Linux kernel feature for performance monitoring. For CPU profiling, async-profiler uses
a software event called <code>cpu-clock</code>, which is driven by high-resolution timers (<a href="https://docs.kernel.org/timers/hrtimers.html">hrtimers</a>) in the kernel.</p>
<p>Here&#39;s the sequence of events during profiling:</p>
<ol>
<li><strong>Setup</strong>: For each thread in the profiled application, async-profiler opens a <a href="https://github.com/async-profiler/async-profiler/blob/c6c2fc1497ab06ff357146ed91fd27c0ce5640ba/src/perfEvents_linux.cpp#L606"><code>perf_event</code></a> file descriptor configured
to generate a signal after a specified interval of CPU time (e.g., 10ms).</li>
<li><strong>Arming the event</strong>: The profiler calls <a href="https://github.com/async-profiler/async-profiler/blob/c6c2fc1497ab06ff357146ed91fd27c0ce5640ba/src/perfEvents_linux.cpp#L645"><code>ioctl(fd, PERF_EVENT_IOC_REFRESH, 1)</code></a> to arm the event for exactly one sample.
This is a one-shot mechanism, combined with the <code>RESET</code> at the end of the handler. The goal is to measure application CPU time only and exclude the signal&#39;s handler own overhead.</li>
<li><strong>Timer fires</strong>: When the configured CPU time elapses, the kernel&#39;s <a href="https://github.com/torvalds/linux/blob/2424e146bee00ddb4d4f79d3224f54634ca8d2bc/kernel/time/hrtimer.c#L1761">hrtimer fires</a> and delivers a signal to the target thread.</li>
<li><strong>Signal handler</strong>: Async-profiler&#39;s signal handler captures the stack trace and <a href="https://github.com/async-profiler/async-profiler/blob/c6c2fc1497ab06ff357146ed91fd27c0ce5640ba/src/perfEvents_linux.cpp#L704">records the sample</a>.
At the end of the handler,
it resets the counter and <a href="https://github.com/async-profiler/async-profiler/blob/c6c2fc1497ab06ff357146ed91fd27c0ce5640ba/src/perfEvents_linux.cpp#L709-L710">re-arms the event</a> for the next sample:</li>
</ol>
<div><div><div><pre><p><span>ioctl(fd, PERF_EVENT_IOC_RESET, 0);    // Clear the counter</span></p><p><span>ioctl(fd, PERF_EVENT_IOC_REFRESH, 1);  // Arm for exactly 1 more sample</span></p></pre></div></div></div>
<p>This cycle repeats for the duration of the profiling session, creating a stream of stack trace samples that are later
aggregated into flame graphs or heatmaps.</p>
<h2 id="the-kernel-bug"><a href="#the-kernel-bug">The Kernel Bug</a></h2>
<p>The kernel bug that caused my machine to freeze was introduced by commit <a href="https://github.com/torvalds/linux/commit/18dbcbfabfff">18dbcbfabfff (&#34;perf: Fix the POLL_HUP delivery breakage&#34;)</a>.
Ironically, this commit was fixing a different bug, but it introduced a deadlock in the cpu-clock event handling.</p>
<p>Here&#39;s what happens in the buggy kernel when the <code>PERF_EVENT_IOC_REFRESH(1)</code> counter reaches zero:</p>
<ol>
<li><a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11824">hrtimer</a> fires for cpu-clock event - <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11749"><code>perf_swevent_hrtimer()</code></a> is called (inside hrtimer interrupt context)</li>
<li><a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11769"><code>perf_swevent_hrtimer()</code> calls</a> <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L10300"><code>__perf_event_overflow()</code></a> - this processes the counter overflow</li>
<li><code>__perf_event_overflow()</code> <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L10333">decides to stop</a> the event (counter reached 0 after <code>PERF_EVENT_IOC_REFRESH(1)</code>) - calls <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11916"><code>cpu_clock_event_stop()</code></a></li>
<li><a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11861"><code>cpu_clock_event_stop()</code></a> calls <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11800"><code>perf_swevent_cancel_hrtimer()</code></a> - this calls <a href="https://github.com/torvalds/linux/blob/18dbcbfabfffc4a5d3ea10290c5ad27f22b0d240/kernel/events/core.c#L11813"><code>hrtimer_cancel()</code></a> to cancel the timer</li>
<li><strong>DEADLOCK</strong>: <a href="https://github.com/torvalds/linux/blob/2424e146bee00ddb4d4f79d3224f54634ca8d2bc/kernel/time/hrtimer.c#L1483-L1494"><code>hrtimer_cancel()</code></a> waits for the hrtimer callback to complete - but we ARE inside the hrtimer callback! The system hangs forever waiting for itself</li>
</ol>
<p>The function <code>hrtimer_cancel()</code> is a blocking call - it spins waiting for any active callback to finish.</p>
<div><div><div><pre><p><span>int hrtimer_cancel(struct hrtimer *timer)</span></p><p><span>{</span></p><p><span>	int ret;</span></p><p><span>	do {</span></p><p><span>		ret = hrtimer_try_to_cancel(timer);</span></p><p><span>		if (ret &lt; 0)</span></p><p><span>			hrtimer_cancel_wait_running(timer);</span></p><p><span>	} while (ret &lt; 0);</span></p><p><span>	return ret;</span></p><p><span>}</span></p></pre></div></div></div>
<p>When called from inside that same callback, it waits forever. Since this happens in interrupt context with interrupts disabled on the CPU,
that CPU becomes completely unresponsive. When this happens on multiple CPUs (which it does, since each thread has its own
<code>perf_event</code>), the entire system freezes.</p>
<details><summary>Click to see the deadlock visualized</summary></details>
<h2 id="the-fix"><a href="#the-fix">The Fix</a></h2>
<p>The <a href="https://github.com/torvalds/linux/commit/eb3182ef0405ff2f6668fd3e5ff9883f60ce8801">kernel patch</a> fixes this deadlock with two changes:</p>
<ol>
<li>Replace <code>hrtimer_cancel()</code> with <code>hrtimer_try_to_cancel()</code></li>
</ol>
<div><div><div><pre><p><span>- hrtimer_cancel(&amp;hwc-&gt;hrtimer);</span></p><p><span>+ hrtimer_try_to_cancel(&amp;hwc-&gt;hrtimer);</span></p></pre></div></div></div>
<p><a href="https://github.com/torvalds/linux/blob/2424e146bee00ddb4d4f79d3224f54634ca8d2bc/kernel/time/hrtimer.c#L1347"><code>hrtimer_try_to_cancel()</code></a> is non-blocking - it returns immediately with:</p>
<ul>
<li><code>0</code> if the timer was not active</li>
<li><code>1</code> if the timer was successfully cancelled</li>
<li><code>-1</code> if the timer callback is currently running</li>
</ul>
<p>Unlike <code>hrtimer_cancel()</code>, it doesn&#39;t spin waiting for the callback to finish. So when called from within the callback itself, it simply returns <code>-1</code> and continues.</p>
<ol start="2">
<li>Use <code>PERF_HES_STOPPED</code> flag as a deferred stop signal</li>
</ol>
<p>The stop function now sets a flag:</p>
<div><div><div><pre><p><span>static void cpu_clock_event_stop(struct perf_event *event, int flags)</span></p><p><span>{</span></p><p><span>+   event-&gt;hw.state = PERF_HES_STOPPED;</span></p><p><span>    perf_swevent_cancel_hrtimer(event);</span></p><p><span>    ...</span></p><p><span>}</span></p></pre></div></div></div>
<p>And the hrtimer callback checks this flag:</p>
<div><div><div><pre><p><span>static enum hrtimer_restart perf_swevent_hrtimer(struct hrtimer *hrtimer)</span></p><p><span>{</span></p><p><span>-   if (event-&gt;state != PERF_EVENT_STATE_ACTIVE)</span></p><p><span>+   if (event-&gt;state != PERF_EVENT_STATE_ACTIVE ||</span></p><p><span>+       event-&gt;hw.state &amp; PERF_HES_STOPPED)</span></p><p><span>        return HRTIMER_NORESTART;</span></p></pre></div></div></div>
<h3 id="how-it-works-together"><a href="#how-it-works-together">How It Works Together</a></h3>
<p>When <code>cpu_clock_event_stop()</code> is called from within the hrtimer callback:</p>
<ol>
<li><code>PERF_HES_STOPPED</code> flag is set</li>
<li><code>hrtimer_try_to_cancel()</code> returns <code>-1</code> (callback running) - but doesn&#39;t block</li>
<li>Execution returns up the call stack back to <code>perf_swevent_hrtimer()</code></li>
<li><code>perf_swevent_hrtimer()</code> completes and returns <code>HRTIMER_NORESTART</code> (because <code>__perf_event_overflow()</code> returned <code>1</code>, indicating the event should stop)</li>
<li>The hrtimer subsystem sees <code>HRTIMER_NORESTART</code> and <a href="https://github.com/torvalds/linux/blob/2424e146bee00ddb4d4f79d3224f54634ca8d2bc/kernel/time/hrtimer.c#L1776-L1778">doesn&#39;t reschedule the timer</a></li>
</ol>
<p>When <code>cpu_clock_event_stop()</code> is called from outside the callback (normal case):</p>
<ol>
<li><code>PERF_HES_STOPPED</code> flag is set</li>
<li><code>hrtimer_try_to_cancel()</code> returns <code>0</code> or <code>1</code> - timer is cancelled immediately</li>
<li>If by chance the callback fires before cancellation completes, it sees <code>PERF_HES_STOPPED</code> and returns <code>HRTIMER_NORESTART</code></li>
</ol>
<p>The <code>PERF_HES_STOPPED</code> flag acts as a safety net to make sure the timer stops regardless of the race between setting the flag and the timer firing.</p>
<h3 id="debugging-a-kernel"><a href="#debugging-a-kernel">Debugging a kernel</a></h3>
<p>The explanation above is my understanding of the kernel bug and the fix based on reading the kernel source code.
I am a hacker, I like to tinker. A theoretical understanding is one thing, but I wanted to see it in action.
But how do you even debug a kernel? I&#39;m not a kernel developer, but I decided to try. Here is how I did it.</p>
<p>My intuition was to use <a href="https://www.qemu.org/">QEMU</a> since it allows one to emulate or virtualize a full machine. QEMU also has a built-in <a href="https://www.sourceware.org/gdb/">GDB</a>
server that allows you to <a href="https://qemu-project.gitlab.io/qemu/system/gdb.html">connect GDB to the emulated machine</a>.</p>
<h3 id="setting-up-qemu-with-ubuntu"><a href="#setting-up-qemu-with-ubuntu">Setting up QEMU with Ubuntu</a></h3>
<p>I downloaded an Ubuntu 25.10 ISO image and created a new empty VM disk image:</p>
<div><div><div><pre><p><span>$ qemu-img create -f qcow2 ubuntu-25.10.qcow2 20G</span></p></pre></div></div></div>
<p>Then I launched QEMU to install Ubuntu:</p>
<div><div><div><pre><p><span>$ qemu-system-x86_64 \</span></p><p><span>    -enable-kvm \</span></p><p><span>    -m 4096 \</span></p><p><span>    -smp 4 \</span></p><p><span>    -drive file=ubuntu-25.10.qcow2,if=virtio \</span></p><p><span>    -cdrom ubuntu-25.10-desktop-amd64.iso \</span></p><p><span>    -boot d \</span></p><p><span>    -vga qxl</span></p></pre></div></div></div>
<p>The second command boots the VM from the ISO image and allows me to install Ubuntu on the VM disk image. I went through
the installation process as usual. I probably could have used a server edition or a prebuilt image, but at this
point I was already in unknown territory, so I wanted to make other things as simple as possible.</p>
<figure><div><p><img alt="QEMU screen showing Ubuntu installation" src="https://jamiepalatnik.com/images/blog/2025-12-11/ubuntu.webp" loading="lazy"/></p><figcaption>Ubuntu installation in QEMU</figcaption></div></figure>
<p>Once the installation was complete, I rebooted the VM:</p>
<div><div><div><pre><p><span>$ qemu-system-x86_64 \</span></p><p><span>    -enable-kvm \</span></p><p><span>    -m 4096 \</span></p><p><span>    -smp 4 \</span></p><p><span>    -drive file=ubuntu-25.10.qcow2,if=virtio \</span></p><p><span>    -netdev user,id=net0,hostfwd=tcp::9000-:9000 \</span></p><p><span>    -device virtio-net-pci,netdev=net0 \</span></p><p><span>    -monitor tcp:127.0.0.1:55555,server,nowait \</span></p><p><span>    -s</span></p></pre></div></div></div>
<p>and downloaded, unpacked and started QuestDB:</p>
<div><div><div><pre><p><span>$ curl -L https://github.com/questdb/questdb/releases/download/9.2.2/questdb-9.2.2-rt-linux-x86-64.tar.gz -o questdb.tar.gz</span></p><p><span>$ tar -xzvf questdb.tar.gz</span></p><p><span>$ cd questdb-9.2.2-rt-linux-x86-64</span></p><p><span>$ ./bin/questdb start</span></p></pre></div></div></div>
<p>This was meant to validate that QuestDB works in the VM at all. Firefox was already installed in the Ubuntu desktop
edition, so I just opened <code>http://localhost:9000</code> in Firefox and verified QuestDB web console was up and running.</p>
<figure><div><p><img alt="QuestDB web console running in Ubuntu in QEMU" src="https://jamiepalatnik.com/images/blog/2025-12-11/console.webp" loading="lazy"/></p><figcaption>QuestDB web console in QEMU</figcaption></div></figure>
<p>The next step was to stop QuestDB and start it with a profiler attached:</p>
<div><div><div><pre><p><span>$ ./bin/questdb stop</span></p><p><span>$ ./bin/questdb start -p</span></p></pre></div></div></div>
<p>At this point, I expected the virtual machine to freeze. However, it didn&#39;t. It was responsive as if nothing bad had
happened. That was a bummer. I wanted to see the deadlock in action!
I thought that perhaps QEMU is in a way shielding the virtual machine from the bug. But then I realized that the
default Ubuntu uses paranoia settings that prevent <code>perf_events</code> from working properly and async-profiler <a href="https://github.com/async-profiler/async-profiler/blob/c6c2fc1497ab06ff357146ed91fd27c0ce5640ba/src/profiler.cpp#L995-L998">falls back to
using <code>ctimer</code></a> when <code>perf_events</code> are restricted. The kernel bug specifically lives in the <code>perf_events</code> hrtimer
code path, so we must force async-profiler to use that path to trigger the bug.</p>
<p>To fix this, I changed the paranoia settings:</p>
<div><div><div><pre><p><span>$ echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid</span></p></pre></div></div></div>
<p>After this, I restarted QuestDB with the profiler again:</p>
<div><div><div><pre><p><span>$ ./bin/questdb stop</span></p><p><span>$ ./bin/questdb start -p</span></p></pre></div></div></div>
<p>And this time, the virtual machine froze as expected! Success! I was able to reproduce the problem in QEMU!</p>
<h3 id="attaching-gdb-to-qemu"><a href="#attaching-gdb-to-qemu">Attaching GDB to QEMU</a></h3>
<p>Now that I was able to reproduce the problem in QEMU, I wanted to attach GDB to the emulated machine to see the deadlock in action.</p>
<p>Let&#39;s start <code>GDB</code> on the host machine and connect it to QEMU&#39;s built-in GDB server:</p>
<div><div><div><pre><p><span>$ gdb</span></p><p><span>GNU gdb (Ubuntu 16.3-1ubuntu2) 16.3</span></p><p><span>[...]</span></p><p><span>(gdb) target remote :1234</span></p><p><span>Remote debugging using :1234</span></p><p><span>warning: No executable has been specified and target does not support</span></p><p><span>determining executable automatically.  Try using the &#34;file&#34; command.</span></p><p><span>0xffffffff82739398 in ?? ()</span></p><p><span>(gdb) info threads</span></p><p><span>  Id   Target Id                    Frame</span></p><p><span>* 1    Thread 1.1 (CPU#0 [running]) 0xffffffff82739398 in ?? ()</span></p><p><span>  2    Thread 1.2 (CPU#1 [running]) 0xffffffff82739398 in ?? ()</span></p><p><span>  3    Thread 1.3 (CPU#2 [running]) 0xffffffff827614d3 in ?? ()</span></p><p><span>  4    Thread 1.4 (CPU#3 [running]) 0xffffffff82739398 in ?? ()</span></p><p><span>(gdb) thread apply all bt</span></p></pre></div></div></div>
<blockquote>
<p>Side note: We just casually attached a debugger to a live kernel! How cool is that?</p>
</blockquote>
<p>We can see 4 threads corresponding to the 4 CPUs in the VM. The <code>bt</code> command shows the stack traces of all threads, but there is not much useful information since we don&#39;t have the kernel symbols loaded in GDB.
Let&#39;s fix this. I am lazy again and take advantage of running exactly the same kernel version as the host machine so I can use the host&#39;s kernel image and symbol files.</p>
<p>On the host machine, we need to add repositories with debug symbols and install the debug symbols for the running kernel:</p>
<div><div><div><pre><p><span>echo &#34;deb http://ddebs.ubuntu.com questing main restricted universe multiverse&#34; | sudo tee /etc/apt/sources.list.d/ddebs.list</span></p><p><span>echo &#34;deb http://ddebs.ubuntu.com questing-updates main restricted universe multiverse&#34; | sudo tee -a /etc/apt/sources.list.d/ddebs.list</span></p><p><span>echo &#34;deb http://ddebs.ubuntu.com questing-proposed main restricted universe multiverse&#34; | sudo tee -a /etc/apt/sources.list.d/ddebs.list</span></p><p><span>sudo apt install ubuntu-dbgsym-keyring</span></p><p><span>sudo apt update</span></p><p><span>sudo apt install linux-image-$(uname -r)-dbgsym</span></p></pre></div></div></div>
<p>With the debug symbols installed, I started GDB again and loaded the kernel image and symbols:</p>
<div><div><div><pre><p><span>$ gdb /usr/lib/debug/boot/vmlinux-$(uname -r)</span></p><p><span>GNU gdb (Ubuntu 16.3-1ubuntu2) 16.3</span></p><p><span>[...]</span></p><p><span>gdb) target remote :1234</span></p><p><span>Remote debugging using :1234</span></p><p><span>0xffffffff9e9614d3 in ?? ()</span></p><p><span>[...]</span></p><p><span>(gdb) info threads</span></p><p><span>  Id   Target Id                    Frame</span></p><p><span>* 1    Thread 1.1 (CPU#0 [running]) 0xffffffff9e9614d3 in ?? ()</span></p><p><span>  2    Thread 1.2 (CPU#1 [running]) 0xffffffff9e939398 in ?? ()</span></p><p><span>  3    Thread 1.3 (CPU#2 [running]) 0xffffffff9e9614d3 in ?? ()</span></p><p><span>  4    Thread 1.4 (CPU#3 [running]) 0xffffffff9e9614d3 in ?? ()</span></p><p><span>(gdb) quit</span></p></pre></div></div></div>
<p>and symbols were still NOT resolved! I had to capitulate and ask a LLM for help. After a bit of brainstorming, we
realized that the kernel is compiled with <a href="https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html">KASLR</a> enabled, so the kernel is loaded at a random address at each boot.
The simplest way to fix this is to disable KASLR, I could not care less about security in my test VM. To disable KASLR, I edited the GRUB configuration, added the <code>nokaslr</code> parameter, updated GRUB and rebooted the VM:</p>
<div><div><div><pre><p><span>$ vim /etc/default/grub</span></p><p><span># Add nokaslr to the GRUB_CMDLINE_LINUX_DEFAULT line</span></p><p><span>GRUB_CMDLINE_LINUX_DEFAULT=&#34;quiet splash nokaslr&#34;</span></p><p><span>$ sudo update-grub</span></p><p><span>$ sudo reboot</span></p></pre></div></div></div>
<p>Then I set the paranoia settings again, started QuestDB with the profiler and attached GDB again. This time, the symbols were resolved correctly!</p>
<div><div><div><pre><p><span>$ gdb /usr/lib/debug/boot/vmlinux-$(uname -r)</span></p><p><span>GNU gdb (Ubuntu 16.3-1ubuntu2) 16.3</span></p><p><span>[...]</span></p><p><span>(gdb) target remote :1234</span></p><p><span>[...]</span></p><p><span>(gdb) info threads</span></p><p><span>  Id   Target Id                    Frame</span></p><p><span>* 1    Thread 1.1 (CPU#0 [running]) csd_lock_wait (csd=0xffff88813bd3a460) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p><p><span>  2    Thread 1.2 (CPU#1 [running]) csd_lock_wait (csd=0xffff88813bd3b520) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p><p><span>  3    Thread 1.3 (CPU#2 [running]) hrtimer_try_to_cancel (timer=0xffff88802343d028) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>  4    Thread 1.4 (CPU#3 [running]) hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p></pre></div></div></div>
<p>This looks much better! We can see that the first 2 threads are stuck in <code>csd_lock_wait()</code> function, presumably waiting for locks held by the other CPUs
and threads 3 and 4 are in <code>hrtimer_try_to_cancel()</code>.</p>
<p>The threads 3 and 4 are the interesting ones since they execute a function related to the kernel bug we are investigating.
Let&#39;s switch to thread 4 and see its stack trace:</p>
<div><div><div><pre><p><span>(gdb) thread 4</span></p><p><span>[Switching to thread 4 (Thread 1.4)]</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>1359	in /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c</span></p><p><span>(gdb) bt</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>#1  hrtimer_cancel (timer=timer@entry=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1488</span></p><p><span>#2  0xffffffff81700605 in perf_swevent_cancel_hrtimer (event=&lt;optimized out&gt;) at /build/linux-8YMEfB/linux-6.17.0/kernel/events/core.c:11818</span></p><p><span>#3  perf_swevent_cancel_hrtimer (event=0xffff888023439f80) at /build/linux-8YMEfB/linux-6.17.0/kernel/events/core.c:11805</span></p><p><span>#4  cpu_clock_event_stop (event=0xffff888023439f80, flags=0) at /build/linux-8YMEfB/linux-6.17.0/kernel/events/core.c:11868</span></p><p><span>#5  0xffffffff81715488 in __perf_event_overflow (event=event@entry=0xffff888023439f80, throttle=throttle@entry=1, data=data@entry=0xffffc90002cd7cc0, regs=0xffffc90002cd7f48) at /build/linux-8YMEfB/linux-6.17.0/kernel/events/core.c:10338</span></p><p><span>#6  0xffffffff81716eaf in perf_swevent_hrtimer (hrtimer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/events/core.c:11774</span></p><p><span>#7  0xffffffff81538a03 in __run_hrtimer (cpu_base=&lt;optimized out&gt;, base=&lt;optimized out&gt;, timer=0xffff88802343a0e8, now=0xffffc90002cd7e58, flags=&lt;optimized out&gt;) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1761</span></p><p><span>#8  __hrtimer_run_queues (cpu_base=cpu_base@entry=0xffff88813bda1400, now=now@entry=48514890563, flags=flags@entry=2, active_mask=active_mask@entry=15) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1825</span></p><p><span>#9  0xffffffff8153995d in hrtimer_interrupt (dev=&lt;optimized out&gt;) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1887</span></p><p><span>#10 0xffffffff813c4ac8 in local_apic_timer_interrupt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/apic/apic.c:1039</span></p><p><span>#11 __sysvec_apic_timer_interrupt (regs=regs@entry=0xffffc90002cd7f48) at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/apic/apic.c:1056</span></p><p><span>#12 0xffffffff82621724 in instr_sysvec_apic_timer_interrupt (regs=0xffffc90002cd7f48) at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/apic/apic.c:1050</span></p><p><span>#13 sysvec_apic_timer_interrupt (regs=0xffffc90002cd7f48) at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/apic/apic.c:1050</span></p><p><span>#14 0xffffffff81000f0b in asm_sysvec_apic_timer_interrupt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/include/asm/idtentry.h:574</span></p><p><span>#15 0x00007b478171db80 in ?? ()</span></p><p><span>#16 0x0000000000000001 in ?? ()</span></p><p><span>#17 0x0000000000000000 in ?? ()</span></p></pre></div></div></div>
<p>We can see the exact sequence of function calls leading to the deadlock: <code>hrtimer_try_to_cancel()</code> called from <code>cpu_clock_event_stop()</code>, called from <code>__perf_event_overflow()</code>, called from <code>perf_swevent_hrtimer()</code>.
This matches our understanding of the bug perfectly! This is the infinite loop in <code>hrtimer_cancel()</code> that causes the
deadlock.</p>
<h2 id="forensics-and-playing-god"><a href="#forensics-and-playing-god">Forensics and Playing God</a></h2>
<p>Okay, I have to admit that seeing a kernel stack trace is already somewhat satisfying, but we have a live (well,
half-dead) kernel under a debugger. Let&#39;s have some fun. I want to touch the deadlock and understand why it took
down the  whole machine, and see if we can perform a miracle and bring it back to life.</p>
<h4>Confirming the suspect</h4>
<p>We know <code>hrtimer_cancel</code> is waiting for a callback to finish. But which callback? The stack trace says <code>perf_swevent_cancel_hrtimer</code>,
but let&#39;s verify the hrtimer struct in memory actually points to the function we blame.</p>
<p>I switched to the stuck thread (Thread 4 in my case) and looked at frame #0:</p>
<div><div><div><pre><p><span>(gdb) thread 4</span></p><p><span>[Switching to thread 4 (Thread 1.4)]</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>1359	in /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c</span></p><p><span>(gdb) frame 0</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>1359	in /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c</span></p><p><span>(gdb) print *timer</span></p><p><span>$1 = {node = {node = {__rb_parent_color = 18446612682661667048, rb_right = 0x0, rb_left = 0x0}, expires = 48514879474}, _softexpires = 48514879474, function = 0xffffffff81716dd0 &lt;perf_swevent_hrtimer&gt;, base = 0xffff88813bda1440, state = 0 &#39;\000&#39;, is_rel = 0 &#39;\000&#39;, is_soft = 0 &#39;\000&#39;, is_hard = 1 &#39;\001&#39;}</span></p></pre></div></div></div>
<p>Let me explain these GDB commands: <code>frame 0</code> selects the innermost stack frame - the function currently executing.
In a backtrace, frame 0 is the current function, frame 1 is its caller, frame 2 is the caller&#39;s caller, and so on.
By selecting frame 0, I can inspect local variables and parameters in <code>hrtimer_try_to_cancel()</code>.</p>
<p>The <code>print *timer</code> command dereferences the <code>timer</code> pointer and displays the contents of the <code>struct hrtimer</code>:</p>
<div><div><div><pre><p><span>struct hrtimer {</span></p><p><span>    struct timerqueue_node  node;</span></p><p><span>    ktime_t                 _softexpires;</span></p><p><span>    enum hrtimer_restart    (*function)(struct hrtimer *);</span></p><p><span>    struct hrtimer_clock_base *base;</span></p><p><span>    u8                      state;</span></p><p><span>    u8                      is_rel;</span></p><p><span>    u8                      is_soft;</span></p><p><span>    u8                      is_hard;</span></p><p><span>};</span></p></pre></div></div></div>
<p>The key field here is <code>function</code> - a pointer to a callback function that takes a <code>struct hrtimer *</code> and returns
<code>enum hrtimer_restart</code>. This callback is invoked when the timer fires. GDB shows it points to <code>0xffffffff81716dd0</code> and helpfully resolves this address to <code>perf_swevent_hrtimer</code>. Since we&#39;re currently <em>inside</em> <code>perf_swevent_hrtimer</code> (look at frame #6 in our backtrace above), this confirms the self-deadlock: the timer is trying to cancel itself while its own callback is still running!</p>
<h4>The Mystery of the &#34;Other&#34; CPUs</h4>
<p>One question remained: If CPUs 3 and 4 are deadlocked in a loop, why did the entire machine freeze? Why couldn&#39;t I just SSH in and kill the process?
The answer lies in those other threads we saw earlier, stuck in <code>csd_lock_wait</code>:</p>
<div><div><div><pre><p><span>(gdb) thread 1</span></p><p><span>[Switching to thread 1 (Thread 1.1)]</span></p><p><span>#0  csd_lock_wait (csd=0xffff88813bd3a460) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p><p><span>warning: 351	/build/linux-8YMEfB/linux-6.17.0/kernel/smp.c: No such file or directory</span></p></pre></div></div></div>
<p><code>CSD</code> stands for Call Function Single Data. In Linux, when one CPU wants another CPU to do something (like flush a TLB
or stop a perf_event), it sends an IPI (Inter-Processor Interrupt).
If the target CPU is busy with interrupts disabled (which is exactly the case for our deadlocked CPUs 3 and 4), it never responds.</p>
<p>The sender (CPU 0) sits there <a href="https://github.com/torvalds/linux/blob/fe90f3967bdb3e13f133e5f44025e15f943a99c5/include/asm-generic/barrier.h#L260-L274">spinning</a>,
waiting for the other <a href="https://github.com/torvalds/linux/blob/83e6384374bac8a9da3411fae7f24376a7dbd2a3/kernel/smp.c#L547">CPU to say &#34;Done!&#34;</a>. Eventually, all CPUs end up waiting for the stuck CPUs and the entire
system grinds to a halt.</p>
<h4>Performing a Kernel Resurrection</h4>
<p>This is the part where the real black magic starts. We know the kernel is stuck in this loop in <code>hrtimer_cancel</code>:</p>
<div><div><div><pre><p><span>do {</span></p><p><span>    ret = hrtimer_try_to_cancel(timer);</span></p><p><span>} while (ret &lt; 0);</span></p></pre></div></div></div>
<p>As long as <code>hrtimer_try_to_cancel</code> returns <code>-1</code> (which it does, because the callback is running), the loop continues
forever.</p>
<p>But we have GDB. We can change reality.</p>
<p>If we force the function to return <code>0</code> (meaning &#34;timer not active&#34;), the loop should break, <code>cpu_clock_event_stop</code>
should finish, and the kernel should unfreeze. It might crash 1 millisecond later because we left the timer in an
inconsistent state, but perhaps it&#39;s worth trying.</p>
<p>First, let&#39;s double-check we are in the innermost frame, inside <code>hrtimer_try_to_cancel</code>:</p>
<div><div><div><pre><p><span>(gdb) thread 4</span></p><p><span>[Switching to thread 4 (Thread 1.4)]</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>warning: 1359	/build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c: No such file or directory</span></p><p><span>(gdb) frame 0</span></p><p><span>#0  hrtimer_try_to_cancel (timer=0xffff88802343a0e8) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>1359	in /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c</span></p></pre></div></div></div>
<p>Use the GDB <code>finish</code> command to let the function run to completion and pause right when it returns to the caller:</p>

<p>We are now sitting at line 1490, right at the check <code>if (ret &lt; 0)</code>.</p>
<div><div><div><pre><p><span>int hrtimer_cancel(struct hrtimer *timer)</span></p><p><span>{</span></p><p><span>	int ret;</span></p><p><span>	do {</span></p><p><span>		ret = hrtimer_try_to_cancel(timer);</span></p><p><span>		if (ret &lt; 0) // &lt;-- we are here</span></p><p><span>			hrtimer_cancel_wait_running(timer);</span></p><p><span>	} while (ret &lt; 0);</span></p><p><span>	return ret;</span></p><p><span>}</span></p></pre></div></div></div>
<p>On x86_64, integer return values are passed in the <code>%rax</code> register.
Since <code>hrtimer_try_to_cancel</code> returns an <code>int</code> (32-bit), we can use <code>$eax</code> (the lower 32 bits of <code>%rax</code>):</p>

<p>Exactly as expected. <code>-1</code> means the timer callback is running, so the loop will continue.
But since the CPU is paused, we can overwrite this value. We can lie to the kernel and tell it the timer was
successfully cancelled (return code 1) or inactive (return code 0). I chose 0.</p>
<div><div><div><pre><p><span>(gdb) set $eax = 0</span></p><p><span>(gdb) print $eax</span></p><p><span>$3 = 0</span></p></pre></div></div></div>
<p>I crossed my fingers and unpaused the VM:</p>
<div><div><div><pre><p><span>(gdb) continue</span></p><p><span>Continuing.</span></p></pre></div></div></div>
<p>And it did nothing. The VM was still frozen. Let&#39;s see what is going on:</p>
<div><div><div><pre><p><span>(gdb) info threads</span></p><p><span>  Id   Target Id                    Frame</span></p><p><span>  1    Thread 1.1 (CPU#0 [running]) csd_lock_wait (csd=0xffff88813bd3a460) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p><p><span>  2    Thread 1.2 (CPU#1 [running]) csd_lock_wait (csd=0xffff88813bd3b520) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p><p><span>  3    Thread 1.3 (CPU#2 [running]) hrtimer_try_to_cancel (timer=0xffff88802343d028) at /build/linux-8YMEfB/linux-6.17.0/kernel/time/hrtimer.c:1359</span></p><p><span>* 4    Thread 1.4 (CPU#3 [running]) csd_lock_wait (csd=0xffff88813bd3b560) at /build/linux-8YMEfB/linux-6.17.0/kernel/smp.c:351</span></p></pre></div></div></div>
<p>Now, thread 4 is also stuck in <code>csd_lock_wait</code>, just like threads 1 and 2. We managed to escape from the infinite
loop in thread 4, but thread 3 is still stuck in <code>hrtimer_try_to_cancel</code>.</p>
<p>We could try the same trick on thread 3, but would this be enough to unfreeze the entire system? For starters, we
tricked the kernel into thinking the timer was inactive, but in reality it is still active.  This is very thin ice
to skate on - we might have just created more problems for ourselves. And more importantly, even if the kernel could
escape the deadlock, the profiler would immediately try to re-arm the timer again, leading us back into the same
deadlock.</p>
<p>So I decided to give up on the resurrection attempt. The kernel was stuck, but at least I understood the problem now and I was pretty happy with my newly acquired kernel debugging skills.</p>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>While I couldn&#39;t perform a miracle and resurrect the frozen kernel, I walked away with a much deeper understanding
of the machinery behind Linux <code>perf_events</code> and hrtimers. I learned how to set up QEMU for kernel debugging, how to attach GDB to a live kernel, and how to inspect kernel data structures in memory.</p>
<p>For QuestDB users, the takeaway is simple: if you are on a kernel version 6.17, use the <code>-e ctimer</code> flag when
profiling. It bypasses the buggy <code>perf_events</code> hrtimer path entirely. Or just wait for either the kernel fix to land
in your distro or the next QuestDB release, which will <a href="https://github.com/questdb/questdb/pull/6473">include</a> an
async-profiler version that <a href="https://github.com/async-profiler/async-profiler/pull/1599">works around this issue</a>.</p>
<p>As for me, I’m going back to my code. The next time my machine freezes, I might just reboot it like a normal person. But where is the fun in that?</p>
<h2 id="addendum:-the-second-resurrection-attempt"><a href="#addendum:-the-second-resurrection-attempt">Addendum: The Second Resurrection Attempt</a></h2>
<p>After writing this post, I kept thinking about that failed resurrection attempt. We got so close: We broke one CPU out of the deadlock, but the other was still stuck. I should have tried harder!
So I started QEMU again, reproduced the deadlock, and this time came with a plan: use GDB to force kernel to kill the QuestDB Java process, so the profiler can&#39;t re-arm the timer.</p>
<p>First, I needed to find the Java process. The <code>perf_event</code> structure has an owner field pointing to the task that created it:</p>
<div><div><div><pre><p><span>(gdb) print event-&gt;owner</span></p><p><span>$1 = (struct task_struct *) 0xffff88810b2ed100</span></p><p><span>(gdb) print ((struct task_struct *)0xffff88810b2ed100)-&gt;comm</span></p><p><span>$2 = &#34;java&#34;</span></p><p><span>(gdb) print ((struct task_struct *)0xffff88810b2ed100)-&gt;pid</span></p><p><span>$3 = 4488</span></p></pre></div></div></div>
<p>Great, we found the Java process with PID 4488. Now, how do you kill a process when the kernel is deadlocked and can&#39;t process signals?
You store the signal directly in memory. <code>SIGKILL</code> is signal 9, which means bit 8 in the signal bitmask:</p>
<div><div><div><pre><p><span>(gdb) set ((struct task_struct *)0xffff88810b2ed100)-&gt;signal-&gt;shared_pending.signal.sig[0] = 0x100</span></p></pre></div></div></div>
<p>With the pending kill signal in place, I broke the deadlock loop as before. The system hit another deadlock - a different CPU was now stuck waiting for a spinlock.
The lock value showed it was held:</p>
<div><div><div><pre><p><span>(gdb) print *((unsigned int *)0xffff88813bda1400)</span></p><p><span>$4 = 1</span></p></pre></div></div></div>
<p>I forcibly released the lock <em>(what could possibly go wrong?)</em>:</p>
<div><div><div><pre><p><span>(gdb) set *((unsigned int *)0xffff88813bda1400) = 0</span></p></pre></div></div></div>
<p>Then broke another <code>hrtimer</code> loop on a different CPU. It was like playing whack-a-mole with deadlocks - each Java thread had its own <code>perf_event</code>, and they were all hitting the same bug.</p>
<p>After a few rounds of this, I ran continue and checked the threads:</p>
<div><div><div><pre><p><span>(gdb) continue</span></p><p><span>Continuing.</span></p><p><span>^C</span></p><p><span>Thread 4 received signal SIGINT, Interrupt.</span></p><p><span>0xffffffff82621d8b in pv_native_safe_halt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/paravirt.c:82</span></p><p><span>(gdb) info threads</span></p><p><span>  Id   Target Id                    Frame</span></p><p><span>  1    Thread 1.1 (CPU#0 [halted ]) 0xffffffff82621d8b in pv_native_safe_halt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/paravirt.c:82</span></p><p><span>  2    Thread 1.2 (CPU#1 [halted ]) 0xffffffff82621d8b in pv_native_safe_halt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/paravirt.c:82</span></p><p><span>  3    Thread 1.3 (CPU#2 [halted ]) 0xffffffff82621d8b in pv_native_safe_halt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/paravirt.c:82</span></p><p><span>* 4    Thread 1.4 (CPU#3 [halted ]) 0xffffffff82621d8b in pv_native_safe_halt () at /build/linux-8YMEfB/linux-6.17.0/arch/x86/kernel/paravirt.c:82</span></p><p><span>(gdb) continue</span></p></pre></div></div></div>
<figure><div><p><img alt="terminal screen showing gdb output" src="https://jamiepalatnik.com/images/blog/2025-12-11/gdb.webp" loading="lazy"/></p><figcaption>GDB output showing all CPUs resting peacefully in halt state</figcaption></div></figure>
<p>I looked at the QEMU window. The desktop was responsive. The mouse moved. Java was gone - killed by the <code>SIGKILL</code> we planted before breaking the deadlock.
We actually did it. We resurrected a kernel-deadlocked machine by lying to it about return values, forcibly releasing locks, and planting signals in process memory.
Would I recommend this in production (or anywhere outside a lab)? Absolutely not. But was it fun? Totally!</p>
<figure><div><p><img alt="QEMU screen showing Ubuntu desktop" src="https://jamiepalatnik.com/images/blog/2025-12-11/lazarus.webp" loading="lazy"/></p><figcaption>Lazarus rising from the dead</figcaption></div></figure></div></div>
  </body>
</html>
