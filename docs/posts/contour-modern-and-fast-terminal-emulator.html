<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/contour-terminal/contour">Original</a>
    <h1>Contour: Modern and fast terminal emulator</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>Adding a new entry to the bestiary, the Minotaur.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/deep-q-leaning-a-maze/minotaur.png" title="minotaur" data-thumbnail="minotaur.png" data-sub-html="&lt;h2&gt;Minotaur by stable diffusion&lt;/h2&gt;&lt;p&gt;minotaur&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="minotaur.png" data-srcset="minotaur.png, minotaur.png 1.5x, minotaur.png 2x" data-sizes="auto" alt="minotaur.png"/>
    </a><figcaption>Minotaur by stable diffusion</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>As a first step toward Reinforcement Learning (RL) let’s write a maze solver using Deep Q-Network (DQN).</p>
<h2 id="bellmans-equation">Bellman’s Equation</h2>
<p>To me DQN seems to be the RL technique requiring the least effort. All you need to do is to balance the left side of the Bellman’s equation with its right side:</p>
<p>$$Q(s, a) = R + \gamma . max_i(Q(s’, a_i))$$</p>
<p>For our purpose <code>Q()</code> is the neural network. <code>s</code> (aka. state) and <code>a</code> (aka. action) are the input of the network, here it would be the maze and the current position. <code>R</code> is the reward for taking action <code>a</code> (i.e. hitting a wall is a <code>-1</code>, keeping on the path is a <code>0</code> and finding the exit is a <code>10</code>). $\gamma$ (gamma) (aka. decay rate) is how much we discount future rewards.</p>
<p>And all we want, is for our network to be consistent by predicting that the expected value of being at position <code>s</code> and taking the action <code>a</code> is the same as having already done action <code>a</code> and having been rewarded for it if we keep playing optimally afterward.</p>
<h2 id="implementation">Implementation</h2>
<p>Because we are only trying to balance the Bellman’s equation we don’t need any extra cleverness. We only need possible positions to look at and evaluate (even if the network weights are totally random at initialization) and let the magic of gradient descent narrow down a consistent <code>Q()</code> for us.</p>
<p>Define the neural network</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>NeuralNetwork</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>        <span>self</span><span>.</span><span>linear_relu_stack</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>INPUT_SIZE</span><span>,</span> <span>HIDDEN_SIZE</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>HIDDEN_SIZE</span><span>,</span> <span>HIDDEN_SIZE</span><span>),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>ReLU</span><span>(),</span>
</span></span><span><span>            <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>HIDDEN_SIZE</span><span>,</span> <span>len</span><span>(</span><span>MOVES</span><span>)),</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>linear_relu_stack</span><span>(</span><span>x</span><span>)</span>        
</span></span><span><span>        <span>return</span> <span>logits</span>
</span></span></code></pre></div><p>I did several experiments with larger hidden layers, or deeper networks and found out that this simple tiny one was outperforming what I could get with much bigger ones. My guess is that the extra complexity was slowing down the training more than it was actually contributing to the quality of the answer.</p>
<p>Here we have several options for generating our training set:</p>
<ul>
<li>self play, we start at the entrance of the maze and move around according to some exploration rate</li>
<li>random position, teleport somewhere in the maze and do a single move</li>
<li>exhaustive play, teleport everywhere in the maze and try every move</li>
</ul>
<p>It feels like self play is the more realistic option for writing a Go engine, but for the purpose of outsmarting the Minotaur I went with exhaustive play. It lets me train much faster by batching a lot of positions and move together in one big vectorized pass of the network, instead of doing moves one by one.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>get_next_pos</span><span>(</span><span>maze</span><span>,</span> <span>rewards</span><span>,</span> <span>pos</span><span>,</span> <span>move</span><span>):</span>
</span></span><span><span>    <span>is_terminal</span> <span>=</span> <span>True</span> <span># default to a terminal state.</span>
</span></span><span><span>    <span>new_pos</span> <span>=</span> <span>pos</span> <span># default to forbidden move.</span>
</span></span><span><span>    <span>reward</span> <span>=</span> <span>HIT_WALL_PENALTY</span> <span># default to hitting a wall.</span>
</span></span><span><span>    <span>x</span><span>,</span> <span>y</span> <span>=</span> <span>pos</span>
</span></span><span><span>    <span>a</span><span>,</span> <span>b</span> <span>=</span> <span>maze</span><span>.</span><span>shape</span>
</span></span><span><span>    <span>i</span><span>,</span> <span>j</span> <span>=</span> <span>move</span>
</span></span><span><span>    <span>if</span> <span>0</span> <span>&lt;=</span> <span>x</span> <span>+</span> <span>i</span> <span>&lt;</span> <span>a</span> <span>and</span> <span>0</span> <span>&lt;=</span> <span>y</span> <span>+</span> <span>j</span> <span>&lt;</span> <span>b</span><span>:</span>
</span></span><span><span>        <span>new_pos</span> <span>=</span> <span>(</span><span>x</span> <span>+</span> <span>i</span><span>,</span> <span>y</span> <span>+</span> <span>j</span><span>)</span>
</span></span><span><span>        <span>reward</span> <span>=</span> <span>get_reward</span><span>(</span><span>rewards</span><span>,</span> <span>new_pos</span><span>)</span>
</span></span><span><span>        <span>is_terminal</span> <span>=</span> <span>maze</span><span>[</span><span>new_pos</span><span>]</span> <span>!=</span> <span>1</span>
</span></span><span><span>    <span>return</span> <span>new_pos</span><span>,</span> <span>reward</span><span>,</span> <span>move</span><span>,</span> <span>is_terminal</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_batch_exhaustive_search</span><span>():</span>
</span></span><span><span>    <span>batch</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>maze</span><span>,</span> <span>rewards</span> <span>=</span> <span>get_maze</span><span>()</span>
</span></span><span><span>    <span>for</span> <span>pos</span> <span>in</span> <span>(</span><span>maze</span> <span>==</span> <span>1</span><span>)</span><span>.</span><span>nonzero</span><span>()</span><span>.</span><span>tolist</span><span>():</span>
</span></span><span><span>        <span>for</span> <span>mm</span> <span>in</span> <span>list</span><span>(</span><span>MOVES</span><span>.</span><span>keys</span><span>()):</span>
</span></span><span><span>            <span>new_pos</span><span>,</span> <span>reward</span><span>,</span> <span>move</span><span>,</span> <span>is_terminal</span> <span>=</span> <span>get_next_pos</span><span>(</span><span>maze</span><span>,</span> <span>rewards</span><span>,</span> <span>pos</span><span>,</span> <span>mm</span><span>)</span>
</span></span><span><span>            <span>batch</span><span>.</span><span>append</span><span>((</span><span>pos</span><span>,</span> <span>move</span><span>,</span> <span>new_pos</span><span>,</span> <span>reward</span><span>,</span> <span>is_terminal</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>maze</span><span>,</span> <span>batch</span>
</span></span></code></pre></div><p>And now we just train long enough for Q to get stable. Have a look at what direction the network predict for each position at different training steps.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/deep-q-leaning-a-maze/policy.png" title="policy" data-thumbnail="policy.png" data-sub-html="&lt;h2&gt;Predictions for the exit&lt;/h2&gt;&lt;p&gt;policy&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="policy.png" data-srcset="policy.png, policy.png 1.5x, policy.png 2x" data-sizes="auto" alt="policy.png"/>
    </a><figcaption>Predictions for the exit</figcaption>
    </figure>
<p>Let’s compare the distances to the exit as computed by BFS with the policy’s predicted reward at each position.</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/deep-q-leaning-a-maze/policy2.png" title="policy2" data-thumbnail="policy2.png" data-sub-html="&lt;h2&gt;BFS Distance vs Predicted Reward&lt;/h2&gt;&lt;p&gt;policy2&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="policy2.png" data-srcset="policy2.png, policy2.png 1.5x, policy2.png 2x" data-sizes="auto" alt="policy2.png"/>
    </a><figcaption>BFS Distance vs Predicted Reward</figcaption>
    </figure>
<h2 id="extra-curicular-activities">Extra curicular activities</h2>
<p>Here’s a random bunch of things that could be used to improve the code:</p>
<ul>
<li>experience replay, if we implement self play it will train slowly, one option is to save the states we encounter and replay them in a batch.</li>
<li>target network, we can fix the <code>Q()</code> on the right side of the Bellman’s equation to a set of weight, run a bunch of training and only then update it with our new Q. This makes the training more stable. This also happen to be somewhat emulated by just running bigger batches so I went with that instead.</li>
<li>convolution, instead of flattening the maze into a 1d vector and feeding everything throug linear layers, we could feed a 2d matrix maze into a bunch of conv2d layers.</li>
<li>one_hot vs raw coordinates, I went with encoding positions in the maze as two one_hot encoded vectors, but another approach would be to feed the X and Y coordinate as a flaot and see what happen.</li>
</ul>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/rl/" target="_blank" rel="noopener noreffer ">https://github.com/peluche/rl/</a></p>


<h2 id="sources">Sources</h2>
<p>The maze’s walls were full of obscure writings, forbidden knowledge on this arcane spell left by Bellman eons ago, some of these writing are transcibed here:</p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/" target="_blank" rel="noopener noreffer ">https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/</a></li>
<li><a href="https://thenerdshow.com/maze.html" target="_blank" rel="noopener noreffer ">https://thenerdshow.com/maze.html</a></li>
<li><a href="https://tomroth.com.au/dqn-nnet/" target="_blank" rel="noopener noreffer ">https://tomroth.com.au/dqn-nnet/</a></li>
<li><a href="https://ai.stackexchange.com/questions/35184/how-do-i-design-the-network-for-deep-q-network" target="_blank" rel="noopener noreffer ">https://ai.stackexchange.com/questions/35184/how-do-i-design-the-network-for-deep-q-network</a></li>
<li><a href="https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes6.pdf" target="_blank" rel="noopener noreffer ">https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes6.pdf</a></li>
</ul>
<p>And a special thanks to <a href="https://github.com/changlinli" target="_blank" rel="noopener noreffer ">https://github.com/changlinli</a> and his awesome set of lectures at the <a href="https://www.recurse.com/scout/click?t=dcdcd5fced9bfab4a02b4dd6bb05199e" target="_blank" rel="noopener noreffer ">Recurse Center</a> for inspiring me to work on this.</p>
</div></div>
  </body>
</html>
