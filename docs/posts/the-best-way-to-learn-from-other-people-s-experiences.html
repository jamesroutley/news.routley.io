<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://commoncog.com/blog/how-to-learn-from-other-peoples-experiences/">Original</a>
    <h1>The Best Way to Learn From Other People&#39;s Experiences</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><article><header><section><span><a href="https://thevaluable.dev/tags/complexity/">#Complexity</a></span></section></header><section><picture>
<source srcset="https://thevaluable.dev/images/2021/measuring_complexity_env/complexity_time.webp" type="image/webp"/><img width="780" height="520" src="https://thevaluable.dev/images/2021/measuring_complexity_env/complexity_time.jpg" alt="The environment brings complexity overtime"/></picture><p>This article is part of a series about complexity metrics:</p><p>The alarm rings suddenly and intensely, waking you up from the food coma you were getting into. Red lights are on, sign of a major crisis. What an idea to eat a massive burger on a release day!</p><p>Everybody’s running in every direction, but nobody seems to go anywhere. Developers begin to type frenetically some nonsense in their terminals. You glance at the metrics: the CI is in the red, the logs are nuts, the dependency graph is through the rough. Everything’s crashing.</p><p>You begin to hear some screams of terrors, accompanying the annoying alarm. It’s the customer service, no doubts!</p><p>The day you’ve anxiously anticipated finally came. It’s over. Everything’s going down. The system collapses on itself. The Apocalypse.</p><p>Fast-forward a horrible death march of three months, your managers, instead of blaming the developers as they usually do, finally seem to understand that software complexity needs to be managed. They assign you the task to find and report the most complex parts of your massive codebase. Then, depending on the finding, a war council will be created to decide of a strategy.</p><p>Instinctively, you begin to launch your good old static analytics tools. But your general knowledge about systems make you pause a moment. Even if looking at the complexity <em>inside</em> the system is useful, it might be even also to also consider the environment <em>influencing</em> the system itself.</p><p>We often think that isolating the parts of a codebase is the key for good software, but total isolation is not possible nor desirable. It begs the question: how to find the most complex parts of our codebases while considering the impact of its environment?</p><p>We’ll try to answer this question in this article. More precisely, we’ll see:</p><ul><li>Pitfalls when considering the social environment of a system, like a codebase.</li><li>How to mix the rate of change of a codebase with complexity metrics.</li><li>What can be the benefits and drawbacks of aging code.</li><li>Studies about the correlation of number of developers and defects.</li><li>What’s the cognitive complexity metric.</li><li>Studies about capturing the thoughts and emotions of a team of developers.</li></ul><p>Are you ready to look at software in a slightly different way? Me neither, but let’s jump in anyway.</p><h2 id="pitfalls-when-considering-the-environment">Pitfalls when Considering the Environment</h2><p>When we consider real world systems, we need to always keep in mind that its environment (what’s outside the system of interest) has always <em>some</em> impact. It’s the case for a codebase, too:</p><ul><li>A codebase is written by humans for the machine to execute it, and for other humans to understand it. As a result, there is a strong social context in software development.</li><li>A codebase evolves over time. Its <a href="https://thevaluable.dev/fighting-software-entropy/">entropy</a> changes, too. Yet, static analytic tools often give information only about a snapshot of the codebase, as it was fixed in time.</li></ul><p>When we begin to add social metrics to find complexity, we need to be careful not to use these metrics against our peers. Blaming developers because we came up with some numbers “proving” that they did a bad job will only create a toxic, competitive culture. <a href="https://www.goodreads.com/en/book/show/35747076-accelerate" target="_blank" rel="noopener">It has been shown</a> that collaboration should, instead, be praised.</p><p>Additionally, measuring complexity in its social context is difficult, like any social study. Our social world is a highly dynamic, ever-changing system. It’s difficult to isolate what you want to measure, and it’s even more difficult to reduce the uncertainty of the <em>consequences</em> of these measurements. Many invisible factors can play a role in the consequences of our actions. Because of this complexity, our intuition, in this context, is likely to be wrong too.</p><p>In short, don’t use metrics related to productivity, cognition, or other human qualities, to squeeze as much work as possible from your exhausted team. Instead, they should be used to improve the processes, and the quality of the codebases. Code and processes are easier to change than people, and speaking about the defects of a codebase is often better perceived than speaking about people “defects”. Some team leaders out there sometimes forget that developers are people, not programs. Don’t make this mistake.</p><p>Tools are not inherently evils. That said, you can use a hammer to fix a house, or to look at someone’s brain. I wouldn’t advise for the later use.</p><h2 id="measuring-changes-with-code-churn">Measuring Changes With Code Churn</h2><p>We saw, in the <a href="https://thevaluable.dev/complexity-metrics-software/">previous article</a>, some complexity metrics we can use on a codebase <em>at a precise point in time</em>. Thinking about it, working with a snapshot is the easiest thing to do, because it’s easier to observe static systems. Yet, what’s easy to measure is not necessarily what we should measure, even if we have tendency, as humans, to conflate the two concepts.</p><p>If a codebase and its environment were really static, nothing would change after a first period of creation. No need to use complexity metrics anymore! But, in reality, a codebase changes often, creating both the opportunity to adapt and reflect The Ever Changing Real World™, and the complexity which will gives you headaches and nightmare.</p><p>That’s why, according to studies, the best metric to find complexity spots in your codebase is to measure the code churn. This metric measures the total number of line of codes added and deleted in a precise period of time.</p><p><a href="https://www.researchgate.net/publication/3188092_Predicting_Fault_Incidence_Using_Software_Change_History" target="_blank" rel="noopener">One of these studies</a> shows that measuring code churn is indeed more accurate than counting the line of codes to find complexity spots. <a href="https://www.researchgate.net/publication/220851904_Does_measuring_code_change_improve_fault_prediction" target="_blank" rel="noopener">Another studies</a> shows that code churn is also effective to predict future defects in a codebase.</p><p>There are two types of churn we can consider: absolute and relative churn. But, according to the studies above, they are both quite similar. To clear any misunderstanding, when I speak about code churn in this article, it refers to absolute code churn.</p><p>I can already see the question in your mind: how to calculate the code churn of a codebase?</p><p>The good old Git can give us the information we seek if we ask it politely. Many tools out there can calculate it for us, but I think it’s useful to know how to do it ourselves. We’ll use a shell, Git, and a bunch of GNU tools. It might work with the BSD counterparts also; I’ve no idea.</p><p>Here’s a Git command you can run in your favorite project’s root directory:</p><div><pre><code data-lang="bash">git log --pretty<span>=</span><span>&#39;&#39;</span> --date<span>=</span>short --numstat
</code></pre></div><p>It will output the churn for every file of the project, from the first commit till the last. The first number displayed is the count of lines added, the second one represents the lines deleted. The data is raw and not really useful, but it’s a good start.</p><p>We can also add to this command two options: <code>--before</code>, <code>--after</code>, or both. It allows to only display the churn in a specific period of time. For example, to display the code churn from 2015 to 2020 included:</p><div><pre><code data-lang="bash">git log --after<span>=</span>2015-01-01 --before<span>=</span>2021-01-01 --pretty<span>=</span><span>&#39;&#39;</span> --date<span>=</span>short --numstat
</code></pre></div><p>Adding <code>awk</code> and <code>sort</code> to the mix, we can go crazy by displaying the count of added and deleted lines per file:</p><pre><code>git log --pretty=&#39;&#39; --date=short --numstat \
| awk &#39;{print $3,$1,$2}&#39; \
| awk &#39;BEGIN{print &#34;added&#34;,&#34;file&#34;,&#34;deleted&#34;} {added[$1] += $2} {deleted[$1] += $3} END{for (i in added) print added[i], i, deleted[i]}&#39; \
| sort -rn
</code></pre><p>The resulting output is already more useful: it will show you what are the files changing the most in your codebase. But it’s also likely that you’ll get many files you don’t really care about, like config files, or minified CSS files for example. After all, the files changing the most in your codebase are the ones which might grow in complexity, but only if there is any sort of complexity there at the first place.</p><p>Now, it could be interesting to filter the files modified in a specific period of time <em>and</em> using some static analysis to get the most complex ones. It could reveal the real complexity spots we’re after.</p><h2 id="combining-changes-and-complexity-metrics">Combining Changes and Complexity Metrics</h2><p>We saw in the <a href="https://thevaluable.dev/complexity-metrics-software/">previous article</a> that no metric can show us the complexity of our code with great reliability. That said, some of them can still give us more information to take a more informed decision. As a result, to have an idea of the complexity of the code in each file, we can simply use the easiest complexity metric we can find, the venerable line of codes (LOC).</p><p>It’s far from being the perfect complexity metric, no doubt about that. Don’t expect 100% accuracy of finding complexity spots for every possible piece of code by only measuring LOC. But, according to studies, <a href="https://thevaluable.dev/complexity-metrics-software/" target="_blank" rel="noopener">no static complexity metric do better</a>, and it still gives us a useful <a href="https://en.wikipedia.org/wiki/Heuristic" target="_blank" rel="noopener">heuristic</a> to find this damn complexity. That’s what we should aim for: usefulness, instead of the impossible perfection.</p><p>So, how to sort the files of our codebase by complexity (using LOC) and by number of changes? Again, we can use Git and some common CLIs to do so. First, we can look at how many times the files in your favorite project were modified, and save the output to a file called “churn”:</p><div><pre><code data-lang="bash">git log --pretty<span>=</span><span>&#39;&#39;</span> --date<span>=</span>short --numstat <span>|</span> awk <span>&#39;{print $3}&#39;</span> <span>|</span> sort <span>|</span> uniq -c <span>|</span> sort -rn <span>|</span> awk <span>&#39;{ print &#34;./&#34; $2,$1 }&#39;</span> &gt; churn
</code></pre></div><p>We don’t look at added and deleted lines this time, the concept of “modified file” is enough for now.</p><p>Next, we can count the lines of code per file and save the result into another file “comp”. We use the CLI <a href="https://github.com/AlDanial/cloc" target="_blank" rel="noopener">cloc</a> to do so:</p><div><pre><code data-lang="bash">cloc ./ --by-file --quiet --csv <span>|</span> awk -F <span>&#39;,&#39;</span> <span>&#39;NR &gt; 2 { print $2,$5 }&#39;</span> <span>|</span> head -n -1 &gt; comp
</code></pre></div><p>Finally, we can merge both results and output a CSV using <a href="https://themouseless.dev/posts/awk-guide-examples-mouseless/" target="_blank" rel="noopener">awk</a> and <a href="https://themouseless.dev/posts/sed-guide-example-mouseless/" target="_blank" rel="noopener">sed</a>:</p><div><pre><code data-lang="bash">awk <span>&#39;{files[$1]=(files[$1]?files[$1]FS$2:$2)} END { for (i in files) print files[i],i }&#39;</span> churn comp <span>\
</span><span></span><span>|</span> tr <span>&#39; &#39;</span> <span>&#39;,&#39;</span> <span>\
</span><span></span><span>|</span> sed <span>&#39;/^.*,.*,.*$/!d&#39;</span> <span>\
</span><span></span><span>|</span> sort -rg
</code></pre></div><p>Running the above command, we get some columns representing, in order:</p><ol><li>How many times each file was modified.</li><li>The lines of code.</li><li>The filename.</li></ol><p>If you’d like a ridiculous one liner combining the three commands above, here you go:</p><div><pre><code data-lang="bash">awk <span>&#39;{files[$1]=(files[$1]?files[$1]FS$2:$2)} END { for (i in files) print files[i],i }&#39;</span> <span>\
</span><span></span><span>=(</span>git log --pretty<span>=</span><span>&#39;&#39;</span> --date<span>=</span>short --numstat <span>|</span> awk <span>&#39;{print $3}&#39;</span> <span>|</span> sort <span>|</span> uniq -c <span>|</span> sort -rn <span>|</span> awk <span>&#39;{ print &#34;./&#34; $2,$1 }&#39;</span><span>)</span> <span>\
</span><span></span><span>=(</span>cloc ./ --by-file --quiet --csv <span>|</span> awk -F <span>&#39;,&#39;</span> <span>&#39;NR &gt; 2 { print $2,$5 }&#39;</span> <span>|</span> head -n -1<span>)</span> <span>\
</span><span></span><span>|</span> tr <span>&#39; &#39;</span> <span>&#39;,&#39;</span> <span>\
</span><span></span><span>|</span> sed <span>&#39;/^.*,.*,.*$/!d&#39;</span> <span>\
</span><span></span><span>|</span> sort -rg
</code></pre></div><p>The result is sorted by number of changes here, but you can save the output in a CSV and open it in an Excel-like application, to sort the result according to your needs.</p><p>You might have noticed that we didn’t choose a time period using the options <code>--before</code> and <code>--after</code> in our Git commands. I would encourage you to do so, to see clearly the complexity trend of specific periods of time. How to choose this time period? As a rule of thumb, we could try to look around important events; for example around releases, or the implementation of a complex functionality.</p><p>This information can be useful if you have a complex functionality to develop, or if you want to know what you should refactor to improve the stability of your application. In general, you need to give extra care to the files with high modification rate and high complexity. Make sure they’re properly tested, and refactor them if needed.</p><p>You can even plot different time periods to see if the complexity of some part of your codebase increase or decrease overtime. You don’t have to do that manually: many programs can do that for you. I like to use <a href="https://github.com/adamtornhill/code-maat" target="_blank" rel="noopener">code-maat</a>, a tool developed by Adam Tornhill (author of <a href="https://pragprog.com/titles/atevol/software-design-x-rays/" target="_blank" rel="noopener">great books</a> about measuring complexity I definitely recommend). If you want to have nice graphs and other visualizations instead of raw data in your shell, I sometimes use <a href="https://github.com/smontanari/code-forensics" target="_blank" rel="noopener">code forensics</a>, a wrapper around code-maat.</p><p>As always, use your knowledge (both technical and domain related) to assess if the results you get are indeed complexity spots. False positives are common, whatever technique you’re using.</p><h2 id="aging-code">Aging Code</h2><picture>
<source srcset="https://thevaluable.dev/images/2021/measuring_complexity_env/old_useless_wise.webp" type="image/webp"/><img width="780" height="520" src="https://thevaluable.dev/images/2021/measuring_complexity_env/old_useless_wise.jpg" alt="Older code is still there because it&#39;s so good... or nobody wants to touch it"/></picture><p>Speaking about period of time, what about code which is too old? How time can influence our codebase?</p><p>According to some studies like <a href="https://www.researchgate.net/publication/3188092_Predicting_Fault_Incidence_Using_Software_Change_History" target="_blank" rel="noopener">this one</a>, code seems to decay overtime. It shows that the older the code, the more likely bugs will appear.</p><p>Taking the bright side of ancient code, it can also mean that the code is good! It’s possible that nobody felt the need to refactor it because of its quality. Again, you’ll need to use your expertise to judge if the aging parts of your codebase are wonderful antique treasures or old rotten cucumbers.</p><p>There is another potential problem with aging code, without considering if it’s “good” or “bad”. The older it gets, the less chances the authors will still be around. That’s a shame, especially if you need some explanations how it works, or why it was designed that way.</p><p>As humans, we have tendency to forget easily. That’s why it’s important to capture the present context when functionalities are being <em>codified</em> in some codebase, by using good naming, writing good requirements or high level documentation. The future developers who need to maintain (or, even worst, change) your code will thank you. Logging all your decisions in a journal can also be a good tool serving generations to come.</p><h2 id="more-developers-more-bugs">More Developers, More Bugs?</h2><p>I thought for a long time that the more developers there are on a project, the more complex the codebase gets. After all, we need to understand each other (a daily challenge by itself), and we need to coordinate. It can quickly create misunderstanding or misconceptions, the potential source of bugs and wrong implementations.</p><p>But if we look at the studies on the subject, nobody found empirical evidences that more developers working on the code correlates to more defects.</p><p>It can still be useful to know who are the developers who wrote most of the code you’re working on. A simple <code>git blame</code> can go a long way if you need some explanations about a piece of code, or about the context at the time it was written.</p><p>As an aside, I prefer thinking of the Git “blame” command as the “praise” command. Going into endless debates about the <em>subjective</em> question of “code quality” or “code cleanness” is often not the best idea. Instead, it’s better to focus on improving the code than blaming its authors.</p><h2 id="cognitive-complexity-comprehension-and-readability">Cognitive Complexity, Comprehension, and Readability</h2><p>It’s now time to reveal the root of all our problems, as developers. It’s no surprise: our brains didn’t evolve to reason accurately about stacks of abstractions with hundreds of states. That’s why we try to automate things as much as possible (not to think about it), and hide the complexity as much as we can, praying that it won’t jump back to our neurons.</p><p>It’s nice to study complexity in our codebase, but it’s even more useful to understand the complexity we can handle in our poor brain. As I stated at the beginning of this article, humans write code for other humans, not only for a dummy compiler.</p><p>To understand what part of our codebase ask for more brain time (or more mental energy) would certainly be beneficial, to go around our limitations. But, as always, studying our own way of thinking is difficult, and far from being a solved problem. If it was, we would have drunk robots all over the place.</p><p>Let’s take understandability: according to <a href="https://arxiv.org/pdf/2007.12520.pdf" target="_blank" rel="noopener">this study</a>, we spend 50% of our time trying to understand the code we’re reading or modifying. It wouldn’t be too bad to reduce this number. So let’s ask: how to find the parts of our codebase which are the most difficult to understand?</p><p>If you let your intuition going wild to answer this question, you’ll come up with a <em>personal opinion</em> on the subject. This opinion won’t necessarily be shared by your colleagues. This can create new endless debates full of subjectivity. Even if everybody agrees, how can you know that you’re indeed right <em>in general</em>?</p><p>Is there any way to measure more objectively this cognitive complexity?</p><p>A couple of years ago, Sonarsource (the company developing <a href="https://www.sonarqube.org/" target="_blank" rel="noopener">Sonarqube</a>, one of the <a href="https://www.researchgate.net/publication/344151709_An_Overview_and_Comparison_of_Technical_Debt_Measurement_Tools" target="_blank" rel="noopener">most used application</a> to measure complexity) came up with the “cognitive complexity” metric, based on <a href="https://github.com/Phantas0s/alexandria-library/blob/master/computing/analysis/_PAPERS/cognition/2018_cognitive_complexity_overview.pdf" target="_blank" rel="noopener">this study</a>. It’s essentially an updated version of the <a href="https://thevaluable.dev/complexity-metrics-software/">cyclomatic complexity</a>, adding more complexity for nested constructs.</p><p>This metric was empirically studied on 22 projects, and the developers of 17 of them found that the metrics were accurate. But the author points out that the sample size was too small for any statistical significance. Despite this, many applications measuring complexity propose this metric.</p><p>From there, other papers tried to validate the metric, like <a href="https://arxiv.org/pdf/2007.12520.pdf" target="_blank" rel="noopener">this one</a>. It looked at many papers about the relation between code and understandability, and tried to compare the measure they made with the cognitive complexity metric.</p><p>At the end, it seems that this metric is correlated with the time the developers spend on the code to understand it. It also seems to match what part of the codebase the developers find complex.</p><p>That said, if you ask some questions to the same developers to see if they really understood the code, the result won’t be correlated with the metric itself. In short, we sometimes think that the code is complex even if we understand it, and other times we think the code is not complex even if we don’t really understanding it. Damn it, brain!</p><p>All in all, if you come across the “cognitive complexity” in one of your tool, it seems to be slightly better than the usual cyclomatic complexity but, again, I would rely more on code churn or aging code than this metric.</p><p>The question of understandability is even more problematic when we ask this question: is it even possible to come up with a general understandability metric, accurate for everybody?</p><p>Another <a href="https://www.researchgate.net/publication/4248300_Cognitive_Complexity_of_Software_and_its_Measurement" target="_blank" rel="noopener">interesting research</a>, using mathematical models this time (instead of asking developers if they feel that the code is complex) class some constructs way more complex to understand than others. The worst of all being recursion, parallel processing, and interrupt processing (a second process interrupting a first one, doing some computations till it ends, and let the first process to continue). I would agree with the statement; but again, it’s only my flawed brain speaking here.</p><p>Readability can also be a source of complexity for our biological neural network. According to this <a href="http://veneraarnaoudova.ca/wp-content/uploads/2018/03/2018-ICPC-Effect-lexicon-cognitive-load.pdf" target="_blank" rel="noopener">other study</a>, lexical inconsistencies have more impact than hundreds of other metrics coming from your favorite static analytic tools.</p><p>Here are some examples of lexical inconsistencies:</p><ul><li>Getters doing other operations before returning the value you want (side effects).</li><li>Setters returning something.</li><li>Misleading comments.</li></ul><p>In short, it’s about being misled by missing or wrong semantics. Keeping a tight consistency between the naming and the behavior of our code seems more important than anything else. As a result, it’s worse spending some time on the documentation (in a very large sense, including comments and naming) and refactoring it when necessary.</p><h2 id="emotional-awareness-via-git-commit-messages">Emotional Awareness via Git Commit Messages</h2><picture>
<source srcset="https://thevaluable.dev/images/2021/measuring_complexity_env/hate_love.webp" type="image/webp"/><img width="780" height="520" src="https://thevaluable.dev/images/2021/measuring_complexity_env/hate_love.jpg" alt="It&#39;s important to understand the feelings of developers on a project"/></picture><p>Our brain is not only a cold machine trying to process logically the outside world. Big news: we have emotions, too.</p><p>Even if we prefer to think of ourselves as logical beasts, our emotions are a big part of our conscious life, want it or not. According to Daniel Goleman in his famous book <a href="https://www.goodreads.com/book/show/26329.Emotional_Intelligence" target="_blank" rel="noopener">Emotional Intelligence</a>, emotions are often the first thing influencing our decisions, before the analytical part of our brain has time to react.</p><p>It was quite useful in the past for survival and it’s still useful today, but not necessarily when we design some beloved applications.</p><p>How to be aware of the negative emotions of our colleagues, to make sure they won’t be the indirect cause of some complexity in the codebase? More and more studies look at this question, like <a href="https://www.researchgate.net/publication/266657943_Sentiment_analysis_of_commit_comments_in_GitHub_An_empirical_study" target="_blank" rel="noopener">this one</a>, which tried to analyze 60425 different Git commits using sentiment analysis algorithms.</p><p>This study (and others before) show that strong emotions affect the quality of the codebase and, potentially, its complexity. It’s not very clear yet in what ways, but it’s still useful to keep that in mind.</p><p>More importantly, this study answers one of the oldest question bothering humanity from the beginning of times: yes, the Git comments are more negative on Monday!</p><p>Weird jokes aside, this study also show that when different nationalities work on the same project, the Github comments are more positive. A good argument to support diversity in our teams.</p><p>You don’t need to use machine learning models to mine the Git comments of your own project. You can simply create cloud words from them. It can give you an idea of the mindset of a developer team.</p><p>To give you a quick idea, you can split your commit messages into words, count how many times each of them appear, and output the first 100:</p><pre><code>git log --pretty=format:&#39;%s&#39; | tr &#39; &#39; &#39;\n&#39; | sed &#39;s/.*/\L&amp;/&#39; | sort | uniq -c | sort -rg | head -n 100
</code></pre><p>Again, if you want to look at Git comments in specific periods of time (during a death march for example, or when the team seemed to enjoy coding some functionality), you can do so with the usual options <code>--before</code> and <code>--after</code>.</p><p>Then, we can quickly clean the data to get rid of the words we don’t care (like articles or prepositions), and plug the result into an application creating word clouds (like <a href="https://worditout.com/word-cloud/create" target="_blank" rel="noopener">this one</a>).</p><p>What do you want to see? Words related to the domain of the codebase. If you see a lot of “fix”, you can ask your colleagues to squash their commits when they fix the most recent changes. It might also mean that developers spend a lot of time maintaining the codebase (which is very often the case).</p><h2 id="the-mysterious-environment">The Mysterious Environment</h2><p>Trying to assess the overall context around the development of an application, and how much it affects the codebase itself, can seem like a daunting task. But, as we saw, there are interesting techniques and metrics out there which can shed a new light on our projects.</p><p>What did we see in this article?</p><ul><li>According to studies, measuring churn is the best way to find complexity spots, and to forecast future defects.</li><li>Combining churn and some complexity metrics (like the count of line of code, or LOC) can help us filter out the interesting complexity spots.</li><li>Code decay needs to be taken into consideration. Old code didn’t change either because developers don’t want to change them (due to their complexity), or because the quality is good enough.</li><li>Even if it seems useful, be careful with the “cognitive complexity” metrics implemented in many applications. It’s not even sure if a cognitive complexity can be applied universally to all developers.</li><li>Aiming for consistency when writing your code seems to be one of the most important factor of a good codebase. Well-chosen name reflecting correctly the behavior they abstract is key, without omitting the <em>important</em> details.</li></ul><p>In general, we shouldn’t forget that rate of changes, the business context, the company culture we work for, and the developers themselves have a great influence on the codebase. Even if this influence can be challenging to measure, it shouldn’t be discarded.</p><div><p>Related Sources</p><ul><li><a href="https://www.goodreads.com/en/book/show/35747076-accelerate" target="_blank" rel="noopener">Accelerate</a> - Nicole Forsgren, Jez Humble, Gene Kim</li><li><a href="https://pragprog.com/titles/atevol/software-design-x-rays/" target="_blank" rel="noopener">Software Design X-Rays</a> - Adam Tornhill</li><li><a href="https://www.researchgate.net/publication/220851904_Does_measuring_code_change_improve_fault_prediction" target="_blank" rel="noopener">Does measuring code change improve fault prediction?</a> - Elaine J. Weyuker</li><li><a href="https://www.researchgate.net/publication/3188092_Predicting_Fault_Incidence_Using_Software_Change_History" target="_blank" rel="noopener">Predicting Fault Incidence Using Software Change History</a> - Todd L. Graves, Alan F. Karr, J. S. Marron, Harvey Siy</li><li><a href="https://www.researchgate.net/publication/344151709_An_Overview_and_Comparison_of_Technical_Debt_Measurement_Tools" target="_blank" rel="noopener">An Overview and Comparison of Technical Debt Measurement Tools</a> - Paris Avgeriou, Davide Taibi, Apostolos Ampatzoglou, Francesca Arcelli Fontana</li><li><a href="https://github.com/Phantas0s/alexandria-library/blob/master/computing/analysis/_PAPERS/cognition/2018_cognitive_complexity_overview.pdf" target="_blank" rel="noopener">Cognitive Complexity - An Overview and Evaluation</a> - G. Ann Campbell</li><li><a href="https://arxiv.org/pdf/2007.12520.pdf" target="_blank" rel="noopener">An Empirical Validation of Cognitive Complexity as a Measure of Source Code Understandability</a> - Marvin Muñoz Barón, Marvin Wyrich, Stefan Wagner</li></ul></div></section></article></div></div>
  </body>
</html>
