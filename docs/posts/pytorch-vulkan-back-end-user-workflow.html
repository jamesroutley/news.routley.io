<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pytorch.org/tutorials/prototype/vulkan_workflow.html">Original</a>
    <h1>PyTorch Vulkan Back End User Workflow</h1>
    
    <div id="readability-page-1" class="page"><section data-toggle="wy-nav-shift" id="pytorch-content-wrap">
        <div>

        

          <div>
            <p>prototype/vulkan_workflow</p>

            <div id="google-colab-link">
              <p><img src="https://pytorch.org/tutorials/_static/images/pytorch-colab.svg"/></p><p>Run in Google Colab</p>
              <p>Colab</p>
            </div>
            <div id="download-notebook-link">
              <p><img src="https://pytorch.org/tutorials/_static/images/pytorch-download.svg"/></p><p>Download Notebook</p>
              <p>Notebook</p>
            </div>
            <div id="github-view-link">
              <p><img src="https://pytorch.org/tutorials/_static/images/pytorch-github.svg"/></p><p>View on GitHub</p>
              <p>GitHub</p>
            </div>
          </div>

        

          
          <div>
          
            <div role="main" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article">
              
  <div id="pytorch-vulkan-backend-user-workflow">

<p><strong>Author</strong>: <a href="https://github.com/IvanKobzarev">Ivan Kobzarev</a></p>
<div id="introduction">
<h2>Introduction<a href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>PyTorch 1.7 supports the ability to run model inference on GPUs that support the Vulkan graphics and compute API. The primary target devices are mobile GPUs on Android devices. The Vulkan backend can also be used on Linux, Mac, and Windows desktop builds to use Vulkan devices like Intel integrated GPUs. This feature is in the prototype stage and is subject to change.</p>
<div id="building-pytorch-with-vulkan-backend">
<h3>Building PyTorch with Vulkan backend<a href="#building-pytorch-with-vulkan-backend" title="Permalink to this heading">¶</a></h3>
<p>Vulkan backend is not included by default. The main switch to include Vulkan backend is cmake option <code><span>USE_VULKAN</span></code>, that can be set by environment variable <code><span>USE_VULKAN</span></code>.</p>
<p>To use PyTorch with Vulkan backend, we need to build it from source with additional settings. Checkout the PyTorch source code from GitHub master branch.</p>
<div id="optional-usage-of-vulkan-wrapper">
<h4>Optional usage of vulkan wrapper<a href="#optional-usage-of-vulkan-wrapper" title="Permalink to this heading">¶</a></h4>
<p>By default, Vulkan library will be loaded at runtime using the vulkan_wrapper library. If you specify the environment variable <code><span>USE_VULKAN_WRAPPER=0</span></code> libvulkan will be linked directly.</p>
</div>

<div id="vulkan-sdk">
<h4>Vulkan SDK<a href="#vulkan-sdk" title="Permalink to this heading">¶</a></h4>
<p>Download VulkanSDK from <a href="https://vulkan.lunarg.com/sdk/home">https://vulkan.lunarg.com/sdk/home</a> and set environment variable <code><span>VULKAN_SDK</span></code></p>
<p>Unpack VulkanSDK to <code><span>VULKAN_SDK_ROOT</span></code> folder, install VulkanSDK following VulkanSDK instructions for your system.</p>
<p>For Mac:</p>
<div><div><pre><span></span>cd $VULKAN_SDK_ROOT
source setup-env.sh
sudo python install_vulkan.py
</pre></div>
</div>
<p>Building PyTorch:</p>
<p>For Linux:</p>
<div><div><pre><span></span><span>cd</span> <span>PYTORCH_ROOT</span>
<span>USE_VULKAN</span><span>=</span><span>1</span> <span>USE_VULKAN_SHADERC_RUNTIME</span><span>=</span><span>1</span> <span>USE_VULKAN_WRAPPER</span><span>=</span><span>0</span> <span>python</span> <span>setup</span><span>.</span><span>py</span> <span>install</span>
</pre></div>
</div>
<p>For Mac:</p>
<div><div><pre><span></span><span>cd</span> <span>PYTORCH_ROOT</span>
<span>USE_VULKAN</span><span>=</span><span>1</span> <span>USE_VULKAN_SHADERC_RUNTIME</span><span>=</span><span>1</span> <span>USE_VULKAN_WRAPPER</span><span>=</span><span>0</span> <span>MACOSX_DEPLOYMENT_TARGET</span><span>=</span><span>10.9</span> <span>CC</span><span>=</span><span>clang</span> <span>CXX</span><span>=</span><span>clang</span><span>++</span> <span>python</span> <span>setup</span><span>.</span><span>py</span> <span>install</span>
</pre></div>
</div>
<p>After successful build, open another terminal and verify the version of installed PyTorch.</p>
<div><div><pre><span></span><span>import</span> <span>torch</span>
<span>print</span><span>(</span><span>torch</span><span>.</span><span>__version__</span><span>)</span>
</pre></div>
</div>
<p>At the time of writing of this recipe, the version is 1.8.0a0+41237a4. You might be seeing different numbers depending on when you check out the code from master, but it should be greater than 1.7.0.</p>
</div>
<div id="android-build">
<h4>Android build<a href="#android-build" title="Permalink to this heading">¶</a></h4>
<p>To build LibTorch for android with Vulkan backend for specified <code><span>ANDROID_ABI</span></code>.</p>
<div><div><pre><span></span><span>cd</span> <span>PYTORCH_ROOT</span>
<span>ANDROID_ABI</span><span>=</span><span>arm64</span><span>-</span><span>v8a</span> <span>USE_VULKAN</span><span>=</span><span>1</span> <span>sh</span> <span>./</span><span>scripts</span><span>/</span><span>build_android</span><span>.</span><span>sh</span>
</pre></div>
</div>
<p>To prepare pytorch_android aars that you can use directly in your app:</p>
<div><div><pre><span></span>cd $PYTORCH_ROOT
USE_VULKAN=1 sh ./scripts/build_pytorch_android.sh
</pre></div>
</div>
</div>
</div>
</div>
<div id="model-preparation">
<h2>Model preparation<a href="#model-preparation" title="Permalink to this heading">¶</a></h2>
<p>Install torchvision, get the default pretrained float model.</p>

<p>Python script to save pretrained mobilenet_v2 to a file:</p>
<div><div><pre><span></span><span>import</span> <span>torch</span>
<span>import</span> <span>torchvision</span>

<span>model</span> <span>=</span> <span>torchvision</span><span>.</span><span>models</span><span>.</span><span>mobilenet_v2</span><span>(</span><span>pretrained</span><span>=</span><span>True</span><span>)</span>
<span>model</span><span>.</span><span>eval</span><span>()</span>
<span>script_model</span> <span>=</span> <span>torch</span><span>.</span><span>jit</span><span>.</span><span>script</span><span>(</span><span>model</span><span>)</span>
<span>torch</span><span>.</span><span>jit</span><span>.</span><span>save</span><span>(</span><span>script_model</span><span>,</span> <span>&#34;mobilenet2.pt&#34;</span><span>)</span>
</pre></div>
</div>
<p>PyTorch 1.7 Vulkan backend supports only float 32bit operators. The default model needs additional step that will optimize operators fusing</p>
<div><div><pre><span></span><span>from</span> <span>torch.utils.mobile_optimizer</span> <span>import</span> <span>optimize_for_mobile</span>
<span>script_model_vulkan</span> <span>=</span> <span>optimize_for_mobile</span><span>(</span><span>script_model</span><span>,</span> <span>backend</span><span>=</span><span>&#39;vulkan&#39;</span><span>)</span>
<span>torch</span><span>.</span><span>jit</span><span>.</span><span>save</span><span>(</span><span>script_model_vulkan</span><span>,</span> <span>&#34;mobilenet2-vulkan.pt&#34;</span><span>)</span>
</pre></div>
</div>
<p>The result model can be used only on Vulkan backend as it contains specific to the Vulkan backend operators.</p>
<p>By default, <code><span>optimize_for_mobile</span></code> with <code><span>backend=&#39;vulkan&#39;</span></code> rewrites the graph so  that inputs are transferred to the Vulkan backend, and outputs are transferred to the CPU backend, therefore, the model can be run on CPU inputs and produce CPU outputs. To disable this, add the argument <code><span>optimization_blocklist={MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER}</span></code> to <code><span>optimize_for_mobile</span></code>. (<code><span>MobileOptimizerType</span></code> can be imported from <code><span>torch.utils.mobile_optimizer</span></code>)</p>
<p>For more information, see the <cite>torch.utils.mobile_optimizer</cite> <a href="https://pytorch.org/docs/stable/mobile_optimizer.html">API documentation</a>.</p>
</div>
<div id="using-vulkan-backend-in-code">
<h2>Using Vulkan backend in code<a href="#using-vulkan-backend-in-code" title="Permalink to this heading">¶</a></h2>
</div>
<div id="c-api">
<h2>C++ API<a href="#c-api" title="Permalink to this heading">¶</a></h2>
<div><div><pre><span></span><span>at</span><span>::</span><span>is_vulkan_available</span><span>()</span>
<span>auto</span> <span>tensor</span> <span>=</span> <span>at</span><span>::</span><span>rand</span><span>({</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>},</span> <span>at</span><span>::</span><span>device</span><span>(</span><span>at</span><span>::</span><span>kCPU</span><span>)</span><span>.</span><span>dtype</span><span>(</span><span>at</span><span>::</span><span>kFloat</span><span>));</span>
<span>auto</span> <span>tensor_vulkan</span> <span>=</span> <span>t</span><span>.</span><span>vulkan</span><span>();</span>
<span>auto</span> <span>module</span> <span>=</span> <span>torch</span><span>::</span><span>jit</span><span>::</span><span>load</span><span>(</span><span>&#34;$PATH&#34;</span><span>);</span>
<span>auto</span> <span>tensor_output_vulkan</span> <span>=</span> <span>module</span><span>.</span><span>forward</span><span>(</span><span>inputs</span><span>)</span><span>.</span><span>toTensor</span><span>();</span>
<span>auto</span> <span>tensor_output</span> <span>=</span> <span>tensor_output</span><span>.</span><span>cpu</span><span>();</span>
</pre></div>
</div>
<p><code><span>at::is_vulkan_available()</span></code> function tries to initialize Vulkan backend and if Vulkan device is successfully found and context is created - it will return true, false otherwise.</p>
<p><code><span>.vulkan()</span></code> function called on Tensor will copy tensor to Vulkan device, and for operators called with this tensor as input - the operator will run on Vulkan device, and its output will be on the Vulkan device.</p>
<p><code><span>.cpu()</span></code> function called on Vulkan tensor will copy its data to CPU tensor (default)</p>
<p>Operators called with a tensor on a Vulkan device as an input will be executed on a Vulkan device. If an operator is not supported for the Vulkan backend the exception will be thrown.</p>
<p>List of supported operators:</p>
<div><div><pre><span></span><span>_adaptive_avg_pool2d</span>
<span>_cat</span>
<span>add</span><span>.</span><span>Scalar</span>
<span>add</span><span>.</span><span>Tensor</span>
<span>add_</span><span>.</span><span>Tensor</span>
<span>addmm</span>
<span>avg_pool2d</span>
<span>clamp</span>
<span>convolution</span>
<span>empty</span><span>.</span><span>memory_format</span>
<span>empty_strided</span>
<span>hardtanh_</span>
<span>max_pool2d</span>
<span>mean</span><span>.</span><span>dim</span>
<span>mm</span>
<span>mul</span><span>.</span><span>Scalar</span>
<span>relu_</span>
<span>reshape</span>
<span>select</span><span>.</span><span>int</span>
<span>slice</span><span>.</span><span>Tensor</span>
<span>transpose</span><span>.</span><span>int</span>
<span>transpose_</span>
<span>unsqueeze</span>
<span>upsample_nearest2d</span>
<span>view</span>
</pre></div>
</div>
<p>Those operators allow to use torchvision models for image classification on Vulkan backend.</p>
</div>
<div id="python-api">
<h2>Python API<a href="#python-api" title="Permalink to this heading">¶</a></h2>
<p><code><span>torch.is_vulkan_available()</span></code> is exposed to Python API.</p>
<p><code><span>tensor.to(device=&#39;vulkan&#39;)</span></code> works as <code><span>.vulkan()</span></code> moving tensor to the Vulkan device.</p>
<p><code><span>.vulkan()</span></code> at the moment of writing of this tutorial is not exposed to Python API, but it is planned to be there.</p>
</div>
<div id="android-java-api">
<h2>Android Java API<a href="#android-java-api" title="Permalink to this heading">¶</a></h2>
<p>For Android API to run model on Vulkan backend we have to specify this during model loading:</p>
<div><div><pre><span></span><span>import</span> <span>org.pytorch.Device</span><span>;</span>
<span>Module</span> <span>module</span> <span>=</span> <span>Module</span><span>.</span><span>load</span><span>(</span><span>&#34;$PATH&#34;</span><span>,</span> <span>Device</span><span>.</span><span>VULKAN</span><span>)</span>
<span>FloatBuffer</span> <span>buffer</span> <span>=</span> <span>Tensor</span><span>.</span><span>allocateFloatBuffer</span><span>(</span><span>1</span> <span>*</span> <span>3</span> <span>*</span> <span>224</span> <span>*</span> <span>224</span><span>);</span>
<span>Tensor</span> <span>inputTensor</span> <span>=</span> <span>Tensor</span><span>.</span><span>fromBlob</span><span>(</span><span>buffer</span><span>,</span> <span>new</span> <span>int</span><span>[]{</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>224</span><span>,</span> <span>224</span><span>});</span>
<span>Tensor</span> <span>outputTensor</span> <span>=</span> <span>mModule</span><span>.</span><span>forward</span><span>(</span><span>IValue</span><span>.</span><span>from</span><span>(</span><span>inputTensor</span><span>))</span><span>.</span><span>toTensor</span><span>();</span>
</pre></div>
</div>
<p>In this case, all inputs will be transparently copied from CPU to the Vulkan device, and model will be run on Vulkan device, the output will be copied transparently to CPU.</p>
<p>The example of using Vulkan backend can be found in test application within the PyTorch repository:
<a href="https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L133">https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L133</a></p>
</div>
<div id="building-android-test-app-with-vulkan">
<h2>Building android test app with Vulkan<a href="#building-android-test-app-with-vulkan" title="Permalink to this heading">¶</a></h2>
<div><div><pre><span></span>cd $PYTORCH_ROOT
USE_VULKAN=1 sh ./scripts/build_pytorch_android.sh
</pre></div>
</div>
<p>Or if you need only specific abi you can set it as an argument:</p>
<div><div><pre><span></span>cd $PYTORCH_ROOT
USE_VULKAN=1 sh ./scripts/build_pytorch_android.sh $ANDROID_ABI
</pre></div>
</div>
<p>Add prepared model <code><span>mobilenet2-vulkan.pt</span></code> to test applocation assets:</p>
<div><div><pre><span></span>cp mobilenet2-vulkan.pt $PYTORCH_ROOT/android/test_app/app/src/main/assets/
</pre></div>
</div>
<div><div><pre><span></span>cd $PYTORCH_ROOT
gradle -p android test_app:installMbvulkanLocalBaseDebug
</pre></div>
</div>
<p>After successful installation, the application with the name ‘MBQ’ can be launched on the device.</p>
</div>
<div id="testing-models-without-uploading-to-android-device">
<h2>Testing models without uploading to android device<a href="#testing-models-without-uploading-to-android-device" title="Permalink to this heading">¶</a></h2>
<p>Software implementations of Vulkan (e.g. <a href="https://swiftshader.googlesource.com/SwiftShader">https://swiftshader.googlesource.com/SwiftShader</a> ) can be used to test if a model can be run using PyTorch Vulkan Backend (e.g. check if all model operators are supported).</p>
</div>
</div>


             </article>
             
            </div>
            

          </div>

        </div>

        
      </section></div>
  </body>
</html>
