<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.bigmessowires.com/2016/05/06/fc8-faster-68k-decompression/">Original</a>
    <h1>FC8 â€“ Faster 68K Decompression (2016)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p><img src="https://www.bigmessowires.com/wp-content/uploads/2016/05/compress-data.jpg" alt="compress-data" width="528" height="249" srcset="https://www.bigmessowires.com/wp-content/uploads/2016/05/compress-data.jpg 528w, https://www.bigmessowires.com/wp-content/uploads/2016/05/compress-data-300x141.jpg 300w" sizes="(max-width: 528px) 100vw, 528px"/></p>
<p>Data compression is fun! Iâ€™ve written a new compression scheme thatâ€™s designed to be as fast as possible to decompress on a 68K CPU, while still maintaining a decent compression density. Iâ€™m calling it FC8, and you can get the generic C implementation and optimized 68K decompressor from <a href="https://github.com/steve-chamberlin/fc8-compression">the projectâ€™s Github repository</a>. Iâ€™ve probably reinvented the wheel with this, and in a non-optimal way too, but once I started I found that I couldnâ€™t stop. My intended platform for FC8 is 68020 and 68030-based vintage Macintosh computers, but it should be easily portable to other classic computers, microcontrollers, and similar minimal systems.</p>
<p>The main loop of the 68K decompressor is exactly 256 bytes, so it fits entirely within the instruction cache of the 68020/030. Decompression speed on a 68030 is about 25% as fast as an optimized memcpy of uncompressed data, which is essentially an unrolled loop of 4-byte move.l instructions with no computation involved. Compared to that, I think 25% is pretty good, but I can always hope for more. ðŸ™‚</p>
<p>In the <a href="https://www.bigmessowires.com/2016/04/28/optimizing-assembly-fast-68k-decompression/">previous post</a>, I described how I was using compression to squeeze a larger rom-disk image into a custom replacement Macintosh ROM that Iâ€™m designing. I began with a compression algorithm called <a href="http://liblzg.bitsnbites.eu/">LZG</a>, written by Marcus Geelnard. It worked well, but the 68K decompression seemed disappointingly slow. I tried to contact the author to discuss it, but couldnâ€™t find any email address or other contact info, so I eventually drifted towards creating my own compression method loosely based on LZG. This became FC8. On a 68030 CPU, FC8 compresses data equally as tightly as LZG and decompresses 1.5x to 2x faster. FC8 retains much of the compression acceleration code from LZG, as well as the idea of quantizing lengths, but the encoding and decompressor are new. </p>
<p>The algorithm is based on the classic <a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78#LZ77">LZ77</a> compression scheme, with a 128K sliding history window and with duplicated data replaced by (distance,length) backref markers pointing to previous instances of the same data. No extra RAM is required during decompression, aside from the input and output buffers. The compressed data is a series of tokens in this format:</p>
<ul>
<li>LIT = 00aaaaaa = next aaaaaa+1 bytes are literals</li>
<li>BR0 = 01baaaaa = backref to offset aaaaa, length b+3</li>
<li>EOF = 01Ã—00000 = end of file</li>
<li>BR1 = 10bbbaaaâ€™aaaaaaaa = backref to offset aaaâ€™aaaaaaaa, length bbb+3</li>
<li>BR2 = 11bbbbbaâ€™aaaaaaaaâ€™aaaaaaaa = backref to offset aâ€™aaaaaaaaâ€™aaaaaaaa, length lookup_table[bbbbb]</li>
</ul>
<p>The encoding may look slightly strange, such as only a single bit for the backref length in BR0, but this produced the best results in my testing with sample data. The length lookup table enables encoding of backrefs up to 256 bytes in length using only 5 bits, though some longer lengths canâ€™t be encoded directly. These are encoded as two successive backrefs, each with a smaller length.</p>
<p>The biggest conceptual changes vs LZG were the introductions of the LIT and EOF tokens. EOF eliminates the need to check the input pointer after decoding each token to determine if decompression is complete, and speeds things up slightly. LIT enables a whole block of literals to be quickly copied to the output buffer, instead of checking each one to see if itâ€™s a backref token. This speeds things up substantially, but also swells the size of the data. In the worst case, a single literal would encode as 1 byte in LZG but 2 bytes in FC8, making it twice as expensive! All the other changes were needed to cancel out the compression bloat introduced by the LIT token, with the end result that FC8 compresses equally as compactly as LZG. Both compressed my sample data to about 63% of original size.</p>
<p>The 68K decompressor code can be viewed <a href="https://github.com/steve-chamberlin/fc8-compression/blob/master/fc8-decompress-68000.c">here</a>.</p>

<p>Several people mentioned the possibility of on-the-fly decompression, since the intended use is a compressed disk image. Thatâ€™s something I plan to explore, but itâ€™s not as simple as it might seem at first. Disk sectors are 512 bytes, but thereâ€™s no way to decompress a specific 512 byte range from the compressed data, since the whole compression scheme depends on having 128K of prior data to draw on for backref matches. You could compress the entire disk image as a series of separate 512 byte blocks, but then the compression density would go to hell. A better solution would compress the entire disk image as a series of larger blocks, maybe 128K or a little smaller, and then design a caching scheme to keep track of whether the block containing a particular sector were already decompressed and available. This would still have a negative impact on the compression density, and it would make disk I/O slower, but would probably still be OK.</p>
<p>Ultimately I think the two decompression approaches each have strengths and weaknesses, so the best choice depends on the requirements.</p>
<p><em>Boot-Time Decompression:</em></p>
<p><em>On-the-Fly Decompression:</em></p>
<p>I discovered that a Macintosh IIci in 8-bit color mode decompresses about 20% slower than in 1-bit color mode. But a IIsi decompresses at the same speed regardless of the color settings. Both machines are using the built-in graphics hardware, which steals some memory cycles from the CPU in order to refresh the display. Iâ€™m not sure why only the IIci showed a dependence on the color depth. Both machines should be faster when using a discrete graphics card, though I didnâ€™t test this.</p>
<p>The original LZG compression showed a much bigger speed difference between the IIci and IIsi, closer to a 50% difference, which I assumed was due to the 32K cache card in the IIci as well as its faster CPU. Itâ€™s not clear why the discrepancy is smaller with FC8, or whether it means the IIci has gotten worse or the IIsi has gotten better, relatively speaking. Compared to the same machine with the LZG compression, FC8 is 1.57x faster on the IIci and 1.99x faster on the IIsi. Based on tests under emulation with MESS, I was expecting a 1.78x speedup.</p>

<p>While working on this, I discovered many places where compression compactness could be traded for decompression speed. My first attempt at FC8 had a minimum match size of 2 bytes instead of 3, which compressed about 0.7% smaller but was 13% slower to decompress due to the larger number of backrefs. At the other extreme, the introduction of a LIT token without any other changes resulted in the fastest decompression speed of all, about 7% faster than FC8, but the compressed files were about 6% larger, and I decided the tradeoff wasnâ€™t worth it. </p>
<p>I explored many other ideas to improve the compression density, but everything I thought of proved to have only a tiny benefit at best, not enough to justify the impact on decompression speed. An algorithm based on something other than LZ77 would likely have compressed substantially more densely, or say a combination of LZ77 and Huffman coding. But decompression of LZ77-based methods are far easier and faster to implement.</p>

<p>It eventually became obvious to me that defining the token format doesnâ€™t tell you much about how to best encode the data in that format. A greedy algorithm seemed to work fairly well, so thatâ€™s what I used. At each point in the uncompressed data, the compressor substitutes the best match it can make (if any) between that data and previous data in the history window. </p>
<p>However, there are some examples where choosing a non-optimal match would allow for an even better match later, resulting in better overall compression. This can happen due to quirks in the quantizing of match lengths, or with long runs of repeated bytes which are only partially matched in the previous data. Itâ€™s a bit like sacrificing your queen in chess, and sometimes you need to accept a short-term penalty in order to realize a long-term benefit. Better compression heuristics that took this into account could probably squeeze another 1% out of the data, without changing the compression format or the decompressor at all.</p>
      
<p><a href="https://www.bigmessowires.com/2016/05/06/fc8-faster-68k-decompression/#comments">Read 11 comments and join the conversation</a>Â       
    </p></div></div>
  </body>
</html>
