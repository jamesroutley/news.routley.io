<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/dblalock/bolt">Original</a>
    <h1>Bolt: Faster matrix and vector operations that run on compressed data</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/dblalock/bolt/blob/master/assets/bolt.jpg?raw=true"><img src="https://github.com/dblalock/bolt/raw/master/assets/bolt.jpg?raw=true" alt="Bolt" width="611px" height="221px"/></a>
  
</p>
<p dir="auto">Bolt is an algorithm for compressing vectors of real-valued data and running mathematical operations directly on the compressed representations.</p>
<p dir="auto">If you have a large collection of mostly-dense vectors and can tolerate lossy compression, Bolt can probably save you 10-200x space and compute time.</p>
<p dir="auto">Bolt also has <a href="https://github.com/dblalock/bolt/blob/master/assets/bolt-theory.pdf?raw=true">theoretical guarantees</a> bounding the errors in its approximations.</p>
<p dir="auto">EDIT: this repo now also features the source code for <a href="https://arxiv.org/abs/2106.10860" rel="nofollow">MADDNESS</a>, our shiny new algorithm for approximate matrix multiplication. MADDNESS has no Python wrapper yet, and is referred to as &#34;mithral&#34; in the source code. Name changed because apparently I&#39;m the only who gets Lord of the Rings references. MADDNESS runs ridiculously fast and, under reasonable assumptions, requires zero multiply-adds. Realistically, it&#39;ll be most useful for speeding up neural net inference on CPUs, but it&#39;ll take another couple papers to get it there; we need to generalize it to convolution and write the CUDA kernels to allow GPU training. </p>
<p dir="auto">EDIT2: Looking for a research project? See our <a href="https://github.com/dblalock/bolt/tree/master/experiments">list of ideas</a>.</p>
<p dir="auto"><strong>NOTE: All below code refers to the Python wrapper for Bolt and has nothing to do with MADDNESS.</strong> It also seems to be <a href="https://github.com/dblalock/bolt/issues/4" data-hovercard-type="issue" data-hovercard-url="/dblalock/bolt/issues/4/hovercard">no longer building</a> for many people. If you want to use MADDNESS, see the <a href="https://github.com/dblalock/bolt/blob/45454e6cfbc9300a43da6770abf9715674b47a0f/experiments/python/vq_amm.py#L273">Python Implementation</a> driven by <a href="https://github.com/dblalock/bolt/blob/45454e6cfbc9300a43da6770abf9715674b47a0f/experiments/python/amm_main.py">amm_main.py</a> or <a href="https://github.com/dblalock/bolt/blob/45454e6cfbc9300a43da6770abf9715674b47a0f/cpp/src/quantize/mithral.cpp">C++ implementation</a>. All code is ugly, but Python code should be pretty easy to add new AMM methods/variations to.**</p>

<h2 dir="auto"><a id="user-content-installing" aria-hidden="true" href="#installing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installing</h2>
<h4 dir="auto"><a id="user-content-python" aria-hidden="true" href="#python"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Python</h4>
<div data-snippet-clipboard-copy-content="  $ brew install swig  # for wrapping C++; use apt-get, yum, etc, if not OS X
  $ pip install numpy  # bolt installation needs numpy already present
  $ git clone https://github.com/dblalock/bolt.git
  $ cd bolt &amp;&amp; python setup.py install
  $ pytest tests/  # optionally, run the tests"><pre>  $ brew install swig  <span><span>#</span> for wrapping C++; use apt-get, yum, etc, if not OS X</span>
  $ pip install numpy  <span><span>#</span> bolt installation needs numpy already present</span>
  $ git clone https://github.com/dblalock/bolt.git
  $ <span>cd</span> bolt <span>&amp;&amp;</span> python setup.py install
  $ pytest tests/  <span><span>#</span> optionally, run the tests</span></pre></div>
<p dir="auto">If you run into any problems, please don&#39;t hesitate to mention it <a href="https://github.com/dblalock/bolt/issues/4" data-hovercard-type="issue" data-hovercard-url="/dblalock/bolt/issues/4/hovercard">in the Python build problems issue</a>.</p>
<h4 dir="auto"><a id="user-content-c" aria-hidden="true" href="#c"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>C++</h4>
<p dir="auto">Install <a href="https://bazel.build" rel="nofollow">Bazel</a>, Google&#39;s open-source build system. Then</p>
<div data-snippet-clipboard-copy-content="  $ git clone https://github.com/dblalock/bolt.git
  $ cd bolt/cpp &amp;&amp; bazel run :main"><pre>  $ git clone https://github.com/dblalock/bolt.git
  $ <span>cd</span> bolt/cpp <span>&amp;&amp;</span> bazel run :main</pre></div>
<p dir="auto">The <code>bazel run</code> command will build the project and run the tests and benchmarks.</p>
<p dir="auto">If you want to integrate Bolt with another C++ project, include <code>cpp/src/include/public.hpp</code> and add the remaining files under <code>cpp/src</code> to your builds. You should let me know if you&#39;re interested in doing such an integration because I&#39;m hoping to see Bolt become part of many libraries and thus would be happy to help you. </p>
<h4 dir="auto"><a id="user-content-notes" aria-hidden="true" href="#notes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Notes</h4>
<p dir="auto">Bolt currently only supports machines with <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#Advanced_Vector_Extensions_2" rel="nofollow">AVX2 instructions</a>, which basically means x86 machines from fall 2013 or later. Contributions for ARM support <a href="https://github.com/dblalock/bolt/issues/2" data-hovercard-type="issue" data-hovercard-url="/dblalock/bolt/issues/2/hovercard">are welcome</a>. Also note that the Bolt Python wrapper is currently configured to require Clang, since GCC apparently <a href="https://github.com/dblalock/bolt/issues/4" data-hovercard-type="issue" data-hovercard-url="/dblalock/bolt/issues/4/hovercard">runs into issues</a>.</p>
<h2 dir="auto"><a id="user-content-how-does-it-work" aria-hidden="true" href="#how-does-it-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How does it work?</h2>
<p dir="auto">Bolt is based on <a href="https://en.wikipedia.org/wiki/Vector_quantization" rel="nofollow">vector quantization</a>. For details, see the <a href="https://arxiv.org/abs/1706.10283" rel="nofollow">Bolt paper</a> or <a href="https://github.com/dblalock/bolt/blob/master/assets/bolt-slides.pdf?raw=true">slides</a>.</p>
<h2 dir="auto"><a id="user-content-benchmarks" aria-hidden="true" href="#benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Benchmarks</h2>
<p dir="auto">Bolt includes a thorough set of speed and accuracy benchmarks. See the <code>experiments/</code> directory. This is also what you want if you want to reproduce the results in the paper.</p>
<p dir="auto">Note that all of the timing results use the raw C++ implementation. At present, the Python wrapper is slightly slower due to Python overhead. If you&#39;re interested in having a full-speed wrapper, let me know and I&#39;ll allocate time to making this happen.</p>
<h2 dir="auto"><a id="user-content-basic-usage" aria-hidden="true" href="#basic-usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Basic usage</h2>
<div data-snippet-clipboard-copy-content="X, queries = some N x D array, some iterable of length D arrays

# these are approximately equal (though the latter are shifted and scaled)
enc = bolt.Encoder(reduction=&#39;dot&#39;).fit(X)
[np.dot(X, q) for q in queries]
[enc.transform(q) for q in queries]

# same for these
enc = bolt.Encoder(reduction=&#39;l2&#39;).fit(X)
[np.sum((X - q) * (X - q), axis=1) for q in queries]
[enc.transform(q) for q in queries]

# but enc.transform() is 10x faster or more"><pre><span>X</span>, <span>queries</span> <span>=</span> <span>some</span> <span>N</span> <span>x</span> <span>D</span> <span>array</span>, <span>some</span> <span>iterable</span> <span>of</span> <span>length</span> <span>D</span> <span>arrays</span>

<span># these are approximately equal (though the latter are shifted and scaled)</span>
<span>enc</span> <span>=</span> <span>bolt</span>.<span>Encoder</span>(<span>reduction</span><span>=</span><span>&#39;dot&#39;</span>).<span>fit</span>(<span>X</span>)
[<span>np</span>.<span>dot</span>(<span>X</span>, <span>q</span>) <span>for</span> <span>q</span> <span>in</span> <span>queries</span>]
[<span>enc</span>.<span>transform</span>(<span>q</span>) <span>for</span> <span>q</span> <span>in</span> <span>queries</span>]

<span># same for these</span>
<span>enc</span> <span>=</span> <span>bolt</span>.<span>Encoder</span>(<span>reduction</span><span>=</span><span>&#39;l2&#39;</span>).<span>fit</span>(<span>X</span>)
[<span>np</span>.<span>sum</span>((<span>X</span> <span>-</span> <span>q</span>) <span>*</span> (<span>X</span> <span>-</span> <span>q</span>), <span>axis</span><span>=</span><span>1</span>) <span>for</span> <span>q</span> <span>in</span> <span>queries</span>]
[<span>enc</span>.<span>transform</span>(<span>q</span>) <span>for</span> <span>q</span> <span>in</span> <span>queries</span>]

<span># but enc.transform() is 10x faster or more</span></pre></div>
<h2 dir="auto"><a id="user-content-example-matrix-vector-multiplies" aria-hidden="true" href="#example-matrix-vector-multiplies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example: Matrix-vector multiplies</h2>
<div data-snippet-clipboard-copy-content="import bolt
import numpy as np
from scipy.stats import pearsonr as corr
from sklearn.datasets import load_digits
import timeit

# for simplicity, use the sklearn digits dataset; we&#39;ll split
# it into a matrix X and a set of queries Q
X, _ = load_digits(return_X_y=True)
nqueries = 20
X, Q = X[:-nqueries], X[-nqueries:]

enc = bolt.Encoder(reduction=&#39;dot&#39;, accuracy=&#39;lowest&#39;) # can tweak acc vs speed
enc.fit(X)

dot_corrs = np.empty(nqueries)
for i, q in enumerate(Q):
    dots_true = np.dot(X, q)
    dots_bolt = enc.transform(q)
    dot_corrs[i] = corr(dots_true, dots_bolt)[0]

# dot products closely preserved despite compression
print &#34;dot product correlation: {} +/- {}&#34;.format(
    np.mean(dot_corrs), np.std(dot_corrs))  # &gt; .97

# massive space savings
print(X.nbytes)  # 1777 rows * 64 cols * 8B = 909KB
print(enc.nbytes)  # 1777 * 2B = 3.55KB

# massive time savings (~10x here, but often &gt;100x on larger
# datasets with less Python overhead; see the paper)
t_np = timeit.Timer(
    lambda: [np.dot(X, q) for q in Q]).timeit(5)        # ~9ms
t_bolt = timeit.Timer(
    lambda: [enc.transform(q) for q in Q]).timeit(5)    # ~800us
print &#34;Numpy / BLAS time, Bolt time: {:.3f}ms, {:.3f}ms&#34;.format(
    t_np * 1000, t_bolt * 1000)

# can get output without offset/scaling if needed
dots_bolt = [enc.transform(q, unquantize=True) for q in Q]"><pre><span>import</span> <span>bolt</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>scipy</span>.<span>stats</span> <span>import</span> <span>pearsonr</span> <span>as</span> <span>corr</span>
<span>from</span> <span>sklearn</span>.<span>datasets</span> <span>import</span> <span>load_digits</span>
<span>import</span> <span>timeit</span>

<span># for simplicity, use the sklearn digits dataset; we&#39;ll split</span>
<span># it into a matrix X and a set of queries Q</span>
<span>X</span>, <span>_</span> <span>=</span> <span>load_digits</span>(<span>return_X_y</span><span>=</span><span>True</span>)
<span>nqueries</span> <span>=</span> <span>20</span>
<span>X</span>, <span>Q</span> <span>=</span> <span>X</span>[:<span>-</span><span>nqueries</span>], <span>X</span>[<span>-</span><span>nqueries</span>:]

<span>enc</span> <span>=</span> <span>bolt</span>.<span>Encoder</span>(<span>reduction</span><span>=</span><span>&#39;dot&#39;</span>, <span>accuracy</span><span>=</span><span>&#39;lowest&#39;</span>) <span># can tweak acc vs speed</span>
<span>enc</span>.<span>fit</span>(<span>X</span>)

<span>dot_corrs</span> <span>=</span> <span>np</span>.<span>empty</span>(<span>nqueries</span>)
<span>for</span> <span>i</span>, <span>q</span> <span>in</span> <span>enumerate</span>(<span>Q</span>):
    <span>dots_true</span> <span>=</span> <span>np</span>.<span>dot</span>(<span>X</span>, <span>q</span>)
    <span>dots_bolt</span> <span>=</span> <span>enc</span>.<span>transform</span>(<span>q</span>)
    <span>dot_corrs</span>[<span>i</span>] <span>=</span> <span>corr</span>(<span>dots_true</span>, <span>dots_bolt</span>)[<span>0</span>]

<span># dot products closely preserved despite compression</span>
<span>print</span> <span>&#34;dot product correlation: {} +/- {}&#34;</span>.<span>format</span>(
    <span>np</span>.<span>mean</span>(<span>dot_corrs</span>), <span>np</span>.<span>std</span>(<span>dot_corrs</span>))  <span># &gt; .97</span>

<span># massive space savings</span>
<span>print</span>(<span>X</span>.<span>nbytes</span>)  <span># 1777 rows * 64 cols * 8B = 909KB</span>
<span>print</span>(<span>enc</span>.<span>nbytes</span>)  <span># 1777 * 2B = 3.55KB</span>

<span># massive time savings (~10x here, but often &gt;100x on larger</span>
<span># datasets with less Python overhead; see the paper)</span>
<span>t_np</span> <span>=</span> <span>timeit</span>.<span>Timer</span>(
    <span>lambda</span>: [<span>np</span>.<span>dot</span>(<span>X</span>, <span>q</span>) <span>for</span> <span>q</span> <span>in</span> <span>Q</span>]).<span>timeit</span>(<span>5</span>)        <span># ~9ms</span>
<span>t_bolt</span> <span>=</span> <span>timeit</span>.<span>Timer</span>(
    <span>lambda</span>: [<span>enc</span>.<span>transform</span>(<span>q</span>) <span>for</span> <span>q</span> <span>in</span> <span>Q</span>]).<span>timeit</span>(<span>5</span>)    <span># ~800us</span>
<span>print</span> <span>&#34;Numpy / BLAS time, Bolt time: {:.3f}ms, {:.3f}ms&#34;</span>.<span>format</span>(
    <span>t_np</span> <span>*</span> <span>1000</span>, <span>t_bolt</span> <span>*</span> <span>1000</span>)

<span># can get output without offset/scaling if needed</span>
<span>dots_bolt</span> <span>=</span> [<span>enc</span>.<span>transform</span>(<span>q</span>, <span>unquantize</span><span>=</span><span>True</span>) <span>for</span> <span>q</span> <span>in</span> <span>Q</span>]</pre></div>
<h2 dir="auto"><a id="user-content-example-k-nearest-neighbor--maximum-inner-product-search" aria-hidden="true" href="#example-k-nearest-neighbor--maximum-inner-product-search"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example: K-Nearest Neighbor / Maximum Inner Product Search</h2>
<div data-snippet-clipboard-copy-content="# search using squared Euclidean distances
# (still using the Digits dataset from above)
enc = bolt.Encoder(&#39;l2&#39;, accuracy=&#39;high&#39;).fit(X)
bolt_knn = [enc.knn(q, k_bolt) for q in Q]  # knn for each query

# search using dot product (maximum inner product search)
enc = bolt.Encoder(&#39;dot&#39;, accuracy=&#39;medium&#39;).fit(X)
bolt_knn = [enc.knn(q, k_bolt) for q in Q]  # knn for each query"><pre><span># search using squared Euclidean distances</span>
<span># (still using the Digits dataset from above)</span>
<span>enc</span> <span>=</span> <span>bolt</span>.<span>Encoder</span>(<span>&#39;l2&#39;</span>, <span>accuracy</span><span>=</span><span>&#39;high&#39;</span>).<span>fit</span>(<span>X</span>)
<span>bolt_knn</span> <span>=</span> [<span>enc</span>.<span>knn</span>(<span>q</span>, <span>k_bolt</span>) <span>for</span> <span>q</span> <span>in</span> <span>Q</span>]  <span># knn for each query</span>

<span># search using dot product (maximum inner product search)</span>
<span>enc</span> <span>=</span> <span>bolt</span>.<span>Encoder</span>(<span>&#39;dot&#39;</span>, <span>accuracy</span><span>=</span><span>&#39;medium&#39;</span>).<span>fit</span>(<span>X</span>)
<span>bolt_knn</span> <span>=</span> [<span>enc</span>.<span>knn</span>(<span>q</span>, <span>k_bolt</span>) <span>for</span> <span>q</span> <span>in</span> <span>Q</span>]  <span># knn for each query</span></pre></div>
<h2 dir="auto"><a id="user-content-miscellaneous" aria-hidden="true" href="#miscellaneous"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Miscellaneous</h2>
<p dir="auto">Bolt stands for &#34;Based On Lookup Tables&#34;. Feel free to use this exciting fact at parties.</p>

</article>
          </div></div>
  </body>
</html>
