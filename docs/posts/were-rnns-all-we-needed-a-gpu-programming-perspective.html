<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dhruvmsheth.github.io/projects/gpu_pogramming_curnn/">Original</a>
    <h1>Were RNNs all we needed? A GPU programming perspective</h1>
    
    <div id="readability-page-1" class="page"><div> <table> <thead> <tr> <th>Resource</th> <th>Link</th> </tr> </thead> <tbody> <tr> <td>Project Repository</td> <td><a href="https://github.com/dhruvmsheth/cuRNN" rel="external nofollow noopener" target="_blank">GitHub</a></td> </tr> </tbody> </table> <p>For my final project in CS179: GPU Programming, I decided to implement the paper <a href="https://arxiv.org/abs/2403.07853" rel="external nofollow noopener" target="_blank">“Were RNNs All We Needed?”</a> by Feng et al. The paper’s core claim is that by making minor simplifications to LSTMs and GRUs, their recurrence can be expressed in a form amenable to the parallel scan algorithm. This changes their training and inference from an $O(T)$ sequential process into an $O(\log T)$ parallel one, which helps with GPU acceleration.</p> <p>My goal was to verify this claim by building both the simplified models (minGRU and minLSTM) and a custom CUDA implementation of the parallel scan to see how much of a speedup was actually achievable. The focus was less on the machine learning application and more on the raw computational performance and the experience of parallelizing a traditionally sequential algorithm.</p> <h3 id="the-sequential-bottleneck-in-standard-rnns">The Sequential Bottleneck in Standard RNNs</h3> <p>Recurrent Neural Networks, by their very nature, process sequences one step at a time. The hidden state at time step $t$, denoted $h_t$, is a function of the input $x_t$ and the <em>previous</em> hidden state, $h_{t-1}$. This dependency is the fundamental barrier to parallelization.</p> <p>Let’s look at a standard GRU. The update equations are: \(r_t = \sigma(W_{ir}x_t + b_{ir} + W_{hr}h_{t-1} + b_{hr})\) \(z_t = \sigma(W_{iz}x_t + b_{iz} + W_{hz}h_{t-1} + b_{hz})\) \(n_t = \tanh(W_{in}x_t + b_{in} + r_t \odot (W_{hn}h_{t-1} + b_{hn}))\) \(h_t = (1 - z_t) \odot n_t + z_t \odot h_{t-1}\)</p> <p>The reset gate ($r_t$) and update gate ($z_t$) both explicitly depend on $h_{t-1}$. You simply cannot compute the gates for the entire sequence in one shot, because each step requires the output from the one before it. This forces a sequential loop, which is notoriously inefficient on parallel hardware like GPUs.</p> <h3 id="gru-and-lstm--mingru-and-minlstm">GRU and LSTM –&gt; minGRU and minLSTM</h3> <p>The crux of the paper is to remove this direct dependency. The simplified models, minGRU and minLSTM, redefine the gates to depend only on the current input, $x_t$.</p> <p>For minGRU, the gates are simplified to:</p> <ul> <li>Update Gate: $z_t = \sigma(W_z x_t + b_z)$</li> <li>Candidate Hidden State: $\tilde{h}_t = W_h x_t + b_h$</li> </ul> <p>The recurrence then becomes: \(h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t\)</p> <p>This equation is now in the form $h_t = a_t \odot h_{t-1} + b_t$, where:</p> <ul> <li>$a_t = (1 - z_t)$</li> <li>$b_t = z_t \odot \tilde{h}_t$</li> </ul> <p>Crucially, because $z_t$ and $\tilde{h}_t$ only depend on $x_t$, we can compute the entire sequence of $(a_t, b_t)$ pairs in parallel with a single, large matrix multiplication. The problem has now shifted from a complex sequential dependency to resolving a linear recurrence relation. A similar simplification is applied to LSTMs to create minLSTM.</p> <h3 id="the-parallel-scan-algorithm">The Parallel Scan Algorithm</h3> <p>This linear recurrence is a classic computer science problem that can be solved efficiently with a parallel scan (also known as a prefix sum). The scan operation takes a sequence and an associative binary operator $\oplus$, and computes the prefix results. For our recurrence, the operator is slightly more complex than simple addition.</p> <p>If we have a transformation $(A, B)$ representing $h_{out} = A \cdot h_{in} + B$, we can define an associative operator $\oplus$ to compose two such transformations: \((A_2, B_2) \oplus (A_1, B_1) = (A_2 A_1, A_2 B_1 + B_2)\)</p> <p>With this operator, we can use an algorithm like Blelloch’s scan, which performs the computation in two phases (up-sweep and down-sweep) on a tree-like structure. This reduces the number of sequential steps from $O(T)$ to $O(\log T)$, making it a perfect fit for the GPU’s architecture. For numerical stability, the paper and my implementation use a log-space version of this scan.</p> <h3 id="implementation--benchmarking">Implementation &amp; Benchmarking</h3> <p>To measure the real-world impact, I compared three implementation paths:</p> <ol> <li> <strong>CPU-seq</strong>: A standard PyTorch loop, <code>for t in range(T): h_t = cell(x_t, h_{t-1})</code>. This represents the classic, non-parallelizable RNN.</li> <li> <strong>CPU-scan</strong>: Computes all gate parameters $(a_t, b_t)$ in one vectorized PyTorch call, but resolves the scan recurrence using a Python loop. This isolates the benefit of vectorization from the parallel scan.</li> <li> <strong>GPU-scan</strong>: My custom CUDA implementation. It uses one kernel to extract all scan parameters and a second kernel to perform the work-efficient Blelloch scan, reducing the recurrence depth to $O(\log T)$.</li> </ol> <p>I ran benchmarks on my personal machine (Intel i9-12900K, RTX 4090).</p> <div> <div> <figure> <picture> <source srcset="/assets/img/cs179/sequential-480.webp 480w,/assets/img/cs179/sequential-800.webp 800w,/assets/img/cs179/sequential-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://dhruvmsheth.github.io/assets/img/cs179/sequential.png" width="100%" height="auto" title="Sequential CPU vs GPU parallelized" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> </div> <p> Log-log plot showing the performance of CPU sequential, CPU parallel-scan, and GPU parallel-scan implementations. The GPU implementation&#39;s logarithmic scaling becomes evident at longer sequence lengths. </p> <h4 id="gru-performance">GRU Performance</h4> <table> <thead> <tr> <th>$T$</th> <th>CPU-seq</th> <th>CPU-scan</th> <th>GPU-scan</th> </tr> </thead> <tbody> <tr> <td>256</td> <td>634 ms</td> <td>32.8 ms</td> <td><strong>25.8 ms</strong></td> </tr> <tr> <td>1,024</td> <td>2,395 ms</td> <td><strong>97.6 ms</strong></td> <td>92.1 ms</td> </tr> <tr> <td>4,096</td> <td>5,493 ms</td> <td><strong>300 ms</strong></td> <td>340 ms</td> </tr> <tr> <td>16,384</td> <td>–</td> <td>2,683 ms</td> <td><strong>1,333 ms</strong></td> </tr> <tr> <td>65,536</td> <td>–</td> <td>10,989 ms</td> <td><strong>5,330 ms</strong></td> </tr> </tbody> </table> <div> <div> <figure> <picture> <source srcset="/assets/img/cs179/plot_gru-480.webp 480w,/assets/img/cs179/plot_gru-800.webp 800w,/assets/img/cs179/plot_gru-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://dhruvmsheth.github.io/assets/img/cs179/plot_gru.png" width="100%" height="auto" title="GRU Performance Comparison" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> </div> <h4 id="lstm-performance">LSTM Performance</h4> <table> <thead> <tr> <th>T</th> <th>CPU-seq</th> <th>CPU-scan</th> <th>GPU-scan</th> </tr> </thead> <tbody> <tr> <td>256</td> <td>701.0 ms</td> <td>37.0 ms</td> <td><strong>22.0 ms</strong></td> </tr> <tr> <td>1,024</td> <td>2,966.7 ms</td> <td><strong>96.8 ms</strong></td> <td>107.8 ms</td> </tr> <tr> <td>4,096</td> <td>12,035.1 ms</td> <td><strong>416.5 ms</strong></td> <td>431.5 ms</td> </tr> <tr> <td>16,384</td> <td>–</td> <td>2,993.6 ms</td> <td><strong>1,693.9 ms</strong></td> </tr> <tr> <td>65,536</td> <td>–</td> <td>13,005.4 ms</td> <td><strong>6,709.8 ms</strong></td> </tr> </tbody> </table> <div> <div> <figure> <picture> <source srcset="/assets/img/cs179/plot_lstm-480.webp 480w,/assets/img/cs179/plot_lstm-800.webp 800w,/assets/img/cs179/plot_lstm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="https://dhruvmsheth.github.io/assets/img/cs179/plot_lstm.png" width="100%" height="auto" title="LSTM Performance Comparison" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>For short sequences ($T &lt; 2048$), the overhead of launching CUDA kernels and memory transfers negates some of the benefits of the parallel scan. The CPU-scan implementation, which benefits from highly optimized BLAS libraries (like MKL), is sometimes faster.</li> <li>For long sequences ($T &gt; 8192$), the algorithmic advantage of the $O(\log T)$ GPU scan becomes dominant. At $T=65,536$, the GPU implementation is roughly <strong>2x faster</strong> than the vectorized CPU version and would be orders of magnitude faster than the purely sequential version.</li> <li>Vectorizing the gate computations alone (<code>CPU-seq</code> vs <code>CPU-scan</code>) provides a massive, constant-factor speedup (around 10x), but the runtime still scales linearly, $O(T)$.</li> </ul> <h3 id="gpu-kernel-profiling-with-nsight">GPU Kernel Profiling with Nsight</h3> <p>To understand the GPU performance better, I used NVIDIA’s Nsight Compute profiler. The initial implementation launched thousands of tiny kernels, one for each time step, which is a classic anti-pattern in GPU programming due to launch overhead.</p> <p>My first major optimization was to fuse the gate computations for all time steps into a single, large kernel (<code>min_gru_extract_scan_params_kernel</code>) that uses shared memory tiling to manage weights and inputs efficiently.</p> <p>Here’s a snapshot of the kernel performance breakdown at $T=4096$ after this optimization:</p> <table> <thead> <tr> <th>Rank</th> <th>Kernel</th> <th>Time/launch</th> <th>Launches</th> <th>% wall-time</th> </tr> </thead> <tbody> <tr> <td>1</td> <td><code>min_gru_extract_scan_params_kernel</code></td> <td>180 $\mu$s</td> <td>1</td> <td><strong>8 %</strong></td> </tr> <tr> <td>2–9</td> <td> <code>compose_offset_kernel</code> (Scan Up-Sweep)</td> <td>~3 $\mu$s</td> <td>12</td> <td>&lt; 1 %</td> </tr> <tr> <td>10–</td> <td> <code>apply_scan_op_kernel</code> (Scan Down-Sweep)</td> <td>~2 $\mu$s</td> <td>4096</td> <td><strong>10 %</strong></td> </tr> <tr> <td>11–</td> <td> <code>matvec_kernel</code> (Output Projection)</td> <td>~93 $\mu$s</td> <td>4096</td> <td><strong>72 %</strong></td> </tr> </tbody> </table>  <p> Nsight Compute profiles showing the performance of the fused gate extraction kernel. </p> <p>The profiling revealed a few things:</p> <ol> <li>Success: The gate extraction kernel, which was a huge bottleneck, now only takes 8% of the total time and is memory-bandwidth bound, saturating L2 bandwidth at 1.9 TB/s. This is a good place to be.</li> <li>New Bottleneck: The primary bottleneck shifted to the final projection layer (<code>matvec_kernel</code>), which consumes 72% of the runtime. This is because I was still launching one kernel per time step (4096 launches!), leading to low occupancy and terrible memory bandwidth utilization (only 23 GB/s).</li> <li>Next Step: The next step is to replace the 4096 <code>matvec</code> launches with a single cuBLAS GEMM call ($C = A \cdot W^T$). This would eliminate the kernel launch overhead and leverage a highly optimized library routine, likely bringing the total latency down significantly.</li> </ol> <h3 id="final-thoughts">Final Thoughts</h3> <p>I made this project as part of my final assignment for my GPU Programming course, CS 179 at Caltech. This project was a great hands-on lesson in parallel algorithms. The claims in “Were RNNs All We Needed?” did seem to hold up: by reformulating the recurrence, RNNs can indeed be parallelized, and the performance gains on GPUs are substantial for long sequences. One of the reasons why this paper gained criticism from the CS commmunity was because there weren’t many experiments to back the claim that miniRNNs performed any better than transformers - and from my rusty recollection of the paper - the only benchmarks the miniRNNs outperformed transformers were niche targetted datasets that did not transfer to real-world benchmarks. Sure, with the parallelization and ripping away unncessary complexity from RNNs, miniRNNs were much more efficient but it was never the end-all be-all.</p> <p>I remember Francois’s quote on this paper from his X thread:</p> <blockquote> <p>Interesting work on reviving RNNs. https://arxiv.org/abs/2410.01201 – in general the fact that there are many recent architectures coming from different directions that roughly match Transformers is proof that architectures aren’t fundamentally important in the curve-fitting paradigm (aka deep learning)</p> <p>Curve-fitting is about embedding a dataset on a curve. The critical factor is the dataset, not the specific hard-coded bells and whistles that constrain the curve’s shape. As long as your curve is sufficiently expressive all architectures will converge to the same performance in the large-data regime.</p> </blockquote> <p>I have a different take on this. All progress in the language modelling in the last decade has come from changes in architectures to be able to generate rich, more expressive curves that fit the target dataset better. If we blindly apply the bitter lesson and throw enough compute to different architectures (reasonable ones), it would be a good signal to see which architecture hits the wall the fastest and which one continues generalizing rather than all converging in the same way eventually.</p> </div></div>
  </body>
</html>
