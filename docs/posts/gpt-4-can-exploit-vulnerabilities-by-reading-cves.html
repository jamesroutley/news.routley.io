<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theregister.com/2024/04/17/gpt4_can_exploit_real_vulnerabilities/">Original</a>
    <h1>GPT-4 can exploit vulnerabilities by reading CVEs</h1>
    
    <div id="readability-page-1" class="page"><div id="body">
<p>AI agents, which combine large language models with automation software, can successfully exploit real world security vulnerabilities by reading security advisories, academics have claimed.</p>
<p>In a newly released <a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2404.08144">paper</a>, four University of Illinois Urbana-Champaign (UIUC) computer scientists – Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang – report that OpenAI&#39;s GPT-4 large language model (LLM) can autonomously exploit vulnerabilities in real-world systems if given a CVE advisory describing the flaw.</p>
<p>&#34;To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description,&#34; the US-based authors explain in their paper.</p>

    

<p>&#34;When given the CVE description, GPT-4 is capable of exploiting 87 percent of these vulnerabilities compared to 0 percent for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit).&#34;</p>
<blockquote>

<p>If you extrapolate to what future models can do, it seems likely they will be much more capable than what script kiddies can get access to today</p>
</blockquote>
<p>The term &#34;one-day vulnerability&#34; refers to vulnerabilities that have been disclosed but not patched. And by CVE description, the team means a CVE-tagged advisory shared by NIST – eg, <a target="_blank" rel="nofollow" href="https://nvd.nist.gov/vuln/detail/CVE-2024-28859">this one</a> for CVE-2024-28859.</p>
<p>The unsuccessful models tested – GPT-3.5, OpenHermes-2.5-Mistral-7B, Llama-2 Chat (70B), LLaMA-2 Chat (13B), LLaMA-2 Chat (7B), Mixtral-8x7B Instruct, Mistral (7B) Instruct v0.2, Nous Hermes-2 Yi 34B, and OpenChat 3.5 – did not include two leading commercial rivals of GPT-4, Anthropic&#39;s Claude 3 and Google&#39;s Gemini 1.5 Pro. The UIUC boffins did not have access to those models, though they hope to test them at some point.</p>

        


        

<p>The researchers&#39; work builds upon <a target="_blank" href="https://www.theregister.com/2024/02/17/ai_models_weaponized/">prior findings</a> that LLMs can be used to automate attacks on websites in a sandboxed environment.</p>
<p>GPT-4, said Daniel Kang, assistant professor at UIUC, in an email to <em>The Register</em>, &#34;can actually autonomously carry out the steps to perform certain exploits that open-source vulnerability scanners cannot find (at the time of writing).&#34;</p>

        

<p>Kang said he expects LLM agents, created by (in this instance) wiring a chatbot model to the <a target="_blank" rel="nofollow" href="https://python.langchain.com/docs/modules/agents/agent_types/react/">ReAct</a> automation framework implemented in LangChain, will make exploitation much easier for everyone. These agents can, we&#39;re told, follow links in CVE descriptions for more information.</p>
<p>&#34;Also, if you extrapolate to what GPT-5 and future models can do, it seems likely that they will be much more capable than what script kiddies can get access to today,&#34; he said.</p>
<ul>

<li><a href="https://www.theregister.com/2024/04/15/stanford_report_ai/">What&#39;s up with AI lately? Let&#39;s start with soaring costs, public anger, regulations...</a></li>

<li><a href="https://www.theregister.com/2024/04/11/genai_amazon_internet/">GenAI will be bigger than the cloud or the internet, Amazon CEO hopes</a></li>

<li><a href="https://www.theregister.com/2024/04/09/ai_datacenters_unsustainable/">Arm CEO warns AI&#39;s power appetite could devour 25% of US electricity by 2030</a></li>

<li><a href="https://www.theregister.com/2024/04/02/ai_dominates_at_darpa_and/">What if AI produces code not just quickly but also, dunno, securely, DARPA wonders</a></li>
</ul>
<p>Denying the LLM agent (GPT-4) access to the relevant CVE description reduced its success rate from 87 percent to just seven percent. However, Kang said he doesn&#39;t believe limiting the public availability of security information is a viable way to defend against LLM agents.</p>
<p>&#34;I personally don&#39;t think security through obscurity is tenable, which seems to be the prevailing wisdom amongst security researchers,&#34; he explained. &#34;I&#39;m hoping my work, and other work, will encourage proactive security measures such as updating packages regularly when security patches come out.&#34;</p>
<p>The LLM agent failed to exploit just two of the 15 samples: Iris XSS (CVE-2024-25640) and Hertzbeat RCE (CVE-2023-51653). The former, according to the paper, proved problematic because the Iris web app has an interface that&#39;s extremely difficult for the agent to navigate. And the latter features a detailed description in Chinese, which presumably confused the LLM agent operating under an English language prompt.</p>
<div><p><img src="https://regmedia.co.uk/2024/02/17/hacker_shutterstock.jpg?x=174&amp;amp;y=115&amp;amp;crop=1" width="174" height="115" alt="haker"/></p><h2 title="We speak to professor who with colleagues tooled up OpenAI&#39;s GPT-4 and other neural nets">How to weaponize LLMs to auto-hijack websites</h2>
<p><a href="https://www.theregister.com/2024/02/17/ai_models_weaponized/"><span>NOW READ</span></a></p></div>
<p>Eleven of the vulnerabilities tested occurred after GPT-4&#39;s training cutoff, meaning the model had not learned any data about them during training. Its success rate for these CVEs was slightly lower at 82 percent, or 9 out of 11.</p>
<p>As to the nature of the bugs, they are all listed in the above paper, and we&#39;re told: &#34;Our vulnerabilities span website vulnerabilities, container vulnerabilities, and vulnerable Python packages. Over half are categorized as &#39;high&#39; or &#39;critical&#39; severity by the CVE description.&#34;</p>

        

<p>Kang and his colleagues computed the cost to conduct a successful LLM agent attack and came up with a figure of $8.80 per exploit, which they say is about 2.8x less than it would cost to hire a human penetration tester for 30 minutes.</p>
<p>The agent code, according to Kang, consists of just 91 lines of code and 1,056 tokens for the prompt. The researchers were asked by OpenAI, the maker of GPT-4, to not release their prompts to the public, though they say they will provide them upon request.</p>
<p>OpenAI did not immediately respond to a request for comment. ®</p>                                


                    </div></div>
  </body>
</html>
