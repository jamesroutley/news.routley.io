<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://saurabhalone.com/blogs/llama3/web">Original</a>
    <h1>Implementing LLaMA3 in 100 Lines of Pure Jax</h1>
    
    <div id="readability-page-1" class="page"><div>
            <article>
                
                
                

                

                <div>

                    <p>In this post, we&#39;ll implement <strong>llama3</strong> from scratch using pure <strong>jax</strong> in just 100 lines of code. Why jax? Because I think it has good aesthetics. Also jax looks like a NumPy wrapper but it has some cool features like <strong>xla</strong>; a linear algebra accelerator, <strong>jit</strong>, <strong>vmap</strong>, <strong>pmap</strong> etc., which makes your training go brr brr.</p>
                    <p>Jax is one of the first libraries which strongly focuses on the soul of <strong>pure functional programming</strong> which makes it more cool.<sup>1</sup></p>
                    <p><strong>Note :</strong></p>
                    <div>
                        
                        <div>
                            <ul>
                                <li>This post assumes familiarity with Python and basic understanding of transformer architectures.</li>
                                <li>This implementation is  for educational purposes, which means it is not for any production stuff but it covers all components of the model.<sup>2</sup></li>
                                <li>If you don&#39;t wanna read this amazing blog post then you can check out all the code at.<sup>3</sup></li>
                            </ul>
                        </div>
                    </div>

                    <p><img src="https://www.scd31.com/posts/images/newllama.png" alt="Llama architecture"/>
                        <img src="https://www.scd31.com/posts/images/llamadark.png" alt="Llama architecture"/>
                    </p>
                </div>


                


                

                <div>
                    <h2 id="section-1">LLaMA3</h2>
                    <p> 
                        At its core, LLaMA 3 is a decoder only transformer language model that generates text one token at a time, 
                        building on previous tokens to predict what comes next ; like completing a sentence word by word.
                   </p>
                    <p>
                        So lets fucking go !! we&#39;re doing it, get your diet coke !! First, we will begin with setting up device<sup>5</sup> and configuring the model.
                    </p>
                </div>



<div>
    <pre><code>
<span>os</span>.<span>environ</span>[<span>&#39;JAX_PLATFORM_NAME&#39;</span>] = <span>&#39;gpu&#39;</span>
<span>os</span>.<span>environ</span>[<span>&#39;XLA_PYTHON_CLIENT_PREALLOCATE&#39;</span>] = <span>&#39;false&#39;</span>
<span>print</span>(<span>&#34;JAX devices:&#34;</span>, <span>jax</span>.<span>devices</span>())</code></pre>
</div>

<p> So these are the hyperparameter we need to train approximately 2 million parameters model.</p>

<div>
    <pre><code>
<span>args</span> = <span>ModelArgs</span>(
    <span>vocab_size</span>=<span>enc</span>.<span>n_vocab</span>,    
    <span>dim</span>=<span>256</span>,                
    <span>n_layers</span>=<span>6</span>,            
    <span>n_heads</span>=<span>8</span>,             
    <span>n_kv_heads</span>=<span>4</span>,          
    <span>max_seq_len</span>=<span>512</span>,       
    <span>norm_eps</span>=<span>1e-5</span>          
)</code></pre></div>

    
<div>
    <h2 id="section-2">Model Weights Initialization</h2>


    
    
<p>
  In pure JAX, we don&#39;t use classes like in PyTorch. We use only pure fucntions why ? cause it makes our code more predictable and easier to parallelize.
   
  A pure function always returns the same output for the same input and doesn’t cause any side effects.<sup>6</sup> For example, if you call F(x), you&#39;ll always get the same y.
</p>

<p>
  Since we aren’t using a framework like PyTorch’s <strong>nn.Module</strong> to automatically track parameters, we must initialize and update our weights manually.
</p>

<p>
  Handling randomness is also different. Instead of relying on a single global seed as in NumPy or PyTorch, in <strong>jax</strong> we need to manage randomness with explicit pseudo-random number generator (PRNG) keys. Each random operation gets its own unique key, which is derived by splitting a parent key. This will help in reproducibility and parallelism.
</p>

<p>For example, below you can see we are creating a key and splitting it into sub keys and then providing that key to the function which involves the randomness.</p>

</div>


<div>
    <pre><code>
key = jax.random.PRNGKey(<span>42</span>)


key, subkey = jax.random.split(key)


weights = jax.random.normal(subkey, (<span>784</span>, <span>512</span>))
</code></pre>
</div>           




<p>
        Now lets start with our Model Weights Initialization, first we create the random values for our parameters with normal ditribuition.
        </p>


<div>
    <pre><code>
<span>def</span> <span>init_weight</span>(key, shape, scale=<span>None</span>):
    
    scale = <span>1.0</span> / math.sqrt(shape[0]) <span>if</span> scale <span>is</span> <span>None</span> <span>else</span> scale
    
    <span>return</span> jax.random.normal(key, shape) * scale
</code></pre></div>



<div>
    <p>
        Next, we&#39;ll identify all the learnable parameters of our model(llama3), assign each a unique key to ensure reproducibility, and apply the initialization process to them.
   </p>
   <p>
        Since weights are essentially numbers stored in arrays, we can use dictionaries to manage them as key-value pairs. </p>
    <p>  First we will start with attention module which has four trainable parameters.
   </p> 
</div>

<div>
    <pre><code>
<span>def</span> <span>init_attention_weights</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>head_dim</span> = <span>dim</span> // <span>n_heads</span>
    
    <span>return</span> {
    <span>&#39;wq&#39;</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>dim</span>, <span>n_heads</span> <span>head_dim</span>)),  
    <span>&#39;wk&#39;</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  
    <span>&#39;wv&#39;</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>n_kv_heads</span> <span>head_dim</span>)),  
    <span>&#39;wo&#39;</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>n_heads</span> <span>head_dim</span>, <span>dim</span>))    
    }</code></pre>
    </div>

<p>Next we have our Feed-forward network which has 3 trainable parameters.</p>
<div>
<pre><code>
<span>def</span> <span>init_ffn_weights</span>(key, dim):
    
    keys = jax.random.split(key, <span>3</span>)
    <span>return</span> {
        <span>&#39;w1&#39;</span>: <span>init_weight</span>(keys[<span>0</span>], (dim, <span>4</span> * dim)),  
        <span>&#39;w2&#39;</span>: <span>init_weight</span>(keys[<span>1</span>], (<span>4</span> * dim, dim)),  
        <span>&#39;w3&#39;</span>: <span>init_weight</span>(keys[<span>2</span>], (dim, <span>4</span> * dim))   
    }
</code></pre>
</div>
<p>Then we combine our weights into transformer block, adding two additional parameters for two layers of RMSNorm.</p>

<div>
    <pre><code>
<span>def</span> <span>init_transformer_block</span>(<span>key</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>return</span> {
    <span>&#39;attention&#39;</span>: init_attention_weights(<span>keys</span>[<span>0</span>], <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>),  
    <span>&#39;ffn&#39;</span>: init_ffn_weights(<span>keys</span>[<span>1</span>], <span>dim</span>),  
    <span>&#39;attention_norm&#39;</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>,), scale=<span>1.0</span>),  
    <span>&#39;ffn_norm&#39;</span>: init_weight(<span>keys</span>[<span>3</span>], (<span>dim</span>,), scale=<span>1.0</span>)  
    }</code></pre>
    </div>

    <p>Finally we assemble <strong>Model&#39;s Weights Initialization</strong> in one place.</p>

    
    <div>
        <pre><code>
<span>def</span> <span>init_model_params</span>(<span>key</span>, <span>vocab_size</span>, <span>dim</span>, <span>n_layers</span>, <span>n_heads</span>, <span>n_kv_heads</span>):
    
    <span>keys</span> = jax.random.split(<span>key</span>, <span>4</span>)
    <span>params</span> = {
        <span>&#39;token_embedding&#39;</span>: init_weight(<span>keys</span>[<span>0</span>], (<span>vocab_size</span>, <span>dim</span>)),  
        <span>&#39;norm_f&#39;</span>: init_weight(<span>keys</span>[<span>1</span>], (<span>dim</span>,), scale=<span>1.0</span>),  
        <span>&#39;output&#39;</span>: init_weight(<span>keys</span>[<span>2</span>], (<span>dim</span>, <span>vocab_size</span>))  
    }
    
    <span>block_keys</span> = jax.random.split(<span>keys</span>[<span>3</span>], <span>n_layers</span>)
    <span>params</span>[<span>&#39;blocks&#39;</span>] = [
        init_transformer_block(<span>k</span>, <span>dim</span>, <span>n_heads</span>, <span>n_kv_heads</span>)
        <span>for</span> <span>k</span> <span>in</span> <span>block_keys</span>
    ]
    <span>return</span> <span>params</span></code></pre>
    </div>


<div>
    <h2 id="section-3">Tokenization</h2>

    <p>
     Tokenization means dividing the text into words and subwords (tokens). 
        
      We will be using <bold>Byte Pair Encoding (BPE)</bold> for training our model (BPE was used in training Llama 3).<sup>7</sup>
    I will not build bpe from scratch we will use <bold>tiktoken</bold> library by openai for bpe.</p>
</div>





<div><pre><code><span>import</span> jax.numpy <span>as</span> jnp
<span>import</span><span></span> tiktoken


enc = tiktoken.get_encoding(<span>&#34;gpt2&#34;</span>)


 
<span>with</span> <span>open</span>(<span>&#39;../shakespeare.txt&#39;</span>, <span>&#39;r&#39;</span>) <span>as</span> f:
    text = f.readlines()[<span>0</span>]  


tokens = enc.encode(text)
data = jnp.array(tokens, dtype=jnp.int32)  


decoded_text = enc.decode(tokens)

<span>print</span>(<span>&#34;original Text:&#34;</span>, text.strip())
<span>print</span>(<span>&#34;encoded Tokens:&#34;</span>, tokens)
<span>print</span>(<span>&#34;decoded Text:&#34;</span>, decoded_text)

</code></pre>
</div> 


<div>
    <h2 id="section-4">Embeddings</h2>

    <p>
        We cannot provide tokens directly to a model because tokens are discrete, while neural networks operate on continuous numerical data this is important for performing mathematical operations. Therefore, we use an embedding layer to convert the discrete tokens into a continuous vector space. These embeddings also help encode the semantic and syntactic relationships between tokens.
   </p>
        
   <p><img src="https://www.scd31.com/posts/images/lemb.png" alt="Llama architecture"/>
            <img src="https://www.scd31.com/posts/images/demb.png" alt="Llama architecture"/>
   </p> 

   <p> 
       There are two types of embeddings: static and dynamic. We use dynamic embeddings to train LLMs. Why? Because static embeddings work well for finding similarities between words and representing them in a similar vector space, as seen in the first image. 
   </p>
   <p>However, they suffer from semantic ambiguity, as shown in the second image.  
       This is where <b>Self-Attention</b> comes in, it refines these embeddings to incorporate context. So, we start with random embeddings and update them according to the context.  
   </p>
</div>


<div><pre><code>

h = params[<span>&#34;token_embedding&#34;</span>][inputs]



</code></pre>
</div>


<div>
    <h2 id="section-5">Root Mean Square Layer Normalization</h2>
    <p>
        RMS normalization is an important layer in llama3 models. It helps keep the training stable by making sure that the numbers in the network don’t become too high or too low. This balance is very important, especially in deep networks.
      </p>
        <p><img src="https://www.scd31.com/posts/images/rsmnorm.png" alt="Llama architecture"/>
     <img src="https://www.scd31.com/posts/images/rsmnorm.png" alt="Llama architecture"/>
</p>
</div> 


<div><pre><code>
<span>def</span> <span>rms_norm</span>(x, weight, eps=<span>1e-5</span>):
    
    variance = jnp.mean(jnp.square(x), axis=-<span>1</span>, keepdims=<span>True</span>)                    
    
    <span>return</span> x * weight * jnp.reciprocal(jnp.sqrt(variance + eps))
</code></pre></div>
    

<div>
    <h2 id="section-6">Rotary Positional Encoding</h2>
    <p>
  Transformers don&#39;t naturally know the order of tokens, so we need to add some position info. In llama3 to solve this we have ROPE. It does this by “rotating” the query and key vectors based on their position.<sup>8</sup>
</p>

<p><img src="https://www.scd31.com/posts/images/rope.png" alt="Llama architecture"/>
    <img src="https://www.scd31.com/posts/images/rope2.png" alt="Llama architecture"/>
</p>

<p><strong>How It Works: </strong></p>


<p>

    <strong>Precompute Rotation Factors:</strong>
    First we create a table of rotation factors using a range of frequencies. This means each token gets its own unique rotation angle.
  </p>
</div>


   

<div>
    <pre><code>
<span>def</span> <span>precompute_freqs_cis</span>(<span>dim</span>: <span>int</span>, <span>end</span>: <span>int</span>, <span>theta</span>: <span>float</span> = <span>10000.0</span>):
    
    <span>freqs</span> = <span>1.0</span> / (<span>theta</span> ** (jnp.arange(<span>0</span>, <span>dim</span> // <span>2</span>, dtype=jnp.float32) / <span>dim</span>))
    
    <span>t</span> = jnp.arange(<span>end</span>, dtype=jnp.float32)
    
    <span>freqs</span> = jnp.outer(<span>t</span>, <span>freqs</span>)
    
    <span>return</span> jnp.complex64(jnp.exp(<span>1j</span> * <span>freqs</span>))</code></pre>
</div>


<div>
<p><strong>Apply the Rotation:</strong></p>
<p>
    <strong>Pair Up Features:</strong>  
     we reshape the vectors so that every two numbers form a pair; imagine them as the real and imaginary parts of a complex number.
  </p>
  <p>
    <strong>Rotate:</strong>  
    We multiply these complex numbers by our precomputed rotation factors. This rotates each pair in the complex plane.
  </p>
  <p>
    <strong>Convert Back:</strong>  
    Finally, we split the rotated complex numbers back into their real and imaginary parts to restore the original shape.
  </p>
<p>
    <strong>Math Behind It:</strong>
    For each pair <span></span><span id="MathJax-Element-1-Frame" tabindex="0" data-mathml="&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-1" aria-hidden="true"><span id="MJXc-Node-2"><span id="MJXc-Node-3"><span>(</span></span><span id="MJXc-Node-4"><span><span id="MJXc-Node-5"><span>x</span></span></span><span><span id="MJXc-Node-6"><span id="MJXc-Node-7"><span id="MJXc-Node-8"><span>2</span></span><span id="MJXc-Node-9"><span>i</span></span></span></span></span></span><span id="MJXc-Node-10"><span>,</span></span><span id="MJXc-Node-11"><span><span id="MJXc-Node-12"><span>x</span></span></span><span><span id="MJXc-Node-13"><span id="MJXc-Node-14"><span id="MJXc-Node-15"><span>2</span></span><span id="MJXc-Node-16"><span>i</span></span><span id="MJXc-Node-17"><span>+</span></span><span id="MJXc-Node-18"><span>1</span></span></span></span></span></span><span id="MJXc-Node-19"><span>)</span></span></span></span><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mn>2</mn><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></math></span></span>, the rotation is given by:
    </p>
  
  <!-- Optional: Include MathJax for rendering math formulas -->
  
</div>  
                            
<div>
    <pre><code>
<span>def</span> <span>apply_rotary_emb</span>(<span>xq</span>, <span>xk</span>, <span>freqs_cis</span>):
    
    xq_r, xk_r = jnp.reshape(<span>xq</span>, (*<span>xq</span>.shape[:-1], -<span>1</span>, <span>2</span>)),    
    jnp.reshape(<span>xk</span>, (*<span>xk</span>.shape[:-1], -<span>1</span>, <span>2</span>))
    
    
    xq_complex = jnp.complex64(xq_r[..., <span>0</span>] + <span>1j</span> * xq_r[..., <span>1</span>])
    xk_complex = jnp.complex64(xk_r[..., <span>0</span>] + <span>1j</span> * xk_r[..., <span>1</span>])
    
    
    freqs_cis = jnp.reshape(<span>freqs_cis</span>, (<span>1</span>, <span>freqs_cis</span>.shape[<span>0</span>], <span>1</span>, <span>freqs_cis</span>.shape[<span>1</span>]))
    
    
    xq_out = xq_complex * <span>freqs_cis</span>
    xk_out = xk_complex * <span>freqs_cis</span>
    
    
    <span>xq</span> = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-<span>1</span>).reshape(<span>xq</span>.shape)
    <span>xk</span> = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-<span>1</span>).reshape(<span>xk</span>.shape)
    
    <span>return</span> <span>xq</span>, <span>xk</span>
</code></pre>
</div>

<div>
    <h2 id="section-7">Group-Query Attention</h2>

    <p>
      Now it&#39;s time for attention. Grouped Query Attention (GQA) is an optimized version of Multi-Head Attention that improves efficiency by sharing key and value representations among multiple query heads. This reduces computational overhead and memory usage, enabling faster inference and better scaling for transformer models.
    At it&#39;s core, it&#39;s just self-attention but with some modification.
    </p>

    
    
    <p><strong>Scaled Dot-Product Attention:</strong></p>
    <p>
    <span></span><span><span id="MathJax-Element-4-Frame" tabindex="0" data-mathml="&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;softmax&lt;/mtext&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-121" aria-hidden="true"><span id="MJXc-Node-122"><span id="MJXc-Node-123"><span>A</span></span><span id="MJXc-Node-124"><span>=</span></span><span id="MJXc-Node-125"><span>softmax</span></span><span id="MJXc-Node-126"><span id="MJXc-Node-127"><span>(</span></span><span id="MJXc-Node-128"><span><span><span id="MJXc-Node-129"><span id="MJXc-Node-130"><span>Q</span></span><span id="MJXc-Node-131"><span><span id="MJXc-Node-132"><span>K</span></span></span><span><span id="MJXc-Node-133"><span>T</span></span></span></span></span></span><span><span id="MJXc-Node-134"><span><span><span>√</span></span><span><span id="MJXc-Node-135"><span id="MJXc-Node-136"><span><span id="MJXc-Node-137"><span>d</span></span></span><span><span id="MJXc-Node-138"><span>h</span></span></span></span></span></span></span></span></span><span></span></span><span></span></span><span id="MJXc-Node-139"><span>)</span></span></span><span id="MJXc-Node-140"><span>V</span></span></span></span><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>h</mi></msub></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math></span></span></span>
    </p>





<div><pre><code>
<span>def</span> <span>attention</span>(<span>params, x, mask, freqs_cis, n_heads, n_kv_heads, cache=None, position=0</span>):
    
    <span>B</span>, <span>T</span>, <span>C</span> = <span>x</span>.<span>shape</span>
    <span>head_dim</span> = <span>C</span> // <span>n_heads</span>
    
    
    <span>q</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>&#39;wq&#39;</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_heads</span>, <span>head_dim</span>)
    <span>k</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>&#39;wk&#39;</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    <span>v</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>&#39;wv&#39;</span>]).<span>reshape</span>(<span>B</span>, <span>T</span>, <span>n_kv_heads</span>, <span>head_dim</span>)
    
    
    <span>q</span>, <span>k</span> = <span>apply_rotary_emb</span>(<span>q</span>, <span>k</span>, <span>freqs_cis</span>[<span>position</span>:<span>position</span> + <span>T</span>])
    
    
    <span>if</span> <span>cache</span> <span>is not None</span>:
        <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>])
        <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>])
    <span>new_cache</span> = (<span>k</span>, <span>v</span>)
    
    
    <span>k</span> = <span>repeat_kv</span>(<span>k</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    <span>v</span> = <span>repeat_kv</span>(<span>v</span>, <span>n_heads</span> // <span>n_kv_heads</span>)
    
    
    <span>q</span>, <span>k</span>, <span>v</span> = <span>map</span>(<span>lambda</span> <span>x</span>: <span>x</span>.<span>transpose</span>(0, 2, 1, 3), (<span>q</span>, <span>k</span>, <span>v</span>))
    <span>scores</span> = <span>jnp</span>.<span>matmul</span>(<span>q</span>, <span>k</span>.<span>transpose</span>(0, 1, 3, 2)) / <span>math</span>.<span>sqrt</span>(<span>head_dim</span>)
    
    
    <span>if</span> <span>mask</span> <span>is not None</span>:
        <span>scores</span> = <span>scores</span> + <span>mask</span>[:, :, :<span>T</span>, :<span>T</span>]
    
    
    <span>scores</span> = <span>jax</span>.<span>nn</span>.<span>softmax</span>(<span>scores</span>, <span>axis</span>=-1)
    <span>output</span> = <span>jnp</span>.<span>matmul</span>(<span>scores</span>, <span>v</span>)
    <span>output</span> = <span>output</span>.<span>transpose</span>(0, 2, 1, 3).<span>reshape</span>(<span>B</span>, <span>T</span>, -1)
    
    <span>return</span> <span>jnp</span>.<span>dot</span>(<span>output</span>, <span>params</span>[<span>&#39;wo&#39;</span>]), <span>new_cache</span>
</code></pre></div>

<p><strong>KV-cache : </strong>It stores previously computed key (K) and value (V) tensors from past tokens. We can cache this kv-cache during inference.</p>


<p><img src="https://www.scd31.com/posts/images/lightkv.png" alt="Llama architecture"/>
    <img src="https://www.scd31.com/posts/images/darkkv.png" alt="Llama architecture"/>
</p>

<div><pre><code></code><span>if</span> <span>cache</span> <span>is not None</span>:
    <span>k</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[0], <span>k</span>], <span>axis</span>=-<span>1</span>)  
    <span>v</span> = <span>jnp</span>.<span>concatenate</span>([<span>cache</span>[1], <span>v</span>], <span>axis</span>=-<span>1</span>)  
<span>new_cache</span> = (<span>k</span>, <span>v</span>)  
</pre></div>

</div>



<div>
    <h2 id="section-8">Feed-forward</h2>

    <p>This is simple feed-forward network with <strong>SiLU</strong> activation function. </p>
    

<div>
    <pre><code><span>def</span> <span>feed_forward</span>(<span>params</span>, <span>x</span>):
    
    <span>w3_</span> = <span>jnp</span>.<span>dot</span>(<span>x</span>, <span>params</span>[<span>&#39;w3&#39;</span>])

    </code></pre>
</div>




</div>

<div>
    <h2 id="section-9">Transformer-block</h2>

    <p>
        This is where all the important components come together in the transformer block. We unpack the pre-initialized weights and distribute them to their respective layers. The transformer blocks include attention, normalization, feed-forward processing layers and residual connections.
      </p>
    
</div>

<div>
    <pre><code>
<span>def</span> transformer_block(<span>params</span>, <span>x</span>, <span>mask</span>, <span>freqs_cis</span>, <span>n_heads</span>, <span>n_kv_heads</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    
    <span>attn_output</span>, <span>new_cache</span> = attention(
        <span>params</span>[<span>&#39;attention&#39;</span>],
        rms_norm(<span>x</span>, <span>params</span>[<span>&#39;attention_norm&#39;</span>]),
        <span>mask</span>,
        <span>freqs_cis</span>,
        <span>n_heads</span>,
        <span>n_kv_heads</span>,
        <span>cache</span>,
        <span>position</span>
    )
    
    
    <span>h</span> = <span>x</span> + <span>attn_output</span>
    
    
    <span>ffn_output</span> = feed_forward(<span>params</span>[<span>&#39;ffn&#39;</span>], rms_norm(<span>h</span>, <span>params</span>[<span>&#39;ffn_norm&#39;</span>]))
    
    
    <span>out</span> = <span>h</span> + <span>ffn_output</span>
    
    <span>return</span> <span>out</span>, <span>new_cache</span></code></pre>
</div>

<div>
    <h2 id="section-10">Forward-Pass</h2>

    <p> The forward pass takes your data through the entire model from converting input tokens into embeddings, through a series of transformer blocks, and finally to the output layer. In other words, it connects all the layers (embedding, transformers, and output) to produce the final predictions.</p></div>




<div>
    <pre><code>
<span>def</span> <span>model_forward</span>(<span>params</span>, <span>inputs</span>, <span>config</span>, <span>cache</span>=<span>None</span>, <span>position</span>=<span>0</span>):
    
    <span>B</span>, <span>T</span> = <span>inputs</span>.shape
    
    
    <span>h</span> = <span>params</span>[<span>&#39;token_embedding&#39;</span>][<span>inputs</span>]
    
    
    <span>freqs_cis</span> = <span>precompute_freqs_cis</span>(<span>config</span>.<span>dim</span> // <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>max_seq_len</span>)
    
    
    <span>mask</span> = <span>jnp</span>.<span>tril</span>(<span>jnp</span>.<span>ones</span>((<span>config</span>.<span>max_seq_len</span>, <span>config</span>.<span>max_seq_len</span>)))
    <span>mask</span> = <span>jnp</span>.<span>where</span>(<span>mask</span> == <span>0</span>, -<span>1e9</span>, <span>0.0</span>)
    <span>mask</span> = <span>mask</span>.<span>astype</span>(<span>h</span>.<span>dtype</span>)
    <span>mask</span> = <span>mask</span>[<span>None</span>, <span>None</span>, :, :]

    
    <span>new_caches</span> = []
    <span>for</span> <span>i</span>, <span>block</span> <span>in</span> <span>enumerate</span>(<span>params</span>[<span>&#39;blocks&#39;</span>]):
        <span>layer_cache</span> = <span>cache</span>[<span>i</span>] <span>if</span> <span>cache</span> <span>is not</span> <span>None</span> <span>else</span> <span>None</span>
        <span>h</span>, <span>layer_cache</span> = <span>transformer_block</span>(
            <span>block</span>, <span>h</span>, <span>mask</span>, <span>freqs_cis</span>,
            <span>config</span>.<span>n_heads</span>, <span>config</span>.<span>n_kv_heads</span>,
            <span>layer_cache</span>, <span>position</span>, training=<span>False</span>)
        <span>new_caches</span>.<span>append</span>(<span>layer_cache</span>)

    
    <span>h</span> = <span>rms_norm</span>(<span>h</span>, <span>params</span>[<span>&#39;norm_f&#39;</span>])
    <span>logits</span> = <span>jnp</span>.<span>dot</span>(<span>h</span>, <span>params</span>[<span>&#39;output&#39;</span>])
    
    <span>return</span> <span>logits</span>, <span>new_caches</span></code></pre>
</div>





          <div>
              <h2 id="section-11">Dataset</h2>
          
              <p>Now the model part is complete so its time to train our model on shakespeare dataset. First we will read our data from <strong>.txt</strong> file then we will encode our data with bpe and then convert it into jax array.</p>
          </div>

<div>
<pre><code>
<span>enc</span> = <span>tiktoken.get_encoding</span>(<span>&#34;gpt2&#34;</span>)


<span>with</span> <span>open</span>(<span>&#39;shakespeare.txt&#39;</span>, <span>&#39;r&#39;</span>) <span>as</span> <span>f</span>:
    <span>text</span> = <span>f.read</span>()


<span>tokens</span> = <span>enc.encode</span>(<span>text</span>)

<span>data</span> = <span>jnp.array</span>(<span>tokens</span>)
</code></pre> 
</div>

<div>
    <h3 id="section-1">Get Batches</h3>
    <p>The get_batch function creates training batches from our Shakespeare dataset. We need to feed our model with chunks of data. For each batch, we randomly select starting positions in the text, this way the model sees a variety of contexts. </p>
    <p>Now, here&#39;s where JAX&#39;s cool vmap feature comes into play. Instead of writing a loop to extract each chunk, we use vmap to automate.</p>
    <p><strong>How does it work ?</strong></p>
    <p> vmap is like a vectorized loop; it takes a function that processes a single index (using <strong>lax.dynamic_slice </strong> to get a sequence of tokens) and applies it to every element in our array of indices. This means our input sequences (x) and corresponding target sequences (y, which are shifted by one token for next-word prediction) are created in one go.</p>

<div>
    <pre><code><span>def</span> <span>get_batch</span>(<span>key</span>, <span>data</span>, <span>batch_size</span>, <span>seq_len</span>):
    
    <span>ix</span> = <span>random</span>.<span>randint</span>(<span>key</span>, (<span>batch_size</span>,), <span>0</span>, <span>len</span>(<span>data</span>) - <span>seq_len</span>)
    
    
    <span>x</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    <span>y</span> = <span>vmap</span>(<span>lambda</span> <span>i</span>: <span>lax</span>.<span>dynamic_slice</span>(<span>data</span>, (<span>i</span> + <span>1</span>,), (<span>seq_len</span>,)))(<span>ix</span>)
    
    <span>return</span> <span>x</span>, <span>y</span>
</code></pre>
</div>
    

   



</div>



            <div>
                                    <h2 id="section-13">Loss Function</h2>
                                    <p>This function computes the cross-entropy loss for a batch during training. It first performs a forward pass using the model to generate logits for the input data. Then, it reshapes both the logits and targets to merge the batch and sequence dimensions. After applying the log softmax to the logits, it extracts the log probabilities corresponding to the correct target tokens and computes their negative mean as the final loss value.</p>
            
                                    
            
            <p>The cross-entropy loss is defined as:</p>
            <p>
             <span></span><span><span id="MathJax-Element-5-Frame" tabindex="0" data-mathml="&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mi class=&#34;MJX-tex-caligraphic&#34; mathvariant=&#34;script&#34;&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mfrac&gt;&lt;munderover&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/munderover&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-141" aria-hidden="true"><span id="MJXc-Node-142"><span id="MJXc-Node-143"><span id="MJXc-Node-144"><span id="MJXc-Node-145"><span>L</span></span></span></span><span id="MJXc-Node-146"><span>=</span></span><span id="MJXc-Node-147"><span>−</span></span><span id="MJXc-Node-148"><span><span><span id="MJXc-Node-149"><span>1</span></span></span><span><span id="MJXc-Node-150"><span>N</span></span></span><span></span></span><span></span></span><span id="MJXc-Node-151"><span><span><span><span><span><span id="MJXc-Node-158"><span id="MJXc-Node-159"><span id="MJXc-Node-160"><span>N</span></span></span></span></span><span><span id="MJXc-Node-152"><span>∑</span></span></span></span></span></span><span><span><span id="MJXc-Node-153"><span id="MJXc-Node-154"><span id="MJXc-Node-155"><span>i</span></span><span id="MJXc-Node-156"><span>=</span></span><span id="MJXc-Node-157"><span>1</span></span></span></span></span></span></span></span><span id="MJXc-Node-161"><span>log</span></span><span id="MJXc-Node-162"><span></span></span><span id="MJXc-Node-163"><span>P</span></span><span id="MJXc-Node-164"><span>(</span></span><span id="MJXc-Node-165"><span><span id="MJXc-Node-166"><span>y</span></span></span><span><span id="MJXc-Node-167"><span>i</span></span></span></span><span id="MJXc-Node-168"><span>)</span></span></span></span><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi mathvariant="script">L</mi></mrow><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span></span>
            </p>
            
            <p>Where:</p>
            <div>
               
            <ul>
              <li><span></span><span id="MathJax-Element-6-Frame" tabindex="0" data-mathml="&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-169" aria-hidden="true"><span id="MJXc-Node-170"><span id="MJXc-Node-171"><span>P</span></span><span id="MJXc-Node-172"><span>(</span></span><span id="MJXc-Node-173"><span><span id="MJXc-Node-174"><span>y</span></span></span><span><span id="MJXc-Node-175"><span>i</span></span></span></span><span id="MJXc-Node-176"><span>)</span></span></span></span><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span> is the probability of the correct class, calculated using the softmax function:</li>
            </ul>
            </div>
            
            <p>
                <span></span><span><span id="MathJax-Element-7-Frame" tabindex="0" data-mathml="&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34; display=&#34;block&#34;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&#34;false&#34;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&#34;false&#34;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&#34;MJX-TeXAtom-ORD&#34;&gt;&lt;msub&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-177" aria-hidden="true"><span id="MJXc-Node-178"><span id="MJXc-Node-179"><span>P</span></span><span id="MJXc-Node-180"><span>(</span></span><span id="MJXc-Node-181"><span><span id="MJXc-Node-182"><span>y</span></span></span><span><span id="MJXc-Node-183"><span>i</span></span></span></span><span id="MJXc-Node-184"><span>)</span></span><span id="MJXc-Node-185"><span>=</span></span><span id="MJXc-Node-186"><span><span><span id="MJXc-Node-187"><span><span id="MJXc-Node-188"><span>e</span></span></span><span><span id="MJXc-Node-189"><span id="MJXc-Node-190"><span id="MJXc-Node-191"><span><span id="MJXc-Node-192"><span>z</span></span></span><span><span id="MJXc-Node-193"><span>i</span></span></span></span></span></span></span></span></span><span><span id="MJXc-Node-194"><span id="MJXc-Node-195"><span><span id="MJXc-Node-196"><span>∑</span></span></span><span><span id="MJXc-Node-197"><span id="MJXc-Node-198"><span id="MJXc-Node-199"><span>j</span></span></span></span></span></span><span id="MJXc-Node-200"><span><span id="MJXc-Node-201"><span>e</span></span></span><span><span id="MJXc-Node-202"><span id="MJXc-Node-203"><span id="MJXc-Node-204"><span><span id="MJXc-Node-205"><span>z</span></span></span><span><span id="MJXc-Node-206"><span>j</span></span></span></span></span></span></span></span></span></span><span></span></span><span></span></span></span></span><span role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow></msup><mrow><munder><mo>∑</mo><mrow><mi>j</mi></mrow></munder><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></math></span></span></span>
            </p>


<div>
    <pre><code>
<span>def</span> <span>compute_loss</span>(<span>params</span>, <span>batch</span>):
    
    <span>inputs</span>, <span>targets</span> = <span>batch</span>
    
    <span>logits</span>, = model_forward(<span>params</span>, <span>inputs</span>, <span>config</span>)
    
    <span>logits</span> = <span>logits</span>.reshape(-<span>1</span>, <span>config</span>.vocab_size)
    <span>targets</span> = <span>targets</span>.reshape(-<span>1</span>)
    
    <span>loss</span> = -jnp.mean(jnp.take_along_axis(jax.nn.log_softmax(<span>logits</span>),
    <span>targets</span>[:, <span>None</span>], axis=<span>1</span>))
    <span>return</span> <span>loss</span></code></pre>
    </div>

       
       <div>
       <h2 id="section-14">Update function</h2>
       
       <p>Now we need to write a function to update our weights. For simplicity, we&#39;re using Stochastic Gradient Descent (SGD) here, though you can also use Adam or AdamW for faster convergence.
       </p>
       
       <p>In the code, you&#39;ll notice the <strong>@jax.jit</strong> decorator. This is one of the features that sets <strong>jax</strong> apart. JIT (Just-In-Time) compilation speeds up execution by converting your Python code into optimized machine code.</p>
        
       <p><strong>How does it work ?</strong></p>

       <p>When you decorate a function with JAX’s jit, it changes how the function executes. Normally, when you call a function, Python runs it line by line. For example, if you have:
       </p>
<div> <pre><code><span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>&#34;HI jiited&#34;</span>) 
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>


    <p>Every time you call sqr, it prints &#34;HI jiited&#34; and then returns the square of the number. However, when you add the @jax.jit decorator:</p>


<div> <pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>sqr</span>(<span>x</span>): 
    <span>print</span>(<span>&#34;HI jiited&#34;</span>)  
    <span>return</span> <span>x</span> * <span>x</span>

<span>print</span>(<span>sqr</span>(<span>2</span>)) 
<span>print</span>(<span>sqr</span>(<span>3</span>)) 
<span>print</span>(<span>sqr</span>(<span>4</span>))</code></pre>
</div>

<p><strong>Jax </strong>first traces your function to build an optimized computation graph. This tracing happens the first time the function is called and converts the Python code into machine code.</p>

<p>Because of this tracing, any side effects like the print statement; are only executed during the initial tracing. Once the function is compiled, other remaining    calls use the compiled version, and you might not see the print output every time.</p>








<div>
<pre><code><span>@</span><span>jax</span>.<span>jit</span>
<span>def</span> <span>update_step</span>(<span>params</span>, <span>batch</span>):
    
    
    <span>loss</span>, <span>grads</span> = <span>jax.value_and_grad</span>(<span>compute_loss</span>)(<span>params</span>, <span>batch</span>)

    
    
    
    <span>params</span> = <span>jax.tree.map</span>(
        <span>lambda</span> <span>p</span>, <span>g</span>: <span>p</span> - <span>config.learning_rate</span> * <span>g</span>,
        <span>params</span>,
        <span>grads</span>
    )

    
    <span>return</span> <span>params</span>, <span>loss</span></code></pre>
</div>
<p>In our <strong>update_step</strong> function, <strong>@jax.jit</strong> compiles the code. The function computes loss and gradients simultaneously with <strong>jax.value_and_grad</strong>, updates the parameters using gradient descent with help of <strong>jax.tree.map</strong>, and returns the updated parameters and loss.</p>

</div>


             
              


              <div>
                 <h2 id="section-15">Trainig-Loop</h2>
                 <p>Finally, its time to train our 2 million parameter model on shakespeare dataset. 
                    We first prepare batches using the <strong>get_batch</strong> which splits our data into batches so we can train faster with 
                     our limited compute.
                 </p>
             </div>

<div>
 <pre><code><span>for</span> <span>epoch</span> <span>in</span> <span>range</span>(<span>num_epochs</span>):
 
   
   <span>epoch_loss</span> = <span>0.0</span>

   <span>for</span> <span>step</span> <span>in</span> <span>range</span>(<span>steps_per_epoch</span>):
   
      
      <span>key</span>, <span>batch_key</span> = <span>random.split</span>(<span>key</span>)
      
      
      <span>batch</span> = <span>get_batch</span>(<span>batch_key</span>, <span>data</span>, <span>config.batch_size</span>, <span>config.max_seq_len</span>)
      
      
      <span>params_state</span>, <span>loss</span> = <span>update_step</span>(<span>params_state</span>, <span>batch</span>)
     
      
      <span>epoch_loss</span> += <span>loss</span>
      
   
      <span>if</span> <span>step</span> % <span>100</span> == <span>0</span>:
            <span>print</span>(<span>f&#34;epoch {epoch + 1}, step {step}/{steps_per_epoch}: loss = {loss:.4f}&#34;</span>)
      

  <span>avg_epoch_loss</span> = <span>epoch_loss</span> / <span>steps_per_epoch</span>
     
 
  <span>epoch_losses</span>.<span>append</span>(<span>avg_epoch_loss</span>)
      
  
  <span>print</span>(<span>f&#34;\nepoch {epoch + 1} | average loss: {avg_epoch_loss:.4f}&#34;</span>)</code></pre>


</div>


<p><img src="https://www.scd31.com/posts/images/train.png" alt="Llama architecture"/>
    <img src="https://www.scd31.com/posts/images/train.png" alt="Llama architecture"/>
</p>
  




               




<hr/>
<div>
    <h3>Thank you for reading this far !! </h3>
    <h3>You can support me :</h3>
    
</div>












    



</div></article>

</div></div>
  </body>
</html>
