<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jepsen.io/analyses/tigerbeetle-0.16.11">Original</a>
    <h1>Jepsen: TigerBeetle 0.16.11</h1>
    
    <div id="readability-page-1" class="page"><p><a href="https://tigerbeetle.com">TigerBeetle</a> is a distributed OLTP database oriented towards financial transactions. We tested TigerBeetle 0.16.11 through 0.16.30. We discovered seven client and server crashes, including a segfault on client close and several panics during server upgrades. Single-node failures could cause significantly elevated latencies for the duration of the fault, and requests were intentionally retried forever, which complicates error handling. We found only two safety issues: missing results for queries with multiple predicates, and a minor issue with a debugging API returning incorrect timestamps. TigerBeetle offered exceptional resilience to disk corruption, including damage to every replica’s files. However, it lacked a way to handle the total loss of a node’s data. As of version 0.16.30, TigerBeetle appeared to meet its promise of Strong Serializability. As of 0.16.45, TigerBeetle had addressed every issue we found, with the exception of indefinite retries. TigerBeetle has written a <a href="https://tigerbeetle.com/blog/2025-06-06-fuzzer-blind-spots-meet-jepsen/">companion blog post</a> to this work. This report was funded by TigerBeetle, Inc., and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>

<p>TigerBeetle is an <a href="https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles">Online Transactional Processing (OLTP)</a> database built for double-entry accounting with a strong emphasis on safety and speed. It builds on the <a href="https://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication (VR)</a> consensus protocol to offer <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializable</a> consistency. Unlike general-purpose databases, TigerBeetle stores only accounts and transfers between them. This data model is well-suited for financial transactions, inventory, ticketing, or utility metering. To store other kinds of information, users typically pair TigerBeetle with other databases, linking them through user-defined identifiers.</p>
<p>TigerBeetle optimizes for <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/oltp#business-transactions-dont-shard-well">high-contention and high-throughput workloads</a>, such as central bank switches or brokerages. A central bank exchange might have only a half-dozen to a few hundred account records—one for each partner bank—and process <a href="https://www.bcb.gov.br/en/statistics/graphicdetail/graficospix/PixTransactionsAmount">hundreds of millions</a> of <a href="https://www.npci.org.in/what-we-do/upi/product-statistics">transactions per day</a> between 647 banks. A large brokerage, after the trading day closes, might need to settle the entire day’s trades as quickly as possible.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> These trades also tend to be concentrated on a small number of popular stocks. Under high contention, per-object concurrency control mechanisms can be the limiting factor in throughput. Instead, TigerBeetle funnels all writes through <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/performance#single-core-by-design">a single core on the primary VR node</a>. This limits throughput to whatever a single node can execute: TigerBeetle is firmly scale-up, not scale-out. To make that single node as fast as possible, TigerBeetle makes extensive use of batching, IO parallelization, a fixed schema, and hardware-friendly optimizations—such as fixed-size, cache-aligned data structures.</p>
<p>Refreshingly, TigerBeetle stresses fault tolerance in their marketing and documentation. They offer <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/safety">explicit models</a> for memory, process, clock, storage, and network faults. ECC RAM is assumed to be correct. Processes may pause or crash. Clocks may jump forward and backward in time. Disks are assumed to not only fail completely, but to tear individual writes or corrupt data. Networks may delay, drop, duplicate, misdirect, and corrupt messages. To mitigate these faults, TigerBeetle combines Viewstamped Replication with techniques from <a href="https://www.usenix.org/system/files/conference/fast18/fast18-alagappan.pdf">Protocol-Aware Recovery</a>, uses extensive checksums stored separately from data blocks, and for critical data, writes and reads multiple copies. TigerBeetle also makes extensive use of runtime correctness assertions to identify and limit the damage from faults and bugs alike.</p>
<p>Unlike most distributed systems, TigerBeetle claims to keep running without data loss <a href="https://web.archive.org/web/20241213103525/docs.tigerbeetle.com/about/safety#durability">if even a single replica retains a copy</a> of a record:</p>
<blockquote>
<p>A record would need to get corrupted on all replicas in a cluster to get lost, and even in that case the system would safely halt.</p>
</blockquote>
<p>To test safety under faults, TigerBeetle employs <a href="https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html">deterministic simulation testing</a>: tests which perform reproducible, pseudo-random operations against the system and ensure that some property holds.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> The <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/vopr/">Viewstamped Operation Replicator (VOPR)</a> test simulates an entire TigerBeetle cluster, including clock, disk, and network interfaces. It simulates clock skew, corrupts reads and writes, loses and reorders network messages, and so on. There are other simulation tests which stress specific subsystems, as well as <a href="https://github.com/tigerbeetle/tigerbeetle/blob/092713075e797e633b6c306fb6d32b2523475f30/docs/internals/HACKING.md">a variety of more traditional integration and unit tests</a>.</p>
<p>TigerBeetle also offers a noteworthy approach to upgrades. Each TigerBeetle binary includes the code not just for that particular version, but several previous versions. For example, the 0.16.21 binary can run 0.16.17, 0.16.18, and so on through 0.16.21. To upgrade, one simply <a href="https://docs.tigerbeetle.com/operating/upgrading/">replaces the binary on disk</a>. TigerBeetle loads the new binary, but continues running with the current version. It then coordinates across the cluster to smoothly roll out each successive version, until all nodes are running the latest version available. This approach does not require operators to carefully sequence the upgrade process. Instead, upgrades are performed automatically, and coupled to the replicated state machine. This also allows TigerBeetle to ensure that an operation which commits on version <span><em>x</em></span> will never commit on any other version—guarding against state divergence.</p>
<h2 data-number="1.1" id="time"> Time</h2>
<p>TigerBeetle defines an <a href="https://docs.tigerbeetle.com/coding/time/">explicit model of time</a>. Viewstamped Replication forms a totally ordered sequence of state transitions, and its view and op numbers can be used as a totally ordered logical clock. Financial systems usually prefer wall clocks, so most TigerBeetle timestamps are in “physical time,” which, like <a href="https://cse.buffalo.edu/tech-reports/2014-04.pdf">Hybrid Logical Clocks</a>, approximate POSIX time with stronger ordering guarantees. Specifically, TigerBeetle leaders <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/vsr/clock.zig#L23">collect POSIX timestamps</a> from all replicas<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and try to find a time which falls within a reasonable margin of error across a quorum of nodes.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> Those timestamps are incorporated into the VR-replicated state machine, and constrained to be strictly monotonic. When no quorum of clocks falls within a <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/config.zig#L122">twenty-second window</a> for longer than sixty seconds, the cluster refuses requests until clocks come back in sync.</p>
<p>As of October 2024, TigerBeetle’s documentation <a href="http://web.archive.org/web/20240823231836/https://docs.tigerbeetle.com/reference/account/#timestamp">described TigerBeetle timestamps</a> as “nanoseconds since UNIX epoch”. This is <a href="https://aphyr.com/posts/378-seconds-since-the-epoch">not quite true</a>: POSIX time is presently twenty-seven seconds less than the actual number of seconds since the epoch. During leap seconds or other negative time adjustments, TigerBeetle’s clock slows to a crawl until values from <code>CLOCK_REALTIME</code> catch up.</p>
<h2 data-number="1.2" id="data-model"> Data Model</h2>
<p><a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/coding/#data-model-overview">TigerBeetle’s data model</a> is specifically intended for <a href="https://en.wikipedia.org/wiki/Double-entry_bookkeeping">double-entry bookkeeping</a>. It has no way to represent arbitrary rows, objects, graphs, blobs, and so on.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> Instead TigerBeetle stores two types of data: <em>accounts</em>, and <em>transfers</em> between them. All fields are fixed-size,<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> and numbers are generally unsigned integers. All values are, with limited exceptions, immutable.</p>
<p>An <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/account/">account</a> represents an entity which sends and receives something. For example, a “Gross Revenues” account might accrue dollars, “Meadow Lake Wind Farm” might generate kilowatt-hours of electricity, and “Beyoncé” would obviously hold an ever-growing number of Grammy awards. Accounts are uniquely identified by a user-defined 128-bit <code>id</code>, a <code>ledger</code> which determines which accounts can interact with each other, a bitfield of <code>flags</code> controlling <a href="https://docs.tigerbeetle.com/reference/account/#flags">various behaviors</a>, a creation <code>timestamp</code>, a user-defined <code>code</code>, and three custom fields of varying sizes: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. There are also four derived fields which represent the current sum of transfers into (<em>credits</em>) and out of (<em>debits</em>) the account: <code>debits_pending</code>, <code>debits_posted</code>, <code>credits_pending</code>, and <code>credits_posted</code>.</p>
<p>A <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/transfer/">transfer</a> is an immutable record which represents an integer quantity moving from one account to another. Like accounts, transfers have a unique, user-specified, 128-bit <code>id</code>, a <code>code</code>, a <code>ledger</code>, a bitfield of <a href="https://docs.tigerbeetle.com/reference/transfer/#flags"><code>flags</code></a>, and three custom fields: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. Transfers also include the <code>debit_account_id</code> and <code>credit_account_id</code> of the two accounts involved, and the integer <code>amount</code> transferred between them.</p>
<p>A single-phase transfer takes effect, or <em>posts</em>, immediately. A transfer can also be executed in <a href="https://docs.tigerbeetle.com/coding/two-phase-transfers/">two phases</a>, represented by two transfer records. The first phase, <em>pending</em>, reserves capacity in the debit and credit account for the given amount. The second phase posts the pending transfer, transferring at most the pending amount. Pending transfers can be explicitly <em>voided</em>, which cancels them, or automatically <em>expire</em>, which is controlled by a <code>timeout</code> field. Posting and voiding transfers use a flag and a <code>pending_id</code> field to indicate which pending transfer they resolve. Pending transfers resolve <a href="https://web.archive.org/web/20240828091507/https://docs.tigerbeetle.com/reference/transfer">at most once</a>.</p>
<p>A special kind of transfer can <a href="https://web.archive.org/web/20250128232234/https://docs.tigerbeetle.com/coding/recipes/close-account/"><em>close</em></a> an account, preventing it from participating in later transfers. Closing transfers are always pending. Account closures can be “un-done” by voiding the closing transfer.</p>
<p>Accounts are immutable with five exceptions: a <code>closed</code> flag, which is derived from closing and re-opening transfers, and four balance fields, which are derived from the sum of pending and posted transfers. Transfers are always immutable. One alters or un-does a transfer by creating a new, compensating transfer.</p>
<h2 data-number="1.3" id="operations"> Operations</h2>
<p>TigerBeetle clients make <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/"><em>requests</em></a> to update or query database state. Each request represents a single kind of logical operation, like <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts">creating accounts</a> or <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers">querying transfers</a>. Requests and their corresponding responses usually involve a batch of <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#batching-events">up to</a> 8190 <em>events</em>, all of the same type. For example, a <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers">create-transfers</a> request includes a batch of transfers to create, and logically<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> returns a batch of results, one per transfer. Read operations generally take a list of IDs, or a query predicate, and return a batch of matching records.</p>
<p>From a database perspective, each TigerBeetle request is a single transaction: an ordered group of micro-operations which execute atomically. Events within a request <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">are executed in order</a>. Each event observes a unique, strictly increasing timestamp.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> There are no interactive transactions, mixed read-write transactions, or indeed any kind of multi-request transactions.</p>
<p>TigerBeetle’s <a href="https://tigerbeetle.com/">home page</a> promises <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>, and the documentation is consistent with this promise. Requests execute <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">at most once</a>, and events within a request “do not interleave with events from other requests.” TigerBeetle also promises several <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/sessions/#guarantees">session safety properties</a>: a session “reads its own writes” and “observes writes in the order that they occur on the cluster.” These are guaranteed by <a href="https://cs.uwaterloo.ca/~kmsalem/pubs/DaudjeeICDE04.pdf">Strong Session Serializability</a>, which in turn is implied by Strong Serializability.</p>
<p>There are two kinds of write requests. The <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts"><code>create_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers"><code>create_transfers</code></a> requests add a series of accounts or transfers to the database. There are also six read requests. Users look up specific accounts or transfers by ID using <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_accounts"><code>lookup_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_transfers"><code>lookup_transfers</code></a>. To query accounts or transfers matching a predicate, one uses <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_accounts"><code>query_accounts</code></a>, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers"><code>query_transfers</code></a>, and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_transfers"><code>get_account_transfers</code></a>. Finally, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_balances"><code>get_account_balances</code></a> reads historical balance information.</p>
<p>Requests are atomic in the sense that either all or none of a request’s events execute. However, specific events in a committed request can <em>logically</em> fail, returning error codes. For example, a <code>create_transfers</code> request might try to create two transfers, the first of which fails due to a balance constraint, and the second of which succeeds. This request can still commit, even though only one of its two transfers was added to the database.</p>
<p>To make one event contingent on another, TigerBeetle offers a sort of logical sub-transaction within a request, called a <em>chain</em>. Each event in a chain succeeds if and only if all others succeed. This allows users to express <a href="https://docs.tigerbeetle.com/coding/recipes/multi-debit-credit-transfers">complex, multi-step transfers</a> that succeed or fail atomically.</p>

<p>We built a <a href="https://github.com/jepsen-io/tigerbeetle/">test suite</a> for TigerBeetle using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>, which combines property-based testing with fault injection. We tested TigerBeetle versions 0.16.11 through 0.16.30, including several development builds. Our tests ran on clusters of three to six<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> Debian nodes, both in LXC containers and on EC2 VMs.</p>
<p>TigerBeetle offers only a “smart” client which connects to every node in the cluster. These clients can mask concurrency errors by routing all requests to a single server.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> In addition to testing this smart-client behavior, we also ran tests with each client restricted to a single node, by passing invalid addresses for the other nodes. Since TigerBeetle followers do not proxy <code>register</code> requests to the leader, most clients spend their time attempting futile requests against inexorable followers. This is fine for safety testing, so long as they time out quickly enough to keep up with leader elections.</p>
<p>TigerBeetle’s domain-specific data model poses a challenge for validation. Jepsen has <a href="https://github.com/jepsen-io/elle">well-established tricks</a> for checking Strict Serializability of lists, sets, and registers, but TigerBeetle has no direct analogue to these structures.</p>
<p>As in our 2022 work on <a href="https://jepsen.io/analyses/radix-dlt-1.0-beta.35.1">Radix DLT</a>, we considered interpreting each account as a list of transfers. Creating a transfer would be interpreted as a pair of appends to the debit and credit accounts. A balance read could, with the help of a constraint solver, often be mapped to a read of a specific set of transfers. However, this leaves account creation and most queries untested. It also makes it difficult to validate the rich semantics of TigerBeetle transfers. For example, TigerBeetle supports balancing transfers, which adjusts the amount of a transfer to ensure the debit (and/or credit) account maintains certain invariants, like a positive or negative balance.</p>
<p>Instead, we decided to take advantage of TigerBeetle’s explicit total order of transactions. In broad strokes, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj">our checker</a> splits the problem into two interlocking parts. First, we check that the apparent timestamps of operations are Strong Serializable. Second, we check that the <em>semantics</em> of those operations, when executed in timestamp order, make sense.</p>
<h2 data-number="2.1" id="timestamp-order"> Timestamp Order</h2>
<p>Verifying timestamp order was relatively straightforward. TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new client API</a> which allowed us to read the timestamp assigned to every successful request. For operations which failed or timed out, we inferred their timestamps from the timestamp assigned to any of their effects. For instance, if the creation of account 3 timed out, but we later read account 3 with timestamp 72, we assumed that write executed at timestamp 72. TigerBeetle’s promise that timestamps are strictly ordered both within and between requests means that this inference should yield an order compatible with the request timestamps. We ignored any failed reads, whether definite or indefinite—this is safe as reads have no semantic side effects.</p>
<p>Timestamp inference required that we eventually observe the effect of every attempted write. We divided the test into two phases: a <em>main phase</em> involving writes and reads, and a <em>final read phase</em> where we tried to read any unseen writes until TigerBeetle definitively responded “yes, this write exists”, or “no, it does not (yet) exist.” Our goal was to infer exactly which operations executed during the main phase, and the timestamps of those operations.<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>If a write was observed, and its inferred timestamp fell before the timestamp of the last successfully acknowledged write,<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj#L269-L310">inferred that it executed</a> during the main phase. If a write was <em>not</em> observed, we assumed that it did not execute during the main phase. There are two possible scenarios:</p>
<ul>
<li><p>TigerBeetle is Strong Serializable. If the write had executed during the main phase, Strong Serializability would have ensured its visibility during the final read phase. Our inference is correct.</p></li>
<li><p>TigerBeetle is not Strong Serializable. If the write did not execute during the main phase, our inference is correct. If it <em>did</em> execute during the main phase, our inference is incorrect. We might encounter false positives or false negatives—but in either case, TigerBeetle has failed to maintain a promised invariant.</p></li>
</ul>
<p>If TigerBeetle were Strong Serializable, our checker would not falsely report an error. If TigerBeetle were to (e.g.) exhibit a <a href="https://jepsen.io/consistency/phenomena/stale-read">stale read</a> or another violation of Strong Serializability, we might detect it indirectly. It could, for example, manifest as a model-checker error on a different operation much earlier in the history. This non-locality is not ideal, but we found it an acceptable tradeoff in exchange for excellent coverage.</p>
<p>Having inferred a set of operations executed during the main phase, and timestamps for each, we used <a href="https://github.com/jepsen-io/elle">Elle</a> to construct a graph over operations. We linked operations by real-time edges when operation A ended before operation B began.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> We also linked operations in ascending timestamp order. A violation of Strong Serializability (as far as timestamps were concerned) would manifest as a cycle in this graph. Elle checks for cycles in roughly linear time, and constructs compact exemplars of consistency violations.</p>
<h2 data-number="2.2" id="model-checking"> Model Checking</h2>
<p>To verify that the semantics of TigerBeetle’s requests and responses were correct, we built a detailed, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">single-threaded model</a> of the TigerBeetle state machine based on the documentation. This model is essentially a datatype with an initial state <span><em>i</em><em>n</em><em>i</em><em>t</em></span> and a transition function <span><em>s</em><em>t</em><em>e</em><em>p</em>(<em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>, <em>i</em><em>n</em><em>v</em><em>o</em><em>k</em><em>e</em>, <em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>t</em><em>e</em>) → <em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>′</span>, which takes a state, the invocation of a request, and the completion of that request, and returns a new state. Illegal transitions (for instance, a read whose completion value does not agree with the state) returned a special <em>invalid</em> state. Given the inferred list of timestamp-sorted operations from the main phase, we stepped through each operation in order. Any invalid state was reported as an error.</p>
<p>We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">modeled the state</a> as an immutable data structure including the current timestamps,<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> maps of IDs to accounts and transfers, transient errors<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, a set of indices to support efficient querying, and a few internal statistics. To model the flow of clocks, we provided each state with a pre-computed map of IDs to timestamps, derived from the reads performed during the test. Whenever one of those IDs was created, we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">advanced the clock</a> to that timestamp.</p>
<p>The state machine is surprisingly complex, involving over 1,600 lines of Clojure and an <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">extensive test suite</a>. A <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">broad array of error conditions</a> had to be handled, including duplicate IDs, non-monotonic timestamps, balance constraints, incompatible flags, and more. Linked chains of events required <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj#L392-L402">speculative execution and rollback</a> of the state—made simpler by our pure, functional approach. We made extensive use of Zach Tellman’s <a href="https://github.com/lacuna/bifurcan">Bifurcan</a>, a thoroughly tested library of high performance persistent data structures.</p>
<p>Modeling the full state machine takes time, but allows extraordinarily detailed verification of correctness. To make checking computationally tractable, typical Jepsen tests use only a handful of carefully selected data types and operations on them. The implicit assumption is that if the database’s concurrency control protocol handles that selected example correctly, it is likely correct for other workloads as well. Modeling the state machine in detail allowed us to check almost<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> every<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> operation TigerBeetle can perform. We verified that observed results of queries matched exactly, down to specific error codes. As discussed in this report, this approach found bugs we would have otherwise missed.</p>
<h2 data-number="2.3" id="generating-operations"> Generating Operations</h2>
<p>The downside of testing so much of TigerBeetle’s state machine is that we must then generate requests which <em>exercise</em> it. Generating syntactically valid requests is easy, but generating requests which often succeed, or queries which return non-empty results, is surprisingly hard.</p>
<p>Our <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/workload/generator.clj">generator</a> maintained extensive in-memory state throughout each test, including probabilistic models of which account and transfer IDs were likely to exist, which transfers were likely pending, what timestamps were likely extant, and what each worker process was currently doing. The generator updated this state with each operation’s invocation and completion.</p>
<p>We selected Zipfian distributed IDs, ledgers, codes, and so on, ensuring a mix of very hot and very cool objects. We used a broad set of parameters to guide stochastic choices of request types, account and transfer IDs, chain lengths, flags, queries, and probabilistic state updates. These parameters were carefully tuned across a variety of concurrencies, request rates, hardware environments, and fault conditions to find a reasonable balance of successes and failures, non-empty query results, attempted invariant violations, and so on.</p>
<h2 data-number="2.4" id="fault-injection"> Fault Injection</h2>
<p>Jepsen provides several kinds of faults “out of the box.” We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj">stressed TigerBeetle</a> with process crashes (<code>SIGKILL</code>), pauses (<code>SIGSTOP</code>), a variety of transitive and non-transitive network partitions, and clock changes ranging from milliseconds to hundreds of seconds, as well as strobing the clock rapidly back and forth. We also <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj#L395-L421">upgraded nodes through several versions</a> during tests.</p>
<p>We also introduced a variety of storage faults via a <a href="https://github.com/jepsen-io/jepsen/blob/8de1c9c9c1f7ac08fdf5c1eae3f709aa19cc3f9b/jepsen/resources/corrupt-file.c">new file corruption nemesis</a>. We flipped random bits to simulate (e.g.) cosmic ray interference. We replaced chunks of the file with other chunks, in an attempt to simulate <a href="https://pages.cs.wisc.edu/~remzi/OSTEP/file-integrity.pdf">misdirected writes</a>. We also took snapshots of chunks of the file, then restored them later, to simulate lost writes.</p>
<p>Each TigerBeetle node has a single data file, which is divided into <em>zones</em> at predictable offsets. Each zone stores a single kind of fixed-sized record. We scoped our faults to specific zones—corrupting, for example, only the write-ahead-log (WAL)’s headers, or restoring a snapshot of just one of the four redundant copies in the superblock zone. In many tests we corrupted multiple zones, or the entire file.</p>

<p>We also targeted a variety of nodes for file corruption. In one scenario, we corrupted data throughout (e.g.) the superblock, but only on a minority of nodes. In a second scenario, we corrupted every node’s data, but selected different chunks of the file for each node. For example, one node in a three-node cluster might corrupt the first, fourth, seventh, and tenth chunks of the grid; another would corrupt the second, fifth, eighth, and so on. We called this a <em>helical</em> disk fault. If you imagine arranging the cluster’s nodes into a ring, and drawing their file offsets along the ring’s symmetry axis, the corrupted chunks “spin” around the ring, forming a helix. Because TigerBeetle’s file layout is (generally speaking) bit-for-bit identical between up-to-date replicas, this avoids corrupting any single record in the database beyond repair.<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>

<h2 data-number="3.1" id="requests-never-time-out-206"> Requests Never Time Out (#206)</h2>
<p>Our first tests of TigerBeetle routinely stalled forever. For example, in <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/first-request-stalls.zip">this test run</a> the very first request never returned, which prevented the test from ever completing. This turned out to be a consequence of an unusual design decision: TigerBeetle <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/requests/#guarantees">actually guaranteed</a> that requests would never time out:</p>
<blockquote>
<p>Requests do not time out. Clients will continuously retry requests until they receive a reply from the cluster. This is because in the case of a network partition, a lack of response from the cluster could either indicate that the request was dropped before it was processed or that the reply was dropped after the request was processed.</p>
</blockquote>
<p>The <a href="https://web.archive.org/web/20240712120653/https://docs.tigerbeetle.com/reference/sessions/#retries">session documentation</a> reaffirmed this stance: a TigerBeetle client “will never time out” and “does not surface network errors”. This is particularly surprising since most systems do expose network errors, whether Strong Serializable or otherwise.</p>
<blockquote>
<p>With TigerBeetle’s strict consistency model, surfacing these errors at the client/application level would be misleading. An error would imply that a request did not execute, when that is not known[.]</p>
</blockquote>
<p>There are, broadly speaking, two classes of errors in distributed systems. A <em>definite</em> error, like a constraint violation, signifies that an operation has not and will never happen. An <em>indefinite</em> error, like a timeout, signifies that the operation may have already happened, might happen later, or might never happen.<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a> Consistent with the documentation, TigerBeetle tries to conceal both kinds of error through an unbounded internal retry loop.</p>
<p>However, TigerBeetle clients actually <em>can</em> produce timeout errors. The Java client’s asynchronous methods, like <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>createTransfersAsync</code></a>, return <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>CompletableFutures</code></a>. <code>CompletableFuture</code> usually represents operations, like network requests, which are subject to indefinite failures. Indeed, timeouts are integral to the datatype: one awaits a future using <code>.get(timeout, timeUnit)</code>, or wraps it in <code>.orTimeout(seconds, timeUnit)</code> to throw a timeout automatically. Likewise, the .Net client’s <a href="https://github.com/tigerbeetle/tigerbeetle/blob/274de35df357b790f0ccddded3531bb2592fbe2f/src/clients/dotnet/TigerBeetle/Client.cs#L65"><code>createTransferAsync</code></a> and friends return <a href="https://learn.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.wait?view=net-9.0#system-threading-tasks-task-wait(system-int32)"><code>Task</code></a> objects which offer timeout-driven <code>Wait()</code> methods.</p>
<p>Even if users constrain themselves to synchronous calls, applications rarely have unbounded time to run. It seems likely that applications will wrap TigerBeetle calls in their own timeouts. If they do not, the application may eventually terminate, which is a worse kind of indefinite failure. Even when applications can wait, their clients (or the human beings waiting for an operation), may give up at any time. The challenge of indefinite errors is intrinsic to asynchronous networks and cannot be eliminated.</p>
<p>Because TigerBeetle clients handle all failures through a silent internal retry mechanism, they unnecessarily convert definite errors into indefinite ones. For example, imagine a common fault: a TigerBeetle server has crashed. An application makes a <code>createTransfer</code> request. Its client attempts to open a TCP connection to submit the request, and receives <code>ECONNREFUSED</code>. The client knows internally that this request cannot possibly have executed: it has a definite failure. However, it refuses to inform the caller, and instead retries again and again. The caller’s only sign of an error is that the client appears to have stalled. When the caller times out or eventually shuts down, that definite failure becomes indefinite. Instead of making indefinite errors impossible, TigerBeetle’s client design <em>proliferates</em> them.</p>
<p>This is an ongoing discussion within TigerBeetle (#<a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a>). Jepsen recommended that TigerBeetle develop a first-class representation for definite and indefinite errors, and return those errors to callers when problems occur. It is perfectly fine to keep automatic retries—perhaps even unbounded ones—but this behavior should be configurable. TigerBeetle clients should take options controlling the maximum time allowed for opening a connection, and for awaiting responses from a submitted request. Users can request unbounded timeouts if desired.</p>
<h2 data-number="3.2" id="client-uninitialized-memory-access-2435"> Client Uninitialized Memory Access (#2435)</h2>
<p>Because synchronous client operations never timed out, our early tests generally failed to terminate. To avoid this problem, we tried wrapping calls to TigerBeetle clients in two kinds of timeouts. In the first, we spawned a new thread to make a synchronous call, and interrupted that thread if it did not complete within a few seconds.<a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>(<span>let</span> [worker (<span>future</span> (.createAccounts</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>                        client accounts))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>      ret    (<span>deref</span> worker <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>  (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>    (<span>do</span> (<span>future-cancel</span> worker)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>        (<span>throw</span> {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>    retval))</span></code></pre></div>
<p>In 0.16.11, this immediately <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/interrupt-segfault.log">segfaulted the entire JVM</a>. TigerBeetle’s Java client is implemented via a <a href="https://en.wikipedia.org/wiki/Java_Native_Interface">JNI</a> binding to a client library written in <a href="https://ziglang.org/">Zig</a>, and a Zig panic crashes the JVM as well. Concerned that our use of multiple threads or a thread interrupt might be at fault, we tried an alternate approach, using the client’s asynchronous methods which returned a <code>CompletableFuture</code>. If that future did not produce a result within a few seconds, we closed the client.</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>(<span>let</span> [<span>future</span> (.createAccountsAsync</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>                c (account-batch accounts))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>       ret   (<span>deref</span> <span>future</span> <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>    (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>      (<span>do</span> (close! client)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>          (throw+ {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>      ret))</span></code></pre></div>
<p>—this too <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/async-close-segfault.log">segfaulted the JVM</a>.</p>
<p>In a closely related issue, calling <code>client.close()</code> in 0.16.11, on a freshly opened client, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/close-unreachable.log">caused the JVM to panic</a> with a <code>reached unreachable code</code> error in <code>tb_client.zig:122</code>.</p>
<p>TigerBeetle traced these problems to uninitialized fields in the client’s request data structure (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">#2435</a>). These fields were normally filled in during request submission. However, if the client was closed between request creation and submission, it would dereference uninitialized pointers from that structure, jumping to random offsets in memory. This issue was fixed in 0.16.12.</p>
<h2 data-number="3.3" id="client-crash-on-eviction-2484"> Client Crash on Eviction (#2484)</h2>
<p>The official TigerBeetle clients crashed the entire process when a server informed them that their session had been evicted. TigerBeetle allows only <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/sessions/#eviction">64 concurrent sessions</a> by default, making it relatively easy to hit this limit.<a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a> TigerBeetle also evicts clients which use a newer client version than the server.</p>
<p>This behavior made it impossible for clients to cleanly recover from eviction, or to back off under load. TigerBeetle changed this behavior in <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">#2484</a>. As of 0.16.13, clients return errors to their callers on eviction, rather than crashing the entire process.</p>
<h2 data-number="3.4" id="elevated-latencies-on-single-node-faults-2739"> Elevated Latencies on Single-Node Faults (#2739)</h2>
<p>When a single node failed, we often saw client latencies jump by three to five orders of magnitude. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/0241210T170510.624-single-node-latency-jump.zip">this test of a five-node cluster, with clients constrained to a single node each</a>, killing a single node caused minimum latencies to rise from less than one millisecond to ten seconds. There were fluctuations down to one second, but in general elevated latencies lasted for the full duration of a fault.</p>
<p><img src="https://jepsen.io/analyses/tigerbeetle-0.16.11/single-node-latency-jump-2.png" alt="A time-series plot of latencies. A red bar shows the period where n3 was dead. Latencies jump dramatically."/><br/>
</p>
<p>Under higher load, the situation could become considerably worse. Consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250214T110802.529-single-node-latency-jump-3.zip">this test run</a> with a three-node cluster, where each client was allowed to connect to all three nodes. A few seconds into the test, we killed node <code>n3</code>. This drove latencies on every client from between 1–50 milliseconds to roughly a hundred seconds per request. This situation persisted for almost a thousand seconds, until we restarted <code>n3</code>.</p>
<p>In the original <a href="https://pmg.csail.mit.edu/papers/vr.pdf">Viewstamped Replication</a> and <a href="https://www.pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a>, a primary sends a <em>prepare</em> message to every secondary when it wants to perform an operation. The secondaries send acknowledgements back to the primary, which can commit once <span><em>f</em></span> replicas have acknowledged. The failure of any single node (shown in red) causes a single acknowledgement to be lost, but does not affect any of the other nodes or their acknowledgements. The system as a whole is relatively insensitive to single-node failures.</p>

<p>TigerBeetle approaches prepares differently. Nodes are arranged in a ring, and the primary sends a single prepare message to the next secondary in the ring. That secondary sends a prepare to the following secondary, and so on, until all nodes have received the message. Acknowledgements are sent directly to the primary. This approach reduces the bandwidth requirements for any single node, but creates a weakness: if the primary must receive <span><em>f</em></span> acknowledgements to commit, the failure of any one of the next <span><em>f</em></span> replicas in the ring will prevent commit entirely. The effect of a single-node failure cascades through the rest of the ring. We opened issue <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">#2739</a> to track this issue.</p>
<p>Version 0.16.30 includes a new tactic to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2761">mitigate this problem</a>. Sending every other prepare message <em>backwards</em> around the ring allows half of prepares to bypass the faulty node. Since prepares must be processed in order, the replicas which receive these counter-ringward prepares must repair the ringward prepare messages they missed. Repairing takes time, but the overall effect is significant. Rather than hundred-second latencies during a single-node fault, 0.1.16 delivered 1–30 second latencies in our tests.</p>
<p>After our collaboration, TigerBeetle continued work on single-node fault tolerance, adding a new series of deterministic performance tests to their simulation framework. As of version 0.16.43, TigerBeetle includes <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">a host of performance improvements</a>. Nodes replicate in both directions around the ring, which reduces latencies and the impact of single failures. The ring topology is now dynamic: and the cluster continually adjusts the order of nodes to minimize latency based on network conditions and faults.</p>

<p>To support Jepsen’s tests, TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/0.16.13/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new, experimental API</a> in 0.16.13 for obtaining the execution timestamp for each request from a header included in response messages. In the Java client, this API routinely returned <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250106T115221.401-duplicate-timestamps.zip">incorrect and duplicate timestamps</a>. For example, both of these <code>create-transfers</code> operations returned identical timestamps.</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>{<span>:index</span>     <span>5827</span>,</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a> <span>:process</span>   <span>1</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a> <span>:value</span>     [<span>:ok</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a> <span>:timestamp</span> <span>1736185975365035812</span>}</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>{<span>:index</span>     <span>5829</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a> <span>:process</span>   <span>11</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a> <span>:value</span>     [<span>:ok</span> <span>:ok</span> <span>:ok</span> <span>:ok</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a> <span>:timestamp</span> <span>1736185975365035812</span>}</span></code></pre></div>
<p>TigerBeetle traced this bug to a <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">mutable singleton response object</a> in the Java client: <code>Batch.EMPTY</code>. Every empty response used the same instance of this object, updating its header to reflect that response’s timestamp. As responses overwrote each other’s headers, callers observed incorrect timestamps. Since TigerBeetle represents entirely-successful responses as empty batches, this happened quite often.</p>
<p>This bug (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">#2495</a>) was fixed in 0.16.14, just seven days after 0.6.13’s release. It did not affect the correctness of the actual data involved, only request timestamps from the Java client’s header API. We believe the impact to users was likely nil.</p>
<h2 data-number="3.6" id="missing-query-results-2544"> Missing Query Results (#2544)</h2>
<p>In version 0.16.13, responses for <code>query_accounts</code>, <code>query_transfers</code>, and <code>get_account_transfers</code> routinely omitted some or all results.<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a> Missing results were always at the end: each response was a (possibly empty) prefix of the correct results. This behavior occurred frequently in healthy clusters. For example, take <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20241205T192824.267-missing-query-results.zip">this test run</a>, where 281 seconds into the test, a client called <code>query_transfers</code> with the following filter:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>{<span>:flags</span>  #{<span>:reversed</span>}</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a> <span>:limit</span>  <span>9</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a> <span>:ledger</span> <span>3</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a> <span>:code</span>   <span>289</span>}</span></code></pre></div>
<p>This query returned a single result:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>[{<span>:amount</span>            34N,</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>  <span>:ledger</span>            <span>3</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>  <span>:debit-account-id</span>  3137N,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>  <span>:pending-id</span>        0N,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>  <span>:credit-account-id</span> 1483N,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>  <span>:user-data</span>         <span>9</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>  <span>:id</span>                327610N,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>  <span>:code</span>              <span>289</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>  <span>:timeout</span>           <span>0</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>  <span>:timestamp</span>         <span>1733448783658756894</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>  <span>:flags</span>             #{<span>:linked</span>}}]</span></code></pre></div>
<p>However, our model expected eight additional transfers which TigerBeetle omitted. For instance, transfer <code>326112</code> had ledger 3 and code 289, and was successfully acknowledged five seconds before this query began. It should have been included in these results, but was not.</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>{<span>:amount</span>            <span>21</span>,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a> <span>:ledger</span>            <span>3</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a> <span>:debit-account-id</span>  123076N,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a> <span>:pending-id</span>        0N,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a> <span>:credit-account-id</span> 51358N,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a> <span>:user-data</span>         <span>2</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a> <span>:id</span>                326112N,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a> <span>:code</span>              <span>289</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a> <span>:timeout</span>           <span>0</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a> <span>:timestamp</span>         <span>1733448782536800935</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a> <span>:flags</span>             #{}}</span></code></pre></div>
<p>Note that this query asked for transfers matching both <code>ledger = 3</code> and <code>code = 289</code>. Queries which filtered on only a single field did not exhibit this problem. TigerBeetle traced the cause to a bug in the zig-zag merge join between multiple indices (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">#2544</a>). When traversing an index, a bounds check prevented scanning the same chunk of records twice. During a join between two indices, the scans informed one another that some records can be safely skipped. This process could push the highest (or lowest) key outside the bounds check in the wrong direction, causing the scan to terminate early. The issue was fixed in version 0.16.17.</p>
<p>This bug went undetected by all four TigerBeetle fuzzers which perform index scans. Two fuzzers, <code>fuzz_lsm_tree</code> and <code>fuzz_lsm_forest</code>, did not perform joins. The other two, <code>vopr</code> and <code>fuzz_lsm_scan</code>, generated objects which happened to appear consecutively in each index—the “zig-zag” part of the merge join was never executed. <a href="https://github.com/tigerbeetle/tigerbeetle/commit/73f13ee2d092fe6998c315b0c4ae5fbc2983154a">Rewriting the scan fuzzer</a> to generate unpredictable objects helped it reproduce this bug quickly.</p>
<h2 data-number="3.7" id="panic-at-the-disk-0-2681a"> Panic! At the Disk 0 (#2681a)</h2>
<p>Occasionally, tests with single-bit file corruption in the superblock, WAL, or grid zones caused TigerBeetle 0.16.20 to <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250127T183251.857-bitflip-padding-panic.zip">crash on startup</a>. The process would print <code>panic: reached unreachable code</code>, then exit.<a href="#fn23" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<p>These crashes were caused by three near-identical bugs in checking sector padding. For example, each superblock header in TigerBeetle’s data file contains an unused padding region normally filled with zeroes. Similarly, entries in the WAL and grid blocks may have zero padding bytes at the end. TigerBeetle’s checksums cover the data stored in each chunk, but exclude the padding. If a bit in the padding flipped from zero to one, the chunk’s checksum would still pass. Then, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/blob/9e754a3dbddfbd6b51ce44805b2718061f799a97/src/vsr/grid.zig#L1080">checked to make sure</a> the padding bytes were still zeroed. When it encountered the flipped bit, that failed assertion caused the server to crash. This is perhaps worth logging, but damage to padding bytes does not compromise safety. The corrupted padding could be re-zeroed or repaired from other replicas.</p>
<p>TigerBeetle’s internal testing with the VOPR did not discover this bug because it corrupted entire sectors, rather than single bits. Corrupting a sector caused the checksum to fail and triggered the repair process. The zero-padding assertion was never reached! TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">revised the VOPR (#2681)</a> to introduce single-byte errors, which reproduced the bugs. As of 0.16.26, TigerBeetle repairs sectors with corrupt padding, instead of crashing.</p>
<h2 data-number="3.8" id="panic-due-to-superblock-bitflips-2681b"> Panic Due to Superblock Bitflips (#2681b)</h2>
<p>In a closely related bug, TigerBeetle could crash with an identical <code>panic: reached unreachable code</code> error, when we flipped bits in the superblock’s region rather than padding. Each of the superblock’s four copies includes a unique two-byte <code>copy</code> number, so that TigerBeetle can detect if a write or read of the superblock was misdirected by the disk. However, each copy of the superblock was supposed to have an identical checksum. Those checksums therefore skipped over the copy number.</p>
<p>When writing a superblock back to disk, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">checked to make sure (#2681)</a> that the copy number was between 0 and 3. If the copy number had been corrupted on disk, and that corrupted version read into memory, that assertion would fail at write time, causing a panic. TigerBeetle resolved this in version 0.16.26 by resetting the copy number, rather than crashing.</p>
<h2 data-number="3.9" id="checkpoint-divergence-on-upgrade-2745"> Checkpoint Divergence on Upgrade (#2745)</h2>
<p>When testing upgrades from 0.16.25 and before to 0.16.26 and higher, we observed repeated TigerBeetle crashes with log messages like <code>panic: checkpoint diverged</code>. For example, this one-minute test <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250213T152133.089-0.16.26-upgrade-diverge.zip">upgraded a five-node cluster from 0.16.25 to 0.16.26</a>, with no other faults. Node n5 detected the new binary at 21:48, and switched to executing 0.16.26 at 22:01. Immediately after starting, it panicked at <code>replica.zig:1766</code>:</p>
<pre><code>2025-02-13 21:22:06.159Z error(replica):
4: on_prepare: checkpoint diverged (op=23040
expect=3779fc8a6a13bf5cf9f995b8895c2609
received=05383d884c680d15e726071358854f67
from=2)
thread 227936 panic: checkpoint diverged</code></pre>
<p>TigerBeetle traced this crash to a change in the <code>CheckpointState</code> structure in 0.16.26. Between checkpoints, TigerBeetle tracks a set of released blocks in the file. In 0.16.26, TigerBeetle changed when that set was flipped to empty. The old version of <code>CheckpointState</code> did not need to track released blocks, because that set was always empty at checkpoint time. The new version included released blocks. To ensure older replicas could still sync state from newer ones, nodes running 0.16.26 could send both the old and new versions of <code>CheckpointState</code>. This allowed a node running 0.16.26 to send a backwards-compatible <code>CheckpointState</code> with an empty set of released blocks to a node running 0.16.25. If that node then restarted on 0.16.26, it would be missing the released blocks which other replicas knew about. Thankfully, the assertion detected this divergence and crashed the node, rather than allowing clients to observe inconsistent data.</p>
<p>We found this bug after several later versions had already been released. Because it requires state synchronization, rather than the normal replication path, we believe healthy and non-lagging clusters shouldn’t encounter this bug. The impact should also be limited to a minority of nodes. Based on these factors, and a lack of test coverage for upgrades in general, TigerBeetle opted not to release a patched version of 0.16.26. Instead, the team updated the changelog (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">#2745</a>) to inform customers of the hazard. Operators should pause all clients and wait for replicas to catch up before upgrading to (or past) 0.16.26.</p>
<h2 data-number="3.10" id="panic-in-release_transition-on-multiple-upgrades-2758"> Panic in <code>release_transition</code> on Multiple Upgrades (#2758)</h2>
<p>In upgrade tests from 0.16.16 to 0.16.28, we found that TigerBeetle could crash with an assertion failure in <code>replica.zig</code>’s <code>release_transition</code> function. This happened when we executed multiple upgrades within ~20 seconds of one another, or when nodes crashed or paused during the upgrade process. We could reproduce this bug reliably–with process pauses, it manifested <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250220T153519-release-transition-panic.zip">a few times per minute</a>.</p>
<p>TigerBeetle traced this problem to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">an over-zealous assertion</a> in the upgrade code (#2758).</p>
<p>Imagine a node currently runs version <span><em>A</em></span>, and an operator replaces its binary with version <span><em>B</em></span>. The node detects that version <span><em>B</em></span> is available, opens the new binary with a <a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd</code></a>, and uses <code>exec()</code> to replace the running process with that new code. Meanwhile, an operator replaces the binary with version <span><em>C</em></span>. The replica starts up with <span><em>B</em></span>, and as a safety check, asserts that the binary on disk (not the <code>memfd</code>!) has version header <span><em>B</em></span>. This assertion fails, since the binary is actually version <span><em>C</em></span>.</p>
<p>TigerBeetle resolved this issue in 0.16.29 by replacing the assertion with a warning message; running a different version than the binary on disk does not actually break safety.</p>
<h2 data-number="3.11" id="panic-on-deprecated-message-types-2763"> Panic on Deprecated Message Types (#2763)</h2>
<p>We encountered another occasional crash in the upgrade from 0.16.26 to 0.16.27. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250221T134853-deprecated-panic.zip">this test run</a>, two nodes crashed shortly after the upgrade. Both logged <code>panic: switch on corrupt value</code>, originating in <code>message_header.zig</code>’s <code>into_any</code> function. The crashed nodes recovered after a restart.</p>
<p>This crash was caused by a switch expression which <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">dispatched based on the type of a message</a>. Prior to 0.16.28, Tigerbeetle removed deprecated message types from these switch expressions. An older node could send a network message of a type that the newer node would have no corresponding <code>switch</code> case for. TigerBeetle resolved the issue in version 0.16.29 by adding the deprecated message types back into the switch statements, and simply ignoring them.</p>
<h2 data-number="3.12" id="no-recovery-path-for-single-node-disk-failure-2767"> No Recovery Path for Single-Node Disk Failure (#2767)</h2>
<p>TigerBeetle offers exceptional resilience to data file corruption. However, disk failure, fires, EBS volume errors, operator errors, and more can cause a node to lose its entire data file, or to corrupt that data beyond repair. TigerBeetle is fault tolerant and can continue running safely with a minority of nodes offline. However, failed nodes do need to be replaced eventually, and most distributed systems have a mechanism for doing so. In systems which support membership changes, the best path is often to add a new, replacement node to the cluster, and to remove the failed node. Others have dedicated replacement procedures.</p>
<p>TigerBeetle, surprisingly, has no story for replacing a failed node. The documentation says nothing on the matter. There is an undocumented recovery procedure: users can run <code>tigerbeetle format</code> to re-initialize the node with an empty data file, and allow TigerBeetle’s automatic repair mechanisms to transfer the data from other nodes. Since our tests often damaged data files beyond repair, we used this reformat approach regularly.</p>
<p>Reformatting nodes works most of the time, but as TigerBeetle explained to Jepsen, it may be unsafe. For example, imagine a committed operation <code>op</code> is present on two out of three nodes, and one of those nodes is reformatted. The cluster now has a two-thirds majority which can execute a view change <em>without</em> observing <code>op</code>; the operation is then lost. In our testing, data loss was infrequent, and limited to just a few operations. For example, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250226T161747-upgrade-reformat.zip">this run</a> lost five acknowledged transfers which were created in two separate requests. Another problem arises when nodes are upgraded. If a node is formatted using a newer binary, but the cluster has not yet completed the transition to that version, the node will <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250225T043429-upgrade-reformat.zip">crash on startup</a> during <code>replica.zig/open</code>.</p>
<p>TigerBeetle had been aware of <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">this issue #2767</a> for some time, and planned to add a safe recovery path for the loss of a node. However, it took time to design, build, and document. After our collaboration, TigerBeetle completed this work. Version 0.16.43 incorporates a new <a href="https://docs.tigerbeetle.com/operating/recovering/"><code>tigerbeetle recover</code></a> command to recovera node which has suffered catastrophic data loss.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a></td>
<td>Requests never time out</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">2435</a></td>
<td>Client uninitialized memory access on close</td>
<td>Interrupt or close a client</td>
<td>0.16.12</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">2484</a></td>
<td>Client crash on eviction</td>
<td>Newer, or too many, clients</td>
<td>0.16.13</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">2739</a></td>
<td>Elevated latencies on single-node fault</td>
<td>Pause or crash</td>
<td>0.16.43</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">2495</a></td>
<td>Incorrect header timestamp from Java client</td>
<td>None</td>
<td>0.16.14</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">2544</a></td>
<td>Missing query results</td>
<td>None</td>
<td>0.16.17</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">2681a</a></td>
<td>Panic on bitflips in chunk padding</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">2681b</a></td>
<td>Panic on bitflips in superblock copy number</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">2745</a></td>
<td>Checkpoint divergence</td>
<td>0.16.26 upgrade during sync</td>
<td>Documented</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">2758</a></td>
<td>Panic in <code>release_transition</code> on upgrades</td>
<td>Upgrades in quick succession</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">2763</a></td>
<td>Panic on deprecated message types</td>
<td>Upgrade</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">2767</a></td>
<td>No recovery path for single-node disk failure</td>
<td>Single-node disk failure</td>
<td>0.16.43</td>
</tr>
</tbody>
</table>

<p>We found two safety issues in TigerBeetle.<a href="#fn24" id="fnref24" role="doc-noteref"><sup>24</sup></a><a href="#fn25" id="fnref25" role="doc-noteref"><sup>25</sup></a> Prior to version 0.16.17, TigerBeetle often omitted results from queries with multiple filters, even in healthy clusters. We also found a very minor issue in which a debugging API in the Java client, added specifically for our tests, returned incorrect and duplicate timestamps for operations. As of 0.16.26 and higher, our findings were consistent with TigerBeetle’s claims of <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>—one of the strongest consistency models for concurrent systems. TigerBeetle preserved this property through various combinations of process pauses, crashes, network partitions, clock errors, disk corruption, and upgrades.</p>
<p>We also found seven crashes in TigerBeetle. Two affected the Java client: an uninitialized memory access caused by a shared mutable data structure, and a design choice to crash the entire process when a server evicted a client. Both were fixed by 0.16.13. Five involved servers: two panics on disk corruption, and three more involving upgrades. All crashes were resolved by 0.16.29, with the exception of #2745, which is now documented.</p>
<p>We found some surprising performance and availability issues in TigerBeetle. Server latencies jumped dramatically when even a single node was unavailable. This behavior is unusual—most consensus systems are relatively insensitive to single-node failures—and stemmed from a design choice to replicate data in a ring, rather than broadcasting from the primary to all backups directly. This behavior was somewhat improved in 0.16.30, but still quite noticeable. After our collaboration, TigerBeetle extended their simulation tests to measure performance under various faults, and used those tests to drive <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">extensive improvements</a> in 0.16.43. The ring topology now continually adapts to observed latencies, and messages are broadcast in both directions around the ring. We believe these improvements should significantly mitigate the latency impact of failures.</p>
<p>TigerBeetle also lacked a safe path to recover a node which had suffered catastrophic disk failure. After our collaboration, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2996">built</a> a new <a href="https://docs.tigerbeetle.com/operating/recovering/">recovery command</a>, which is available as of 0.16.43.</p>
<p>Only one issue remains unresolved. By design, client requests are retried forever, which complicates error handling. TigerBeetle plans to address this, but the work will take some time.</p>
<p>We recommend users upgrade to 0.16.43, which addresses all but one of the issues reported here. Users should exercise particular caution during the upgrade to (or past) 0.16.26; consult <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/CHANGELOG.md#tigerbeetle-01626">the release notes</a>. We also suggest that users simulate single-node failures in a testing environment, and measure how their application responds to elevated latencies.</p>
<p>TigerBeetle exhibits a refreshing dedication to correctness. The architecture appears sound: Viewstamped Replication is a well-established consensus protocol, and TigerBeetle’s integration of flexible quorums and protocol-aware recovery seem to have allowed improved availability and extreme resilience to data file corruption. Integrating these protocols does not appear to have compromised the key invariant of Strong Serializability. Most of our findings involved crashes or performance degradation, rather than safety errors. Moreover, several of those crashes were due to overly cautious assertions.</p>
<p>We attribute this robustness in large part to TigerBeetle’s extensive simulation, integration, and property-based tests, which caught a broad range of safety bugs both before and during our engagement. As we brought new issues to the TigerBeetle team, they quickly expanded their internal test suite to reproduce them. We are confident that TigerBeetle’s investment in careful engineering and rigorous testing will continue to pay off, and we’re excited to see these techniques adopted by more databases in the future.</p>
<p>As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<h2 data-number="4.1" id="disk-faults"> Disk Faults</h2>
<p>TigerBeetle offers exceptional resilience to disk faults. In our tests, it recovered from bitflips and other kinds of file corruption in almost every part of a node’s data file, so long as corruption was limited to a minority of nodes. In some file zones, like the grid, TigerBeetle tolerated the loss or corruption of all but one copy. In the superblock, client replies, and grid zones of the data file, TigerBeetle could recover our “helical” faults, in which every node experienced data corruption spread across disjoint regions of the file.</p>
<p>As previously noted, bitflips in the superblock copy number, or in various zero-padding regions, could cause TigerBeetle to crash. These issues have been resolved as of 0.16.26.</p>
<p>Rolling back all four copies of a node’s superblock to an earlier version can permanently disable a TigerBeetle node; it will crash upon detecting WAL entries newer than the superblock. TigerBeetle considers this outside their fault model, and we concur. Given that TigerBeetle performs four separate, sequential writes of the superblock and reads them back to confirm their presence, it seems remarkably unlikely to encounter this by accident.</p>
<p>Helical faults in the WAL can permanently disable a TigerBeetle cluster. The most recent “head” entry of the WAL is critical, and since some nodes may lag behind others, they may have heads at different file offsets. In our tests, helical faults often corrupted the head of the WAL on a majority of nodes, rendering the entire cluster unusable.</p>
<p>When a node’s disk file is lost or corrupted beyond repair, TigerBeetle currently has no safe path for recovery. We recommend users exercise caution when reformatting a failed node. Avoid upgrades when a node is down, and try to reformat a node only when the remainder of the cluster appears healthy. If possible, pause clients and check node logs before upgrading: none should be logging sync-related messages.</p>
<h2 data-number="4.2" id="retries"> Retries</h2>
<p>Users should think carefully about the official TigerBeetle clients’ retry behavior. By default, clients retry operations forever. Synchronous operations will never time out; you may need to implement your own timeouts. The futures returned by asynchronous calls do offer APIs with timeouts, but the client will continue retrying those operations forever. Long-lasting unavailability could cause TigerBeetle clients to consume unbounded memory as they attempt to buffer and retry an ever-growing set of requests.</p>
<p>This retry behavior flattens definite and indefinite failures into indefinite ones: <em>everything</em> becomes a timeout. Contrary to TigerBeetle’s documentation, indefinite network errors are very much possible. Indeed, they are more likely in TigerBeetle than in systems which return distinct network errors! Moreover, TigerBeetle users may find it more difficult to implement (e.g.) exponential backoff or load-shedding circuit breakers: in order to abandon a single request, the entire client must be torn down.</p>
<p>Jepsen recommends that users carefully consider and test their timeout behavior during faults. We also suggest TigerBeetle introduce at least two kinds of error, so users can distinguish between definite and indefinite faults. Finally, clients should take configurable timeouts, so users can bound their time and memory consumption.</p>
<h2 data-number="4.3" id="crashing-as-a-way-of-life"> Crashing as a Way of Life</h2>
<p>TigerBeetle prizes safety, and employs defensive programming techniques to ensure it. In addition to carefully designed algorithms and extensive testing, both client and server code are full of assertions which double-check that intended invariants have been preserved. Assertion failures crash the entire program to preserve safety. When this happens, clients or servers may be partially or totally unavailable, sometimes for minutes, sometimes permanently. Many of our findings involved an assertion which turned what <em>would</em> have been a safety hazard into a simple crash: a welcome tradeoff for safety-critical systems.</p>
<p>This is a sensible approach. Complex systems <a href="https://how.complexsystems.fail/">ensure safety</a> through an interlocking set of guards: each guard screens out errors the others might have missed. Abandoning possibly-incorrect execution is also a core tenet of Erlang/OTP’s <a href="https://erlang.org/pipermail/erlang-questions/2003-March/007870.html">“let it crash”</a> ethos. However, TigerBeetle’s approach is not without drawbacks.</p>
<p>First, several of the crashes we found in this report were due to overly conservative assertions. For instance, prior to 0.16.26, TigerBeetle crashed on encountering non-zero bytes in unused padding regions on disk. Safety was never endangered by these errors, but the crashes compromised availability—and could push users into the dangerous recovery path of reformatting.</p>
<p>Second, in 0.16.11, TigerBeetle’s client library forcibly crashed the entire application process when a client used a newer version than the server, or when the server simply had too many connections. These are errors that a well-designed application can and should recover from—for instance, through an exponential backoff and retry system, or by coordinating with other clients. Crashing the process, instead of returning an error code or throwing an exception, denies the application the ability to make these mitigations.</p>
<p>In Erlang, “let it crash” means more than simply abandoning computation early. It is integrally linked with Erlang’s actor model, which allows actors to crash independently of one another. It also relies on Erlang’s <em>supervisor trees</em>: every actor has a supervisor which is notified of a crash and can restart the failed computation. In TigerBeetle, the failure domain is the entire POSIX process, and the supervisor, where one exists, is something like the init system or Kubernetes. These supervisors generally lack visibility into <em>why</em> the crash happened or how to recover, and they are often not equipped to adapt to changing circumstances. They may restart the process over and over again, crashing every time. On repeated crashes, they may give up on the process forever.</p>
<p>Despite these limitations, we feel TigerBeetle makes a reasonable compromise. TigerBeetle is intended for financial systems of record where integrity is key, and overly-cautious assertions can be fixed easily as they arise. Those assertions also help to experimentally validate and guide the mental models of engineers working on TigerBeetle. TigerBeetle’s clients have shifted more towards returning error codes, rather than crashing outright.</p>
<p>More generally, we encourage engineers to think about fault domains when designing error paths. Ask, “If we must crash, how can we keep a part of the system running?” And after a crash, “How will that part recover?” This is especially important for client libraries, which are guests in another system’s home.</p>
<h2 data-number="4.4" id="future-work"> Future Work</h2>
<p>TigerBeetle includes a <a href="https://docs.tigerbeetle.com/reference/transfer/#timeout">timeout mechanism</a> for pending transfers. We do not know how to robustly test this system, since timeouts may, by design, not void a transfer until some time after their deadline has passed. We would like to revisit timeout semantics with an eye towards establishing quantitative bounds.</p>
<p>During the course of this research, Jepsen, TigerBeetle, and <a href="https://antithesis.com/">Antithesis</a> collaborated to run Jepsen’s TigerBeetle test suite within Antithesis’s environment—taking advantage of Antithesis’ deterministic simulation, fault injection, and time-travel debugging capabilities. These experiments are still in the early stages, but could lay the groundwork for a powerful, complementary analysis of distributed systems.</p>
<p>Multi-version systems are also devilishly hard to pull off. While TigerBeetle already had excellent test coverage for single versions, they lacked fuzz tests for cross-version upgrades. Our tests found several issues in the upgrade process, and TigerBeetle plans to expand their testing of upgrades in the future. Similarly, membership changes in distributed systems are notoriously challenging, and currently unimplemented in TigerBeetle. As TigerBeetle builds support for adding and removing nodes, we anticipate a rich opportunity for further testing.</p>
<p>Finally, TigerBeetle’s approach to retries has been the subject of ongoing discussion, and redesigning their approch will take time. We anticipate further work towards a robust client representation of errors.</p>
<p><em>This work would not have been possible without the invaluable assistance of the TigerBeetle team, including Fabio Arnold, Rafael Batiati, Chaitanya Bhandari, Lewis Daly, Joran Dirk Greef, djg, Alex Kladov, Federico Lorenzi, and Tobias Ziegler. Our thanks to <a href="https://github.com/duckinator">Ellen Marie Dash</a> for helping write the new file-corruption nemesis used in this research. We are grateful to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This report was funded by TigerBeetle, Inc. and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>

  </div>
</article></div>
  </body>
</html>
