<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.the-pans.com/notes-on-the-foundationdb-paper/">Original</a>
    <h1>How FoundationDB works and why it works (2021)</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
<article>
<div>
<section>
<p>FoundationDB is a very impressive database. Its <a href="https://www.foundationdb.org/files/fdb-paper.pdf">paper</a> won the best industry paper award in SIGMOD&#39;21. In this post, I will explain, in detail, how FDB works and discuss a few very interesting design choices they made. It&#39;s a dense paper packed with neat ideas. Many details (sometimes even proof of correctness) are not included in the paper. I added proof wherever necessary.</p>
<h2 id="whatisfoundationdb">What is FoundationDB?</h2>
<p>It&#39;s a <em>non-sharded</em>, <em>strict serializable</em>, fault tolerant, <em>key-value</em> store that supports point writes, reads and range reads. Notice that it provides a key-value API (not SQL). It&#39;s also not sharded, meaning the entire key space is essentially on one logical shard. That&#39;s it. Once you have a strict serializable key-value store, you can layer a SQL engine and secondary indexes on top. A strict serializable (can be relaxed if needed obviously) key-value store is the foundation (a smaller reusable component), upon which you can build distributed databases almost however you want. This is a <em>great</em> design choice.</p>
<h2 id="boldclaims">Bold Claims</h2>
<p>The paper makes a few jaw-dropping claims in the intro.</p>
<ul>
<li>FDB is strict serializable and lock-free. How is this possible?
<ul>
<li>me: Don&#39;t you have to take locks to guarantee order?</li>
</ul>
</li>
<li>For a global FDB deployment, it can avoid x-region write latency.
<ul>
<li>me: Hmm... OK. It&#39;s possible if you make some assumptions about failure domains.</li>
</ul>
</li>
<li>FDB can tolerate <em>f</em> failures with only <em>f+1</em> replicas.
<ul>
<li>me: You gotta be kidding me!</li>
</ul>
</li>
</ul>
<p>We will come back to all these jaw-dropping claims. If these claims don&#39;t excite you, this post and the paper are probably not for you.</p>
<h2 id="architecture">Architecture</h2>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-05-at-11.01.20-AM.png" alt="Screen-Shot-2021-07-05-at-11.01.20-AM" loading="lazy"/></p>
<p>FDB takes the idea of microservice to the extreme (they call it decoupling). Almost every single functionality is handled by a distinct service (stateful or stateless). Embrace yourself for many names for different roles.</p>
<h4 id="controlplane">Control Plane</h4>
<p>The most important Control Plane service in the diagram is the <em>Coordinator</em>. It basically stores small metadata about the entire FDB deployment/configuration (e.g. the current epoch, which gets bumped every time a reconfiguration/recovery takes place). <em>ClusterController</em> monitors the health of all servers (presumably via heartbeats, as it&#39;s not mentioned in the paper).</p>
<h4 id="dataplane">Data Plane</h4>
<p>FDB breaks the Data Plane into three parts, Transaction System, Log System and Storage System. Log System and Storage System are mostly what you would expect – a distributed log and a disaggregated, sharded storage. The Transaction System is the most interesting one, among which the <em>Sequencer</em> is the most critical service.</p>
<h2 id="transactionmanagement">Transaction Management</h2>
<ol>
<li>A client is always connected to a Proxy, which communicates with FDB internal services.</li>
<li>The Proxy will get a read-version in the form of HLC (hybrid logical clock) from the <em>Sequencer</em>. Notice that there&#39;s only one (not logical) physical <em>Sequencer</em> in the system, which serves as a single point of serialization (ordering). The <em>Sequencer</em> conceptually has the following API, which keeps producing monotonically increasing HLCs(or LSNs).</li>
<li>The client then issues reads to <em>Storage</em> with the specific read-version, acquired from the <em>Sequencer</em>. Storage System will return data at the specific point-in-time indicated by the read-version (an HLC) – MVCC.</li>
<li>The client then asks <em>Sequencer</em> again for a commit-version.</li>
<li>Next, it sends the write-set <em>and</em> the read-set to the <em>Resolver</em>. <em>Resolvers</em> are range-sharded and they keep a short history of recent writes. Here, it detects if there are any conflicts (i.e. if the data read earlier in the transaction has changed or not, between <code>[read-version, commit-version]</code>).</li>
<li>If no conflict is detected, commit the writes to the Log System with commit-version. Once enough ACKs have been received from the logs, the <em>Proxy</em> returns success to the client.</li>
</ol>
<p>It&#39;s basically Optimistic Concurrency Control (OCC), very straightforward. In this example, it actually doesn&#39;t even need MVCC. Because if there are conflicts, i.e. what the client read has changed, the transaction will be either aborted or retried anyway. MVCC support in FDB is there to support snapshot reads, and read-only transactions.</p>
<p><em>Edit: Bhaskar Muppana, one of the authors of the paper, pointed <a href="https://www.linkedin.com/feed/update/urn:li:activity:6823635315471863808?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6823635315471863808%2C6826392929918160896%29">one mistake</a> I had in the following paragraph. It has been corrected. Thanks, Bhaskar.</em></p>
<p><s>The reason why <em>Proxy</em> is needed is that it allows reading uncommitted writes within a transaction, which is a very common semantic provided by almost all relational databases. It&#39;s achieved by buffering uncommitted writes locally on the Proxy servers, and merging these writes with storage data for reads on the same keys.</s> Client (notice not <em>Proxy</em>) caches uncommitted writes to support read-uncommitted-writes in the same transaction. This type of read-repair is only feasible for a simple k/v data model. Anything slightly more complicated, e.g. a graph data model, would introduce a significant amount of complexity. Caching is done on the client, so read queries can bypass the entire transaction system. Reads can be served either locally from client cache or from storage nodes.</p>
<p><em>Proxy</em> sits between clients and the transaction system. Its responsibility is to serve as a semi-stateful &#34;client&#34;. E.g. it remembers the <em>KCV</em> to aid recovery; it performs batching (of requests from multiple clients) for reducing <em>Sequencer</em> qps.</p>
<p><em>Resolvers</em> are range-sharded, hence they can perform conflict checks in parallel. That means <em>Proxy</em> needs to wait to hear back from all <em>Resolvers</em> involved in each transaction. How does a <em>Resolver</em> get recently <em>committed</em> keys and their versions? It doesn&#39;t actually. Otherwise, it would require a distributed transaction between the <em>Log</em> and the <em>Resolver</em>. The workaround is that <em>Resolvers</em> keep a set of recent <strong>write-attempts</strong> and their versions. It&#39;s possible that a key appears to be recently written in a <em>Resolver</em> while in fact its transaction is rolled back, leading to false positives. FDB claims the false positives are manageable because:</p>
<ol>
<li>most workloads fall onto one <em>Resolve</em> (hence it would know if it needs to record the commit-attempt or not)</li>
<li>the MVCC window is 5 seconds. Waiting it out is not the end of the world, for conflicts are only around a single key – partial unavailability.</li>
</ol>
<p>How about racing commits? E.g. A Resolver can first admit a transaction with read-version <code>Vr</code> and commit-version <code>Vc</code>. Later, another transaction with commit-version <code>Vr &lt; Vc2 &lt; Vc</code> can hit the Resolver. In this case, the log servers will make sure to accept Vc2 first before accepting Vc, preserving linearizability. All messages in all log servers are sorted in LSN (See 2.4.2, &#34;Similarly, StorageServers pull log data from LogServers in increasing LSNs as well.&#34;).</p>
<p><em>Resolver</em> doesn&#39;t need locks to detect conflicts. That&#39;s why FDB claims itself to be &#34;lock-free&#34;. I found this mention of &#34;lock-free&#34; misleading. In order to realize strict serializability, locks must be used <em>somewhere</em> in the system to implement this total ordering. The <em>Sequencer</em> might have locks inside (not mentioned in the paper). Or even if there&#39;s only a single IO thread in the <em>Sequencer</em>, the kernel must use locks to maintain a queue of network packets. Even atomic memory operations are essentially implemented by locks in hardware. Without locks, ordering <em>can&#39;t</em> be achieved.</p>
<p>I know you are probably thinking about what if <em>Sequencer</em> or <em>Resolver</em> goes down. We will get there.</p>
<h2 id="loggingprotocol">Logging Protocol</h2>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-16-at-11.57.52-AM.png" alt="Screen-Shot-2021-07-16-at-11.57.52-AM" loading="lazy"/></p>
<p>FDB&#39;s logging protocol, at first glance, seems pretty straightforward. It has a group of logs, and a separate group of storage servers with distinct replication factors and topologies – another example of physical decoupling in FDB. If you dive a little deeper, you will notice that things are much more interesting.</p>
<p>There are multiple <em>LogServers</em>. Remember, FDB provides an abstraction of one logical shard (non-sharded) database. Commonly, people use logs for sequencing (e.g. Raft), in which case, each log is mapped to a logical shard. So how does FDB provide a single shard abstraction with multiple logs? The answer is <strong>FDB decouples <em>sequencing</em> from <em>logging</em></strong>. There is only one <em>Sequencer</em> in the system, which maps to the single shard abstraction (ordering). It enables FDB to have multiple logs, as they do <em>not</em> perform the job of ordering, which logs in other systems usually do. This design is just brilliant. Whoever performs the job of sequencing must be singular. You will get higher throughput and lower latency by having a dedicated role just doing sequencing, and <em>nothing</em> else. In contrast, systems like Raft couple sequencing and logging, which inherently pay a performance cost. In FDB, the <em>Sequencer</em> is physically a singleton – there&#39;s a single process. We will talk about failure recovery very soon. Here&#39;s what the logging protocol looks like,</p>
<ul>
<li><em>Proxy</em> broadcasts a message to <strong>all</strong> <em>LogServers</em>. Each message will have a common header including <code>LSN</code>, <code>prevLSN</code> and <code>KCV</code>. <code>KCV</code> is the last known committed version from the <em>Proxy</em>. It should always be less than the <code>LSN</code> (<code>KCV &lt; LSN</code>), as the transaction associated with <code>LSN</code> is still in progress (hence not committed). <code>KCV</code> captures an incomplete local view of the world, which will be used later for the recovery process.</li>
<li>Not all broadcast messages include data. E.g. key <code>A</code> is stored on storage shard <code>S</code>. <em>Coordinator</em> maintains an affinity map (each storage server has one preferred LogServer). In this case, data will only be included in the message sent to shard <code>S</code>&#39;s preferred LogServer (and potentially additional LogServers to satisfy replication requirements). All other messages to other LogServers will only include the common header.</li>
<li>Once the data is in the log, it will apply to the storage node. It&#39;s not super interesting, although there are some performance improvements mentioned in the paper.</li>
</ul>
<p>There are a few interesting details above. <em>Proxy</em> broadcasts a message to &#34;all&#34; logs. Why all logs instead of only the replicas for the key? Because all messages are applied in order on each log. There can be no gaps. Also, as you will see later, that it&#39;s critical for the recovery process. Another related question is why sending messages without data to other logs? This is a performance optimization, as only logs associated with the underlying storage nodes need to have data stored.</p>
<p>Because FDB decouples sequencing from logging, now committing to the logs becomes a distributed transaction problem – e.g. what if log-1 has some data stored, while log-2 doesn&#39;t, for an aborted transaction. Does FDB need to perform 2pc? No, actually.</p>
<p>FDB here basically adopts the <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiEw8fzoPLxAhX6KFkFHU1eB14QFjAAegQIBRAD&amp;url=http%3A%2F%2Fcs.yale.edu%2Fhomes%2Fthomson%2Fpublications%2Fcalvin-sigmod12.pdf&amp;usg=AOvVaw0XE8ck-WBihvUiGgAnzr2H">Calvin</a> approach. It&#39;s the same idea that Prof. Abadi <a href="https://dbmsmusings.blogspot.com/2019/01/its-time-to-move-on-from-two-phase.html">posted</a>. The keyword is <em>deterministic</em>. We don&#39;t need 2pc here, because <em>Proxy</em> writing to multiple logs is performed <em>blindly</em> (a.k.a unconditionally). Sure, errors can happen. If they are transient e.g. timeout, <em>Proxy</em> can always try again – it will never be blocked by another log commit. This is, by the way, why FDB can afford letting <em>Proxies</em> broadcast each message to <em>all</em> logs.</p>
<p>In this case, the only failure scenario that can actually block log commit and require rollback, is when <em>LogServers</em> actually fail (e.g. crash). We will talk about the recovery process soon.</p>
<p>Notice that in the diagram above, there are 5 logs and 10 storage nodes (and one <em>Sequencer</em> not in the diagram). <code>#StorageNode &gt; #Logs &gt; #Sequencer</code> is not a coincidence. A single <em>Sequencer</em> can support multiple logs, as a <em>Sequencer</em> is much more lightweight (not having to write to disk). A single log again can support multiple storage nodes, as logs are relatively cheaper than storage (you can prune logs, sequential writes are cheaper, etc.).</p>
<h2 id="recoveryprocess">Recovery Process</h2>
<p>Finally, the real fun begins. What happens if the <em>Sequencer</em> fails? <em>Resolvers</em> fail? <em>LogServers</em> fail?</p>
<ol>
<li>Failures are detected by the <em>ClusterController</em> (via heartbeats according to the <a href="https://github.com/apple/foundationdb/blob/master/design/recovery-internals.md">source</a>), who explicitly triggers the recovery process.</li>
<li>The <em>Sequencer</em> locks and reads the previous states from <em>Coordinators</em>. It can be a new <em>Sequencer</em> instance, running the recovery process, if the previous <em>Sequencer</em> is in a faulty state. That&#39;s why we need to lock the <em>Coordinator</em> here, to avoid multiple <em>Sequencers</em> making topology changes at the same time. The paper doesn&#39;t talk about what happens if the recovery Sequencer itself crashes, leaving the Coordinators locked forever. This is a classic consensus problem. Given the Coordinators are already running Paxos, this is not a hard problem to solve. E.g. You can think of each recovery itself being a Paxos Proposal. If a recovery Sequencer died halfway, another Sequencer can always come and discover the previous chosen value if any, or propose a new epoch – starting a new recovery process.</li>
<li>Previous states include information about all old <em>LogServers</em>. The <em>Sequencer</em> then stops old logs from accepting new transactions. The <em>Sequencer</em> knowing it&#39;s running a recovery process, obviously won&#39;t accept new transactions. But if there&#39;s another old <em>Sequencer</em> instance running, it can still accept new transactions. Hence we need to stop the old logs to avoid potential dataloss. Another reason for contacting the old <em>LogServers</em> is to discover the end of the redo log (the last committed LSN).</li>
<li>The <em>Sequencer</em> recruits a new set of <em>Proxies</em>, <em>Resolvers</em>, and <em>LogServers</em>.</li>
<li>Write the new system states to the <em>Coordinators</em>, and release the lock.</li>
<li>Finally, the new <em>Sequencer</em> starts accepting new transactions.</li>
</ol>
<p>Unlike other fault tolerance algorithms, e.g. Paxos, <strong>FDB&#39;s recovery process has <em>downtime</em></strong> – from the shutdown of old <em>LogServers</em> to when the new <em>Sequencer</em> starts accepting new transactions. One can argue that this is not really <em>fault tolerant</em>. I personally think it&#39;s totally fine to have partial downtime to heal a system (given a short one, which is the case for FDB), e.g. unavailability for a single shard. It&#39;s not like systems such as Paxos or Raft have 100% availability even when the simple majority of hosts are always up and running. However FDB has only <em>one</em> logical shard. Its recovery process causes downtime for the entire deployment and the entire key space.</p>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-19-at-11.33.17-AM.png" alt="Screen-Shot-2021-07-19-at-11.33.17-AM" loading="lazy"/></p>
<p>P50 downtime for FDB&#39;s recovery process is 3.08s. That means no one can write to FDB at all during this window. The paper argues clients can still read from storage nodes unaffected.</p>
<blockquote>
<p>During the recovery, read-write transactions were temporarily blocked and were retried after timeout. However, client reads were not impacted, because they are served by StorageServers.</p>
</blockquote>
<p>This statement seems not completely accurate. Snapshot reads are unaffected. But if <em>Sequencer</em> is down, no read-only transactions can be processed either. FDB is read-heavy. It&#39;s very reasonable to assume that most of the production read workloads are snapshot reads (instead of read-only transactions). Claiming minimum client visible impact is reasonable.</p>
<p>If a system allows downtime, it can tolerate a lot of replicas going down without affecting it&#39;s availability. E.g. With the simplest leader-follower setup, a single leader can have any number of followers (let&#39;s say 10). In this case, with a total replication factor of 11, in theory, we can tolerate up to 10 failures without a single second of downtime.</p>
<p><em>Proxies</em> are easy to recover, as they are mostly stateless (just keeping track of a <code>KCV</code> locally). <em>Resolvers</em> are stateful, but it&#39;s safe to discard all <em>Resolvers</em> data from the previous epoch. Because it&#39;s impossible to have conflicts spanning across two epochs, as you can&#39;t have read-write transactions that span two epochs (as a completely new set of Proxies are recruited). That leaves us with the logs and the <em>Sequencer</em>, which are harder to recover. We just need to know the last committed LSN from the old logs to start a new <em>Sequencer</em> instance. Let&#39;s take a closer look at the recovery process of the <em>LogServers</em>.</p>
<h4 id="logrecovery">Log Recovery</h4>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-20-at-3.15.38-PM.png" alt="Screen-Shot-2021-07-20-at-3.15.38-PM" loading="lazy"/></p>
<p>For a LogServer deployment with <em>m</em> hosts and replication factor <em>k</em>, here&#39;s how the algorithm looks like:</p>
<ul>
<li>Each log maintains <code>DV</code> (Durable Version, the maximum persisted LSN) and <code>KCV</code> (Known last committed version, the latest committed version known from the <em>Proxy</em>). Both <em>DV</em> and <em>KCV</em> are piggy-backed on each message from Proxy to the logs.</li>
<li>The <em>Sequencer</em> attempts to stop <em>all</em> old logs.</li>
<li>After hearing back from <em>m-k+1</em> logs, the <em>Sequencer</em> knows the previous epoch has committed transactions <strong>at least</strong> up to <code>max(KCVs)</code>. We call <code>max(KCVs)</code> here <code>PEV</code> (Previous Epoch&#39;s end Version). The new <em>Sequencer</em> will start its epoch from <code>PEV+1</code>.</li>
</ul>
<p>It seems pretty straightforward. However, the paper mentions another variable <code>RV</code> (Recovery Version) to be <code>min(DVs)</code>, and it talks about copying data between <code>[PEV+1, RV]</code> from the old logs to new logs and discarding logs after <em>RV</em>. This is for,</p>
<ol>
<li>providing failure atomicity when log commit fails partially (in which case, we need to perform rollback)</li>
<li>preserving (healing replication factor) for data that are committed at LSN &gt; PEV</li>
</ol>
<h4 id="proofofcorrectness">Proof of Correctness</h4>
<p><em>The paper didn&#39;t include any proof for the correctness of the recovery process. I will explain and prove its correctness here.</em></p>
<p>Notice that any log message arrived at the logs have already passed the conflict detection, hence safe to be durably stored. Especially given the fact that data stored might have already been read by another client, we can and should err on the side of preserving more of the old logs except when it&#39;s not safe to do so. We know for sure that all transactions before <em>PEV</em> are committed. We just need to find the <em>tail</em> of the redo log, where</p>
<ol>
<li>partial failures might be happening, for which we must rollback, and</li>
<li>no clients have ever seen the data yet (because it&#39;s the tail)</li>
</ol>
<p>Let&#39;s take a closer look at this diagram.</p>
<p><img src="https://blog.the-pans.com/content/images/2021/07/Screen-Shot-2021-07-20-at-11.16.41-PM.png" alt="Screen-Shot-2021-07-20-at-11.16.41-PM" loading="lazy"/></p>
<p>For it to be correct (that we can safely discard data after <em>RV</em> and preserve data before <em>RV</em>), the following conditions need to be met:</p>
<ol>
<li>DV<sub>m</sub>&#39;s cross-range-shard transaction cannot be partially committed (no rollback required).</li>
<li>No clients can read any data written after DV<sub>m</sub>.</li>
</ol>
<p>Let&#39;s try proof by contradiction on #1 and #2.</p>
<p>What if DM<sub>m</sub>&#39;s transaction were partially committed (specifically, rollback is required)? Notice that &#34;partial&#34; here specifically means cross-range-shard transaction, e.g. a transaction that sets both key A and key B, key A is stored with data in its logs, but key B is not. In this case, we have to rollback A. Logs can tolerate <em>k-1</em> failures (<em>k</em> being the data replication factor). If at least one surviving B replica has the data for transaction DM<sub>m</sub>, no rollback is required because we have all the data for T<sub>m</sub>, for which we will finish the commit process (heal the replication factor). Otherwise, let&#39;s say <em>k-1</em> B replicas that got DM<sub>m</sub> data crashed, there must be at least one B replica up and running that has not received the DM<sub>m</sub> message. On that log, its <em>DV</em> must be smaller than DM<sub>m</sub>. Contradiction.</p>
<p>Now let&#39;s move on to #2. Notice that <code>DV &gt;= KCV</code> is true locally for each log, because <code>LSN &gt; KCV</code> is true on each <em>Proxy</em>. Log local <em>DV</em> can sometimes be the same as <em>KCV</em> because reads from the <em>Proxies</em> don&#39;t set <em>DV</em> but can bring log local <em>KCV</em> up to date. Even better, <code>max(KCVs) &lt;= min(DVs)</code> should hold always, thanks to the fact that each log commit requires broadcasting the message to <em>all</em> logs. This answers our question earlier why Proxies need to broadcast each log message to <em>all</em> logs. Now it&#39;s obvious why #2 holds, because FDB can only serve reads based on <em>KCV</em> (and not <em>DV</em>). So for any <em>DV &gt; DV<sub>m</sub></em>, those transactions must not be committed.</p>
<p>With #1 and #2 combined, we can be confident that we can safely treat <em>RV</em> as the tail of the redo log. The presence of a <em>KCV</em> on a log, implies that the associated transaction is fully replicated (having <em>k</em> copies). It&#39;s not covered in the paper, but conceptually we can think of these messages having been durably flushed to storage nodes (safe to discard from the logs). For messages between <code>[PEV+1, RV]</code>, they <em>might</em> be committed transactions. It&#39;s safe to err on the side of treating them as committed, as all writes to the logs have already passed conflict detection. These cases would be indistinguishable from reply timeouts. But due to the lack of information, we don&#39;t know for sure if messages in the <code>[PEV+1, RV]</code> window have been durably stored in storage nodes or not. The easiest way to handle it is to start the new epoch at <em>PEV+1</em> and replay messages from <code>[PEV+1, RV]</code> from the old logs.</p>
<h2 id="replicationsummary">Replication Summary</h2>
<p>Given the extreme decoupling, each component has its own replication strategy.</p>
<ul>
<li><em>Coordinator</em> runs on Paxos, which can tolerate <em>f</em> failures with a cluster size of <em>2f+1</em>.</li>
<li><em>LogServers</em> can tolerate <em>f</em> failures for replication factor <em>f+1</em>, which is orthogonal to the number of servers. Storage nodes share a similar strategy. E.g. there can be a total of 5 logs, with a replication factor of 3, which can tolerate 2 failures.</li>
</ul>
<h2 id="thoseclaimsatthebeginning">Those Claims at the Beginning</h2>
<p>I think FDB is a great system, probably the best k/v store. However, I do think those bold claims at the beginning of the paper are, frankly, misleading.</p>
<ul>
<li>&#34;FDB provides strict serializability while being lock-free&#34;. Well, to be more precise, it&#39;s meant to say the conflict detection is lock-free. <em>Sequencer</em> is not lock-free for example.</li>
<li>&#34;For global FDB deployment, it can avoid x-region write latency.&#34; Well, it&#39;s based on semi-sync in a local region in a different failure domain. It requires assumptions about failure domains.</li>
<li>&#34;FDB can tolerate <em>f</em> failures with only <em>f+1</em> replicas.&#34; Well, this only applies to logs and storage nodes. If simply majority of the <em>Coordinator</em> fail, FDB is totally hosed for example.</li>
</ul>
<h2 id="summary">Summary</h2>
<p>FDB is probably the best k/v store for regional deployment out there. It&#39;s rock solid thanks to its simulation framework. The FDB team, in my opinion, made all the best design choices, which is just remarkable. The paper is dense. A lot of important details are only briefly mentioned or even omitted, which is probably due to the paper length limit. Congratulations to the FoundationDB team.</p>



</section>



</div>
</article>
</div></div>
  </body>
</html>
