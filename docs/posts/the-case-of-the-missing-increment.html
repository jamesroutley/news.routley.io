<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.computerenhance.com/p/the-case-of-the-missing-increment">Original</a>
    <h1>The Case of the Missing Increment</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png" width="1456" height="819" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:2505962,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8b38f99-6e0f-410b-be71-51e1c883487c_5120x2880.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a></figure></div><p><span>Computer Enhance was never meant to be a place for publishing novel investigations into software performance, but sometimes it happens by accident. We already saw this once when bonus materials for the </span><a href="https://www.computerenhance.com/p/table-of-contents" rel="">Performance-Aware Programming</a><span> course lead us to discover </span><a href="https://www.computerenhance.com/p/probing-os-page-fault-behavior" rel="">a previously undocumented 16-ahead mapping pattern</a><span> in the Windows memory manager.</span></p><p><span>In that same vein, on a specific set of Intel CPUs, some of our tutorial loops hit a </span><em>mostly</em><span> undocumented performance anomaly in the Golden Cove microarchitecture. I say “mostly” because — SPOILER WARNING — once I figured out what was happening, I </span><em>was</em><span> able to search for and find a single solitary hint about it in a writeup of one of Intel’s Architecture Day presentations.</span></p><p>But I’m getting ahead of myself.</p><p><span>The scene of this particular whodunnit may seem unlikely: the performance anomaly occurs in some of the </span><em>simplest</em><span> machine code we could consider. In fact, you might say it’s </span><em>oversimplified</em><span> code, since it doesn’t do any useful work. It’s entire raison d&#39;être was illustrating hardware behavior for educational purposes.</span></p><p><span>But in my experience, extremely simple machine code is actually one of the most common places to witness something strange. Since you’re handing the CPU a few highly-specific instructions without most of the code that would otherwise be interleaved, you’re pushing the boundaries of what its designers expected to encounter in real use. At this point, you’re </span><em>more</em><span> likely — not less — to feel the edges of the microarchitecture than you are in a more typical scenario.</span></p><p><span>The specific oversimplified code that kicks off our mystery comes from the course’s </span><a href="https://www.computerenhance.com/p/linking-directly-to-asm-for-experimentation" rel="">introductory video on how to link to ASM files for microbenchmarking</a><em>. </em><span>It wasn’t even a performance exploration!</span></p><p>But that’s fortunate for our purposes, because even if you haven’t taken Part I of the course yet, and don’t know how to read assembly language, the source code is simple enough that you can probably understand what it does anyway:</p><pre><code>.loop:
    inc rax
    cmp rax, rcx
    jb .loop</code></pre><p><span>This loop consists of just three instructions. In higher-level-language terminology, the first instruction — </span><code>inc rax</code><span> — adds one to a variable. The second — </span><code>cmp rax, rcx</code><span> — determines if that variable is less than the desired iteration count</span><span>. And the third — </span><code>jb .loop</code><span> — repeats the loop if it was.</span></p><p>In a higher-level language, you might write the same loop like this:</p><pre><code><span>do {                // .loop
    i++;            // inc rax
} while(i &lt; count); // cmp rax, rcx </span><em>combined with</em><span> jb .loop</span></code></pre><p><span>Any x64 chip can run this loop. It does not involve any fancy instructions from the AVX or AVX-512 instruction sets that might only exist on newer processors. In fact, it’s </span><em>almost </em><span>simple enough to run on the </span><em>original </em><span>8086/88</span><em>!</em><span> The only thing preventing it from assembling to 8086 code is that it happens to use 64-bit registers instead of 16-bit registers. Change the </span><code>rax</code><span> and </span><code>rcx</code><span> to </span><code>ax</code><span> and </span><code>cx</code><span> and it would assemble to valid 8086 code.</span></p><p><span>What’s more, not only is the loop itself simple to understand, but if you know </span><a href="https://www.computerenhance.com/p/cpu-front-end-basics" rel="">some</a><span> </span><a href="https://www.computerenhance.com/p/branch-prediction" rel="">modern</a><span> </span><a href="https://www.computerenhance.com/p/code-alignment" rel="">microarchitecture</a><span> </span><a href="https://www.computerenhance.com/p/the-rat-and-the-register-file" rel="">basics</a><span>, its </span><em>performance</em><span> is easy to understand — or at least, it would have been, if it weren’t for the anomaly we’re about to discuss.</span></p><p><span>There is a subtlety regarding </span><code>inc rax</code><span> and how the flags are updated, but it doesn’t actually affect this particular analysis, so you don’t need to know it to understand the mystery. In fact, flags are not really important here, so </span><strong>I will be omitting the flags tracking in the following breakdowns</strong><span> so they are easier to read. But please keep in mind that of course there </span><em>are</em><span> flags being generated and tracked through this loop, so if this </span><em>were</em><span> a scenario where flag operations affected the outcome, you would have to consider that.</span></p><p><span>That caveat aside, assuming we feed this loop a sufficiently high value for </span><code>rcx</code><span> (the count), the jump back to the top of the loop will occur the vast majority of the time. Only on the very last iteration will it fall through to the next block of code. This means the front-end of the CPU should correctly predict the repetition and feed the back-end a repeating series of microoperations that effectively encode this</span><span> (not counting the flag writes, as I just mentioned):</span></p><pre><code>    rax &lt;- rax + 1
    cmp rax, rcx / jb
    rax &lt;- rax + 1
    cmp rax, rcx / jb
    rax &lt;- rax + 1
    cmp rax, rcx / jb
    rax &lt;- rax + 1
    cmp rax, rcx / jb
    ...</code></pre><p><span>Since register names are just that — names — the renaming stage will turn this into something conceptually similar to </span><a href="https://en.wikipedia.org/wiki/Static_single-assignment_form" rel="">Single Static Assignment form</a><span>:</span></p><pre><code>    g2 &lt;- g1 + 1
    cmp g2, g0 / jb
    g3 &lt;- g2 + 1
    cmp g3, g0 / jb
    g4 &lt;- g3 + 1
    cmp g4, g0 / jb
    g5 &lt;- g4 + 1
    cmp g5, g0 / jb
    ...</code></pre><p><span>where the numbered </span><code>g</code><span>’s are register file entries for general purpose registers. Since </span><code>rcx</code><strong> </strong><span>(here depicted as residing in </span><code>g0</code><span>) does not change, the dependency structure for this loop is entirely based on the increment. For every increment, there is a pair of microoperations that must wait for it to complete, one of which is the </span><em>next</em><span> increment:</span></p><pre><code>    g2 &lt;- g1 + 1

    // Must wait for g2
    cmp g2, g0 / jb
    g3 &lt;- g2 + 1

    // Must wait for g3
    cmp g3, g0 / jb
    g4 &lt;- g3 + 1

    // Must wait for g4
    cmp g4, g0 / jb
    g5 &lt;- g4 + 1

    // Must wait for g5
    cmp g5, g0 / jb
    ...</code></pre><p><span>This leads us to our straightforward steady-state expectation for the performance of this loop. Because the back-end must wait for the result of a previous increment before performing each pair of dependent microoperations, and the fastest possible delivery of a result is one core clock cycle, we would expect this loop to have a maximum throughput of </span><em>one core clock cycle</em><span> per iteration.</span></p><p><span>So, for example, if we have a processor with a boost clock of 5GHz, we would expect it perform </span><em>no more than</em><span> five billion iterations of this loop per second: one iteration for every core clock cycle it observes.</span></p><p>If we measure the performance of this loop on a variety of x64 microarchitectures, we get exactly the performance we expect. The Skylake, Zen 2, and Zen 4 processors we have here at the office all run this loop at one cycle per iteration when measured.</p><p><span>Furthermore, if we run </span><em>simulations</em><span> of this loop using </span><a href="https://uica.uops.info/" rel="">the best publicly available Intel microarchitecture simulator</a><span>, the simulator predicts it will take one cycle for each iteration on every Intel architecture it supports: Sandy Bridge, Ivy Bridge, Haswell, Broadwell, Skylake, Skylake-X, Kaby Lake, Coffee Lake, Cascade Lake, Ice Lake, Tiger Lake, and Rocket Lake.</span></p><p>Everything checks out. So what’s the problem?</p><p><span>As I learned 20+ years ago at </span><a href="https://www.radgametools.com/" rel="">RAD Game Tools</a><span>, you learn about all sorts of weird PC hardware anomalies when you ship to a wide audience. Unless you have regular access to a massive testing lab covering thousands of combinations of PC hardware, there’s no way you can foresee some of the crazy things that happen in the wild. So I fully expected to have a few weird things come up when people in the </span><a href="https://www.computerenhance.com/p/table-of-contents" rel="">Performance-Aware Programming</a><span> course started doing their own microbenchmarking.</span></p><p><span>So far, there have only been two: one is an as-yet-uninvestigated Kaby Lake memory throughput anomaly</span><span>, and the other is the Alder Lake performance of the loop I described in the previous section.</span></p><p><span>Specifically, the machine I used to reproduce the anomaly was an </span><a href="https://www.intel.com/content/www/us/en/products/sku/134594/intel-core-i712700k-processor-25m-cache-up-to-5-00-ghz/specifications.html" rel="">i7-12700K</a><span>. This is an Alder Lake processor with a maximum boost clock of 5GHz. Following the logic from the previous section, if you run our simple </span><code>inc</code><span>/</span><code>cmp</code><span>/</span><code>jb</code><span> loop on this processor, it should max out at five billion iterations per second — again, one core cycle for every iteration. Based on our analysis, we would expect anything more than that to be both practically </span><em>and theoretically</em><span> impossible on this processor.</span></p><p>By basic math, we can also say the theoretical maximum for any number of iterations. Three billion iterations should take three billion cycles. Two billion iterations? Two billion cycles. And so on.</p><p>In the particular case we tested in the course, we were running 1,073,741,824 iterations. While that may seem like an oddly specific number, it’s just the number of bytes in a gigabyte of memory (2^30, or 1024*1024*1024, or however you’d like to think of it).</p><p><span>When run on my Skylake processor with boost clock disabled, this test took 1,074,730,560 cycles as measured by the chip-wide timestamp counter (</span><a href="https://www.computerenhance.com/p/introduction-to-rdtsc" rel="">read via rdtsc</a><span>). This unavoidably includes a little bit of inaccuracy at the edges, since normally execution is highly overlapped, so it’s hard to declare a performance measurement in isolation. But even so, the results line up extremely well with our expectations: 1,074,730,560 cycles divided by 1,073,741,824 iterations is approximately 1.0009 cycles per iteration — very close to 1.</span></p><p>However, when run on the Alder Lake i7-12700K, we get something rather different: the timestamp counter reports only 391,416,518 cycles elapsed. 391,416,518 cycles divided by 1,073,741,824 iterations is 0.364535 cycles per iteration — that’s 2.7x faster than should be theoretically possible!</p><p><span>At first glance, this is not immediately concerning. If you’ve done a lot of microbenchmarking before, you know that on modern chips the </span><code>rdtsc</code><span>-read timestamp counter counts cycles at the base clock rate and </span><em>does not account</em><span> for things like single-core clock boosting. So your first thought is probably “the boost clock must be around three times the base clock on this processor”.</span></p><p>But is it? That seems like a pretty high boost clock ratio, no?</p><p><span>If we </span><a href="https://www.intel.com/content/www/us/en/products/sku/134594/intel-core-i712700k-processor-25m-cache-up-to-5-00-ghz/specifications.html" rel="">look back at the spec sheet</a><span>, we find that the base clock is 3.60GHz for the P-cores and 2.70GHz for the E-cores, while the max boost clock is 5.00GHz for the P-cores, and 3.80GHz for the E-cores. In order for there to be a 2.7x multiplier between the base clock and the boost clock, even taking the maximum boost of 5Ghz, we would expect a base clock value of around 1.8Ghz — nowhere near even the E-core base clock, let alone the P-core base clock.</span></p><p><span>What’s more, if you actually </span><em>measure</em><span> the number of timestamp counter cycles that occur in one second as observed by the core running the test, you get right around 3.6GHz. So we more-or-less know that we’re running on a P-core, that its TSC is running at 3.6GHz, and that the maximum multiplier we would expect from boosting is therefore 5GHz/3.6GHz.</span></p><p><span>Unfortunately, that’s 1.4x — approximately </span><em>half </em><span>of the 2.7x observed in practice.</span></p><p>So even taking the boost clock into account, we have no way to explain this anomaly. Just to throw the ratios at you one other way, for our 1,073,741,824 iterations we are seeing 391,416,518 timestamps elapse out of 3,600,000,000 each second. That means the test is taking 0.108727 seconds total. If the CPU were in fact taking 1 (boosted) core cycle per iteration at 5GHz, that would be 1,073,741,824 core cycles out of 5,000,000,000 each second, or 0.214748 seconds.</p><p><span>In other words, at max boost, we would expect our test to take 0.214748 seconds, but it only takes 0.108727 seconds when we measure it. That’s </span><em>double</em><span> the predicted performance. Unless we’ve done something wrong, Alder Lake </span><em>appears</em><span> to run our loop at </span><em>twice</em><span> the theoretical maximum!</span></p><p><span>That simply can’t be right, can it? I don’t know about you, but I certainly assumed it couldn’t be, As a Windows developer, that made me feel a certain sense of dread, because at that point, I had the sinking feeling I might once again be forced to experience… </span><em>The Ultimate Sadness</em><span>.</span></p><p><span>For those who don’t know, The Ultimate Sadness is the category of programming tasks that involve things like being forced to use XML as a data format. It’s that moment when you have a programming problem that would be trivial to solve if the underlying substrate was sane, but instead it seems as if someone designed it </span><em>specifically </em><span>to make your life miserable.</span></p><p><span>In this particular case, the substrate in question is </span><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/event-tracing-for-windows--etw-" rel="">Event Tracing for Windows</a><span>, AKA </span><a href="https://caseymuratori.com/blog_0025" rel="">the worst API ever made</a><span>.</span></p><p><span>The reason Windows developers often look at performance using bare </span><code>rdtsc</code><span> readings isn’t because that’s the best way to measure things. </span><code>rdtsc</code><span> on modern processors is really just a glorified wall clock. We would </span><em>much</em><span> prefer to employ the whole suite of detailed performance monitoring counters (or “PMCs” for short) present in modern x64 chips.</span></p><p><span>But as Windows developers, we usually don’t, because stock Windows makes it </span><em>a massive pain in the butt</em><span> to read them. While </span><code>rdtsc</code><span> is convenient to call at any time from a user-level application, there is no similarly convenient way for a user-level application to read a whole host of other valuable PMCs offered by x64 processors.</span></p><p>Broadly speaking, Windows developers only get three options:</p><ol><li><p>Install a third-party suite that comes with a kernel-level driver for PMC collection (VTune, Intel Performance Counter Monitor, etc.)</p></li><li><p>Port to an environment that has better native PMC access (Linux, direct boot from BIOS, etc.)</p></li><li><p>Suffer The Ultimate Sadness that is Event Tracing for Windows, either by directly calling the API, or using some combination of front-end tools (wpr, xperf, tracelog, etc.)</p></li></ol><p><span>No matter what option you pick, you’re looking at something substantially more annoying than the ten seconds it takes to type </span><code>__rdtsc()</code><span> into your code</span><span>. Since the only Alder Lake machine I had access to was a remote Windows machine that didn’t belong to me, I more-or-less had to choose option 3, which meant subjecting myself to The Ultimate Sadness.</span></p><p>In an effort to avoid spreading The Ultimate Sadness to all of you, I’ll spare you the gory details and get right to the data. Using the needlessly-limited functionality of ETW, we can indirectly collect two additional PMC counters that will give us a more precise picture of what is going on when Alder Lake runs this loop.</p><p><span>The first counter is exposed as “TotalIssues” by ETW, and counts the number of instructions a core believes it has issued.  The second counter is exposed as “UnhaltedCoreCycles” — or equivalently for this purpose we could use the PMC exposed under the ETW name “TotalCycles” — and counts the number of clock cycles the core itself has gone through (not the invariant TSC cycles, but the core’s </span><em>actual</em><span> cycles, including boosting).</span></p><p>If we collect these counters while running our loop on my boost-disabled Skylake, we get deltas in line with what we expect:</p><pre><code>1073970036 TSC elapsed / 1073741824 iterations [0 switches]
  3221373030 TotalIssues
  1073970333 TotalCycles
  1073970331 UnhaltedCoreCycles</code></pre><p>The number of cycles observed by the core is approximately equal to the number of timestamps (TSC) elapsed, just like we expect when the core is not allowed to boost. The number of instructions issued is roughly three times (~2.999500x) the number of cycles observed, which is also what we expect if each iteration took one cycle: there are three instructions in the loop, so if on average they execute in one cycle, there should be three issues per cycle.</p><p>But what happens if we collect the same counters from the Alder Lake machine?</p><pre><code>388096288 TSC elapsed / 1073741824 iterations [0 switches]
  3221327798 TotalIssues
   536959840 TotalCycles
   536959830 UnhaltedCoreCycles</code></pre><p><span>Unlike the Skylake machine, the TSC elapsed differs from the cycles elapsed, but that’s expected since we haven’t disabled boost on this machine. The ratio of cycles to TSCs is ~1.383573x, which is right in line with the expected maximum boost clock ratio of 5.0GHz vs. 3.6GHz (~1.388889x). So our PMC collection certainly </span><em>seems</em><span> correct, and confirms that something close to the maximum boost clock is kicking in — no surprises there.</span></p><p><span>But then we get to the “TotalIssues” number. Again, the theoretical maximum issues per cycle for this loop should be three, and it </span><em>was </em><span>three on Skylake.</span></p><p><span>Here on Alder Lake, however, we get 3,221,327,798 issues in 536,959,840 core cycles — an issue rate of </span><em>six instructions per cycle</em><span> (~5.999197x). Instead of disproving our previous results, this appears to completely confirm them: according to the PMCs, Alder Lake </span><em>really is</em><span> processing two iterations of our loop every core clock cycle.</span></p><p><span>Unless it’s just straight-up lying, the core is legitimately operating at </span><em>double</em><span> the expected maximum throughput.</span></p><p><span>Alder Lake P-cores use a microarchitecture Intel calls </span><a href="https://en.wikipedia.org/wiki/Golden_Cove" rel="">Golden Cove</a><span>. Since we’re clearly observing the loop run at a boost clock near 5GHz, there is no doubt the measurements we’re taking are on one of these Golden Cove P-cores, not one of the less-powerful E-cores.</span></p><p><span>In one limited sense, then, our results </span><em>aren’t</em><span> surprising: Golden Cove cores </span><em>are</em><span> supposed to be able to execute six instructions per cycle. That’s their theoretical maximum instruction throughput. So at least the core isn’t violating its basic operating parameters as stated by Intel.</span></p><p><span>But in the broader sense, these results </span><em>are</em><span> rather surprising. Unless I made a serious error in collecting the data — which is always a possibility with delicate testing like this — we now have proof that a Golden Cove core can perform </span><em>two serially dependent increments </em><span>on a single clock cycle. While in theory there’s no reason a core couldn’t have sufficient logic to produce that result, I did not know that was possible on any x64 core until this experience.</span></p><p><span>Specifically, a sustained throughput of 6 instructions per cycle for this loop means that 2 of those 6 instructions are increments of </span><code>rax</code><span>:</span></p><pre><code>inc rax ; [A]
inc rax ; [B]</code></pre><p>Again, written out more explicitly, that would be:</p><pre><code><code>g2 &lt;- g1 + 1 ; [A]
g3 &lt;- g2 + 1 ; [B]</code></code></pre><p><span>Normally, to execute </span><code>[B]</code><span>, the result </span><code>g2</code><span> has to be forwarded from the execution unit computing </span><code>[A]</code><span> to the execution unit that will compute </span><code>[B]</code><span>, because </span><code>[B]</code><span> requires </span><code>g2</code><span> as an input and it is not known until </span><code>[A]</code><span> computes it. This is why we expect to wait </span><em>at least </em><span>one cycle per increment: it should take at least that long to compute and forward the result from the </span><em>previous</em><span> increment.</span></p><p><span>However, we are clearly seeing </span><em>both</em><span> </span><code>[A]</code><span> </span><em>and</em><span> </span><code>[B]</code><span> complete in the same cycle. Something very unexpected is happening here.</span></p><p>All of the computation-unit-based explanations I could think of would be extraordinarily weird and probably can’t be true. These would be nonsense explanations like the adder ALUs and scheduler having a magic mode where they operate twice in one clock cycle; two ALUs being combined to do one double-increment while optionally writing back two sets of flags; the core boosting to 10GHz but the PMCs still cap at 5GHz for some other reason; etc.</p><p>I’m not a hardware designer, so my imagination is quite limited here. Maybe there are more obvious ways you would do this that I just don’t know. But, unless there are, I don’t see a simple explanation for how the scheduling and execution parts of the core could produce this result by themselves.</p><p><span>Which leaves the front-end and the rename/elimination parts of the core. Here, I </span><em>was</em><span> able to come up with an explanation that </span><em>wasn’t</em><span> completely ridiculous.</span></p><p><span>The idea is that perhaps, without making a big deal about it, Intel added the ability </span><em>for the rename stage itself </em><span>to adjust immediate addition operations in order to remove dependencies</span><em>. </em><span>If that were the case, it would be possible, pre-scheduling, for the core to rewrite this:</span></p><pre><code><code>inc rax ; [A]
inc rax ; [B]</code></code></pre><p>not only in the way we would normally expect:</p><pre><code><code>g2 &lt;- g1 + 1 ; [A]
g3 &lt;- g2 + 1 ; [B]</code></code></pre><p>but also optionally in a less-dependent form:</p><pre><code><code>g2 &lt;- g1 + 1     ; [A]
g3 &lt;- (g1+1) + 1 ; [B]</code></code></pre><p><span>This way, instead of only </span><em>one</em><span> increment becoming ready to execute on production of a result, </span><em>two</em><span> become ready instead. As soon as </span><code>g1</code><span> is computed, </span><em>both</em><span> </span><code>[A]</code><span> and </span><code>[B]</code><span> can begin executing, instead of </span><code>[B]</code><span> stalling until </span><code>[A]</code><span> completes.</span></p><p>Furthermore, it seems plausible that — if this feature were restricted to operating between two or more add-immediate instructions — it could be done entirely at rename time just by altering the constants. Thanks to associativity, the renamer could reassociate this:</p><pre><code><code>g3 &lt;- (g1+1) + 1 ; [B]</code></code></pre><p>like this:</p><pre><code><code>g3 &lt;- g1 + (1 + 1) ; [B]</code></code></pre><p>which simplifies to this:</p><pre><code><code>g3 &lt;- g1 + 2 ; [B]</code></code></pre><p>That would be no more difficult to represent in the pipeline than any other immediate addition. Nothing downstream of the renamer would have to change.</p><p><span>I have no idea how plausible any of this is, but it at least </span><em>sounds</em><span> plausible to me, which is more than I can say for any other explanation I came up with.</span></p><p><span>Armed with both a reasonable certainty that the CPU </span><em>was</em><span> sustaining two iterations per core clock cycle, and a plausible theory about how that might happen, I tried again to find a documentation of this Golden Cove capability somewhere.</span></p><p><span>I’d also done this at the outset, of course, but found nothing. The </span><a href="https://www.agner.org/optimize/microarchitecture.pdf" rel="">Agner Fog microarchitecture manual</a><span> doesn’t mention anything like this, and doesn’t seem to cover Golden Cove in much detail. The </span><a href="https://www.intel.com/content/www/us/en/content-details/671488/intel-64-and-ia-32-architectures-optimization-reference-manual-volume-1.html" rel="">Intel optimization manual</a><span> does list a number of enhancements for Golden Cove, but doesn’t mention anything like this.</span></p><p><span>However, once I suspected I was looking for something involving optimized immediate addition, after a fair bit of search term juggling I was finally able to find one — yes, </span><em>just one</em><span> — possible confirmation that my theory is correct. It comes from </span><a href="https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity/5" rel="">an AnandTech</a><a href="https://www.anandtech.com/show/17047/the-intel-12th-gen-core-i912900k-review-hybrid-performance-brings-hybrid-complexity/5" rel=""> writeup</a><span> of a slide presentation Intel gave on one of their “Architecture Days”. It appears to be downstream of the following slide, where the “Smarter” box claims that more instructions are “executed” at the “rename / allocation” stage:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg" width="1456" height="791" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:791,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F804b1c45-11d7-4fec-9fbe-5b74f2c1a7f5_1840x1000.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>While that doesn’t really tell us much, AnandTech must have received information from Intel privately afterward — or seen some additional public disclosure that I haven’t been able to find — because they included this more detailed elaboration in their writeup:</p><p><span>Unfortunately, I still have </span><em>no idea</em><span> where they got this information. I went back and watched the Architecture Day video myself, and all the presenter says is what’s written on the slide: some instructions are now “executed” at the rename / allocation stage.</span></p><p><span>Given the ambiguity here, I would consider this mystery </span><em>mostly</em><span> solved, but not quite. We now know essentially what is happening, and why. But since I can’t find a complete description of how this new feature works, a piece of the puzzle remains missing.</span></p><p><span>Specifically, AnandTech used the phrase “treated as NOPs”. If that’s accurate, then the hypothetical mechanism I described might not be correct. My proposed method doesn’t </span><em>eliminate</em><span> the immediate additions — it just </span><em>rebalances</em><span> them to shorten the length of dependency chains. In my hypothetical model, this:</span></p><pre><code><code>inc rax
inc rax
inc rax
inc rax</code></code></pre><p>at best becomes this:</p><pre><code><code>g2 &lt;- g1 + 1
g3 &lt;- g1 + 2
g4 &lt;- g1 + 3
g5 &lt;- g1 + 4</code></code></pre><p>which is still 4 micro-operations — the same number it would have taken without the immediate folding. The benefit is solely that all four micro-ops can now run in parallel, whereas previously they would have had to be serialized. At no point did any increment turn into a NOP!</p><p><span>Of course, it’s possible that </span><em>both</em><span> my hypothesis </span><em>and</em><span> AnandTech’s statement are true. The reference to NOPs could be talking about a subsequent step that happens </span><em>after</em><span> the step I proposed. If the allocator notices that a register name has been overwritten, and the result of a not-yet-scheduling addition can therefore never be referenced, it could perform a kind of dead-code elimination and discard the operation ahead of time.</span></p><p><span>For example, in the four-micro-op sequence above, if there were no uses of </span><code>rax</code><span> in-between increments, then as it renamed the </span><code>inc</code><span>s it could notice that the results of the </span><code>g2</code><span>, then </span><code>g3</code><span>, then </span><code>g4</code><span> micro-ops could never be used. If it noticed this early enough, it could discard some of them before ever sending them to the scheduling queues.</span></p><p><span>That might be plausible, but, I’m way outside my area of expertise here. I’m really just guessing. I do software, not hardware, so I would much prefer to find some </span><em>actual</em><span> documentation on this new microarchitectural feature!</span></p><p>Absent that, I do think it would be possible to construct a series of microbenchmarks — hopefully using Linux so one could get easier and more extensive PMC access — that taken together would suggest what’s going on internally. It’s not hard to imagine feeding Golden Cove a series of slightly different instruction sequences to see exactly what it can and can’t do with immediate additions.</p><p>Unfortunately, that process would require permanent access to an Alder Lake machine, which I do not have. So, sadly, it will have to fall to someone else out there who likes poking at x64 cores!</p><p><em>If you enjoyed this article, and would like to receive more like it, you can put your email address in the box below to access both our free and paid subscription options:</em></p><p><span>I would once again like to thank Mārtiņš Možeiko’s for fearlessly producing great reference material for some of the world’s most finicky tools and APIs. While investigating this anomaly, I wrote some new profiling code that streamlined the process of collecting PMCs from ETW. In doing so, I was helped tremendously by Mārtiņš </span><a href="https://gist.github.com/mmozeiko/bd5923bcd9d20b5b9946691932ec95fa" rel="">miniperf reference code</a><span>. ETW is incredibly flakey, and often fails to work for undocumented or convoluted system configuration reasons. Having miniperf to work from and check against once was of the few things that allowed me to stay (mostly) sane in the process.</span></p></div></div></div>
  </body>
</html>
