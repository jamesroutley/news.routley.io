<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mobiusml.github.io/1bit_blog/">Original</a>
    <h1>Towards 1-bit Machine Learning Models</h1>
    
    <div id="readability-page-1" class="page"><div>
					<!-- <p class="page-description"><img src="./baby_aana.png" /></p> -->
					<figure><a href="https://mobiusml.github.io/1bit_blog/figs/aana_1bit.png"><img src="https://mobiusml.github.io/1bit_blog/figs/aana_1bit.png"/></a>
					</figure>
					<p>
						<strong><strong><strong><strong><strong><strong><strong>Table of
													Contents</strong></strong></strong></strong></strong></strong></strong>
					</p>
					<nav>
						
												
												
						
						
						

							<hr/>

							<p> Test it out at </p>
							<!-- <ul style="list-style-type:none;"> -->
							<p><a target="_blank" href="https://huggingface.co/collections/mobiuslabsgmbh/llama2-7b-hqq-6604257a96fc8b9c4e13e0fe">
								<img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.svg" alt="huggingface Model" width="120"/></a></p>
							<p><a target="_blank" href="https://colab.research.google.com/drive/15A6sVvdLqL654Td3vOe6QLnJiNZ7d-lF?usp=sharing">
								<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
							  </a></p>
							
							<!-- <div class="table_of_contents-item table_of_contents-indent-0"> <small>Support code is
									available at <a href="https://github.com/mobiusml/hqq"><mark
											class="highlight-gray">https://github.com/mobiusml/hqq</mark></a></small>
							</div> -->
						
						<hr/>

					</nav>

				</div><div>
					<h2 id="intro">Introduction</h2>
					<p>Quantizing small pre-trained models at extremely low bit-widths presents a significant challenge. While we have demonstrated that larger models, like <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral</a>, perform well with <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ">2-bit quantization</a>, smaller models, such as the popular <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama2-7B</a>, struggle at such extreme quantization levels. Furthermore, the quality deteriorates significantly with 1-bit quantization. </p>

						<p>The aim of this experiment is to demonstrate to the community the expected outcomes when fine-tuning such models under the most extreme quantization settings. To our surprise, fine-tuning just a fraction of the parameters (approximately 0.65%) significantly improves the output quality. Specifically, we observe that:</p>
					
					<ul>
						<li><b>1-bit</b>: directly applying 1-bit quantization to small models like the Llama2-7B yields suboptimal results. However, when the model is fine-tuned, its output quality improves substantially. Remarkably, the fine-tuned 1-bit base model surpasses the performance of <a href="https://arxiv.org/abs/2402.04396">Quip# 2-bit</a>, despite being trained on only ~2.8K samples with a context-window of 1024.</li>	</ul>
					
					<h2 id="hqq">Efficient Matmul for Low-Bit Quantization</h2>

					<p>HQQ&#39;s dequantization step is a linear operation, requiring both a scaling and a zero-point parameter. We show in this section how to rewrite the dequantization step in a way to directly take advantage of extreme low-bit matrix multiplication.
					</p>

					<h3 id="hqq_formula1">Rethinking Dequantization</h3>
					<p><a href="https://mobiusml.github.io/hqq_blog/">The dequantization step in HQQ</a> can be expressed as \( W_{r} = (W_{q} - z)s \), where \( W_{r} \) represents the dequantized weights, \( W_{q} \) the quantized weights, and the meta-parameters \( z \) and \( s \) correspond to the zero-point and scaling factor vectors, respectively. To simplify the explanation in this article, we omit the reshaping steps that are necessary when employing grouping.</p>

	
					<p><img src="https://mobiusml.github.io/1bit_blog/figs/quantization_op.png"/></p><p>
						The matrix multiplication operation in the forward pass (ignoring the bias term) becomes:
						$$ xW_{r} = x((W_{q} - z)s). $$ 
					</p>
					
					<p>
						To leverage the low-bit matrix multiplication, we need to separate \( xW_{q} \) from the rest. We can rewrite the operation as follows:
						$$ xW_{r} = x(W_{q}s + u), $$
						where \( u=-z \odot s \) and \( \odot \) denotes point-wise multiplication (Hadamard product). Note that since \( u \) is a vector, we cannot directly perform matrix multiplication between \( x \) and \( u \). However, it can be formulated as a rank-1 matrix multiplication:
						

						$$ xW_{r} = x(W_{q})s + x\mathbf{1}^{T}u.  ~~~~~~~~~ \style{font-size:50%}{\rm (Eq.1)} $$ 
					</p>
					
					<p>
						In both the 1-bit and 2-bit settings, matrix multiplication with the quantized weights can be implemented as additions and does not require actual multiplication:
					</p>
					
					<ol>
						<li>In the case of binary weights, \( W_{q} \) consists of \( 0 \)s and \( 1 \)s, requiring only additions.</li>
						<li>In the case of 2-bit quantization, we can rewrite \( W_{q} \) as the sum of a binary and a ternary matrix, both of which can fully take advantage of multiplication-free matmul and can be implemented in a fused kernel. The only change required is to use the \([-1,0,1,2]\) range instead of the original \([0,1,2,3]\) one:
						$$ \underset{\text{2bit}}{\left[\begin{array}{cc}
						2 &amp; 1\\
						0 &amp; -1
						\end{array}\right]} = \underset{\text{binary}}{\left[\begin{array}{cc}
						1 &amp; 1\\
						0 &amp; 0
						\end{array}\right]} + \underset{\text{ternary}}{\left[\begin{array}{cc}
						1 &amp; 0\\
						0 &amp; -1
						\end{array}\right]} $$
						</li>
					</ol>
					


					<h2 id="training">Fine-tuning with Low-Rank Adapters</h2>
					<p>
					Methods like BitNet train the full network from scratch. Instead, we follow the direction of training low-rank adapters (<a href="https://arxiv.org/abs/2106.09685">LoRA</a>/<a href="https://arxiv.org/abs/2305.14314">QLoRA</a>), which is currently the most popular way for fine-tuning large models. </p>

					<p>As the rightmost term in Eq. 1 indicates that the zero-point acts as a rank-1 matrix error correction term between \( W_{q}s \) and the original weights, the low-rank adapter essentially increases the rank of this correction term, leading to better quantization results.
					</p>

					<!-- <p>
					<a href="https://arxiv.org/abs/2106.09685">LoRA</a> and <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> are currently the most popular ways of fine-tuning large models. The reformulation above essentially means that the zero-point acts as a rank-1 matrix error correction term. We can improve the quality of quantization by increasing the rank of the correction term, which can be achieved via QLoRA fine-tuning.</p> -->

					<p>Let \({L_{A}}\) and \({L_{B}}\) be the low-rank adapter parameters of rank r, the matrix multiplication operation in the forward pass becomes:</p><p>

					$$ x(W_{q})s + x\mathbf{1}^{T}u + xL_{A}^{T}L_{B} $$

					</p><p>As detailed in our previous work on <a href="https://mobiusml.github.io/low-rank-llama2/">low-rank Llama pruning</a>, the rank of the sum of two matrices is lower or equal than the sum of their ranks. Therefore, \( x\mathbf{1}^{T}u + xL_{A}^{T}L_{B} \) can be merged as a rank r+1 term to get:
					$$ x(W_{q})s + x\bar{L_{A}}^{T}\bar{L_{B}}, $$
					where \({ \bar{L_{A}} }\) and \({ \bar{L_{B}} }\) are obtained via a low-rank decomposition of the matrix \(\mathbf{1}^{T}u + L_{A}^{T}L_{B}\).</p>


					<h2 id="datasets">Datasets</h2>
					<p>The low-rank adapters were trained using Supervised Fine-Tuning (SFT) on various open-source datasets. Different datasets were used for the base model and the chat model. Details are provided below:</p>

					<h3>Base Model</h3>
					<ol>
						<li>wikitext-2-raw-v1 (~2.8K): This dataset was used in its entirety to fine-tune the base model, providing a foundation of general language 
							understanding.</li>
					</ol>


					<h3>Chat Model</h3>

					<ol>
					<li><a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco">timdettmers/openassistant-guanaco</a>: The full dataset was utilized to fine-tune the chat model.</li>
					<li><a href="https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k">microsoft/orca-math-word-problems-200k</a>: A subset from this dataset was used to enhance the model&#39;s ability to solve mathematical word problems. </li>
					<li><a href="https://huggingface.co/datasets/meta-math/MetaMathQA">meta-math/MetaMathQA</a>: Another subset from this dataset was employed to further improve the model&#39;s mathematical reasoning capabilities. </li>
					<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized">HuggingFaceH4/ultrafeedback_binarized (chosen answers only)</a>: A subset of the chosen answers from this dataset was used to fine-tune the model&#39;s ability to generate coherent and relevant responses. </li>		
					</ol>
					<p>In terms of subset size, we used randomly selected 10K samples for the 2-bit and 25K samples for the 1-bit model.</p>

					<h2 id="benchmark">Benchmarks</h2>
					<p>We compared the performance of the Llama2-7B model in three configurations: FP16 (full precision), HQQ (without fine-tuning), and HQQ+ (with adapter layers) using a group-size of 8. The Llama2-7B model was chosen for these experiments because of its relatively smaller size, well-known architecture, and ease of experimentation. We evaluated the performance of both the pretrained base model and the chat model.</p>

					<h3>Base Models</h3>

					<p>For the base models, we included the results of <a href="https://arxiv.org/abs/2402.04396">Quip#</a> (2-bit), a state-of-the-art quantization method by Tseng et al. We were unable to find a functioning 1-bit model other than ours for Llama-7b; however, we will include the 2-bit Quip# results as a reference.</p>

					<h4>1-bit Model</h4>
					<table>
						<tbody><tr>
						  <th>Models</th>
						  <th>FP16</th>
						  <th>HQQ (1-bit)</th>
						  <th>HQQ+ (1-bit)</th>
						  <th>Quip# (2-bit)</th>
						</tr>
						<tr>
						  <td>Wiki Perplexity</td>
						  <td>5.18</td>
						  <td>9866</td>
						  <td><b>8.53</b></td>
						  <td>8.54</td>
						</tr>
						<tr>
						  <td>VRAM (GB)</td>
						  <td>13.5</td>
						  <td><b>1.76</b></td>
						  <td>1.85</td>
						  <td>2.72</td>
						</tr>
						<tr>
						  <td>forward time (sec)</td>
						  <td><b>0.1</b></td>
						  <td>0.231</td>
						  <td>0.257</td>
						  <td>0.353</td>
						</tr>
					  </tbody></table>

					<p>1-bit quantization led to a significant quality loss compared to the full-precision model, rendering it almost unusable. However, with the introduction of adapter layers, the HQQ+ 1-bit model reduced its perplexity to 8.53, making it slightly better and comparable to the Quip# 2-bit model, which has a perplexity of 8.54, despite having only binary weights.</p>

					<h4>2-bit Model</h4>

					<table>
						<tbody><tr>
						  <th>Models</th>
						  <th>FP16</th>
						  <th>HQQ (2-bit)</th>
						  <th>HQQ+ (2-bit)</th>
						  <th>Quip# (2-bit)</th>
						</tr>
						<tr>
						  <td>Wiki Perplexity</td>
						  <td>5.18</td>
						  <td>6.06</td>
						  <td><b>5.14</b></td>
						  <td>8.54</td>
						</tr>
						<tr>
						  <td>VRAM (GB)</td>
						  <td>13.5</td>
						  <td><b>2.6</b></td>
						  <td>2.69</td>
						  <td>2.72</td>
						</tr>
						<tr>
						  <td>forward time (sec)</td>
						  <td><b>0.1</b></td>
						  <td>0.221</td>
						  <td>0.27</td>
						  <td>0.353</td>
						</tr>
					  </tbody></table>
					  
					  <p>
					  The HQQ 2-bit model already outperforms Quip# without any calibration. After fine-tuning the adapter layers, the model remarkably achieves a lower perplexity than the full-precision model. This is a significant finding, as it suggests that quantization with HQQ+ not only reduces the memory footprint and computational requirements but can also potentially improve the model&#39;s language modeling performance. </p><h3>Chat Models</h3>
					<p> For the chat models benchmark, we also include some smaller full-precision models to see how low-bit quantized models compare to smaller full-precision models. 
					</p>
					<h4>1-bit Model</h4> 
					<table>
						<tbody><tr>
						  <th>Models</th>
						  <th>FP16</th>
						  <th>HQQ (1-bit)</th>
						  <th>HQQ+ (1-bit)</th>
						  <th>Qwen1.5-0.5B-Chat</th>
						</tr>
						<tr>
						  <td>ARC (25-shot)</td>
						  <td>53.67</td>
						  <td>21.59</td>
						  <td>31.14</td>
						  <td>30.55</td>
						</tr>
						<tr>
						  <td>HellaSwag (10-shot)</td>
						  <td>78.56</td>
						  <td>25.66</td>
						  <td>52.96</td>
						  <td>44.07</td>
						</tr>
						<tr>
						  <td>MMLU (5-shot)</td>
						  <td>48.16</td>
						  <td>25.08</td>
						  <td>26.54</td>
						  <td>33.82</td>
						</tr>
						<tr>
						  <td>TruthfulQA-MC2</td>
						  <td>45.32</td>
						  <td>47.81</td>
						  <td>43.16</td>
						  <td>42.95</td>
						</tr>
						<tr>
						  <td>Winogrande (5-shot)</td>
						  <td>72.53</td>
						  <td>49.72</td>
						  <td>60.54</td>
						  <td>54.62</td>
						</tr>
						<tr>
						  <td>GSM8K (5-shot)</td>
						  <td>23.12</td>
						  <td>0</td>
						  <td>11</td>
						  <td>7.66</td>
						</tr>
						<tr>
						  <td>Average</td>
						  <td>53.56</td>
						  <td>28.31</td>
						  <td>37.56</td>
						  <td>35.61</td>
						</tr>
					  </tbody></table>
					  <p>The HQQ+ 1-bit model achieved an average score of 37.56 across all benchmarks, which is higher than that of Qwen1.5-0.5B-Chat (35.61). While there is still a performance gap between the 1-bit models and the FP16 model, we are optimistic of improving the performance with further fine-tuning. </p>


					<h4>2-bit Model</h4> 
					<p>To further contextualize our results, we compared the 2-bit HQQ+ models with smaller language models, such as <a href="https://huggingface.co/wandb/gemma-2b-zephyr-sft">gemma-2b-zephyr-sft</a>.</p>
				

					<table>
						<tbody><tr>
						  <th>Models</th>
						  <th>FP16</th>
						  <th>HQQ (2-bit)</th>
						  <th>HQQ+ (2-bit)</th>
						  <th>gemma-2b-zephyr-sft</th>
						</tr>
						<tr>
						  <td>ARC (25-shot)</td>
						  <td>53.67</td>
						  <td>45.56</td>
						  <td>47.01</td>
						  <td>49.74</td>
						</tr>
						<tr>
						  <td>HellaSwag (10-shot)</td>
						  <td>78.56</td>
						  <td>73.59</td>
						  <td>73.74</td>
						  <td>72.38</td>
						</tr>
						<tr>
						  <td>MMLU (5-shot)</td>
						  <td>48.16</td>
						  <td>43.18</td>
						  <td>43.33</td>
						  <td>41.37</td>
						</tr>
						<tr>
						  <td>TruthfulQA-MC2</td>
						  <td>45.32</td>
						  <td>43.1</td>
						  <td>42.66</td>
						  <td>34.42</td>
						</tr>
						<tr>
						  <td>Winogrande (5-shot)</td>
						  <td>72.53</td>
						  <td>67.32</td>
						  <td>71.51</td>
						  <td>66.93</td>
						</tr>
						<tr>
						  <td>GSM8K (5-shot)</td>
						  <td>23.12</td>
						  <td>9.7</td>
						  <td>28.43</td>
						  <td>18.27</td>
						</tr>
						<tr>
						  <td>Average</td>
						  <td>53.56</td>
						  <td>47.08</td>
						  <td>51.11</td>
						  <td>47.18</td>
						</tr>
					  </tbody></table>

					  <p>
						Without any calibration, the HQQ 2-bit Llama2-7B-chat model performed very closely to the fine-tuned Gemma 2B model. The HQQ+ 2-bit model achieved an average score of 51.11, which is relatively close to the FP16 model&#39;s score of 53.56. Notably, after fine-tuning on math and reasoning data, the quantized model surpassed the FP16 model in GSM8K score, achieving 28.43 compared to 23.12. This observation raises an important question and sparks a debate about the optimal strategy for building efficient and effective language models.
					</p>


					<h4>A New Debate: Quantized Models or Smaller Models?</h4>
					<p>On one hand, training smaller models from scratch offers the advantage of reduced computational requirements and faster training times. Models such as <a href="https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat">Qwen1.5</a> have shown promising results and can be attractive options for certain applications. However, our findings indicate that heavily quantizing larger models using techniques like HQQ+ can yield superior performance while still maintaining a relatively small memory footprint. </p>

					<p>It&#39;s important to note that these results are for Llama2-7B, which is a relatively small model. When quantized to smaller bits without an adapter layer, as in the case of vanilla HQQ, we observe high benchmark performance for larger models. For example, our previously <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ"> quantized Mixtral model</a> quantized via vanilla HQQ, demonstrates how significant memory footprint reductions can be achieved while maintaining high performance, significantly outperforming smaller models.</p>


					<h2 id="conclusion">Conclusion</h2>
						<p>The experimental 2 and 1-bit quantized Llama2-7B models with a low-rank adapter, quantized using the proposed HQQ+ approach, showcase the potential of extreme low-bit quantization in machine learning models. Despite the challenges posed by such extreme settings, the fine-tuned models demonstrate significant improvements in output quality. We show that the fine-tuned models can take advantage of the optimized low-bit matrix multiplication formulation, which could significantly reduce the memory and compute requirements, making larger models more accessible. 
						While binary and ternary matmul kernels are still not available, we hope this work will spark more interest to develop both software and hardware that could fully take advantage of this approach in the near future.
						</p>
					

					<h2 id="citations">Citation</h2>
					<div>								
					<pre><code>
	@misc{badri2023hqq,
	title = {Towards 1-bit Machine Learning Models},
	url = {https://mobiusml.github.io/1bit_blog/},
	author = {Hicham Badri and Appu Shaji},
	month = {March},
	year = {2024}
	}
					</code></pre>
							</div>


							

				</div></div>
  </body>
</html>
