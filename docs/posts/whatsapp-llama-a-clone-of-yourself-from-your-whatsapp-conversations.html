<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Ads-cmu/WhatsApp-Llama">Original</a>
    <h1>Show HN: WhatsApp-Llama: A clone of yourself from your WhatsApp conversations</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repository is a fork of the <code>facebookresearch/llama-recipes</code>, adapted to fine-tune a Llama 7b chat model to replicate your personal WhatsApp texting style. By simply inputting your WhatsApp conversations, you can train the LLM to respond just like you do! Llama 7B chat is finetuned using parameter efficient finetuning (QLoRA) and int4 quantization on a single GPU (P100 with 16GB gpu memory).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-my-results" aria-hidden="true" tabindex="-1" href="#my-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>My Results</h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>Quick Learning</strong>: The fine-tuned Llama model picked up on my texting nuances rapidly.</p>
<ul dir="auto">
<li>The average words generated in the finetuned Llama is <em>300%</em> more more than vanilla Llama. I usually type longer replies, so this checks out</li>
<li>The model accurately replicated common phrases I say and my emoji usage</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Turing Test with Friends</strong>: As an experiment, I asked my friends to ask me 3 questions on WhatsApp, and responded with 2 candidate responses (one from me and one from the LLM). My friends then had to guess which candidate response was mine and which one was Llama&#39;s.</p>
</li>
</ol>
<p dir="auto">The result? The model fooled <em>10%</em> (2/20) of my friends. Some of the model&#39;s responses were eerily similar to my own. Here are some examples:</p>
<ul dir="auto">
<li>
<p dir="auto"><em>Example 1</em>:</p>
  <p dir="auto">
      <a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/107292631/266695409-65361711-79eb-4cf1-862d-fae93a6b674a.png"><img width="628" alt="image" src="https://user-images.githubusercontent.com/107292631/266695409-65361711-79eb-4cf1-862d-fae93a6b674a.png"/></a>
  </p>
</li>
<li>
<p dir="auto"><em>Example 2</em>:</p>
  <p dir="auto">
      <a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/107292631/266695510-700fadd0-086f-40fc-8337-5071accec94f.png"><img width="630" alt="image" src="https://user-images.githubusercontent.com/107292631/266695510-700fadd0-086f-40fc-8337-5071accec94f.png"/></a>
  </p>
<p dir="auto">I believe that with access to more compute, this number could easily be pushed to ~40% (which would be near random guessing).</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<p dir="auto">Here&#39;s a step-by-step guide on setting up this repository and creating your own customized dataset:</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-1-exporting-whatsapp-chats" aria-hidden="true" tabindex="-1" href="#1-exporting-whatsapp-chats"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>1. Exporting WhatsApp Chats</h3>
<p dir="auto">Details on how to export your WhatsApp chats can be found <a href="https://faq.whatsapp.com/1180414079177245/?cms_platform=android" rel="nofollow">here</a>. I exported 10 WhatsApp chats from friends who I speak to often. Be sure to exclude media while exporting. Each chat was saved as <code>&lt;friend_name&gt;Chat.txt</code>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-2-preprocessing-the-dataset" aria-hidden="true" tabindex="-1" href="#2-preprocessing-the-dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>2. Preprocessing the Dataset</h3>
<p dir="auto">Complete the steps below to convert the exported chat into a format suitable for training:</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-convert-text-files-to-json" aria-hidden="true" tabindex="-1" href="#convert-text-files-to-json"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Convert text files to json:</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python preprocessing.py &lt;your_name&gt; &lt;your_contact_name&gt; &lt;friend_name&gt; &lt;friend_contact_name&gt; &lt;folder_path&gt;"><pre>python preprocessing.py <span>&lt;</span>your_name<span>&gt;</span> <span>&lt;</span>your_contact_name<span>&gt;</span> <span>&lt;</span>friend_name<span>&gt;</span> <span>&lt;</span>friend_contact_name<span>&gt;</span> <span>&lt;</span>folder_path<span>&gt;</span></pre></div>
<ol dir="auto">
<li><code>your_name</code> refers to your name (Llama will learn this name)</li>
<li><code>your_contact_name</code> refers to how you&#39;ve saved your number on your phone</li>
<li><code>friend_name</code> refers to the name of your friend (Llama will learn this name)</li>
<li><code>friend_contact_name</code> refers to the name you&#39;ve used to save your friend&#39;s contact</li>
<li><code>folder_path</code> should be the path in which you&#39;ve stored your whatsapp chats.</li>
</ol>
<p dir="auto">You&#39;ll need to run this command once for every friend&#39;s chat you&#39;ve exported</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-convert-json-files-to-csv" aria-hidden="true" tabindex="-1" href="#convert-json-files-to-csv"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Convert json files to csv</h4>
<p dir="auto">Once you&#39;re done converting all texts to json, you can run the command below to create the dataset</p>
<div dir="auto" data-snippet-clipboard-copy-content="python prepare_dataset.py &lt;dataset_folder&gt; &lt;your_name&gt; &lt;save_file&gt;"><pre>python prepare_dataset.py <span>&lt;</span>dataset_folder<span>&gt;</span> <span>&lt;</span>your_name<span>&gt;</span> <span>&lt;</span>save_file<span>&gt;</span></pre></div>
<ol dir="auto">
<li><code>dataset_folder</code> refers to the folder with your json files</li>
<li><code>your_name</code> refers to your name (Llama will learn this name)</li>
<li><code>save_file</code> file path of the final csv</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-3-validating-the-dataset" aria-hidden="true" tabindex="-1" href="#3-validating-the-dataset"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>3. Validating the Dataset</h3>
<p dir="auto">Here&#39;s the expected format for the preprocessed dataset:</p>
<div data-snippet-clipboard-copy-content="| ID |   Context  |    Reply   |
| -- | ---------- | ---------- |
| 1  | You: Hi    | What&#39;s up? |
|    | Friend: Hi |            |
"><pre><code>| ID |   Context  |    Reply   |
| -- | ---------- | ---------- |
| 1  | You: Hi    | What&#39;s up? |
|    | Friend: Hi |            |

</code></pre></div>
<p dir="auto">Ensure your dataset looks like the above to verify you&#39;ve done it correctly.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-4-model-configuration" aria-hidden="true" tabindex="-1" href="#4-model-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>4. Model Configuration</h3>
<p dir="auto">Once you&#39;re done with the above steps, run WhatsApp_Finetune.ipyb</p>
<ul dir="auto">
<li>
<p dir="auto">If you&#39;re using a <strong>P100 GPU</strong>, load the model in <strong>4 bits</strong>:</p>
</li>
<li>
<p dir="auto">If you&#39;re using an <strong>A100 GPU</strong>, you can load the model in <strong>8 bits</strong>:</p>
</li>
</ul>
<p dir="auto">PEFT adds around 4.6M parameters, or 6% of total model weights.</p>
<p dir="auto">Additionally, you&#39;ll need to make the following 2 changes to <code>ft_datasets/whatsapp_dataset.py</code>:</p>
<ol dir="auto">
<li>Update the prompt to one of your choosing (line 8)</li>
<li>Update the file path of your dataset in the dataset.load_dataset() command (line 5)</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-5-training-time" aria-hidden="true" tabindex="-1" href="#5-training-time"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>5. Training Time</h3>
<p dir="auto">For reference, a 10MB dataset will complete 1 epoch in approximately 7 hours on a P100 GPU. My results shared above were achieved after training for just 1 epoch.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-conclusion" aria-hidden="true" tabindex="-1" href="#conclusion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Conclusion</h2>
<p dir="auto">This adaptation of the Llama model offers a fun way to see how well a LLM can mimic your personal texting style. Remember to use AI responsibly and inform your friends if you&#39;re using the model to chat with them!</p>
<hr/>
</article>
          </div></div>
  </body>
</html>
