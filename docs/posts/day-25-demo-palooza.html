<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jamiepalatnik.com/day-25-demo-palooza/">Original</a>
    <h1>Day 25: Demo-Palooza!</h1>
    
    <div id="readability-page-1" class="page">
    
    <p>[Simon Tatham, 2025-02-14]</p>
    <ul>
      <li>
        <a href="#intro">Introduction</a>
      </li>
      <li>
        <a href="#boolean">XOR in boolean logic</a>
        <ul>
          <li>
            <a href="#what">What is it?</a>
            <ul>
              <li>
                <a href="#exclusive-or">â€œExclusive ORâ€</a>
              </li>
              <li>
                <a href="#unequal">The â€˜not equalsâ€™ operator</a>
              </li>
              <li>
                <a href="#condinv">Conditional inversion</a>
              </li>
              <li>
                <a href="#mod2">Parity, or sum mod 2</a>
              </li>
              <li>
                <a href="#mod2diff">Difference mod 2</a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#properties">What properties does it have?</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="#bitwise">Bitwise XOR on integers</a>
        <ul>
          <li>
            <a href="#properties-bitwise">Still has all the same properties</a>
          </li>
          <li>
            <a href="#diff-bitwise">Bitwise difference</a>
          </li>
          <li>
            <a href="#condinv-bitwise">Bitwise conditional inverter</a>
          </li>
          <li>
            <a href="#carryless">Addition or subtraction without carrying</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="#applications">Some applications of XOR</a>
        <ul>
          <li>
            <a href="#cryptography">Cryptography: combine a plaintext with a keystream</a>
          </li>
          <li>
            <a href="#pixels">Pixel graphics: draw things so itâ€™s easy to undraw
      them again</a>
          </li>
          <li>
            <a href="#halfadder">The â€œhalf-adder identityâ€</a>
            <ul>
              <li>
                <a href="#average">Averaging two integers</a>
              </li>
              <li>
                <a href="#constructing-xor">Implementing XOR itself</a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#bitswap">Swapping bits in a word</a>
            <ul>
              <li>
                <a href="#shift">Using XOR and shifts</a>
              </li>
              <li>
                <a href="#parity">Using XOR and just a two-bit word</a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#swap">Three XORs make a swap</a>
          </li>
          <li>
            <a href="#nim">Winning at the game of Nim</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="#maths">Mathematical concepts that look like XOR</a>
        <ul>
          <li>
            <a href="#symmdiff">Symmetric difference of sets</a>
          </li>
          <li>
            <a href="#exp2">Groups of exponent 2</a>
          </li>
          <li>
            <a href="#nimsum">Nim-sum</a>
          </li>
          <li>
            <a href="#gf2"><i>GF</i>(2), the finite field of order 2</a>
            <ul>
              <li>
                <a href="#gf2linalg">Linear algebra over <i>GF</i>(2)</a>
                <ul>
                  <li>
                    <a href="#ecc">Error-correcting codes</a>
                  </li>
                </ul>
              </li>
              <li>
                <a href="#gf2poly">Polynomials over <i>GF</i>(2)</a>
                <ul>
                  <li>
                    <a href="#crc">CRCs</a>
                  </li>
                  <li>
                    <a href="#fields">Making larger finite fields</a>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <a href="#footnotes">Footnotes</a>
      </li>
    </ul>
    <h2 id="intro">Introduction</h2>
    <p>Recently I was called on to explain the â€˜XORâ€™ operator to
      somebody who vaguely knew of its existence, but didnâ€™t have a
      good intuition for what it was useful for and how it
      behaved.</p>
    <p>For me, this was one of those â€˜future shockâ€™ moments when you
      realise the world has moved on. When I got started in computers,
      you had to do low-level bit twiddling to get anything very
      interesting done, so you pretty much couldnâ€™t <em>avoid</em>
      learning about XOR. But these days, to a high-level programmer,
      itâ€™s much more of an optional thing, and you can perfectly well
      not know much about it.</p>
    <p>So I collected some thoughts together and gave a lecture on
      XOR. Slightly to my own surprise, I was able to spend a full
      hour talking about it â€“ and then over the course of the next
      couple of weeks I remembered several other things I could
      usefully have mentioned.</p>
    <p>And once Iâ€™d gone to the effort of collecting all those
      thoughts into one place, it seemed a waste not to write it all
      down somewhere more permanent. (The collecting is the hardest
      part!)</p>
    <p>So hereâ€™s a text version of my XOR talk â€“ or rather, the talk
      that it <em>would</em> have been if Iâ€™d remembered to include
      everything the first time round.</p>
    <h2 id="boolean">XOR in boolean logic</h2>
    <p>Weâ€™ll start by looking at XOR as a boolean logic operator, or
      equivalently a logic gate: a function that takes two individual
      bits, and outputs a single bit.</p>
    <h3 id="what">What is it?</h3>
    <p>If Iâ€™m going right back to first principles, then I should
      start by actually <em>defining</em> XOR.</p>
    <p>Any boolean function of two inputs can be defined by showing
      its truth table: for each of the four combinations of inputs,
      say what the output is. So Iâ€™ll start by showing the truth table
      for XOR.</p>
    <figure>
      <div>
        <div>
          <table>
            <tbody><tr><th><i>a</i></th><th><i>b</i></th><th><i>a</i>Â XORÂ <i>b</i></th></tr>
            <tr><th>0</th><th>0</th><td>0</td></tr>
            <tr><th>0</th><th>1</th><td>1</td></tr>
            <tr><th>1</th><th>0</th><td>1</td></tr>
            <tr><th>1</th><th>1</th><td>0</td></tr>
          </tbody></table>
        </div>
        <div>
          <table>
            <tbody><tr><td colspan="2" rowspan="2"></td><th colspan="2"><i>b</i></th></tr>
            <tr><th>0</th><th>1</th></tr>
            <tr><th rowspan="2"><i>a</i></th><th>0</th><td>0</td><td>1</td></tr>
            <tr><th>1</th><td>1</td><td>0</td></tr>
          </tbody></table>
        </div>
      </div>
      <figcaption>The truth table of XOR, shown in two different ways</figcaption>
    </figure>
    <p>But just saying what it <em>is</em> doesnâ€™t give a good
      intuition for what things itâ€™s useful for, or when to use it. So
      in the next few sections Iâ€™ll present a few different ways
      of <em>thinking about</em> what XOR does.</p>
    <p>For such a simple operation, itâ€™s possible to describe what it
      does in a lot of quite different-looking ways. But all of them
      are true at once! So you donâ€™t need to <em>keep</em> thinking
      about XOR as being just one of the following concepts. You can
      suddenly switch from thinking of it one way to another, any time
      thatâ€™s convenient. Nothing stops being true if you do.</p>
    <h4 id="exclusive-or">â€œExclusive ORâ€</h4>
    <p>The X in â€˜XORâ€™ stands for the word â€œexclusiveâ€. (Some assembly
      language syntaxes abbreviate it as â€˜EORâ€™ instead, on the grounds
      that â€œexclusiveâ€ doesnâ€™t actually begin with an X.
      But <em>most</em> people prefer the X spelling, in my
      experience.)</p>
    <p>In the normal English language, the conjunction â€˜orâ€™ has an
      ambiguity: if I say â€˜you can do this <em>or</em> thatâ€™, and in
      fact someone tries to do both this <em>and</em> that, does that
      count? It depends on the context. A parent telling a child â€œYou
      can have this dessert <em>or</em> that oneâ€ certainly means â€˜but
      not bothâ€™ â€“ thatâ€™s the whole point. But on the other hand, if
      the same parent wants the child to do something useful, and says
      â€œDo your homework, or tidy your room!â€, theyâ€™d probably be
      extra <em>pleased</em> if the child did both.</p>
    <p>So, when you want to be clear about which version of â€˜orâ€™ you
      mean, you might say that youâ€™re <em>including</em> the â€˜bothâ€™
      case, or <em>excluding</em> it.</p>
    <p>In computing, â€˜ORâ€™ as a boolean operator is always taken to
      mean the <em>inclusive</em> version: <i>A</i> or <i>B</i> or
      both. And when you want <i>A</i> or <i>B</i> but <em>not</em>
      both, you talk about <em>exclusive</em> OR.</p>
    <p>Looking at the truth tables above, you can see that thatâ€™s
      exactly what the XOR operation is doing:</p>
    <ul>
      <li>If <i>a</i>Â =Â 1, <em>or</em> if <i>b</i>Â =Â 1,
        then <i>a</i>Â XORÂ <i>b</i>Â =Â 1.</li>
      <li>But not both: if <i>a</i>
        <em>and</em> <i>b</i> are both 1,
        then <i>a</i>Â XORÂ <i>b</i>Â =Â 0.</li>
    </ul>
    <h4 id="unequal">The â€˜not equalsâ€™ operator</h4>
    <p>Another way to look at the same truth table
      is: <i>a</i>Â XORÂ <i>b</i>Â =Â 1 whenever the inputs <i>a</i>
      and <i>b</i> are <em>different</em> from each other. If theyâ€™re
      the same (either both 0 or both 1),
      then <i>a</i>Â XORÂ <i>b</i>Â =Â 0.</p>
    <p>So you could look at <i>a</i>Â XORÂ <i>b</i> as meaning the same
      thing as <i>a</i>Â â‰ Â <i>b</i>: itâ€™s a way to <em>compare</em> two
      boolean values with each other.</p>
    <h4 id="condinv">Conditional inversion</h4>
    <p>A third way to look at the same truth table is to consider each
      value of one of the inputs <i>a</i>, and look at what XOR does
      to the other variable <i>b</i> in each case:</p>
    <ul>
      <li>If <i>a</i>Â =Â 0, then <i>a</i>Â XORÂ <i>b</i> is the same thing as just <i>b</i>.</li>
      <li>If <i>a</i>Â =Â 1, then <i>a</i>Â XORÂ <i>b</i> is the <em>opposite</em> of <i>b</i>: 0 becomes 1 and 1 becomes 0.</li>
    </ul>
    <p>So another way to look at the XOR operation is that youâ€™re
      either going to leave <i>b</i> alone, or invert it (swapping 0
      and 1), and <i>a</i> tells you which one to do. If you imagine
      XOR as a tiny computing device, you could think of the
      input <i>b</i> as â€˜dataâ€™ that the device is operating on,
      and <i>a</i> as a â€˜controlâ€™ input that tells the device what to
      do with the data, with the possible choices being â€˜flipâ€™ or
      â€˜donâ€™t flipâ€™.</p>
    <p><i>a</i>Â XORÂ <i>b</i> means: <strong>invert <i>b</i>, but only
        if <i>a</i> is true.</strong></p>
    <p>But the same is <em>also</em> true the other way round,
      because <i>a</i>Â XORÂ <i>b</i> is the same thing
      as <i>b</i>Â XORÂ <i>a</i>. You can swap your point of view to
      thinking of <i>a</i> as the â€˜dataâ€™ input and <i>b</i> as
      â€˜controlâ€™, and nothing changes â€“ the operation is the same
      either way round.</p>
    <p>That is, <i>a</i>Â XORÂ <i>b</i> also
      means: <strong>invert <i>a</i>, but only if <i>b</i> is
      true!</strong></p>
    <h4 id="mod2">Parity, or sum mod 2</h4>
    <p>Hereâ€™s yet <em>another</em> way to look at the XOR
      operation. <i>a</i>Â XORÂ <i>b</i> tells you whether an <em>odd
        number</em> of the inputs are true:</p>
    <ul>
      <li>If <i>a</i>Â =Â <i>b</i>Â =Â 0, then <em>zero</em> of the inputs
        are true. 0 is even, and <i>a</i>Â XORÂ <i>b</i>Â =Â 0.</li>
      <li>If <i>a</i>Â =Â 1 and <i>b</i>Â =Â 0, or the other way round,
        then <em>one</em> of the inputs is true. 1 is odd,
        and <i>a</i>Â XORÂ <i>b</i>Â =Â 1.</li>
      <li>If <i>a</i>Â =Â <i>b</i>Â =Â 1, then <em>two</em> of the inputs
        are true. 2 is even, and <i>a</i>Â XORÂ <i>b</i>Â =Â 0 again.</li>
    </ul>
    <p>Put another way: if you <em>add together</em> the two inputs,
      and then reduce the result modulo 2 (that is, divide by 2 and
      take the remainder), you get the same answer
      as <i>a</i>Â XORÂ <i>b</i>.</p>
    <p>In particular, if you XOR together <em>more</em> than two
      values, the overall result will tell you whether an odd or even
      number of the inputs were 1 â€“ no matter how many inputs you
      combine.</p>
    <h4 id="mod2diff">Difference mod 2</h4>
    <p>So XOR corresponds to addition mod 2. But it also corresponds
      to <em>subtraction</em> mod 2, at the same time: if you take the
      difference <i>a</i>Â âˆ’Â <i>b</i> and reduce <em>that</em> mod 2,
      you still get the same answer <i>a</i>Â XORÂ <i>b</i>!</p>
    <ul>
      <li>If <i>a</i>Â =Â <i>b</i>, then the
        difference <i>a</i>Â âˆ’Â <i>b</i> is 0, and so
        is <i>a</i>Â XORÂ <i>b</i>.</li>
      <li>If <i>a</i>Â â‰ Â <i>b</i>, then the
        difference <i>a</i>Â âˆ’Â <i>b</i> is either +1 or âˆ’1. Mod 2,
        those are the same as each other â€“ theyâ€™re both just 1.</li>
    </ul>
    <h3 id="properties">What properties does it have?</h3>
    <p>If you have a complicated boolean expression involving lots of
      XORs, itâ€™s useful to know how you can manipulate the expression
      to simplify it.</p>
    <p>To begin with, XOR is both <strong>commutative</strong>
      and <strong>associative</strong>, which mean (respectively)
      that</p>
    <p><i>a</i>Â XORÂ <i>b</i>Â =Â <i>b</i>Â XORÂ <i>a</i></p>
    <p>(<i>a</i>Â XORÂ <i>b</i>)Â XORÂ <i>c</i>Â =Â <i>a</i>Â XORÂ (<i>b</i>Â XORÂ <i>c</i>)</p>
    <p>In practice, what this means is that if you see a long list of
      values or variables XORed together, you <em>donâ€™t need to
      worry</em> about what order theyâ€™re combined in, because it
      doesnâ€™t make any difference. You can rearrange the list of
      inputs into any order you like, and choose any subset of them to
      combine first, and the answer will be the same regardless.</p>
    <p>(This is perhaps most easily seen by thinking of XOR as
      â€˜addition mod 2â€™, because addition is also both commutative and
      associative â€“ whether or not itâ€™s mod anything â€“ so XOR inherits
      both properties.)</p>
    <p>Secondly, <strong>0 acts as an identity</strong>, which means
      that XORing anything with zero just gives you the same thing you
      started with:</p>
    <p><i>a</i>Â XORÂ 0Â =Â 0Â XORÂ <i>a</i>Â =Â <i>a</i></p>
    <p>So if you have a long list of things XORed together, and you
      can see that one of them is 0, you can just delete it from the
      list.</p>
    <p>Thirdly, <strong>everything is
      self-inverse</strong><label for="footnoteinput-inverse"><sup id="footnote-inverse">1</sup><span>I apologise for
      using â€˜invertâ€™ and â€˜inverseâ€™ in two senses in this article. In
      boolean logic, â€˜invertingâ€™ a signal generally means the NOT
      operator, interchanging 1 with 0 (or true with false, if you
      prefer). But in mathematics, an â€˜inverseâ€™ is a value that
      cancels out another value to get you back to the identity, in a
      way which varies depending on the operation you use to combine
      them (if youâ€™re talking about addition itâ€™s âˆ’<i>x</i>, and for
      multiplication itâ€™s 1/<i>x</i>). I hope that everywhere Iâ€™ve
      used the word at all itâ€™s clear from context which sense I mean
      it in.</span><span>1</span></label>: if you XOR any value with <em>itself</em>,
      you always get zero.</p>
    <p><i>a</i>Â XORÂ <i>a</i>Â =Â 0</p>
    <p>One consequence of this is that if you have a long list of
      variables being XORed together, and the same variable occurs
      twice in the list, you can just remove both copies. <em>Anything
      appearing twice cancels out.</em> For example, the two copies
      of <i>b</i> can be removed in this expression:</p>
    <p><i>a</i>Â XORÂ <i><span>b</span></i>Â XORÂ <i>c</i>Â XORÂ <i><span>b</span></i>Â =Â <i>a</i>Â XORÂ <i>c</i></p>
    <p>Another consequence is that if you have a
      value <i>a</i>Â XORÂ <i>b</i>, and you want to recover
      just <i>a</i>, you can do it by XORing in another copy
      of <i>b</i> (if you know it), to cancel the one thatâ€™s already
      in the expression. <em>Putting something in a second time is the
      same as removing it:</em></p>
    <p>(<i>a</i>Â XORÂ <i>b</i>)Â XORÂ <i>b</i>Â =Â <i>a</i>Â XORÂ (<i>b</i>Â XORÂ <i>b</i>)Â =Â <i>a</i>Â XORÂ 0Â =Â <i>a</i></p>
    <h2 id="bitwise">Bitwise XOR on integers</h2>
    <p>Now weâ€™ve seen what XOR does on individual bits, itâ€™s time to
      apply it to something larger.</p>
    <p>To a computer itâ€™s most natural to represent an integer in
      binary, so that it looks like a string of 0s and 1s. So if you
      have two integers, you can combine them <em>bitwise</em> (that
      is, â€˜treating each bit independentlyâ€™) using a logical operation
      like XOR: you write the numbers in binary one above the
      other<label for="footnoteinput-twos-complement"><sup id="footnote-twos-complement">2</sup><span>What if the integers are
      negative? Normally, in the finite integer sizes that computers
      handle in hardware, negative integers are represented in twoâ€™s
      complement, i.e. mod 2<sup><i>n</i></sup>. So âˆ’1 is the integer
      with all bits 1, for example. Thereâ€™s a reasonably natural way
      to extend this to <em>arbitrarily large</em> integers, by
      pretending the string of 1 bits on the left goes all the way to
      infinity (and it can even be made mathematically rigorous!). In
      this view, the XOR of two positive numbers, or two negative
      numbers, is positive, because at a high enough bit position each
      bit is XORing two 0s or two 1s; but the XOR of a positive and
      negative number is negative, because sooner or later youâ€™re
      always XORing a 0 with a 1 bit. Some languages, such as Python,
      actually implement this â€“ you can try it out at the interactive
      prompt!</span><span>2</span></label>, and set each output bit to the result of
      combining the corresponding bits of both input numbers via
      XOR.</p>
    <p>Here are a couple of examples, one small and carefully chosen,
      and the other large and random-looking:</p>
    <figure>
      <div>
        <div>
          <table>
            <tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr>
            <tr><th><i>a</i></th><td><code>1010</code></td><td><code>A</code></td><td><code>10</code></td></tr>
            <tr><th><i>b</i></th><td><code>1100</code></td><td><code>C</code></td><td><code>12</code></td></tr>
            <tr><th><i>a</i>Â XORÂ <i>b</i></th><td><code>0110</code></td><td><code>6</code></td><td><code>Â 6</code></td></tr>
          </tbody></table>
        </div>
        <div>
          <table>
            <tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr>
            <!-- In case anyone is curious, these inputs aren't
                 randomly chosen 32-bit numbers. They're the cube
                 roots of 3 and 5, mod 2^32. -->
            <tr><th><i>a</i></th><td><code>11001111001000011111000101111011</code></td><td><code>CF21F17B</code></td><td><code>3475108219</code></td></tr>
            <tr><th><i>b</i></th><td><code>10011011010000100100111001011101</code></td><td><code>9B424E5D</code></td><td><code>2604813917</code></td></tr>
            <tr><th><i>a</i>Â XORÂ <i>b</i></th><td><code>01010100011000111011111100100110</code></td><td><code>5463BF26</code></td><td><code>1415823142</code></td></tr>
          </tbody></table>
        </div>
      </div>
      <figcaption>Small and large examples of applying bitwise XOR to two integers</figcaption>
    </figure>
    <p>The small example contains one bit for each element of the XOR
      truth table. If you look at vertically aligned bits in the
      binary column of the table, youâ€™ll see that in the rightmost
      place both <i>a</i> and <i>b</i> have a 0 bit; in the leftmost
      place they both have a 1 bit; in between, thereâ€™s a position
      where only <i>a</i> has a 1, and one where only <i>b</i> has a
      1. And in each position, the output bit is the boolean XOR of
      the two input bits.</p>
    <p>Of course, this idea of â€˜bitwiseâ€™ logical operations â€“ taking
      an operation that accepts a small number of input bits, and
      applying it one bit at a time to a whole binary integer â€“ is not
      limited to XOR. Bitwise AND and bitwise OR are also well defined
      operations, and both useful. But this particular article is
      about XOR, so Iâ€™ll stick to that one.</p>
    <h3 id="properties-bitwise">Still has all the same properties</h3>
    <p><a href="#properties">Earlier</a> I discussed a number of
      mathematical laws obeyed by the one-bit version of XOR: itâ€™s
      commutative, associative, has 0 as an identity, and every
      possible input is self-inverse in the sense that XORing it with
      itself gives 0.</p>
    <p>In the bitwise version applied to integers, all of these things
      are still true:</p>
    <ul>
      <li><i>a</i>Â XORÂ <i>b</i>Â =Â <i>b</i>Â XORÂ <i>a</i> for any integers <i>a</i> and <i>b</i></li>
      <li>(<i>a</i>Â XORÂ <i>b</i>)Â XORÂ <i>c</i>Â =Â <i>a</i>Â XORÂ (<i>b</i>Â XORÂ <i>c</i>) similarly</li>
      <li><i>a</i>Â XORÂ 0Â =Â 0Â XORÂ <i>a</i>Â =Â <i>a</i>, where 0 means the <em>integer</em> zero, i.e. with all its binary bits 0</li>
      <li><i>a</i>Â XORÂ <i>a</i>Â =Â 0</li>
    </ul>
    <p>So if you have a complicated expression containing bitwise
      XORs, you can simplify it in all the same ways you could do it
      with single-bit XORs.</p>
    <h3 id="diff-bitwise">Bitwise difference</h3>
    <p>With individual bits, I <a href="#unequal">said earlier</a>
      that <i>a</i>Â XORÂ <i>b</i> means the same thing
      as <i>a</i>Â â‰ Â <i>b</i>: given two input bits, it returns 1 if
      the bits are different from each other, and 0 if theyâ€™re the
      same.</p>
    <p>Of course, applied bitwise to whole integers, this is true
      separately in every bit position: each bit of the output integer
      tells you whether the two corresponding input bits were unequal.</p>
    <p>So if <i>a</i>Â =Â <i>b</i>, then <i>a</i>Â XORÂ <i>b</i>Â =Â 0. And
      if <i>a</i>Â â‰ Â <i>b</i>, then <i>a</i>Â XORÂ <i>b</i>Â â‰ Â 0, because
      two unequal integers must have disagreed in at least one bit
      position, so that bit will be set in their XOR.</p>
    <p>So in some sense you can still use bitwise XOR to tell you
      whether two entire integers are equal: itâ€™s 0 if they are, and
      nonzero if theyâ€™re not. But bitwise XOR gives you more detail
      than that: it also gives you a specific list of <em>which bit
      positions</em> they differ in.</p>
    <h3 id="condinv-bitwise">Bitwise conditional inverter</h3>
    <p>I also said <a href="#condinv">earlier</a> that you could see
      XOR as a â€˜conditional inversionâ€™ operator: imagine one
      input <i>b</i> to be â€˜dataâ€™, and the other input <i>a</i> to be
      a â€˜controlâ€™ input that says whether you want to flip the data
      bit.</p>
    <p>Applied bitwise to whole integers, this is still true, but more
      usefully so: the control input <i>a</i> says <em>which</em> data
      bits you want to flip.</p>
    <p>For example, in the Unicode character encoding (and also in its
      ancestor ASCII), every character you might need to store in a
      string is represented by an integer. The choice of integer
      encodings has some logic to it (at least in places). In
      particular, the upper-case Latin letters A,Â B,Â C,Â â€¦,Â Z and their
      lower-case equivalents a,Â b,Â c,Â â€¦,Â z have encodings that differ
      in just one bit: A is 65, or binary 01000001, and a is 97, or
      binary 01100001. So if you know that a character value
      represents a Latin letter<label for="footnoteinput-emoji"><sup id="footnote-emoji">3</sup><span>Of course, this
      doesnâ€™t apply to <em>all</em> Unicode characters! Most donâ€™t
      have a concept of upper or lower case at all. And unfortunately,
      this rule isnâ€™t even obeyed by all of the characters that do. It
      was consistently true in ASCII, and in some of the descendants
      of ASCII, but Unicode as a whole wasnâ€™t able to stick 100% to
      the principle. If you take this too far, you might get strange
      ideas, like the lower-case version of the car emoji being a â€˜no
      pedestriansâ€™ sign:
<span>&gt;&gt;&gt; chr(ord(&#39;ğŸš—&#39;) ^ 32)</span></span><span>3</span></label>, you can swap it from upper case
      to lower case <em>or vice versa</em> by flipping just that one
      bit between 0 and 1.</p>
    <p>And you can do that most easily, in machine code or any other
      programming language, by XORing it with a number that has just
      that one bit set, namely binary 00100000, or decimal 32. For
      example, in Python (where the <code>^</code> operator means
      bitwise XOR):</p>
    <figure>
      <div>
        <div>
          <pre>&gt;&gt;&gt; chr(ord(&#39;A&#39;) ^ 32)
&#39;a&#39;
&gt;&gt;&gt; chr(ord(&#39;x&#39;) ^ 32)
&#39;X&#39;</pre>
        </div>
      </div>
      <figcaption>Flipping case by XORing with 32 in Python</figcaption>
    </figure>
    <p>Of course, the other thing I said earlier also applies: it
      doesnâ€™t matter <em>which</em> of the inputs to XOR you regard as
      the data, and which as the control. The operation is the same
      both ways round.</p>
    <p>In particular, suppose youâ€™ve already XORed two values <i>a</i>
      and <i>b</i> to obtain a number <i>d</i> that tells you which
      bits they differ in. Then you can turn either of <i>a</i>
      or <i>b</i> into the other one, by XORing with the
      difference <i>d</i>, because that flips exactly the set of bits
      where <i>a</i> has the opposite value to <i>b</i>.</p>
    <p>In other words, these three statements are
      all <em>equivalent</em> â€“ if any one of them is true, then so
      are the other two:</p>
    <p><i>a</i>Â XORÂ <i>b</i>Â =Â <i>d</i></p>
    <h3 id="carryless">Addition or subtraction without carrying</h3>
    <p>Another thing I mentioned about XOR on single bits is that it
      corresponds to <a href="#mod2">addition mod 2</a>, or
      equivalently, <a href="#mod2diff">subtraction mod 2</a>.</p>
    <p>That isnâ€™t quite how it works once you do it bitwise on whole
      words<label for="footnoteinput-well-actually"><sup id="footnote-well-actually">4</sup><span>I
      suppose, <em>technically</em>, you could argue that it is still
      literally true that <i>a</i>Â XORÂ <i>b</i>, <i>a</i>Â +Â <i>b</i>,
      and <i>a</i>Â âˆ’Â <i>b</i> are all congruent to each other mod 2,
      because all that means is that they all have the same low-order
      bit, which they do. But that isnâ€™t a
      particularly <em>useful</em> thing to say, because it ignores
      the way all the higher-order bits do something completely
      different!</span><span>4</span></label>. Each individual pair of bits is added mod
      2, but each of those additions is independent.</p>
    <p>One way to look at this is: suppose you were a schoolchild just
      learning addition, and youâ€™d simply forgotten that it was
      necessary to carry an overflowed sum between digit positions at
      all. So for every column of the sum, youâ€™d add up the input
      digits in that column, write down the low-order digit of
      whatever you got, and ignore the carry completely.</p>
    <p>If this imaginary schoolchild were to do this procedure in
      binary rather than decimal, the operation theyâ€™d be performing
      is exactly bitwise XOR! <strong>Bitwise XOR is like binary
      addition, without any carrying.</strong></p>
    <p><a href="#halfadder">Later on</a> Iâ€™ll show an interesting
      consequence of this, by considering what <em>does</em> happen to
      the carry bits, and how you can put them back in again
      afterwards. Thereâ€™s also a <a href="#gf2poly">mathematically
      meaningful and useful interpretation</a> of the corresponding
      â€˜carrylessâ€™ version of <em>multiplication</em>, in which you
      make a shifted version of one of the inputs for each 1 bit in
      the other, and then add them together in this carryless XOR
      style instead of via normal addition.</p>
    <h2 id="applications">Some applications of XOR</h2>
    <p>Now weâ€™ve seen what XOR <em>is</em>, and a few different things
      itâ€™s <em>like</em>, letâ€™s look at some things itâ€™s used for.</p>
    <h3 id="cryptography">Cryptography: combine a plaintext with a keystream</h3>
    <p>XOR is used all over cryptography, in many different ways. I
      wonâ€™t go into the details of <em>all</em> of those ways â€“ Iâ€™m
      sure I donâ€™t even know all of them myself! â€“ but Iâ€™ll show a
      simple and commonly used one.</p>
    <p>Suppose you have a message to encrypt. One really simple thing
      you could do would be to take an equally long stream of
      random-looking data â€“ usually known as a â€˜keystreamâ€™ â€“ and
      combine the two streams, a byte or a word at a time, in some way
      that the receiver can undo. So if your message was â€œhelloâ€, for
      example, you might simply transmit a byte made by combining â€œhâ€
      with keystream byte #1, then one thatâ€™s â€œeâ€ combined with
      keystream byte #2, and so on.</p>
    <p>This seems absurdly simple â€“ surely real cryptography is
      terrifyingly complicated compared to this? But it really is a
      thing that happens. As long as each byte is combined with the
      keystream byte in a way that makes it possible to recover the
      original byte at the other end, this is a completely sensible
      way to encrypt a message<label for="footnoteinput-integrity"><sup id="footnote-integrity">5</sup><span>However,
      encrypting the message is only half the story. Given a
      good-quality keystream, this technique is good enough to
      assure <em>confidentiality</em>, meaning that an eavesdropper
      canâ€™t find out what youâ€™re saying. But it doesnâ€™t do one single
      thing to ensure <em>integrity</em>, meaning that if your message
      is modified by an attacker, the receiver can detect the
      tampering and know not to trust the modified message. Integrity
      protection is a completely separate problem. A common mistake in
      novice cryptosystem design is to leave it out, assuming that if
      nobody can figure out what the message says, then â€œsurelyâ€
      nobody can work out how to tamper with it in a useful way
      either. Even with more complicated encryption methods than what
      Iâ€™m describing here, this never goes well!</span><span>5</span></label>!</p>
    <p>Thatâ€™s not to say that there isnâ€™t terrifyingly complicated
      stuff <em>somewhere</em> in the setup. But in this particular
      context, the complicated part is in how you <em>generate</em>
      your stream of random-looking bytes; the final step where you
      combine it with the message is the easy part. One option is for
      your keystream to be a huge amount of genuinely random data, as
      big as the total size of all messages youâ€™ll ever need to send;
      this is known as a â€˜one-time padâ€™, and is famously actually
      unbreakable â€“ but also amazingly impractical for almost all
      purposes. More usually you use a stream cipher, or a block
      cipher run in a counter mode, to expand a manageably small
      actual <em>key</em> into a keystream as long as you need.</p>
    <p>Anyway. Iâ€™ve so far left out the detail of exactly how you
      â€œcombineâ€ the keystream with the message. But given the subject
      of this article, you can probably guess that itâ€™s going to be
      XOR.</p>
    <p>XOR isnâ€™t the only thing that would <em>work</em>. If your
      message and your keystream are each made of bytes, then there
      are plenty of other ways to combine two bytes that let you
      recover one of them later. For example, addition mod
      2<sup>8</sup> would be fine: you could make each encrypted byte
      by <em>adding</em> the message byte and keystream byte, and then
      the receiver (who has access to exactly the same keystream)
      would subtract the keystream byte from the sum to recover the
      message byte.</p>
    <p>But in practice, XOR is generally whatâ€™s used, because itâ€™s
      simpler. Not so much for software â€“ CPUs generally have an â€˜ADDâ€™
      instruction and an â€˜XORâ€™ instruction which are just as fast and
      easy to use as each other â€“ but encryption is also often done in
      hardware, by specially made circuitry. And if youâ€™re building
      hardware, addition is more complicated than XOR, because you
      have to worry about carrying between bit positions, which costs
      more space on the chip (extra logic gates) and also extra time
      (propagating signals from one end to the other of the addition).
      XOR avoids both problems: in custom hardware, itâ€™s far cheaper.</p>
    <p>(Itâ€™s also <em>very</em> slightly more convenient that the
      sender and receiver donâ€™t have to do <em>different</em>
      operations. With XOR, the sender who applies the keystream and
      the receiver who takes it off again are both doing exactly the
      same thing, instead of one adding and the other
      subtracting.)</p>
    <h3 id="pixels">Pixel graphics: draw things so itâ€™s easy to undraw
      them again</h3>
    <p>Letâ€™s set the wayback machine to the mid-1980s, and go back in
      time to when computers were smaller and simpler (at least, the
      kind you could afford to have in your home). Home computer
      graphics systems stored a very small number of bits for each
      pixel on the screen, meaning that you could only display a
      limited number of different colours at a time; and even with
      that limitation on framebuffer size, home computers had so
      little RAM in total that it was a struggle to store two entire
      copies of what was displayed on the screen and still have enough
      memory for anything else (like whatever program
      was <em>generating</em> that screenful of graphics).</p>
    <p>In an environment limited like that, what do you do if you want
      to draw an object that appears and disappears, or that moves
      around gradually?</p>
    <p>If your moving object is <em>opaque</em>, then every time you
      â€˜undrawâ€™ it, you have to restore whatever was supposed to be on
      the screen behind it. That means you have to <em>remember</em>
      what was behind it â€“ either by storing the actual pixels, or by
      storing some recipe that knows how to recreate the missing patch
      of graphics from scratch. Either one costs memory, and the
      second option probably costs time as well, and you donâ€™t have a
      lot of either to spare.</p>
    <p>Nevertheless, you do that when you really have to. But when
      you <em>donâ€™t</em> have to, itâ€™s always helpful to take
      shortcuts.</p>
    <p>So one common graphical dodge was: <em>donâ€™t make graphical
      objects opaque if you donâ€™t have to</em>. Any time you can get
      away with it, prefer to draw a thing on the screen in a way that
      is <em>reversible</em>: combine each pixel <i>M</i> of the
      moving object with the existing screen pixel <i>S</i>, in such a
      way that you can undo the operation later, recovering the
      original screen pixel <i>S</i> from the combined
      value <i>C</i> by remembering what <i>M</i> was.</p>
    <p>As in the previous section, there are plenty of ways to do this
      in principle. You could imagine treating the bits of each pixel
      as an <i>n</i>-bit integer for some small <i>n</i>, and doing
      addition and subtraction on them (again, mod
      2<sup><i>n</i></sup>). For example, you could draw by addition,
      setting <i>C</i>Â =Â <i>S</i>Â +Â <i>M</i>, and undraw by
      subtraction to recover <i>S</i>Â =Â <i>C</i>Â âˆ’Â <i>M</i>.</p>
    <p>But these systems typically had far fewer than 8 bits per
      pixel, so each byte of the screen would have more than one pixel
      packed into it. Or, worse, the screen might be laid out in â€˜bit
      planesâ€™, with several widely separated blocks of memory each
      containing one bit of every pixel. Doing ordinary addition on
      the pixel values is very awkward in both cases.</p>
    <p>In particular, consider the first of those possibilities, where
      you have several pixels packed into a byte. Suppose the 8 bits
      of the byte are treated as two 4-bit integers, or four 2-bit
      integers. In order to do parallel <em>addition</em> of each of
      those small chunks, you canâ€™t use the normal CPUâ€™s addition
      instruction, because a carry off the top of one pixel value
      propagates into the next, causing unwanted graphical effects. So
      youâ€™d somehow need to arrange to suppress the
      carry <em>between</em> pixels, but leave the
      carry <em>within</em> a pixel alone.</p>
    <p>So itâ€™s much easier not to try this in the first place.
      Combining the pixel values via XOR instead of addition means
      that you automatically avoid carries between pixels, because
      there are no carries <em>at all</em>. This is also more
      convenient in the â€˜bit planeâ€™ memory layout, because each bit of
      a pixel is treated independently; if you tried to do ordinary
      addition in that situation, youâ€™d have to make extra effort to
      propagate carries between corresponding bits in each bit
      plane.</p>
    <p>Hereâ€™s a simple example, in which two lines have been drawn
      crossing each other. You can see that in the second diagram,
      drawn using XOR, thereâ€™s a missing pixel at the point where the
      two lines cross. That pixel is part of <em>both</em> lines, so
      itâ€™s been drawn twice. Each time it was drawn, the pixel value
      flipped between black and white, so drawing it twice sets it
      back to the background colour.</p>
    
    <p>That missing pixel looks like a tiny blemish. The first version
      of the picture looks <em>nicer</em>. But it makes it harder to
      undraw one of those lines later. If you just undraw it by
      setting all the pixels of one line back to white, then you leave
      a hole in the remaining line. And if you donâ€™t want to leave a
      hole, then you need some method of remembering which
      pixel <em>not</em> to reset to white.</p>
    <p>In the XOR version of the picture, itâ€™s the easiest thing in
      the world to undraw one line and leave the other one undamaged.
      Simply draw the line <em>again</em> that you want to
      undraw. <em>Putting a second copy of something into an XOR
      combination is the same as removing it.</em> This kind of
      graphical â€˜blemishâ€™ was considered a reasonable price to pay, in
      situations where efficient undrawing and redrawing was
      needed.</p>
    <p>Most types of 1980s home computers had an option to draw in XOR
      mode, for this kind of reason. It was one of the easiest ways
      for a beginning computer programmer to draw pretty animations. A
      common type of demo was to simulate two points moving around the
      screen along a pair of independent paths, and draw a line
      between the two points at each moment â€“ but to allow the
      previous few versions of the line to persist as well as the
      latest one, undrawing each one after it had been on the screen
      for a few frames.</p>
    <p>Hereâ€™s an example of the kind of thing. In this example each
      end of the line is following a path given by a different
      <a href="https://en.wikipedia.org/wiki/Lissajous_curve">Lissajous
      curve</a>:</p>
    <div>
      <div>
        <figure>
          <img src="https://jamiepalatnik.com/day-25-demo-palooza/animated-lines.gif"/>
          <figcaption>Moving-line animation using XOR drawing</figcaption>
        </figure>
      </div>
    </div>
    <p>Because weâ€™re drawing the lines in XOR mode, the procedure for
      turning each frame of this animation into the next involves
      drawing only two lines: the new one that weâ€™re adding at the
      leading edge of the motion, and the one thatâ€™s about to vanish
      from the trailing edge. So the program only has to remember the
      endpoint coordinates of the 10 (or however many) lines it
      currently has on screen â€“ or perhaps not even bother with that,
      and just recalculate the (<i>n</i>Â âˆ’Â 10)th coordinate in each
      frame to work out what line to undraw.</p>
    <p>So this lets you produce pretty and interesting results, with
      almost no memory cost (you donâ€™t have to store extra copies of
      lots of pixels). It uses very little CPU as well (you donâ€™t have
      to redraw <em>all</em> the lines in every frame, only the ones
      youâ€™re adding and removing), which makes the animation able to
      run faster.</p>
    <p>This style of animation was a common thing for beginning
      programmers to play with, but it was also used by professionals.
      Perhaps most famously, a moving line drawn in this style acted
      as the main antagonist of the 1981 video
      game <a href="https://en.wikipedia.org/wiki/Qix">Qix</a>.</p>
    <p>Even in the 1990s, with slightly larger computers, XOR-based
      drawing and undrawing was still a useful thing: in early GUI
      environments like the Amiga, when you dragged a window around
      the screen with the mouse, the system didnâ€™t have enough CPU to
      redraw the entire window as you went. Instead it would draw an
      outline frame for where the window was going to end up, and once
      youâ€™d put it in the right place, youâ€™d let go of the mouse
      button and the window would be properly redrawn in the new
      location, just once. And again the outline frame would be drawn
      using XOR, so that it didnâ€™t cost extra memory to remember how
      to undraw it on each mouse movement.</p>
    <p>But going back to the above animation: if you watch it
      carefully, you might notice that when the moving endpoints
      change direction and a lot of the lines cross each other, the
      missing pixels at the crossing points give rise to a sort of
      interference effect. Those missing pixels started off as a
      necessity to save memory, but it turns out theyâ€™re also quite
      pretty in their own right.</p>
    <p>Another favourite kind of 1980s graphical program for beginners
      was to explore those interference patterns more fully. Hereâ€™s an
      image created by drawing a sequence of lines, all with one
      endpoint in the bottom left corner of the frame, and with the
      other endpoints being every single pixel along the top row. In
      ordinary drawing mode this would just be a slow way to fill in a
      big black triangle. But in XOR mode, the lines interfere with
      each other wherever more than one of them passes through the
      same pixel:</p>
    <div>
      <div>
        <figure>
          <img src="https://jamiepalatnik.com/day-25-demo-palooza/interfering-lines.png"/>
          <figcaption>Interference pattern between overlapping lines drawn with XOR</figcaption>
        </figure>
      </div>
    </div>
    <p>Thatâ€™s actually interesting, and it takes some time to work out
      whatâ€™s going on!</p>
    <p>The topmost row is all black, because every pixel on that row
      is drawn just once, by the line whose endpoint is at exactly
      that position. But half way up, thereâ€™s an all-<em>white</em>
      row, because the <i>n</i> lines in the image have
      only <i>n</i>Â /Â 2 pixels to pass through on their way to the top
      row. So you get two lines passing through each pixel, so every
      pixel is drawn exactly twice, and cancels out back to white.
      Similarly, a third of the way up thereâ€™s another black row where
      every pixel is drawn exactly <em>three</em> times, and so on.
      And generally, 1Â /Â <i>n</i> of the way from bottom to top, you
      expect an all-black row if <i>n</i> is odd, or an all-white row
      if <i>n</i> is even.</p>
    <p>But in between those 1Â /Â <i>n</i> rows, something much more
      interesting happens, when the number of lines going through each
      pixel isnâ€™t the same everywhere on the row. Those interference
      patterns are related to the distribution of rounding errors in
      computing the horizontal position of each pixel of the line.</p>
    <p>Iâ€™ll end this section with a question that <em>I</em> donâ€™t
      know the answer to. Suppose you continue this
      interference-pattern drawing further to the right, past the
      point where the line slopes at 45Â°:</p>
    <div>
      <div>
        <figure>
          <img src="https://jamiepalatnik.com/day-25-demo-palooza/interfering-lines-2.png"/>
          <figcaption>A similar interference pattern going beyond the
            45Â° slope</figcaption>
        </figure>
      </div>
    </div>
    <p>At the 45Â° mark, something changes fundamentally. Those
      horizontal black and white stripes have changed direction, and
      now theyâ€™re sloping at what looks like a constant angle.</p>
    <p>I understand why <em>something</em> has changed. In this style
      of pixelated line drawing, a line thatâ€™s closer to horizontal
      than vertical can have multiple pixels on each row, and only one
      in each column, whereas if itâ€™s closer to vertical than
      horizontal, itâ€™s the other way round. So thatâ€™s a good reason
      why the picture should look fundamentally different
      in <em>some</em> way on opposite sides of the 45Â° line.</p>
    <p>But why does that give rise to a <em>consistently sloping</em>
      set of coloured stripes, and not any other kind of weird effect?
      Iâ€™ve never sat down and worked that one out.</p>
    <h3 id="halfadder">The â€œhalf-adder identityâ€</h3>
    <p>I said <a href="#mod2">in a previous section</a> that one way
      to look at the expression <i>a</i>Â XORÂ <i>b</i> is that itâ€™s the
      remainder mod 2 of <i>a</i>Â +Â <i>b</i>. Another way to say that
      is that itâ€™s the <em>low-order binary digit</em>
      of <i>a</i>Â +Â <i>b</i>.</p>
    <p>What about the high one?</p>
    <p>To answer that, letâ€™s start with a truth table:</p>
    <figure>
      <div>
        <div>
          <table>
            <tbody><tr><th><i>a</i></th><th><i>b</i></th><th><i>a</i>Â +Â <i>b</i></th><th>High bit</th><th>Low bit</th></tr>
            <tr><th>0</th><th>0</th><td>0</td><td>0</td><td>0</td></tr>
            <tr><th>0</th><th>1</th><td>1</td><td>0</td><td>1</td></tr>
            <tr><th>1</th><th>0</th><td>1</td><td>0</td><td>1</td></tr>
            <tr><th>1</th><th>1</th><td>2</td><td>1</td><td>0</td></tr>
          </tbody></table>
        </div>
      </div>
      <figcaption>Truth tables for the two bits of the sum <i>a</i>Â +Â <i>b</i></figcaption>
    </figure>
    <p>Here, Iâ€™ve shown the actual value of the
      sum <i>a</i>Â +Â <i>b</i> for each possible input. Then Iâ€™ve
      written each of those values in binary, so that 0 and 1 become
      00 and 01, and 2 becomes 10. You can see that the high bit of
      the sum is only 1 if <em>both</em> inputs are set.</p>
    <p>In other words: the low bit of <i>a</i>Â +Â <i>b</i>
      is <i>a</i>Â XORÂ <i>b</i>, and the high bit
      is <i>a</i>Â ANDÂ <i>b</i>. In other words, <i>a</i>Â +Â <i>b</i>Â =Â (<i>a</i>Â XORÂ <i>b</i>)Â +Â 2Â Ã—Â (<i>a</i>Â ANDÂ <i>b</i>).</p>
    <p>That makes sense, if <i>a</i> and <i>b</i> are individual bits.
      What if we did the same operations bitwise, on whole integers?</p>
    <p>If <i>a</i> and <i>b</i> are 32-bit integers (for example),
      then weâ€™ve already <a href="#carryless">seen above</a>
      that <i>a</i>Â XORÂ <i>b</i> is the integer you get if you add the
      bits in each position, but not carrying from one bit position to
      the next. But also, <i>a</i>Â ANDÂ <i>b</i> is an integer
      consisting of <em>exactly</em> those carry bits: the <i>n</i>th
      bit of <i>a</i>Â ANDÂ <i>b</i> is 1 precisely when a carry
      bit <em>should</em> have been generated in that bit position but
      wasnâ€™t.</p>
    <p>If there should have been a carry bit out of the <i>n</i>th
      place, then it should have been added to the (<i>n</i>Â +Â 1)st
      place. But itâ€™s not too late â€“ we can add it anyway!</p>
    <p>In fact, the above equation for single bits <i>a</i>,Â <i>b</i>
      is <em>still</em> true<label for="footnoteinput-proof-exercise"><sup id="footnote-proof-exercise">6</sup><span>If youâ€™re
      mathematically minded and still donâ€™t find this instantly
      obvious, you might want to try actually <em>proving</em> it. I
      leave this mostly as an exercise for the reader, but a hint
      would be: break down each of the input integers <i>a</i>
      and <i>b</i> as the sum of terms like
      2<sup><i>n</i></sup><i>a</i><sub><i>n</i></sub> (each one being
      an individual bit of one of the inputs times a power of 2), and
      collect together the terms on each side that involve the
      same <i>n</i>.</span><span>6</span></label>, even when <i>a</i>,Â <i>b</i> are whole
      integers, and the XOR and AND operators are bitwise:</p>
    <p><i>a</i>Â +Â <i>b</i>Â =Â (<i>a</i>Â XORÂ <i>b</i>)Â +Â 2Â Ã—Â (<i>a</i>Â ANDÂ <i>b</i>)</p>
    <p>Just to show it in action, hereâ€™s a demonstration set of
      values, chosen so that the binary values cover a lot of possible
      bit combinations, but the decimal values make it immediately
      clear that the final line is also <i>a</i>Â +Â <i>b</i>:</p>
    <figure>
      <div>
        <div>
          <table>
            <tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr>
            <tr><th><i>a</i></th><td><code>00111101011011000100101001010010</code></td><td><code>3D6C4A52</code></td><td><code>1030507090</code></td></tr>

            <tr><th><i>b</i></th><td><code>00001100001010011011100010000000</code></td><td><code>0C29B880</code></td><td><code>Â 204060800</code></td></tr>

            <tr><th><i>a</i>Â ANDÂ <i>b</i></th><td><code>00001100001010000000100000000000</code></td><td><code>0C280800</code></td><td><code>Â 203950080</code></td></tr>

            <tr><th>2Â Ã—Â (<i>a</i>Â ANDÂ <i>b)</i></th><td><code>00011000010100000001000000000000</code></td><td><code>18501000</code></td><td><code>Â 407900160</code></td></tr>

            <tr><th><i>a</i>Â XORÂ <i>b</i></th><td><code>00110001010001011111001011010010</code></td><td><code>3145F2D2</code></td><td><code>Â 826667730</code></td></tr>

            <tr><th>(<i>a</i>Â XORÂ <i>b</i>)Â +Â 2Â Ã—Â (<i>a</i>Â ANDÂ <i>b)</i></th><td><code>01001001100101100000001011010010</code></td><td><code>499602D2</code></td><td><code>1234567890</code></td></tr>
          </tbody></table>
        </div>
      </div>
      <figcaption>A demonstration case of adding via AND and XOR</figcaption>
    </figure>
    <p>I think of this as the â€œhalf-adder identityâ€ (although as far
      as I know thatâ€™s not a standard name). The name comes from the
      hardware design concept of a â€œhalf-adderâ€, which is a small
      logic component consisting of just an AND and XOR gate, doing
      addition of two bits in hardware:</p>
    <figure>
      <div>
        <p><img src="https://jamiepalatnik.com/day-25-demo-palooza/half-adder.svg"/>
        </p>
      </div>
      <figcaption>A half-adder implemented with two logic gates</figcaption>
    </figure>
    <p>Itâ€™s called a <em>half</em> adder because in order to do full
      binary addition you need two of these per bit, because you have
      to add <em>three</em> input bits: two from the input integers
      and one carried from the bit position to the right. So you build
      a <em>full</em> adder out of two half-adders, one
      adding <i>a</i> to <i>b</i> and the second adding the sum of
      those two to <i>c</i>, with one extra
      gate<label for="footnoteinput-or-xor"><sup id="footnote-or-xor">7</sup><span>The extra gate that combines the carry
      bits can be either an OR gate or an XOR gate, whichever is
      easier. It doesnâ€™t matter which, because the two types of gate
      only disagree in the case where both their inputs are 1, and in
      this circuit, that canâ€™t happen! If the carry from the first
      half-adder is 1, then its other output <em>must</em> be 0, which
      means thereâ€™s no carry from the second half-adder.</span><span>7</span></label> to
      combine the two carry bits:</p>
    <figure>
      <div>
        <p><img src="https://jamiepalatnik.com/day-25-demo-palooza/full-adder.svg"/>
        </p>
      </div>
      <figcaption>A full adder implemented with two half-adders and an extra gate</figcaption>
    </figure>
    <p>â€œBut hang on,â€ you might be thinking. â€œIf you need two
      half-adders per bit to do a full addition, how is that
      â€˜half-adder identityâ€™ of yours getting away with
      just <em>one</em> pair of AND and XOR operations per bit?â€ Or
      you might think: â€œYouâ€™ve generated an <em>output</em> carry from
      each bit position, but where have you handled the <em>input</em>
      carry?â€ Or you might think â€œHang on, how do you arrange for a
      carry in the lowest bit of the addition to potentially propagate
      all the way up to the top, when the only left shift in this
      expression moves data by only one bit?â€</p>
    <p>The answer to all three questions is: because of the + sign on
      the right-hand side of the identity. After we compute the
      outputs of <em>one</em> half-adder on each pair of input bits,
      producing a word full of low bits and a word full of carries â€¦
      we recombine the two words <em>using an addition</em>. Thatâ€™s
      what finishes the job of propagating carries.</p>
    <p>In other words, unlike the hardware half-adder, the â€œhalf-adder
      identityâ€ <em>doesnâ€™t</em> build an addition out of only simpler
      operations. It builds an addition out of two simpler
      operations <em>and an addition</em>.</p>
    <p>â€œWell, in <em>that</em> case,â€ you might think, â€œisnâ€™t it a
      complete cheat, and not useful for anything?â€</p>
    <p>Not quite! Itâ€™s true that this identity is not <em>often</em>
      useful. But â€˜not oftenâ€™ isnâ€™t â€˜neverâ€™, and in unusual
      circumstances there are uses for it.</p>
    <h4 id="average">Averaging two integers</h4>
    <p>Hereâ€™s one occasional use for the half-adder identity.</p>
    <p>Suppose you need to calculate the average of two integers, each
      the size of your CPUâ€™s registers (letâ€™s say, 32 bits). In other
      words, you want to add two integers, and then divide by 2 (or,
      equivalently, shift right by 1 bit).</p>
    <p>But adding two 32-bit integers makes a 33-bit sum, which
      doesnâ€™t fit in your register. If you just do the simple thing â€“
      add and then shift â€“ then you potentially get the wrong answer,
      because the topmost bit has been lost in between the addition
      and the right shift.</p>
    <p>Most CPUs have a good answer to this. The add instruction can
      (perhaps optionally) set a â€˜carry flagâ€™ to indicate whether a 1
      bit was carried off the top of an addition; also, thereâ€™s
      usually a special form of right shift by 1 bit that brings the
      carry bit back in to the top of the word. (On Arm, that
      instruction is called RRX; on x86, RCR.) So the way to average
      two numbers without an overflow problem is to do ADD followed by
      RRX.</p>
    <p>But a few CPUs canâ€™t do it that way. Some architectures donâ€™t
      have a carry flag at all, or indeed any other flags: for
      example, MIPS, RISC-V, and the historic DEC Alpha. In other
      situations, there is a carry flag, but no convenient RRX
      instruction, so that it would take a lot of effort to recover
      the carry bit and put it at the top of the shifted-right word;
      for example, the initial versions of Armâ€™s space-efficient
      â€œThumbâ€ instruction set discarded the RRX instruction, simply
      because there wasnâ€™t room for everything.</p>
    <p>So, if you <em>canâ€™t</em> do ADD and RRX, how do you most
      efficiently compute your average?</p>
    <p>The half-adder identity provides the answer. If the sum of the
      two inputs is expressed like this (with the &lt;&lt; operation
      meaning â€˜shift leftâ€™) â€¦</p>
    <p><i>a</i>Â +Â <i>b</i>Â =Â (<i>a</i>Â XORÂ <i>b</i>)Â +Â (<i>a</i>Â ANDÂ <i>b</i>)Â &lt;&lt;Â 1</p>
    <p>â€¦ but what you want is the same sum shifted right by one bit,
      then all you have to do is to shift the XOR term <em>right</em>,
      instead of the AND term left:</p>
    <p>(<i>a</i>Â +Â <i>b</i>)Â &gt;&gt;Â 1Â =Â (<i>a</i>Â XORÂ <i>b</i>)Â &gt;&gt;Â 1Â +Â (<i>a</i>Â ANDÂ <i>b</i>)</p>
    <h4 id="constructing-xor">Implementing XOR itself</h4>
    <p>Also on the theme of â€˜compensating for missing features of your
      CPUâ€™, hereâ€™s another thing you can do with the half-adder
      identity.</p>
    <p>In the 1970s, Data General made a CPU with such a small
      instruction set that it didnâ€™t even <em>implement</em> the
      bitwise XOR operation â€“ it had AND, but not XOR. On that CPU,
      what would you have done if you needed XOR?</p>
    <p>The answer is to use the half-adder identity in reverse!
      Instead of starting from <i>a</i>Â XORÂ <i>b</i> and
      making <i>a</i>Â +Â <i>b</i>, do it the other way round. Given the
      equation</p>
    <p><i>a</i>Â +Â <i>b</i>Â =Â (<i>a</i>Â XORÂ <i>b</i>)Â +Â 2Â Ã—Â (<i>a</i>Â ANDÂ <i>b</i>)</p>
    <p>you can rearrange it to get</p>
    <p><i>a</i>Â XORÂ <i>b</i>Â =Â (<i>a</i>Â +Â <i>b</i>)Â âˆ’Â 2Â Ã—Â (<i>a</i>Â ANDÂ <i>b</i>)</p>
    <p>So if you have addition and bitwise AND, you can build XOR out
      of them!</p>
    <p>(Exercise for the reader: you can also make bitwise OR in the
      same way, by removing the â€œ2Â Ã—â€ in the expression â€“ subtract
      (<i>a</i>Â ANDÂ <i>b</i>) <em>without</em> doubling it first.)</p>
    <h3 id="bitswap">Swapping bits in a word</h3>
    <p>Suppose you have a collection of bits packed into an integer,
      and you want to swap the positions of two of them. Whatâ€™s the
      best way?</p>
    <p>Thereâ€™s no one answer to that question, because a lot depends
      on what hardware youâ€™re doing it on (and what useful
      capabilities it has), and also, on what else you want to do at
      the same time. For example, swapping a <em>lot</em> of pairs of
      bits can often be done more efficiently than by swapping one
      pair at a time. So this is one of these simple-looking questions
      that becomes surprisingly complicated if you add â€œâ€¦ and do it
      absolutely as fast as possibleâ€ to the requirements.</p>
    <p>But more than one of the good techniques for solving this
      problem are based on XOR, because you can divide the problem
      into two essential cases:</p>
    <ul>
      <li>If the two bits you want to swap are both 0, or both 1, then
        swapping them <em>makes no difference anyway</em>. So in that
        situation, you donâ€™t need to do anything at all.</li>
      <li>If one of the bits is 0 and the other is 1, then swapping
        them <em>is the same as inverting each one</em>.</li>
    </ul>
    <p>In other words, a reasonable strategy is: first find out if the
      bits are different, and then, if so, invert both.</p>
    <p>Both of those are applications of XOR. If you XOR the two bits
      with each other, that will give you 1 if theyâ€™re different. And
      then you can use that value as input to another XOR, using it as
      a â€˜controlled bit flipâ€™ to invert both bits, but only if they
      were different to start with.</p>
    <h4 id="shift">Using XOR and shifts</h4>
    <p>Hereâ€™s a piece of pseudocode that uses that principle to swap
      two bits. It assumes that <code>pos0</code>
      and <code>pos1</code> are the <em>positions</em> of the two bits
      to be swapped, with 0 meaning the units bit, 1 the bit to its
      left, and so on. It also assumes that <code>pos1 &gt;
      pos0</code>.</p>
    <figure>
      <div>
        <div>
          <pre># Constants derived from pos0 and pos1
bit0 = (1 &lt;&lt; pos0)      # an integer with just the bit at pos0 set
distance = pos1 - pos0  # distance between the two bits

# Computation starting from the input value
diff_all = input XOR (input &gt;&gt; distance)
diff_one = diff_all AND bit0
diff_dup = diff_one XOR (diff_one &lt;&lt; distance)
output = input XOR diff_dup</pre>
        </div>
      </div>
      <figcaption>Pseudocode to swap two bits by XOR and shifts</figcaption>
    </figure>
    <p>(If you need to swap the same pair of bit positions in a lot of
      inputs, the first two lines can be precomputed just once and
      reused.)</p>
    <p>The first value we compute, <code>diff_all</code>, is made by
      shifting the input right by the distance between the two bit
      positions. So each bit of <code>diff_all</code> tells you the
      difference between a pair of the input bits separated by that
      distance.</p>
    <p>But weâ€™re only interested in one <em>particular</em> pair of
      the input bits. So next we compute <code>diff_one</code>, which
      uses bitwise AND to pick out just a single bit
      of <code>diff_all</code>. This will be 0 if the two bits we want
      to swap are the same; if theyâ€™re different, it will be equal
      to <code>bit0</code> (i.e. it will have a single bit set at
      position <code>pos0</code>).</p>
    <p>If we XORed that value back into <code>input</code>, it would
      conditionally flip the lower of the two bits we want to swap.
      But we want to conditionally flip <em>both</em> of them. So now
      we duplicate the useful bit of <code>diff_one</code> into the
      other bit position, by shifting it left by the distance between
      the two bits, and combining that with <code>diff_one</code>
      itself to get <code>diff_dup</code>. This has a 1
      in <em>both</em> of the target bit positions if the two bits
      need to be flipped, and is still all 0 otherwise.</p>
    <p>So now, XORing that into the input will flip both bits if
      necessary, and leave them alone otherwise.</p>
    <p>This looks like a lot of effort to swap two bits. But one nice
      thing about it is that itâ€™s just as easy to swap <em>lots</em>
      of pairs of bits, if every pair is the same distance apart.
      (That is, swapping bit <i>i</i> with bit <i>iÂ +Â d</i>, for
      multiple different values of <i>i</i> but the same <i>d</i>
      every time.) In that situation, the only thing you need to
      change is the step that makes <code>diff_one</code>
      from <code>diff_all</code>: instead of ANDing with
      a <em>one</em>-bit constant, AND with a <em>multiple</em>-bit
      constant, containing the low bit of every pair you want to
      swap.</p>
    <p>Why might you want to do <em>that</em> in turn? Because thereâ€™s
      a general technique called a <em>BeneÅ¡ network</em> which lets
      you encode a completely arbitrary permutation in the form of a
      series of stages, with each stage swapping a lot of pairs of
      things separated by the same distance. The distances iterate
      through powers of 2 and then back again: for example, you might
      do a set of swaps with distances 16, 8, 4, 2, 1, 2, 4, 8, 16.
      (Or the other way round if you prefer â€“ it doesnâ€™t really
      matter <em>what</em> order you go through the distances in.) So
      if you want to permute the bits of a word in a completely
      arbitrary way, a computational primitive that swaps a lot of
      bits at equal separation is just what you want for each stage of
      a BeneÅ¡ network.</p>
    <h4 id="parity">Using XOR and just a two-bit word</h4>
    <p>In the simpler case where youâ€™re only swapping a single pair of
      bits, here are a couple of simpler (therefore faster)
      techniques.</p>
    <p>Suppose that you have an integer called <code>bits</code>,
      which has exactly two bits set, at the positions you want to
      swap. What happens if you make the bitwise AND of that value
      with your input? There are three possibilities:</p>
    <ul>
      <li>(<code>input</code>Â ANDÂ <code>bits</code>)Â =Â 0. Both bits are
        0, so you donâ€™t need to do anything to swap them.</li>
      <li>(<code>input</code>Â ANDÂ <code>bits</code>)Â =Â <code>bits</code>.
        Both bits are 1, so you still donâ€™t need to do anything.</li>
      <li>Otherwise, the two bits have different values, so you can
        swap them by XORing the input with <code>bits</code>.</li>
    </ul>
    <p>On several CPU architectures this kind of test is easier than
      doing the full business above with shifts â€“ especially because
      you donâ€™t need the input bit positions specified in multiple
      ways. For example, many CPU architectures would let you do
      something like this (though the syntax of the instructions would
      vary)</p>
    <figure>
      <div>
        <div>
          <pre>  tmp = input AND bits         # zero if both bits are 0
  if tmp == 0, jump to â€˜doneâ€™  # in which case we have no work to do
  tmp = tmp XOR bits           # now zero if both bits are 1
  if tmp == 0, jump to â€˜doneâ€™  # so again we have nothing to do
  input = input XOR bits       # in any other case, flip both bits
done:</pre>
        </div>
      </div>
      <figcaption>â€˜Machine pseudocodeâ€™ to swap two bits</figcaption>
    </figure>
    <p>You could typically do this with about five instructions,
      because CPUs will typically set a flag as a side effect of each
      AND or XOR operation to tell you whether the result was zero.
      Or, if not that, they might have a single instruction to jump to
      a different location depending on whether a register is zero.</p>
    <p>On the 32-bit Arm architecture you can do this in just three
      instructions, because you donâ€™t need the jumps: instead, you can
      make the two XOR operations <em>conditional</em>, because 32-bit
      Arm generally lets you make an instruction conditional on the
      current CPU flags without needing a separate branch instruction
      to skip past it.</p>
    <p>And on x86, you can also do it in three instructions for a
      completely different reason. x86 has an even stranger processor
      status flag called the â€˜parity flagâ€™. This is set whenever the
      result of an operation has <em>an odd number of 1 bits</em>. And
      in this situation, thatâ€™s exactly the case where you need to
      flip the bits! So you donâ€™t need to test separately for the
      â€˜both bits 0â€™ and â€˜both bits 1â€™ cases: they can be tested
      together, by checking if the parity flag says there were an even
      number of 1 bits in the result of the AND operation.</p>
    <figure>
      <div>
        <div>
          <pre>  ANDS    tmp, input, bits
  EORSNE  tmp, tmp, bits
  EORNE   input, input, bits</pre>
        </div>
        <div>
          <pre>  TST     input, bits
  JPE     done
  XOR     input, bits
done:</pre>
        </div>
      </div>
      <figcaption>Arm and x86 machine code to swap two bits</figcaption>
    </figure>
    <h3 id="swap">Three XORs make a swap</h3>
    <p>In a computer program, how do you swap two numbers?</p>
    <p>It depends on the platform. Some programming languages have a
      dedicated function for it, like C++â€™s <code>std::swap</code>.
      Others have a convenient syntax for assigning multiple variables
      at once: in Rust you can write <code>(a, b) = (b, a)</code>
      (assuming the variables are <code>mut</code>), and in Python the
      same thing works without even needing the brackets. Even some
      machine languages support swapping two registers: for example,
      x86 has the XCHG (exchange) instruction.</p>
    <p>But suppose youâ€™re in a simpler language without any of those
      features, like C. Or suppose youâ€™re programming machine code on
      an architecture without a â€˜swap two registersâ€™
      instruction<label for="footnoteinput-swp"><sup id="footnote-swp">8</sup><span>There is an Arm instruction called
      SWP for â€˜swapâ€™, but itâ€™s not what you want for this kind of
      purpose. It swaps a register with a value in <em>memory</em>,
      not two registers. Itâ€™s not really for ordinary computation:
      itâ€™s intended for atomically synchronising between threads,
      which is very hard to get right and well beyond the scope of
      this article!</span><span>8</span></label>. The usual idiom is to use a temporary
      variable:</p>
    <figure>
      <div>
        <div>
          <pre>a_orig = a;
a = b;
b = a_orig;</pre>
        </div>
      </div>
      <figcaption>C code for swapping two integers <code>a</code>
        and <code>b</code></figcaption>
    </figure>
    <p>You need the temporary variable because in C you can only
      assign one variable at a time, so after you execute the
      assignment <code>a = b</code>, both variables now have the same
      value (namely the value originally in <code>b</code>), and
      youâ€™ve lost the previous value of <code>a</code> completely â€“
      unless youâ€™d saved it in a third location first.</p>
    <p>But just occasionally you donâ€™t have a spare register, or at
      least it would cost you a lot of extra effort to free one up. It
      turns out that thereâ€™s a different way to swap two
      values, <em>without</em> needing a spare storage location, using
      three XORs:</p>
    <figure>
      <div>
        <div>
          <pre>a = a XOR b;
b = b XOR a;
a = a XOR b;</pre>
        </div>
      </div>
      <figcaption>Pseudocode for swapping <code>a</code>
        and <code>b</code> using three XORs</figcaption>
    </figure>
    <p>Why does <em>that</em> work?!</p>
    <p>Itâ€™s difficult to explain this clearly, because we donâ€™t have
      separate names for the <em>variables</em> weâ€™re mutating, and
      the <em>values</em> that are stored in them. So letâ€™s start by
      making up some names. Letâ€™s say that the value that
      was <em>originally</em> stored in <code>a</code> is
      called <code>a_orig</code> (as I already did above in that C
      snippet). And similarly, the original value of <code>b</code> is
      called <code>b_orig</code>.</p>
    <p>Then this is what happens with the three XORs:</p>
    <figure>
      <div>
        <div>
          <table>
            <tbody><tr><td></td><th>Value in <code>a</code></th><th>Value in <code>b</code></th></tr>
            <tr><th>Before doing anything</th><td><code>a_orig</code></td><td><code>b_orig</code></td></tr>
            <tr><th>After <code>a = aÂ XORÂ b</code></th><td><code>a_origÂ XORÂ b_orig</code></td><td><code>b_orig</code></td></tr>
            <tr><th>After <code>b = bÂ XORÂ a</code></th><td><code>a_origÂ XORÂ b_orig</code></td><td><code>a_orig</code></td></tr>
            <tr><th>After <code>a = aÂ XORÂ b</code></th><td><code>b_orig</code></td><td><code>a_orig</code></td></tr>
          </tbody></table>
        </div>
      </div>
      <figcaption>Intermediate states of the three-XOR swap algorithm</figcaption>
    </figure>
    <p>The basic idea is that there are three values we care about:
      the two input values, and their bitwise XOR. And if you have
      three values, one of which is the XOR of the other two, then
      knowing <em>any two</em> of them is enough to work out the
      third, because <em>each</em> of them is the XOR of the other
      two. So at each step of this algorithm, we set one of the
      variables to the XOR of both variables, and thatâ€™s always a
      reversible operation (in fact doing exactly the same thing
      again <em>would</em> reverse it), so <em>information is never
      lost</em>. At every stage, XORing the two values we have in our
      variables recovers whichever one we currently <em>donâ€™t</em>
      have. And it takes three XORs to rotate through all the possible
      options, which is why after three steps weâ€™re back to having the
      original values of <i>a</i> and <i>b</i> â€“ but the other way
      round.</p>
    <p>I should say at this point that this isnâ€™t a trick you
      can <em>only</em> do with XOR â€“ it doesnâ€™t depend on XOR having
      any essential magic that other operations donâ€™t. You can use the
      same technique with additions and subtractions if you prefer, by
      setting one variable to the <em>sum</em> of the other two:</p>
    <figure>
      <div>
        <div>
          <pre>a = a + b;     // now a = (a_orig + b_orig)
b = a - b;     // now b = (a_orig + b_orig) - b_orig = a_orig
a = a - b;     // now a = (a_orig + b_orig) - a_orig = b_orig</pre>
        </div>
      </div>
      <figcaption>Pseudocode for swapping <code>a</code>
        and <code>b</code> by addition and subtraction</figcaption>
    </figure>
    <p>But the XOR version is more common (among people who want to do
      this trick at all). Partly thatâ€™s because itâ€™s simpler: all
      three operations are the same, whereas in the additive version
      all <em>three</em> operations are different (the two
      subtractions are opposite ways round, in the sense of which of
      their inputs they overwrite).</p>
    <p>But also, the XOR version is more flexible, because â€“ just like
      in the <a href="https://jamiepalatnik.com/day-25-demo-palooza/pixels">pixel graphics</a> application â€“ you can
      use it in situations where addition would cause unwanted extra
      carries.</p>
    <p>For example, hereâ€™s a case where you can swap with three XORs,
      which wouldnâ€™t work with three additions. Suppose
      that <code>a</code> and <code>b</code> are 32-bit registers,
      and <code>n</code> is some number less than 32:</p>
    <figure>
      <div>
        <div>
          <pre>a = a XOR (b &lt;&lt; n);
b = b XOR (a &gt;&gt; n);
a = a XOR (b &lt;&lt; n);</pre>
        </div>
      </div>
      <figcaption>Swap part of <code>a</code> with part
      of <code>b</code></figcaption>
    </figure>
    <p>The effect of this code is to swap <em>parts</em> of the two
      values. Specifically, only the upper (32Â âˆ’Â <i>n</i>) bits
      of <code>a</code> are affected, because every time we XOR
      something into <code>a</code>, the value we XOR in has been
      shifted left by <i>n</i> bits, so its lower <i>n</i> bits are
      zero. Similarly, the value XORed into <code>b</code> has been
      shifted right <i>n</i> bits, so its top <i>n</i> bits are zero,
      so the operation only affects the lower (32Â âˆ’Â <i>n</i>) bits
      of <code>b</code>.</p>
    <figure>
      <div>
        <p><img src="https://jamiepalatnik.com/day-25-demo-palooza/shifted-swap.svg"/>
        </p>
      </div>
      <figcaption>The parts of <code>a</code> and <code>b</code>
        affected by the above code</figcaption>
    </figure>
    <p>In fact, this code precisely swaps those two segments of the
      inputs: the upper (32Â âˆ’Â <i>n</i>) bits of <code>a</code> are
      swapped with the lower (32Â âˆ’Â <i>n</i>) bits
      of <code>b</code>.</p>
    <p>One use for this trick Iâ€™ve seen in the past: suppose that you
      were operating on a buffer of graphics data with one byte per
      pixel. Youâ€™ve loaded a word from the buffer containing four
      pixels <i>PQRS</i>, and youâ€™re trying to magnify it by a factor
      of two, so that you want to create words containing <i>PPQQ</i>
      and <i>RRSS</i>.</p>
    <p>The neatest way I know to do it is to use this â€˜shifted swapâ€™
      technique twice:</p>
    <ul>
      <li>Initialise both <code>a</code> and <code>b</code> to copies
        of the input word <i>PQRS</i>.</li>
      <li>Do a shifted swap with <i>n</i>Â =Â 8, swapping the top 24
        bits of <code>b</code> (containing <i>PQR</i>) with the bottom 24 of <code>a</code> (containing <i>QRS</i>). Now <code>a</code>Â =Â <i>PPQR</i>, and <code>b</code>Â =Â <i>QRSS</i>.</li>
      <li>Do a second shifted swap, this time with <i>n</i>Â =Â 24,
        swapping the top 8 bits of <code>b</code>
        (containing <i>Q</i>) with the bottom 8 of <code>a</code>
        (containing <i>R</i>). Now <code>a</code>Â =Â <i>PPQQ</i>,
        and <code>b</code>Â =Â <i>RRSS</i>. Done!</li>
    </ul>
    <h3 id="nim">Winning at the game of Nim</h3>
    <p>Possibly the <em>strangest</em> application of XOR that I know
      of is in game theory.</p>
    <p>In the simple combinatorial game of â€˜Nimâ€™, there are some piles
      of counters. On your turn, you must choose one pile and remove
      any number of counters you like from it, from 1 to the whole
      pile, or anywhere in between (but not 0). You lose if you have no
      possible move<label for="footnoteinput-misere"><sup id="footnote-misere">9</sup><span>In fact, Nim comes in two
      well-known forms, with opposite win conditions. In the variant
      discussed here, your aim is to take the last counter and leave
      your opponent with no move. In the other variant, the win
      condition is exactly the reverse: you aim to make
      your <em>opponent</em> take the last counter! The strategy for
      the latter is a tiny bit more complicated, but not too bad. I
      only discuss the simple version here.</span><span>9</span></label>, which only
      happens if there are no counters at all remaining (because your
      opponent took the last one).</p>
    <p>How do you determine a good move in this game?</p>
    <figure>
      <div>
        <p><img src="https://jamiepalatnik.com/day-25-demo-palooza/nim-example.svg"/>
        </p>
      </div>
      <figcaption>The unique winning move here is to take 3 counters
        from the pile of 12. But why?</figcaption>
    </figure>
    <p>The basic idea is to identify a set of <em>losing
      positions</em>: states of the game in which, if itâ€™s your turn,
      youâ€™re in trouble. This set should have the properties that:</p>
    <ol>
      <li>the position where you have actually lost counts as a losing
        position. (Itâ€™d be embarrassing to get that wrong.)</li>
      <li>from any losing position, <em>every</em> possible move
        leaves the other player in a non-losing position. (If youâ€™re
        in trouble, you stay in trouble no matter what you do.)</li>
      <li>from any non-losing position, there should be <em>at least
        one</em> move which leaves the other player in a losing
        position. (If you have the advantage, thereâ€™s a way to keep
        it, though you may have to think about it.)</li>
    </ol>
    <p>For a game like Nim, where every move reduces the total number
      of counters, you could analyse the game computationally by
      iterating through all the possible positions in order of
      increasing size, and for each one, classify it as â€˜losingâ€™ or
      â€˜non-losingâ€™ according to the above rules, by checking the
      results of smaller positions youâ€™ve already classified. Of
      course this only gets you the status of a finite number of
      positions (until you run out of patience to run the computer
      program), but you might hope to see a pattern, which you could
      then try to prove.</p>
    <p>If you try this, it turns out that there is indeed a pattern,
      and itâ€™s a surprising one. The losing positions in Nim are
      precisely positions in which <em>the bitwise XOR of all the pile
      sizes is zero!</em></p>
    <p>Two of the three criteria above are easy to check:</p>
    <ol>
      <li>If there are no counters at all, then XORing all the pile
        sizes together gives you zero, because there arenâ€™t any piles
        at all (or, equivalently, they all have zero height). So the
        â€˜you have lostâ€™ position is a losing position by this XOR
        rule. âœ“</li>
      <li>If youâ€™re in a losing position (all the piles XOR to zero),
        then your move must change exactly one of the pile sizes, say
        from <i>a</i> to <i>b</i>. So the XOR of the pile sizes has
        changed by <i>a</i>Â XORÂ <i>b</i>, which
        (since <i>a</i>Â â‰ Â <i>b</i>) isnâ€™t zero. So any move from a
        losing position goes to a non-losing position. âœ“</li>
    </ol>
    <p>The third condition is the trickiest. From
      a <em>non</em>-losing position â€“ if the piles XOR to some
      nonzero value <i>x</i> â€“ there always needs to be <em>at least
      one</em> move that makes the piles XOR to zero again.</p>
    <p>If you were allowed to <em>add</em> counters to a pile as well
      as removing them, this would be easy. Just modify any pile you
      like by XORing its size with <i>x</i> (which will necessarily
      change it from its previous value), and then the sizes XOR to
      zero.</p>
    <p>But in some cases, this makes a pile bigger, and thatâ€™s not
      allowed. So we need to show that thereâ€™s at least one pile that
      is made <em>smaller</em> by doing this.</p>
    <p>First, how can you tell whether XORing some other
      number <i>n</i> with <i>x</i> makes it bigger or smaller? The
      answer is to look at the <em>highest</em> 1 bit in <i>x</i>. If
      that bit is also 1 in <i>n</i>, it will be 0
      in <i>n</i>Â XORÂ <i>x</i>, and that means <i>n</i>Â XORÂ <i>x</i>
      will be smaller than <i>n</i>. This is true <em>no matter</em>
      what the lower-order bits do, because even if all the lower bits
      change from 0 to 1, the sum of those effects will still be
      (just) smaller than the effect of the high bit changing from 1
      to 0.</p>
    <p>So, if the piles XOR to <i>x</i>, then weâ€™re looking for pile
      sizes which have a 1 in the same place as the highest 1 bit
      of <i>x</i>. Those are exactly the piles for which
      XORing <i>x</i> into the size will make it smaller â€“ meaning we
      can modify that pile in a way that is both within the rules, and
      creates a losing position for the other player.</p>
    <p>So we can find a legal winning move if thereâ€™s at least one
      pile size with a 1 in that bit position. But of course there
      must be one, <em>because</em> that bit is set in <i>x</i>: if
      every pile size has 0 in a given bit position, then <i>x</i>
      does too!</p>
    <p>For an example, letâ€™s calculate whatâ€™s going on in the picture
      above. The three pile sizes are 12, 10 and 3. In binary, those
      are 1100, 1010 and 0011. The bitwise XOR of those values is
      0101, so this isnâ€™t a losing position. To win, we must change a
      pile size by XORing it with 0101. Two of the pile sizes would be
      made bigger by this operation: the two smaller sizes, 1010 and
      0011, have a 0 in the relevant position (theyâ€™re of the
      form <i>x</i>0<i>yz</i>), so they would become 1111 and 0110
      respectively, each larger than their original size. But the
      largest pile, with size 1100, has that bit set, so it becomes
      1001, a smaller value. Therefore, thereâ€™s only one winning move,
      and itâ€™s to reduce the largest pile from size 12 to size 9 by
      removing three counters, as the picture shows.</p>
    <p>One reason I find this a particularly strange place for bitwise
      XOR to show up is that it doesnâ€™t to have anything to do with
      your choice of number base. If youâ€™re writing two
      integers <em>in binary</em>, then it might seem very natural to
      combine corresponding pairs of digits in various ways. But if
      you write the same integers in base 10, or some other base like
      3 or 5, then youâ€™d find yourself imagining a totally different
      set of â€˜digit-wiseâ€™ operations analogous to the bitwise ones
      (like taking the minimum, or maximum, or sum mod 10, of each
      pair of corresponding decimal digits). So youâ€™d expect bitwise
      XOR to show up in situations where it was important that youâ€™d
      written a number in binary. But the winning strategy in Nim
      doesnâ€™t depend on what base you write the pile sizes in, or even
      whether you wrote them down in place-value notation at all â€“ so
      the appearance of bitwise XOR <em>seems</em> to be saying that
      binary is important to the underlying mathematics, whether
      humans have thought of it or not!</p>
    <h2 id="maths">Mathematical concepts that look like XOR</h2>
    <p>That last example, from game theory, is moving more in the
      direction of mathematics rather than practical computing. So
      this is a good moment to change direction and talk about some
      concepts in pure mathematics that are basically XOR with a
      different name, or in a not-very-subtle disguise.</p>
    <p>In some cases, thereâ€™s a whole further area of study that
      follows on from the XOR-like operation, showing that XOR isnâ€™t
      just a useful thing in its own right â€“ itâ€™s also the starting
      point of a lot more.</p>
    <p>In the following sections the mathematics will be more advanced
      than itâ€™s been until now: I donâ€™t have space to describe every
      concept from absolute scratch, so in each section Iâ€™ll have to
      rely on some background knowledge that makes it possible to
      explain the new concept briefly.</p>
    <p>If that isnâ€™t your thing, then I hope youâ€™ve enjoyed the
      previous parts of the article!</p>
    <h3 id="symmdiff">Symmetric difference of sets</h3>
    <p>Iâ€™ll start with a simple one. Thereâ€™s a natural correspondence
      between Boolean logic operations, and operations on sets in set
      theory. For any set <i>X</i>, you can imagine asking the yes/no
      question â€˜Is this particular thing a member of <i>X</i>?â€™. Then,
      set operations on the sets themselves (like union and
      intersection) correspond naturally to Boolean logic operations
      on the answers to those membership queries.</p>
    <p>For example, if you have two sets <i>X</i> and <i>Y</i>, then
      when is some element <i>a</i> in the union <i>X</i>Â âˆªÂ <i>Y</i>?
      Precisely when either <i>a</i> is
      in <i>X</i>, <em>or</em> <i>a</i> is in <i>Y</i>, or both. The
      union operator corresponds to the Boolean (inclusive) OR.</p>
    <p>Similarly, <i>a</i>Â âˆˆÂ (<i>X</i>Â âˆ©Â <i>Y</i>) precisely
      when <i>a</i>Â âˆˆÂ <i>X</i> <em>and</em> <i>a</i>Â âˆˆÂ <i>Y</i>. The
      intersection operator corresponds to AND.</p>
    <p>In this model, what corresponds to XOR? Itâ€™s the <em>symmetric
      difference</em> operator, written <i>X</i>Â âˆ†Â <i>Y</i>: the set
      of elements that are in <em>exactly one</em> of <i>X</i>
      and <i>Y</i>, no matter which one it
      is. <i>a</i>Â âˆˆÂ (<i>X</i>Â âˆ†Â <i>Y</i>) precisely
      when (<i>a</i>Â âˆˆÂ <i>X</i>) XOR (<i>a</i>Â âˆˆÂ <i>Y</i>).</p>
    <p>This correspondence between XOR and symmetric difference means
      that the âˆ† operator has all the same properties as XOR â€“ for
      example, itâ€™s both commutative and associative. Proving this is
      a common introductory exercise in simple set theory, and doing
      it directly can easily lead to half a page of tedious algebra;
      but understanding symmetric difference as â€˜basically XORâ€™, and
      XOR in turn as the same thing as addition mod 2, makes it clear
      that symmetric difference inherits commutativity and
      associativity from addition itself.</p>
    <h3 id="exp2">Groups of exponent 2</h3>
    <p>In group theory, if <i>g</i> is an element of a group <i>G</i>,
      you can ask: is any power of <i>g</i> equal to the group
      identity <i>e</i>? If so, whatâ€™s the <em>smallest</em>
      number <i>n</i>Â &gt;Â 0 such
      that <i>g</i><sup><i>n</i></sup>Â =Â <i>e</i>? This number is
      called the <em>order</em> of the element <i>g</i>. (If there is
      no such integer <i>n</i>, so that all the powers of <i>g</i> are
      different, then we say that <i>g</i> has infinite order.)</p>
    <p>Instead of asking about the order of one specific element
      of <i>G</i> at a time, you can also ask a similar question for
      the whole group at once: is there a single number <i>n</i> such
      that, for <em>every</em>
      element <i>g</i>Â âˆˆÂ <i>G</i>, <i>g</i><sup><i>n</i></sup>Â =Â <i>e</i>?
      The smallest such number is called the <em>exponent</em> of the
      group <i>G</i>. (Again, it may be infinite. If itâ€™s finite, then
      itâ€™s also the lowest common multiple of all the orders of
      individual elements.)</p>
    <p>If a group has exponent 2 in particular, then that means every
      element
      is <em>self-inverse</em>: <i>g</i><sup>2</sup>Â =Â <i>e</i> for
      all <i>g</i>. A standard exercise is to show that this also
      makes the group <em>abelian</em>, i.e. the group operation is
      commutative, i.e. <i>gh</i>Â =Â <i>hg</i> for
      all <i>g</i>,<i>h</i>.</p>
    <p>Group operations are also associative, by definition of a
      group. So in this situation, we have an operation thatâ€™s
      commutative, associative, has an identity (namely <i>e</i>), and
      everything is self-inverse. So if you have a long list of group
      elements combined together, you can reorder it to bring
      identical elements together, and then any two copies of the same
      element cancel out.</p>
    <p>That sounds a lot like XOR â€“ and it is. Every group of exponent
      2 can be understood as a special case of XOR, by imagining that
      each element of the group corresponds to a function (on some
      set <i>X</i> that depends on the group) taking values in {0,Â 1},
      and combining two elements has the effect of taking the bitwise
      XOR (or sum mod 2) of their associated functions.</p>
    <!-- Proof: being abelian, a group G of exponent 2 can be regarded
        as a vector space over GF(2), with scalar multiplication
        defined trivially as 1g=g and 0g=e. All vector spaces have a
        basis (even if it requires AC to construct in some cases), so
        let B be a basis for this space. Then G can also be regarded
        as the set of functions B â†’ GF(2) with finite support, under
        elementwise addition mod 2, i.e. under XOR. -->
    <p>Not every such group contains <em>all</em> the possible
      functions from <i>X</i>Â â†’Â {0,Â 1}. Every <em>finite</em> group of
      exponent 2 does (as long as you donâ€™t define <i>X</i> to have
      spare unused elements), but infinite groups can be more subtle.
      You might have, for example, the set of functions from
      â„•Â â†’Â {0,Â 1} that are eventually all 0, or eventually constant, or
      eventually periodic.</p>
    <p>But all groups of exponent 2 correspond to <em>some</em> set of
      functions with codomain {0,Â 1}, under bitwise XOR.</p>
    <h3 id="nimsum">Nim-sum</h3>
    <p>In an <a href="#nim">earlier section</a> we saw that the losing
      positions in the game of Nim are characterised by the pile sizes
      XORing to zero.</p>
    <p>This isnâ€™t just an isolated mathematical curiosity about one
      obscure game. Nim is central to the theory of <em>Sprague-Grundy
      analysis</em>, which proves a large class of other games to be
      â€˜equivalentâ€™ to Nim in the sense that you can analyse them using
      the same technique.</p>
    <p>However, the class of games that this works for doesnâ€™t include
      most games you might normally play. Itâ€™s limited
      to <em>impartial</em> games, which are those where the set of
      permitted moves donâ€™t change depending on which playerâ€™s turn it
      is. Chess, for example, is <em>not</em> impartial, because each
      piece belongs to a specific player, and the other player isnâ€™t
      allowed to move it. Itâ€™s not enough that one playerâ€™s pieces are
      basically the same as the other playerâ€™s, and move by the same
      rules: chess would only be impartial if both players were
      allowed to move <em>any piece they liked</em>, regardless of
      colour.</p>
    <p>The basic idea is that for any position <i>P</i> in any
      impartial game, you can assign it a number <i>n</i>, known as
      a <em>Grundy number</em>. Then you can treat position <i>P</i>
      as â€˜essentiallyâ€™ the same as a Nim pile containing <i>n</i>
      counters, in the sense that for every smaller
      number <i>m</i>Â &lt;Â <i>n</i>, thereâ€™s a move that
      turns <i>P</i> into a position with Grundy number <i>m</i>, but
      no move that leaves the Grundy number unchanged at <i>n</i>
      itself. (There may or may not be moves that go
      to <em>larger</em> numbers; in that respect Grundy numbers
      differ from actual Nim piles, but this difference turns out not
      to matter.)</p>
    <p>In many of these games, itâ€™s natural to combine multiple
      instances of the game into one bigger one, in a way that gives
      the player the choice of which subgame to make a move in. This
      operation is called making a <em>composite</em>, or sometimes
      a <em>disjunctive sum</em>, of the smaller games. For example, a
      Nim game with multiple piles of counters is exactly the
      composite of smaller Nim games each containing just one of the
      piles, because on your turn you must choose just one of the
      subgames (piles) to modify, and make a move that would be legal
      in that subgame by itself.</p>
    <p>How do you work out the Grundy number of a composite game? One
      very convenient way is to first find the Grundy number of each
      component (which are smaller and simpler games, so this is
      usually easier). Then the overall game is a composite of smaller
      games, each one equivalent to a Nim pile of a particular size â€“
      and so its Grundy number is obtained the same way youâ€™d evaluate
      a position in Nim, by combining the smaller gamesâ€™ Grundy
      numbers using bitwise XOR.</p>
    <p>A concrete example is the game of â€˜kaylesâ€™, which starts off
      with a row of counters equally spaced, and on your move you may
      remove any single counter, or two counters directly adjacent. So
      most of the possible first moves divide the starting row into
      two smaller rows, which you then have to play in separately
      (thereâ€™s no move you can make that affects both). Sprague-Grundy
      analysis saves you from having to analyse every
      possible <em>combination</em> of kayles rows: instead, you only
      need to work out the Grundy number of each length
      of <em>single</em> kayles row, and then the Grundy number of a
      composite of multiple rows can be worked out by XOR.</p>
    <p>So bitwise XOR is crucial to this entire branch of game theory.
      For this reason, the operation of bitwise XOR on non-negative
      integers is sometimes referred to by game theorists as
      the <em>nim-sum</em>.</p>
    <h3 id="gf2"><i>GF</i>(2), the finite field of order 2</h3>
    <p>A <em>field</em>, in mathematics, means an algebraic structure
      in which you can add, subtract, multiply and divide any two
      numbers (except for dividing by 0), and you still get a number
      within the original field, and those operations behave
      â€˜sensiblyâ€™ in the same ways youâ€™re used to, both individually
      and in combination with each other. For example, you
      expect <i>a</i>Â +Â <i>b</i>Â =Â <i>b</i>Â +Â <i>a</i>,
      and <i>a</i>Â âˆ’Â <i>a</i>Â =Â 0,
      and <i>a</i>(<i>b</i>Â +Â <i>c</i>)Â =Â <i>ab</i>Â +Â <i>ac</i>.</p>
    <p>Well-known fields include the real numbers, â„; their superset,
      the complex numbers â„‚; their subset, the rational numbers â„š.
      There are also a great many intermediate fields between â„š and â„,
      such as the set of all numbers of the form <i>a</i>Â +Â <i>b</i>âˆš2
      for rational <i>a</i> and <i>b</i>. A well-known example of
      something thatâ€™s <em>not</em> a field is the integers, â„¤: you
      can add, subtract and multiply integers just fine and still get
      an integer, but if you divide two integers, you can easily get
      something that isnâ€™t an integer any more.</p>
    <p>All of those fields have infinite size. If nothing else, they
      contain the integers: you can start from 1, and keep adding 1,
      and get an endless sequence of numbers, all different and all
      still in the field.</p>
    <p>But thereâ€™s also such a thing as a <em>finite</em> field: a
      structure that obeys all the same rules as any other field, but
      has a finite number of different elements.</p>
    <p>A finite field has a fundamentally different nature from the
      fields Iâ€™ve mentioned so far. If you do that same experiment I
      just mentioned in a finite field â€“ start with 1 and keep adding
      1 â€“ you <em>canâ€™t</em> get an infinite sequence of different
      values, because there arenâ€™t infinitely <em>many</em> different
      values at all. Sooner or later, you must repeat a number youâ€™ve
      run into before.</p>
    <p>In particular, this means that if you count up 1, 2, 3, â€¦ in a
      finite field, you must at some point find that one of those
      numbers is equal to <em>zero</em> again. So finite fields have
      the nature of <em>modular</em> arithmetic, rather than ordinary
      arithmetic: thereâ€™s always some positive number <i>p</i> (known
      as the <em>characteristic</em> of the field, and as it turns
      out, always prime) such that adding 1 to itself <i>p</i> times
      gives zero, and therefore, adding <em>anything</em> to
      itself <i>p</i> times also gives
      zero<label for="footnoteinput-infinite-positive-characteristic"><sup id="footnote-infinite-positive-characteristic">10</sup><span>However, the
      converse isnâ€™t true. Any finite field has positive
      characteristic, but not every field with positive characteristic
      is finite. There are also <em>infinite</em> fields with this
      same modular-arithmetic property. But we wonâ€™t get as far as
      discussing those here.</span><span>10</span></label>. (Which means that you also
      arenâ€™t allowed to divide by <i>p</i>, because you canâ€™t divide
      by zero, and <i>p</i> is the same thing as zero in this
      context.)</p>
    <p>In fact, the simplest example of a finite field
      is <em>precisely</em> modular arithmetic. For a prime <i>p</i>,
      the integers mod <i>p</i> have all the properties of a field, as
      long as you interpret â€˜dividing by <i>n</i>â€™ to mean multiplying
      by the modular inverse of <i>n</i> mod <i>p</i>.</p>
    <p>And the simplest example of <em>that</em> is to take <i>p</i>
      to be the smallest prime of all, namely 2. If you do that, you
      get a field with just two numbers in it: 0 and 1! This field is
      called various things, including <i>GF</i>(2) and ğ”½<sub>2</sub>.
      (â€˜GFâ€™ stands for â€˜Galois fieldâ€™, after the mathematician who
      pioneered research in this area.)</p>
    <p>This field is so small that itâ€™s possible to just <em>list</em>
      all the answers to every basic arithmetic operation:</p>
    <ul>
      <li>Addition works mod 2, so 1Â +Â 1Â =Â 0. Every other addition is
        the same as in normal integers: 0Â +Â 0Â =Â 0 and 0Â +Â 1Â =Â 1.</li>
      <li>Subtraction is <em>exactly the same thing as addition</em>,
        because âˆ’1 and +1 are the same thing in any â€˜mod 2â€™ context.
        In particular, 0Â âˆ’Â 1Â =Â 1 (again, because âˆ’1Â =Â +1), and
        everything else is the same as in normal integers: 0Â âˆ’Â 0Â =Â 0,
        1Â âˆ’Â 1Â =Â 0 and 1Â âˆ’Â 0Â =Â 1.</li>
      <li>Multiplication works <em>exactly</em> like normal integers:
        any multiplication involving 0 gives 0, and 1Â Ã—Â 1Â =Â 1.</li>
      <li>The only case of division thatâ€™s allowed <em>at all</em> is
        dividing a number by 1, which just gives you the same number
        back again. And you canâ€™t divide by 0 at all.</li>
    </ul>
    <p>To put this another way: the elements of this field look like
      booleans (with the usual convention of 0Â =Â false and 1Â =Â true),
      and addition and subtraction both behave like the XOR operator.
      Multiplication behaves like AND: the product is 1 only if both
      inputs are 1, because otherwise, at least one input is 0, and
      multiplying by 0 gives 0.</p>
    <p>This means that weâ€™ve just found out another algebraic property
      of the XOR operator: AND distributes over it, which is to say that</p>
    <p><i>a</i>Â ANDÂ (<i>b</i>Â XORÂ <i>c</i>)Â =Â (<i>a</i>Â ANDÂ <i>b</i>)Â XORÂ (<i>a</i>Â ANDÂ <i>c</i>)</p>
    <p>because thatâ€™s just the translation into Boolean algebra of the
      ordinary algebraic
      identity <i>a</i>(<i>b</i>Â +Â <i>c</i>)Â =Â <i>ab</i>Â +Â <i>ac</i>,
      which is true in <i>GF</i>(2) just like in any other field.</p>
    <p>This tiny field seems as if itâ€™s surely too trivial to actually
      be useful for anything. But it isnâ€™t!</p>
    <h4 id="gf2linalg">Linear algebra over <i>GF</i>(2)</h4>
    <p>All of linear algebra â€“ vectors, matrices, and all that â€“
      starts from the definition of a <em>vector space</em>. That in
      turn depends on the starting point of choosing a <em>field</em>
      which will act as the â€˜scalarsâ€™ of your vector space, and the
      elements of your matrices. Depending on the field you choose,
      the vectors and matrices behave differently. (For example,
      rational, real and complex matrices will disagree on whether a
      matrix is diagonalisable, or has a square root.)</p>
    <p>You can make a vector space over any field you like. Even over
      the trivially simple <i>GF</i>(2), if you want to. If you do
      that, then vectors are particularly simple: each vector looks
      like a sequence of numbers which are all either 0 or 1. You
      could imagine representing this as just a string of individual
      bits in a computer.</p>
    <p>When you add two vectors or matrices, you add each component
      separately, using whatever addition is appropriate to your
      field. If the field is <i>GF</i>(2), that means the addition is
      mod 2, i.e. it works like XOR, independently in each
      component. <em>Adding two vectors or matrices
      over</em> <i>GF</i>(2) <em>corresponds exactly to bitwise
      XOR.</em></p>
    <p>What about multiplying a matrix <i>M</i> by a vector <i>v</i>?
      In ordinary real-number linear algebra, one way to look at this
      is that the output is a linear combination of the columns of the
      matrix <i>M</i>, and the coefficients of the linear combination
      are given by the components of the vector <i>v</i>. That is,
      the <i>i</i>th column of <i>M</i> is multiplied by
      the <i>i</i>th component of <i>v</i>, and all those products are
      added together.</p>
    <p>Over <i>GF</i>(2), this is particularly simple, because the
      components of <i>v</i> are all either 0 or 1, so multiplying one
      of those into a column of <i>M</i> either zeroes it out
      completely, or leaves it unchanaged. So <i>v</i> is just
      specifying a <em>subset</em> of the columns of <i>M</i>. And
      then those columns are added together like vectors
      over <i>GF</i>(2), i.e. combined as if by bitwise XOR.</p>
    <p>Of course, you have to ask why anyone would bother. Whatâ€™s the
      use of vectors and matrices in which the components work mod 2
      in this way? They clearly donâ€™t represent anything in geometry
      (like vectors over â„ do), or anything in quantum mechanics or
      signal processing (which are both applications of vectors over
      â„‚). Is this just a mathematical curiosity not ruled out by the
      definition of a vector space, or are there uses for it?</p>
    <h5 id="ecc">Error-correcting codes</h5>
    <p>There are! And hereâ€™s an example.</p>
    <p>If youâ€™re communicating over a noisy channel like a radio, you
      often want to transmit your data with some redundancy, so that
      if a few bits of your message arenâ€™t received correctly at the
      other end, the receiver can tell that it happened, and perhaps
      even reconstruct the correct message in spite of the errors.</p>
    <p>This idea in general is known as an <em>error-correcting
      code</em>. The general idea is that you expand an original
      message of (say) <i>m</i> bits into some larger number of
      bits <i>n</i>Â &gt;Â <i>m</i> which you send, and then the
      receiver decodes the <i>n</i> bits they receive to get back your
      original <i>m</i>-bit message. So there are only
      2<sup><i>m</i></sup> strings you might have fed to the encoder
      as input, and therefore only 2<sup><i>m</i></sup> of the
      2<sup><i>n</i></sup> possible <i>n</i>-bit strings could have
      been produced as output. The idea of an error-correcting code is
      to â€˜space outâ€™ the valid <i>n</i>-bit codewords in the overall
      space of <i>n</i>-bit strings, so that any two valid codewords
      differ in a large number of bits, and a small number of errors
      canâ€™t turn one into another. If any two valid codewords differ
      by at least <i>k</i> bits, for example, then a transmission
      error that alters fewer than <i>k</i> bits can
      be <em>detected</em> (the receiver recognises that the received
      string isnâ€™t a valid codeword), and an error altering fewer
      than <i>k</i>Â /Â 2 bits can be corrected, by finding
      the <em>nearest</em> valid codeword.</p>
    <p>(Incidentally, â€˜codesâ€™ in this sense arenâ€™t <em>secret</em>
      codes, like in cryptography. The word â€˜codeâ€™ in this context has
      the wider meaning of â€˜any way to convert your message into
      something convenient to send, so that the receiver knows how to
      get the message back at the other endâ€™. For this application, we
      donâ€™t mind if other people can <em>also</em> reconstruct the
      message. Of course, if you wanted to protect the message against
      eavesdroppers <em>and</em> against transmission errors, you
      might put a layer of encryption inside your error-correcting
      code!)</p>
    <p>One very popular way to construct these codes is to make
      them <em>linear</em>, which means that the code basically works
      by having a pair of matrices over <i>GF</i>(2), so that each one
      takes an input string of bits (represented as a vector), and
      outputs another string of bits:</p>
    <ul>
      <li>A <em>generator matrix</em> is used by the sender, to expand
        the original <i>m</i>-bit message into a longer <i>n</i>-bit
        codeword.</li>
      <li>A <em>check matrix</em> is used by the receiver, to find out
        whether the received codeword is valid. Any valid codeword
        multiplied by the check matrix should give the zero vector â€“
        but if the codeword has errors, then the output vector (known
        as the <em>syndrome</em>) contains enough information to
        identify them.</li>
    </ul>
    <p>This is a convenient system because doing it with vectors and
      matrices makes the syndrome independent of the message. That is,
      if the same pattern of error bits occurs in two different
      messages, both of them generate the same syndrome vector. So the
      receiver only needs a lookup table that maps every possible
      syndrome to the pattern of error bits it generates â€“
      they <em>donâ€™t</em> need a separate version of that table for
      each possible codeword.</p>
    <p>And in a few cases you donâ€™t <em>even</em> need the lookup
      table. Hereâ€™s a particularly pretty concrete example of a linear
      code, known as a <em>Hamming code</em>:</p>
    <p>Suppose <i>n</i>, the length of the code, is one less than a
      power of 2. For this example Iâ€™ll take <i>n</i>Â =Â 15, but any
      other number of this form (3, 7, 15, 31, 63, â€¦) also works.</p>
    <p>Weâ€™ll start by saying what the <em>receiver</em> does. The
      receiver has a 15-bit string to analyse. They index the bits
      with all the non-zero 4-bit binary numbers, so that the bits are
      numbered 0001, 0010, 0011, â€¦, 1110, 1111. Then they XOR together
      the indexes of all the 1 bits in the message. Legal codewords
      are defined to be any bit string for which the result of this
      XOR operation is zero.</p>
    <p>So, if you take a legal codeword and flip any single bit, the
      result of this XOR process is not zero, and better than that, it
      directly tells you the index of the bit that was flipped! So a
      Hamming code can correct any one-bit error in a codeword,
      without even needing a lookup table.</p>
    <p>How does the sender construct a codeword? The easiest way is to
      reserve the bits with power-of-2 indices as check bits, and fill
      in all the rest. So the 11 bits corresponding to indices
      that <em>arenâ€™t</em> 1, 2, 4 or 8 are your data bits, which you
      fill in with the actual message. Then the sender XORs together
      the indices of all the 1 bits so far, and sets the final four
      bits however is necessary to make the result become zero: if the
      lowest bit in the XOR value is 1, they set the bit with index 1
      to cancel it out, and the same for the other three bit
      positions.</p>
    <p>So a 15-bit Hamming code lets you transmit 11 bits of actual
      data, and uses the other 4 bits to allow a one-bit error to be
      corrected. In general, a (2<sup><i>d</i></sup>Â âˆ’Â 1)-bit Hamming
      code carries 2<sup><i>d</i></sup>Â âˆ’Â 1Â âˆ’Â <i>d</i> bits of data,
      using the other <i>d</i> bits to correct a one-bit error. In
      other words, a longer Hamming code is more efficient (more
      useful message data per bit transmitted), but correspondingly
      less good at correcting errors (still only one error allowed per
      codeword, but the codewords are longer).</p>
    <p>This is a linear code, because both the encoding and checking
      processes Iâ€™ve described can be written down as a matrix over
      <i>GF</i>(2). The process of XORing together the indices of set
      bits in the received data is exactly the same thing as
      multiplying by a matrix whose columns contain the nonzero binary
      integers 0001, 0010, â€¦, 1111. And the process of constructing a
      message is the same thing as multiplying by a matrix in which
      each column sets one of the bits with a non-power-of-2 index
      plus whatever power-of-2 bits cancel it out â€“ for example, there
      will be a column that sets the bit with index 0101, and then
      cancels it by also setting the bits 0001 and 0100.</p>
    <h4 id="gf2poly">Polynomials over <i>GF</i>(2)</h4>
    <p>Another thing we can do with any field, including <i>GF</i>(2),
      is to consider the set of <em>polynomials</em> with coefficients
      in that field.</p>
    <p>That is, we consider expressions of the form</p>
    <p><i>a</i><sub>0</sub>Â +Â <i>a</i><sub>1</sub><i>x</i>Â +Â <i>a</i><sub>2</sub><i>x</i><sup>2</sup>Â +Â â€¦Â +Â <i>a</i><sub><i>n</i></sub><i>x</i><sup><i>n</i></sup></p>
    <p>in which the numbers <i>a</i><sub><i>i</i></sub> are all either
      0 or 1, and <i>x</i> is an abstract symbol that doesnâ€™t
      represent an actual number at
      all<label for="footnoteinput-formal-polynomials"><sup id="footnote-formal-polynomials">11</sup><span>You might argue:
      if <i>x</i> is a number in <i>GF</i>(2), then itâ€™s either 0 or
      1, and in either case, <i>x</i><sup>2</sup>,
      or <i>x</i><sup>3</sup>, or any higher power of <i>x</i>, are
      all equal to <i>x</i> itself. So under that assumption, any
      polynomial of this kind could be simplified by squashing all the
      higher-order terms down into the <i>x</i><sup>1</sup> term. But
      we <em>donâ€™t</em> do that here, because we donâ€™t make the
      assumption that <i>x</i> is one of the two numbers we know
      about: we leave open the possibility that any two
      powers <i>x</i><sup><i>i</i></sup>
      and <i>x</i><sup><i>j</i></sup> might be
      unequal.</span><span>11</span></label>.</p>
    <p>If you want to add or multiply two of these polynomials, you do
      it exactly as if you were manipulating ordinary polynomials with
      integer or real coefficients: simplify using the
      rule <i>x</i><sup><i>i</i></sup><i>x</i><sup><i>j</i></sup>Â =Â <i>x</i><sup><i>i</i>Â +Â <i>j</i></sup>,
      collect together terms with the same power of <i>x</i>, and
      evaluate each coefficient. The only difference is that the final
      evaluation happens in <i>GF</i>(2), which is equivalent to
      saying â€˜after you calculate the coefficients, reduce each one
      mod 2â€™.</p>
    <p>For example, suppose you wanted to add the polynomials
      (1Â +Â <i>x</i>Â +Â <i>x</i><sup>2</sup>) and
      (<i>x</i>Â +Â <i>x</i><sup>3</sup>). In the ordinary integers, the
      sum would be
      1Â +Â <i>2x</i>Â +Â <i>x</i><sup>2</sup>Â +Â <i>x</i><sup>3</sup>.
      Over <i>GF</i>(2), the answer is exactly the same except that
      the 2<i>x</i> term vanishes, because weâ€™re working mod 2, so 2
      is just the same thing as 0. So you just get
      1Â +Â <i>x</i><sup>2</sup>Â +Â <i>x</i><sup>3</sup>.</p>
    <p>In other words, if <i>P</i> and <i>Q</i> are two polynomials,
      then the <i>x</i><sup><i>i</i></sup> term of <i>P</i>Â +Â <i>Q</i>
      is simply the sum of the <i>x</i><sup><i>i</i></sup> terms
      in <i>P</i> and <i>Q</i> themselves. But the sum is mod 2, i.e.
      it corresponds to XOR. <em>Each coefficient of the sum is the
      XOR of the same coefficient in the two inputs.</em></p>
    <p>In other words, if you consider a polynomial to be represented
      by just its sequence of coefficients, then <em>addition of
      polynomials over</em> <i>GF</i>(2) <em>corresponds exactly to
      bitwise XOR</em>.</p>
    <p>What about multiplication? The same rule works: multiply the
      polynomials the same way you would normally, and then reduce the
      coefficients mod 2. So the product
      (1Â +Â <i>x</i>Â +Â <i>x</i><sup>2</sup>)(<i>x</i>Â +Â <i>x</i><sup>3</sup>),
      for example, would normally work out
      to <i>x</i>Â +Â <i>x</i><sup>2</sup>Â +Â 2<i>x</i><sup>3</sup>Â +Â <i>x</i><sup>4</sup>Â +Â <i>x</i><sup>5</sup>,
      but again, reducing the coefficients mod 2 makes the
      2<i>x</i><sup>3</sup> term vanish. So over <i>GF</i>(2), the product is <i>x</i>Â +Â <i>x</i><sup>2</sup>Â +Â <i>x</i><sup>4</sup>Â +Â <i>x</i><sup>5</sup>.</p>
    <p>Another way to describe this algorithm is: to calculate the
      product of two polynomials <i>P</i> and <i>Q</i>, for each
      term <i>x</i><sup><i>i</i></sup> in <i>P</i>, make the partial
      product <i>Q</i><i>x</i><sup><i>i</i></sup>, which looks just
      like <i>Q</i> itself except that itâ€™s â€˜shifted upwardsâ€™
      by <i>i</i> places â€“ the power of <i>x</i> in each term is
      larger by <i>i</i> than it originally was. Then add together all
      of those partial products, in â€˜mod 2â€™ fashion.</p>
    <p>This looks very similar to the algorithm you use for
      multiplying two ordinary integers <i>a</i> and <i>b</i> if
      youâ€™re given them written in binary. In that algorithm, you
      again make a shifted value <i>b</i>Â Ã—Â 2<sup><i>i</i></sup> for
      every power of 2 corresponding to a 1 bit in <i>a</i>. The only
      difference is that you combine the partial products at the end
      using ordinary integer addition, instead of bitwise XOR.</p>
    <p>In software that deals with polynomials over <i>GF</i>(2) a
      lot, itâ€™s actually very convenient to <em>represent</em> one of
      these polynomials as a binary number, with the bit that would
      normally have value 2<sup><i>i</i></sup> instead being taken to
      be the coefficient of <i>x</i><sup><i>i</i></sup>. For example,
      you might represent
      1Â +Â <i>x</i><sup>2</sup>Â +Â <i>x</i><sup>3</sup> using the
      integer 1Â +Â 2<sup>2</sup>Â +Â 2<sup>3</sup>Â =Â 13. (As if youâ€™d
      forgotten about the â€˜mod 2â€™ business and just evaluated the
      original polynomial at <i>x</i>Â =Â 2.)</p>
    <p>In that representation, multiplying polynomials looks almost
      exactly like multiplying integers in binary â€“ you make a shifted
      copy of one of the inputs <i>a</i> for each set bit of <i>b</i>,
      and then combine them all â€“ except that combining all the values
      at the end is done â€˜without carryingâ€™, i.e. itâ€™s replaced with
      XOR. Some CPU architectures even provide a built-in hardware
      instruction to do this modified type of multiplication. The x86
      architecture calls it a name like â€˜carryless multiplicationâ€™,
      with instruction names including the string <code>CLMUL</code>;
      the Arm architecture calls it â€˜polynomial multiplicationâ€™, and
      youâ€™ll probably find a P in the name of any instruction that
      does it.</p>
    <h5 id="crc">CRCs</h5>
    <p>These polynomials over <i>GF</i>(2) behave in some ways
      similarly to the integers: you can add, subtract and multiply
      them as much as you like, but when you try to divide two
      polynomials, the result often isnâ€™t still a polynomial, and you
      have to decide what to do about that.</p>
    <p>Just like dividing in the integers, one thing you can do is to
      deliver a quotient and a remainder: if youâ€™re asked to
      calculate <i>P</i>Â /Â <i>Q</i> and you find that <i>P</i> isnâ€™t a
      multiple of <i>Q</i>, you can find a nearby polynomial
      that <em>is</em> a multiple of <i>Q</i>, and return the result
      of dividing <em>that</em> by <i>Q</i>, plus the â€˜remainderâ€™
      thatâ€™s the difference between <i>P</i> itself and the polynomial
      you substituted.</p>
    <p>A well-known kind of checksum, used to verify transmission of
      network packets in Ethernet and for many other similar purposes,
      is called a Cyclic Redundancy Check (CRC), and it works like
      this:</p>
    <ul>
      <li>Choose a polynomial <i>P</i> over <i>GF</i>(2). (There are
        several polynomials people like to use, typically with degree
        32 or sometimes 16. But for a given application, both sender
        and receiver must agree on a specific one.)</li>
      <li>Given a message to transmit, expressed as a sequence of
        bits, pretend that the entire message is itself a giant
        polynomial <i>M</i> over <i>GF</i>(2), with the coefficients
        given by the bits of the message.</li>
      <li>Divide <i>M</i> by <i>P</i>, throw away the quotient, and
        keep the remainder. That is, reduce <i>M</i> mod <i>P</i>.</li>
    </ul>
    <p>You can see this, in some ways, as very similar to the
      error-correcting codes I mentioned in a <a href="#ecc">previous
      section</a>. A CRC will only <em>detect</em> errors, not correct
      them; and because itâ€™s a tiny number of bits appended to a huge
      message, its power to detect errors is much smaller than those
      more rigorous codes. If youâ€™re trying to transmit over a noisy
      radio channel then you probably use full error-correcting codes;
      CRCs are for the kind of situation where <em>almost</em> every
      transmission is free of error, and very rarely thereâ€™s either
      one flipped bit or a sudden burst of random noise, and itâ€™s fine
      to deal with the problem by telling the other end â€˜whoops, that
      one packet didnâ€™t arrive intact, please re-send itâ€™.</p>
    <h5 id="fields">Making larger finite fields</h5>
    <p>In a <a href="#gf2poly">previous section</a> I said that
      the <em>simplest</em> examples of finite fields consisted of the
      integers mod <i>p</i>, for some prime <i>p</i>. But theyâ€™re not
      the only examples.</p>
    <p>Let me describe the process of making one of these simplest
      finite fields <i>GF</i>(<i>p</i>) in slightly more detail:</p>
    <ul>
      <li>Start with the integers.</li>
      <li>Choose a specific value <i>p</i> which isnâ€™t divisible by
        anything smaller.</li>
      <li>Reduce everything else mod <i>p</i> â€“ that is, consider
        things as the same if they differ by a multiple of <i>p</i>.
        This leaves only finitely many things counted as
        different.</li>
      <li>Because <i>p</i> is prime, it turns out that there is now
        always a multiplicative inverse of any value not equivalent to
        0. So division now works, and weâ€™ve made a field.</li>
    </ul>
    <p>It turns out that all the <em>other</em> finite fields that
      exist can be made by following exactly the same procedure a
      second time â€“ except that instead of starting with the integers,
      you start with the set of polynomials
      over <i>GF</i>(<i>p</i>).</p>
    <p>That is: first pick <i>p</i> and make the finite field of
      integers mod <i>p</i>. Now pick a polynomial <i>Q</i> with
      coefficients in <i>GF</i>(<i>p</i>), which
      is <em>irreducible</em> â€“ that is, it isnâ€™t the product of any
      two smaller polynomials â€“ and reduce all the rest of the
      polynomials over <i>GF</i>(<i>p</i>) mod <i>Q</i>.</p>
    <p>The result is always a finite field. It still has
      characteristic <i>p</i> â€“ that is, adding together <i>p</i>
      copies of the same thing gives zero. But it has more
      than <i>p</i> elements. In fact, if the polynomial <i>Q</i> has
      degree <i>d</i> (that is, its highest-order term
      is <i>x</i><sup><i>d</i></sup>), then the new field has
      <i>p</i><sup><i>d</i></sup>
      elements<label for="footnoteinput-unique-finite-fields"><sup id="footnote-unique-finite-fields">12</sup><span>In fact, it turns
      out that <em>all</em> finite fields of a given
      size <i>p</i><sup><i>d</i></sup> are the same as each other.
      Choosing a different irreducible polynomial of the same degree
      changes the <em>representation</em>, but not the underlying
      thing being represented â€“ as if you still had all the same
      numbers, but you just changed the name of each
      one.</span><span>12</span></label>.</p>
    <p>You can do this for any prime <i>p</i>. But in this article
      weâ€™re concerned with <i>p</i>Â =Â 2 in particular. Here are the
      first few irreducible polynomials over <i>GF</i>(2):</p>
    <figure>
      <div>
        <p>
<i>x</i></p>
      </div>
      <figcaption>Irreducible polynomials over <i>GF</i>(2) up to degree 4</figcaption>
    </figure>
    <p>A more compact way to write the same information is to use the
      notation I mentioned earlier, of representing each one as an
      integer whose pattern of bits gives its coefficients, equivalent
      to evaluating the polynomial at <i>x</i>Â =Â 2. Then the start of
      the sequence of irreducible polynomials looks like this:</p>
    <p>2, 3, 7, 11, 13, 19, 25, 31, 37, 41, 47, 55, 59, 61, 67, 73, 87, 91, 97, 103, 109, 115, 117, 131, 137, 143, 145, 157, 167, 171, 185, 191, 193, 203, 211, 213, 229, 239, 241, 247, 253, 283, 285, 299, 301, 313, 319, 333, 351, 355, 357, 361, 369, 375, 379, 391, 395, 397, 415, 419, 425, 433, 445, 451, 463, 471, 477, 487, 499, 501, 505, â€¦</p>
    <p>Read like a sequence of integers<label for="footnoteinput-oeis"><sup id="footnote-oeis">13</sup><span>Any
      mathematically interesting sequence of integers has a very good
      chance of being catalogued in
      the <a href="https://oeis.org/">On-Line Encyclopedia of Integer
      Sequences</a>, and this is certainly not an exception. Its
      catalogue index
      is <a href="https://oeis.org/A014580">A014580</a>.</span><span>13</span></label>,
      these numbers fascinate me. Theyâ€™re like prime numbers from a
      parallel universe. If addition were done without carrying, these
      would <em>be</em> the prime numbers. And, just as youâ€™d hope for
      something from a parallel universe, it takes you an extra look
      to spot the difference, because they look very similar to start
      with â€“ many of the initial ones are <em>also</em> ordinary
      integer primes. (Though not all â€“ 25 and 55 are the first
      composite ones; and not all the integer primes appear â€“ 5, 17,
      and 23 are the first ones missing.)</p>
    <p>Thereâ€™s a lot more to say about these larger finite fields. But
      Iâ€™ll finish up by mentioning what theyâ€™re <em>useful</em> for.
      They crop up all over cryptography:</p>
    <ul>
      <li>A finite field of order 2<sup>8</sup> forms a key building
        block of the standard AES cipher, and also of another cipher
        (Twofish) which <em>nearly</em> became AES (it was another
        finalist in the competition to choose a standard block
        cipher).</li>
      <li>A finite field of the much larger size 2<sup>128</sup> is
        used in GCM, which is a fast scheme that combines bulk
        encryption with integrity protection (known as â€˜AEADâ€™).</li>
      <li>Finite fields of large power-of-2 size are also sometimes
        (though not always) used in elliptic-curve cryptography, for
        both key exchange and digital signatures.</li>
      <li>Elliptic-curve cryptography is probably going to be
        abandoned at some point, because of the possibility of working
        quantum computers being built that can attack it. Finite
        fields of power-of-2 order <em>also</em> appear in at least
        one of the replacement â€˜post-quantumâ€™ schemes, as part of the
        decoding algorithm in the â€œClassic McElieceâ€ key encapsulation
        system.</li>
    </ul>
    <p>These things are big business â€“ and thatâ€™s mostly why CPU
      architectures bother to implement that polynomial multiplication
      primitive at all!</p>
    
    <p>With any luck, you should be able to read the footnotes of this
      article in place, by clicking on the superscript footnote number
      or the corresponding numbered tab on the right side of the page.</p>
    <p>But just in case the CSS didnâ€™t do the right thing, hereâ€™s the
      text of all the footnotes again:</p>
<p><a href="#footnote-inverse">1.</a> I apologise for
      using â€˜invertâ€™ and â€˜inverseâ€™ in two senses in this article. In
      boolean logic, â€˜invertingâ€™ a signal generally means the NOT
      operator, interchanging 1 with 0 (or true with false, if you
      prefer). But in mathematics, an â€˜inverseâ€™ is a value that
      cancels out another value to get you back to the identity, in a
      way which varies depending on the operation you use to combine
      them (if youâ€™re talking about addition itâ€™s âˆ’<i>x</i>, and for
      multiplication itâ€™s 1/<i>x</i>). I hope that everywhere Iâ€™ve
      used the word at all itâ€™s clear from context which sense I mean
      it in.</p>
<p><a href="#footnote-twos-complement">2.</a> What if the integers are
      negative? Normally, in the finite integer sizes that computers
      handle in hardware, negative integers are represented in twoâ€™s
      complement, i.e. mod 2<sup><i>n</i></sup>. So âˆ’1 is the integer
      with all bits 1, for example. Thereâ€™s a reasonably natural way
      to extend this to <em>arbitrarily large</em> integers, by
      pretending the string of 1 bits on the left goes all the way to
      infinity (and it can even be made mathematically rigorous!). In
      this view, the XOR of two positive numbers, or two negative
      numbers, is positive, because at a high enough bit position each
      bit is XORing two 0s or two 1s; but the XOR of a positive and
      negative number is negative, because sooner or later youâ€™re
      always XORing a 0 with a 1 bit. Some languages, such as Python,
      actually implement this â€“ you can try it out at the interactive
      prompt!</p>
<p><a href="#footnote-emoji">3.</a> Of course, this
      doesnâ€™t apply to <em>all</em> Unicode characters! Most donâ€™t
      have a concept of upper or lower case at all. And unfortunately,
      this rule isnâ€™t even obeyed by all of the characters that do. It
      was consistently true in ASCII, and in some of the descendants
      of ASCII, but Unicode as a whole wasnâ€™t able to stick 100% to
      the principle. If you take this too far, you might get strange
      ideas, like the lower-case version of the car emoji being a â€˜no
      pedestriansâ€™ sign:
<span>&gt;&gt;&gt; chr(ord(&#39;ğŸš—&#39;) ^ 32)</span></p>
<p><a href="#footnote-well-actually">4.</a> I
      suppose, <em>technically</em>, you could argue that it is still
      literally true that <i>a</i>Â XORÂ <i>b</i>, <i>a</i>Â +Â <i>b</i>,
      and <i>a</i>Â âˆ’Â <i>b</i> are all congruent to each other mod 2,
      because all that means is that they all have the same low-order
      bit, which they do. But that isnâ€™t a
      particularly <em>useful</em> thing to say, because it ignores
      the way all the higher-order bits do something completely
      different!</p>
<p><a href="#footnote-integrity">5.</a> However,
      encrypting the message is only half the story. Given a
      good-quality keystream, this technique is good enough to
      assure <em>confidentiality</em>, meaning that an eavesdropper
      canâ€™t find out what youâ€™re saying. But it doesnâ€™t do one single
      thing to ensure <em>integrity</em>, meaning that if your message
      is modified by an attacker, the receiver can detect the
      tampering and know not to trust the modified message. Integrity
      protection is a completely separate problem. A common mistake in
      novice cryptosystem design is to leave it out, assuming that if
      nobody can figure out what the message says, then â€œsurelyâ€
      nobody can work out how to tamper with it in a useful way
      either. Even with more complicated encryption methods than what
      Iâ€™m describing here, this never goes well!</p>
<p><a href="#footnote-proof-exercise">6.</a> If youâ€™re
      mathematically minded and still donâ€™t find this instantly
      obvious, you might want to try actually <em>proving</em> it. I
      leave this mostly as an exercise for the reader, but a hint
      would be: break down each of the input integers <i>a</i>
      and <i>b</i> as the sum of terms like
      2<sup><i>n</i></sup><i>a</i><sub><i>n</i></sub> (each one being
      an individual bit of one of the inputs times a power of 2), and
      collect together the terms on each side that involve the
      same <i>n</i>.</p>
<p><a href="#footnote-or-xor">7.</a> The extra gate that combines the carry
      bits can be either an OR gate or an XOR gate, whichever is
      easier. It doesnâ€™t matter which, because the two types of gate
      only disagree in the case where both their inputs are 1, and in
      this circuit, that canâ€™t happen! If the carry from the first
      half-adder is 1, then its other output <em>must</em> be 0, which
      means thereâ€™s no carry from the second half-adder.</p>
<p><a href="#footnote-swp">8.</a> There is an Arm instruction called
      SWP for â€˜swapâ€™, but itâ€™s not what you want for this kind of
      purpose. It swaps a register with a value in <em>memory</em>,
      not two registers. Itâ€™s not really for ordinary computation:
      itâ€™s intended for atomically synchronising between threads,
      which is very hard to get right and well beyond the scope of
      this article!</p>
<p><a href="#footnote-misere">9.</a> In fact, Nim comes in two
      well-known forms, with opposite win conditions. In the variant
      discussed here, your aim is to take the last counter and leave
      your opponent with no move. In the other variant, the win
      condition is exactly the reverse: you aim to make
      your <em>opponent</em> take the last counter! The strategy for
      the latter is a tiny bit more complicated, but not too bad. I
      only discuss the simple version here.</p>
<p><a href="#footnote-infinite-positive-characteristic">10.</a> However, the
      converse isnâ€™t true. Any finite field has positive
      characteristic, but not every field with positive characteristic
      is finite. There are also <em>infinite</em> fields with this
      same modular-arithmetic property. But we wonâ€™t get as far as
      discussing those here.</p>
<p><a href="#footnote-formal-polynomials">11.</a> You might argue:
      if <i>x</i> is a number in <i>GF</i>(2), then itâ€™s either 0 or
      1, and in either case, <i>x</i><sup>2</sup>,
      or <i>x</i><sup>3</sup>, or any higher power of <i>x</i>, are
      all equal to <i>x</i> itself. So under that assumption, any
      polynomial of this kind could be simplified by squashing all the
      higher-order terms down into the <i>x</i><sup>1</sup> term. But
      we <em>donâ€™t</em> do that here, because we donâ€™t make the
      assumption that <i>x</i> is one of the two numbers we know
      about: we leave open the possibility that any two
      powers <i>x</i><sup><i>i</i></sup>
      and <i>x</i><sup><i>j</i></sup> might be
      unequal.</p>
<p><a href="#footnote-unique-finite-fields">12.</a> In fact, it turns
      out that <em>all</em> finite fields of a given
      size <i>p</i><sup><i>d</i></sup> are the same as each other.
      Choosing a different irreducible polynomial of the same degree
      changes the <em>representation</em>, but not the underlying
      thing being represented â€“ as if you still had all the same
      numbers, but you just changed the name of each
      one.</p>
<p><a href="#footnote-oeis">13.</a> Any
      mathematically interesting sequence of integers has a very good
      chance of being catalogued in
      the <a href="https://oeis.org/">On-Line Encyclopedia of Integer
      Sequences</a>, and this is certainly not an exception. Its
      catalogue index
      is <a href="https://oeis.org/A014580">A014580</a>.</p>
  

</div>
  </body>
</html>
