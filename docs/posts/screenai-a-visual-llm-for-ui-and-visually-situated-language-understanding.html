<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/">Original</a>
    <h1>ScreenAI: A visual LLM for UI and visually-situated language understanding</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                    <section>
                        
                        



<p>We introduce ScreenAI, a vision-language model for user interfaces and infographics that achieves state-of-the-art results on UI and infographics-based tasks. We are also releasing three new datasets: Screen Annotation to evaluate the layout understanding capability of the model, as well as ScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its QA capability.</p>
                        
                    </section>
                
                
                
                


<section>
    
    <div>
        <p data-block-key="df7wk">Screen user interfaces (UIs) and infographics, such as charts, diagrams and tables, play important roles in human communication and human-machine interaction as they facilitate rich and interactive user experiences. UIs and infographics share similar design principles and visual language (e.g., icons and layouts), that offer an opportunity to build a single model that can understand, reason, and interact with these interfaces. However, because of their complexity and varied presentation formats, infographics and UIs present a unique modeling challenge.</p><p data-block-key="9f7at">To that end, we introduce “<a href="https://arxiv.org/abs/2402.04615" target="_blank" rel="noopener noreferrer">ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a>”. ScreenAI improves upon the <a href="https://arxiv.org/abs/2305.18565" target="_blank" rel="noopener noreferrer">PaLI architecture</a> with the flexible patching strategy from <a href="https://arxiv.org/abs/2210.03347" target="_blank" rel="noopener noreferrer">pix2struct</a>. We train ScreenAI on a unique mixture of datasets and tasks, including a novel Screen Annotation task that requires the model to identify UI element information (i.e., type, location and description) on a screen. These text annotations provide large language models (LLMs) with screen descriptions, enabling them to automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art results on UI- and infographic-based tasks (<a href="https://x-lance.github.io/WebSRC/" target="_blank" rel="noopener noreferrer">WebSRC</a> and <a href="https://github.com/aburns4/MoTIF" target="_blank" rel="noopener noreferrer">MoTIF</a>), and best-in-class performance on <a href="https://github.com/vis-nlp/ChartQA" target="_blank" rel="noopener noreferrer">Chart QA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1" target="_blank" rel="noopener noreferrer">DocVQA</a>, and <a href="https://arxiv.org/abs/2104.12756" target="_blank" rel="noopener noreferrer">InfographicVQA</a> compared to models of similar size. We are also releasing three new datasets: <a href="https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#screen-annotation-dataset-details" target="_blank" rel="noopener noreferrer">Screen Annotation</a> to evaluate the layout understanding capability of the model, as well as <a href="https://github.com/google-research-datasets/screen_qa/tree/main?tab=readme-ov-file#short_answers-directory" target="_blank" rel="noopener noreferrer">ScreenQA Short</a> and <a href="https://github.com/google-research-datasets/screen_qa?tab=readme-ov-file#complexqa" target="_blank" rel="noopener noreferrer">Complex ScreenQA</a> for a more comprehensive evaluation of its QA capability.</p>
    </div>


    
</section>

                
                


<section>
    
    <p>
        <h2 data-block-key="df7wk">ScreenAI</h2>
    </p>


    
</section>

                
                


<section>
    
    <div>
        <p data-block-key="df7wk">ScreenAI’s architecture is based on <a href="https://arxiv.org/abs/2209.06794" target="_blank" rel="noopener noreferrer">PaLI</a>, composed of a multimodal encoder block and an autoregressive decoder. The PaLI encoder uses a <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer">vision transformer</a> (ViT) that creates image embeddings and a multimodal encoder that takes the concatenation of the image and text embeddings as input. This flexible architecture allows ScreenAI to solve vision tasks that can be recast as text+image-to-text problems.</p><p data-block-key="fnskt">On top of the PaLI architecture, we employ a flexible patching strategy introduced in pix2struct. Instead of using a fixed-grid pattern, the grid dimensions are selected such that they preserve the native aspect ratio of the input image. This enables ScreenAI to work well across images of various aspect ratios.</p><p data-block-key="bnr6j">The ScreenAI model is trained in two stages: a pre-training stage followed by a fine-tuning stage. First, self-supervised learning is applied to automatically generate data labels, which are then used to train ViT and the language model. ViT is frozen during the fine-tuning stage, where most data used is manually labeled by human raters.</p>
    </div>


    
</section>

                
                
    

<!-- mode: '' -->


  
    <div>
      <video playsinline="" muted="true" loop="true" preload="auto">
        <source src="https://storage.googleapis.com/gweb-research2023-media/media/ScreenAI-model.mp4" type="video/mp4"/>
      </video>
      <div aria-label="Video Play/pause">
        <p><span>play silent looping video</span>
          <span>pause silent looping video</span>
        </p>
        
      </div>
      
      <p data-block-key="f4d4n">ScreenAI model architecture.</p>
      
    </div>
  



                
                


<section>
    
    <p>
        <h2 data-block-key="xjbkq">Data generation</h2>
    </p>


    
</section>

                
                


<section>
    
    <p data-block-key="xjbkq">To create a pre-training dataset for ScreenAI, we first compile an extensive collection of screenshots from various devices, including desktops, mobile, and tablets. This is achieved by using <a href="https://arxiv.org/abs/1910.10683" target="_blank" rel="noopener noreferrer">publicly accessible web pages</a> and following the programmatic exploration approach used for the <a href="https://dl.acm.org/doi/10.1145/3126594.3126651" target="_blank" rel="noopener noreferrer">RICO dataset</a> for mobile apps. We then apply a layout annotator, based on the <a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener noreferrer">DETR</a> model, that identifies and labels a wide range of UI elements (e.g., image, pictogram, button, text) and their spatial relationships. Pictograms undergo further analysis using an <a href="https://arxiv.org/abs/2210.02663" target="_blank" rel="noopener noreferrer">icon classifier</a> capable of distinguishing 77 different icon types. This detailed classification is essential for interpreting the subtle information conveyed through icons. For icons that are not covered by the classifier, and for infographics and images, we use the PaLI image captioning model to generate descriptive captions that provide contextual information. We also apply an <a href="https://cloud.google.com/use-cases/ocr" target="_blank" rel="noopener noreferrer">optical character recognition</a> (OCR) engine to extract and annotate textual content on screen. We combine the OCR text with the previous annotations to create a detailed description of each screen.</p>


    
</section>

                
                
    

<!-- mode: '' -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-2.width-800.png" alt="ScreenAI-2"/>
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-2.width-800.png" alt="ScreenAI-2" loading="lazy"/>
      
        <p data-block-key="f4d4n">A mobile app screenshot with generated annotations that include UI elements and their descriptions, e.g., TEXT elements also contain the text content from OCR, IMAGE elements contain image captions, LIST_ITEMs contain all their child elements.</p>
      
    </picture>
  



                
                


<section>
    
    <p>
        <h3 data-block-key="xjbkq">LLM-based data generation</h3>
    </p>


    
</section>

                
                


<section>
    
    <p data-block-key="xjbkq">We enhance the pre-training data&#39;s diversity using <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/" target="_blank" rel="noopener noreferrer">PaLM 2</a> to generate input-output pairs in a two-step process. First, screen annotations are generated using the technique outlined above, then we craft a prompt around this schema for the LLM to create synthetic data. This process requires prompt engineering and iterative refinement to find an effective prompt. We assess the generated data&#39;s quality through human validation against a quality threshold.</p>


    
</section>

                
                <div>
  <p><code data-block-key="5c1oc">You only speak JSON. Do not write text that isn’t JSON.</code></p>
  <p data-block-key="v2dgk">A sample prompt for QA data generation.</p>
</div>
                
                


<section>
    
    <div>
        <p data-block-key="xjbkq">By combining the natural language capabilities of LLMs with a structured schema, we simulate a wide range of user interactions and scenarios to generate synthetic, realistic tasks. In particular, we generate three categories of tasks:</p><ul><li data-block-key="dvi3e"><b>Question answering</b>: The model is asked to answer questions regarding the content of the screenshots, e.g., “When does the restaurant open?”</li><li data-block-key="7isdo"><b>Screen navigation</b>: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., “Click the search button.”</li><li data-block-key="a56rn"><b>Screen summarization</b>: The model is asked to summarize the screen content in one or two sentences.</li></ul>
    </div>


    
</section>

                
                
    

<!-- mode: '' -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-3.width-800.png" alt="ScreenAI-3"/>
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-3.width-800.png" alt="ScreenAI-3" loading="lazy"/>
      
        <p data-block-key="f4d4n">Block diagram of our workflow for generating data for QA, summarization and navigation tasks using existing ScreenAI models and LLMs. Each task uses a custom prompt to emphasize desired aspects, like questions related to counting, involving reasoning, etc.</p>
      
    </picture>
  



                
                
    

<!-- mode: '' -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-1.width-800.png" alt="ScreenAI-1"/>
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-1.width-800.png" alt="ScreenAI-1" loading="lazy"/>
      
        <p data-block-key="f4d4n">LLM-generated data. Examples for screen QA, navigation and summarization. For navigation, the action bounding box is displayed in red on the screenshot.</p>
      
    </picture>
  



                
                


<section>
    
    <p>
        <h2 data-block-key="xjbkq">Experiments and results</h2>
    </p>


    
</section>

                
                


<section>
    
    <div>
        <p data-block-key="xjbkq">As previously mentioned, ScreenAI is trained in two stages: pre-training and fine-tuning. Pre-training data labels are obtained using self-supervised learning and fine-tuning data labels comes from human raters.</p><p data-block-key="ao9ia">We fine-tune ScreenAI using public QA, summarization, and navigation datasets and a variety of tasks related to UIs. For QA, we use well established benchmarks in the multimodal and document understanding field, such as <a href="https://github.com/vis-nlp/ChartQA" target="_blank" rel="noopener noreferrer">ChartQA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1" target="_blank" rel="noopener noreferrer">DocVQA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=tasks" target="_blank" rel="noopener noreferrer">Multi page DocVQA</a>, <a href="https://arxiv.org/abs/2104.12756" target="_blank" rel="noopener noreferrer">InfographicVQA</a>, <a href="https://ocr-vqa.github.io/" target="_blank" rel="noopener noreferrer">OCR VQA</a>, <a href="https://x-lance.github.io/WebSRC/" target="_blank" rel="noopener noreferrer">Web SRC</a> and <a href="https://github.com/google-research-datasets/screen_qa" target="_blank" rel="noopener noreferrer">ScreenQA</a>. For navigation, datasets used include <a href="https://github.com/google-research-datasets/uibert/tree/main" target="_blank" rel="noopener noreferrer">Referring Expressions</a>, <a href="https://github.com/aburns4/MoTIF" target="_blank" rel="noopener noreferrer">MoTIF</a>, <a href="https://arxiv.org/abs/2209.15099" target="_blank" rel="noopener noreferrer">Mug</a>, and <a href="https://github.com/google-research/google-research/tree/master/android_in_the_wild" target="_blank" rel="noopener noreferrer">Android in the Wild</a>. Finally, we use <a href="https://github.com/google-research-datasets/screen2words" target="_blank" rel="noopener noreferrer">Screen2Words</a> for screen summarization. Along with the fine-tuning datasets, we evaluate the fine-tuned ScreenAI model using three novel benchmarks:</p><ol><li data-block-key="3s465">Screen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities.</li><li data-block-key="d6g8q">ScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks.</li><li data-block-key="1oj5v">Complex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios.</li></ol><p data-block-key="3m7r7">The fine-tuned ScreenAI model achieves state-of-the-art results on various UI and infographic-based tasks (<a href="https://x-lance.github.io/WebSRC/" target="_blank" rel="noopener noreferrer">WebSRC</a> and <a href="https://github.com/aburns4/MoTIF" target="_blank" rel="noopener noreferrer">MoTIF</a>) and best-in-class performance on <a href="https://github.com/vis-nlp/ChartQA" target="_blank" rel="noopener noreferrer">Chart QA</a>, <a href="https://rrc.cvc.uab.es/?ch=17&amp;com=evaluation&amp;task=1" target="_blank" rel="noopener noreferrer">DocVQA</a>, and <a href="https://arxiv.org/abs/2104.12756" target="_blank" rel="noopener noreferrer">InfographicVQA</a> compared to models of similar size. ScreenAI achieves competitive performance on Screen2Words and OCR-VQA. Additionally, we report results on the new benchmark datasets introduced to serve as a baseline for further research.</p>
    </div>


    
</section>

                
                
    

<!-- mode: '' -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-6.width-800.png" alt="ScreenAI-6"/>
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-6.width-800.png" alt="ScreenAI-6" loading="lazy"/>
      
        <p data-block-key="f4d4n">Comparing model performance of ScreenAI with state-of-the-art (SOTA) models of similar size.</p>
      
    </picture>
  



                
                


<section>
    
    <p data-block-key="xjbkq">Next, we examine ScreenAI’s scaling capabilities and observe that across all tasks, increasing the model size improves performances and the improvements have not saturated at the largest size.</p>


    
</section>

                
                
    

<!-- mode: '' -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-5.width-800.png" alt="ScreenAI-5"/>
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/ScreenAI-5.width-800.png" alt="ScreenAI-5" loading="lazy"/>
      
        <p data-block-key="f4d4n">Model performance increases with size, and the performance has not saturated even at the largest size of 5B params.</p>
      
    </picture>
  



                
                


<section>
    
    <p>
        <h2 data-block-key="xjbkq">Conclusion</h2>
    </p>


    
</section>

                
                


<section>
    
    <p data-block-key="xjbkq">We introduce the ScreenAI model along with a unified representation that enables us to develop self-supervised learning tasks leveraging data from all these domains. We also illustrate the impact of data generation using LLMs and investigate improving model performance on specific aspects with modifying the training mixture. We apply all of these techniques to build multi-task trained models that perform competitively with state-of-the-art approaches on a number of public benchmarks. However, we also note that our approach still lags behind large models and further research is needed to bridge this gap.</p>


    
</section>

                
                


<section>
    
    <p>
        <h2 data-block-key="xjbkq">Acknowledgements</h2>
    </p>


    
</section>

                
                


<section>
    
    <p data-block-key="xjbkq"><i>This project is the result of joint work with Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu Sharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel Barcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania Bedrax-Weiss for their insightful feedback and discussions, along with Rahul Aralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We also thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou, and Matt Sharifi for their leadership, vision and support. We are very grateful toTom Small for helping us create the animation in this post.</i></p>


    
</section>

                

                


<section aria-label="List of footnotes">
  
</section>

                
<section>
    
</section>

            </div></div>
  </body>
</html>
