<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/GeeeekExplorer/nano-vllm">Original</a>
    <h1>Nano-Vllm: lightweight vLLM implementation built from scratch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">A lightweight vLLM implementation built from scratch.</p>

<ul dir="auto">
<li>ðŸš€ <strong>Fast offline inference</strong> - Comparable inference speeds to vLLM</li>
<li>ðŸ“– <strong>Readable codebase</strong> - Clean implementation in ~ 1,200 lines of Python code</li>
<li>âš¡ <strong>Optimization Suite</strong> - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/GeeeekExplorer/nano-vllm.git"><pre>pip install git+https://github.com/GeeeekExplorer/nano-vllm.git</pre></div>

<p dir="auto">See <code>example.py</code> for usage. The API mirrors vLLM&#39;s interface with minor differences in the <code>LLM.generate</code> method.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from nanovllm import LLM, SamplingParams
llm = LLM(&#34;/YOUR/MODEL/PATH&#34;, enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = [&#34;Hello, Nano-vLLM.&#34;]
outputs = llm.generate(prompts, sampling_params)
outputs[0][&#34;text&#34;]"><pre><span>from</span> <span>nanovllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>
<span>llm</span> <span>=</span> <span>LLM</span>(<span>&#34;/YOUR/MODEL/PATH&#34;</span>, <span>enforce_eager</span><span>=</span><span>True</span>, <span>tensor_parallel_size</span><span>=</span><span>1</span>)
<span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(<span>temperature</span><span>=</span><span>0.6</span>, <span>max_tokens</span><span>=</span><span>256</span>)
<span>prompts</span> <span>=</span> [<span>&#34;Hello, Nano-vLLM.&#34;</span>]
<span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>(<span>prompts</span>, <span>sampling_params</span>)
<span>outputs</span>[<span>0</span>][<span>&#34;text&#34;</span>]</pre></div>

<p dir="auto">See <code>bench.py</code> for benchmark.</p>
<p dir="auto"><strong>Test Configuration:</strong></p>
<ul dir="auto">
<li>Hardware: RTX 4070 Laptop (8GB)</li>
<li>Model: Qwen3-0.6B</li>
<li>Total Requests: 256 sequences</li>
<li>Input Length: Randomly sampled between 100â€“1024 tokens</li>
<li>Output Length: Randomly sampled between 100â€“1024 tokens</li>
</ul>
<p dir="auto"><strong>Performance Results:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Inference Engine</th>
<th>Output Tokens</th>
<th>Time (s)</th>
<th>Throughput (tokens/s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>vLLM</td>
<td>133,966</td>
<td>98.37</td>
<td>1361.84</td>
</tr>
<tr>
<td>Nano-vLLM</td>
<td>133,966</td>
<td>93.41</td>
<td>1434.13</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><a href="https://www.star-history.com/#GeeeekExplorer/nano-vllm&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/9d77943eb142ab5773e78a27135fb2d3df2e3b3718b57943b7abd006ae6034a8/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d47656565656b4578706c6f7265722f6e616e6f2d766c6c6d26747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&amp;type=Date"/></a></p>
</article></div></div>
  </body>
</html>
