<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dublog.net/blog/commoditize-complement/">Original</a>
    <h1>Big tech wants to make AI cost nothing</h1>
    
    <div id="readability-page-1" class="page"><div><p>Earlier this week, Meta both open sourced and released the model weights for Llama 3.1, an extraordinarily powerful large language model (LLM) which is competitive with the best of what Open AI’s ChatGPT and Anthropic’s Claude can offer.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p><img src="https://solar.lowtechmagazine.com/images/commoditized/llama3_1perf.png" alt="Llama 3.1 Performance"/></p><p>The terms of release (as with previous Llama releases) are incredibly generous. Any organization with fewer than 700 million monthly active users can do with Llama 3.1 as they wish meaning that virtually all companies not named Google, Apple, Microsoft, or Netflix can start baking Llama directly into their products.</p><p>But why would Meta release this model for free? Is it out of pure altruism? Perhaps a ploy to improve the optics for a company that for the past several years has borne the brunt of bipartisan political anger over privacy concerns?</p><p>The apparent magnanimity of this release reminded me of a very classic business strategy in Silicon Valley - “commoditize your complement”. Best articulated by Joel Spolsky’s <a href="https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/">“Strategy Letter V”</a> from 22 years ago, the idea is that as the value of a product’s complement goes down towards the lowest sustainable “commodity price”, demand for that product in turn goes higher.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>For example, in the 90s, Microsoft made money by selling Windows operating systems, and so by making PCs as cheap as possible (by pushing for standardization/modularity of all PC components), Microsoft simultaneously decreased the cost of PC hardware components while increasing demand for its software product. Likewise, when Google released its Android operating system to smartphone vendors, it wasn’t necessarily acting in the interests of those manufacturers. In fact, smartphones - quite possibly the most technologically advanced hardware of all time - became commoditized to the point where just about anyone in the third world could buy one for under $20, reducing profit margins for manufacturers to razor thin levels. The expansion of smartphones however, expanded the market for Google’s search product and ad sales far beyond the size of the desktop computer market.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></p><h2 id="llms-are-being-commoditized">LLMs are being commoditized</h2><p>Right now, I believe we are at a similar crossroads with general purpose Large Language Models (LLMs). According to a recent Sequoia article, the cost to recoup recent AI spending on NVIDIA GPU based data center spending <em>alone</em> is upwards of $600 billion dollars.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> To recoup those enormous costs, businesses need to make well above that number to justify the spending, yet OpenAI’s subscription revenue is reportedly just around $3.4 billion<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>, with other players likely falling well behind. With Meta releasing an essentially free LLM that is both open-source and open weight, freely accessible to all at <a href="https://www.meta.ai/">meta.ai</a>, we would expect that value to go <em>down</em> and not <em>up</em> over the next several months.</p><p>Yet there are even bigger models on the horizon. According to Jensen Huang’s March 2024 GTC Keynote, it <em>only</em> takes roughly 8000 H100 GPUs 90 days to train a 1.8 Trillion Parameter Mixture-of-Experts GPT-4 scale model.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> According to Meta’s Llama whitepaper<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>, Llama 3.1 405 B model was pre-trained using 16,000 H100 GPUs in 54 days.</p><blockquote><p><img src="https://solar.lowtechmagazine.com/images/commoditized/gpufail.png" alt="“Failure Rates over 54 days of pre-training”"/></p></blockquote><p>Despite the enormous technical challenges of training such a large model, according to Meta’s engineering blog, by the end of 2024, Meta will have the equivalent of 600,000 H100s on tap!<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. If all of these were somehow dedicated to pre-training LLMs (rather than say inference or building recommender systems for Instagram Reels), that’s the equivalent of having the ability to spit out 75 GPT-4 scale models every 90 days or about 300 such models every year!</p><p>This means that (assuming scaling laws persist, and more tokens can be sourced) the next generation of multi-modal hyperscale transformer models which are currently being trained will dwarf what came before… and it’s quite possible newcomers like OpenAI and Anthropic might not even be able to contend with whatever larger companies like Meta release next. Even nation-states like China can cower at the might of 600,000 H100s!</p><blockquote><p><img src="https://solar.lowtechmagazine.com/images/commoditized/nextgenblackwell.png" alt="Meta has not disclosed how many next-gen Blackwell GPUs it intends to purchase"/></p></blockquote><p>And Meta’s not the only big tech company open sourcing LLM models. NVIDIA has released Nemotron-3 (340 B), Microsoft has released Phi and Florence models, Google has released Gemma, and even smaller companies like Cohere and Mistral have released their model weights.</p><h2 id="what-is-the-complement-to-llms">What is the complement to LLMs?</h2><p>So if multiple players are giving away LLMs for free, what is the natural complement to an LLM? For companies like Google, Microsoft, NVIDIA, and Amazon, the answer is simple - servers. Bigger models require more GPUs (or TPUs) to run, so if you rent out server space or sell GPUs, giving away “AI” for free is good business (safety concerns be damned!).</p><p>What’s interesting about the recent Llama 3.1 release is that <em>Meta doesn’t rent out its servers</em>. In fact just about every major cloud provider - AWS, Google Cloud, Azure - stand to benefit from the Llama 3.1 release monetarily in a bigger way than Meta since they can immediately start renting out their data centers to smaller companies running this larger Llama model and fine-tuned derivatives for inference.</p><p>Zuckerberg provides some possible answers to the paradox of <em>Meta</em> being the company to open source the biggest LLM. One is standardization.<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup> Meta has a long legacy of open sourcing (and commoditizing) internal tooling (such as Presto and React) which subsequently became standardized in the marketplace.</p><p>There are some other more compelling reasons for open sourcing tools like Llama, however, which Zuckerberg gave in a talk with Bloomberg.<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> That reason is user-content generation. By giving users the ability to create AI generated content and the means to independently fine-tune pre-trained models (which would otherwise be prohibitively expensive to train), the amount of <em>unique</em> user-generated content may go up, along with engagement with Meta’s platforms. This might be the end-goal for a company like Meta, which makes most of its money selling ads off its user network.</p><p>Another possible reason is that there simply is no real value in having a second-place general purpose LLM, particularly for a company like Meta which users may not trust enough to rely on for subscription API-based access. This is quite possibly the same conclusion reached by companies like Mistral, Cohere, Google, and others. In fact, at the very moment I’m writing this, <a href="https://mistral.ai/news/mistral-large-2407/">Mistral just released its Mistral Large 2 model under a research license</a>.</p><h2 id="what-will-happen-to-the-ai-startups">What will happen to the AI Startups?</h2><p>The big losers in the commoditization of LLMs may ultimately be the current hot and distruptive AI startups - companies like OpenAI, Anthropic, Character.ai, Cohere, and Mistral. When the 5 largest companies on the SP500 start giving away your main product for free, a reckoning may be on its way.</p><p>The CEOs of the largest tech companies need not fear scale – they only need fear being out-innovated towards irrelevance.</p><p>There is still the question of whether the current path of scaling ever larger multimodal transformer models will ultimately lead to artificial general intelligence (AGI) or even artificial superintelligence (ASI). If these smaller companies have some sort of modeling or R&amp;D edge that doesn’t simply involve having a massive number of GPUs, then perhaps there is still a chance they can outflank the megacorps. After all, OpenAI started doing fundamental R&amp;D - DOTA2 bots, robotics, and research into reinforcement learning. The original GPT model was a mere side-project. Perhaps these LLMs may even be a distraction from the fundamental research that will lead to more capable models and avenues of research.</p><p>Regardless, the sheer scale of the current infrastructure build-out gives me hope. The end of the dotcom bubble in 2001 was preceded by a massive infrastructure build out as well. The laying of fiberoptic cable and broadband infrastructure paved the way for Web 2.0 companies like Facebook and Google, even after a massive stock market collapse. And just like how that infrastructure build-out enabled things like cloud computing and streaming video, the current AI infrastructure buildout may also enable breakthroughs in other areas such as robotics, autonomous vehicles, and drug development.</p><blockquote><p><img src="https://solar.lowtechmagazine.com/images/commoditized/next_step_robots.jpg" alt="Next step is terminators for sure"/></p></blockquote></div></div>
  </body>
</html>
