<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.min.io/nvme_benchmark/">Original</a>
    <h1>Updated MinIO NVMe Benchmarks: 2.6Tpbs on Get and 1.6 on Put</h1>
    
    <div id="readability-page-1" class="page"><section>
                        <p>MinIO is a strong believer in transparency and data driven discussions. It is why <a href="https://blog.min.io/tag/benchmarks/">we publish our benchmarks</a> and challenge the rest of the industry to do so as well. <br/></p><p>It also is why we develop tools that allow a clean, clear measurement of performance and can be easily replicated. We want people to test for themselves. <br/></p><p>Further, we do our benchmarks on commodity hardware without tuning. This is fundamentally different from the highly tuned, specialized hardware approaches used by other vendors which, predictability, have given benchmarks a bad name. <br/></p><p>We challenge the rest of the industry to follow suit.</p><p>We recently updated our benchmark for primary storage. For our customers, primary storage utilizes NVMe drives due to their price/performance characteristics. We will update our HDD benchmark shortly for those customers looking to understand HDD price/performance.</p><p>In this post we will cover the benchmarking environment, the tools, how to replicate this on your own and the detailed results. For those looking for a quick take, the 32 node MinIO cluster results can be summarized as follows:<br/></p><!--kg-card-begin: html--><table><colgroup><col/><col/><col/><col/><col/><col/></colgroup><tbody><tr><td><p dir="ltr"><span>Instance Type</span></p></td><td><p dir="ltr"><span>PUT/Write</span></p></td><td><p dir="ltr"><span>GET/Read</span></p></td><td><p dir="ltr"><span>Parity</span></p></td><td><p dir="ltr"><span>mc CLI ver. </span></p></td><td><p dir="ltr"><span>MinIO ver.</span></p></td></tr><tr><td><p dir="ltr"><span>i3en.24xlarge</span></p></td><td><p dir="ltr"><span>165 GiB/sec</span></p></td><td><p dir="ltr"><span>325 GiB/sec</span></p><br/></td><td><p dir="ltr"><span>EC:4</span></p><br/></td><td><p dir="ltr"><span>RELEASE.2021-12-29T06-52-55Z</span></p><br/></td><td><p dir="ltr"><span>RELEASE.2021-12-29T06-49-06Z</span></p><br/></td></tr></tbody></table><!--kg-card-end: html--><p>On an aggregate basis this delivers PUT throughput of 1.32 Tbps and GET throughput of 2.6 Tbps. We believe this to be the fastest in the industry. <br/></p><p><strong>Benchmarking Setup</strong></p><p>MinIO believes in benchmarking on the same HW it would recommend to its customers. For primary storage, we recommend NVMe. We have followed this recommendation for over a year now as our customers have shown us that the price/performance characteristics of NVMe represent the sweet spot for these primary storage workloads. </p><!--kg-card-begin: html--><table><colgroup><col/><col width="64"/><col width="112"/><col/><col/><col/><col/></colgroup><tbody><tr><td><p dir="ltr"><span>Instance</span></p></td><td><p dir="ltr"><span># Nodes</span></p></td><td><p dir="ltr"><span>AWS Instance Type</span></p></td><td><p dir="ltr"><span>CPU</span></p></td><td><p dir="ltr"><span>MEM</span></p></td><td><p dir="ltr"><span>Storage</span></p></td><td><p dir="ltr"><span>Network</span></p></td></tr><tr><td><p dir="ltr"><span>Server</span></p></td><td><p dir="ltr"><span>32</span></p></td><td><p dir="ltr"><span>i3en.24xlarge</span></p></td><td><p dir="ltr"><span>96</span></p></td><td><p dir="ltr"><span>768GB</span></p></td><td><p dir="ltr"><span>8x7500GB</span></p></td><td><p dir="ltr"><span>100 Gbps</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>For the software, we used the default Ubuntu 20.04 install on AWS, the latest release of MinIO and our built in Speedtest capability. <br/></p><!--kg-card-begin: html--><table><colgroup><col/><col/></colgroup><tbody><tr><td><p dir="ltr"><span>Property</span></p></td><td><p dir="ltr"><span>Value</span></p></td></tr><tr><td><p dir="ltr"><span>Server OS</span></p></td><td><p dir="ltr"><span>RELEASE.2021-12-29T06-52-55Z</span></p></td></tr><tr><td><p dir="ltr"><span>MinIO Version</span></p></td><td><p dir="ltr"><span>RELEASE.2021-12-29T06-49-06Z</span></p></td></tr><tr><td><p dir="ltr"><span>Benchmark Tool</span></p></td><td><p dir="ltr"><span>mc admin speedtest</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>Speedtest is built into the MinIO Server and is accessed through the Console UI or <strong>mc admin speedtest</strong> command. It requires no special skills or additional software. You can read more about it <a href="https://blog.min.io/introducing-speedtest-for-minio/">here</a>. </p><p>The performance of each drive was measured using the command dd. DD is a unix tool used to  perform bit-by-bit copy of data from one file to another. It provides options to control the block  size of each read and write.</p><p>Here is a sample of a single NVMe drive’s Write Performance with 16MB block-size, O_DIRECT  option for a total of 64 copies. Note that we achieve greater than 1.1 GB/sec of write performance for each drive.</p><figure><img src="https://lh3.googleusercontent.com/f661cjCeImtiwhq_r86tBRYP2BPhEFR33Maz50MDJD0hGC1e-MUPCTBef3-h3S-pWB6Qjk41EqbkBsPatI4aaLxaipE6CbsREeCTtI4EBzTxrFANTuxgZrY-5VKHuAxarec3Cbbp" alt=""/></figure><p>Here is the output of a single HDD drive’s Read Performance with 16MB block-size using the  O_DIRECT option and a total count of 64. Note that we achieved greater than 2.3 GB/sec of  read performance for each drive. <br/></p><figure><img src="https://lh3.googleusercontent.com/DliFHtB9cyeUQ6Gvw-TtxRqsJGcrQUtfSGumqie5VDkzw5mKrJizXCpXkBXCF-W48I-elkK_sEYJe_FhG86M4bmDbmFNSMgp7fyxrIHsozSY7dPE6_26746hzZpu9Ej1dgoX1syX" alt=""/></figure><p><strong>Measuring JBOD Performance</strong></p><p>JBOD performance with O_DIRECT was measured using https://github.com/minio/dperf. dperf is a filesystem benchmark tool that generates and measures filesystem performance for both read and write. dperf command operating with 64 parallel threads, 4MB block-size and O_DIRECT by default.</p><figure><img src="https://lh5.googleusercontent.com/Of6nNv8PMNuM8mSGghjPRvk647It-F0yXgFN0Feu01s8kmy06lMxZhHDmrEQeswfOOsChCHmE_ndGhhaqx-NL6WvZcfrb3devnBmzJVBa5DmzFYrvi7wWqQ7eJkovDNNuYUawv6Q" alt=""/></figure><p><strong>Network Performance</strong></p><p>The network hardware on these nodes allows a maximum of 100 Gbit/sec. 100 Gbit/sec equates  to 12.5 Gbyte/sec (1 Gbyte = 8 Gbit).</p><p>Therefore, the maximum throughput that can be expected from each of these nodes would be  12.5 Gbyte/sec.</p><p><strong>Running the 32-node Distributed MinIO benchmark </strong><br/></p><p>MinIO ran Speedtest in autotune mode. The autotune mode incrementally increases the load to pinpoint maximum aggregate throughput.</p><p>$ mc admin speedtest minio/</p><p>The test will run and present results on screen. The test may take anywhere from a few seconds to several minutes to execute depending on your MinIO cluster. The flag -v indicates verbose mode. The user can determine the appropriate Erasure Code setting. We recommend EC:4 but include EC:2 and EC:4 below.</p><h3 id="minio_storage_class_standard-ec-2"><strong>MINIO_STORAGE_CLASS_STANDARD=EC:2</strong></h3><figure><img src="https://lh5.googleusercontent.com/4os4GTdlR7bDDrGCqPJ5s4bFWxn_OQWsKB5illLcOivxB1ijfowOME-dDUrLGlSJ576Rg43uh4ycD3svC-KjVrIhyM6g4hADAo5LQIMlU8GDj1SG0N8k1DdXgt0S5J3WfiaK8SWd" alt=""/></figure><h3 id="minio_storage_class_standard-ec-3"><strong>MINIO_STORAGE_CLASS_STANDARD=EC:3</strong></h3><figure><img src="https://lh4.googleusercontent.com/AhGOX25E5Y-LLMxCuSwYlTcqCFYFYmyffJc7o2MBD0Zw_-aosy3MHUVxTS-M8QjvlhMo-f0kPedY1bE5SvebjqJlMm3C8y40ZFrq3kqq5eI90n3Qquw48Uqj1ws-GnlTooGjWOWI" alt=""/></figure><h3 id="minio_storage_class_standard-ec-4-default-"><strong>MINIO_STORAGE_CLASS_STANDARD=EC:4 (default)</strong></h3><figure><img src="https://lh3.googleusercontent.com/mnRN7W45rblIw8JQxiR9uVPqe6kT7whIjlOAJPoAaeHb9WzHoBl10YfScj5_KY4TXP6kFpgn1ocuLrbH6q6EWW5GSs-la9Z2ugNShZ9pZCPBEgYcvb9NAJO3UAi3D1h2E-CbwfCg" alt=""/></figure><p>The average network bandwidth utilization during the write phase was 77 Gbit/sec and during  the read phase was 84.6 Gbit/sec. This represents client traffic as well as internode traffic. The portion of this bandwidth available to clients is about half for both reads and writes.</p><p>The network was almost entirely choked during these tests. Higher throughput can be expected if a dedicated network was available for inter-node traffic.</p><p>Note that the write benchmark is slower than read because benchmark tools do not account for write amplification (traffic from parity data generated during writes). In this case, the 100 Gbit  network is the bottleneck as MinIO gets close to hardware performance for both reads and writes. <br/></p><p><strong>Conclusion</strong></p><p>Based on the results above, we found that MinIO takes complete advantage of the available  hardware. Its performance is only constrained by the underlying hardware available to it. This benchmark has been tested with our recommended configuration for performance workloads and can be easily replicated in an hour for less than $350.</p><p>You can <a href="https://min.io/resources/docs/MinIO-Throughput-Benchmarks-on-NVMe-SSD-Speedtest.pdf">download a PDF of the Benchmark here</a>. You can download MinIO <a href="https://min.io/download">here</a>. If you have any questions, ping us on <a href="mailto:hello@min.io">hello@min.io</a> or join the <a href="https://slack.min.io/">Slack community</a>.</p>

                                            </section></div>
  </body>
</html>
