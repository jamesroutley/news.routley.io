<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/FlashMLA">Original</a>
    <h1>DeepSeek Open Source FlashMLA â€“ MLA Decoding Kernel for Hopper GPUs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">FlashMLA is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences serving.</p>
<p dir="auto">Currently released:</p>
<ul dir="auto">
<li>BF16</li>
<li>Paged kvcache with block size of 64</li>
</ul>




<div dir="auto" data-snippet-clipboard-copy-content="python tests/test_flash_mla.py"><pre>python tests/test_flash_mla.py</pre></div>
<p dir="auto">Achieving up to 3000 GB/s in memory-bound configuration and 580 TFLOPS in computation-bound configuration on H800 SXM5, using CUDA 12.6.</p>

<div dir="auto" data-snippet-clipboard-copy-content="from flash_mla import get_mla_metadata, flash_mla_with_kvcache

tile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)

for i in range(num_layers):
    ...
    o_i, lse_i = flash_mla_with_kvcache(
        q_i, kvcache_i, block_table, cache_seqlens, dv,
        tile_scheduler_metadata, num_splits, causal=True,
    )
    ..."><pre><span>from</span> <span>flash_mla</span> <span>import</span> <span>get_mla_metadata</span>, <span>flash_mla_with_kvcache</span>

<span>tile_scheduler_metadata</span>, <span>num_splits</span> <span>=</span> <span>get_mla_metadata</span>(<span>cache_seqlens</span>, <span>s_q</span> <span>*</span> <span>h_q</span> <span>//</span> <span>h_kv</span>, <span>h_kv</span>)

<span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>num_layers</span>):
    ...
    <span>o_i</span>, <span>lse_i</span> <span>=</span> <span>flash_mla_with_kvcache</span>(
        <span>q_i</span>, <span>kvcache_i</span>, <span>block_table</span>, <span>cache_seqlens</span>, <span>dv</span>,
        <span>tile_scheduler_metadata</span>, <span>num_splits</span>, <span>causal</span><span>=</span><span>True</span>,
    )
    ...</pre></div>

<ul dir="auto">
<li>Hopper GPUs</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.0 and above</li>
</ul>

<p dir="auto">FlashMLA is inspired by <a href="https://github.com/dao-AILab/flash-attention/">FlashAttention 2&amp;3</a> and <a href="https://github.com/nvidia/cutlass">cutlass</a> projects.</p>

<div dir="auto" data-snippet-clipboard-copy-content="@misc{flashmla2025,
      title={FlashMLA: Efficient MLA decoding kernel}, 
      author={Jiashi Li},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/FlashMLA}},
}"><pre><span>@misc</span>{<span>flashmla2025</span>,
      <span>title</span>=<span><span>{</span>FlashMLA: Efficient MLA decoding kernel<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Jiashi Li<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/FlashMLA}<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
