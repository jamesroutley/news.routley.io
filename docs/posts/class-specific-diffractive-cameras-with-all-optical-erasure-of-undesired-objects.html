<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3">Original</a>
    <h1>Class-specific diffractive cameras with all-optical erasure of undesired objects</h1>
    
    <div id="readability-page-1" class="page"><article lang="en">
                    <div>
                        
    


                        <ul data-test="article-identifier">
                            
    
        <li data-test="article-category">Research Article</li>
    
    
        <li>
            <span data-test="open-access">Open Access</span>
        </li>
    
    

                            <li><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2022-08-15">15 August 2022</time></a></li>
                        </ul>

                        
                        <ul data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yi-Luo" aria-label="Read more about Yi Luo" data-author-popup="auth-Yi-Luo">Yi Luo</a><sup><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup><sup> <a href="#na1">na1</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tianyi-Gan" aria-label="Read more about Tianyi Gan" data-author-popup="auth-Tianyi-Gan">Tianyi Gan</a><sup><a href="#Aff1">1</a>,<a href="#Aff3">3</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jingtian-Hu" aria-label="Read more about Jingtian Hu" data-author-popup="auth-Jingtian-Hu">Jingtian Hu</a><sup><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yuhang-Li" aria-label="Read more about Yuhang Li" data-author-popup="auth-Yuhang-Li">Yuhang Li</a><sup><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yifan-Zhao" aria-label="Read more about Yifan Zhao" data-author-popup="auth-Yifan-Zhao">Yifan Zhao</a><sup><a href="#Aff1">1</a>,<a href="#Aff3">3</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Deniz-Mengu" aria-label="Read more about Deniz Mengu" data-author-popup="auth-Deniz-Mengu">Deniz Mengu</a><sup><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup>, </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mona-Jarrahi" aria-label="Read more about Mona Jarrahi" data-author-popup="auth-Mona-Jarrahi">Mona Jarrahi</a><sup><a href="#Aff1">1</a>,<a href="#Aff3">3</a></sup> &amp; </li><li><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Aydogan-Ozcan" aria-label="Read more about Aydogan Ozcan" data-author-popup="auth-Aydogan-Ozcan" data-corresp-id="c1">Aydogan Ozcan<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a><span> 
            <a href="http://orcid.org/0000-0002-0717-683X"><span>ORCID: </span>orcid.org/0000-0002-0717-683X</a></span><sup><a href="#Aff1">1</a>,<a href="#Aff2">2</a>,<a href="#Aff3">3</a></sup> </li></ul>
                        <p data-container-section="info">
                            
    <a data-test="journal-link" href="https://elight.springeropen.com/"><i data-test="journal-title">eLight</i></a>

                            <b data-test="journal-volume"><span>volume</span> 2</b>, Article number: <span data-test="article-number">14</span> (<span data-test="article-publication-year">2022</span>)
            <a href="#citeas" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    <div>
        <ul>
            
                <li>
                    <p>1393 <span>Accesses</span></p>
                </li>
            
            
            
                
                    <li>
                        <p>102 <span>Altmetric</span></p>
                    </li>
                
            
            <li>
                <p><a href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span>details</span></a></p>
            </li>
        </ul>
    </div>

                        
                        
    

    

                        
                    </div>

                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div id="Abs1-section"><h2 id="Abs1">Abstract</h2><p>Privacy protection is a growing concern in the digital era, with machine vision techniques widely used throughout public and private settings. Existing methods address this growing problem by, e.g., encrypting camera images or obscuring/blurring the imaged information through digital algorithms. Here, we demonstrate a camera design that performs class-specific imaging of target objects with instantaneous all-optical erasure of other classes of objects. This diffractive camera consists of transmissive surfaces structured using deep learning to perform selective imaging of target classes of objects positioned at its input field-of-view. After their fabrication, the thin diffractive layers collectively perform optical mode filtering to accurately form images of the objects that belong to a target data class or group of classes, while instantaneously erasing objects of the other data classes at the output field-of-view. Using the same framework, we also demonstrate the design of class-specific permutation and class-specific linear transformation cameras, where the objects of a target data class are pixel-wise permuted or linearly transformed following an arbitrarily selected transformation matrix for all-optical class-specific encryption, while the other classes of objects are irreversibly erased from the output image. The success of class-specific diffractive cameras was experimentally demonstrated using terahertz (THz) waves and 3D-printed diffractive layers that selectively imaged only one class of the MNIST handwritten digit dataset, all-optically erasing the other handwritten digits. This diffractive camera design can be scaled to different parts of the electromagnetic spectrum, including, e.g., the visible and infrared wavelengths, to provide transformative opportunities for privacy-preserving digital cameras and task-specific data-efficient imaging.</p></div></section>
                    
    
        <p>
            <svg width="24" height="24" aria-hidden="true" focusable="false"><use xlink:href="#icon-info"></use></svg>
            <a href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/peer-review" data-track="click" data-track-category="article body" data-track-action="open peer review reports" data-track-label="10.1186/s43593-022-00021-3">Peer Review reports</a>
        </p>
    


                    <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>Digital cameras and computer vision techniques are ubiquitous in modern society. Over the past few decades, computer vision-assisted applications have been adapted massively in a wide range of fields [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="J. Scharcanski, Bringing vision-based measurements into our daily life: a grand challenge for computer vision systems. Front. ICT 3, 3 (2016)" href="#ref-CR1" id="ref-link-section-d61071694e474">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="X. Feng, Y. Jiang, X. Yang, M. Du, X. Li, Computer vision algorithms and hardware implementations: a survey. Integration 69, 309–320 (2019)" href="#ref-CR2" id="ref-link-section-d61071694e474_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="M. Al-Faris, J. Chiverton, D. Ndzi, A.I. Ahmed, A review on computer vision-based methods for human action recognition. J. Imaging 6, 46 (2020)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR3" id="ref-link-section-d61071694e477">3</a>], such as video surveillance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="X. Wang, Intelligent multi-camera video surveillance: a review. Pattern Recogn. Lett. 34, 3–19 (2013)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR4" id="ref-link-section-d61071694e480">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="N. Haering, P.L. Venetianer, A. Lipton, The evolution of video surveillance: an overview. Mach. Vis. Appl. 19, 279–290 (2008)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR5" id="ref-link-section-d61071694e483">5</a>], autonomous driving assistance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="E. D. Dickmannsin, The development of machine vision for road vehicles in the last decade. in IEEE Intelligent Vehicle Symposium, 2002, vol. 1 (2002), p. 268–281." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR6" id="ref-link-section-d61071694e486">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="J. Janai, F. Güney, A. Behl, A. Geiger, Computer vision for autonomous vehicles: problems, datasets and state of the art. CGV 12, 1–308 (2020)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR7" id="ref-link-section-d61071694e490">7</a>], medical imaging [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="A. Esteva et al., Deep learning-enabled medical computer vision. NPJ Digit. Med. 4, 1–9 (2021)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR8" id="ref-link-section-d61071694e493">8</a>], facial recognition, and body motion tracking [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="M. Tistarelli, M. Bicego, E. Grosso, Dynamic face recognition: from human to machine vision. Image Vis. Comput. 27, 222–232 (2009)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR9" id="ref-link-section-d61071694e496">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="T.B. Moeslund, E. Granum, A survey of computer vision-based human motion capture. Comput. Vis. Image Underst. 81, 231–268 (2001)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR10" id="ref-link-section-d61071694e499">10</a>]. With the comprehensive deployment of digital cameras in workspaces and public areas, a growing concern for privacy has emerged due to the tremendous amount of image data being collected continuously [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="G. Singh, G. Bhardwaj, S.V. Singh, V. Garg, Biometric identification system: security and privacy concern, in Artificial intelligence for a sustainable industry 4.0. ed. by S. Awasthi, C.M. Travieso-González, G. Sanyal, D. Kumar Singh (Springer International Publishing, Berlin, 2021), pp. 245–264. 
                  https://doi.org/10.1007/978-3-030-77070-9_15
                  
                " href="#ref-CR11" id="ref-link-section-d61071694e502">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="A. Acquisti, L. Brandimarte, G. Loewenstein, Privacy and human behavior in the age of information. Science 347, 509–514 (2015)" href="#ref-CR12" id="ref-link-section-d61071694e502_1">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="A. Acquisti, L. Brandimarte, J. Hancock, How privacy’s past may shape its future. Science 375, 270–272 (2022)" href="#ref-CR13" id="ref-link-section-d61071694e502_2">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="W.N. Price, I.G. Cohen, Privacy in the age of medical big data. Nat. Med. 25, 37–43 (2019)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR14" id="ref-link-section-d61071694e505">14</a>]. Some commonly used methods address this concern by applying post-processing algorithms to conceal sensitive information from the acquired images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="J.R. Padilla-López, A.A. Chaaraoui, F. Flórez-Revuelta, Visual privacy protection methods: a survey. Expert Syst. Appl. 42, 4177–4195 (2015)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR15" id="ref-link-section-d61071694e509">15</a>]. Following the computer vision-aided detection of the sensitive content, traditional image redaction algorithms, such as image blurring [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="C. Neustaedter, S. Greenberg, M. Boyle, Blur filtration fails to preserve privacy for home-based video conferencing. ACM Trans. Comput.-Hum. Interact. 13, 1–36 (2006)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR16" id="ref-link-section-d61071694e512">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="A. Frome, et al., Large-scale privacy protection in Google Street View. in 2009 IEEE 12th International Conference on Computer Vision (2009), pp. 2373–2380. 
                  https://doi.org/10.1109/ICCV.2009.5459413
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR17" id="ref-link-section-d61071694e515">17</a>], encryption [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="F. Dufaux, T. Ebrahimi, Scrambling for privacy protection in video surveillance systems. IEEE Trans. Circuits Syst. Video Technol. 18, 1168–1174 (2008)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR18" id="ref-link-section-d61071694e518">18</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="W. Zeng, S. Lei, Efficient frequency domain selective scrambling of digital video. IEEE Trans. Multimed. 5, 118–129 (2003)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR19" id="ref-link-section-d61071694e521">19</a>], and image inpainting [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="A. Criminisi, P. Perez, K. Toyama, Region filling and object removal by exemplar-based image inpainting. IEEE Trans. Image Process. 13, 1200–1212 (2004)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR20" id="ref-link-section-d61071694e524">20</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="K. Inai, M. Pålsson, V. Frinken, Y. Feng, S. Uchida, Selective concealment of characters for privacy protection. in 2014 22nd International Conference on Pattern Recognition (2014), p. 333–338. 
                  https://doi.org/10.1109/ICPR.2014.66
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR21" id="ref-link-section-d61071694e528">21</a>] are performed to secure private information such as human faces, plate numbers, or background objects. In recent years, deep learning techniques have further strengthened these algorithmic privacy preservation methods in terms of their robustness and speed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="R. Uittenbogaard et al., Privacy protection in street-view panoramas using depth and multi-view imagery. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, 2019), pp. 10573–10582. 
                  https://doi.org/10.1109/CVPR.2019.01083
                  
                ." href="#ref-CR22" id="ref-link-section-d61071694e531">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="K. Brkic, I. Sikiric, T. Hrkac, Z. Kalafatic, I know that person: generative full body and face de-identification of people in images. in 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2017), pp. 1319–1328. 
                  https://doi.org/10.1109/CVPRW.2017.173
                  
                ." href="#ref-CR23" id="ref-link-section-d61071694e531_1">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="F. Pittaluga, S. Koppal, A. Chakrabarti, Learning privacy preserving encodings through adversarial training. in 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), (IEEE, 2019), pp. 791–799. 
                  https://doi.org/10.1109/WACV.2019.00089
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR24" id="ref-link-section-d61071694e534">24</a>]. Despite the success of these software-based privacy protection techniques, there exists an intrinsic risk of raw data exposure given the fact that the subsequent image processing is executed after the raw data recording/digitization and transmission, especially when the required digital processing is performed on a remote device, e.g., a cloud-based server.</p><p>Another set of solutions to such privacy concerns can be implemented at the hardware/board level, in which the data processing happens right after the digital quantization of an image, but before its transmission. Such solutions protect privacy by performing in-situ image modifications using camera-integrated online processing modules. For instance, by embedding a digital signal processor (DSP) or Trusted Platform Module (TPM) into a smart camera, the sensitive information can be encrypted or deidentified [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="A. Chattopadhyay, T. E. Boult, PrivacyCam: a privacy preserving camera using uCLinux on the Blackfin DSP. in 2007 IEEE Conference on Computer Vision and Pattern Recognition (2007), pp. 1–8, 
                  https://doi.org/10.1109/CVPR.2007.383413
                  
                ." href="#ref-CR25" id="ref-link-section-d61071694e540">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="T. Winkler, B. Rinner, TrustCAM: security and privacy-protection for an embedded smart camera based on trusted computing. in 2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (2010), pp. 593–600, 
                  https://doi.org/10.1109/AVSS.2010.38
                  
                ." href="#ref-CR26" id="ref-link-section-d61071694e540_1">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Mrityunjay, P. J. Narayanan, The de-identification camera. in 2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (2011), pp. 192–195. 
                  https://doi.org/10.1109/NCVPRIPG.2011.48
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR27" id="ref-link-section-d61071694e543">27</a>]. These camera integration solutions provide an additional layer of protection against potential attacks during the data transmission stage; however, they do not completely resolve privacy concerns as the original information is already captured digitally, and adversarial attacks can happen right after the camera’s digital quantization.</p><p>Implementing these image redaction algorithms or embedded DSPs for privacy protection also creates some environmental impact as a compromise. To support the computation/processing of massive amounts of visual data being generated every day [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="53 Important statistics about how much data is created every day. Financesonline.com. (2021). 
                  https://financesonline.com/how-much-data-is-created-every-day/
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR28" id="ref-link-section-d61071694e549">28</a>], i.e., billions of images and millions of hours of videos, the demand for digital computing power and data storage space rapidly increases, posing a major challenge for sustainability [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="P. Dhar, The carbon impact of artificial intelligence. Nature Machine Intelligence 2, 423–425 (2020)" href="#ref-CR29" id="ref-link-section-d61071694e552">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="S. Thakur, A. Chaurasia, Towards Green Cloud Computing: Impact of carbon footprint on environment. in 2016 6th International Conference—Cloud System and Big Data Engineering (Confluence), (2016), pp. 209–213. 
                  https://doi.org/10.1109/CONFLUENCE.2016.7508115
                  
                ." href="#ref-CR30" id="ref-link-section-d61071694e552_1">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="L. Belkhir, A. Elmeligi, Assessing ICT global emissions footprint: trends to 2040 &amp; recommendations. J. Clean. Prod. 177, 448–463 (2018)" href="#ref-CR31" id="ref-link-section-d61071694e552_2">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="M. Durante, Computational power: the impact of ICT on law, society and knowledge (Routledge, London, 2021). 
                  https://doi.org/10.4324/9781003098683
                  
                " href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR32" id="ref-link-section-d61071694e555">32</a>].</p><p>Intervening into the light propagation and image formation stage and passively enforcing privacy before the image digitization can potentially provide more desired solutions to both of these challenges outlined earlier. For example, some of the existing works use customized optics or sensor read-out circuits to modify the image formation models, so that the sensor only captures low-resolution images of the scene and, therefore, the identifying information can be concealed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pittaluga, F. &amp; Koppal, S. J. Privacy preserving optics for miniature vision sensors. in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), (IEEE, 2015), pp. 314–324. 
                  https://doi.org/10.1109/CVPR.2015.7298628
                  
                ." href="#ref-CR33" id="ref-link-section-d61071694e561">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="F. Pittaluga, A. Zivkovic, S. J. Koppal, Sensor-level privacy for thermal cameras. in 2016 IEEE International Conference on Computational Photography (ICCP) (2016), pp. 1–12. 
                  https://doi.org/10.1109/ICCPHOT.2016.7492877
                  
                ." href="#ref-CR34" id="ref-link-section-d61071694e561_1">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="C. Hinojosa, J. C. Niebles, H. Arguello, Learning privacy-preserving Optics for Human Pose Estimation. in 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (IEEE, 2021), pp. 2553–2562. 
                  https://doi.org/10.1109/ICCV48922.2021.00257
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR35" id="ref-link-section-d61071694e564">35</a>]. Such methods sacrifice the image quality of the entire sample field-of-view (FOV) for privacy preservation, and therefore, a delicate balance between the final image quality and privacy preservation exists; a change in this balance for different objects can jeopardize imaging performance or privacy. Furthermore, degrading the image quality of the entire FOV limits the applicable downstream tasks to low-resolution operations such as human pose estimation. In fact, sacrificing the entire image quality can be unacceptable under some circumstances such as e.g., in autonomous driving. Additionally, since these methods establish a blurred or low-resolution pixel-to-pixel mapping between the input scene and the output image, the original information of the samples can be potentially retrieved via digital inverse models, using e.g., blind image deconvolution or estimation of the inherent point-spread function.</p><p>Here, we present a new camera design using diffractive computing, which images the target types/classes of objects with high fidelity, while all-optically and instantaneously erasing other types of objects at its output (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig1">1</a>). This computational camera processes the optical modes that carry the sample information using successive diffractive layers optimized through deep learning by minimizing a training loss function customized for class-specific imaging. After the training phase, these diffractive layers are fabricated and assembled together in 3D, forming a computational imager between an input FOV and an output plane. This camera design is not based on a standard point-spread function, and instead the 3D-assembled diffractive layers collectively act as an optical mode filter that is statistically optimized to pass through the major modes of the target classes of objects, while filtering and scattering out the major representative modes of the other classes of objects (learned through the data-driven training process). As a result, when passing through the diffractive camera, the input objects from the target classes form clear images at the output plane, while the other classes of input objects are all-optically erased, forming non-informative patterns similar to background noise, with lower light intensity. Since all the spatial information of non-target object classes is instantaneously erased through light diffraction within a thin diffractive volume, their direct or low-resolution images are never recorded at the image plane, and this feature can be used to reduce the image storage and transmission load of the camera. Except for the illumination light, this object class-specific camera design does not utilize external computing power and is entirely based on passive transmissive layers, providing a highly power-efficient solution to task-specific and privacy-preserving imaging.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="699"/></picture></a></div><p>Object class-specific imaging using a diffractive camera. <b>a</b> Illustration of a three-layer diffractive camera trained to perform object class-specific imaging with instantaneous all-optical erasure of the other classes of objects at its output FOV. <b>b</b> The experimental setup for the diffractive camera testing using coherent THz illumination</p></div></figure></div><p>We experimentally demonstrated the success of this new class-specific camera design using THz radiation and 3D-printed diffractive layers that were assembled together (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig1">1</a>) to specifically and selectively image only one data class of the MNIST handwritten digit database [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Y. LeCun, et al., Handwritten Digit Recognition With A Back-Propagation Network. in Advances in Neural Information Processing Systems vol. 2, (Morgan-Kaufmann, 1989)." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR36" id="ref-link-section-d61071694e604">36</a>], while all-optically rejecting the images of all the other handwritten digits at its output FOV. Despite the random variations observed in handwritten digits (from human to human), our analysis revealed that any arbitrary handwritten digit/class or group of digits could be selected as the target, preserving the same all-optical rejection/erasure capability for the remaining classes of handwritten digits. Besides handwritten digits, we also showed that the same framework can be generalized to class-specific imaging and erasure of more complicated objects, such as some fashion products [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="H. Xiao, K. Rasul, R. Vollgraf, Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. (2017). 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR37" id="ref-link-section-d61071694e607">37</a>]. Additionally, we demonstrated class-specific imaging of input FOVs with multiple objects simultaneously present, where only the objects that belong to the target class were imaged at the output plane, while the rest were all-optically erased. Furthermore, this class-specific camera design was shown to be robust to variations in the input illumination intensity and the position of the input objects. Apart from direct imaging of the target objects from specific data classes, we further demonstrated that this diffractive imaging framework can be used to design class-specific permutation and class-specific linear transformation cameras that output pixel-wise permuted or linearly transformed images (following an arbitrarily selected image transformation matrix) of the target class of objects, while all-optically erasing other types of objects at the output FOV—performing class-specific encryption all-optically.</p><p>The teachings of this diffractive camera design can inspire future imaging systems that consume orders of magnitude less computing and transmission power as well as less data storage, helping with our global need for task-specific, data-efficient and privacy-aware modern imaging systems.</p></div></div></section><section data-title="Results"><div id="Sec2-section"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Class-specific imaging using diffractive cameras</h3><p>We first numerically demonstrate the class-specific camera design using the MNIST handwritten digit dataset, to selectively image handwritten digit ‘2’ (the object class of interest) while instantaneously erasing the other handwritten digits. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>a, a three-layer diffractive imager with phase-only modulation layers was trained under an illumination wavelength of <span>\(\lambda\)</span>. Each diffractive layer contains 120 <span>\(\times\)</span> 120 trainable transmission phase coefficients (i.e., diffractive features/neurons), each with a size of ~ 0.53<span>\(\lambda\)</span>. The axial distance between the input/sample plane and the first diffractive layer, between any two consecutive diffractive layers, and between the last diffractive layer and the output plane were all set to ~ 26.7<span>\(\lambda\)</span>. The phase modulation values of the diffractive neurons at each transmissive layer were iteratively updated using a stochastic gradient-descent-based algorithm to minimize a customized loss function, enabling object class-specific imaging. For the data class of interest, the training loss terms included the normalized mean square error (NMSE) and the negative Pearson Correlation Coefficient (PCC) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="J. Benesty, J. Chen, Y. Huang, I. Cohen, Pearson correlation coefficient. in Noise reduction in speech processing (Springer Berlin Heidelberg, 2009)." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR38" id="ref-link-section-d61071694e698">38</a>] between the output image and the input, aiming to optimize the image fidelity at the output plane for the correct class of objects. For all the other classes of objects (to be all-optically erased), we penalized the statistical similarity between the output image and the input object (see “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Sec10">Methods</a>” section for details). This well-balanced training loss function enabled the output images from the non-target classes of objects (i.e., the handwritten digits 0, 1, 3–9) to be all-optically erased at the output FOV, forming speckle-like background patterns with lower average intensity, whereas all the input objects of the target data class (i.e., handwritten examples of digit 2) formed high-quality images at the output plane. The resulting diffractive layers that are learned through this data-driven training process are reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>b, which collectively function as a spatial mode filter that is data class-specific.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="463"/></picture></a></div><p>Design schematic and blind testing results of the class-specific diffractive camera. <b>a</b> The physical layout of the three-layer diffractive camera design. <b>b</b> Phase modulation patterns of the converged diffractive layers of the camera. <b>c</b> The blind testing results of the diffractive camera. The output images were normalized using the same constant for visualization</p></div></figure></div><p>After its training, we numerically tested this diffractive camera design using 10,000 MNIST test digits, which were not used during the training process. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>c reports some examples of the blind testing output of the trained diffractive imager and the corresponding input objects. These results demonstrate that the diffractive camera learned to selectively image the input objects that belong to the target data class, even if they have statistically diverse styles due to the varying nature of human handwriting. As desired, the diffractive camera generates unrecognizable noise-like patterns for the input objects from all the other data classes, all-optically erasing their information at its output plane. Stated differently, the image formation is intervened at the coherent wave propagation stage for the undesired data classes, where the characteristic optical modes that statistically represent the input objects of these non-target data classes are scattered out of the output FOV of our diffractive camera.</p><p>Importantly, this diffractive camera is <i>not</i> based on a standard point-spread function-based pixel-to-pixel mapping between the input and output FOVs, and therefore, it does not automatically result in signals within the output FOV for the transmitting input pixels that statistically overlap with the objects from the target data class. For example, the handwritten digits ‘3’ and ‘8’ in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>c were completely erased at the output FOV, regardless of the considerable amount of common (transmitting) pixels that they statistically share with the handwritten digit ‘2’. Instead of developing a spatially-invariant point-spread function, our designed diffractive camera statistically learned the characteristic optical modes possessed by different training examples, to converge as an optical mode filter, where the main modes that represent the target class of objects can pass through with minimum distortion of their relative phase and amplitude profiles, whereas the spatial information carried by the characteristic optical modes of the other data classes were scattered out. The deep learning-based optimization using the training images/examples is the key for the diffractive camera to statistically learn which optical modes must be filtered out and which group of modes needs to pass through the diffractive layers so that the output images accurately represent the spatial features of the input objects for the correct data class. As detailed in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Sec10">Methods</a>” section, the training loss function and its penalty terms for the target data class and the other classes are crucial for achieving this performance.</p><p>In addition to these results summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>, the same class-specific imaging system can also be adapted to selectively image input objects of other data classes by simply re-dividing the training image dataset into desired/target vs. unwanted classes of objects. To demonstrate this, we show different diffractive camera designs in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S1, where the same class-specific performance was achieved for the selective imaging of e.g., handwritten test objects from digits ‘5’ or ‘7’, while all-optically erasing the other data classes at the output FOV. Even more remarkable, the diffractive camera design can also be optimized to selectively image a desired <i>group of data classes</i>, while still rejecting the objects of the other data classes. For example, Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S1 reports a diffractive camera that successfully imaged handwritten test objects belonging to digits ‘2’, ‘5’, and ‘7’ (defining the target group of data classes), while erasing all the other handwritten digits all-optically. Stated differently, the diffractive camera was in this case optimized to selectively image three different data classes in the same design, while successfully filtering out the remaining data classes at its output FOV (see Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S1).</p><p>To further demonstrate the success of the presented class-specific diffractive camera design for processing more complicated objects, we extended it to specifically image only one class of fashion products [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="H. Xiao, K. Rasul, R. Vollgraf, Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. (2017). 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR37" id="ref-link-section-d61071694e774">37</a>] (i.e., trousers). As shown in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S2, a seven-layer diffractive camera was designed to achieve class-specific imaging of trousers within the Fashion MNIST dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="H. Xiao, K. Rasul, R. Vollgraf, Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. (2017). 
                  https://doi.org/10.48550/arXiv.1708.07747
                  
                ." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR37" id="ref-link-section-d61071694e780">37</a>], while all-optically erasing/rejecting four other classes of the fashion products (i.e., dresses, sandals, sneakers, and bags). These results, summarized in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S2, further demonstrate the successful generalization of our class-specific diffractive imaging approach to more complex objects.</p><p>Next, we evaluated the diffractive camera’s performance with respect to the number of transmissive layers in its design (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a> and Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S1). Except for the number of diffractive layers, all the other hyperparameters of these camera designs were kept the same as before, for both the training and testing procedures. The patterns of the converged diffractive layers of each camera design are illustrated in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S3. The comparison of the class-specific imaging performance of these diffractive cameras with different numbers of trainable transmissive layers can be found in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>. Improved fidelity of the output images corresponding to the objects from the target data class can be observed as the number of diffractive layers increases, exhibiting higher image contrast, closely matching the input object features (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>a). At the same time, for the input objects from the non-target data classes, all the three diffractive camera designs generated unrecognizable noise-like patterns, all-optically erasing their information at the output. The same depth advantage can also be observed when another digit or a group of digits were selected as the target data classes. In Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S1, we compare the diffractive camera designs with three, five, and seven successive layers and demonstrate that deeper diffractive camera designs with more layers imaged the target classes of objects with higher fidelity and contrast compared to those with fewer diffractive layers.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="447"/></picture></a></div><p>Performance advantages of deeper diffractive cameras. <b>a</b> Comparison of the output images using diffractive camera designs with three, four, and five layers. The output images at each row were normalized using the same constant for visualization. <b>b</b> Quantitative comparison of the three diffractive camera designs. The left panel compares the average PCC values calculated using input objects from the target data class only (i.e., 1032 different handwritten digits). The middle panel compares the average absolute PCC values calculated using input objects from the other data classes (i.e., 8968 different handwritten digits). The right panel plots the average output intensity ratio (<span>\(R\)</span>) of the target to non-target data classes</p></div></figure></div><p>We also quantified the blind testing performance of each diffractive camera design by calculating the average PCC value between the output images and the ground truth (i.e., input objects); see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>b. For this quantitative analysis, the MNIST testing dataset was first divided into target class objects (<span>\({n}_{1}=\)</span> 1032 handwritten test objects for digit ‘2’) and non-target class objects (<span>\({n}_{2}=\)</span> 8968 handwritten test objects for all the other digits), and the average PCC value was calculated separately for each object group. For the target data class of interest, the higher PCC value presents an improved imaging fidelity. For the other, non-target data classes, however, the <i>absolute</i> PCC values were used as an “erasure figure-of-merit”, as the PCC values close to either 1 or −1 can indicate interpretable image information, which is undesirable for object erasure. Therefore, the average PCC values of the target class objects (<span>\({n}_{1}\)</span>) and the average absolute PCC values of the non-target classes of objects (<span>\({n}_{2}\)</span>) are presented in the first two charts in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>b. The depth advantage of the class-specific diffractive camera designs is clearly demonstrated in these results, where a deeper diffractive imager with e.g., five transmissive layers achieved (1) a better output image fidelity and a higher average PCC value for imaging the target class of objects, and (2) an improved all-optical erasure of the undesired objects (with a lower absolute PCC value) for the non-target data classes as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>b.</p><p>In addition to these, a deeper diffractive camera also creates a stronger signal intensity separation between the output images of the target and non-target data classes. To quantify this signal-to-noise ratio advantage at the output FOV, we defined the average output intensity ratio (<span>\(R\)</span>) of the target to non-target data classes as:</p><div id="Equ1"><p><span>$$\begin{array}{c}R=\frac{\frac{1}{{n}_{1}}{\sum }_{i=1}^{{n}_{1}}{\overline{O} }_{i}^{+}}{\frac{1}{{n}_{2}}{\sum }_{i=1}^{{n}_{2}}{\overline{O} }_{i}^{-}}\end{array}$$</span></p><p>
                    (1)
                </p></div><p>where the numerator is the average output intensity of <span>\({n}_{1}=\)</span> 1032 test objects from the target data class (denoted as <span>\({O}_{i}^{+})\)</span>, and the denominator is the average output intensity of <span>\({n}_{2}=\)</span> 8968 test objects from all the other data classes (denoted as <span>\({O}_{i}^{-})\)</span>. The <span>\(R\)</span> values of three-, four-, and five-layer diffractive camera designs were found to be 1.354, 1.464, and 1.532, respectively, as summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>b. These quantitative results once again confirm that a deeper diffractive camera with more trainable layers exhibits a better performance in its class-specific imaging task and achieves an improved signal-to-noise ratio at its output.</p><p>Note that, a class-specific diffractive camera trained with the standard grayscale MNIST images retains its designed functionality even when the input objects face varying illumination conditions. To demonstrate this, we first blindly tested the five-layer diffractive camera design reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>a under varying levels of intensity (from low to high intensity and eventually saturated, where the grayscale features of the input objects became binary). As reported in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM2">2</a>: Movie S1, the diffractive camera selectively images the input objects from the target class and robustly erases the information of the non-target classes of input objects, regardless of the intensity, even when the objects became saturated and structurally deformed from their grayscale features. We further blindly tested the same five-layer diffractive camera design reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>a with the input objects illuminated under spatially non-uniform intensity distributions, deviating from the training features. As shown in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM3">3</a>: Movie S2, the class-specific diffractive camera still worked as designed under non-uniform input illumination intensities, demonstrating its effectiveness and robustness in handling complex scenarios with varying lighting conditions. These input distortions highlighted in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM2">2</a>: Movie S1, Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM3">3</a>: Movie S2 were never seen/used during the training phase, and illustrate the generalization performance of our diffractive camera design as an optical mode filter, performing class-specific imaging.</p><h3 id="Sec4">Simultaneous imaging of multiple objects from different data classes</h3><p>In a more general scenario, multiple objects of different classes can be presented in the same input FOV. To exemplify such an imaging scenario, the input FOV of the diffractive camera was divided into 3 <span>\(\times\)</span> 3 subregions, and a random handwritten digit/object could appear in each subregion (see e.g., Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>). Based on this larger FOV with multiple input objects, a three-layer and a five-layer diffractive camera were separately designed to selectively image the whole input plane, all-optically erasing all the presented objects from the non-target data classes (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>a). The design parameters of these diffractive cameras were the same as the cameras reported in the previous subsection, except that each diffractive layer was expanded from 120 <span>\(\times\)</span> 120 to 300 <span>\(\times\)</span> 300 diffractive pixels to accommodate the increased input FOV. During the training phase, 48,000 MNIST handwritten digits appeared randomly at each subregion, and the handwritten digit ‘2’ was selected as our target data class to be specifically imaged. The diffractive layers of the converged camera designs are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>b for the three-layer diffractive camera and in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>c for the five-layer diffractive camera.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="831"/></picture></a></div><p>Simultaneous imaging of multiple objects of different data classes using a diffractive camera. <b>a</b> Schematic and the blind testing results of a three-layer diffractive camera and a five-layer diffractive camera. The output images in each row were normalized using the same constant for visualization. <b>b</b> Phase modulation patterns of the converged diffractive layers for the three-layer diffractive camera design. <b>c</b> Phase modulation patterns of the converged diffractive layers for the five-layer diffractive camera design</p></div></figure></div><p>During the blind testing phase of each of these diffractive cameras, the input test objects were randomly generated using the combinations of 10,000 MNIST test digits (not included in the training). Our imaging results reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>a reveal that these diffractive camera designs can selectively image the handwritten test objects from the target data class, while all-optically erasing the other objects from the remaining digits in the same FOV, regardless of which subregion they are located at. It is also demonstrated that, compared with the three-layer design, the deeper diffractive camera with five trained layers generated output images with improved fidelity and higher contrast for the target class of objects, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a>a. At the same time, this deeper diffractive camera achieved stronger suppression of the objects from the non-target data classes, generating lower output intensities for these undesired objects.</p><h3 id="Sec5">Class-specific camera design with random object displacements over a large input field-of-view</h3><p>In consideration of different imaging scenarios, where the target objects can appear at arbitrary spatial locations within a large input FOV, we further demonstrated a class-specific camera design that selectively images the input objects from a given data class within a large FOV. As the space-bandwidth product at the input (SBP<sub>i</sub>) and the output (SBP<sub>o</sub>) planes increased in this case, we used a deeper architecture with more diffractive neurons, since in general the number of trainable diffractive features in a given design needs to scale proportional to SBP<sub>i</sub> × SBP<sub>o</sub> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="O. Kulce, D. Mengu, Y. Rivenson, A. Ozcan, All-optical information-processing capacity of diffractive surfaces. Light Sci. Appl. 10, 25 (2021)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR39" id="ref-link-section-d61071694e1424">39</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="O. Kulce, D. Mengu, Y. Rivenson, A. Ozcan, All-optical synthesis of an arbitrary linear transformation using diffractive surfaces. Light Sci. Appl. 10, 196 (2021)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR40" id="ref-link-section-d61071694e1428">40</a>]. Therefore, we used seven diffractive layers, each with 300 <span>\(\times\)</span> 300 diffractive neurons/pixels. During the training phase, 48,000 MNIST handwritten digits were randomly placed within the input FOV of the camera, one by one, and the handwritten digit ‘2’ was selected to be specifically imaged at the corresponding location on the output/image plane while the input objects from the other classes were to be erased (see the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Sec10">Methods</a>” section). As demonstrated in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM4">4</a>: Movie S3, test objects from the target data class (the handwritten digit ‘2’) can be faithfully imaged regardless of their varying locations, while the objects from the other data classes were all-optically erased, only yielding noisy images at the output plane. This deeper diffractive camera exhibits class-specific imaging over a larger input FOV regardless of the random displacements of the input objects. The blind testing performance shown in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM4">4</a>: Movie S3 can be further improved with wider and deeper diffractive camera architectures with more trainable features to better cope with the increased space-bandwidth product at the input and output fields-of-view.</p><h3 id="Sec6">Class-specific permutation camera design</h3><p>Apart from directly imaging the objects from a target data class, a class-specific diffractive camera can also be designed to output pixel-wise permuted images of target objects, while all-optically erasing other types of objects. To demonstrate this class-specific image permutation as a form of all-optical encryption, we designed a five-layer diffractive permutation camera, which takes MNIST handwritten digits as its input and performs an all-optical permutation only on the target data class (e.g., handwritten digit ‘2’). The corresponding inverse permutation operation can be sequentially applied on the pixel-wise permuted output images to recover the original handwritten digits, ‘2’. The other handwritten digits, however, will be all-optically erased, with noise-like features appearing at the output FOV, before and after the inverse permutation operation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>a). Stated differently, the all-optical permutation of this diffractive camera operates on a specific data class, whereas the rest of the objects from other data classes are irreversibly lost/erased at the output FOV.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Fig. 5"><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="866"/></picture></a></div><p>Class-specific permutation camera. <b>a</b> Illustration of a five-layer diffractive camera trained to perform class-specific permutation operation (denoted as <span>\({\varvec{P}}\)</span>) with instantaneous all-optical erasure of the other classes of objects at its output FOV. Application of an inverse permutation (<span>\({{\varvec{P}}}^{-1}\)</span>) to the output images recovers the original objects of the target data class, whereas the rest of the objects from other data classes are irreversibly lost/erased at the output FOV. The output images were normalized using the same constant for visualization. <b>b</b> Phase modulation patterns of the converged diffractive layers of the class-specific permutation camera</p></div></figure></div><p>To design this class-specific permutation camera, a random permutation matrix <span>\({\varvec{P}}\)</span> was first generated (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>), which describes a unique one-to-one mapping of each image pixel at the input FOV to a new location/pixel at the output FOV. This randomly selected, desired permutation matrix <span>\({\varvec{P}}\)</span> was applied to each input image <span>\(G\)</span> and the resulting permuted image <span>\(({\varvec{P}}G)\)</span> was used as the ground truth throughout the training process of the permutation camera. The training loss function remained the same as in the previous five-layer diffractive design reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>a; however, instead of calculating the loss using the output and the input (<span>\(G\)</span>) images, this class-specific permutation camera design was optimized by minimizing the loss calculated using the output images and the permuted input images (<span>\({\varvec{P}}G\)</span>). The converged diffractive layers of this class-specific permutation camera are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>b.</p><p>During the blind testing phase, the designed class-specific permutation camera was tested with 10,000 MNIST digits, never used in the training phase. As demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>a, this permutation camera learned to selectively permute the input objects that belong to the target class (i.e., the handwritten digit ‘2’), generating output intensity patterns that closely resemble <span>\({\varvec{P}}G\)</span>. This class-specific all-optical permutation operation performed by the diffractive camera resulted in uninterpretable patterns of the target objects at the output FOV, which cannot be decoded without the knowledge of the permutation matrix, <span>\({\varvec{P}}\)</span>. On the other hand, for the input objects that belong to other data classes, the same permutation camera design generated noise-like, low-intensity patterns that do not match the permuted images (<span>\({\varvec{P}}G\)</span>). In fact, by applying the inverse permutation (<span>\({{\varvec{P}}}^{-1}\)</span>) operation on the output images of the diffractive camera, the original digits of interest from the target data class can be faithfully reconstructed, whereas all the other classes of objects ended up in noise-like patterns (see the last column of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>a), which illustrates the success of this class-specific permutation camera.</p><h3 id="Sec7">Class-specific linear transformation camera design</h3><p>As a more general and even more challenging case of the class-specific permutation camera reported in the previous section, here we report the design of a class-specific linear transformation camera (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a>), which performs an arbitrarily selected invertible linear transformation at its output FOV for a desired class of objects, while all-optically erasing the other classes of objects, i.e., they cannot be retrieved even if the inverse linear transformation were to be applied at the output of the camera. To achieve this goal, we designed a seven-layer linear transformation diffractive camera, which takes MNIST handwritten digits as its input and performs an all-optical linear transformation only on the target data class (which was selected as the handwritten digit ‘2’). During its blind testing phase, the designed class-specific linear transformation camera was tested with 10,000 MNIST digits, never used in the training phase. After the linear transformation operation performed all-optically through the diffractive camera, the output images become uninterpretable, i.e., become encrypted (unless one has the “key”, i.e., the inverse transformation matrix). The corresponding inverse linear transformation, i.e., the key, can be subsequently applied to the transformed output images of the target class of objects to recover the original handwritten input digits, ‘2’. Similar to the class-specific permutation camera design (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>), the other handwritten digits are all-optically erased, with noise-like features appearing at the output FOV, which cannot be retrieved back even after the inverse linear transformation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a>). Stated differently, the all-optical linear transformation (i.e., the “lock” or the encryption) of this diffractive camera only operates on the objects of a specific data class (where the key would be able to bring the images of the objects back through an inverse linear transformation), whereas the rest of the objects from the other data classes are irreversibly lost/erased at the output FOV even if one has access to the correct key (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a>).</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Fig. 6"><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig6_HTML.png?as=webp"/><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="826"/></picture></a></div><p>Class-specific linear transformation camera. <b>a</b> Illustration of a seven-layer diffractive camera trained to perform a class-specific linear transformation (denoted as <span>\({\varvec{T}}\)</span>) with instantaneous all-optical erasure of the other classes of objects at its output FOV. This class-specific all-optical linear transformation operation performed by the diffractive camera results in uninterpretable patterns of the target objects at the output FOV, which cannot be decoded without the knowledge of the transformation matrix, <span>\({\varvec{T}}\)</span>, or its inverse. By applying the inverse linear transformation (<span>\({{\varvec{T}}}^{-1}\)</span>) on the output images of the diffractive camera, the original images of interest from the target data class can be faithfully reconstructed. On the other hand, the input objects from the other data classes end up in noise-like patterns both before and after applying the inverse linear transformation, demonstrating the success of this class-specific linear transformation camera design. The output images were normalized using the same constant for visualization. <b>b</b> Phase modulation patterns of the converged diffractive layers of the class-specific linear transformation camera</p></div></figure></div><h3 id="Sec8">Experimental demonstration of a class-specific diffractive camera</h3><p>We experimentally demonstrated the proof of concept of a class-specific diffractive camera by fabricating and assembling the diffractive layers using a 3D printer and testing it with a continuous wave source at <span>\(\lambda =\)</span> 0.75 mm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>a). For these experiments, we trained a three-layer diffractive camera design using the same configuration as the system reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>, with the following changes: (1) the diffractive camera was “vaccinated” during its training phase against potential experimental misalignments [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="D. Mengu et al., Misalignment resilient diffractive optical networks. Nanophotonics 9, 4207–4219 (2020)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR41" id="ref-link-section-d61071694e1969">41</a>], by introducing random displacements to the diffractive layers during the iterative training and optimization process (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>b, see the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Sec10">Methods</a>” section for details); (2) the handwritten MNIST objects were down-sampled to 15 <span>\(\times\)</span> 15 pixels to form the 3D-fabricated input objects; (3) an additional image contrast-related penalty term was added to the training loss function to enhance the contrast of the output images from the target data class, which further improved the signal-to-noise ratio of the diffractive camera design. The resulting diffractive layers, including the pictures of the 3D-printed camera, are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>b, c.</p><div data-test="figure" data-container-section="figure" id="figure-7" data-title="Fig. 7"><figure><figcaption><b id="Fig7" data-test="figure-caption-text">Fig. 7</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig7_HTML.png?as=webp"/><img aria-describedby="Fig7" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="692"/></picture></a></div><p>Experimental setup for object class-specific imaging using a diffractive camera. <b>a</b> Schematic of the experimental setup using THz illumination. <b>b</b> Schematic of the misalignment resilient training of the diffractive camera and the converged phase patterns. <b>c</b> Photographs of the 3D printed and assembled diffractive system</p></div></figure></div><p>To blindly test the 3D-assembled diffractive camera (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>c), 12 different MNIST handwritten digits, including three digits from the target data class (digit ‘2’) and nine digits from the other data classes were used as the input test objects of the diffractive camera. The output FOV of the diffractive camera (36 <span>\(\times\)</span> 36 mm<sup>2</sup>) was scanned using a THz detector forming the output images. The experimental imaging results of our 3D-printed diffractive camera are demonstrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig8">8</a>, together with the input test objects and the corresponding numerical simulation results for each input object. The experimental results show a high degree of agreement with the numerically expected results based on the optical forward model of our diffractive camera, and we observe that the test objects from the target data class were imaged well, while the other non-target test objects were completely erased at the output FOV of the camera. The success of these proof-of-concept experimental results further confirms the feasibility of our class-specific diffractive camera design.</p><div data-test="figure" data-container-section="figure" id="figure-8" data-title="Fig. 8"><figure><figcaption><b id="Fig8" data-test="figure-caption-text">Fig. 8</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig8_HTML.png?as=webp"/><img aria-describedby="Fig8" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_Fig8_HTML.png" alt="figure 8" loading="lazy" width="685" height="665"/></picture></a></div><p>Experimental results of object class-specific imaging using a 3D-printed diffractive camera</p></div></figure></div></div></div></section><section data-title="Discussion"><div id="Sec9-section"><h2 id="Sec9">Discussion</h2><div id="Sec9-content"><p>We reported a diffractive camera design that performs class-specific imaging of target objects while instantaneously erasing other objects all-optically, which might inspire energy-efficient, task-specific and secure solutions to privacy-preserving imaging. Unlike conventional privacy-preserving imaging methods that rely on post-processing of images after their digitization, our diffractive camera design enforces privacy protection by selectively erasing the information of the non-target objects during the light propagation, which reduces the risk of recording sensitive raw image data.</p><p>To make this diffractive camera design even more resilient against potential adversarial attacks, one can monitor the illumination intensity as well as the output signal intensity and accordingly trigger the camera recording only when the output signal intensity is above a certain threshold. Based on the intensity separation that is created by the class-specific imaging performance of our diffractive camera, an intensity threshold can be determined at the output image sensor to trigger image capture only when a sufficient number of photons are received, which would eliminate the recording of any digital signature corresponding to non-target objects at the input FOV. Such an intensity threshold-based recording for class-specific imaging also eliminates unnecessary storage and transmission of image data by only digitizing the target information of interest from the desired data classes.</p><p>In addition to securing the information of the undesired objects by all-optically erasing them at the output FOV, the class-specific permutation and class-specific linear transformation camera designs reported in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a> can further perform all-optical image encryption for the desired classes of objects, providing an additional layer of data security. Through the data-driven training process, the class-specific permutation camera learns to apply a randomly selected permutation operation on the target class of input objects, which can only be inverted with the knowledge of the inverse permutation operation; this class-specific permutation camera can be used to further secure the confidentiality of the images of the target data class.</p><p>Compared to the traditional digital processing-based methods, the presented diffractive camera design has the advantages of speed and resource savings since the entire non-target object erasure process is performed as the input light diffracts through a thin camera volume at the speed of light. The functionality of this diffractive camera can be enabled on demand by turning on the coherent illumination source, without the need for any additional digital computing units or an external power supply, which makes it especially beneficial for power-limited and continuously working remote systems.</p><p>It is important to emphasize that the presented diffractive camera system does not possess a traditional, spatially-invariant point-spread function. A trained diffractive camera system performs a learned, complex-valued linear transformation between the input and output fields that statistically represents the coherent imaging of the input objects from the target data class. Through the data-driven training process using examples of the input objects, this complex-valued linear transformation performed by the diffractive camera converged into an optical mode filter that, by and large, preserves the phase and amplitude distributions of the propagating modes that characteristically represent the objects of the target data class. Because of the additional penalty terms that are used to all-optically erase the undesired data classes, the same complex-valued linear transformation also acts as a modal filter, scattering out the characteristic modes that statistically represent the other types of objects that do not belong to the target data class. Therefore, each class-specific diffractive camera design results from this data-driven learning process through training examples, optimized via error backpropagation and deep learning.</p><p>Also, note that the experimental proof of concept for our diffractive camera was demonstrated using a spatially-coherent and monochromatic THz illumination source, whereas the most commonly used imaging systems in the modern digital world are designed for visible and near-infrared wavelengths, using broadband and incoherent (or partially-coherent) light. With the recent advancements in state-of-the-art nanofabrication techniques such as electron-beam lithography [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="C. Vieu et al., Electron beam lithography: resolution limits and applications. Appl. Surf. Sci. 164, 111–117 (2000)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR42" id="ref-link-section-d61071694e2104">42</a>] and two-photon polymerization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="X. Zhou, Y. Hou, J. Lin, A review on the processing accuracy of two-photon polymerization. AIP Adv. 5, 030701 (2015)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR43" id="ref-link-section-d61071694e2107">43</a>], diffractive camera designs can be scaled down to micro-scale, in proportion to the illumination wavelength in the visible spectrum, without altering their design and functionality. Furthermore, it has been demonstrated that diffractive systems can be optimized using deep learning methods to all-optically process broadband signals [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Y. Luo et al., Design of task-specific optical systems using broadband diffractive neural networks. Light Sci. Appl. 8, 112 (2019)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR44" id="ref-link-section-d61071694e2110">44</a>]. Therefore, nano-fabricated, compact diffractive cameras that can work in the visible and IR parts of the spectrum using partially-coherent broadband radiation from e.g., light-emitting diodes (LEDs) or an array of laser diodes would be feasible in the near future.</p></div></div></section><section data-title="Methods"><div id="Sec10-section"><h2 id="Sec10">Methods</h2><div id="Sec10-content"><h3 id="Sec11">Forward-propagation model of a diffractive camera</h3><p>For a diffractive camera with <i>N</i> diffractive layers, the forward propagation of the optical field can be modeled as a sequence of (1) free-space propagation between the <i>l</i>th and (<i>l</i> + 1)th layers (<span>\(l=0, 1, 2, \dots , N\)</span>), and (2) the modulation of the optical field by the <i>l</i>th diffractive layer (<span>\(l=1, 2, \dots , N)\)</span>, where the 0th layer denotes the input/object plane and the (<i>N</i> + 1)th layer denotes the output/image plane. The free-space propagation of the complex field is modeled following the angular spectrum approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="X. Lin et al., All-optical machine learning using diffractive deep neural networks. Science 361, 1004–1008 (2018)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR45" id="ref-link-section-d61071694e2222">45</a>]. The optical field <span>\({u}^{l}\left(x, y\right)\)</span> right after the <i>l</i>th layer after being propagated for a distance of <span>\(d\)</span> can be written as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="A. Ozcan, E. McLeod, Lensless imaging and sensing. Annu. Rev. Biomed. Eng. 18, 77–102 (2016)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR46" id="ref-link-section-d61071694e2283">46</a>]:</p><div id="Equ2"><p><span>$$\begin{array}{c}{\mathbb{P}}_{\mathbf{d}}{ u}^{l}\left(x,y\right)={\mathcal{F}}^{-1}\left\{\mathcal{F}\left\{{u}^{l}\left(x,y\right)\right\}H({f}_{x},{f}_{y};d)\right\}\end{array}$$</span></p><p>
                    (2)
                </p></div><p>where <span>\({\mathbb{P}}_{\mathbf{d}}\)</span> represents the free-space propagation operator, <span>\(\mathcal{F}\)</span> and <span>\({\mathcal{F}}^{-1}\)</span> are the two-dimensional Fourier transform and the inverse Fourier transform operations, and <span>\(H({f}_{x}, {f}_{y};d)\)</span> is the transfer function of free space:</p><div id="Equ3"><p><span>$${H\left( {{f_x},{f_y};d} \right) = \left\{ {\begin{array}{*{20}{l}}{{\rm{exp}}\left\{ {jkd\sqrt {1 - {{\left( {\frac{{2\pi {f_x}}}{k}} \right)}^2} - {{\left( {\frac{{2\pi {f_y}}}{k}} \right)}^2}} } \right\},}&amp;{f_x^2 + f_y^2 &lt; \frac{1}{{{\lambda ^2}}}}\\{0,}&amp;{f_x^2 + f_y^2 \ge \frac{1}{{{\lambda ^2}}}}\end{array}} \right.}$$</span></p><p>
                    (3)
                </p></div><p>where <span>\(j=\sqrt{-1}\)</span>, <span>\(k= \frac{2\pi }{\lambda }\)</span> and <span>\(\lambda\)</span> is the wavelength of the illumination light. <span>\({f}_{x}\)</span> and <span>\({f}_{y}\)</span> are the spatial frequencies along the <span>\(x\)</span> and <span>\(y\)</span> directions, respectively.</p><p>We consider only the phase modulation of the transmitted field at each layer, where the transmittance coefficient <span>\({t}^{l}\)</span> of the <i>l</i>th diffractive layer can be written as:</p><div id="Equ4"><p><span>$$\begin{array}{c}{t}^{l}\left(x,y\right)=exp\left\{j{\phi }^{l}\left(x,y\right)\right\}\end{array}$$</span></p><p>
                    (4)
                </p></div><p>where <span>\({\phi }^{l}\left(x,y\right)\)</span> denotes the phase modulation of the trainable diffractive neuron located at <span>\(\left(x,y\right)\)</span> position of the <i>l</i>th diffractive layer. Based on these definitions, the complex optical field at the output plane of a diffractive camera can be expressed as:</p><div id="Equ5"><p><span>$$\begin{array}{c}o\left(x,y\right)={\mathbb{P}}_{{\mathbf{d}}_{{\varvec{N}},{\varvec{N}}+1}}\left(\prod_{l=N}^{1}{t}^{l}\left(x,y\right)\cdot {\mathbb{P}}_{{\mathbf{d}}_{{\varvec{l}}-1,\boldsymbol{ }\boldsymbol{ }{\varvec{l}}}}\right)g(x,y)\end{array}$$</span></p><p>
                    (5)
                </p></div><p>where <span>\({d}_{l-1,l}\)</span> represents the axial distance between the (<i>l</i> − 1)th and the <i>l</i>th layers, <span>\(g\left(x,y\right)\)</span> is the input optical field, which is the amplitude of the input objects (handwritten digits) used in this work.</p><h3 id="Sec12">Training loss function</h3><p>The reported diffractive camera systems were optimized by minimizing the loss functions that were calculated using the intensities of the input and output images. The input and output intensities <span>\(G\)</span> and <span>\(O\)</span>, respectively, can be written as:</p><div id="Equ6"><p><span>$$\begin{array}{c}G\left(x,y\right)={\left|g\left(x,y\right)\right|}^{2}\end{array}$$</span></p><p>
                    (6)
                </p></div><div id="Equ7"><p><span>$$\begin{array}{c}O\left(x,y\right)={\left|o\left(x,y\right)\right|}^{2}\end{array}$$</span></p><p>
                    (7)
                </p></div><p>The loss function, calculated using a batch of training input objects <span>\({\varvec{G}}\)</span> with the corresponding output images <span>\({\varvec{O}}\)</span> can be defined as:</p><div id="Equ8"><p><span>$$\begin{array}{c}Loss\left({\varvec{O}},{\varvec{G}}\right)=Los{s}_{+}\left({{\varvec{O}}}^{+}, {{\varvec{G}}}^{+}\right)+ Los{s}_{-}\left({{\varvec{O}}}^{-},\boldsymbol{ }{{\varvec{G}}}^{-},{G}_{k}^{+}\right)\end{array}$$</span></p><p>
                    (8)
                </p></div><p>where <span>\({{\varvec{O}}}^{+},{{\varvec{G}}}^{+}\)</span> represent the output and input images from the target data class (i.e., desired object class), and <span>\({{\varvec{O}}}^{-},{{\varvec{G}}}^{-}\)</span> represent the output and input images from the other data classes (to be all-optically erased), respectively.</p><p>The <span>\(Los{s}_{+}\)</span> is designed to reduce the NMSE and enhance the correlation between any target class input object <span>\({O}^{+}\)</span> and its output image <span>\({G}^{+}\)</span>, so that the diffractive camera learns to faithfully reconstruct the objects from the target data class, i.e.,</p><div id="Equ9"><p><span>$$\begin{array}{c}Los{s}_{+}\left({O}^{+},{G}^{+}\right)={\alpha }_{1}\times NMSE\left({O}^{+}, { G}^{+}\right)+ {\alpha }_{2}\times \left(1-\mathrm{PCC}\left({O}^{+}, {G}^{+}\right)\right)\end{array}$$</span></p><p>
                    (9)
                </p></div><p>where <span>\({\alpha }_{1}\)</span> and <span>\({\alpha }_{2}\)</span> are constants and NMSE is defined as:</p><div id="Equ10"><p><span>$$\begin{array}{c}NMSE\left({O}^{+},{G}^{+}\right)=\frac{1}{MN}\sum_{m,n}{\left(\frac{{O}_{m,n}^{+}}{\mathrm{max}({O}^{+})}-{G}_{m,n}^{+}\right)}^{2}\end{array}$$</span></p><p>
                    (10)
                </p></div><p><span>\(m\)</span> and <span>\(n\)</span> are the pixel indices of the images, and <span>\(MN\)</span> represents the total number of pixels in each image. The output image <span>\({O}^{+}\)</span> was normalized by its maximum pixel value, <span>\(\mathrm{max}({O}^{+})\)</span>. The PCC value between any two images <span>\(A\)</span> and <span>\(B\)</span> is calculated using [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="J. Benesty, J. Chen, Y. Huang, I. Cohen, Pearson correlation coefficient. in Noise reduction in speech processing (Springer Berlin Heidelberg, 2009)." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR38" id="ref-link-section-d61071694e4481">38</a>]:</p><div id="Equ11"><p><span>$$\begin{array}{c}PCC(A,B)=\frac{\sum \left(A-\overline{A }\right)\left(B-\overline{B }\right)}{\sqrt{\sum {\left(A-\overline{A }\right)}^{2}\sum {\left(B-\overline{B }\right)}^{2}}}\end{array}$$</span></p><p>
                    (11)
                </p></div><p>The term <span>\(\left(1-\mathrm{PCC}\left({O}^{+}, {G}^{+}\right)\right)\)</span> was used in <span>\(Los{s}_{+}\)</span> in order to maximize the correlation between <span>\({O}^{+}\)</span> and <span>\({G}^{+}\)</span>, as well as to ensure a non-negative loss value since the PCC value of any two images is always between − 1 and 1.</p><p>Different from<span>\(Los{s}_{+}\)</span>, the <span>\(Los{s}_{-}\)</span> function is designed to <i>reduce</i> (1) the <i>absolute</i> correlation between the output <span>\({O}^{-}\)</span> and its corresponding input <span>\({G}^{-}\)</span>, (2) the <i>absolute</i> correlation between <span>\({O}^{-}\)</span> and an arbitrary object <span>\({G}_{k}^{+}\)</span> from the target class, and (3) the correlation between <span>\({O}^{-}\)</span> and itself shifted by a few pixels <span>\({O}_{\mathrm{sft}}^{-}\)</span>, which can be formulated as:</p><div id="Equ12"><p><span>$$\begin{array}{c}Los{s}_{-}\left({O}^{-},{G}^{-},{G}_{k}^{+}\right)={\beta }_{1}\times \left|\mathrm{PCC}\left({O}^{-}, {G}^{-}\right)\right|+{\beta }_{2}\times \left|\mathrm{PCC}\left({O}^{-}, {G}_{k}^{+}\right)\right|+ {\beta }_{3}\times PCC\left({O}^{-}, {O}_{\mathrm{sft}}^{-}\right)\end{array}$$</span></p><p>
                    (12)
                </p></div><p>where <span>\({\beta }_{1}\)</span>, <span>\({\beta }_{2}\)</span> and <span>\({\beta }_{3}\)</span> are constants. Here the <span>\({G}_{k}^{+}\)</span> refers to an image of an object from the target data class in the training set, which was randomly selected for every training batch, and the subscript <span>\(k\)</span> refers to a random index. In other words, within each training batch, the <span>\(\mathrm{PCC}\left({O}^{-}, {G}_{k}^{+}\right)\)</span> was calculated using the output image from the non-target data class and a random ground truth image from the target class. By adding such a loss term, we prevent the diffractive camera from converging to a solution where all the output images look like the target object. The <span>\({O}_{\mathrm{sft}}^{-}\)</span> was obtained using:</p><div id="Equ13"><p><span>$$\begin{array}{c}{O}_{\mathrm{sft}}^{-} \left(x,y\right)={O}^{-}\left(x-{s}_{x},y-{s}_{y}\right)\end{array}$$</span></p><p>
                    (13)
                </p></div><p>where <span>\({s}_{x}={s}_{y}=5\)</span> denote the number of pixels that <span>\({O}^{-}\)</span> is shifted in each direction. Intuitively, a natural image will maintain a high correlation with itself, shifted by a small amount, while an image of random noise will not. By minimizing <span>\(\mathrm{PCC}\left({O}^{-}, {O}_{\mathrm{sft}}^{-}\right)\)</span>, we forced the diffractive camera to generate uninterpretable noise-like output patterns for input objects that do not belong to the target data class.</p><p>The coefficients <span>\(\left({\alpha }_{1}, {\alpha }_{2}, {\beta }_{1},{\beta }_{2},{\beta }_{3}\right)\)</span> in the two loss functions were empirically set to (1, 3, 6, 3, 2).</p><h3 id="Sec13">Digital implementation and training scheme</h3><p>The diffractive camera models reported in this work were trained with the standard MNIST handwritten digit dataset under <span>\(\lambda =0.75\; \mathrm{mm}\)</span> illumination. Each diffractive layer has a pixel/neuron size of 0.4 mm, which only modulates the phase of the transmitted optical field. The axial distance between the input plane and the first diffractive layer, the distances between any two successive diffractive layers, and the distance between the last diffractive layer and the output plane are set to 20 mm, i.e., <span>\({d}_{l-1,l}=20\;\mathrm{ mm }\,(l=1,2,\dots , N+1)\)</span>. For the diffractive camera models that take a single MNIST image as its input (e.g., reported in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>), each diffractive layer contains 120 <span>\(\times\)</span> 120 diffractive pixels. During the training, each 28 <span>\(\times\)</span> 28 MNIST raw image was first linearly upscaled to 90 <span>\(\times\)</span> 90 pixels. Next, the upscaled training dataset was augmented with random image transformations, including a random rotation by an angle within <span>\([-10^\circ , +10^\circ ]\)</span>, a random scaling by a factor within [0.9, 1.1], and a random shift in each lateral direction by an amount of <span>\([-2.13\lambda , +2.13\lambda ]\)</span>.</p><p>For the diffractive camera model reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig4">4</a> that takes multiplexed objects as its input, each diffractive layer contains 300 <span>\(\times\)</span> 300 diffractive pixels. The MNIST training digits were first upscaled to 90 <span>\(\times\)</span> 90 pixels and then randomly transformed with <span>\([-10^\circ , +10^\circ ]\)</span> angular rotation, [0.9, 1.1] scaling, and <span>\([-2.13\lambda , +2.13\lambda ]\)</span> translation. Nine different handwritten digits were randomly selected and arranged into 3 <span>\(\times\)</span> 3 grids, generating a multiplexed input image with 270 <span>\(\times\)</span> 270 pixels for the diffractive camera training.</p><p>For the diffractive permutation camera reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig5">5</a>, each diffractive layer contains 120 <span>\(\times\)</span> 120 diffractive pixels. The design parameters of this class-specific permutation camera were kept the same as the five-layer diffractive camera reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>a, except that the handwritten digits were down-sampled to 15 <span>\(\times\)</span> 15 pixels considering that the required computational training resources for the permutation operation increase quadratically with the total number of input image pixels. The MNIST training digits were augmented using the same random transformations as described above. The 2D permutation matrix <span>\({\varvec{P}}\)</span> was generated by randomly shuffling the rows of a 225 <span>\(\times\)</span> 225 identity matrix. The inverse of <span>\({\varvec{P}}\)</span> was obtained by using the transpose operation, i.e., <span>\({{\varvec{P}}}^{-1}={{\varvec{P}}}^{{\varvec{T}}}\)</span>. The training loss terms for the class-specific permutation camera remained the same as described in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ8">8</a>), (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ9">9</a>), and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ12">12</a>), except that the permuted input images (<span>\({\varvec{P}}G\)</span>) were used as the ground truth, i.e.,</p><div id="Equ14"><p><span>$$\begin{array}{c}Los{s}_{\mathrm{Permutation}}\left(O,{\varvec{P}}G\right)\,=\,Los{s}_{+}\left({O}^{+}, {{\varvec{P}}G}^{+}\right)+ Los{s}_{-}\left({O}^{-},\boldsymbol{ }{{\varvec{P}}G}^{-},{{\varvec{P}}G}_{k}^{+}\right)\end{array}$$</span></p><p>
                    (14)
                </p></div><p>For the seven-layer diffractive linear transformation camera reported in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a>, each diffractive layer contains 300 <span>\(\times\)</span> 300 diffractive neurons, and the axial distance between any two consecutive planes was set to 45 mm (i.e., <span>\({d}_{l-1,l}=20\)</span> mm, for <span>\(l=1, 2, \dots , N+1)\)</span>. The 2D linear transformation matrix <span>\({\varvec{T}}\)</span> was generated by randomly creating an invertible matrix with each row having 20 non-zero random entries, and normalized so that the summation of each row is 1 (for conserving energy); see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig6">6</a> for the selected <span>\({\varvec{T}}\)</span>. The invertibility of <span>\({\varvec{T}}\)</span> was validated by calculating its determinant. During the training, the loss functions were applied to the diffractive camera output and the ground truth after the inverse linear transformation, i.e., <span>\({{\varvec{T}}}^{-1}O\)</span> and <span>\({{\varvec{T}}}^{-1}({\varvec{T}}G)\)</span>. The other details of the training loss terms for the class-specific linear transformation camera remained the same as described in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ8">8</a>), (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ9">9</a>), and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ12">12</a>).</p><p>The diffractive camera trained with the Fashion MNIST dataset (reported in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM1">1</a>: Fig. S2) contains seven diffractive layers, each with 300 <span>\(\times\)</span> 300 pixels/neurons. The axial distance between any two consecutive planes was set to 45 mm (i.e., <span>\({d}_{l-1,l}=20\)</span> mm, for <span>\(l=\mathrm{1,2},\dots , N+1)\)</span>. During the training, each Fashion MNIST raw image was linearly upsampled to 90 <span>\(\times\)</span> 90 pixels and then augmented with random transformations of <span>\([-10^\circ , +10^\circ ]\)</span> angular rotation, [0.9, 1.1] physical scaling, and <span>\([-2.13\lambda , +2.13\lambda ]\)</span> lateral translation. The loss functions used for training remained the same as described in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ8">8</a>), (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ9">9</a>), and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ12">12</a>).</p><p>The spatial displacement-agnostic diffractive camera design with the larger input FOV (reported in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM4">4</a>: Movie S3) contains seven diffractive layers, each with 300 <span>\(\times\)</span> 300 pixels/neurons. The axial distance between any two consecutive planes was set to 45 mm (i.e., <span>\({d}_{l-1,l}=20\)</span> mm, for <span>\(l=\mathrm{1,2},\dots , N+1)\)</span>. During the training, each MNIST raw image was linearly upsampled to 90 × 90 pixels, and then was randomly placed within a larger input FOV of 140 × 140 pixels for training. The loss functions were the same as described in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ8">8</a>), (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ9">9</a>), and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Equ12">12</a>). The input objects distributed within a FOV of 120 × 120 pixels were demonstrated during the blind testing shown in Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#MOESM4">4</a>: Movie S3.</p><p>The MNIST handwritten digit dataset was divided into training, validation, and testing datasets without any overlap, with each set containing 48,000, 12,000, and 10,000 images, respectively. For the diffractive camera trained with the Fashion MNIST dataset, five different classes (i.e., trousers, dresses, sandals, sneakers, and bags) were selected for the training, validation, and testing, with each set containing 24,000, 6000, and 5000 images without overlap, respectively.</p><p>The diffractive camera models reported in this paper were trained using the Adam optimizer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="D. P. Kingma, J. Ba, Adam: a method for stochastic optimization. 
                  arXiv:1412.6980
                  
                 [cs] (2017)." href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR47" id="ref-link-section-d61071694e7116">47</a>] with a learning rate of 0.03. The batch size used for all the trainings was 60. All models were trained and tested using PyTorch 1.11 with a GeForce RTX 3090 graphical processing unit (NVIDIA Inc.). The typical training time for a three-layer diffractive camera (e.g., in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig2">2</a>) is ~ 21 h for 1000 epochs.</p><h3 id="Sec14">Experimental design</h3><p>For the experimentally validated diffractive camera design shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>, an additional contrast loss <span>\({\mathrm{L}}_{\mathrm{c}}\)</span> was added to <span>\(Los{s}_{+}\)</span> i.e.,</p><div id="Equ15"><p><span>$$\begin{array}{c}Los{s}_{+}\left({O}^{+},{G}^{+}\right)\,=\,{\alpha }_{1}\times NMSE\left({O}^{+}, { G}^{+}\right)+ {\alpha }_{2}\times \left(1-\mathrm{PCC}\left({O}^{+}, {G}^{+}\right)\right)+{\alpha }_{3}\times {\mathrm{L}}_{\mathrm{c}}\left({O}^{+}, {G}^{+}\right)\end{array}$$</span></p><p>
                    (15)
                </p></div><p>The coefficients <span>\(\left({\alpha }_{1}, {\alpha }_{2}, {\alpha }_{3}\right)\)</span> were empirically set to (1, 3, 5) and <span>\({\mathrm{L}}_{\mathrm{c}}\)</span> is defined as:</p><div id="Equ16"><p><span>$$\begin{array}{c}{\mathrm{L}}_{\mathrm{c}}\left({O}^{+}, {G}^{+}\right)=\frac{\sum \left({O}^{+}\cdot \left(1-\widehat{{G}^{+}}\right)\right)}{\sum \left({O}^{+}\cdot \widehat{{G}^{+}}\right)+\varepsilon }\end{array}$$</span></p><p>
                    (16)
                </p></div><p>where <span>\(\varepsilon =1{\mathrm{e}}^{-6}\)</span> was added to the denominator to avoid divide-by-zero error. <span>\(\widehat{{G}^{+}}\)</span> is a binary mask indicating the transmissive regions of the input object <span>\({G}^{+}\)</span>, which is defined as:</p><div id="Equ17"><p><span>$${\widehat {{G^ + }}\left( {m,n} \right) = \left\{ {\begin{array}{*{20}{l}}{1,}&amp;{{G^ + }(m,n) &gt; 0.5}\\{0,}&amp;{otherwise}\end{array}} \right.}$$</span></p><p>
                    (17)
                </p></div><p>By adding this image contrast related training loss term, the output images of the target objects exhibit enhanced contrast which is especially helpful in non-ideal experimental conditions.</p><p>In addition, the MNIST training images were first linearly downsampled to 15 × 15 pixels and then upscaled to 90 × 90 pixels using nearest-neighbor interpolation. Then, the resulting input objects were augmented using the same parameters as described before and were fed into the diffractive camera for training. Each diffractive layer had 120 × 120 trainable diffractive neurons.</p><p>To overcome the challenges posed by the fabrication inaccuracies and mechanical misalignments during the experimental validation of the diffractive camera, we vaccinated our diffractive model during the training by deliberately introducing random displacements to the diffractive layers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="D. Mengu et al., Misalignment resilient diffractive optical networks. Nanophotonics 9, 4207–4219 (2020)" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#ref-CR41" id="ref-link-section-d61071694e7858">41</a>]. During the training process, a 3D displacement <span>\({\varvec{D}}= \left({D}_{x},{ D}_{y},{ D}_{z}\right)\)</span> was randomly added to each diffractive layer following the uniform <span>\({\text{(U)}}\)</span> random distribution:</p><div id="Equ18"><p><span>$$\begin{array}{c}{D}_{x} \sim {\text{U}}\left(-{\Delta }_{x, tr}, {\Delta }_{x, tr}\right)\end{array}$$</span></p><p>
                    (18)
                </p></div><div id="Equ19"><p><span>$$\begin{array}{c}{D}_{y} \sim {\text{U}}\left(-{\Delta }_{y, tr}, {\Delta }_{y,tr}\right)\end{array}$$</span></p><p>
                    (19)
                </p></div><div id="Equ20"><p><span>$$\begin{array}{c}{D}_{z} \sim {\text{U}}\left(-{\Delta }_{z, tr}, {\Delta }_{z,tr}\right)\end{array}$$</span></p><p>
                    (20)
                </p></div><p>where <span>\({D}_{x}\)</span> and <span>\({D}_{y}\)</span> denote the random lateral displacement of a diffractive layer in <span>\(x\)</span> and <span>\(y\)</span> directions, respectively. <span>\({D}_{z}\)</span> denotes the random displacement added to the axial distances between any two consecutive diffractive layers. <span>\({\Delta }_{*, tr}\)</span> represents the maximum amount of shift allowed along the corresponding axis, which was set as <span>\({\Delta }_{x,tr}={\Delta }_{y,tr}=\)</span> 0.4 mm (~ 0.53<span>\(\lambda\)</span>), and <span>\({\Delta }_{z, tr}=\)</span> 1.5 mm (2<span>\(\lambda\)</span>) throughout the training process. <span>\({D}_{x},{ D}_{y}\)</span>, and <span>\({D}_{z}\)</span> of each diffractive layer were independently sampled from the given uniform random distributions. The diffractive camera model used for the experimental validation was trained for 50 epochs.</p><h3 id="Sec15">Experimental THz imaging setup</h3><p>We validated the fabricated diffractive camera design using a THz continuous wave scanning system. The phase values of the diffractive layers were first converted into height maps using the refractive index of the 3D printer material. Then, the layers were printed using a 3D printer (Pr 110, CADworks3D). A layer holder that sets the positions of the input plane, output plane, and each diffractive layer was also 3D printed (Objet30 Pro, Stratasys) and assembled with the printed layers. The test objects were 3D printed (Objet30 Pro, Stratasys) and coated with aluminum foil to define the transmission areas.</p><p>The experimental setup is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig7">7</a>a. The THz source used in the experiment was a WR2.2 modular amplifier/multiplier chain (AMC) with a compatible diagonal horn antenna (Virginia Diode Inc.). The input of AMC was a 10 dBm RF input signal at 11.1111 GHz (<i>f</i><sub><i>RF1</i></sub>) and after being multiplied 36 times, the output radiation was at 0.4 THz. The AMC was also modulated with a 1 kHz square wave for lock-in detection. The output plane of the diffractive camera was scanned with a 1 mm step size using a single-pixel Mixer/AMC (Virginia Diode Inc.) detector mounted on an XY positioning stage that was built by combining two linear motorized stages (Thorlabs NRT100). A 10 dBm RF signal at 11.083 GHz (<i>f</i><sub><i>RF2</i></sub>) was sent to the detector as a local oscillator to down-convert the signal to 1 GHz. The down-converted signal was amplified by a low-noise amplifier (Mini-Circuits ZRL-1150-LN+) and filtered by a 1 GHz (± 10 MHz) bandpass filter (KL Electronics 3C40-1000/T10-O/O). Then the signal passed through a tunable attenuator (HP 8495B) for linear calibration and a low-noise power detector (Mini-Circuits ZX47-60) for absolute power detection. The detector output was measured by a lock-in amplifier (Stanford Research SR830) with the 1 kHz square wave used as the reference signal. Then the lock-in amplifier readings were calibrated into linear scale. A digital 2 × 2 binning was applied to each measurement of the intensity field to match the training feature size used in the design phase.</p></div></div></section>
                    <section data-title="Availability of data and materials"><div id="availability-of-data-and-materials-section"><h2 id="availability-of-data-and-materials">Availability of data and materials</h2><p>All the data and methods that support this work are present in the main text and the Additional files. The deep learning models in this work employ standard libraries and scripts that are publicly available in PyTorch. The MNIST handwritten digits database is available online at: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol><li data-counter="1."><p id="ref-CR1">J. Scharcanski, Bringing vision-based measurements into our daily life: a grand challenge for computer vision systems. Front. ICT <b>3</b>, 3 (2016)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.3389%2Ffict.2016.00003" aria-label="Article reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Bringing%20vision-based%20measurements%20into%20our%20daily%20life%3A%20a%20grand%20challenge%20for%20computer%20vision%20systems&amp;journal=Front.%20ICT&amp;volume=3&amp;publication_year=2016&amp;author=Scharcanski%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">X. Feng, Y. Jiang, X. Yang, M. Du, X. Li, Computer vision algorithms and hardware implementations: a survey. Integration <b>69</b>, 309–320 (2019)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.vlsi.2019.07.005" aria-label="Article reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20vision%20algorithms%20and%20hardware%20implementations%3A%20a%20survey&amp;journal=Integration&amp;volume=69&amp;pages=309-320&amp;publication_year=2019&amp;author=Feng%2CX&amp;author=Jiang%2CY&amp;author=Yang%2CX&amp;author=Du%2CM&amp;author=Li%2CX">
                    Google Scholar</a> 
                </p></li><li data-counter="3."><p id="ref-CR3">M. Al-Faris, J. Chiverton, D. Ndzi, A.I. Ahmed, A review on computer vision-based methods for human action recognition. J. Imaging <b>6</b>, 46 (2020)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.3390%2Fjimaging6060046" aria-label="Article reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20on%20computer%20vision-based%20methods%20for%20human%20action%20recognition&amp;journal=J.%20Imaging&amp;volume=6&amp;publication_year=2020&amp;author=Al-Faris%2CM&amp;author=Chiverton%2CJ&amp;author=Ndzi%2CD&amp;author=Ahmed%2CAI">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">X. Wang, Intelligent multi-camera video surveillance: a review. Pattern Recogn. Lett. <b>34</b>, 3–19 (2013)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2013PaReL..34....3W" aria-label="ADS reference 4">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.patrec.2012.07.005" aria-label="Article reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Intelligent%20multi-camera%20video%20surveillance%3A%20a%20review&amp;journal=Pattern%20Recogn.%20Lett.&amp;volume=34&amp;pages=3-19&amp;publication_year=2013&amp;author=Wang%2CX">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">N. Haering, P.L. Venetianer, A. Lipton, The evolution of video surveillance: an overview. Mach. Vis. Appl. <b>19</b>, 279–290 (2008)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs00138-008-0152-0" aria-label="Article reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20evolution%20of%20video%20surveillance%3A%20an%20overview&amp;journal=Mach.%20Vis.%20Appl.&amp;volume=19&amp;pages=279-290&amp;publication_year=2008&amp;author=Haering%2CN&amp;author=Venetianer%2CPL&amp;author=Lipton%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="6."><p id="ref-CR6">E. D. Dickmannsin, The development of machine vision for road vehicles in the last decade. in <i>IEEE Intelligent Vehicle Symposium, 2002</i>, vol. 1 (2002), p. 268–281.</p></li><li data-counter="7."><p id="ref-CR7">J. Janai, F. Güney, A. Behl, A. Geiger, Computer vision for autonomous vehicles: problems, datasets and state of the art. CGV <b>12</b>, 1–308 (2020)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20vision%20for%20autonomous%20vehicles%3A%20problems%2C%20datasets%20and%20state%20of%20the%20art&amp;journal=CGV&amp;volume=12&amp;pages=1-308&amp;publication_year=2020&amp;author=Janai%2CJ&amp;author=G%C3%BCney%2CF&amp;author=Behl%2CA&amp;author=Geiger%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">A. Esteva et al., Deep learning-enabled medical computer vision. NPJ Digit. Med. <b>4</b>, 1–9 (2021)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41746-020-00373-5" aria-label="Article reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning-enabled%20medical%20computer%20vision&amp;journal=NPJ%20Digit.%20Med.&amp;volume=4&amp;pages=1-9&amp;publication_year=2021&amp;author=Esteva%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="9."><p id="ref-CR9">M. Tistarelli, M. Bicego, E. Grosso, Dynamic face recognition: from human to machine vision. Image Vis. Comput. <b>27</b>, 222–232 (2009)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.imavis.2007.05.006" aria-label="Article reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20face%20recognition%3A%20from%20human%20to%20machine%20vision&amp;journal=Image%20Vis.%20Comput.&amp;volume=27&amp;pages=222-232&amp;publication_year=2009&amp;author=Tistarelli%2CM&amp;author=Bicego%2CM&amp;author=Grosso%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="10."><p id="ref-CR10">T.B. Moeslund, E. Granum, A survey of computer vision-based human motion capture. Comput. Vis. Image Underst. <b>81</b>, 231–268 (2001)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1006%2Fcviu.2000.0897" aria-label="Article reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20computer%20vision-based%20human%20motion%20capture&amp;journal=Comput.%20Vis.%20Image%20Underst.&amp;volume=81&amp;pages=231-268&amp;publication_year=2001&amp;author=Moeslund%2CTB&amp;author=Granum%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">G. Singh, G. Bhardwaj, S.V. Singh, V. Garg, Biometric identification system: security and privacy concern, in <i>Artificial intelligence for a sustainable industry 4.0</i>. ed. by S. Awasthi, C.M. Travieso-González, G. Sanyal, D. Kumar Singh (Springer International Publishing, Berlin, 2021), pp. 245–264. <a href="https://doi.org/10.1007/978-3-030-77070-9_15">https://doi.org/10.1007/978-3-030-77070-9_15</a></p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2F978-3-030-77070-9_15" aria-label="Chapter reference 11">Chapter</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Biometric%20identification%20system%3A%20security%20and%20privacy%20concern&amp;pages=245-264&amp;publication_year=2021&amp;author=Singh%2CG&amp;author=Bhardwaj%2CG&amp;author=Singh%2CSV&amp;author=Garg%2CV">
                    Google Scholar</a> 
                </p></li><li data-counter="12."><p id="ref-CR12">A. Acquisti, L. Brandimarte, G. Loewenstein, Privacy and human behavior in the age of information. Science <b>347</b>, 509–514 (2015)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2015Sci...347..509A" aria-label="ADS reference 12">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.aaa1465" aria-label="Article reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Privacy%20and%20human%20behavior%20in%20the%20age%20of%20information&amp;journal=Science&amp;volume=347&amp;pages=509-514&amp;publication_year=2015&amp;author=Acquisti%2CA&amp;author=Brandimarte%2CL&amp;author=Loewenstein%2CG">
                    Google Scholar</a> 
                </p></li><li data-counter="13."><p id="ref-CR13">A. Acquisti, L. Brandimarte, J. Hancock, How privacy’s past may shape its future. Science <b>375</b>, 270–272 (2022)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2022Sci...375..270A" aria-label="ADS reference 13">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.abj0826" aria-label="Article reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20privacy%E2%80%99s%20past%20may%20shape%20its%20future&amp;journal=Science&amp;volume=375&amp;pages=270-272&amp;publication_year=2022&amp;author=Acquisti%2CA&amp;author=Brandimarte%2CL&amp;author=Hancock%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="14."><p id="ref-CR14">W.N. Price, I.G. Cohen, Privacy in the age of medical big data. Nat. Med. <b>25</b>, 37–43 (2019)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41591-018-0272-7" aria-label="Article reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Privacy%20in%20the%20age%20of%20medical%20big%20data&amp;journal=Nat.%20Med.&amp;volume=25&amp;pages=37-43&amp;publication_year=2019&amp;author=Price%2CWN&amp;author=Cohen%2CIG">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">J.R. Padilla-López, A.A. Chaaraoui, F. Flórez-Revuelta, Visual privacy protection methods: a survey. Expert Syst. Appl. <b>42</b>, 4177–4195 (2015)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.eswa.2015.01.041" aria-label="Article reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20privacy%20protection%20methods%3A%20a%20survey&amp;journal=Expert%20Syst.%20Appl.&amp;volume=42&amp;pages=4177-4195&amp;publication_year=2015&amp;author=Padilla-L%C3%B3pez%2CJR&amp;author=Chaaraoui%2CAA&amp;author=Fl%C3%B3rez-Revuelta%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="16."><p id="ref-CR16">C. Neustaedter, S. Greenberg, M. Boyle, Blur filtration fails to preserve privacy for home-based video conferencing. ACM Trans. Comput.-Hum. Interact. <b>13</b>, 1–36 (2006)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1145%2F1143518.1143519" aria-label="Article reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Blur%20filtration%20fails%20to%20preserve%20privacy%20for%20home-based%20video%20conferencing&amp;journal=ACM%20Trans.%20Comput.-Hum.%20Interact.&amp;volume=13&amp;pages=1-36&amp;publication_year=2006&amp;author=Neustaedter%2CC&amp;author=Greenberg%2CS&amp;author=Boyle%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="17."><p id="ref-CR17">A. Frome, et al., <i>Large-scale privacy protection in Google Street View</i>. in 2009 IEEE 12th International Conference on Computer Vision (2009), pp. 2373–2380. <a href="https://doi.org/10.1109/ICCV.2009.5459413">https://doi.org/10.1109/ICCV.2009.5459413</a>.</p></li><li data-counter="18."><p id="ref-CR18">F. Dufaux, T. Ebrahimi, Scrambling for privacy protection in video surveillance systems. IEEE Trans. Circuits Syst. Video Technol. <b>18</b>, 1168–1174 (2008)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1109%2FTCSVT.2008.928225" aria-label="Article reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Scrambling%20for%20privacy%20protection%20in%20video%20surveillance%20systems&amp;journal=IEEE%20Trans.%20Circuits%20Syst.%20Video%20Technol.&amp;volume=18&amp;pages=1168-1174&amp;publication_year=2008&amp;author=Dufaux%2CF&amp;author=Ebrahimi%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="19."><p id="ref-CR19">W. Zeng, S. Lei, Efficient frequency domain selective scrambling of digital video. IEEE Trans. Multimed. <b>5</b>, 118–129 (2003)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1109%2FTMM.2003.808817" aria-label="Article reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20frequency%20domain%20selective%20scrambling%20of%20digital%20video&amp;journal=IEEE%20Trans.%20Multimed.&amp;volume=5&amp;pages=118-129&amp;publication_year=2003&amp;author=Zeng%2CW&amp;author=Lei%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="20."><p id="ref-CR20">A. Criminisi, P. Perez, K. Toyama, Region filling and object removal by exemplar-based image inpainting. IEEE Trans. Image Process. <b>13</b>, 1200–1212 (2004)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2004ITIP...13.1200C" aria-label="ADS reference 20">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1109%2FTIP.2004.833105" aria-label="Article reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting&amp;journal=IEEE%20Trans.%20Image%20Process.&amp;volume=13&amp;pages=1200-1212&amp;publication_year=2004&amp;author=Criminisi%2CA&amp;author=Perez%2CP&amp;author=Toyama%2CK">
                    Google Scholar</a> 
                </p></li><li data-counter="21."><p id="ref-CR21">K. Inai, M. Pålsson, V. Frinken, Y. Feng, S. Uchida, <i>Selective concealment of characters for privacy protection</i>. in 2014 22nd International Conference on Pattern Recognition (2014), p. 333–338. <a href="https://doi.org/10.1109/ICPR.2014.66">https://doi.org/10.1109/ICPR.2014.66</a>.</p></li><li data-counter="22."><p id="ref-CR22">R. Uittenbogaard et al., <i>Privacy protection in street-view panoramas using depth and multi-view imagery</i>. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, 2019), pp. 10573–10582. <a href="https://doi.org/10.1109/CVPR.2019.01083">https://doi.org/10.1109/CVPR.2019.01083</a>.</p></li><li data-counter="23."><p id="ref-CR23">K. Brkic, I. Sikiric, T. Hrkac, Z. Kalafatic, I know that person: generative full body and face de-identification of people in images. in <i>2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</i> (2017), pp. 1319–1328. <a href="https://doi.org/10.1109/CVPRW.2017.173">https://doi.org/10.1109/CVPRW.2017.173</a>.</p></li><li data-counter="24."><p id="ref-CR24">F. Pittaluga, S. Koppal, A. Chakrabarti, Learning privacy preserving encodings through adversarial training. in <i>2019 IEEE Winter Conference on Applications of Computer Vision (WACV), </i>(IEEE, 2019), pp. 791–799. <a href="https://doi.org/10.1109/WACV.2019.00089">https://doi.org/10.1109/WACV.2019.00089</a>.</p></li><li data-counter="25."><p id="ref-CR25">A. Chattopadhyay, T. E. Boult, PrivacyCam: a privacy preserving camera using uCLinux on the Blackfin DSP. in <i>2007 IEEE Conference on Computer Vision and Pattern Recognition</i> (2007), pp. 1–8, <a href="https://doi.org/10.1109/CVPR.2007.383413">https://doi.org/10.1109/CVPR.2007.383413</a>.</p></li><li data-counter="26."><p id="ref-CR26">T. Winkler, B. Rinner, TrustCAM: security and privacy-protection for an embedded smart camera based on trusted computing. in <i>2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance</i> (2010), pp. 593–600, <a href="https://doi.org/10.1109/AVSS.2010.38">https://doi.org/10.1109/AVSS.2010.38</a>.</p></li><li data-counter="27."><p id="ref-CR27">Mrityunjay, P. J. Narayanan, The de-identification camera. in <i>2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics</i> (2011), pp. 192–195. <a href="https://doi.org/10.1109/NCVPRIPG.2011.48">https://doi.org/10.1109/NCVPRIPG.2011.48</a>.</p></li><li data-counter="28."><p id="ref-CR28">53 Important statistics about how much data is created every day. Financesonline.com. (2021). <a href="https://financesonline.com/how-much-data-is-created-every-day/">https://financesonline.com/how-much-data-is-created-every-day/</a>.</p></li><li data-counter="29."><p id="ref-CR29">P. Dhar, The carbon impact of artificial intelligence. Nature Machine Intelligence <b>2</b>, 423–425 (2020)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs42256-020-0219-9" aria-label="Article reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20carbon%20impact%20of%20artificial%20intelligence&amp;journal=Nature%20Machine%20Intelligence&amp;volume=2&amp;pages=423-425&amp;publication_year=2020&amp;author=Dhar%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="30."><p id="ref-CR30">S. Thakur, A. Chaurasia, Towards Green Cloud Computing: Impact of carbon footprint on environment. in <i>2016 6th International Conference—Cloud System and Big Data Engineering (Confluence), </i>(2016), pp. 209–213. <a href="https://doi.org/10.1109/CONFLUENCE.2016.7508115">https://doi.org/10.1109/CONFLUENCE.2016.7508115</a>.</p></li><li data-counter="31."><p id="ref-CR31">L. Belkhir, A. Elmeligi, Assessing ICT global emissions footprint: trends to 2040 &amp; recommendations. J. Clean. Prod. <b>177</b>, 448–463 (2018)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.jclepro.2017.12.239" aria-label="Article reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessing%20ICT%20global%20emissions%20footprint%3A%20trends%20to%202040%20%26%20recommendations&amp;journal=J.%20Clean.%20Prod.&amp;volume=177&amp;pages=448-463&amp;publication_year=2018&amp;author=Belkhir%2CL&amp;author=Elmeligi%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="32."><p id="ref-CR32">M. Durante, <i>Computational power: the impact of ICT on law, society and knowledge</i> (Routledge, London, 2021). <a href="https://doi.org/10.4324/9781003098683">https://doi.org/10.4324/9781003098683</a></p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.4324%2F9781003098683" aria-label="Book reference 32">Book</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20power%3A%20the%20impact%20of%20ICT%20on%20law%2C%20society%20and%20knowledge&amp;publication_year=2021&amp;author=Durante%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="33."><p id="ref-CR33">Pittaluga, F. &amp; Koppal, S. J. Privacy preserving optics for miniature vision sensors. in <i>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</i> (IEEE, 2015), pp. 314–324. <a href="https://doi.org/10.1109/CVPR.2015.7298628">https://doi.org/10.1109/CVPR.2015.7298628</a>.</p></li><li data-counter="34."><p id="ref-CR34">F. Pittaluga, A. Zivkovic, S. J. Koppal, Sensor-level privacy for thermal cameras. in <i>2016 IEEE International Conference on Computational Photography (ICCP)</i> (2016), pp. 1–12. <a href="https://doi.org/10.1109/ICCPHOT.2016.7492877">https://doi.org/10.1109/ICCPHOT.2016.7492877</a>.</p></li><li data-counter="35."><p id="ref-CR35">C. Hinojosa, J. C. Niebles, H. Arguello, Learning privacy-preserving Optics for Human Pose Estimation. in <i>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</i> (IEEE, 2021), pp. 2553–2562. <a href="https://doi.org/10.1109/ICCV48922.2021.00257">https://doi.org/10.1109/ICCV48922.2021.00257</a>.</p></li><li data-counter="36."><p id="ref-CR36">Y. LeCun, et al., Handwritten Digit Recognition With A Back-Propagation Network. in <i>Advances in Neural Information Processing Systems</i> vol. 2, (Morgan-Kaufmann, 1989).</p></li><li data-counter="37."><p id="ref-CR37">H. Xiao, K. Rasul, R. Vollgraf, Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. (2017). <a href="https://doi.org/10.48550/arXiv.1708.07747">https://doi.org/10.48550/arXiv.1708.07747</a>.</p></li><li data-counter="38."><p id="ref-CR38">J. Benesty, J. Chen, Y. Huang, I. Cohen, Pearson correlation coefficient. in <i>Noise reduction in speech processing</i> (Springer Berlin Heidelberg, 2009).</p></li><li data-counter="39."><p id="ref-CR39">O. Kulce, D. Mengu, Y. Rivenson, A. Ozcan, All-optical information-processing capacity of diffractive surfaces. Light Sci. Appl. <b>10</b>, 25 (2021)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021LSA....10...25K" aria-label="ADS reference 39">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41377-020-00439-9" aria-label="Article reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=All-optical%20information-processing%20capacity%20of%20diffractive%20surfaces&amp;journal=Light%20Sci.%20Appl.&amp;volume=10&amp;publication_year=2021&amp;author=Kulce%2CO&amp;author=Mengu%2CD&amp;author=Rivenson%2CY&amp;author=Ozcan%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">O. Kulce, D. Mengu, Y. Rivenson, A. Ozcan, All-optical synthesis of an arbitrary linear transformation using diffractive surfaces. Light Sci. Appl. <b>10</b>, 196 (2021)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021LSA....10..196K" aria-label="ADS reference 40">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41377-021-00623-5" aria-label="Article reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=All-optical%20synthesis%20of%20an%20arbitrary%20linear%20transformation%20using%20diffractive%20surfaces&amp;journal=Light%20Sci.%20Appl.&amp;volume=10&amp;publication_year=2021&amp;author=Kulce%2CO&amp;author=Mengu%2CD&amp;author=Rivenson%2CY&amp;author=Ozcan%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">D. Mengu et al., Misalignment resilient diffractive optical networks. Nanophotonics <b>9</b>, 4207–4219 (2020)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1515%2Fnanoph-2020-0291" aria-label="Article reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Misalignment%20resilient%20diffractive%20optical%20networks&amp;journal=Nanophotonics&amp;volume=9&amp;pages=4207-4219&amp;publication_year=2020&amp;author=Mengu%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="42."><p id="ref-CR42">C. Vieu et al., Electron beam lithography: resolution limits and applications. Appl. Surf. Sci. <b>164</b>, 111–117 (2000)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2000ApSS..164..111V" aria-label="ADS reference 42">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2FS0169-4332%2800%2900352-4" aria-label="Article reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Electron%20beam%20lithography%3A%20resolution%20limits%20and%20applications&amp;journal=Appl.%20Surf.%20Sci.&amp;volume=164&amp;pages=111-117&amp;publication_year=2000&amp;author=Vieu%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="43."><p id="ref-CR43">X. Zhou, Y. Hou, J. Lin, A review on the processing accuracy of two-photon polymerization. AIP Adv. <b>5</b>, 030701 (2015)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2015AIPA....5c0701Z" aria-label="ADS reference 43">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1063%2F1.4916886" aria-label="Article reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20on%20the%20processing%20accuracy%20of%20two-photon%20polymerization&amp;journal=AIP%20Adv.&amp;volume=5&amp;publication_year=2015&amp;author=Zhou%2CX&amp;author=Hou%2CY&amp;author=Lin%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="44."><p id="ref-CR44">Y. Luo et al., Design of task-specific optical systems using broadband diffractive neural networks. Light Sci. Appl. <b>8</b>, 112 (2019)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019LSA.....8..112L" aria-label="ADS reference 44">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41377-019-0223-1" aria-label="Article reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20of%20task-specific%20optical%20systems%20using%20broadband%20diffractive%20neural%20networks&amp;journal=Light%20Sci.%20Appl.&amp;volume=8&amp;publication_year=2019&amp;author=Luo%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="45."><p id="ref-CR45">X. Lin et al., All-optical machine learning using diffractive deep neural networks. Science <b>361</b>, 1004–1008 (2018)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018Sci...361.1004L" aria-label="ADS reference 45">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://www.ams.org/mathscinet-getitem?mr=3837095" aria-label="MathSciNet reference 45">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.aat8084" aria-label="Article reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=All-optical%20machine%20learning%20using%20diffractive%20deep%20neural%20networks&amp;journal=Science&amp;volume=361&amp;pages=1004-1008&amp;publication_year=2018&amp;author=Lin%2CX">
                    Google Scholar</a> 
                </p></li><li data-counter="46."><p id="ref-CR46">A. Ozcan, E. McLeod, Lensless imaging and sensing. Annu. Rev. Biomed. Eng. <b>18</b>, 77–102 (2016)</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1146%2Fannurev-bioeng-092515-010849" aria-label="Article reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Lensless%20imaging%20and%20sensing&amp;journal=Annu.%20Rev.%20Biomed.%20Eng.&amp;volume=18&amp;pages=77-102&amp;publication_year=2016&amp;author=Ozcan%2CA&amp;author=McLeod%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="47."><p id="ref-CR47">D. P. Kingma, J. Ba, Adam: a method for stochastic optimization. <a href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</a><i> [cs]</i> (2017).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1186/s43593-022-00021-3?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>The authors acknowledge the assistance of Dr. GyeoRe Han (UCLA) on 3D printing.</p></div></section><section data-title="Funding"><div id="Fun-section"><h2 id="Fun">Funding</h2><p>The Ozcan Research Group at UCLA acknowledges the support of ONR (Grant # N00014-22-1-2016). The Jarrahi Research Group at UCLA acknowledges the support of the Department of Energy (Grant # DE-SC0016925).</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>Bijie Bai and Yi Luo contributed equally to this work</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Electrical and Computer Engineering Department, University of California, Los Angeles, CA, 90095, USA</p><p>Bijie Bai, Yi Luo, Tianyi Gan, Jingtian Hu, Yuhang Li, Yifan Zhao, Deniz Mengu, Mona Jarrahi &amp; Aydogan Ozcan</p></li><li id="Aff2"><p>Bioengineering Department, University of California, Los Angeles, 90095, USA</p><p>Bijie Bai, Yi Luo, Jingtian Hu, Yuhang Li, Deniz Mengu &amp; Aydogan Ozcan</p></li><li id="Aff3"><p>California NanoSystems Institute (CNSI), University of California, Los Angeles, CA, USA</p><p>Bijie Bai, Yi Luo, Tianyi Gan, Jingtian Hu, Yuhang Li, Yifan Zhao, Deniz Mengu, Mona Jarrahi &amp; Aydogan Ozcan</p></li></ol><h3 id="contributions">Contributions</h3><p>AO conceived the research and initiated the project. BB, YL, and DM developed the numerical simulation codes. BB, YL, TG, JH, YL, and YZ performed the fabrication of the diffractive system and conducted the experiments. All the authors participated in the analysis and discussion of the results. BB, YL, TG, JH, and AO prepared the manuscript and all authors contributed to the manuscript. AO and MJ supervised the project. All authors read and approved the final manuscript.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:ozcan@ucla.edu">Aydogan Ozcan</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
              
                <h3>Competing interests</h3>
                <p>AO and MJ serve as Editors of the journal; no other author reported any competing interests.</p>
              
            </div></div></section><section data-title="Supplementary Information"><div id="Sec16-section"><h2 id="Sec16">Supplementary Information</h2><div id="Sec16-content"><div data-test="supplementary-info">
                  
                  <div id="MOESM2"><p><b>Additional file 2: Movie S1.</b> Blind testing results of a five-layer diffractive camera design (reported in the main text Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>) with input objects at different intensity levels.</p></div>
                  <div id="MOESM3"><h3><a href="https://elight.springeropen.com/articles/10.1186/esm/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_MOESM3_ESM.avi">43593_2022_21_MOESM3_ESM.avi</a></h3><p><b>Additional file 3: Movie S2.</b> Blind testing results of a five-layer diffractive camera design (reported in the main text Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>) with input objects modulated by 50% transmission filters applied at different sub-regions of the input field-of-view.</p></div>
                  <div id="MOESM4"><p><b>Additional file 4: Movie S3.</b> Blind testing results of a seven-layer diffractive camera design with input objects continuously shifted throughout a large input field-of-view.</p></div>
                <div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_MOESM1_ESM.pdf" data-supp-info-image=""><b>Additional file 1: Figure S1.</b></a></h3><p> Blind testing results of diffractive camera designs that selectively image different data classes. <b>Figure S2.</b> Blind testing results of a seven-layer diffractive camera design that selectively images trousers in the Fashion MNIST dataset, while all-optically erasing 4 other types of fashion objects (i.e., dresses, sandals, sneakers, and bags). <b>Figure S3.</b> Converged diffractive layers for the diffractive camera designs with different numbers of diffractive layers.</p></div><div data-test="supp-item" id="MOESM3"><h3><a data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1186%2Fs43593-022-00021-3/MediaObjects/43593_2022_21_MOESM3_ESM.avi" data-supp-info-image=""><b>Additional file 3: Movie S2.</b></a></h3><p> Blind testing results of a five-layer diffractive camera design (reported in the main text Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://elight.springeropen.com/articles/10.1186/s43593-022-00021-3#Fig3">3</a>) with input objects modulated by 50% transmission filters applied at different sub-regions of the input field-of-view.</p></div></div></div></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#39;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#39;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=To%20image%2C%20or%20not%20to%20image%3A%20class-specific%20diffractive%20cameras%20with%20all-optical%20erasure%20of%20undesired%20objects&amp;author=Bijie%20Bai%20et%20al&amp;contentID=10.1186%2Fs43593-022-00021-3&amp;copyright=The%20Author%28s%29&amp;publication=2662-8643&amp;publicationDate=2022-08-15&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1186/s43593-022-00021-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186/s43593-022-00021-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Bai, B., Luo, Y., Gan, T. <i>et al.</i> To image, or not to image: class-specific diffractive cameras with all-optical erasure of undesired objects.
                    <i>eLight</i> <b>2, </b>14 (2022). https://doi.org/10.1186/s43593-022-00021-3</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="https://citation-needed.springer.com/v2/references/10.1186/s43593-022-00021-3?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2022-05-31">31 May 2022</time></span></p></li><li><p>Revised<span>: </span><span><time datetime="2022-07-16">16 July 2022</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2022-07-25">25 July 2022</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2022-08-15">15 August 2022</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1186/s43593-022-00021-3</span></p></li></ul></div></div></div></div></section>

                    


                </article></div>
  </body>
</html>
