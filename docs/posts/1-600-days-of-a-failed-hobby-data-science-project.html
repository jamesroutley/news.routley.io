<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lellep.xyz/blog/failed-data-science-project.html">Original</a>
    <h1>1,600 days of a failed hobby data science project</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>⭐ I spent &gt;1,600 days working on a data science project that then failed because I lost interest. This article is to cope
with the failure and maybe help you (and me) to finish successful data science projects by summarising
a few learnings into a checklist, see <a href="#learnings">below</a>.</p>
<p>Nobody really likes to speak about failures. The same holds true for failed data science projects.
In this text, I document my most recent data science project which I worked on for 1,600 days -
days that I won’t get back despite the project failing.
Ultimately, it is important to communicate about failed projects similarly to successful projects
as those failed ones are the ones that help us to acquire scar tissue (<a href="https://youtu.be/I2ZK3ngNvvI?si=MkoLkVLl34ChHdW0&amp;t=111">thanks Andrej</a>) which makes us better data science
practitioners. And the latter is what we should all strive for!</p>

<div id="the-planned-project-its-failure">
<h2><a href="#table-of-contents">The planned project <span>&amp;</span> its failure</a></h2>
<p>After completing <a href="https://lellep.xyz/blog/cycling-in-marburg-1-intro.html">a substantial hobby data analysis project</a> involving 1.5 million Nextbike
datapoints, I wanted to explore a different data modality: text. While my previous
work had focused on numerical analysis, I wanted to explore text as data source while delivering
some value. This opportunity presented itself in March 2020, just as <span>COVID</span>-19 began making
headlines. I discovered that Germany’s public news outlet, <a href="https://www.tagesschau.de/">Tagesschau</a>,
had launched a <span>COVID</span>-focused newsticker, aggregating everything from case numbers to policy changes
and breaking incidents.</p>
<p>The <span>COVID</span> newsticker provided both a compelling topic and an ideal text data
source. My plan was to scrape the entire newsticker over time and analyse it for both
metadata patterns, such as posting frequency, and deeper semantic insights about the people,
topics and countries involved. When <span>COVID</span> became less of an issue the Tagesschau shut down their
<span>COVID</span> newsticker in 2022-Q3 but launched a new one covering the emerging Ukraine-Russia war.
I seamlessly extended my project to collect also this new data stream.</p>
<p>I stopped working on the project in 2024-Q3 after 1,600 days of collecting data and some short data exploration
because life got busier, my priorities shifted and I had less free time at hand. Unfortunately, I never performed
any sort of serious data analyses except aforementioned short data exploration that I did after one week
into the project in 2020-Q2.</p>
</div>
<div id="data-source-tagesschau-newsticker">
<h2><a href="#table-of-contents">Data source: Tagesschau newsticker</a></h2>
<p>The raw data format of the newsticker is shown in the figure below and explained in words below
the figure.</p>
<div>
<p><a href="https://lellep.xyz/blog/images/failed_data_science_project/2024-11-01_liveblog_data_format.jpg">
<img alt="" src="https://lellep.xyz/blog/images/failed_data_science_project/2024-11-01_liveblog_data_format.jpg"/>
</a></p><p>Raw format of newsticker data on Tagesschau website. Explanation see main text.</p>
</div>
<p>Every day featured a list of short news stories (shown in green),
each containing a timestamp, headline, body text and optional multimedia elements like Twitter/X
feeds, videos or audio clips. Each day normally linked back to the previous day.</p>
<p>I collected all the URLs and downloaded the underlying raw data from 2020-Q1 to 2024-Q3 for around
1,600 days while working towards my PhD and writing up my thesis. Then, I stopped because I lost
interest in the project and didn’t have time to continue it anymore.</p>
</div>
<div id="implementation-of-project-system-design">
<h2><a href="#table-of-contents">Implementation of project: System Design</a></h2>
<p>The system is a semi-automatic data pipeline which extracts raw data from the German news outlet Tagesschau
once a day and turns the raw unstructured data into structured news snippets that make up a dataset.
If I would have finished the project, this dataset would then have been released and used for
a number of analyses using Python.</p>
<p>The project system design is shown visually in the below figure and explained with words afterwards.</p>
<div>
<p><a href="https://lellep.xyz/blog/images/failed_data_science_project/2024-11-01_system_design.jpg">
<img alt="" src="https://lellep.xyz/blog/images/failed_data_science_project/2024-11-01_system_design.jpg"/>
</a></p><p>System design: Semi-automatic data pipeline for dataset curation that is used for analyses.</p>
</div>
<p>The data collection process involved a daily ritual of manually visiting the Tagesschau website to
capture links to both the <span>COVID</span> and later Ukraine war newstickers. While this manual approach constituted
the bulk of the project’s effort, it was necessitated by Tagesschau’s unstructured <span>URL</span> schema, which
made automated link collection impractical.</p>
<p>Every four to eight weeks, I would use Python to batch process these accumulated URLs, downloading the
underlying <span>HTML</span> pages as raw data. The emphasis on preserving raw <span>HTML</span> proved vital when Tagesschau
repeatedly altered their newsticker <span>DOM</span> structure throughout Q2 2020. This experience underscored a
fundamental data engineering principle: <strong>raw data is king</strong>. While parsers can be rewritten, lost data is irretrievable.</p>
<p>The technical pipeline itself was straightforward: Python scripts with BeautifulSoup4 parsed the
<span>HTML</span> pages into structured news items, which were then stored in an SQLite database. The end goal
was twofold: to share this unique dataset with the broader community and to conduct what I optimistically
labeled as “❤️ Cool Analyses” - though these analyses would ultimately remain unrealized.</p>
<p>Many parts of the project were on autopilot. That is except for two parts: (1) the manual
<span>ETL</span> pipeline part and (2) the actual analyses. While part (2) would have been the actual fun part, I spent
1,600 days doing (1) starting in ~2020-03 and ultimately abandonning the project in 2024-Q3.</p>
</div>
<div id="my-learnings-for-me-and-others">
<h2><a href="#table-of-contents">My learnings for me and others</a></h2>
<p>In the future, I will use the following short checklist of advice to conduct successful (data science) projects.
The list is roughly ordered by decreasing relevance:</p>
<ol>
<li>Consider story telling from day one. While <a href="https://lellep.xyz/blog/cycling-in-marburg-1-intro.html">my Nextbike project</a>
had that built-in, this project would have been much more difficult to talk about because of the
negative topics of <span>COVID</span> and a war.</li>
<li>Get first analyses results out quickly based on a small dataset and don’t just collect data
up front to “analyse it later” - this is what I did here and it’s bad. Data postprocessing and
releasing some results as teasers are mostly important for you as after all, quick results keep
us motivated; e.g. for me, the present Tagesschau project was very interesting in the beginning
but I lost interest over time because of mindless data collecting. Ideally, given these first
results follow “<a href="https://austinkleon.com/show-your-work/">Show Your Work</a>” approach by making
the results public; this is super difficult for perfectionists.</li>
<li>Automate the h*ll out of your data science project (in particular the data acquisition part)
to reduce daily friction. If the automation process takes considerable work, then perceive it as a separate
topic to speak about online, again following the “<a href="https://austinkleon.com/show-your-work/">Show Your Work</a>” approach. For example,
the manual step in the process of my project could have been a beautiful and innovative machine
learning opportunity. Back then, I would have trained a specialised model (or used a pretrained specialised model)
but since LLMs made so much progress during the runtime of this project from 2020-Q1 to 2024-Q4, I would now
rather consider a foundational model wrapped as an <span>AI</span> agent instead; for example, I would try to find a
foundation model to do the job of for example finding the right link on the Tagesschau website, which was
by far the most draining part of the whole project.</li>
<li>Data cadence is relevant: Do you have to collect the data yourself over a period of time?
If yes, how often do you have to collect it - as in how often per hour for example? For the present
project, I had to scrape data only once per day, which is doable manually. For <a href="https://lellep.xyz/blog/cycling-in-marburg-1-intro.html">my previous NextBike project</a>,
however, I scraped data every 30 seconds. Since I scaped an <span>API</span>, the automation of that process was fairly simple.</li>
<li>Store raw data if possible. This allows you to condense it later.</li>
<li>Consider using the cloud for simplicity, reliability and last but certainly not least developing
real-life employability skills (<em>I am looking at you, University, that often fails to deliver the latter</em>).</li>
</ol>
</div>
<div id="conclusion">
<h2><a href="#table-of-contents">Conclusion</a></h2>
<p>Go out and build something!</p>
<p>A failed data science project is part of the learning experience and contributes
to your battle scar tissue; battle scar tissue through projects is a great concept from Andrej Karpathy
(see <a href="https://youtu.be/I2ZK3ngNvvI?si=MkoLkVLl34ChHdW0&amp;t=111">here</a> and <a href="https://stevengong.co/notes/Scar-Tissue">there</a>)
<span>&amp;</span> certainly makes us better data practitioners.</p>
<p>Personally, I would have loved to use the beautiful applied <span>ML</span> opportunity of identifying the right
newsticker link on the Tagesschau website using <span>NLP</span> to remove any manual labour but unfortunately, I don’t have
the time to do so. Also, I would have loved to evaluate the ~100,000 news
snippets that I collected and shared the dataset - but I will not; although I still think that these news analyses
would be super interesting!</p>
<p>Good news is that Tagesschau keeps releasing newstickers like for example for the <span>US</span> election 2024.
If you’re interested in picking up the project and getting some battle scars in, then please
drop me a message - I am happy to share the whole project with no strings attached :-).</p>
<hr/>
<p>Thanks to everyone who proof-read early versions of this text:
<a href="https://www.linkedin.com/in/drsimonbarnes">Simon Barnes</a>.</p>
</div>

    </div></div>
  </body>
</html>
