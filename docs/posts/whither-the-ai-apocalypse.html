<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://brian.abelson.live/log/2023/05/31/whither-the-AI-apocalypse.html">Original</a>
    <h1>Whither the AI Apocalypse?</h1>
    
    <div id="readability-page-1" class="page"><div>
                            <p>Ten years ago, I wrote <a href="https://brian.abelson.live/log/2021/11/12/whither-the-pageview-apocalypse.html">an essay</a> for my <a href="https://www.opennews.org/what/fellowships/info/">Open News Fellowship</a> which critically assessed the Web Analytics industry’s proclamation that the “the pageview is dead.” The essay got some traction online and was eventually used a framework for <a href="https://www.salon.com/2013/10/09/the_end_of_pageviews_and_michele_bachmanns_rapture/">analyzing a right-wing politician’s use of the “rapture” when criticizing Obama’s foreign policy</a>. The gist of the argument, which borrowed from Jacques Derrida’s 1982 essay <a href="https://brian.abelson.live/assets/pdf/derrida-of-an-apocalyptic-tone-recently-adopted-in-philosophy.pdf"><em>Of an Apocalyptic Tone Recently Adopted In Philosophy</em></a>, was that doomsayers are primarily “concerned with seducing you into accepting the terms on which their continued existence, their vested interests, and their vision of ‘the end’ are all equally possible.” As Derrida concisely put it: “the subject of [apocalyptic] discourse [hopes] to arrive at its end through the end.” A decade later, I can confidently say that the pageview is very much still alive and that <a href="https://venturebeat.com/data-infrastructure/app-analytics-provider-mixpanel-becomes-latest-unicorn/">the leaders of the analytics companies who predicted otherwise are now very rich</a>.</p>

<p>––</p>

<p>Yesterday, <a href="https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html">a collection of academic and industry leaders collectively signed a statement</a> which issued this stark, apocalyptic proclamation:</p>

<blockquote>
  <p>Mitigating the risk of extinction from A.I. should be a global priority alongside other societal-scale risks, such as pandemics and nuclear war.</p>
</blockquote>

<p>The signatories’ implication was that since AI was a “societal-scale” risk, it required sophisticated governmental regulation on the scale of other apocalyptic threats (note their blatant omission of climate change).  By positioning themselves as “AI experts,” it also put them in a natural position to help craft and shape these regulations given the complexity of the underlying technologies. While they were purposefully vague on what an AI apocalypse might look like, most doomsday scenarios follow the concept of the <a href="https://en.wikipedia.org/wiki/Technological_singularity">“singularity”</a> in which AI undergoes “a ‘runaway reaction’ of self-improvement cycles, each new and more intelligent generation appearing more and more rapidly, causing an ‘explosion’ in intelligence and resulting in a powerful superintelligence that qualitatively far surpasses all human intelligence.” As the story goes, this superintelligence, no longer moored by its creators, begins to act in its own interests and evenually wipes out humanity.  A particularly sophisticated and influential example of this narrative <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">was published on the online forum LessWrong last year</a>.</p>

<p>––</p>

<p>Earlier this month, an interesting document leaked which was confirmed to have originated from a Google AI researcher. The essay, entitled <a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"><em>We Have No Moat, And Neither Does OpenAI</em></a>, traces the rapid development of open-source alternatives to OpenAI’s and Google’s LLMs over the past few months. It argues that the open-source community is much better equipped to push the forefront of LLM research because it’s more nimble, less bound by bureaucratic inefficiencies, and has adopted tools like <a href="https://arxiv.org/abs/2106.09685">“Low-Rank Adaptation”</a> for fine-turning models without the need of large clusters of GPUs. As the author summarizes:</p>

<blockquote>
  <ul>
    <li>We have no secret sauce. Our best hope is to learn from and collaborate with what others are doing outside Google.</li>
    <li>People will not pay for a restricted model when free, unrestricted alternatives are comparable in quality. We should consider where our value add really is.</li>
    <li>Giant models are slowing us down. In the long run, the best models are the ones which can be iterated upon quickly.</li>
  </ul>
</blockquote>

<p>For anyone who has been closely following the development of open-source tooling for LLMs over the past couple of months, these assertions are not particularly controversial. Each day brings a fresh crop of tweets and blog posts announcing a new software package, model, hosting platform, or technique for advancing LLM development. These tools are then quickly adopted and iterated upon leading to the next day’s announcements. In many ways, this rapid advancement mirrors the ‘runaway reaction’ at the heart of the singularity narrative.</p>

<p>––</p>

<p>If we understand apocalyptic narratives as a rhetorical sleight-of-hand, then we must ask the same questions of the AI industry which my previous essay asked of Web Analytics:</p>

<blockquote>
  <p>What then of the [AI] apocalypse and the prophets who giddily proclaim it? To what ends are these revelations leading us? What strategic aims and benefits are these claims predicated upon?</p>
</blockquote>

<p>Given the above arguments outlined out in the leaked document (and backed up by anecdotal evidence) we can conclude that the “existential threat” AI companies are most concerned with is their inability to monopolize the profits to be had from the nascent <a href="https://www.nytimes.com/2023/05/31/magazine/ai-start-up-accelerator-san-francisco.html">“AI boom.”</a> <a href="https://www.nytimes.com/2023/04/16/technology/google-search-engine-ai.html">Google is particularly vulnerable to this threat</a> given its heavy reliance on search-related advertising which <a href="https://www.cnbc.com/2021/05/18/how-does-google-make-money-advertising-business-breakdown-.html">reportedly accounts for more thant 80% of its yearly revenue</a>. Regulation, while absolutely necessary, could be shaped in a way to potentially stifle the rapid development of the open-source community by requiring complex security, privacy, or other restrictions which could make it illegal (or at least cost prohibitive) for an individual to develop and deploy an LLM from their laptop. Since many “AI experts” directly work for, or recieve funding from the companies that signed the letter, it stands to reason that the aim of this proclamation is to ensure that they have a hand in shaping the eventual regulations in a way which ensures their monopolistic domination of the AI industry.</p>

<p>––</p>

<p>I have a different take on the “singularity” which is informed by recently completing <a href="https://www.amazon.com/Palo-Alto-History-California-Capitalism/dp/031659203X"><em>Palo Alto</em></a> – an excellent history of American capitalism <em>vis-a-vis</em> the tech industry.  The basic idea is that capitalists have long sought to replace employees with automation and, for those tasks they can’t fully automate, to make the workers performing them function more like machines. In this reading, the “singularity” is not achieved simply by making machines “sentient,” but by simultaneously turning humans into machines, effectively lowering the bar an AI has to jump over to achieve sentience; if you work in an Amazon Fulfillment Center, you are already functioning very close to a machine, and much of your work is probably focused on training <a href="https://www.youtube.com/watch?v=r2VcA7nMJs8&amp;vl=en">the robots that will replace you</a>. This is no less of a disastrous scenario, but at least in this reading we can rightfully point the finger at the capitalists for ending the world rather than AI. To modify William Gibson’s adage (<a href="https://www.nytimes.com/2012/01/15/books/review/distrust-that-particular-flavor-by-william-gibson-book-review.html">which might not actually be his</a>): “the apocalypse is already here it’s just not evenly distributed.”</p>

                        </div></div>
  </body>
</html>
