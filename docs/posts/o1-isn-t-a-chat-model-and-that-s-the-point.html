<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/o1-skill-issue">Original</a>
    <h1>O1 isn&#39;t a chat model (and that&#39;s the point)</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em><span>swyx here: We’re proud to feature our first guest post</span><span> of 2025! It has spawned great discussions on </span><a href="https://x.com/gdb/status/1878489681702310392" rel="">gdb</a><span>, </span><a href="https://x.com/benhylak/status/1878237490194366744" rel="">Ben</a><span>, and </span><a href="https://x.com/daniel_mac8/status/1878423666309902404" rel="">Dan’s</a><span> pages. See also </span><a href="https://youtu.be/NkHcSpOOC60" rel="">our YouTube discussion</a><span>.</span></em></p><p><em><span>Since o1’s launch in October and o1 pro/o3’s announcement in December, many have been struggling to figure out their takes, both </span><a href="https://x.com/tigransloyan/status/1864845328752808167?s=46" rel="">positive</a><span> and </span><a href="https://news.ycombinator.com/item?id=42565606" rel="">negative</a><span>. We took a </span><a href="https://www.latent.space/p/chatgpt-max" rel="">strongly positive stance</a><span> at the </span><a href="https://x.com/jasondeanlee/status/1870883464767242360?s=46" rel="">nadir of o1 Pro sentiment</a><span> and mapped out what it would likely take for OpenAI to have a $2000/month agent product (</span><a href="https://www.theverge.com/2024/11/13/24295879/openai-agent-operator-autonomous-ai" rel="">rumored to be launched in the next few weeks</a><span>). Since then, </span><a href="https://x.com/lmarena_ai/status/1873695386323566638" rel="">o1 has sat comfortably at #1</a><span> across ALL LMArena leaderboards (soon to have default </span><a href="https://latent.space/p/lmarena" rel="">Style Control as we discussed on pod</a><span>).</span></em></p><p><em><span>We’ve been following Ben Hylak’s work on the Apple VisionOS for a bit, and invited him to </span><a href="https://www.youtube.com/watch?v=5nOLb27hQ5w" rel="">speak at the World’s Fair</a><span>. He has since launched </span><a href="https://x.com/benhylak/status/1839014819753775167" rel="">Dawn Analytics</a><span>, and continued to publish unfiltered thoughts about o1 — initially as a loud skeptic, and slowly becoming a daily user. We love </span><a href="https://www.swyx.io/guo-lai-ren" rel="">mind-changers</a><span> in both its meanings, and think this same conversation is happening all over the world as people struggle to move from the chat paradigm to the brave new world of reasoning and $x00/month prosumer AI products like Devin (</span><a href="https://www.youtube.com/watch?v=T7NWjoD_OuY" rel="">spoke at WF</a><span>, </span><a href="https://www.cognition.ai/blog/devin-generally-available" rel="">now GA</a><span>). Here are our thoughts.</span></em></p><p><em><span>PSA: Due to overwhelming demand (&gt;15x applications:slots), we are closing CFPs for </span><a href="https://apply.ai.engineer/" rel="">AI Engineer Summit</a><span> tomorrow. Last call! Thanks, we’ll be reaching out to all shortly!</span></em></p><p><strong>How did I go from hating o1 to using it everyday for my most important questions?</strong></p><p><strong>I learned how to use it.</strong></p><p><span>When </span><a href="https://buttondown.com/ainews/archive/ainews-200-chatgpt-pro-and-o1-fullpro-with-vision/" rel="">o1 pro was announced</a><span>, I subscribed without flinching. </span><strong>To justify the $200/mo price tag, it just has to provide 1-2 Engineer hours a month</strong><span> (the less we have to hire at </span><a href="http://dawnai.com/" rel="">dawn</a><span>, the better!)</span></p><p><span>But at the end of a day filled with earnest attempts to get the model to work — I concluded that </span><strong>it was garbage</strong><span>.</span></p><p>Every time I asked a question, I had to wait 5 minutes only to be greeted with a massive wall of self-contradicting gobbledygook, complete with unrequested architecture diagrams + pro/con lists.</p><p><span>I </span><a href="https://x.com/benhylak/status/1864835651725910023" rel="">tweeted as much</a><span> and a lot of people agreed — but more interestingly to me, some disagreed vehemently. In fact, they were mind-blown by just how good it was.</span></p><p>Sure, people often get very hypey about OpenAI after launches (it’s the second best strategy to go viral, right after being negative.)</p><p>But this felt different — these takes were coming from folks deep in the trenches.</p><p>The more I started talking to people who disagreed with me, the more I realized I was getting it completely wrong:</p><p><strong>I was using o1 like a chat model — but o1 is not a chat model.</strong></p><p>If o1 is not a chat model — what is it?</p><p>I think of it like a “report generator.” If you give it enough context, and tell it what you want outputted, it’ll often nail the solution in one-shot.</p><blockquote><p><em><span>swyx’s Note: OpenAI does </span><a href="https://platform.openai.com/docs/guides/reasoning#advice-on-prompting" rel="">publish advice on prompting o1</a><span>, but we find it incomplete, and in a sense you can view this article as a “Missing Manual” to lived experience using o1 and o1 pro in practice.</span></em></p></blockquote><p>Give a ton of context. Whatever you think I mean by a “ton” — 10x that.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png" width="1456" height="1212" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1212,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:853789,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5407ad16-67a5-4683-aa4c-0af8caaa0f5f_2020x1682.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>When you use a chat model like Claude 3.5 Sonnet or 4o, you often start with a simple question and some context. If the model needs more context, it’ll often ask you for it (or it’ll be obvious from the output).</p><blockquote><p><em><span>(Putting </span><a href="https://x.com/benhylak/status/1878514144766480777" rel="">context at the end</a><span> is better for OpenAI models - per OpenAI’s own docs)</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg" width="523" height="334.2841666666667" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:767,&#34;width&#34;:1200,&#34;resizeWidth&#34;:523,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;Image&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa76ee755-c0a1-405d-b662-c37027b9c3f6_1200x767.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div></blockquote><p><span>You iterate back and forth with the model, correcting it + expanding on requirements, until the desired output is achieved. It’s almost like pottery. </span><strong>The chat models essentially pull context from you</strong><span> via this back and forth. Overtime, our questions get quicker + lazier — as lazy as they can be while still getting a good output.</span></p><p><span>o1 will just take lazy questions at face value and doesn’t try to pull the context from you. Instead, you need to </span><strong>push as much context as you can into o1</strong><span>.</span></p><p>Even if you’re just asking a simple engineering question:</p><ul><li><p>Explain everything that you’ve tried that didn’t work</p></li><li><p>Add a full dump of all your database schemas</p></li><li><p>Explain what your company does, how big it is (and define company-specific lingo)</p></li></ul><p><span>In short, treat o1 like a new hire. Beware that </span><em><span>o1’s mistakes include reasoning about how much it should reason.</span><strong> </strong></em><span>Sometimes the variance fails to accurately map to task difficulty. e.g. if the task is really simple, it will often spiral into reasoning rabbit holes for no reason. </span><em><span>Note: the o1 API allows you to </span><a href="https://buttondown.com/ainews/archive/ainews-o1-api-4o4o-mini-in-realtime-api-webrtc/" rel="">specify low/medium/high reasoning_effort</a><span>, but that is not exposed to ChatGPT users.</span></em></p><blockquote><p><strong>Tips to make it easier giving o1 context</strong></p><ol><li><p><span>I suggest using the </span><strong>Voice Memos app</strong><span> on your mac/phone. I just describe the entire problem space for 1-2 minutes, and then paste that transcript in.</span></p><ul><li><p>I actually have a note where I keep long segments of context to re-use.</p></li><li><p><em><span>swyx: I use </span><a href="https://carelesswhisper.app" rel="">Careless Whisper</a><span> by Sarav from the LS Discord</span></em></p></li></ul></li><li><p>The AI assistants that are popping up inside of products can often make this extraction easier. For example, if you use Supabase, try asking the Supabase Assistant to dump/describe all of the relevant tables/RPC’s/etc.</p></li></ol></blockquote><p><span>Once you’ve stuffed the model with as much context as possible — </span><em><strong>focus on explaining what you want the output to be.</strong></em></p><p><span>With most models, we’ve been trained to tell the model </span><em>how</em><span> we want it to answer us. e.g.“You are an expert software engineer. Think slowly + carefully”</span></p><p><span>This is the opposite of how I’ve found success with o1. I don’t instruct it on the </span><em>how</em><span> — only the </span><em>what</em><span>. Then let o1 take over and plan and resolve its own steps. This is what the autonomous reasoning is for, and can actually be much faster than if you were to manually review and chat as the “human in the loop”.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png" width="1200" height="819.2307692307693" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:994,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:207759,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F022016e9-09a2-4070-bf64-e2f1bbe56955_2120x1448.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>swyx’s poor illustration attempt</figcaption></figure></div><blockquote><p><em><strong>swyx’s pro tip</strong><span>: developing really good criteria for what you consider to be “good” vs “bad” helps you </span><strong>give the model a way to evaluate its own output</strong><span> and self-improve/fix its own mistakes. Essentially </span><strong><span>you’re </span><a href="https://x.com/swyx/status/1878554396784820662" rel="">moving the LLM-as-Judge</a><span> into the prompt and letting o1 run it whenever needed.</span></strong></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png" width="465" height="291.11675126903555" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/e4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:740,&#34;width&#34;:1182,&#34;resizeWidth&#34;:465,&#34;bytes&#34;:103078,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4d8ee50-e218-4389-b485-bf44ff275188_1182x740.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><em><span>As a bonus, this eventually gives you LLM-as-Judge evaluators you can use for</span><a href="https://www.interconnects.ai/p/openais-reinforcement-finetuning" rel=""> Reinforcement Finetuning</a><span> when it is GA.</span></em></p></blockquote><p><span>This requires you to </span><strong>really know exactly what you want</strong><span> (and you should really ask for one specific output per prompt — it can only reason at the beginning!)</span></p><p>Sounds easier than it is! Did I want o1 to implement a specific architecture in production, create a minimal test app, or just explore options and list pros/cons? These are all entirely different asks.</p><p>o1 often defaults to explaining concepts with a report-style syntax — completely with numbered headings and subheadings. If you want to skip the explanations and output complete files — you just need to explicitly say that.</p><p>Since learning how to use o1, I’ve been pretty mind-blown by its ability to generate the right answer the first time. It’s really pretty much better in every single way (besides cost/latency). Here are a few little moments where this has particularly stood out:</p><p><strong>What o1 does well:</strong></p><ul><li><p><strong>Perfectly one-shotting entire/multiple files</strong><span>: This, by far, is o1’s most impressive ability. I copy/paste a ton of code in, a ton of context about what I’m building, and it’ll completely one-shot the entire file (or files!), usually free of errors, following existing patterns I have in my codebase.</span></p></li><li><p><strong>Hallucinates Less</strong><span>: In general, it just seems to confuse things less. For example, o1 really nails bespoke query languages (like ClickHouse and New Relic), where Claude often confuses the syntax for Postgres.</span></p></li><li><p><strong>Medical Diagnoses: </strong><span>My girlfriend is a dermatologist — so whenever any friend or anyone in my extended family has any sort of skin issue, they’re sure to send her a picture! Just for fun, I started asking o1 in parallel. It’s usually shockingly close to the right answer — maybe 3/5 times. More useful for medical professionals — </span><strong>it almost always provides an extremely accurate differential diagnosis.</strong></p></li><li><p><strong>Explaining Concepts: </strong><span>I’ve found that it is very good at explaining very difficult engineering concepts, with examples. It’s almost like it generates an entire article.</span></p><p>When I’m working on difficult architectural decisions, I will often have o1 generate multiple plans, with pros/cons for each, and even compare those plans. I’ll copy/paste the responses as PDF’s, and compare them — almost like I’m considering proposals.</p></li><li><p><strong>Bonus: Evals. </strong><span>I have historically been very skeptical of using LLM as a Judge for Evals, because fundamentally the judge model often suffers from the same failure modes as what generated the outputs in the first place. o1, however, shows a ton of promise — it is often able to determine if a generation is correct or not with very little context.</span></p></li></ul><p><strong>What o1 doesn’t do well (yet):</strong></p><ul><li><p><strong>Writing in specific voices/styles:  </strong><span>No, I did not use o1 to write this post :)</span></p><p>I’ve found that it’s pretty bad at writing anything, especially in specific voices or styles. It has a very academic/corporate report style that it wants to follow. I think that there are just so many reasoning tokens biasing the tone in that direction, it’s very hard to break free from that.</p><p>Here’s an example of me trying to get it to write this post — this is after much back and forth — it just wants to produce a bland school report.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png" width="508" height="341.8523878437048" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:930,&#34;width&#34;:1382,&#34;resizeWidth&#34;:508,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a333a21-de27-4e1b-adba-558d743d811d_1382x930.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div></li><li><p><strong>Building an Entire App: </strong><span>o1 is mindblowingly good at one-shotting entire files. that being said, despite some of the more… optimistic… demos you might see on twitter — o1 is not going to build an entire SaaS for you, at least not with a </span><em><strong>lot</strong></em><span> of iteration. </span><strong>But it </strong><em><strong>can</strong></em><strong> pretty much one-shot entire features, especially if they’re front-end or simple backend features</strong><span>.</span></p></li></ul><p><strong>Latency fundamentally changes our experience of a product.</strong></p><blockquote><p><em><span>swyx: we agree - as much as </span><a href="https://www.latent.space/p/inference-fast-and-slow" rel="">6 grades of AI latency</a><span> are common now.</span></em></p><div data-component-name="DigestPostEmbed"><div><a href="https://www.latent.space/p/inference-fast-and-slow" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b148d1c-c5ee-4345-bc8f-6c4c79992894_1071x446.png"/><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b148d1c-c5ee-4345-bc8f-6c4c79992894_1071x446.png" sizes="100vw" alt="Inference, Fast and Slow" width="140" height="140"/></picture></div></a></div></div></blockquote><p>Consider the differences between mail, email and texting — it’s mainly just latency. A voice message vs. a phone call — latency. A video vs a Zoom — latency. And so on.</p><p>I call o1 a “report generator” because it’s clearly not a chat model — it feels a lot more like email.</p><p>This hasn&#39;t yet manifested in o1&#39;s product design. I would love to see the design more honestly reflected in the interface.</p><p><span>Here are some </span><strong>specific AI</strong><span> </span><strong>UX tips for anyone building o1-based products</strong><span>:</span></p><ol><li><p><span>Make it easier to see the hierarchy of the response (think a </span><strong>mini table of contents</strong><span>)</span></p></li><li><p><strong>Similarly, make the hierarchy more easily navigable.</strong><span> Since every request is usually larger than the height of the window, I would take a Perplexity like approach where each question/answer page gets a section vs. freeform scroll. Within an answer, things like sticky headers, collapsible headers, etc. could really help)</span></p></li><li><p><strong>Make it easier to manage and see the context you’re providing</strong><span> to the model. (Ironically Claude’s UI does a much better job of this — when you paste in a long piece of text, it renders as a little attachment). I also find that ChatGPT Projects don’t work nearly as well as Claude’s, so I’m copying and pasting stuff </span><em>a lot.</em></p></li></ol><p>Side note:</p><ul><li><p><span>Separately ChatGPT is </span><em>REALLY</em><span> buggy when it comes to o1. The descriptions of reasoning are comical, it often completely fails to generate, and most often doesn’t work on the mobile app.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png" width="962" height="390" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/faaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:390,&#34;width&#34;:962,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaaa7faf-3a1c-4675-b1e1-e929a7143dc3_962x390.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption>A beautiful day in… Kenya??</figcaption></figure></div></li></ul><p>I’m really excited to see how these models actually get used.</p><p>I think o1 will make certain products possible for the first time — for example, products that can benefit from high-latency, long running background intelligence.</p><p>What sort of tasks is a user willing to wait 5 minutes for? An hour? A day? 3-5 business days? </p><p>A bunch, I think, if it’s designed correctly.</p><p>As models get more expensive, experimentation gets harder to justify. It’s easier than ever to waste $1000s of dollars in just minutes.</p><p>o1-preview and o1-mini support streaming, but they don’t support structured generation or system prompts. o1 supports structured generation and system prompts, but not streaming yet. </p><p>Given how long a response takes, streaming feels like a requirement.</p><p>It will be very cool to see what developers actually do with the model as they get to work in 2025.</p><p><span>After the success of this post, we </span><a href="https://youtu.be/NkHcSpOOC60" rel="">followed up on YouTube</a><span>:</span></p><div id="youtube2-NkHcSpOOC60" data-attrs="{&#34;videoId&#34;:&#34;NkHcSpOOC60&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/NkHcSpOOC60?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><em><span>swyx: Thanks Ben! Last plug - if you’re </span><strong>building agents</strong><span> with o1, or managing a team of AI engineers, you should definitely apply to </span><a href="https://www.latent.space/p/2025-summit" rel="">AIES NYC</a><span>.</span></em></p></div></div></div>
  </body>
</html>
