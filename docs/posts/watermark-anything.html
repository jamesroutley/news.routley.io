<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/watermark-anything">Original</a>
    <h1>Watermark Anything</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Implementation and pretrained models for the paper <a href="https://arxiv.org/abs/ARXIV_LINK" rel="nofollow"><strong>Watermark Anything</strong></a>.
Our approach allows for embedding (possibly multiple) localized watermarks into images.</p>

<p dir="auto">[<a href="https://arxiv.org/abs/2411.07231" rel="nofollow"><code>arXiv</code></a>]
[<a href="https://colab.research.google.com/github/facebookresearch/watermark-anything/blob/main/notebooks/colab.ipynb" rel="nofollow"><code>Colab</code></a>]
[<a href="https://notebooklm.google.com/notebook/6c69b3f8-b1a6-41c4-92fb-c416903ceb49/audio" rel="nofollow"><code>Podcast</code></a>]</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://jvns.ca/facebookresearch/watermark-anything/blob/main/assets/splash_wam.jpg"><img src="https://jvns.ca/facebookresearch/watermark-anything/raw/main/assets/splash_wam.jpg" alt="Watermark Anything Overview"/></a></p>


<p dir="auto">This repos was tested with Python 3.10.14, PyTorch 2.5.1, CUDA 12.4, Torchvision 0.20.1:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n &#34;watermark_anything&#34; python=3.10.14
conda activate watermark_anything
conda install pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia"><pre>conda create -n <span><span>&#34;</span>watermark_anything<span>&#34;</span></span> python=3.10.14
conda activate watermark_anything
conda install pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia</pre></div>
<p dir="auto">Install the required packages:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>

<p dir="auto">Download the pre-trained model weights <a href="https://dl.fbaipublicfiles.com/watermark_anything/checkpoint.pth" rel="nofollow">here</a>, or via command line:</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://dl.fbaipublicfiles.com/watermark_anything/checkpoint.pth -P checkpoints/ -P checkpoints/"><pre>wget https://dl.fbaipublicfiles.com/watermark_anything/checkpoint.pth -P checkpoints/ -P checkpoints/</pre></div>

<p dir="auto">For training our models we use the <a href="https://cocodataset.org/#home" rel="nofollow">COCO</a> dataset, with additional safety filters and where faces are blurred.</p>

<p dir="auto">See <code>notebooks/inference.ipynb</code> for a notebook with the following scripts as well as vizualizations.</p>
<details>
<summary>Imports, load model and specify folder with images to watermark:</summary>
</details>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p dir="auto">You can specify the <code>wam.scaling_w</code> factor, which controls the imperceptibility/robustness trade-off. Increasing it will lead to worse images but more robust watermarks, and vice versa.
By default, it is set to 2.0, feel free to increase or decrease it to test how it influences the metrics.</p>
</div>

<p dir="auto">Example of script for watermark embedding, detection and decoding for one message:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Define a 32-bit message to be embedded into the images
wm_msg = torch.randint(0, 2, (32,)).float().to(device)

# Proportion of the image to be watermarked (0.5 means 50% of the image).
# This is used here to show the watermark localization property. In practice, you may want to use a predifined mask or the entire image.
proportion_masked = 0.5

# Iterate over each image in the directory
for img_ in os.listdir(img_dir):
    # Load and preprocess the image
    img_path = os.path.join(img_dir, img_)
    img = Image.open(img_path).convert(&#34;RGB&#34;)
    img_pt = default_transform(img).unsqueeze(0).to(device)  # [1, 3, H, W]
    
    # Embed the watermark message into the image
    outputs = wam.embed(img_pt, wm_msg)

    # Create a random mask to watermark only a part of the image
    mask = create_random_mask(img_pt, num_masks=1,mask_percentage=proportion_masked)  # [1, 1, H, W]
    img_w = outputs[&#39;imgs_w&#39;] * mask + img_pt * (1 - mask)  # [1, 3, H, W]

    # Detect the watermark in the watermarked image
    preds = wam.detect(img_w)[&#34;preds&#34;]  # [1, 33, 256, 256]
    mask_preds = F.sigmoid(preds[:, 0, :, :])  # [1, 256, 256], predicted mask
    bit_preds = preds[:, 1:, :, :]  # [1, 32, 256, 256], predicted bits
    
    # Predict the embedded message and calculate bit accuracy
    pred_message = msg_predict_inference(bit_preds, mask_preds).cpu().float()  # [1, 32]
    bit_acc = (pred_message == wm_msg).float().mean().item()

    # Save the watermarked image and the detection mask
    mask_preds_res = F.interpolate(mask_preds.unsqueeze(1), size=(img_pt.shape[-2], img_pt.shape[-1]), mode=&#34;bilinear&#34;, align_corners=False)  # [1, 1, H, W]
    save_image(unnormalize_img(img_w), f&#34;{output_dir}/{img_}_wm.png&#34;)
    save_image(mask_preds_res, f&#34;{output_dir}/{img_}_pred.png&#34;)
    save_image(mask, f&#34;{output_dir}/{img_}_target.png&#34;)
    
    # Print the predicted message and bit accuracy for each image
    print(f&#34;Predicted message for image {img_}: &#34;, pred_message[0].numpy())
    print(f&#34;Bit accuracy for image {img_}: &#34;, bit_acc)"><pre><span># Define a 32-bit message to be embedded into the images</span>
<span>wm_msg</span> <span>=</span> <span>torch</span>.<span>randint</span>(<span>0</span>, <span>2</span>, (<span>32</span>,)).<span>float</span>().<span>to</span>(<span>device</span>)

<span># Proportion of the image to be watermarked (0.5 means 50% of the image).</span>
<span># This is used here to show the watermark localization property. In practice, you may want to use a predifined mask or the entire image.</span>
<span>proportion_masked</span> <span>=</span> <span>0.5</span>

<span># Iterate over each image in the directory</span>
<span>for</span> <span>img_</span> <span>in</span> <span>os</span>.<span>listdir</span>(<span>img_dir</span>):
    <span># Load and preprocess the image</span>
    <span>img_path</span> <span>=</span> <span>os</span>.<span>path</span>.<span>join</span>(<span>img_dir</span>, <span>img_</span>)
    <span>img</span> <span>=</span> <span>Image</span>.<span>open</span>(<span>img_path</span>).<span>convert</span>(<span>&#34;RGB&#34;</span>)
    <span>img_pt</span> <span>=</span> <span>default_transform</span>(<span>img</span>).<span>unsqueeze</span>(<span>0</span>).<span>to</span>(<span>device</span>)  <span># [1, 3, H, W]</span>
    
    <span># Embed the watermark message into the image</span>
    <span>outputs</span> <span>=</span> <span>wam</span>.<span>embed</span>(<span>img_pt</span>, <span>wm_msg</span>)

    <span># Create a random mask to watermark only a part of the image</span>
    <span>mask</span> <span>=</span> <span>create_random_mask</span>(<span>img_pt</span>, <span>num_masks</span><span>=</span><span>1</span>,<span>mask_percentage</span><span>=</span><span>proportion_masked</span>)  <span># [1, 1, H, W]</span>
    <span>img_w</span> <span>=</span> <span>outputs</span>[<span>&#39;imgs_w&#39;</span>] <span>*</span> <span>mask</span> <span>+</span> <span>img_pt</span> <span>*</span> (<span>1</span> <span>-</span> <span>mask</span>)  <span># [1, 3, H, W]</span>

    <span># Detect the watermark in the watermarked image</span>
    <span>preds</span> <span>=</span> <span>wam</span>.<span>detect</span>(<span>img_w</span>)[<span>&#34;preds&#34;</span>]  <span># [1, 33, 256, 256]</span>
    <span>mask_preds</span> <span>=</span> <span>F</span>.<span>sigmoid</span>(<span>preds</span>[:, <span>0</span>, :, :])  <span># [1, 256, 256], predicted mask</span>
    <span>bit_preds</span> <span>=</span> <span>preds</span>[:, <span>1</span>:, :, :]  <span># [1, 32, 256, 256], predicted bits</span>
    
    <span># Predict the embedded message and calculate bit accuracy</span>
    <span>pred_message</span> <span>=</span> <span>msg_predict_inference</span>(<span>bit_preds</span>, <span>mask_preds</span>).<span>cpu</span>().<span>float</span>()  <span># [1, 32]</span>
    <span>bit_acc</span> <span>=</span> (<span>pred_message</span> <span>==</span> <span>wm_msg</span>).<span>float</span>().<span>mean</span>().<span>item</span>()

    <span># Save the watermarked image and the detection mask</span>
    <span>mask_preds_res</span> <span>=</span> <span>F</span>.<span>interpolate</span>(<span>mask_preds</span>.<span>unsqueeze</span>(<span>1</span>), <span>size</span><span>=</span>(<span>img_pt</span>.<span>shape</span>[<span>-</span><span>2</span>], <span>img_pt</span>.<span>shape</span>[<span>-</span><span>1</span>]), <span>mode</span><span>=</span><span>&#34;bilinear&#34;</span>, <span>align_corners</span><span>=</span><span>False</span>)  <span># [1, 1, H, W]</span>
    <span>save_image</span>(<span>unnormalize_img</span>(<span>img_w</span>), <span>f&#34;<span><span>{</span><span>output_dir</span><span>}</span></span>/<span><span>{</span><span>img_</span><span>}</span></span>_wm.png&#34;</span>)
    <span>save_image</span>(<span>mask_preds_res</span>, <span>f&#34;<span><span>{</span><span>output_dir</span><span>}</span></span>/<span><span>{</span><span>img_</span><span>}</span></span>_pred.png&#34;</span>)
    <span>save_image</span>(<span>mask</span>, <span>f&#34;<span><span>{</span><span>output_dir</span><span>}</span></span>/<span><span>{</span><span>img_</span><span>}</span></span>_target.png&#34;</span>)
    
    <span># Print the predicted message and bit accuracy for each image</span>
    <span>print</span>(<span>f&#34;Predicted message for image <span><span>{</span><span>img_</span><span>}</span></span>: &#34;</span>, <span>pred_message</span>[<span>0</span>].<span>numpy</span>())
    <span>print</span>(<span>f&#34;Bit accuracy for image <span><span>{</span><span>img_</span><span>}</span></span>: &#34;</span>, <span>bit_acc</span>)</pre></div>

<details>
<summary>Example of script for watermark embedding, detection and decoding for multiple messages:</summary>
</details>


<p dir="auto">Pretraining for robustness:</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --nproc_per_node=2  train.py \
    --local_rank -1  --output_dir &lt;PRETRAINING_OUTPUT_DIRECTORY&gt; \
    --augmentation_config configs/all_augs.yaml --extractor_model sam_base --embedder_model vae_small \
    --img_size 256 --batch_size 16 --batch_size_eval 32 --epochs 300 \
    --optimizer &#34;AdamW,lr=5e-5&#34; --scheduler &#34;CosineLRScheduler,lr_min=1e-6,t_initial=300,warmup_lr_init=1e-6,warmup_t=10&#34; \
    --seed 42 --perceptual_loss none --lambda_i 0.0 --lambda_d 0.0 --lambda_w 1.0 --lambda_w2 10.0 \
    --nbits 32 --scaling_i 1.0 --scaling_w 0.3 \
    --train_dir &lt;COCO_TRAIN_DIRECTORY_PATH&gt; --train_annotation_file &lt;COCO_TRAIN_ANNOTATION_FILE_PATH&gt; \
    --val_dir &lt;COCO_VALIDATION_DIRECTORY_PATH&gt; --val_annotation_file &lt;COCO_VALIDATION_ANNOTATION_FILE_PATH&gt; "><pre>torchrun --nproc_per_node=<span>2</span>  train.py \
    --local_rank <span>-1</span>  --output_dir <span>&lt;</span>PRETRAINING_OUTPUT_DIRECTORY<span>&gt;</span> \
    --augmentation_config configs/all_augs.yaml --extractor_model sam_base --embedder_model vae_small \
    --img_size <span>256</span> --batch_size <span>16</span> --batch_size_eval <span>32</span> --epochs <span>300</span> \
    --optimizer <span><span>&#34;</span>AdamW,lr=5e-5<span>&#34;</span></span> --scheduler <span><span>&#34;</span>CosineLRScheduler,lr_min=1e-6,t_initial=300,warmup_lr_init=1e-6,warmup_t=10<span>&#34;</span></span> \
    --seed <span>42</span> --perceptual_loss none --lambda_i 0.0 --lambda_d 0.0 --lambda_w 1.0 --lambda_w2 10.0 \
    --nbits <span>32</span> --scaling_i 1.0 --scaling_w 0.3 \
    --train_dir <span>&lt;</span>COCO_TRAIN_DIRECTORY_PATH<span>&gt;</span> --train_annotation_file <span>&lt;</span>COCO_TRAIN_ANNOTATION_FILE_PATH<span>&gt;</span> \
    --val_dir <span>&lt;</span>COCO_VALIDATION_DIRECTORY_PATH<span>&gt;</span> --val_annotation_file <span>&lt;</span>COCO_VALIDATION_ANNOTATION_FILE_PATH<span>&gt;</span> </pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Finetuning for Multiple Watermarks and Imperceptibility</h3><a id="user-content-finetuning-for-multiple-watermarks-and-imperceptibility" aria-label="Permalink: Finetuning for Multiple Watermarks and Imperceptibility" href="#finetuning-for-multiple-watermarks-and-imperceptibility"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Finetuning the model for handling multiple watermarks and ensuring imperceptibility:</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --nproc_per_node=8 train.py \
    --local_rank 0 --debug_slurm --output_dir &lt;FINETUNING_OUTPUT_DIRECTORY&gt;\
    --augmentation_config configs/all_augs_multi_wm.yaml --extractor_model sam_base --embedder_model vae_small \
    --resume_from &lt;PRETRAINING_OUTPUT_DIRECTORY&gt;/checkpoint.pth \
    --attenuation jnd_1_3_blue --img_size 256 --batch_size 8 --batch_size_eval 16 --epochs 200 \
    --optimizer &#34;AdamW,lr=1e-4&#34; --scheduler &#34;CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5&#34; \
    --seed 42 --perceptual_loss none --lambda_i 0 --lambda_d 0 --lambda_w 1.0 --lambda_w2 6.0 \
    --nbits 32 --scaling_i 1.0 --scaling_w 2.0 --multiple_w 1 --roll_probability 0.2 \
    --train_dir &lt;COCO_TRAIN_DIRECTORY_PATH&gt; --train_annotation_file &lt;COCO_TRAIN_ANNOTATION_FILE_PATH&gt; \
    --val_dir &lt;COCO_VALIDATION_DIRECTORY_PATH&gt; --val_annotation_file &lt;COCO_VALIDATION_ANNOTATION_FILE_PATH&gt;"><pre>torchrun --nproc_per_node=<span>8</span> train.py \
    --local_rank <span>0</span> --debug_slurm --output_dir <span>&lt;</span>FINETUNING_OUTPUT_DIRECTORY<span>&gt;</span>\
    --augmentation_config configs/all_augs_multi_wm.yaml --extractor_model sam_base --embedder_model vae_small \
    --resume_from <span>&lt;</span>PRETRAINING_OUTPUT_DIRECTORY<span>&gt;</span>/checkpoint.pth \
    --attenuation jnd_1_3_blue --img_size <span>256</span> --batch_size <span>8</span> --batch_size_eval <span>16</span> --epochs <span>200</span> \
    --optimizer <span><span>&#34;</span>AdamW,lr=1e-4<span>&#34;</span></span> --scheduler <span><span>&#34;</span>CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5<span>&#34;</span></span> \
    --seed <span>42</span> --perceptual_loss none --lambda_i <span>0</span> --lambda_d <span>0</span> --lambda_w 1.0 --lambda_w2 6.0 \
    --nbits <span>32</span> --scaling_i 1.0 --scaling_w 2.0 --multiple_w <span>1</span> --roll_probability 0.2 \
    --train_dir <span>&lt;</span>COCO_TRAIN_DIRECTORY_PATH<span>&gt;</span> --train_annotation_file <span>&lt;</span>COCO_TRAIN_ANNOTATION_FILE_PATH<span>&gt;</span> \
    --val_dir <span>&lt;</span>COCO_VALIDATION_DIRECTORY_PATH<span>&gt;</span> --val_annotation_file <span>&lt;</span>COCO_VALIDATION_ANNOTATION_FILE_PATH<span>&gt;</span></pre></div>

<p dir="auto">The model is licensed under the <a href="https://jvns.ca/facebookresearch/watermark-anything/blob/main/LICENSE">CC-BY-NC</a>.</p>

<p dir="auto">See <a href="https://jvns.ca/facebookresearch/watermark-anything/blob/main/.github/CONTRIBUTING.md">contributing</a> and the <a href="https://jvns.ca/facebookresearch/watermark-anything/blob/main/.github/CODE_OF_CONDUCT.md">code of conduct</a>.</p>

<ul dir="auto">
<li><a href="https://github.com/facebookresearch/audioseal"><strong>AudioSeal</strong></a></li>
<li><a href="https://github.com/facebookresearch/segment-anything/"><strong>Segment Anything</strong></a></li>
</ul>

<p dir="auto">If you find this repository useful, please consider giving a star ‚≠ê and please cite as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{sander2024watermark,
  title={Watermark Anything with Localized Messages},
  author={Sander, Tom and Fernandez, Pierre and Durmus, Alain and Furon, Teddy and Douze, Matthijs},
  journal={arXiv preprint arXiv:2411.07231},
  year={2024}
}"><pre><span>@article</span>{<span>sander2024watermark</span>,
  <span>title</span>=<span><span>{</span>Watermark Anything with Localized Messages<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Sander, Tom and Fernandez, Pierre and Durmus, Alain and Furon, Teddy and Douze, Matthijs<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2411.07231<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2024<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
