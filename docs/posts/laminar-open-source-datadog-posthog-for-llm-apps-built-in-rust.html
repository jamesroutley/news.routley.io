<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lmnr-ai/lmnr">Original</a>
    <h1>Show HN: Laminar – Open-Source DataDog &#43; PostHog for LLM Apps, Built in Rust</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://www.ycombinator.com/companies/laminar-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/3bf938994198a5b1d850adab39ba81e5675045b3b2b496b9856ce7b833eae93a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5332342d6f72616e6765" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-S24-orange"/></a>
<a href="https://x.com/lmnrai" rel="nofollow"><img src="https://camo.githubusercontent.com/7217689996b50018699ee10564c16fa62744b484f8ab968cfcd2b4b992212b15/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c6d6e726169" alt="X (formerly Twitter) Follow" data-canonical-src="https://img.shields.io/twitter/follow/lmnrai"/></a>
<a href="https://discord.gg/nNFUUDAKub" rel="nofollow"> <img src="https://camo.githubusercontent.com/3c567c02e658b3bbc878a42c214883dc7706c5206ceea744dd66138c5b9e348f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e5f446973636f72642d3436343634363f266c6f676f3d646973636f7264266c6f676f436f6c6f723d353836354632" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Join_Discord-464646?&amp;logo=discord&amp;logoColor=5865F2"/> </a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps.</h2><a id="user-content-laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps" aria-label="Permalink: Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps." href="#laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzUxMjAsIm5iZiI6MTcyNTUzNDgyMCwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMTM0MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU3ZmVlMjhlMTE5YzMyOTEyYjI1MzRmNmM1NTA2MDQ0MTg5ZTE1MzQ0MTZmYWZiZDBmYTFmMjVhMzU1NjQ5NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.iODu2PndInzIWsVXH_fNWc1OSfeYZPLxx0V4_8TAxsY"><img width="1439" alt="traces" src="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzUxMjAsIm5iZiI6MTcyNTUzNDgyMCwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMTM0MFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU3ZmVlMjhlMTE5YzMyOTEyYjI1MzRmNmM1NTA2MDQ0MTg5ZTE1MzQ0MTZmYWZiZDBmYTFmMjVhMzU1NjQ5NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.iODu2PndInzIWsVXH_fNWc1OSfeYZPLxx0V4_8TAxsY"/></a>
<p dir="auto">Think of it as DataDog + PostHog for LLM apps.</p>
<ul dir="auto">
<li>OpenTelemetry-based instrumentation: automatic for LLM / vector DB calls with just 2 lines of code + decorators to track functions (powered by an amazing <a href="https://github.com/traceloop/openllmetry">OpenLLMetry</a> open-source package by TraceLoop).</li>
<li>Semantic events-based analytics. Laminar hosts background job queues of LLM pipelines. Outputs of those pipelines are turned into metrics. For example, you can design a pipeline which extracts &#34;my AI drive-through agent made an upsell&#34; data, and track this metric in Laminar.</li>
<li>Built for scale with a modern stack: written in Rust, RabbitMQ for message queue, Postgres for data, Clickhouse for analytics</li>
<li>Insightful, fast dashboards for traces / spans / events</li>
</ul>
<p dir="auto">Read the <a href="https://docs.lmnr.ai" rel="nofollow">docs</a>.</p>
<p dir="auto">This is a work in progress repo and it will be frequently updated.</p>


<p dir="auto">The easiest way to get started is with a generous free tier on our managed platform -&gt; <a href="https://www.lmnr.ai" rel="nofollow">lmnr.ai</a></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Self-hosting with Docker compose</h3><a id="user-content-self-hosting-with-docker-compose" aria-label="Permalink: Self-hosting with Docker compose" href="#self-hosting-with-docker-compose"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Start local version with docker compose.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:lmnr-ai/lmnr
cd lmnr
docker compose up"><pre>git clone git@github.com:lmnr-ai/lmnr
<span>cd</span> lmnr
docker compose up</pre></div>
<p dir="auto">This will spin up the following containers:</p>
<ul dir="auto">
<li>app-server – the core app logic, backend, and the LLM proxies</li>
<li>rabbitmq – message queue for sending the traces and observations reliably</li>
<li>qdrant – vector database</li>
<li>semantic-search-service – service for interacting with qdrant and embeddings</li>
<li>frontend – the visual front-end dashboard for interacting with traces</li>
<li>postgres – the database for all the application data</li>
<li>clickhouse – columnar OLAP database for more efficient event and trace analytics</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Instrumenting Python code</h3><a id="user-content-instrumenting-python-code" aria-label="Permalink: Instrumenting Python code" href="#instrumenting-python-code"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">First, create a project and generate a Project API Key. Then,</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install lmnr
echo &#34;LMNR_PROJECT_API_KEY=&lt;YOUR_PROJECT_API_KEY&gt;&#34; &gt;&gt; .env"><pre>pip install lmnr
<span>echo</span> <span><span>&#34;</span>LMNR_PROJECT_API_KEY=&lt;YOUR_PROJECT_API_KEY&gt;<span>&#34;</span></span> <span>&gt;&gt;</span> .env</pre></div>
<p dir="auto">To automatically instrument LLM calls of popular frameworks and LLM provider libraries just add</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
L.initialize(project_api_key=&#34;&lt;LMNR_PROJECT_API_KEY&gt;&#34;)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>&#34;&lt;LMNR_PROJECT_API_KEY&gt;&#34;</span>)</pre></div>
<p dir="auto">In addition to automatic instrumentation, we provide a simple <code>@observe()</code> decorator, if you want to trace inputs / outputs of functions</p>

<div dir="auto" data-snippet-clipboard-copy-content="import os
from openai import OpenAI

from lmnr import observe, Laminar as L
L.initialize(project_api_key=&#34;&lt;LMNR_PROJECT_API_KEY&gt;&#34;)

client = OpenAI(api_key=os.environ[&#34;OPENAI_API_KEY&#34;])

@observe()  # annotate all functions you want to trace
def poem_writer(topic=&#34;turbulence&#34;):
    prompt = f&#34;write a poem about {topic}&#34;
    response = client.chat.completions.create(
        model=&#34;gpt-4o&#34;,
        messages=[
            {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt},
        ],
    )
    poem = response.choices[0].message.content
    return poem

if __name__ == &#34;__main__&#34;:
    print(poem_writer(topic=&#34;laminar flow&#34;))"><pre><span>import</span> <span>os</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>

<span>from</span> <span>lmnr</span> <span>import</span> <span>observe</span>, <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>&#34;&lt;LMNR_PROJECT_API_KEY&gt;&#34;</span>)

<span>client</span> <span>=</span> <span>OpenAI</span>(<span>api_key</span><span>=</span><span>os</span>.<span>environ</span>[<span>&#34;OPENAI_API_KEY&#34;</span>])

<span>@<span>observe</span>()  <span># annotate all functions you want to trace</span></span>
<span>def</span> <span>poem_writer</span>(<span>topic</span><span>=</span><span>&#34;turbulence&#34;</span>):
    <span>prompt</span> <span>=</span> <span>f&#34;write a poem about <span><span>{</span><span>topic</span><span>}</span></span>&#34;</span>
    <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
        <span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>,
        <span>messages</span><span>=</span>[
            {<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;You are a helpful assistant.&#34;</span>},
            {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>prompt</span>},
        ],
    )
    <span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>
    <span>return</span> <span>poem</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>print</span>(<span>poem_writer</span>(<span>topic</span><span>=</span><span>&#34;laminar flow&#34;</span>))</pre></div>

<p dir="auto">You can send events in two ways:</p>
<ul dir="auto">
<li><code>.event(name, value)</code> – instant event with a value.</li>
<li><code>.evaluate_event(name, evaluator, data)</code> –  event that is evaluated by evaluator pipeline based on the data.</li>
</ul>
<p dir="auto">Note that to run an evaluate event, you need to crate an evaluator pipeline and create a target version for it.</p>
<p dir="auto">Laminar processes background job queues of pipeline processes and records outputs of pipelines as events.</p>
<p dir="auto">Read our <a href="https://docs.lmnr.ai" rel="nofollow">docs</a> to learn more about event types and how they are created and evaluated.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
# ...
poem = response.choices[0].message.content

# this will register True or False value with Laminar
L.event(&#34;topic alignment&#34;, topic in poem)

# this will run the pipeline `check_wordy` with `poem` set as the value
# of `text_input` node, and write the result as an event with name
# &#34;excessive_wordiness&#34;
L.evaluate_event(&#34;excessive_wordiness&#34;, &#34;check_wordy&#34;, {&#34;text_input&#34;: poem})"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span># ...</span>
<span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>

<span># this will register True or False value with Laminar</span>
<span>L</span>.<span>event</span>(<span>&#34;topic alignment&#34;</span>, <span>topic</span> <span>in</span> <span>poem</span>)

<span># this will run the pipeline `check_wordy` with `poem` set as the value</span>
<span># of `text_input` node, and write the result as an event with name</span>
<span># &#34;excessive_wordiness&#34;</span>
<span>L</span>.<span>evaluate_event</span>(<span>&#34;excessive_wordiness&#34;</span>, <span>&#34;check_wordy&#34;</span>, {<span>&#34;text_input&#34;</span>: <span>poem</span>})</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Laminar pipelines as prompt chain managers</h4><a id="user-content-laminar-pipelines-as-prompt-chain-managers" aria-label="Permalink: Laminar pipelines as prompt chain managers" href="#laminar-pipelines-as-prompt-chain-managers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can create Laminar pipelines in the UI and manage chains of LLM calls there.</p>
<p dir="auto">After you are ready to use your pipeline in your code, deploy it in Laminar by selecting the target version for the pipeline.</p>
<p dir="auto">Once your pipeline target is set, you can call it from Python in just a few lines.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L

L.initialize(&#39;&lt;YOUR_PROJECT_API_KEY&gt;&#39;)

result = l.run(
    pipeline = &#39;my_pipeline_name&#39;,
    inputs = {&#39;input_node_name&#39;: &#39;some_value&#39;},
    # all environment variables
    env = {&#39;OPENAI_API_KEY&#39;: &#39;sk-some-key&#39;},
)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>

<span>L</span>.<span>initialize</span>(<span>&#39;&lt;YOUR_PROJECT_API_KEY&gt;&#39;</span>)

<span>result</span> <span>=</span> <span>l</span>.<span>run</span>(
    <span>pipeline</span> <span>=</span> <span>&#39;my_pipeline_name&#39;</span>,
    <span>inputs</span> <span>=</span> {<span>&#39;input_node_name&#39;</span>: <span>&#39;some_value&#39;</span>},
    <span># all environment variables</span>
    <span>env</span> <span>=</span> {<span>&#39;OPENAI_API_KEY&#39;</span>: <span>&#39;sk-some-key&#39;</span>},
)</pre></div>

<p dir="auto">To learn more about instrumenting your code, check out our client libraries:</p>
<p dir="auto"><a href="https://www.npmjs.com/package/@lmnr-ai/lmnr" rel="nofollow"> <img src="https://camo.githubusercontent.com/6b3081997512b3addac3266d3dcaa06d9fe0cfdc9d34a7b64f56c68ee0e10398/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f2534306c6d6e722d61692532466c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d6e706d266c6f676f436f6c6f723d434233383337" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/%40lmnr-ai%2Flmnr?label=lmnr&amp;logo=npm&amp;logoColor=CB3837"/> </a>
<a href="https://pypi.org/project/lmnr/" rel="nofollow"> <img src="https://camo.githubusercontent.com/667df376d1224a1681f52ee5b358b52d73094911caecd6bcfa5fd55e6622df40/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d70797069266c6f676f436f6c6f723d333737354139" alt="PyPI - Version" data-canonical-src="https://img.shields.io/pypi/v/lmnr?label=lmnr&amp;logo=pypi&amp;logoColor=3775A9"/> </a></p>
<p dir="auto">To get deeper understanding of the concepts, follow on to the <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a> and <a href="https://docs.lmnr.ai/tutorials" rel="nofollow">tutorials</a>.</p>
</article></div></div>
  </body>
</html>
