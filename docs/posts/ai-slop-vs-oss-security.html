<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://devansh.bearblog.dev/ai-slop/">Original</a>
    <h1>AI Slop vs. OSS Security</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-11-03T17:57Z">
                    03 Nov, 2025
                </time>
            </i>
        </p>
    

    <blockquote>
<p>Disclosure: In previous version of this article, I used AI to refine my raw thoughts (as English isn&#39;t my first language), which is quite ironic given the topic of this article. I own up to it. The AI refinement somewhat took away the soul of what I was originally trying to convey. I have reverted it to the original version of my raw thoughts, so some mistakes or errors are likely. The only AI tool used for crafting this piece is Perplexity, for research purposes and not for generation of content.</p>
</blockquote>
<p>I have now spent almost a decade in the bug bounty industry, started out as a bug hunter (who initially used to submit reports with minimal impact, low-hanging fruits like RXSS, SQLi, CSRF, etc.), then moved on to complex chains involving OAuth, SAML, parser bugs, supply chain security issues, etc., and then became a vulnerability triager for HackerOne, where I have triaged/reviewed thousands of vulnerability submissions. I have now almost developed an instinct that tells me if a report is BS or a valid security concern just by looking at it. I have been at HackerOne for the last 5 years (Nov 2020 - Present), currently as a team lead, overseeing technical services with a focus on triage operations.</p>
<p>One decade of working on both sides, first as a bug hunter, and then on the receiving side reviewing bug submissions, has given me a unique vantage point on how the industry is fracturing under the weight of AI-generated bug reports (sometimes valid submissions, but most of the time, the issues are just plain BS). I have seen cases where it was almost impossible to determine whether a report was a hallucination or a real finding. Even my instincts and a decade of experience failed me, and this is honestly frustrating, not so much for me, because as part of the triage team, it is not my responsibility to fix vulnerabilities, but I do sympathize with maintainers of OSS projects whose inboxes are drowning. Bug bounty platforms have already started taking this problem seriously, as more and more OSS projects are complaining about it.</p>
<p>This is my personal writing space, so naturally, these are my personal views and observations. These views might be a byproduct of my professional experience gained at HackerOne, but in no way are they representative of my employer. I am sure HackerOne, as an organization, has its own perspectives, strategies, and positions on these issues. My analysis here just reflects my own thinking about the systemic problems I see and potential solutions(?).</p>
<hr/>
<h3 id="what-exactly-is-the-problem">What Exactly is the Problem?</h3><p>There are two kinds of AI-generated reports:</p>
<ul>
<li>AI-generated valid reports</li>
<li>AI-generated non-valid reports</li>
</ul>
<p>I call the latter “AI slop” and the first one is still fine, in my opinion. As long as the security report being submitted is technically valid, falls within scope, and demonstrates impact, even if it is written by AI, it is still acceptable. I see both kinds of reports on a daily basis, and I would much rather see researchers use AI to structure their thoughts. But leaving the “security research” and validation of the report to AI is where I draw the line. If you can’t reproduce what you are reporting, and your finding is based on an assumption spit out by an LLM, that’s what I call AI slop, and that’s the problem I’ll be discussing in this piece.</p>
<p>AI has infiltrated vulnerability reporting, and they mirror the social dynamics that plague any feedback system (be it mass swiping profiles on Hinge/tinder, or submitting mass job applications, or perhaps doing mass marketing/sales outreach)</p>
<p>A &#34;security researcher&#34; who just pastes LLM-generated output into a vulnerability submission form neither knows enough about the actual codebase being examined nor understands the security implications well enough to provide the insight that projects need. Each project has a different threat model. Something might be considered a valid vulnerability in Project A, but the behavior will be deemed acceptable in Project B. The AI doesn&#39;t know this difference, and it is very bad at understanding the threat-model context. It merely pattern-matches (like a regex, but not literally). It sees functions that look similar to vulnerable patterns and invents scenarios where they might be exploited, regardless of whether those scenarios are even possible in the actual implementation.</p>
<p>Some actors mass-submit AI-generated vulnerability reports, driven by incentives. The incentives are not always monetary, and sometimes take the form of a CVE. Industry has started treating CVEs as trophies. New-age startups are hunting for CVEs, as it looks nice to have them as a trophy, it gives them marketing, and is overall a good look for their startup and it&#39;s capabilities. I have no issues with the latter, as long as they are a net positive to the security space. We&#39;ll get to the CVE scene in a later section of this article. Coming to actors who mass-submit invalid AI-generated vulnerabilities (they feel productive and entrepreneurial?). Some genuinely believe their AI has found something real; others know it might not even be valid, but they submit it in the hopes maintainers will sort it out. The incentive (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>$</mi></mrow></math> + CVE) is to submit as many reports as possible and see what sticks, because even a 5% hit rate on a hundred submissions is better than the effort of manually verifying five findings.</p>
<p>As a result, <a href="https://daniel.haxx.se/blog/2025/07/14/death-by-a-thousand-slops/">Daniel Stenberg, who maintains curl</a>, is now seeing about 20% of all security submissions as AI-generated slop. The rate of genuine vulnerabilities has dropped to approximately 5%. This ratio is borderline scary. For every real vulnerability, there are now four fake ones. And every fake one consumes hours of expert time to disprove.</p>
<hr/>
<h3 id="human-capital-is-limited">Human Capital is Limited</h3><p>HackerOne platform has many OSS projects in scope of various Bug Bounty programs. Imagine yourself as one of the maintainers of one such OSS program. A security report lands in your inbox. It claims there&#39;s a buffer overflow in a specific function. Just by looking at the report, its content, and impact statement, it looks like something worth investigating. The report is well-formatted, also includes CVE-style nomenclature, and uses appropriate technical language. As a responsible maintainer, you can&#39;t just dismiss it. You alert your security team—volunteers, by the way, who have day jobs and families and maybe three hours a week for this work.</p>
<p>Three people read the report. One person even tries to reproduce the issue using the steps provided. They can&#39;t, because the steps reference test cases that don&#39;t exist. Another person examines the source code. The function mentioned in the report doesn&#39;t exist in that form. A third person checks whether there&#39;s any similar functionality that might be vulnerable in the way described. There isn&#39;t.</p>
<p>After an hour and a half of combined effort across three people, that&#39;s 4.5 person hours you just spent. You&#39;ve confirmed what you suspected, this report is garbage. Probably AI-generated garbage, based on the telltale signs of hallucinated function names and impossible attack vectors.</p>
<p>You close the report. You don&#39;t get those hours back. And tomorrow, two more reports just like it will arrive.</p>
<p><a href="https://daniel.haxx.se/blog/2025/07/14/death-by-a-thousand-slops/">The curl project has seven people on its security team</a>. They collaborate on every submission, with three to four members typically engaging with each report. In early July 2025, they were receiving approximately two security reports per week. The math is brutal. If you have three hours per week to contribute to an open source project you love, and a single false report consumes all of it, you&#39;ve contributed nothing that week except proving someone&#39;s AI hallucinated a vulnerability.</p>
<p>The emotional toll compounds exponentially. <a href="https://daniel.haxx.se/blog/2025/07/14/death-by-a-thousand-slops/">Stenberg describes it as &#34;mind-numbing stupidities&#34;</a> that the team must process. It&#39;s not just frustration, it&#39;s the specific demoralization that comes from having your expertise and goodwill systematically exploited by people who couldn&#39;t be bothered to verify their submissions before wasting your time.</p>
<hr/>
<h3 id="burnout-burnout-burnout">Burnout Burnout Burnout</h3><p>According to <a href="https://www.intel.com/content/www/us/en/developer/articles/community/maintainer-burnout-a-problem-what-are-we-to-do.html">Intel&#39;s annual open source community survey</a>, 45% of respondents identified maintainer burnout as their top challenge. <a href="https://www.sonarsource.com/blog/maintainer-burnout-is-real/">The Tidelift State of the Open Source Maintainer Survey</a> is even more stark: 58% of maintainers have either quit their projects entirely (22%) or seriously considered quitting (36%).</p>
<p>Why exactly are they quitting? The top reason, cited by 54% of maintainers, is that other things in their life and work took priority over open source contributions. Over half (51%) reported losing interest in the work. And 44% explicitly identified experiencing burnout.</p>
<p><a href="https://www.sonarsource.com/blog/maintainer-burnout-is-real/">The percentage of maintainers who said they weren&#39;t getting paid enough to make maintenance work worthwhile rose from 32% to 38%</a> between survey periods. These are people maintaining infrastructure that powers billions of dollars of commercial activity, and they&#39;re getting nothing. Or maybe they get $500 a year from GitHub Sponsors while companies make millions off their work.</p>
<p>The maintenance work itself is rarely rewarding. You&#39;re not building exciting new features. You&#39;re just addressing technical debt, responding to user demands, handling security issues, and now you have to sort through AI-generated garbage to find the occasional legitimate report as well.</p>
<p><em>When you&#39;re volunteering out of love in a market society, you&#39;re setting yourself up to be exploited.</em></p>
<p>And the exploitation is getting worse. Toxic communities, hyper-responsibility for critical infrastructure, and now the weaponization of AI to automate the creation of work for maintainers. This all is adding up to an unsustainable situation.</p>
<p>One Kubernetes contributor put it simply, <a href="https://www.intel.com/content/www/us/en/developer/articles/community/maintainer-burnout-a-problem-what-are-we-to-do.html">&#34;If your maintainers are burned out, they can&#39;t be protecting the code base like they&#39;re going to need to be.&#34;</a> This shifts maintainer wellbeing from a human resources problem into a security imperative. Burned-out maintainers miss things. They make mistakes. They eventually quit, leaving projects unmaintained or understaffed. Which eventually affects security of these projects.</p>
<hr/>
<h3 id="what-ai-slop-looks-like">What AI Slop Looks Like</h3><p>A typical AI slop report will reference function names that don&#39;t exist in the codebase. The AI has seen similar function names in its training data and invents somewhat valid sounding variations. It will describe operations, memory related stuff that would indeed be problematic if they existed as described in the report, but which bear no relationship to how the code actually works.</p>
<p><a href="https://gist.github.com/bagder/07f7581f6e3d78ef37dfbfc81fd1d1cd">One report to curl</a> claimed an HTTP/3 vulnerability and included fake function calls and behaviors that appeared nowhere in the actual codebase. Stenberg has publicly shared a <a href="https://gist.github.com/bagder/07f7581f6e3d78ef37dfbfc81fd1d1cd">list of AI-generated security submissions</a> received through HackerOne, and they all follow similar patterns, professional formatting, appropriate jargon, and completely fabricated technical details.</p>
<p>The sophistication varies. Some of these reports look absolute madness, while some look like P1 security issues, almost worthy of starting incident response. In most cases, both are BS, when examined closely.</p>
<p>Some reports are obviously generated by someone who just pasted a repository URL into ChatGPT and asked it to find vulnerabilities. Others show more effort, the submitter may have fed actual code snippets to the AI and then submitted its analysis without verification. Both are equally useless to maintainers, but the latter takes longer to disprove because the code snippets are real even if the vulnerability analysis is hallucinated.</p>
<p>LLMs usually fail in the context of security research, as they&#39;re designed to be helpful and provide positive responses. When you prompt an LLM to generate a vulnerability report, it will generate one regardless of whether a vulnerability exists. The model has no concept of truth—only. It assembles technical terminology into patterns that resemble security reports it has seen during training, but it cannot verify whether the claims it&#39;s making are accurate.</p>
<p><strong>AI can generate the form of security research without the substance.</strong></p>
<hr/>
<h3 id="the-cve-system-is-collapsing">The CVE System is Collapsing</h3><p>While AI slop is flooding individual project inboxes, <a href="https://autobahn-security.com/blog/cve-database-crisis/">the broader CVE infrastructure is facing its own existential crisis</a>. And these crises compound each other in dangerous ways.</p>
<p><a href="https://autobahn-security.com/blog/cve-database-crisis/">In April 2025, MITRE Corporation announced that its contract to maintain the Common Vulnerabilities and Exposures program would expire.</a> The Department of Homeland Security failed to renew the long-term contract, creating a funding lapse that affects everything: national vulnerability databases, advisories, tool vendors, and incident response operations.</p>
<p><a href="https://autobahn-security.com/blog/cve-database-crisis/">The National Vulnerability Database experienced catastrophic problems throughout 2024.</a> CVE submissions jumped 32% while creating massive processing delays. By March 2025, NVD had analyzed fewer than 300 CVEs, leaving more than 30,000 vulnerabilities backlogged. Approximately 42% of CVEs lack essential metadata like severity scores and product information.</p>
<p>Now layer AI slop onto this already stressed system. Invalid CVEs are being assigned at scale. <a href="https://lwn.net/Articles/944209/">A 2023 analysis by former insiders suggested that only around 20% of CVEs were valid, with the remainder being duplicates, invalid, or inflated.</a> The issues include multiple CVEs being assigned for the same bug, CNAs siding with reporters over project developers even when there&#39;s no genuine dispute, and reporters receiving CVEs based on test cases rather than actual distinct vulnerabilities.</p>
<p>The result is that the vulnerability tracking system everyone relies on is becoming less trustworthy exactly when we need it most. Security teams can&#39;t rely on CVE assignments to prioritize their work. Developers don&#39;t trust vulnerability scanners because false positive rates are through the roof. The signal-to-noise ratio has deteriorated so badly that the entire system risks becoming useless.</p>
<hr/>
<h3 id="what-doesnt-work">What Doesn&#39;t Work</h3><p>Banning submitters doesn&#39;t work at scale. You can ban an account, but creating new accounts is trivial. HackerOne implements reputation scoring where points are gained or lost based on report validity, but this hasn&#39;t stemmed the tide because the cost of creating throwaway accounts is essentially zero.</p>
<p>Asking people to &#34;please verify before submitting&#34; doesn&#39;t work. The incentive structure rewards volume, and people either genuinely believe their AI-generated reports are valid or don&#39;t care enough to verify. Polite requests assume good faith, but much of the slop comes from actors who have no stake in the community norms.</p>
<p>Trying to educate submitters about how AI works doesn&#39;t scale. For every person you educate, ten new ones appear with fresh GPT accounts. The problem isn&#39;t knowledge—it&#39;s incentives.</p>
<p>Simply closing inboxes or shutting down bug bounty programs &#34;works&#34; in the sense that it stops the slop, but it also stops legitimate security research. Several projects have done this, and now they&#39;re less secure because they&#39;ve lost a channel for responsible disclosure.</p>
<p>None of the easy answers work because this isn&#39;t an easy problem.</p>
<hr/>
<h3 id="what-might-actually-work">What Might Actually Work</h3><p>More and more programs should bring in mandatory disclosure requirements. This could act as the first line of defense against the AI slop. <a href="https://socket.dev/blog/django-joins-curl-in-pushing-back-on-ai-slop-security-reports">Both curl and Django now require submitters to disclose whether AI was used in generating reports.</a> <a href="https://opensourcesecurity.io/2025/2025-05-curl_vs_ai_with_daniel_stenberg/">Curl&#39;s approach is particularly direct: disclose AI usage upfront and ensure complete accuracy before submission.</a> If AI usage is disclosed, researcher could expect extensive follow-up questions demanding more rigorous proof that the bug is genuine before the team invests time in verification. This works psychologically. It forces submitters to acknowledge they&#39;re using AI, which makes them more conscious of their responsibility to verify. It also gives maintainers grounds to reject slop immediately if AI usage was undisclosed but becomes obvious during review. <a href="https://socket.dev/blog/django-joins-curl-in-pushing-back-on-ai-slop-security-reports">Django goes further with a section titled &#34;Note for AI Tools&#34; that directly addresses language models themselves, reiterating that the project expects no hallucinated content, no fictitious vulnerabilities, and a requirement to independently verify that reports describe reproducible security issues.</a></p>
<p>Raise the bar for what qualifies as a valid PoC. Require technical evidence such as screencasts showing reproducibility, integration or unit tests demonstrating the fault, docker setup for reproducing the bug, or complete reproduction steps with logs and source code, this will naturally make it much harder to submit slop. AI can generate a description of a vulnerability, but it cannot generate working exploit code for a vulnerability that doesn&#39;t exist. Raising the bar for what qualifies as a valid PoC will force the submitter to actually verify their claim. If they can&#39;t reproduce it, they can&#39;t prove it, and you don&#39;t waste time investigating.</p>
<p>Some sort of reputation and trust backed systems could also work as a social mechanism for filtering. Only users with a history of valid submissions get unrestricted reporting privileges or monetary bounties. New reporters could be required to have established community members vouch for them, creating a web-of-trust model. This mirrors how the world worked before bug bounty platforms commodified security research. The only downside is, it risks creating an insider club. But the upside is that it filters out low-effort actors who won&#39;t invest in building reputation.</p>
<p>Having some form of economic friction for submitting vulnerabilities. Charge a nominal refundable fee—say $50—for each submission from new or unproven users. If the report is valid, they get the fee back plus the bounty. If it&#39;s invalid, you keep the fee. This immediately makes mass AI submission uneconomical. If someone&#39;s submitting 50 AI-generated reports hoping one sticks, that&#39;s now $2,500 at risk. But for a legitimate researcher submitting one carefully verified finding, $50 is a trivial barrier that gets refunded anyway. Some projects are considering dropping monetary rewards entirely. The logic is that if there&#39;s no money involved, there&#39;s no incentive for speculative submissions. But this risks losing legitimate researchers who rely on bounties as income. It&#39;s a scorched earth approach that solves the slop problem by eliminating the entire ecosystem.</p>
<p>AI-Assisted Triage is also popping up as a solution, it is exactly like fighting fire with fire. Use AI tools trained specifically to identify AI-generated slop and flag it for immediate rejection. <a href="https://www.hackerone.com/blog/beyond-the-noise-hai-triage-insight-agent">HackerOne&#39;s Hai Triage system embodies this approach, using AI agents to cut through noise before human analysts validate findings.</a> The risk is, what if your AI filter rejects legitimate reports? What if it&#39;s biased against certain communication styles or methodologies? You&#39;ve just automated discrimination. But the counterargument is that human maintainers are already overwhelmed, and imperfect filtering is better than drowning.</p>
<p><strong>Make the slop public, it will build accountability</strong>. <a href="https://gist.github.com/bagder/07f7581f6e3d78ef37dfbfc81fd1d1cd">Curl recently formalized that all submitted security reports will be made public once reviewed and deemed non-sensitive.</a> This means that fabricated or misleading reports won&#39;t just be rejected, they&#39;ll be exposed to public scrutiny. This works as both deterrent (like a Wall of Shame) and educational tool. If you know your slop report will be publicly documented with your name attached, you might think twice. And when other researchers see examples of what doesn&#39;t constitute a valid report, they learn what standards they need to meet. The downside is that public shaming can be toxic and might discourage good-faith submissions from inexperienced researchers. Projects implementing this approach need to be careful about tone and focus on the technical content rather than attacking submitters personally.</p>
<hr/>
<h3 id="sustainability-is-hard">Sustainability is Hard</h3><p>Every hour spent evaluating slop reports is an hour not spent on features, documentation, or actual security improvements. And maintainers are already working for free, maintaining infrastructure that generates billions in commercial value. When 38% of maintainers cite not getting paid enough as a reason for quitting, and <a href="https://darktechinsights.com/open-source-hidden-costs-developers/">97% of open source maintainers are unpaid despite massive commercial exploitation of their work</a>, the system is already broken.</p>
<p>By definition of Open Source, you could argue that, since it is &#34;open&#34; it cannot be exploited, and maintainers should not expect monetary aid for the opensource work they are doing. I find this to be absolutely BS. AI slop is just the latest exploitation vector (and not the only one). It&#39;s the most visible one right now, but it&#39;s not the root cause. The root cause is that we&#39;ve built a global technology infrastructure on the volunteer labor of people who get nothing in return except burnout and harassment.</p>
<p>So what does sustainability actually look like?</p>
<p>I&#39;ll be blunt, it looks like money. Real money. Not GitHub Sponsors donations that average $500 a year. Not swag and conference tickets. Actual salaries commensurate with the value being created. Companies that build products on open source infrastructure need to fund the maintainers of that infrastructure. This could happen through direct employment, foundation grants, or the Open Source Pledge model where companies commit percentages of revenue.</p>
<p>Second, it looks like better tooling and automation THAT ACTUALLY WORKS, genuinely reduces workload rather than creating new forms of work. It could be anything, maybe an automated dependency management? continuous security scanning integrated into development workflows? or some kind of sophisticated triage assistance? The goal is to make maintenance less time consuming so burnout becomes less likely.</p>
<p>Third, it looks like shared workload and team building. No single volunteer should be a single point of failure. Building teams with checks and balances where members keep each other from taking on too much creates sustainability.</p>
<p>Fourth, it looks like culture change. Fostering empathy in interactions, starting communications with gratitude even when rejecting contributions, and publicly acknowledging the critical work maintainers perform reduces emotional toll.</p>
<p>Fifth, it looks like advocacy and policy at organizational and governmental levels. Recognition that <a href="https://tfir.io/kubernetes-maintainer-burnout-open-source-sustainability/">maintainer burnout represents existential threat to technology infrastructure</a>. Development of regulations requiring companies benefiting from open source to contribute resources. Establishment of security standards that account for the realities of volunteer-run projects.</p>
<p>I feel like without addressing these fundamentals, no amount of technical sophistication will prevent collapse.</p>
<hr/>
<h3 id="the-arms-race-ahead">The Arms Race Ahead</h3><p>The CVE slop crisis is just the beginning. We are just getting started and entering an arms race between AI-assisted attackers or abusers and AI-assisted defenders, and nobody knows how it ends.</p>
<p><a href="https://www.hackerone.com/press-release/hackerone-report-finds-210-spike-ai-vulnerability-reports-amid-rise-ai-autonomy">HackerOne&#39;s research indicates that 70% of security researchers now use AI tools in their workflow.</a> AI-powered testing is becoming the industry standard. <a href="https://www.hackerone.com/press-release/hackerone-report-finds-210-spike-ai-vulnerability-reports-amid-rise-ai-autonomy">The emergence of fully autonomous hackbots—AI systems that submitted over 560 valid reports in the first half of 2025—signals both opportunity and threat.</a></p>
<p>The divergence will be between researchers who use AI as a tool to enhance genuinely skilled work versus those who use it to automate low-effort spam. The former represents the promise of democratizing security research and scaling our ability to find vulnerabilities. The latter represents the threat of making the signal to noise problem completely unmanageable.</p>
<p>This probably means moving toward more exclusive models. Invite-only programs. Dramatically higher standards for participation. Reputation systems that take years to build. New models for coordinated vulnerability disclosure that assume AI-assisted research as the baseline and require proof beyond &#34;here&#39;s what the AI told me.&#34;</p>
<p>It might mean the end of open bug bounty programs as we know them. Maybe that&#39;s necessary. Maybe the experiment of &#34;anyone can submit anything&#34; was only viable when the cost of submitting was high enough to ensure some minimum quality. Now that AI has reduced that cost to near-zero, the experiment might fail soon if things don&#39;t improve.</p>
<p>So, net-net, here&#39;s where we are:</p>
<p>When it comes to vulnerability reports, what matters is who submits them and whether they&#39;ve actually verified their claims. Accepting reports from everyone indiscriminately is backfiring catastrophically because projects are latching onto submissions that sound plausible while ignoring the cumulative evidence that most are noise.</p>
<p>You want to receive reports from someone who has actually verified their claims, understands the architecture of what they&#39;re reporting on, and isn&#39;t trying to game the bounty system or offload verification work onto maintainers.</p>
<p>Such people exist, but they&#39;re becoming harder to find amidst the mountains of AI-generated content. That&#39;s why projects have to be selective about which reports they investigate and which submitters they trust.</p>
<p>Ultimately, the sustainability of open source security depends on recognizing that people who maintain critical infrastructure deserve more than exploitation.</p>
<p>They deserve compensation, support, reasonable expectations, and protection from abuse. Without addressing these fundamentals, no amount of technical sophistication will prevent the slow collapse of the collaborative model that has produced so much of the digital infrastructure modern life depends on.</p>
<p>The crisis isn&#39;t merely about bad vulnerability reports. It&#39;s about whether we&#39;ll choose to sustain the human foundation of technological progress, or whether we&#39;ll let it burn out under the weight of automated exploitation.</p>
<p>That&#39;s the choice we&#39;re facing. And right now, we&#39;re choosing wrong.</p>


    

    
        

        
            


        
    


  </div></div>
  </body>
</html>
