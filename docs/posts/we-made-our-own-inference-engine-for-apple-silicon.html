<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/trymirai/uzu">Original</a>
    <h1>Show HN: We made our own inference engine for Apple Silicon</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <img alt="Mirai" src="https://camo.githubusercontent.com/01de85e87697f7dc83e653d4955e09189c2f80f5112a2239453c911178e2179d/68747470733a2f2f6172746966616374732e7472796d697261692e636f6d2f736f6369616c2f6769746875622f757a752d6865616465722e6a7067" data-canonical-src="https://artifacts.trymirai.com/social/github/uzu-header.jpg"/>
  </picture></themed-picture>
</p>
<p dir="auto"><a href="https://artifacts.trymirai.com/social/about_us.mp3" rel="nofollow"><img src="https://camo.githubusercontent.com/376cef10029e3a5f2c65d1960616bda0add350d5c985b6799aaab081748b15ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c697374656e2d506f64636173742d726564" alt="Listen to our podcast" data-canonical-src="https://img.shields.io/badge/Listen-Podcast-red"/></a>
<a href="https://docsend.com/v/76bpr/mirai2025" rel="nofollow"><img src="https://camo.githubusercontent.com/9fbf0e520d3c9dada48e683cf63f4aa64e52d3a1aa5a7f44db9a4168254ce5a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f566965772d4465636b2d726564" alt="View our deck" data-canonical-src="https://img.shields.io/badge/View-Deck-red"/></a>
<a href="mailto:alexey@getmirai.co,dima@getmirai.co,aleksei@getmirai.co?subject=Interested%20in%20Mirai"><img src="https://camo.githubusercontent.com/72023ea3f5719fe79e2a703c2088ed2a8c5d01ea12f239a96d4ec8a8e626dd60/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53656e642d456d61696c2d677265656e" alt="Contact us" data-canonical-src="https://img.shields.io/badge/Send-Email-green"/></a>
<a href="https://docs.trymirai.com/components/inference-engine" rel="nofollow"><img src="https://camo.githubusercontent.com/6e7463eda6953beb56fc9442bb42d4935f57c7352ea9a5e347242bddfe945bb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f526561642d446f63732d626c7565" alt="Read docs" data-canonical-src="https://img.shields.io/badge/Read-Docs-blue"/></a>
<a href="https://www.evalapply.org/trymirai/uzu/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/371a602912faf7afd95fb8d7c0d8846e048157c4f50ae5ede617f56320429998/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c7565" alt="License" data-canonical-src="https://img.shields.io/badge/License-MIT-blue"/></a></p>

<p dir="auto">A high-performance inference engine for AI models on Apple Silicon. Key features:</p>
<ul dir="auto">
<li>Simple, high-level API</li>
<li><a href="https://docs.trymirai.com/components/inference-engine#before-we-start" rel="nofollow">Hybrid architecture</a>, where layers can be computed as GPU kernels or via MPSGraph (a low-level API beneath CoreML with <a href="https://trymirai.com/blog/iphone-hardware" rel="nofollow">ANE</a> access)</li>
<li>Unified model configurations, making it easy to add support for new models</li>
<li>Traceable computations to ensure correctness against the source-of-truth implementation</li>
<li>Utilizes unified memory on Apple devices</li>
</ul>

<p dir="auto">For a detailed explanation of the architecture, please refer to the <a href="https://docs.trymirai.com/components/inference-engine" rel="nofollow">documentation</a>.</p>

<p dir="auto"><code>uzu</code> uses its own model format. To export a specific model, use <a href="https://github.com/trymirai/lalamo">lalamo</a>. First, get the list of supported models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv run lalamo list-models"><pre>uv run lalamo list-models</pre></div>
<p dir="auto">Then, export the specific one:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv run lalamo convert meta-llama/Llama-3.2-1B-Instruct --precision float16"><pre>uv run lalamo convert meta-llama/Llama-3.2-1B-Instruct --precision float16</pre></div>
<p dir="auto">Alternatively, you can download a prepared model using the sample script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./scripts/download_test_model.sh $MODEL_PATH"><pre>./scripts/download_test_model.sh <span>$MODEL_PATH</span></pre></div>

<ul dir="auto">
<li><a href="https://github.com/trymirai/uzu-swift">uzu-swift</a> - a prebuilt Swift framework, ready to use with SPM</li>
</ul>

<p dir="auto">You can run <code>uzu</code> in a <a href="https://docs.trymirai.com/components/cli" rel="nofollow">CLI</a> mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --release -p cli -- help"><pre>cargo run --release -p cli -- <span>help</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="
Usage: uzu_cli [COMMAND]
​
Commands:
  run    Run a model with the specified path
  serve  Start a server with the specified model path
  help   Print this message or the help of the given subcommand(s)"><pre>Usage: uzu_cli [COMMAND]
​
Commands:
  run    Run a model with the specified path
  serve  Start a server with the specified model path
  <span>help</span>   Print this message or the <span>help</span> of the given subcommand(s)</pre></div>

<p dir="auto">First, add the <code>uzu</code> dependency to your <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
uzu = { git = &#34;https://github.com/trymirai/uzu&#34;, branch = &#34;main&#34;, package = &#34;uzu&#34; }"><pre>[<span>dependencies</span>]
<span>uzu</span> = { <span>git</span> = <span><span>&#34;</span>https://github.com/trymirai/uzu<span>&#34;</span></span>, <span>branch</span> = <span><span>&#34;</span>main<span>&#34;</span></span>, <span>package</span> = <span><span>&#34;</span>uzu<span>&#34;</span></span> }</pre></div>
<p dir="auto">Then, create an inference <code>Session</code> with a specific model and configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use std::path::PathBuf;
use uzu::{
    backends::metal::sampling_config::SamplingConfig,
    session::{
        session::Session, session_config::SessionConfig,
        session_input::SessionInput, session_output::SessionOutput,
        session_run_config::SessionRunConfig,
    },
};

fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let model_path = PathBuf::from(&#34;MODEL_PATH&#34;);
    
    let mut session = Session::new(model_path.clone())?;
    session.load_with_session_config(SessionConfig::default())?;

    let input = SessionInput::Text(&#34;Tell about London&#34;.to_string());

    let tokens_limit = 128;
    let run_config = SessionRunConfig::new_with_sampling_config(
        tokens_limit,
        Some(SamplingConfig::default())
    );

    let output = session.run(input, run_config, Some(|_: SessionOutput| {
        return true;
    }));
    println!(&#34;{}&#34;, output.text);
    Ok(())
}"><pre><span>use</span> std<span>::</span>path<span>::</span><span>PathBuf</span><span>;</span>
<span>use</span> uzu<span>::</span><span>{</span>
    backends<span>::</span>metal<span>::</span>sampling_config<span>::</span><span>SamplingConfig</span><span>,</span>
    session<span>::</span><span>{</span>
        session<span>::</span><span>Session</span><span>,</span> session_config<span>::</span><span>SessionConfig</span><span>,</span>
        session_input<span>::</span><span>SessionInput</span><span>,</span> session_output<span>::</span><span>SessionOutput</span><span>,</span>
        session_run_config<span>::</span><span>SessionRunConfig</span><span>,</span>
    <span>}</span><span>,</span>
<span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> model_path = <span>PathBuf</span><span>::</span><span>from</span><span>(</span><span>&#34;MODEL_PATH&#34;</span><span>)</span><span>;</span>
    
    <span>let</span> <span>mut</span> session = <span>Session</span><span>::</span><span>new</span><span>(</span>model_path<span>.</span><span>clone</span><span>(</span><span>)</span><span>)</span>?<span>;</span>
    session<span>.</span><span>load_with_session_config</span><span>(</span><span>SessionConfig</span><span>::</span><span>default</span><span>(</span><span>)</span><span>)</span>?<span>;</span>

    <span>let</span> input = <span>SessionInput</span><span>::</span><span>Text</span><span>(</span><span>&#34;Tell about London&#34;</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>let</span> tokens_limit = <span>128</span><span>;</span>
    <span>let</span> run_config = <span>SessionRunConfig</span><span>::</span><span>new_with_sampling_config</span><span>(</span>
        tokens_limit<span>,</span>
        <span>Some</span><span>(</span><span>SamplingConfig</span><span>::</span><span>default</span><span>(</span><span>)</span><span>)</span>
    <span>)</span><span>;</span>

    <span>let</span> output = session<span>.</span><span>run</span><span>(</span>input<span>,</span> run_config<span>,</span> <span>Some</span><span>(</span>|_<span>:</span> <span>SessionOutput</span>| <span>{</span>
        <span>return</span> <span>true</span><span>;</span>
    <span>}</span><span>)</span><span>)</span><span>;</span>
    <span>println</span><span>!</span><span>(</span><span>&#34;{}&#34;</span><span>,</span> output<span>.</span>text<span>)</span><span>;</span>
    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>

<p dir="auto">Here are the performance metrics for various models:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><code>Apple M2</code>, <code>tokens/s</code></th>
<th>Llama-3.2-1B-Instruct</th>
<th>Qwen2.5-1.5B-Instruct</th>
<th>Qwen3-0.6B</th>
<th>Qwen3-4B</th>
<th>R1-Distill-Qwen-1.5B</th>
<th>SmolLM2-1.7B-Instruct</th>
<th>Gemma-3-1B-Instruct</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>uzu</code></td>
<td>35.17</td>
<td>28.32</td>
<td>68.9</td>
<td>11.28</td>
<td>20.47</td>
<td>25.01</td>
<td>41.50</td>
</tr>
<tr>
<td><code>llama.cpp</code></td>
<td>32.48</td>
<td>25.85</td>
<td>5.37</td>
<td>1.08</td>
<td>2.81</td>
<td>23.74</td>
<td>37.68</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto">Note that all performance comparisons were done using bf16/f16 precision. Comparing quantized models isn&#39;t entirely fair, as different engines use different quantization approaches. For running llama.cpp, we used LM Studio (v0.3.17, Metal llama.cpp runtime v1.39.0). It&#39;s also worth mentioning that using the <code>release</code> build profile is crucial for obtaining the most accurate performance metrics.</p>
</blockquote>

<p dir="auto">This project is licensed under the MIT License. See the <a href="https://www.evalapply.org/trymirai/uzu/blob/main/LICENSE">LICENSE</a> file for details.</p>
</article></div></div>
  </body>
</html>
