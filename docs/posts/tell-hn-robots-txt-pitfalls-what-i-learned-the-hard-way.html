<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=41960003">Original</a>
    <h1>Tell HN: Robots.txt pitfalls – what I learned the hard way</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>This applies to sites indexed on Google that hope to gain organic traffic. As an indie blogger and SEO enthusiast, I foolishly updated my robots.txt file to prevent indexing of certain unwanted parts of my site, leading to subtle repercussions that I couldn&#39;t have foreseen.</p><p>A few days ago, while reading about SEO, I came across the concept of a &#34;crawl budget.&#34; Apparently, Google allocates a specific crawl budget to your indexed site, and the more useless content it has to index and store on its servers, the more it affects your site—resulting in delays for new content indexing, favicon updates, and robots.txt crawling.</p><p>Being a minimalist and utilitarian, I decided to prevent indexing of the `/uploads/` directory on my site since it mostly contained images used in my articles. I thought blocking this &#34;useless content&#34; would free up more crawling budget for my primary content, i.e., articles. So, I added this directory to my site&#39;s robots.txt:</p><pre><code>  # Group 1
  User-agent: *
  Disallow: /public/
  Disallow: /drafts/
  Disallow: /theme/
  Disallow: /page*
  Disallow: /uploads/

  Sitemap: https://prahladyeri.github.io/sitemap.xml

</code></pre><p>
The way search engines work means there&#39;s typically a 5-7 day gap between updating the robots.txt file and crawlers processing it. After about a week, I noticed that my site&#39;s favicon disappeared from SERPs on mobile browsers! Instead, there was a bland (empty) icon in its place. That’s when I realized that my favicons also resided in the `/uploads/` directory. After I recently optimized the favicon format by switching from WEBP to PNG, Google was unable to crawl and index the new favicon at all!</p><p>Once I realized this mistake, I removed the blocking of `/uploads/` from the robots.txt and requested a recrawl. But who knows how long it will take for Google&#39;s systems to sync this change and start showing the site&#39;s favicon back in SERPs! Two lessons learned:</p><p>1. The robots.txt file is highly sensitive; avoid modifying it if possible.
2. Applying SEO is like steering an extremely large ship or vessel. You pull a lever now, and the ship only moves after several days!</p></div></div></div>
  </body>
</html>
