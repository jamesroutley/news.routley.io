<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/">Original</a>
    <h1>Maximizing the Potential of LLMs: A Guide to Prompt Engineering</h1>
    
    <div id="readability-page-1" class="page"><div>
        






<article>
  <h2>Maximizing the Potential of LLMs: A Guide to Prompt Engineering</h2>
  <share-button>
    
    </share-button><a href="https://www.ruxu.dev/articles/ai"><span>Artificial Intelligence</span></a>
  
  <figure>
<img alt="OpenAI" src="https://www.ruxu.dev/articles/ai/maximizing-the-potential-of-llms/assets/images/openai.png"/>
</figure>
<p>Language models have rapidly improved in recent years, with large language models (LLMs) such as GPT-3 and GPT-4 taking center stage. These models have become popular due to their ability to perform a great variety of tasks with incredible skill. Also, as the number of parameters of these models (in the billions!) has increased, these models have unpredictably gained new abilities.</p>
<p>In this article, we will explore LLMs, the tasks they can perform, their shortcomings, and various prompt engineering strategies.</p>
<h3 id="what-are-llms" tabindex="-1">What are LLMs?</h3>
<p>LLMs are neural networks that have been trained on vast amounts of text data. The training process allows the models to learn patterns in the text, including grammar, syntax, and word associations. The models use these learned patterns to generate human-like text, making them ideal for natural language processing (NLP) tasks.</p>
<h3 id="which-llms-are-available" tabindex="-1">Which LLMs are available?</h3>
<p>There are several LLMs available, with GPT-4  being the most popular. Other models include LLaMA, PaLM, BERT, and T5. Each model has its strengths and weaknesses, some of them are open and others are closed and only usable via API.</p>
<h3 id="shortcomings-of-llms" tabindex="-1">Shortcomings of LLMs</h3>
<p>Despite their impressive performance, LLMs have several limitations. One significant drawback is their inability to reason beyond the information provided in the prompt. Additionally, LLMs can generate biased text based on the data they were trained on. It is also challenging to control the output of LLMs, making it necessary to use prompt engineering strategies to achieve the desired output.</p>
<h2 id="which-tasks-can-you-perform" tabindex="-1">Which tasks can you perform?</h2>
<p>We can direct LLMs to perform specific tasks for us by formatting the prompts for each case. Here&#39;s a list of those tasks and prompt templates and examples for each.</p>
<h3 id="text-summarization" tabindex="-1">Text Summarization</h3>
<p>LLMs can generate summaries of long texts, making it easier to understand and digest the content. We can ask to summarize a text with the following prompt template:</p>
<p><code>Template</code></p>
<pre><code>&lt;Full text&gt;</code></pre>
<p>For example, we can ask to summarize this text with the following prompt:</p>
<p><code>Prompt</code></p>
<pre><code>In a mechanical watch the watch&#39;s gears are turned by a spiral spring called a mainspring. In a manual watch energy is stored in the mainspring by turning a knob, the crown on the side of the watch. Then the energy from the mainspring powers the watch movement until it runs down, requiring the spring to be wound again.</code></pre>
<p>Which gives the following output:</p>
<p><code>Output</code></p>
<pre><code>A mechanical watch uses a mainspring to power the movement of the watch&#39;s gears. In a manual watch, the mainspring is wound by turning a knob, while a self-winding watch uses the natural motion of the wearer&#39;s body to wind the mainspring through an oscillating weight and ratcheted winding mechanism.</code></pre>

<p>LLMs can identify and extract specific information from a large body of text. This is useful in fields such as journalism and business intelligence. The following template instructs the model to extract information:</p>
<p><code>Template</code></p>
<pre><code>&lt;Full text&gt;</code></pre>
<p>For example, we can ask to extract the names of places from the following text:</p>
<p><code>Prompt</code></p>
<pre><code>The earliest credible evidence for a successful design is the watch made by the Swiss watchmaker Abraham-Louis Perrelet, who lived in Le Locle. In late 1776 or early 1777, he invented a self-winding mechanism for pocket watches using an oscillating weight inside the watch that moved up and down. The Geneva Society of Arts, reporting on this watch in 1777, stated that 15 minutes walking was necessary to fully wind the watch.</code></pre>
<p>And the output is:</p>
<p><code>Output</code></p>
<pre><code>Le Locle, Geneva.</code></pre>
<h3 id="question-answering" tabindex="-1">Question Answering</h3>
<p>LLMs can also answer questions based on the information provided in the prompt. The following template can be used to direct the LLM to answer a question:</p>
<p><code>Template</code></p>
<pre><code>Answer the question based on the context below. &lt;Other instructions&gt;. Respond &#34;Not sure&#34; if not sure about the answer.</code></pre>
<p>For example, with the following prompt:</p>
<p><code>Prompt</code></p>
<pre><code>Answer the question based on the context below. Answer in a single sentence. Respond &#34;Not sure&#34; if not sure about the answer.</code></pre>
<p>And ChatGPT responds with:</p>
<p><code>Output</code></p>
<pre><code>No, automatic watches do not need to be manually wound if worn enough because the natural motion of the wearer provides energy to wind the mainspring.</code></pre>
<h3 id="text-classification" tabindex="-1">Text Classification</h3>
<p>LLMs can classify text into predefined categories, such as sentiment analysis or spam detection. You can format a text classification instruction as follows:</p>
<p><code>Template</code></p>
<pre><code>Classify the text into &lt;List of choices&gt;:</code></pre>
<p>For example, we can try to classify a text for whether its sentiment is positive, neutral or negative:</p>
<p><code>Prompt</code></p>
<pre><code>Classify the text into positive, neutral or negative:</code></pre>
<p><code>Output</code></p>
<pre><code>Negative</code></pre>
<h3 id="conversation" tabindex="-1">Conversation</h3>
<p>LLMs can generate human-like conversation, making them useful in chatbots and customer service applications. We can instruct the LLM to follow the conversation acting in a specific manner.</p>
<p><code>Template</code></p>
<pre><code>The following is a conversation with &lt;Actor&gt;. The &lt;Actor&gt; should answer &lt;Further instructions&gt;.</code></pre>
<p>The LLM will fill in the response of the type of actor we instructed it to, and we can continue the conversation by passing the whole conversation with the new responses to it again. For example:</p>
<p><code>Prompt</code></p>
<pre><code>The following is a conversation with a lawyer. The lawyer should answer simply and in layman terms.</code></pre>
<p><code>Output</code></p>
<pre><code>Sure, I can help. The first step is to check the ticket for any errors or mistakes. If you find any, you can use those as grounds to dispute the ticket. If there are no mistakes, you can still dispute the ticket by providing evidence to support your case, such as photos or witness statements. You can also request a hearing to present your case in person.</code></pre>
<p><code>Prompt</code></p>
<pre><code>The following is a conversation with a lawyer. The lawyer should answer simply and in layman terms.</code></pre>
<p><code>Output</code></p>
<pre><code>To request a hearing, you&#39;ll need to follow the instructions on the back of the ticket or on the website provided. Usually, you&#39;ll need to submit a written request within a certain timeframe, along with any evidence or supporting documents you have. Once your request is received, you&#39;ll be notified of the date, time, and location of your hearing.</code></pre>
<h3 id="code-generation" tabindex="-1">Code Generation</h3>
<p>LLMs can generate code, making it easier for developers to create complex software programs. We can write a code comment describing what we wish the code to do and the LLM will be able to do it for us to the best of its effort. The more complex things we wish the code to do, the more specific and descriptive we will need to be in the prompt comment.</p>
<p>The template may vary depending on the programming language we want to generate but I would advise to roughly follow this template:</p>
<p><code>Template</code></p>
<pre><code>&lt;Comment symbol of your language of choice (/* or &#39;&#39;&#39; or #)&gt;</code></pre>
<p>For example, if we want to fetch an API with pagination we could write the following comment:</p>
<p><code>Prompt</code></p>
<pre><code>/* Method: fetchFirst5Pages</code></pre>
<p><code>Output</code></p>
<pre><code>Here&#39;s an example implementation of the fetchFirst5Pages method in JavaScript, assuming the use of the fetch function:</code></pre>
<h3 id="reasoning" tabindex="-1">Reasoning</h3>
<p>LLMs can struggle with some basic tasks, such as arithmetic problems. However, if we ask the LLM to reason upon its response, we can help it get to the right answer. This is the template I recommend:</p>
<p><code>Template</code></p>
<pre><code>Q: &lt;Question&gt;</code></pre>
<p>In this example we can see that a simple arithmetic question fails if we ask it without reasoning:</p>
<p><code>Prompt</code></p>
<pre><code>Answer the following question. Respond &#34;Not sure&#34; if not sure about the answer.</code></pre>
<p><code>Output</code></p>
<pre><code>No. The sum of prime numbers in the list [2, 4, 6, 7, 9, 14] is not a multiple of 3. Therefore, the answer is &#34;No&#34;.</code></pre>
<p>However, if we instruct the LLM to reason its answer, we get a very different result:</p>
<p><code>Prompt</code></p>
<pre><code>Q: The sum of prime numbers in the following list are a multiple of 3: [2, 4, 6, 7, 9, 14]</code></pre>
<p><code>Output</code></p>
<pre><code>Sure, let&#39;s work this out step by step.</code></pre>
<h2 id="strategies" tabindex="-1">Strategies</h2>
<p>While these templates will help you to squeeze the potential of LLMs for your needs, sometimes they can still struggle with certain complex tasks. A few techniques have been developed to help LLMs get to a satisfactory answer. Until now, we have only used zero-shot prompting, which we will now define. However, we can use more complex techniques.</p>
<h3 id="zero-shot-prompting" tabindex="-1">Zero-shot prompting</h3>
<p>We have been using zero-shot prompting for all examples in this article. Zero-shot prompting means passing a prompt to the LLM with just the question we want it to answer, without any examples of how it should respond in other cases. For example, in an earlier example we did:</p>
<p><code>Prompt</code></p>
<pre><code>Classify the text into positive, neutral or negative:</code></pre>
<p><code>Output</code></p>
<pre><code>Negative</code></pre>
<p>This might not be enough context for the model to answer exactly how we want it to. Let&#39;s see what alternatives we have.</p>
<h3 id="few-shot-prompting" tabindex="-1">Few-shot prompting</h3>
<p>As opposed to zero-shot prompting, with few-shot prompting we give the LLM a few examples of how it should respond before we ask the question. For example, let&#39;s say we want to classify the sentiment of a text like in the previous example. But we want it to give us the answer in a very specific format every time. We could give it a few examples beforehand like so:</p>
<p><code>Prompt</code></p>
<pre><code>Classify the text into positive, neutral or negative:</code></pre>
<p><code>Output</code></p>
<pre><code>Neg</code></pre>
<h3 id="chain-of-thought-prompting" tabindex="-1">Chain of thought prompting</h3>
<p>For even more complex tasks, we can combine few-shot prompting with making the model reason about its answer. We can use the same example as before to see how reasoning and few-shot prompting affect the result:</p>
<p><code>Prompt</code></p>
<pre><code>Q: The sum of prime numbers in the following list are a multiple of 3: [2, 4, 6, 7, 9, 14]</code></pre>
<p><code>Output</code></p>
<pre><code>The prime numbers in the list are 5 and 7. Their sum is 12. 12 % 3 = 0. The answer is yes.</code></pre>
<h2 id="final-note" tabindex="-1">Final note</h2>
<p>LLMs have revolutionized the field of natural language processing, but to maximize their potential, prompt engineering is essential. By understanding the tasks LLMs can perform, their shortcomings, and various prompt engineering strategies, developers can harness the power of LLMs to create innovative and effective solutions. More strategies and techniques will likely be developed in the close future so be tuned in to further advancements in this field to maximize LLM potential even more. Moreover, as LLMs continue to grow bigger with billions of additional parameters, it is probable that more tasks that we cannot even think of right now will be very possible. It&#39;s amazing to think of what will be possible using these new tools and which use cases will they serve us in the future.</p>

</article>
      </div></div>
  </body>
</html>
