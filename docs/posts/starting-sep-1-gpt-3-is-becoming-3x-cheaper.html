<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.com/api/pricing/">Original</a>
    <h1>Starting Sep 1, GPT-3 is becoming 3X Cheaper</h1>
    
    <div id="readability-page-1" class="page"><article id="post-api-pricing">

  <header>
        
    <p>
      Simple and flexible. Only pay for what you use.
    </p>
    <a href="https://beta.openai.com/signup">Get started</a>
    <a href="https://openai.com/contact-sales">Contact sales</a>
  </header>

  <div>
    <div>
      <div>
        <svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg">
          <path d="M19.8333 11.6667H14L16.0883 3.36C16.1021 3.23244 16.0735 3.10387 16.0069 2.99421C15.9403 2.88455 15.8394 2.79991 15.7198 2.75341C15.6002 2.7069 15.4686 2.70112 15.3454 2.73696C15.2222 2.77281 15.1143 2.84827 15.0383 2.95166L7.67664 15.4117C7.61373 15.4999 7.57667 15.604 7.5696 15.7122C7.56253 15.8203 7.58574 15.9283 7.63663 16.024C7.68751 16.1197 7.76405 16.1994 7.85767 16.254C7.95129 16.3086 8.05828 16.3361 8.16664 16.3333H14L11.9116 24.64C11.8978 24.7676 11.9264 24.8961 11.993 25.0058C12.0596 25.1154 12.1606 25.2001 12.2801 25.2466C12.3997 25.2931 12.5313 25.2989 12.6545 25.263C12.7777 25.2272 12.8857 25.1517 12.9616 25.0483L20.3233 12.5883C20.3862 12.5001 20.4233 12.396 20.4303 12.2878C20.4374 12.1797 20.4142 12.0717 20.3633 11.976C20.3124 11.8803 20.2359 11.8006 20.1423 11.746C20.0487 11.6914 19.9417 11.6639 19.8333 11.6667Z" fill="#0000F8"></path>
        </svg>
        <p><span>We&#39;re making our API more affordable on September 1, thanks to progress in making our models run more efficiently.</span>
      </p></div>

      
    </div>
  </div>

  
  
  <section id="prices">
    <p>
      <h3>Base models</h3>
      <h4>Showing pricing as of September 1, 2022</h4>
    </p>

    <div>
      
        <div>
          <div>
            <div>
              
              <p><span>$0.0008</span>
                <span>$0.0004</span>
                <span> / 1K tokens</span>
              </p>
            </div>
          </div>
        </div>
      
        <div>
          <div>
            <div>
              <p>
                Babbage 
              </p>
              <p><span>$0.0012</span>
                <span>$0.0005</span>
                <span> / 1K tokens</span>
              </p>
            </div>
          </div>
        </div>
      
        <div>
          <div>
            <div>
              <p>
                Curie 
              </p>
              <p><span>$0.0060</span>
                <span>$0.0020</span>
                <span> / 1K tokens</span>
              </p>
            </div>
          </div>
        </div>
      
        <div>
          <div>
            <div>
              
              <p><span>$0.0600</span>
                <span>$0.0200</span>
                <span> / 1K tokens</span>
              </p>
            </div>
          </div>
        </div>
      
    </div>
    <div>
      <div><p>Multiple models, each with different capabilities and price points. <strong>Ada</strong> is the fastest model, while <strong>Davinci</strong> is the most powerful.</p>

<p>Prices are per 1,000 tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words. This paragraph is 35 tokens.</p>


</div>
    </div>
  </section>

  
  
  <section>
    <div>
      
        <div>
          <h3><span>flag</span>Start for free</h3>
          <p>Start experimenting with $18 in free credit that can be used during your first 3 months.</p>
        </div>
      
        <div>
          <h3><span>barup</span>Pay as you go</h3>
          <p>To keep things simple and flexible, pay only for the resources you use.</p>
        </div>
      
        <div>
          <h3><span>check</span>Choose your model</h3>
          <p>Use the right model for the job. We offer a spectrum of capabilities and price points.</p>
        </div>
      
    </div>
  </section>
  <section id="quotas">
    <hr/>
    <div>
      <div>
        <div>
          <h3>Fine-tuned models</h3>
          <p>Create your own custom models by <a href="https://beta.openai.com/docs/guides/fine-tuning">fine-tuning</a> our base models with your training data. Tokens used to train a model are billed at 50% of our base prices. Once you fine-tune a model, you&#39;ll be billed only for the tokens you use in requests to that model.</p>
          <p><a id="faq-fine-tuning-pricing-button" href="https://openai.com/api/pricing/#faq-fine-tuning-pricing-calculation">Learn more</a>
        </p></div>
      </div>
      <div>
        <div>
          <h4>Pricing will remain the same on September 1, 2022</h4>
          <table>
            <tbody><tr>
              <th>Model</th>
              <th>Training</th>
              <th>Usage</th>
            </tr>
            <tr>
              <td>Ada</td>
              <td>$0.0004 <span>/ 1K tokens</span></td>
              <td>$0.0016 <span>/ 1K tokens</span></td>
            </tr>
            <tr>
              <td>Babbage</td>
              <td>$0.0006 <span>/ 1K tokens</span></td>
              <td>$0.0024 <span>/ 1K tokens</span></td>
            </tr>
            <tr>
              <td>Curie</td>
              <td>$0.0030 <span>/ 1K tokens</span></td>
              <td>$0.0120 <span>/ 1K tokens</span></td>
            </tr>
            <tr>
              <td>Davinci</td>
              <td>$0.0300 <span>/ 1K tokens</span></td>
              <td>$0.1200 <span>/ 1K tokens</span></td>
            </tr>
          </tbody></table>
        </div>
      </div>
    </div>
    <hr/>
    <div>
      <div>
        <div>
          <h3>Embedding models</h3>
          <p>Build advanced search, clustering, topic modeling, and classification functionality with our <a href="https://beta.openai.com/docs/guides/embeddings">embeddings</a> offering.</p>
        </div>
      </div>
      <div>
        <div>
          <h4>Showing pricing as of September 1, 2022</h4>
          <table>
            <tbody><tr>
              <th>Model</th>
              <th>Usage</th>
            </tr>
            <tr>
              <td>Ada</td>
              <td>
                <span>$0.0080</span>
                <span>$0.0040</span>
                <span>/ 1K tokens</span>
            </td>
            </tr>
            <tr>
              <td>Babbage</td>
              <td>
                <span>$0.0120</span>
                <span>$0.0050</span>
                <span>/ 1K tokens</span>
              </td>
            </tr>
            <tr>
              <td>Curie</td>
              <td>
                <span>$0.0600</span>
                <span>$0.0200</span>
                <span>/ 1K tokens</span>
              </td>
            </tr>
            <tr>
              <td>Davinci</td>
              <td>
                <span>$0.6000</span>
                <span>$0.2000</span>
                <span>/ 1K tokens</span>
              </td>
            </tr>
          </tbody></table>
        </div>
      </div>
    </div>
  </section>

  
  <section id="quotas">
    <hr/>
    <div>
      <p>
        <h2>Usage quotas</h2>
      </p>
      <div>
        <div><p>Because this technology is new, we also want to make sure that rollouts are done responsibly. When you sign up, you’ll be granted an initial spend limit, or quota, and we’ll increase that limit over time as you build a track record with your application.</p>

<p>If you need more tokens, you can always request a quota increase. When you’re ready to go live, you’ll submit a <a href="https://beta.openai.com/docs/going-live">Pre-launch Review Request</a> which will also cover any additional quota increase requests.</p>
</div>
      </div>
    </div>
  </section>

  
  
  <section id="faq">
    <div>
      <div>
        <div>
          <h2 id="faq-title">Frequently Asked Questions</h2>

          <div>
            
              <hr/>
              <div id="faq-token">
                <p>
                <label for="faq-token-input">
                  <div>
                    <p>What&#39;s a token?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p>You can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.</p>

<p>To learn more about how tokens work and estimate your usage…</p>

<ul>
  <li>Experiment with our interactive <a href="https://beta.openai.com/tokenizer">Tokenizer tool</a>.</li>
  <li>Log in to your account and enter text into the Playground. The counter in the footer will display how many tokens are in your text.</li>
</ul>

                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-which-model">
                <p>
                <label for="faq-which-model-input">
                  <div>
                    <p>Which model should I use?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>While Davinci is generally the most capable model, the other models can perform certain tasks extremely well and, in some cases, significantly faster. They also have cost advantages. For example, Curie can perform many of the same tasks as Davinci, but faster and for 1/10th the cost. We encourage developers to experiment to find the model that’s most efficient for your application. Visit our documentation for a more detailed <a href="https://beta.openai.com/docs/engines">model comparison</a>.</p>
                </div>
              </div>


            
              <hr/>
              <div id="faq-token-usage">
                <p>
                <label for="faq-token-usage-input">
                  <div>
                    <p>How will I know how many tokens I&#39;ve used each month?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>Log in to your account to view your <a href="https://beta.openai.com/account/usage">usage tracking dashboard</a>. This page will show you how many tokens you&#39;ve used during the current and past billing cycles.</p>
                </div>
              </div>


            
              <hr/>
              <div id="faq-spending">
                <p>
                <label for="faq-spending-input">
                  <div>
                    <p>How can I manage my spending?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>You can configure a usage <strong>hard limit</strong> in your billing settings, after which we&#39;ll stop serving your requests. You may also configure a <strong>soft limit</strong> to receive an email alert once you pass a certain usage threshold.</p>
                </div>
              </div>


            
              <hr/>
              <div id="faq-playground-usage">
                <p>
                <label for="faq-playground-usage-input">
                  <div>
                    <p>Does Playground usage count against my quota?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>Yes, we treat Playground usage the same as regular API usage.</p>
                </div>
              </div>


            
              <hr/>
              <div id="faq-completions-pricing">
                <p>
                <label for="faq-completions-pricing-input">
                  <div>
                    <p>How is pricing calculated for Completions?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p><a href="https://beta.openai.com/docs/api-reference/completions">Completions</a> requests are billed based on the number of tokens sent in your prompt plus the number of tokens in the completion(s) returned by the API.</p>

<p>The <code>best_of</code> and <code>n</code> parameters may also impact costs. Because these parameters generate multiple completions per prompt, they act as multipliers on the number of tokens returned.</p>

<p>Your request may use up to <code>num_tokens(prompt) + max_tokens * max(n, best_of)</code> tokens, which will be billed at the per-engine rates outlined at the top of this page.</p>

<p>In the simplest case, if your prompt contains 10 tokens and you request a single 90 token completion from the davinci engine, your request will use 100 tokens and will cost $0.006.</p>

<p>You can limit costs by reducing prompt length or maximum response length, limiting usage of <code>best_of</code>/<code>n</code>, adding appropriate stop sequences, or using engines with lower per-token costs.</p>

                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-fine-tuning-pricing-calculation">
                <p>
                <label for="faq-fine-tuning-pricing-calculation-input">
                  <div>
                    <p>How is pricing calculated for Fine-tuning?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p>There are two components to fine-tuning pricing: training and usage.</p>

<p>When training a fine-tuned model, the total tokens used will be billed according to our <a href="#quotas">training rates</a> (50% of our base model rates). Note that the number of training tokens depends on the number of tokens in your training dataset <strong>and</strong> your chosen number of <a href="https://beta.openai.com/docs/guides/fine-tuning/hyperparameters">training epochs</a>. The default number of epochs is 4.</p>

<p>
  (Tokens in your training file * Number of training epochs) = Total training tokens
</p>


                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-classifications-pricing">
                <p>
                <label for="faq-classifications-pricing-input">
                  <div>
                    <p>How is pricing calculated for Classifications?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p><a href="https://beta.openai.com/docs/api-reference/classifications">Classifications</a> requests are billed based on the number of tokens in the inputs you provide. Internally this endpoint makes calls to the <a href="https://beta.openai.com/docs/api-reference/searches">search</a> and <a href="https://beta.openai.com/docs/api-reference/completions">completions</a> endpoints, so its costs are a function of the costs of those endpoints.</p>

<p>The actual cost per token is based upon which <a href="https://beta.openai.com/docs/engines">models</a> you select to perform both the search and the completion, which are controlled by the <code>search_model</code> and <code>model</code> parameters respectively.</p>

<p>You may provide a <code>file</code> containing the examples to search over, or you can explicitly specify <code>examples</code> in your request. Providing a file makes search faster and more cost effective when the number of examples you&#39;d like to search over is greater than <code>max_examples</code>. In this scenario, costs are largely based on the number of examples reranked (controlled by <code>max_examples</code>) and the total length of those examples. If you pass <code>examples</code> in your request instead, costs are based on the total length of all those examples.</p>

<p>The length of the <code>query</code> passed into the model as well as the final classification label that is generated will also factor into costs.</p>

<p>You can use the <code>return_prompt</code> debugging flag to understand the length of the final combined prompt that will be sent to the <a href="https://beta.openai.com/docs/api-reference/completions">completions</a> endpoint to generate the classification label.</p>

                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-search-pricing">
                <p>
                <label for="faq-search-pricing-input">
                  <div>
                    <p>How is pricing calculated for Search?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p><a href="https://beta.openai.com/docs/api-reference/searches">Search</a> requests are billed based on the total number of tokens in the documents you provide, plus the tokens in the query and the tokens needed to instruct the model on how to perform the operation. The API also uses a reference document to generate a response, adding 1 to the total document count. These tokens are billed at the per-engine rates outlined at the top of this page.</p>

<p>You may provide a <code>file</code> containing the documents to search over, or you can explicitly specify <code>documents</code> in your request. Providing a file makes search faster and more cost effective when the number of documents you&#39;d like to search over is greater than <code>max_rerank</code>. In this scenario, costs are largely based on the number of documents reranked (controlled by <code>max_rerank</code>) and the total length of those documents. If you pass <code>documents</code> in your request instead, costs are based on the total length of all those documents.</p>

<p>Below you&#39;ll find the formula for calculating overall token consumption. The 14 represents the additional tokens the API uses per document to accomplish the Semantic Search task, and the added 1 is a reference document:</p>

<div><p>
  Number of tokens in all of your documents</p></div>

<p>As an example, if you had 5 documents (plus one added by the API) with token lengths of <code>12, 34, 22, 33, 78</code> (179 total) and your query was 8 tokens, the total tokens consumed would be: <code>179 + (6 * 14) + (6 * 8) = 311</code></p>

<p>You may use the <a href="https://gpttools.com/searchtokens">Search Token Estimator</a> or see the code from the <a href="https://repl.it/@NikolasTezak/API-TokenLoadEstimator#main.py">Python Estimator</a> to further understand search token usage.</p>

                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-answers-pricing">
                <p>
                <label for="faq-answers-pricing-input">
                  <div>
                    <p>How is pricing calculated for Answers?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <div>
                    <p><a href="https://beta.openai.com/docs/api-reference/answers">Answers</a> requests are billed based on the number of tokens in the inputs you provide and the answer that the model generates. Internally, this endpoint makes calls to the <a href="https://beta.openai.com/docs/api-reference/searches">Search</a> and <a href="https://beta.openai.com/docs/api-reference/completions">Completions</a> APIs, so its costs are a function of the costs of those endpoints.</p>

<p>The actual cost per token is based upon which <a href="https://beta.openai.com/docs/engines">models</a> you select to perform both the search and the completion, which are controlled by the <code>search_model</code> and <code>model</code> parameters respectively.</p>

<p>You may provide a <code>file</code> containing the documents to search over, or you can explicitly specify <code>documents</code> in your request. Providing a file makes search faster and more cost effective when the number of documents you&#39;d like to search over is greater than <code>max_rerank</code>. In this scenario, costs are largely based on the number of documents reranked (controlled by <code>max_rerank</code>) and the total length of those documents. If you pass <code>documents</code> in your request instead, costs are based on the total length of all those documents.</p>

<p>The length of <code>examples</code>, <code>examples_context</code>, <code>question</code> and the length of the generated answer (controlled by <code>max_tokens</code>/<code>stop</code>) will also impact costs.</p>

<p>You can use the <code>return_prompt</code> debugging flag to understand the length of the final combined prompt that will be sent to the <a href="https://beta.openai.com/docs/api-reference/completions">completions</a> endpoint to generate the answer.</p>

                  </div>
                </div>
              </div>


            
              <hr/>
              <div id="faq-sla">
                <p>
                <label for="faq-sla-input">
                  <div>
                    <p>Is there an SLA on the various models?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>We will be publishing an SLA soon. In the meantime you can visit our <a href="https://status.openai.com/">Status page</a> to monitor service availability and view historical uptime. If your company or application has specific requirements, please <a href="https://openai.com/contact-sales">contact our sales team</a>.</p>
                </div>
              </div>


            
              <hr/>
              <div id="faq-azure-availability">
                <p>
                <label for="faq-azure-availability-input">
                  <div>
                    <p>Is the API available on Microsoft Azure?</p>
                    <p><span>navigatedown</span>
                    <span>navigateup</span>
                  </p></div>
                </label></p><div>
                  <p>Yes. Azure customers can access the OpenAI API on Azure with the compliance, regional support, and enterprise-grade security that Azure offers. <a href="https://azure.microsoft.com/en-us/services/openai-service/#overview">Learn more</a> or contact <a href="mailto:sales@openai.com">sales@openai.com</a>.</p>
                </div>
              </div>


            
            <hr/>
          </div>

        </div>
      </div>
    </div>
  </section>
  
  
</article></div>
  </body>
</html>
