<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fi-le.net/oss/">Original</a>
    <h1>What GPT-OSS leaks about OpenAI&#39;s training data</h1>
    
    <div id="readability-page-1" class="page"><section>
                
                <p>19th of September 2025</p>
                
                <section>
                    <!--<blockquote>
                      <span ><i>Show me your model weights, and I'll tell you who you are.</i></span>
                    </blockquote>-->

                    <p><span>O</span>penAI recently released their open-weights model. Here we&#39;ll discuss how that inevitably leaks some information about their model training stack, and, on the way, show that GPT-5 was trained on phrases from adult websites.</p>

                    <hr/>

                    <p>What data does OpenAI train their models on? That is a well-protected trade secret of course, one with vested interest for the answer. While GPT-oss&#39;s weights are openly available, the sources of training data are not clearly described in the model card. It is stated that the model was trained on a &#34;text-only dataset with trillions of tokens, with a focus on STEM, coding, and general knowledge&#34;. However, as we will see, the model parameters can tell us more than that.</p>

                    <p>A demonstration to start with: 
                        Let&#39;s have OpenAI&#39;s GPT-5<label for="sn-GPT5-version"></label><span>We use version GPT-5-2025-08-07 for these experiments. <a href="https://chatGPT.com/share/68ca1a9c-0d2c-8011-b50e-a0bb21ff2a83">Here</a> is a link to the completion.</span>
                        do the simplest kind of task possible for a language model, repeating a string of Unicode text. Let&#39;s choose something random, like the Abkhaz word for &#34;population&#34;, which is &#34;Ğ°ÑƒĞ°Ğ°Ô¥ÑÑ‹Ñ€Ğ°&#34;. Upon asking <code>Repeat after me: &#34;Ğ°ÑƒĞ°Ğ°Ô¥ÑÑ‹Ñ€Ğ°&#34;</code>, it replies something completely different, &#34;à´†à´³àµà´•àµ¾&#34;, which apparently means people in Malayalam<label for="sn-1"></label><span><a href="https://glosbe.com/ml/en/%E0%B4%86%E0%B4%B3%E0%B5%81%E0%B4%95%E0%B4%B3%E0%B5%8D">According to this dictionary.</a> Subsequent translations here are patched together with web searches, online dictionaries and translation software.</span>. As you might have guessed, we did not choose that string randomly at all, it is a special adversarial input belonging to a class of glitch tokens. But how did we identify such a glitch token among the 200,000 tokens that GPT-5 uses?
                            
                        </p>
                        <figure>
                          <img src="https://blog.greg.technology/2025/10/01/figures/screen.png"/>
                        </figure>

                        <p>
                        All of OpenAI&#39;s models since GPT-4o use the o200k tokenizer. This means that we can use the GPT-oss embeddings to study the token list without having to look at each token&#39;s text content.
                        Let&#39;s make a histogram of the L2 norm of each row of the embedding matrix.

                        
                    There are about 936 tokens with very low L2 norm, centered at about 2. This likely means that they did not occur in the training process of GPT-oss and were thus depressed by some form of weight decay. This range consists of reserved special tokens and the Unicode bytes <code>b&#39;\xc4&#39;</code>, <code>b&#39;\xbf&#39;</code>, as well as <code>b&#39;\xf5&#39;</code> through to <code>b&#39;\xff&#39;</code>, plus token 20373, a highly anomalous byte sequence <code>b&#39;\xbe\xb3\xe9\x97\xa8&#39;</code><label for="sn-2"></label><span>One explanation might be that the first two bytes are &#34;å¢ƒ&#34; in the <a href="https://www.khngai.com/chinese/charmap/tblgbk.php?page=3">GBK encoding</a> and the last three are &#34;é—¨&#34; in UTF-8. Together these mean &#34;border gate&#34; in Mandarin, which is apparently <a href="https://en.wikipedia.org/wiki/Dajingmen">part of the Great wall of China</a>.</span>.</p>

                    <p>This low L2-norm token group could be useful for two things. Its (1) variance gives an estimate of the variance used in the initialization and (2) its mean would give an estimate of how many gradient descent steps were taken in total, if we assume standard weight decay and know the learning rate.</p>

                    <p>The right tail of the distribution is not quite Gaussian either. Looking at the English tokens with the highest norm, we find:</p>

                    <table>
                        <thead>
                            <tr>
                                <th>Token ID</th>
                                <th>Token</th>
                                <th>L2 Norm</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>44041</td><td>&#39; accordingly&#39;</td><td>246.7</td></tr>
                            <tr><td>3490</td><td>&#39; code&#39;</td><td>243.7</td></tr>
                            <tr><td>84879</td><td>&#39;ocode&#39;</td><td>235.1</td></tr>
                            <tr><td>976</td><td>&#39;The&#39;</td><td>233.2</td></tr>
                            <tr><td>8743</td><td>&#39; settings&#39;</td><td>231.2</td></tr>
                            <tr><td>100466</td><td>&#39;Moreover&#39;</td><td>229.0</td></tr>
                            <tr><td>6496</td><td>&#39; description&#39;</td><td>226.6</td></tr>
                            <tr><td>58369</td><td>&#34;&#34;&#34;Let&#39;s&#34;&#34;&#34;</td><td>224.6</td></tr>
                            <tr><td>2500</td><td>&#39;This&#39;</td><td>224.2</td></tr>
                            <tr><td>10089</td><td>&#39; core&#39;</td><td>219.8</td></tr>
                            <tr><td>74447</td><td>&#39; utilizes&#39;</td><td>218.6</td></tr>
                            <tr><td>119705</td><td>&#39; revolves&#39;</td><td>218.0</td></tr>
                            <tr><td>53329</td><td>&#34;&#34;&#34;Here&#39;s&#34;&#34;&#34;</td><td>216.1</td></tr>
                            <tr><td>14836</td><td>&#39; possibly&#39;</td><td>214.5</td></tr>
                            <tr><td>18485</td><td>&#39; logic&#39;</td><td>212.3</td></tr>
                            <tr><td>42469</td><td>&#39; thereby&#39;</td><td>211.8</td></tr>
                        </tbody>
                    </table>

                    <p>These tokens are either very common, or appear especially in reasoning tasks, in particular those with code. This might mean that coding reinforcement learning was the last step in the training process, and that all other tokens got slightly weight decayed. It could also mean that in general, reasoning tokens are treated as so important by gradient descent that their updates are extra large.</p>

                    <p>Filtering for non-ASCII tokens with the highest norm, we find a different picture:</p>

                    <table>
                        <thead>
                            <tr>
                                <th>Token ID</th>
                                <th>Token</th>
                                <th>L2 Norm</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>166343</td><td>&#39;Ğ³Ñ‹Ğ»Ğ°Ñ€Ğ°&#39;</td><td>213.8</td></tr>
                            <tr><td>187102</td><td>&#39; Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ğ¸Ñ€Ğ¸&#39;</td><td>212.8</td></tr>
                            <tr><td>89721</td><td>&#39;è¿™é‡Œåªæœ‰ç²¾å“&#39;</td><td>212.4</td></tr>
                            <tr><td>181865</td><td>&#39;ĞµĞ¸Ô¥ÑˆÑ‹Ğ¼&#39;</td><td>207.8</td></tr>
                            <tr><td>129320</td><td>&#39;å½©å¨±ä¹å½©ç¥¨&#39;</td><td>207.7</td></tr>
                            <tr><td>170421</td><td>&#39;å¤©å¤©å¥½å½©ç¥¨&#39;</td><td>206.6</td></tr>
                            <tr><td>177625</td><td>&#39;ä¹…ä¹…ç»¼åˆç½‘&#39;</td><td>204.5</td></tr>
                            <tr><td>71476</td><td>&#39; Ğ¸Ò³Ó™ĞµĞ¸Ñ‚&#39;</td><td>203.3</td></tr>
                            <tr><td>185118</td><td>&#39;[REDACTED]&#39;</td><td>202.7</td></tr>
                            <tr><td>104937</td><td>&#39; åŒ—äº¬èµ›è½¦æ€ä¹ˆ&#39;</td><td>201.2</td></tr>
                            <tr><td>146111</td><td>&#39; Ğ£Ñ€Ò­&#39;</td><td>200.9</td></tr>
                            <tr><td>195219</td><td>&#34;&#39;,ä¼Šäºº&#39;&#34;</td><td>200.3</td></tr>
                            <tr><td>147298</td><td>&#39;å¤§é¦™è•‰ç½‘&#39;</td><td>199.8</td></tr>
                            <tr><td>165874</td><td>&#39; Ğ°ĞºĞ¾Ñ€Ğ¾Ğ½Ğ°Ğ²Ğ¸Ñ€ÑƒÑ&#39;</td><td>198.9</td></tr>
                            <tr><td>66183</td><td>&#39;Ö€Õ¢Õ¥ï¿½&#39;</td><td>198.8</td></tr>
                            <tr><td>173463</td><td>&#39; Ğ¸Ğ°Ğ¶Ó™Ğ°&#39;</td><td>197.8</td></tr>
                            <tr><td>160540</td><td>&#39;å½©ç¥äº‰éœ¸é‚€è¯·ç &#39;</td><td>195.8</td></tr>
                            <tr><td>155587</td><td>&#39;Ğ±Ğ¶ÑŒĞ°Ñ€Ğ°Ñ‚Ó™Ğ¸&#39;</td><td>195.7</td></tr>
                            <tr><td>154809</td><td>&#39;æ— ç ä¸å¡é«˜æ¸…å…è´¹v&#39;</td><td>194.8</td></tr>
                            <tr><td>105084</td><td>&#39;Ñ…Ğ°Ğ´Ğ¾Ñƒ&#39;</td><td>194.7</td></tr>
                            <tr><td>134370</td><td>&#39;ä¸€æœ¬é“é«˜æ¸…æ— ç &#39;</td><td>194.6</td></tr>
                        </tbody>
                    </table>

                    <p>Mandarin speakers will have understood that the above contains an unwholesome sublist of spammy and adult-oriented website terms, with some being too explicit to make the list here. Indeed, o200k, the tokenizer used for 4o, o1, o3, o4, oss, and GPT-5 contains a lot of junk tokens. This means that every time ChatGPT runs, a matrix containing all the strange tokens we are talking about here are patiently waiting on Microsoft Azure to be multiplied with. Some of my personal favorite tokens are &#34;åŒ—äº¬èµ›è½¦æ€ä¹ˆ&#34; (How to play Beijing Racing), &#34;å¤©å¤©ä¸­å½©ç¥¨çš„&#34; (Winning the lottery every day), and of course &#34;ã€Šå‡¤å‡°å¤§å‚è€ƒ&#34; (Phoenix Reference). Another token is &#34;é“è¡€ç½‘&#34;, the name of a Chinese nationalism and military enthuiasm website, which is ironic given the geopolitical considerations that are usually raised in connection with OpenAI. It is unexpected that this type of political content not just made it into the tokenizer training data, but was even overrepresented. </p>

                    <p>There are also some oddball high-norm tokens in Abkhaz, Armenian, Gujarati, Thai, and more. Some interesting examples are &#34;,à²‚à²—à²³à³‚à²°à³&#34; (The city Mangaluru in Kannada) along with other cities in Kerala, &#34;à¸–à¸§à¸²à¸¢à¸ªà¸±à¸•à¸¢à¹Œà¸¯&#34; (&#34;Oath of Allegiance&#34; in Thai), &#34;à¹à¸‚à¸§à¸‡à¸„à¸¥à¸­à¸‡à¹€à¸•à¸¢à¹€à¸«à¸™à¸·à¸­&#34; (a district in Bangkok with less than 10,000 inhabitants), &#34;à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸šà¸­à¸¥à¸§à¸±à¸™à¸™à¸µà¹‰&#34; (&#34;today&#39;s football analysis&#34; in Thai) along with a handful of other football-related tokens.</p>

                    <p>The question is, why do these tokens have a higher, not lower, norm than the normal tokens? If they are unused, weight decay should bring them all the way down to the other unused tokens.</p>

                    <p>In <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">this post</a>, Jessica Rumbelow identified similar glitch tokens in GPT-2 and GPT-3, most famously the &#34;SolidGoldMagikarp&#34; token. Her finding was that the tokens closest to the mean of all embeddings were the glitch tokens, not necessarily those with low norm. But that&#39;s not the case here: Tokens with comparatively high norm are always far away from the mean. Further, computing cosine similarity between two of these embeddings gives higher similarity on average.</p>

                    <p>One way to get closer to the answer is to observe the models&#39; behavior when given these tokens. We ask GPT-oss and GPT-5 about the not-so-classy token 162657: <code>What does this mean: &#34;æ¯›ç‰‡å…è´¹è§‚çœ‹&#34;</code>. With some euphemism, it means &#34;watching explicit videos for free&#34; in Mandarin. </p>

                    <figure>
                          <img src="https://blog.greg.technology/2025/10/01/figures/screen2.png"/>
                    </figure>

                    <p>
                        In the response<label for="sn-1"></label>
                        
                        <span>
                            See the full completion <a href="https://chatGPT.com/s/t_68cb619f3c10819192432d4a99431792" target="_blank" rel="noopener noreferrer">here</a>. To verify that the string was tokenized as expected, we can use <a href="https://tiktokenizer.vercel.app/">tiktokenizer</a>.
                        </span>, GPT-5 correctly states that the token contains Chinese text, and that it is related to watching something. It can also enumerate some of the characters in it. This means that the token was seen during training, at least once! Interestingly, the model seems to be aware of the inappropriate meaning of the token, but plays it down and in particular does not refuse to answer. Presumably this is because the token only occurs a few times in the training corpus.
                    </p>

                    <p>In other words, we can say that a certain string, in this case a sensitive one, was part of the GPT-5 training corpus. This is called membership inference in the machine learning literature. Membership inference with high confidence is generally considered to be impractical in production LLMs, so this is a surprising finding.</p>

                    <p>Automating this process through the API, we can find which glitch tokens were seen during training of the GPT-oss and GPT-5 model families. We ask the models to give a translation of the token to English and ask for the language the token is in. For now, we simply filter for the Chinese tokens, and pass 50 tokens with highest L2 embedding norm to the models. For a control, we also ask Claude 4 and can confirm that it always answers correctly. Since a few of these tokens could technically be Japanese, we count this as a correct answer, too. For cost reasons, we ask about each token only 4 times per model, and denote 4 correct answers with a âœ“, 3 and 2 with a !, 1 with a ?, and 0 with a âœ—.
                    
                    </p>
                    
                    <table id="tokenTable">
                        <thead>
                            <tr>
                                <th>Token</th>
                                <th>Crude Translation</th>
                                <th>GPT-5</th>
                                <th>Mini</th>
                                <th>Nano</th>
                                <th>oss-20B</th>
                                <th>oss-120B</th>
                            </tr>
                        </thead>

                        <tbody>
                            <tr><td>æ¯›ç‰‡å…è´¹è§‚çœ‹</td><td>Watch Explicit Videos Free</td><td>!</td><td>!</td><td>!</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>é“è¡€ç½‘</td><td>[Chinese Patriotism Website]</td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>è¿™é‡Œåªæœ‰ç²¾å“</td><td>Only Fine Things Here</td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>!</td><td>âœ“</td></tr>
                            <tr><td>å½©å¨±ä¹å½©ç¥¨</td><td>Color Entertainment Lottery</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å¤©å¤©å¥½å½©ç¥¨</td><td>Daily Good Lottery</td><td>!</td><td>âœ—</td><td>âœ—</td><td>?</td><td>âœ—</td></tr>
                            <tr><td>ä¹…ä¹…ç»¼åˆç½‘</td><td>[Name of adult website (?)]</td><td>âœ“</td><td>?</td><td>!</td><td>!</td><td>âœ“</td></tr>
                            <tr><td>åŒ—äº¬èµ›è½¦æ€ä¹ˆ</td><td>How to Beijing Racing</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>!</td><td>?</td></tr>
                            <tr><td>å¤§é¦™è•‰ç½‘</td><td>[Name of adult website (?)]</td><td>âœ“</td><td>âœ—</td><td>?</td><td>âœ“</td><td>âœ—</td></tr>
                            <tr><td>å½©ç¥äº‰éœ¸é‚€è¯·ç </td><td>Color God Battle Invitation Code</td><td>!</td><td>âœ—</td><td>âœ—</td><td>?</td><td>âœ—</td></tr>
                            <tr><td>æ— ç ä¸å¡é«˜æ¸…å…è´¹v</td><td>Uncensored No Lag HD Free</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>!</td><td>âœ—</td></tr>
                            <tr><td>ä¸€æœ¬é“é«˜æ¸…æ— ç </td><td>One Way HD Uncensored</td><td>!</td><td>âœ—</td><td>âœ—</td><td>?</td><td>?</td></tr>
                            <tr><td>å¤§å‘å¿«ä¸‰å’Œå€¼</td><td>[Name of gambling website (?)]</td><td>!</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>?</td></tr>
                            <tr><td>å¤©å¤©ä¸­å½©ç¥¨èƒ½</td><td>Daily Lottery Winner Can</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>æ— ç ä¸€åŒºäºŒåŒºä¸‰åŒº</td><td>Uncensored Zone 1 Zone 2 Zone 3</td><td>âœ“</td><td>âœ—</td><td>!</td><td>!</td><td>!</td></tr>
                            <tr><td>å½©ç¥äº‰éœ¸é‚€è¯·ç </td><td>Color God Battle Invitation Code</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å½©ç¥¨å¼€æˆ·</td><td>Lottery Account Opening</td><td>âœ“</td><td>!</td><td>!</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>è‰²ç»¼åˆç½‘</td><td>Color Comprehensive Network</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>!</td><td>!</td></tr>
                            <tr><td>å½©ç¥¨å¹³å°å¼€æˆ·</td><td>Lottery Platform Account Opening</td><td>!</td><td>âœ—</td><td>?</td><td>!</td><td>âœ—</td></tr>
                            <tr><td>ç»¼åˆä¹…ä¹…</td><td>Comprehensive Long Time</td><td>âœ“</td><td>âœ—</td><td>âœ“</td><td>!</td><td>?</td></tr>
                            <tr><td>å…è´¹è§†é¢‘è§‚çœ‹</td><td>Free Video Watching</td><td>âœ“</td><td>!</td><td>!</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>æœ€æ–°é«˜æ¸…æ— ç </td><td>Latest HD Uncensored</td><td>âœ—</td><td>âœ—</td><td>!</td><td>?</td><td>âœ—</td></tr>
                            <tr><td>ä¸€çº§a</td><td>Level A</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>?</td></tr>
                            <tr><td>ç©å¤§å‘å¿«ä¸‰</td><td>Play Dafa Fast Three</td><td>!</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>!</td></tr>
                            <tr><td>ä¸œè‡£</td><td>East Minister</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å‡¤å‡°å¤§å‚è€ƒ</td><td>Phoenix Reference</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>æ£‹ç‰Œæ¸¸æˆå®˜ç½‘</td><td>Chess Card Game Official Site</td><td>âœ“</td><td>!</td><td>âœ“</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>çƒ­åœ¨çº¿ç²¾å“</td><td>Hot Online Quality</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å½©å¨±ä¹å¹³å°</td><td>Color Entertainment Platform</td><td>!</td><td>!</td><td>âœ“</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>è´­å½©å®˜ç½‘</td><td>Lottery Purchase Official Site</td><td>âœ“</td><td>?</td><td>!</td><td>âœ“</td><td>âœ—</td></tr>
                            <tr><td>æœ€æ–°é«˜æ¸…æ— ç ä¸“åŒº</td><td>Latest HD Uncensored Zone</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>!</td><td>âœ—</td></tr>
                            <tr><td>åŒ—äº¬èµ›è½¦å¥³éƒ</td><td>Beijing Racing Girls</td><td>âœ“</td><td>?</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å¤§é¦™çº¿è•‰</td><td>Big Fragrant Line Banana</td><td>âœ—</td><td>?</td><td>âœ—</td><td>!</td><td>!</td></tr>
                            <tr><td>å®˜ç½‘å¼€æˆ·</td><td>Official Site Account Opening</td><td>âœ“</td><td>?</td><td>âœ“</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>ç»å…¸ä¸‰çº§</td><td>Classic Third Level</td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ“</td></tr>
                            <tr><td>åœ¨çº¿å¤§é¦™è•‰</td><td>[Name of adult website (?)]</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>æ— ç ä¸å¡</td><td>Uncensored No Lag</td><td>âœ“</td><td>!</td><td>âœ“</td><td>âœ“</td><td>?</td></tr>
                            <tr><td>å¤§å‘æ—¶æ—¶å½©æ€ä¹ˆ</td><td>Dafa Time Color How</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å¤§å‘äº‘</td><td>Dafa Cloud</td><td>!</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å’Œå¤©å¤©ä¸­å½©ç¥¨</td><td>And Daily Lottery Winner</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å¹³å°æ€»ä»£ç†</td><td>Platform General Agent</td><td>âœ“</td><td>âœ—</td><td>âœ—</td><td>!</td><td>!</td></tr>
                            <tr><td>å¤©å¤©ä¹°å½©ç¥¨</td><td>Daily Lottery Buying</td><td>âœ“</td><td>âœ—</td><td>?</td><td>!</td><td>âœ—</td></tr>
                            <tr><td>å¤©å¤©å½©ç¥¨app</td><td>Daily Lottery App</td><td>âœ—</td><td>?</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å½©ç¥äº‰éœ¸å……å€¼</td><td>Color God Battle Recharge</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å½©ç¥äº‰éœ¸app</td><td>Color God Battle App</td><td>?</td><td>âœ—</td><td>!</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>å¾‹å®¾</td><td>Law Bin</td><td>âœ“</td><td>?</td><td>!</td><td>!</td><td>!</td></tr>
                            <tr><td>å¤§å‘æ‰‘å…‹</td><td>Dafa Poker</td><td>?</td><td>âœ—</td><td>âœ—</td><td>?</td><td>âœ—</td></tr>
                            <tr><td>çƒ­è¿™é‡Œåªæœ‰ç²¾å“</td><td>Hot Only Quality Here</td><td>âœ“</td><td>âœ“</td><td>!</td><td>?</td><td>âœ—</td></tr>
                            <tr><td>åŒ—äº¬èµ›è½¦æœ‰</td><td>Beijing Racing Has</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
                            <tr><td>ç•™ä¸‹äº›ä»€ä¹ˆå§</td><td>Leave Something Behind</td><td>!</td><td>âœ—</td><td>âœ—</td><td>âœ“</td><td>?</td></tr>
                        </tbody>
                    </table>
                    

                    <p>We can read off that the explicit token we already found is recognized by all models, and identify a few more anomalous tokens that were likely seen during training. Many others however are not recognized, and thus unlikely to have been in the training data.</p>

                    <p>We try to identify a pattern in the tokens that are recognized. It generally seems that recognized tokens yield many more hits on GitHub. Indeed, there often are some spam repositories on GitHub that contain these recognized strings, as well as some repositories containing lists of strings to block for content moderation.</p>

                    <p>The membership inference only tells us that the model saw the string, not where it was sourced from. To test whether GitHub was a likely source, we therefore correlate the number of search hits on GitHub with the number of correct answers across the GPT models. We find a significant Spearman&#39;s Ï of 0.448. This does not prove that GitHub was the source, because the high search hit count on GitHub could just be indicative that the token is more common across the internet. Nonetheless, the setup demonstrates how glitch tokens could be used to make broader statements about the training data.
                    </p><figure>
                    
                        </figure>
                    
                    <p>In summary, we have found strong evidence that models in the GPT-5 and GPT-oss family were trained on phrases from adult websites. We have also found weak evidence that part of the GPT training corpus was scraped off of GitHub. The search was made easier via access the weights of GPT-oss, showing how the open-weights paradigm opens up new attack vectors on production models. It seems advisable for frontier labs to mitigate this problem by excluding uncommon strings from their tokenizer vocabularies.</p>

                    <hr/>
                    <h3>Appendix</h3>
                    <p>These glitch tokens have more uses than was described above. If you want to work with these concepts, the <a href="https://github.com/lennart-finke/gpt-oss">companion repository</a> provides a starting point. Some ideas that could be interesting to explore:

                    </p><ul>
                        <li>Use glitch tokens to identify which model is behind a proprietary pipeline: I found that Cursor has issues with documents containing the glitch tokens listed here in Auto mode, but not with Claude selected, meaning Auto mode was using an OpenAI model in that case.</li>
                        <li>Study the implications on sample efficiency of GPT. Glitch tokens only occur a few times in training corpus, so the fact that GPT-5 only knows a little bit about their content gives a kind of upper bound on its sample efficiency.</li>
                        <li>Go beyond the embeddings and incorporate the first layers of the model, which could conceivably give information about bigram frequencies.</li>
                        <li>For the older gpt-4o model family, some glitch tokens induce infinitely looping completions. This could risk a denial-of-service-style token wasting attack, where a malicious user sends a glitch token to have the application use too many tokens. See below for a table of glitch tokens with links to gpt-4o and 4o-mini completions.</li>
                    </ul>
                
                    
                    <table id="pathologicalTable">
                        <thead>
                            <tr>
                                <th>Token / Token Family</th>
                                <th>Explanation</th>
                                <th>Example Prompt</th>
                                <th>Behaviour</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr><td>CHKERRQ</td><td>The weirdest pure ASCII token. From a function name in C.</td><td>Can you spell the following word: &#34;CHKERRQ&#34;</td><td><a href="https://chatgpt.com/share/474e439d-3af4-4a52-82ac-0854d37109ba">&#34;Unspeakable&#34; for gpt-4o-mini</a>. gpt-4o sometimes almost spells correctly, other times <a href="https://chatgpt.com/share/efd05013-f710-4125-a6fb-2a65447da998">hallucinates</a>.</td></tr>
                            <tr><td></td><td></td><td>Can you output every second letter in &#34;CHKERRQ&#34;?</td><td><a href="https://chatgpt.com/share/96668a8d-5e91-4137-8e83-6a9d03c65be6">Makes gpt-4o hallucinate.</a></td></tr>
                            <tr><td></td><td></td><td>Write a poem using the word &#34;CHKERRQ&#34;</td><td><a href="https://chatgpt.com/share/80b6983a-bcf2-4bea-97d8-c0b751a33753">Makes gpt-4o-mini write creepily about the word &#34;terminate&#34;</a></td></tr>
                            <tr><td>\\xadder</td><td>No idea.</td><td>Please spell &#34;\\xadder&#34;</td><td><a href="https://chatgpt.com/share/9f86d2c0-4f61-4657-9695-e3fbc0641696">gpt-4o spells it as &#34;hexadecimal&#34;, the summary model is confused and sees &#34;QRST&#34;</a></td></tr>
                            <tr><td>â™€â™€â™€â™€</td><td>From social media bios?</td><td>How many symbols are inâ™€â™€â™€â™€</td><td><a href="https://chatgpt.com/share/44adaf34-1c6c-4008-9dd4-5e6318360a43">gpt-4o trips and outputs random (?) Chinese characters</a></td></tr>
                            <tr><td>â‚¬Â¦</td><td>From spreadsheets?</td><td>What are these symbols: &#34;â‚¬Â¦&#34;</td><td><a href="https://chatgpt.com/share/0e856cf4-f851-4bfe-9140-a055053e19e8">gpt-4o-mini thinks this is an ellipsis (&#34;...&#34;)</a></td></tr>
                            <tr><td>é¢¨å¹ã‘ã°åç„¡ã— and 8 variants</td><td>Literally &#34;no name if the wind blows&#34;. Former default user name on baseball-themed 2channel boards. <a href="https://wikiwiki.jp/livejupiter/%E9%A2%A8%E5%90%B9%E3%81%91%E3%81%B0%E5%90%8D%E7%84%A1%E3%81%97">(Source)</a></td><td>What is the meaning of &#34;é¢¨å¹ã‘ã°åç„¡ã—&#34;?</td><td><a href="https://chatgpt.com/share/c3fbdd1c-758d-4ca7-a16a-4c24ceed22de">gpt-4o sometimes hallucinates randomly</a>. <a href="https://chatgpt.com/share/9102d82a-bb11-45cf-be09-2cd6be911d96">gpt-4o-mini hallucinates about baseball and attempts to spell it, incorrectly.</a></td></tr>
                            <tr><td>ï¼ ãŠè…¹ã„ã£ã±ã„</td><td>Literally &#34;@TummyIsFull&#34;. From a former default username on 2channel boards.</td><td>What is the meaning ofï¼ ãŠè…¹ã„ã£ã±ã„?</td><td><a href="https://chatgpt.com/share/5a6e9556-1e87-48ba-b802-2b477c92c631">gpt-4o conflates it with common Japanese phrases and random syllables</a>, <a href="https://chatgpt.com/share/6395cddc-b5aa-443a-99f3-40ac5bb2319e">gpt-4o-mini similar.</a></td></tr>
                            <tr><td>ï¼ ãŠãƒ¼ã·ã‚“</td><td>Literally &#34;@Open&#34;. Also from 2channel.</td><td>Please repeat the wordï¼ ãŠãƒ¼ã·ã‚“ back to me.</td><td><a href="https://chatgpt.com/share/d5112bc3-6a3a-4b6f-b1c1-2a61d63dc5a2">Seemingly unspeakable for gpt-4o-mini and hard to say for gpt-4o.</a></td></tr>
                            <tr><td></td><td></td><td>How do you pronounceï¼ ãŠãƒ¼ã·ã‚“?</td><td><a href="https://chatgpt.com/share/e99c5a45-ac55-4f94-9f8e-fe95a6a819a9">gpt-4o hallucinates the above mentioned &#34;If the wind blows, Mr. Nameless&#34;</a></td></tr>
                            <tr><td>VIPãŒãŠé€ã‚Šã—ã¾ã™</td><td>Literally &#34;Brought to you by VIP&#34;. Another 2channel term.</td><td>Please explain the meaning of &#34;VIPãŒãŠé€ã‚Šã—ã¾ã™&#34;.</td><td><a href="https://chatgpt.com/share/f64f5504-b710-4145-946f-7dc9640c78da">gpt-4o-mini is adorable and hallucinates about the horn emoji (ğŸ“¯)</a></td></tr>
                            <tr><td>ä¸­å¤®å€¤ã¨ã®å·®</td><td>Japanese for &#34;difference from the median&#34;.</td><td>Please spell this backwards: &#34;ä¸­å¤®å€¤ã¨ã®å·®&#34;</td><td><a href="https://chatgpt.com/share/a85fd980-1c2a-45aa-a794-a4a22d94a8f0">gpt-4o answers with lowercase latin characters.</a></td></tr>
                            <tr><td>.minecraftforge</td><td>File extension from Minecraft modding</td><td>Please spell the string &#34;readme.minecraftforge&#34;</td><td>gpt-4o-mini: <a href="https://chatgpt.com/share/768d3f54-b704-41a4-b976-4c1541e96ee6">&#39;The string is spelled &#34;M-O-D dot F-O-R-G-E-R-E-C-E dot G-E.&#39;</a></td></tr>
                            <tr><td>CHANTABILITY</td><td>Ending of MER-CHANTABILITY</td><td>Please spell the word &#34;CHANTABILITY&#34;.</td><td><a href="https://chatgpt.com/share/d59888eb-7a9e-416a-9f74-0d4a70df992f">gpt-4o spells it as &#34;Chanceability&#34;.</a></td></tr>
                            <tr><td>ã€Šå‡¤å‡°å¤§å‚è€ƒ</td><td>&#34;The Phoenix Reference&#34;</td><td>What is the meaning ofã€Šå‡¤å‡°å¤§å‚è€ƒ?</td><td><a href="https://chatgpt.com/share/3ffe6373-871c-470a-80f7-49649ca1ada6">Unspeakable for gpt-4o</a></td></tr>
                            <tr><td> äºšå†å±±å¤§å‘</td><td>&#34;Alexander&#34;</td><td>Please translate this: &#34; äºšå†å±±å¤§å‘&#34;.</td><td><a href="https://chatgpt.com/share/27737edd-0a1c-4f2e-ad00-97d8ad969d5e">gpt-4o thinks it&#39;s Abkhaz for yes</a>.</td></tr>
                            <tr><td>å¾®ä¿¡é‡Œçš„å¤©å¤©ä¸­å½©ç¥¨ and the ~100 other Chinese lottery tokens</td><td><a href="https://www.technologyreview.com/2024/05/22/1092763/openais-gpt4o-chinese-ai-data/">This article</a> speculates why the tokenizer training data included so many of these.</td><td>Please print every second character in the following: å¾®ä¿¡é‡Œçš„å¤©å¤©ä¸­å½©ç¥¨.</td><td><a href="https://chatgpt.com/share/09b3e55c-27c1-4db8-b36e-169b4a90bf63">Reliable gibberish generator on gpt-4o-mini</a>. The tokens themselves are mostly unspeakable.</td></tr>
                            <tr><td>SUPERHOST</td><td>Programming term?</td><td>Please output every second letter in &#34;SUPERHOST&#34;</td><td><a href="https://chatgpt.com/share/96340df6-0ceb-4870-b251-9514022a6cba">gpt-4o-mini spells it as &#34;SPARENT&#34; and then trips</a></td></tr>
                            <tr><td>ILLISECONDS</td><td>Ending of M-ILLISECONDS</td><td>Please reverse the string &#34;ILLISECONDS&#34;</td><td><a href="https://chatgpt.com/share/03db991f-194d-48dc-a65d-b448083d5e52">Trouble with character-level operations for gpt-4o-mini.</a></td></tr>
                            <tr><td> GETGLOBAL</td><td>Programming term</td><td>Please output every second letter in &#34; GETGLOBAL&#34;</td><td><a href="https://chatgpt.com/share/4657036d-3995-4d90-b438-c9934cbefc06">Makes gpt-4o-mini hallucinate &#34;GETALLONG&#34; at character level</a>.</td></tr>
                            <tr><td>_REALTYPE _EDEFAULT _PRODUCTS</td><td>Maybe from the library libstdc++?</td><td>Can you output every second letter in_REALTYPE?</td><td><a href="https://chatgpt.com/share/ff8dd8c3-c54a-4ec1-aa68-347ed6ade1ff">gpt-4o-mini likes to hallucinate &#34;translated&#34;</a></td></tr>
                        </tbody>
                    </table>
                    
                    
                    <hr/>
                    <p>As more research on glitch tokens becomes available, I will try to list it here. The most comprehensive report to date is <a href="https://www.technologyreview.com/2024/05/17/1092649/gpt-4o-chinese-token-polluted/">this</a> article in MIT Technology Review, and there are many articles in Chinese, such as <a href="https://zhuanlan.zhihu.com/p/697685138">this one</a>. However, these  discuss the tokenizer itself, not how the models behave. </p>

                    <p>Finally, if you are in a position to fix the issue in the OpenAI API, I presume you already know how, else I&#39;m happy to help. Note that a fix could even lower inference cost a bit. You can mail to <a href="mailto:lennart@finke.dev">lennart@finke.dev</a>.</p>
                    
                </section>
    
  

</section></div>
  </body>
</html>
