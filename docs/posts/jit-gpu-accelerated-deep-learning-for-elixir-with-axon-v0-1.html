<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://seanmoriarity.com/2022/06/16/jit-gpu-accelerated-deep-learning-for-elixir-with-axon-v0-1/">Original</a>
    <h1>JIT/GPU accelerated deep learning for Elixir with Axon v0.1</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<p>I am excited to announce the official v0.1.0 release of <a href="https://github.com/elixir-nx/axon">Axon</a> and <a href="https://github.com/elixir-nx/axon_onnx">AxonOnnx</a>. A lot has changed (and improved) since the initial <a href="https://seanmoriarity.com/2021/04/08/axon-deep-learning-in-elixir/">public announcement of Axon</a>. In this post I will explore Axon and its internals, and give reasoning for some of the design decisions made along the way.</p>



<p>You can view the official documentation here:</p>



<ul><li><a href="https://hexdocs.pm/axon/Axon.html">Axon</a></li><li><a href="https://hexdocs.pm/axon_onnx/AxonOnnx.html">AxonOnnx</a></li></ul>



<h2>What is Axon?</h2>



<p>At a high-level, Axon is a library for creating and training neural networks. Axon is implemented in pure Elixir and relies on <a href="https://github.com/elixir-nx/nx">Nx</a> to compile Neural Networks to the CPU/GPU just-in-time. It consists of a few components which are loosely tied together:</p>



<h3>Functional API</h3>



<p>The functional API are “low-level” implementations of common neural network operations. It’s similar to <code>torch.functional</code> or <code>tf.nn</code> in the Python ecosystem. The functional API offers no conveniences—just implementations. These implementations are all written in <code>defn</code>, so they can be JIT compiled, or composed with Nx transformations like <code>grad</code> in other <code>defn</code> functions.</p>



<p>The Functional API consists of:</p>



<ul><li><a href="https://hexdocs.pm/axon/Axon.Activations.html">Axon.Activations</a></li><li><a href="https://hexdocs.pm/axon/Axon.Initializers.html">Axon.Initializers</a></li><li><a href="https://hexdocs.pm/axon/Axon.Layers.html">Axon.Layers</a></li><li><a href="https://hexdocs.pm/axon/Axon.Losses.html">Axon.Losses</a></li><li><a href="https://hexdocs.pm/axon/Axon.Metrics.html">Axon.Metrics</a></li><li><a href="https://hexdocs.pm/axon/Axon.Recurrent.html">Axon.Recurrent</a></li></ul>



<h3>Model Creation API</h3>



<p>The model creation API is a high-level API for creating and executing neural networks. The API will be covered in-depth in this post, so I’ll omit the details here.</p>



<h3>Optimization API</h3>



<p>The optimization API is built to mirror the beautiful <a href="https://github.com/deepmind/optax">Optax</a> library. Optax is literally my favorite library that’s not written in Elixir. The idea is to implement optimizers using composable higher-order functions. I highly recommend checking out the <a href="https://hexdocs.pm/axon/Axon.Updates.html">Axon.Updates</a> documentation as well as the Optax library.</p>



<h3>Loop API</h3>



<p>The Loop API is an API for writing loops (like training and evaluation loops) in a functional style. Elixir is a functional language, which means we cannot take advantage of mutable state in the same way you would be able to in Python frameworks. To get around this, <code>Axon.Loop</code> constructs loops as reducers over data with some state. The API itself is inspired by the <a href="https://pytorch.org/ignite/index.html">PyTorch Ignite</a> library.</p>



<p>The Loop API is still a work-in-progress, so you should expect significant improvements in subsequent Axon releases.</p>



<h2>What really is a Neural Network?</h2>



<p>There are really two interpretations of this question I’d like to explore:</p>



<ol><li>What is a neural network <em>mathematically</em>?</li><li>What is a neural network in the eyes of Axon?</li></ol>



<p>Mathematically a neural network is just a composition of linear and non-linear transformations with some learnable parameters. In Nx, you can implement a neural network relatively easily with <code>defn</code>:</p>


<div><pre title="">defn feed_forward_network(x, w1, b1, w2, b2, w3, b3) do
  x
  |&gt; Nx.dot(w1)
  |&gt; Nx.add(b1)
  |&gt; Nx.sigmoid()
  |&gt; Nx.dot(w2)
  |&gt; Nx.add(b2)
  |&gt; Nx.sigmoid()
  |&gt; Nx.dot(w3)
  |&gt; Nx.add(b3)
  |&gt; Nx.sigmoid()
end
</pre></div>


<p>There’s really nothing more to it! Of course, implementing neural networks in Nx now introduces a lot of painful boilerplate. The goal of Axon is to abstract away the boilerplate, and make creating and training neural networks a breeze in Elixir.</p>



<p>So what is a neural network in the eyes of Axon? Axon sees a neural network as an Elixir struct:</p>


<div><pre title="">  defstruct [
    :id,
    :name,
    :output_shape,
    :parent,
    :parameters,
    :args,
    :op,
    :policy,
    :hooks,
    :opts,
    :op_name
  ]
</pre></div>


<p>Of particular importance in this struct are: <code>parent</code>, <code>parameters</code>, and <code>op</code>. <code>parent</code> is a list of parent networks which are also Axon structs. It’s a recursive data structure which represents a computation graph with some additional metadata relevant to specific neural network tasks. <code>parameters</code> represent a list of trainable parameters attached to this layer. They’re automatically initialized when the model is initialized, and will be part of the training process within Axon’s internal APIs. <code>op</code> is a function that’s applied on <code>parent</code> and <code>parameters</code>. In laymen’s terms, Axon views a neural network as just a function of other “neural networks” (Axon structs) and trainable parameters. In fact, you can “wrap” any function you want into a neural network with <code>Axon.layer</code>:</p>



<pre>defn dense(input, weight, bias, _opts \\ []) do
  input
  |&gt; Nx.dot(weight)
  |&gt; Nx.add(bias)
end

input = Axon.input({nil, 32}, &#34;features&#34;)
weight = Axon.param({32, 64}, &#34;weight&#34;)
bias = Axon.param({64}, &#34;bias&#34;)

Axon.layer(&amp;dense/4, [input, weight, bias])</pre>



<p>Notice I only had to define an <code>input</code> layer and two trainable parameters using Axon’s built-in function. Using <code>Axon.layer</code> should feel a lot like using Elixir’s <code>apply</code> — you’re just applying a function to some specialized inputs. All but a few of Axon’s built-in layers are implemented in essentially this same manner:</p>



<ol><li>Define an implementation function in <a href="https://github.com/elixir-nx/axon/blob/main/lib/axon/layers.ex">Axon.Layers</a></li><li>Wrap the implementation in a layer with a public interface in <a href="https://github.com/elixir-nx/axon/blob/main/lib/axon.ex">Axon</a></li></ol>



<h2>It’s just a Graph</h2>



<p>The “magic” of Axon is its compiler, which knows how to convert Axon structs into meaningful initialization and prediction functions. Model execution comes in the form of two functions: <code>Axon.init/3</code> and <code>Axon.predict/4</code>. <code>Axon.init/3</code> returns a model’s initial parameters:</p>



<pre>model = Axon.input({nil, 32}) |&gt; Axon.dense(64)

model_state = Axon.init(model)</pre>



<p>For prediction, you need both a model and a compatible model state:</p>



<pre>model = Axon.input({nil, 32}) |&gt; Axon.dense(64)

model_state = Axon.init(model)
input = Nx.random_uniform({1, 32})

Axon.predict(model, model_state, input)</pre>



<p>Both <code>Axon.init/3</code> and <code>Axon.predict/4</code> take additional compilation options; however, it’s recommended you use global configuration rather than compilation options. For example, rather than:</p>



<pre>Axon.predict(model, model_state, input, compiler: EXLA)</pre>



<p>You should use:</p>



<pre>EXLA.set_as_nx_default([:tpu, :cuda, :rocm, :host])

Axon.predict(model, model_state, input)</pre>



<p><code>Axon.init/3</code> also optionally accepts initial parameters to initialize portions of a model from an initial state (e.g. if trying to fine-tune a model). This is where <code>Axon.namespace/2</code> comes in handy. You can “tag” a part of a model as belonging to a particular namespace, and initialize without needing to know anything about the underlying architecture:</p>



<pre>{bert, bert_params} = get_bert_model()
bert = bert |&gt; Axon.namespace(&#34;bert&#34;)

model = bert |&gt; Axon.dense(1)

model_state = Axon.init(model, %{&#34;bert&#34; =&gt; bert_params})</pre>



<p><code>Axon.namespace/2</code> is one of the few layers with special meaning in Axon. There’s also <code>Axon.input/2</code>, <code>Axon.constant/3</code>, and <code>Axon.container/3</code>. Input layers are symbolic representations of model inputs. Each input is associated with a unique name used to reference it when passing names to a model. For example, if you have multiple inputs, you can give them semantic meanings:</p>



<pre>text_features = Axon.input({nil, 32}, &#34;text_features&#34;)
cat_features = Axon.input({nil, 32}, &#34;cat_features&#34;)</pre>



<p>With named inputs, you don’t have to worry about passing things out of order, since you’ll always reference an input by it’s name:</p>



<pre>model = Axon.add(text_features, cat_features)

Axon.predict(model, model_state, %{&#34;text_features&#34; =&gt; text_inp, &#34;cat_features&#34; =&gt; cat_inp})</pre>



<p><code>Axon.constant/3</code> allows you to introduce constant-values into the graph. Be warned that introducing large constants will have negative impacts on the performance of the model.</p>



<p><code>Axon.container/3</code> can accept any valid <a href="https://hexdocs.pm/nx/Nx.Container.html">Nx container</a>. This is particularly useful for giving semantic meaning to outputs:</p>



<pre>model = Axon.container(%{
  last_hidden_state: last_hidden_state,
  pooler_output: pooler_output
})</pre>



<p><strong>Every other Axon built-in layer is treated in the same way as custom layers by the compiler.</strong> This means that (besides for the few “special layers”) there’s no difference between what you can do with a custom layer and what you can do with a built-in layer. They’re both handled <a href="https://github.com/elixir-nx/axon/blob/a859be766e52d89e1140060d1e52e79667bd6fa1/lib/axon/compiler.ex#L369">by the same clause in the Axon compiler</a>.</p>



<p>In the Axon interpretation of a neural network, every <em>execution</em> of a graph is seen as a <strong>specialized compilation of the graph</strong>. In other words, initialization and prediction are just two types of compilation. There’s nothing stopping you from implementing your own specialized compilation of an Axon graph in the same way. For example, an older version of Axon implemented a macro <code>Axon.penalty</code> which compiled a graph into a regularization function. Axon also implements the <code>Inspect</code> protocol—which itself can be seen as a symbolic compilation of the graph.</p>



<h2>Maybe you don’t like my API…</h2>



<p>The Axon interpretation of a “model” is intentionally as flexible as possible. All you need to do is build a data structure. This means that if you’re not satisfied with Axon’s model creation API, you can create your own! As long as you finish with an Axon struct, your model will work with the rest of Axon’s components. The Axon struct is really the unifying data structure for every component of the Axon library. I would love to see some cool Neural Network DSLs pop-up which build off of the lower-level components Axon defines.</p>



<h2>Converting to Other Formats</h2>



<p>Another benefit of the Axon data structure is portability. If you can traverse the Axon graph, you can lower or compile it into a meaningful function or representation, such as ONNX. This is exactly the functionality AxonOnnx provides—you can take a pre-trained model from popular frameworks like PyTorch and TensorFlow, convert them to ONNX, and then import them into Elixir with <code>AxonOnnx.import</code>. For example, you can take any of the ONNX supported models in <a href="https://huggingface.co/docs/transformers/serialization">HuggingFace Transformers</a> and import them in Axon with ease!</p>



<p>Just export the model you want:</p>


<div><pre title="">$ python -m transformers.onnx --model=bert-base-cased models/

</pre></div>


<p>And load it with AxonOnnx:</p>



<pre>{bert, bert_params} = AxonOnnx.import(&#34;path/to/bert.onnx&#34;)</pre>



<p>The ability to import and use external models is an absolute <em>must</em> for any Neural network library (especially given the pace of progress in deep learning). AxonOnnx enables Elixir programmers to utilize pre-trained models from the Python ecosystem without needing to implement or train them from scratch.</p>



<p>This also means you can integrate some pretty cool pre-trained models with established projects like Phoenix and LiveView. For example <a href="https://github.com/thehaigo/live_onnx">live_onnx</a>, implements a sample ML application using AxonOnnx and LiveView.</p>



<p>You should note that we are still actively working to enable support for all of ONNX’s operations. If you have a model you’d like to see supported, please feel free to open an issue or a PR 🙂</p>



<h2>Future Work</h2>



<p>If you look at the <a href="https://github.com/elixir-nx/axon/issues">issues tracker</a> you’ll notice there’s still much work to be done; however, the core components of Axon are at a stable point. This means you can use Axon with a reasonable expectation of stability. Moving forward, you can expect the following from Axon:</p>



<ul><li>First-class transformer model support</li><li>More integration with <a href="https://livebook.dev/">Livebook</a></li><li>Mixed precision training</li><li>Multi-device training</li></ul>



<p>Additionally, I’d like to build out a large collection of Axon examples. If you are looking for a place to get started in the Nx ecosystem, please feel free to open a pull request which demonstrates Axon applied to a unique problem set. If you’re looking for inspiration, check out <a href="https://keras.io/examples/">Keras Examples</a>.</p>



<h2>Acknowledgements</h2>



<p>I am very grateful to <a href="https://dockyard.com/">DockYard</a> and their support of the Elixir Machine Learning Ecosystem from the beginning. Additionally, Axon would not be where it is today without the hard work of all of the Nx contributors and the individuals Erlang Ecosystem Foundation ML WG. The Elixir community is nothing short of amazing, and I hope Axon can play a small part in seeing the community grow.</p>



<h2><a href="https://gist.github.com/josevalim/8c1fcee737f28ca188ebfb020540536c#functional-api"></a></h2>
	</div></div>
  </body>
</html>
