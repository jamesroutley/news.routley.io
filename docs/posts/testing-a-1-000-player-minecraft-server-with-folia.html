<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cubxity.dev/blog/folia-test-june-2023">Original</a>
    <h1>Testing a 1,000 player Minecraft server with Folia</h1>
    
    <div id="readability-page-1" class="page"><article><h2>Introduction</h2><p><a target="_blank" rel="noreferrer" href="https://papermc.io/software/folia">Folia</a> emerges as a promising fork of <a target="_blank" rel="noreferrer" href="https://papermc.io/software/paper">Paper</a>, boasting an innovative implementation of regionalized multithreading on the server. Traditional Minecraft servers have always faced limitations when it came to player capacity, often struggling to support more than a few hundred players at a time. This is because Minecraft servers primarily rely on a single thread to handle all game logic and player interactions. </p><p><a rel="noreferrer" href="https://cubxity.dev/">Myself</a>, <a target="_blank" rel="noreferrer" href="https://github.com/spottedleaf">Spottedleaf</a>, and <a target="_blank" rel="noreferrer" href="https://iptables.sh/">Michael</a> conducted this test to evaluate and analyze Folia&#39;s performance and stability under various conditions. We would like to thank <a target="_blank" rel="noreferrer" href="https://www.twitch.tv/tubbo">Tubbo</a> for streaming this test event.</p><h2>Our Test</h2><p>We wanted to conduct a test with Folia and see how it can perform on “regular” hardware and configurations. The <a target="_blank" rel="noreferrer" href="https://paper-chan.moe/folia/#records">previous public test</a> ran on absurdly powerful hardware, which would not be realistic in many of the use cases. However, it&#39;s important to note that this test only provides a glimpse into the potential of Folia and its regionalized multithreading capabilities.</p><p>The purpose of this test was to gather as much data as possible, while testing different game configurations and seeing how they performed. </p><h2>Configuration</h2><h3>Hardware</h3><div><p><img alt="Neofetch on our test machine." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1076%2FfyWKy0IRyu6gKpCyNHbg&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Neofetch on our test machine.</p></div><p>Our test was conducted on Hetzner’s <a target="_blank" rel="noreferrer" href="https://www.hetzner.com/dedicated-rootserver/ax102">AX102</a> with the following configuration:</p><ul><li><strong>CPU</strong>: AMD Ryzen 9 7950X3D</li><li><strong>RAM</strong>: 128GB DDR5</li><li><strong>Storage</strong>: 2 x 1.92 TB NVMe SSD in RAID 1</li><li><strong>Networking</strong>: 10Gbps NIC and uplink</li></ul><h3>Software</h3><ul><li><strong>Distribution</strong>: Debian Bookworm (12)</li><li><strong>Kernel</strong>: <code>6.1.0-9-amd64</code></li><li><strong>Java</strong>: <code>21-testing</code></li></ul><pre>$ uname -a
Linux test-fsn1-game01 6.1.0-9-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.27-1 (2023-05-08) x86_64 GNU/Linux

$ java -version
openjdk version &#34;21-testing&#34; 2023-09-19
OpenJDK Runtime Environment (build 21-testing-builds.shipilev.net-openjdk-jdk-shenandoah-b110-20230615)
OpenJDK 64-Bit Server VM (build 21-testing-builds.shipilev.net-openjdk-jdk-shenandoah-b110-20230615, mixed mode, sharing)</pre><h3>Minecraft</h3><p>Our Minecraft server was running Minecraft 1.20.1 on Folia build <a target="_blank" rel="noreferrer" href="https://github.com/PaperMC/Folia/compare/7c6e2514d2ecd9f5f7db911aab29bfe7f373eb9e...Cubxity:Folia:09d8e7d4a871561e44ab851676dfd0d32dddf649"><code>09d8e7d</code></a> (Oops). The server ran with a 100 GiB heap allocated, and <a target="_blank" rel="noreferrer" href="https://wiki.openjdk.org/display/shenandoah/Main">Shenandoah GC</a> was used as the garbage collector. Furthermore, Spottedleaf and Michael decided that we should try <a target="_blank" rel="noreferrer" href="https://openjdk.org/jeps/404">generational Shenandoah GC</a> in OpenJDK 21. </p><div><p><img alt="Our conversation about Java 21." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A708%2Cheight%3A129%2FOTQoTEpdSC6UJxMppWf1&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Our conversation about Java 21.</p></div><h4>Paper Configuration</h4><p>config/paper-global.yml:</p><pre>chunk-loading-basic:
  player-max-chunk-generate-rate: 40.0
  player-max-chunk-load-rate: 40.0
  player-max-chunk-send-rate: 40.0
chunk-system:
  io-threads: 2
  worker-threads: 1
misc:
  region-file-cache-size: 512
proxies:
  proxy-protocol: true
thread-regions:
  threads: 6</pre><p>config/paper-world-defaults.yml:</p><pre>environment:
  treasure-maps:
    enabled: false</pre><h4>Spigot Configuration</h4><pre>settings:
  netty-threads: 6</pre><h4>Bukkit Configuration</h4><pre>spawn-limits:
  monsters: 9
  animals: 7
  water-animals: 4
  water-ambient: 7
  water-underground-creature: 3
  axolotls: 3
  ambient: 4
ticks-per:
  monster-spawns: 30
  water-spawns: 30
  water-ambient-spawns: 30
  water-underground-creature-spawns: 30
  axolotl-spawns: 30
  ambient-spawns: 30</pre><h4>Minecraft Configuration</h4><pre>allow-nether=false
hide-online-players=true
max-players=1001
network-compression-threshold=-1
spawn-protection=0
simulation-distance=5
view-distance=8</pre><h4>JVM Flags</h4><pre>-Xms100G
-Xmx100G
-XX:+AlwaysPreTouch
-XX:+UnlockDiagnosticVMOptions
-XX:+UnlockExperimentalVMOptions
-XX:+HeapDumpOnOutOfMemoryError
-XX:+UseLargePages
-XX:LargePageSizeInBytes=2M
-XX:+UseShenandoahGC
-XX:ShenandoahGCMode=generational
-XX:-ShenandoahPacing
-XX:+ParallelRefProcEnabled
-XX:ShenandoahGCHeuristics=adaptive
-XX:ShenandoahInitFreeThreshold=55
-XX:ShenandoahGarbageThreshold=30
-XX:ShenandoahMinFreeThreshold=20
-XX:ShenandoahAllocSpikeFactor=10
-XX:ParallelGCThreads=10
-XX:ConcGCThreads=3
-Xlog:gc*:logs/gc.log:time,uptime:filecount=15,filesize=1M
-Dchunky.maxWorkingCount=600</pre><p><em>JMX flags were stripped.</em></p><h4>Initial Thread Allocations</h4><ul><li><strong>GC</strong>: 3 concurrent</li><li><strong>Chunk System IO</strong>: 2</li><li><strong>Chunk System Worker</strong>: 1</li><li><strong>Netty</strong>: 6</li><li><strong>Region Threads</strong>: 6</li></ul><p><ins><strong>Total: 18</strong></ins></p><h3>Tools</h3><ul><li><a target="_blank" rel="noreferrer" href="https://github.com/cubxity/unifiedmetrics">UnifiedMetrics</a>: Plugin used to export Minecraft server metrics.</li><li><a target="_blank" rel="noreferrer" href="https://github.com/pop4959/Chunky">Chunky</a>: Plugin used to pre-generate chunks.</li><li><a target="_blank" rel="noreferrer" href="https://github.com/prometheus/node_exporter">node_exporter</a>: Used to export machine metrics.</li><li><a rel="noreferrer" href="https://victoriametrics.com/">VictoriaMetrics</a>: Used to scrape and store metrics data (Prometheus compatible).</li><li><a target="_blank" rel="noreferrer" href="https://grafana.com/">Grafana</a>: Observability platform used to visualize and monitor metrics.</li><li><a target="_blank" rel="noreferrer" href="https://visualvm.github.io/">VisualVM</a>: Tool used to monitor JMX metrics.</li></ul><h2>Methodology</h2><p>The server was prepared with a 100k x 100k block pre-generated world. Our custom plugin distributed new players to the least-occupied region. We had predefined spawn points as shown below. The reasoning behind this was to prevent concentrated areas with a high number of players. Furthermore, Folia benefits from having multiple regions due to its <a target="_blank" rel="noreferrer" href="https://docs.papermc.io/folia/reference/region-logic">regionalized multithreading implementation</a>, allowing for better utilization of CPU resources and improved performance.</p><div><p><img alt="Spawn points plotted on a plane." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A688%2Cheight%3A663%2Fjs0YVNHSUqn2FGCoWy1A&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Spawn points plotted on a plane.</p></div><p>The test was presented as an event and was streamed by Tubbo. Players were spread into 49 different teams across the map. Each team consisted of around 20 players.</p><h2>Results</h2><p>The event started around <strong>16:00 UTC</strong> and we were able to gather <strong>1,000</strong> players on the server. </p><div><p><img alt="1,000 players shown in Grafana." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A841%2FAxISWOStCmeyIMv2VayQ&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>1,000 players shown in Grafana.</p></div><p>Shortly after we unfroze the players, we experienced some lag. This lasted for ~1 minute or so. We suspected that this may be due to the sudden player movements overwhelming the Netty threads. We peaked at almost 2 Gbps outbound then.</p><div><p><img alt="Network throughput graph on Grafana." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A810%2FpA5rLClKSCOHhAikycmx&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Network throughput graph on Grafana.</p></div><p>The server ran fine for a while until it didn&#39;t. We had 6 ticking regions, which were completely utilized. A normal Paper server ticks on a single thread, and can probably handle 100 players with optimized settings. By using the same logic, we should&#39;ve had <em>at least</em> 10 threads for things to run smoothly. However, Folia has more overhead than a normal Paper server due to scheduling, so that should be kept in mind.</p><div><p><img alt="Output of /tps." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1158%2F92bnNDnRa6yZSlFBDrLd&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Output of /tps.</p></div><p>Despite the lag, the server ran surprisingly well. The server ran on a consumer-grade CPU and a commonly available hardware configuration. Sustaining 1,000 players at a playable TPS could&#39;ve been possible with better thread allocations, but, we don&#39;t know for sure since we never got the chance to test that out. Our CPU usage was hovering around 10-14 logical cores out of 32. This meant that we could&#39;ve potentially allocated more region threads and IO threads. However, we only had 16 physical cores available, so pushing the usage over 16 logical cores may have resulted in decreased performance, depending on how the workload is scheduled.</p><div><p><img alt="JVM CPU metrics in Grafana." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A884%2F3AeQ0gZQVeuWG5c3Fq3O&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>JVM CPU metrics in Grafana.</p></div><p>After a while, our server crashed. Preliminary analysis suggests that this may have been caused by an unforeseen bug in the custom patch implemented. <a target="_blank" rel="noreferrer" href="https://github.com/Cubxity/Folia/commit/9b5ea4906952ef3285ee31a372a6f0b9f589d8f7">The fix</a> was supposed to be deployed before the test, but we built Folia from the wrong branch. Regardless, we also took the opportunity to increase the thread counts:</p><ul><li><strong>Netty</strong>: 6 → 10</li><li><strong>Region Threads</strong>: 6 → 12</li></ul><p>Soon after, the server was up and running with our patch applied. We had ~630 concurrent players and it performed well at a constant 20 TPS.</p><div><p><img alt="Smooth 20 TPS with 600 players." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A839%2FcXeLpPwQTdunMngsn1CJ&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Smooth 20 TPS with 600 players.</p></div><div><p><img alt="Output from /tps." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1122%2F2SPr8yJIT0OgR2bjxfte&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Output from /tps.</p></div><p>For fun, we enabled chat for a short duration and everyone got kicked with the following message:</p><div><p><img alt="Oops." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A1011%2FwD5jD3GReCG2gZwL1Afw&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Oops.</p></div><p>This has not been investigated yet, but it is most likely related to incorrect handling of chat signing. It is unknown whether this is related to Folia.</p><p>We were using <a target="_blank" rel="noreferrer" href="https://openjdk.org/jeps/404">generational Shenandoah GC</a> in Java 21. During the period when 1,000 players were online, we reached a maximum of ~7.9GB/s heap allocation and our GC was hovering around 2-3GB/s when averaged over a minute. </p><div><p><img alt="Heap allocation graph." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A807%2FfexrbgndTyqyRrFfjCOz&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>Heap allocation graph.</p></div><div><p><img alt="GC throughput graph." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A790%2Fzwe3GL2pRl6hUCfzSbKt&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>GC throughput graph.</p></div><p>During the entire test, our GC pauses were mostly fine. The median GC pause duration was ~3 ms. </p><div><p><img alt="GC pauses graph." loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=640&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 640w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=750&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 750w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=828&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 828w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=1080&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1080w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=1200&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1200w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=1920&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 1920w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=2048&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 2048w, /_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ 3840w" src="https://cubxity.dev/_next/image?url=https%3A%2F%2Fmedia.graphassets.com%2Fresize%3Dwidth%3A1910%2Cheight%3A828%2FZ6hy6RGNRjStKJeyxO9x&amp;w=3840&amp;q=75&amp;dpl=dpl_H7XmF3Naqp53pTzV2RFSK1BFhTrZ"/></p><p>GC pauses graph.</p></div><h2>Conclusion</h2><ul><li>Folia ran <strong>surprisingly well</strong> on our hardware. We could&#39;ve reached higher player counts if the threads were allocated accordingly.</li><li>We still don&#39;t have enough data to properly determine the optimal thread allocations.</li></ul><h2>What Next?</h2><ul><li>Performing another test on the same hardware with different thread allocations would be useful to determine the &#34;optimal&#34; settings.</li><li>Running more tests on similar hardware or other common configurations.</li></ul><h2>Links</h2><ul><li><a target="_blank" rel="noreferrer" href="https://snapshots.raintank.io/dashboard/snapshot/BKfFdyCQ7hEAB6KcF47MYGPzM7oOkN7d">Grafana snapshot (UnifiedMetrics)</a></li><li><a target="_blank" rel="noreferrer" href="https://snapshots.raintank.io/dashboard/snapshot/i162v2rj9BNvSgkwChdmq2NcTlFoLRgi">Grafana snapshot (node_exporter)</a></li><li><a target="_blank" rel="noreferrer" href="https://www.twitch.tv/videos/1849538310">Tubbo’s stream VOD</a></li><li><a target="_blank" rel="noreferrer" href="https://papermc.io/software/folia">Folia on PaperMC&#39;s website</a></li><li><a target="_blank" rel="noreferrer" href="https://github.com/papermc/folia">Folia&#39;s repository</a></li><li><a target="_blank" rel="noreferrer" href="https://paper-chan.moe/folia/">Paper Chan&#39;s Folia article</a></li></ul><h2>Thanks To</h2><ul><li><a target="_blank" rel="noreferrer" href="https://github.com/spottedleaf">Spottedleaf</a> for developing and maintaining Folia, and the organization behind it, <a target="_blank" rel="noreferrer" href="https://papermc.io/">PaperMC</a>.</li><li><a target="_blank" rel="noreferrer" href="https://iptables.sh/">Michael</a> for assisting with the test.</li><li>Innit, Inc. for providing the hardware and resources.</li><li><a target="_blank" rel="noreferrer" href="https://www.twitch.tv/tubbo">Tubbo</a> for engaging the audience and streaming the test.</li><li>Players who participated in the test!</li></ul><h2>Supporting Folia &amp; PaperMC</h2><p>Interested in supporting the development of Folia and PaperMC software? See <a target="_blank" rel="noreferrer" href="https://papermc.io/sponsors">sponsors</a>.</p><time datetime="2023-06-25T19:07:39.877895+00:00">Updated <!-- -->Sun Jun 25 2023</time></article></div>
  </body>
</html>
