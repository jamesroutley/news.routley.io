<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lucumr.pocoo.org/2025/6/12/agentic-coding/">Original</a>
    <h1>Agentic Coding Recommendations</h1>
    
    <div id="readability-page-1" class="page"><div>
        
  

  
  <p data-date="2025-06-12T00:00:00">written on Thursday, June 12, 2025</p>
  

  <p>There is currently an explosion of people sharing their experiences with
agentic coding.  After my last <a href="https://lucumr.pocoo.org/2025/6/4/changes/">two</a> <a href="https://lucumr.pocoo.org/2025/6/10/genai-criticism/">posts</a> on the topic, I received quite a few
questions about my own practices.  So, here goes nothing.</p>
<div id="preface">
<h2>Preface</h2>
<p>For all intents and purposes, here&#39;s what I do: I predominently use
<a href="https://www.anthropic.com/claude-code">Claude Code</a> with the cheaper
Max subscription for $100 a month <a href="#footnote-1" id="footnote-reference-1">[1]</a>.  That works well for several
reasons:</p>
<ul>
<li>I exclusively use the cheaper Sonnet model. It&#39;s perfectly adequate for
my needs, and in fact, I prefer its outputs over the more expensive Opus
model.</li>
<li>I optimize my tool usage to be token efficient. I avoid screenshots and
browser interactions wherever possible.  More on that later.</li>
</ul>
<p>My general workflow involves assigning a job to an agent (which
effectively has full permissions) and then waiting for it to complete the
task.  I rarely interrupt it, unless it&#39;s a small task.  Consequently, the
role of the IDE — and the role of AI in the IDE — is greatly diminished;
I mostly use it for final edits.  This approach has even revived my usage
of Vim, which lacks AI integration.</p>
<p>One caveat: I expect this blog post to age very poorly.  The pace of
innovation here is insane; what was true a month ago barely holds true
today.  That&#39;s why I&#39;m sticking to concepts I believe have staying power.</p>
<p>If you want to a small session of me working on an Open Source library
with it, I have <a href="https://www.youtube.com/watch?v=sQYXZCUvpIc">a recording you can watch</a>.</p>
</div>
<div id="the-basics">
<h2>The Basics</h2>
<p>I disable all permission checks.  Which basically means I run <tt>claude
<span>--dangerously-skip-permissions</span></tt>.  More specifically I have an alias
called <cite>claude-yolo</cite> set up.  Now you can call that irresponsible and
there are definitely risks with it, but you can manage those risks with
moving your dev env into docker.  I will however say that if you can watch
it do its thing a bit, it even works surprisingly well without
dockerizing.  YMMV.</p>
<p>MCP.  This is a term you cannot avoid.  It basically is a standardized
protocol to give agents access to more tools.  Honestly: at this point I
barely use it, but I do use it.  The reason I barely use it is because
Claude Code is very capable of just running regular tools.  So MCP for me
is really only needed if I need to give Claude access to something that
finds too hard to use otherwise.  A good example for this is the
<a href="https://github.com/microsoft/playwright-mcp">playwright-mcp</a> for
browser automation.  I use it because I haven&#39;t found anything better yet.
But for instance when I want my agent to poke around in my database, I
just uses whatever it finds to be available.  In my case it loves to use
<cite>psql</cite> and that&#39;s more than good enough.</p>
<p>In general I really only start using MCP if the alternative is too
unreliable.  That&#39;s because MCP servers themselves are sometimes not super
reliable and they are an extra thing that can go wrong.  Trying to keep
things very simple.  My custom tools are normal scripts that it just runs.</p>
</div>
<div id="choice-of-language">
<h2>Choice Of Language</h2>
<p>I&#39;ve evaluated agent performance across different languages my workload,
and if you can choose your language, I strongly recommend Go for new
backend projects.  Several factors strongly favor Go:</p>
<ul>
<li><strong>Context system:</strong> Go provides a capable copy-on-write data bag that
explicitly flows through the code execution path, similar to contextvars
in Python or .NET&#39;s execution context.  Its explicit nature greatly
simplifies things for AI agents.  If the agent needs to pass stuff to
any call site, it knows how to do it.</li>
<li><strong>Test caching:</strong> Surprisingly crucial for efficient agentic loops. In
Rust, agents sometimes fail because they misunderstand <cite>cargo test</cite>&#39;s
invocation syntax.  In Go, tests run straightforwardly and
incrementally, significantly enhancing the agentic workflow.  It does
not need to figure out which tests to run, go does.</li>
<li><strong>Go is sloppy:</strong> Rob Pike famously described Go as suitable for
developers who aren&#39;t equipped to handle a complex language.  Substitute
“developers” with “agents,” and it perfectly captures why Go&#39;s
simplicity benefits agentic coding.</li>
<li><strong>Structural interfaces:</strong> interfaces in Go are structural.  If a type
has the methods an interface expects, then it conforms.  This is
incredibly easy for LLMs to “understand”.  There is very little surprise
for the agent.</li>
<li><strong>Go has low eco-system churn:</strong> Go&#39;s entire ecosystem embraces
backwards compatiblity and explicit version moves.  This greatly reduces
the likelihood of AI generating outdated code — starkly contrasting
JavaScript&#39;s fast-moving ecosystem for instance.</li>
</ul>
<p>For comparison, Python — my initial choice — often poses significant
challenges.  Agents struggle with Python&#39;s magic (eg: Pytest’s fixture
injection) or complex runtime challenges (eg: wrong event loop when
working with async), frequently producing incorrect code that even the
agentic loop has challenges resolving.  Python also has practical
performance problems.  I don&#39;t mean that it writes slow code, i mean that
the agent loop is really slow.  That&#39;s because the agent loves to spawn
processes and test scripts, and it can take quite a while for the
interpreter to boot up and initialize the entire application.</p>
<p>On the frontend I settled on tailwind, react with tanstack&#39;s query and
router as well as vite.  I&#39;m not amazingly happy with it, but I found it
better than the alternatives.  Tailwind and vite are great, no complaints
there.  Tanstack&#39;s file based router does not make me super happy.  In
parts because it likes to have dollar signs in the file names and those
really like to confuse the agent.  For instance it&#39;s quite common that it
tries to edit <cite>$param.tsx</cite> but it edits the file <cite>.tsx</cite> instead because it
to confused by shell interpolation.  It&#39;s a minor thing, but a very
annoying thing.</p>
</div>

<div id="it-s-all-about-speed">
<h2>It&#39;s All About Speed</h2>
<p>Agentic coding&#39;s inefficiency largely arises from inference cost and
suboptimal tool usage.  Let me reiterate: quick, clear tool responses are
vital.  What we did not talk about yet is that some tools are &#34;emergent,&#34;
temporarily written by agents themselves.  Quick compilation and execution
significantly boost productivity of the agent.  So how can we help it?</p>
<p>With the right instructions it must be possible for the AI to create a new
tool, by following existing conventions very quickly.  This is necessary
because you want the AI to write some new code, and run it.  There is a
big difference in the quality and speed of the flow, if that tool takes
3ms to run vs it compiles for 5 seconds and then needs another minute to
boot and connect to database and kafka broker and 100 lines of nonsensical
log output.</p>
<p>If your stuff is indeed slow, then consider vibe-coding a demon that you
can dynamically load stuff into.  As an example Sentry takes too long to
reload code and it takes too long to restart.  To trial some agentic
coding there my workaround was a module that watches a file system
location and just imports and executes all python modules placed there,
then writes the outputs into a log it can cat.  That&#39;s not perfect, but it
was a significant help for the agent to evaluate some basic code in the
context of the application.</p>
<p>Balancing log verbosity is crucial: informative yet concise logs optimize
token usage and inference speed, avoiding unnecessary costs and rate
limits.  If you cannot find the balance, provide some easy to turn knobs
for the AI to control.</p>
<p>In an idea setup you get useful log output as a natural byproduct of the
agent writing code.  Getting observability from the first shot of code
generation beats writing code, failing to run it and only then going back
to a debug loop where debug information is added.</p>
</div>
<div id="stability-and-copy-paste">
<h2>Stability and Copy/Paste</h2>
<p>Stable ecosystems are what you really want.  LLMs are great with Go and
they love to use Flask, because those are quite stable ecosystems with
little churn.  The same thing is true for your codebase.  The AI likes to
leave all kinds of breadcrumbs lying around when writing code that can
turn into confusion later.  For instance I have seen the agents leave
useful comments about why it chose one path over another.  If you
nilly-willy let the AI upgrade libraries where some of those decisions no
longer make sense, you now might have the AI continue making a now
outdated pattern.</p>
<p>In theory this should be the same for agents and humans, but the reality
is that agents make upgrades so “cheap” that it&#39;s tempting to just let the
AI do it and see if tests still pass.  I do not find this to be a
successful path at all.  Be even more conservative about upgrades than
before.</p>
<p>Likewise with AI I strongly prefer more code generation over using more
dependencies.  I wrote about why you <a href="https://lucumr.pocoo.org/2025/1/24/build-it-yourself/">should write your own code</a> before, but the more I work with
agentic coding, the more I am convinced of this.</p>
</div>
<div id="write-simple-code">
<h2>Write Simple Code</h2>
<p>Simple code significantly outperforms complex code in agentic contexts.  I
just recently wrote about <a href="https://lucumr.pocoo.org/2025/2/20/ugly-code/">ugly code</a> and I think in the
context of agents this is worth re-reading.  Have the agent do “the
dumbest possible thing that will work”.</p>
<ul>
<li><strong>Prefer functions</strong> with clear, descriptive and longer than usual
function names over classes.</li>
<li>Avoid inheritance and overly clever hacks.</li>
<li><strong>Use plain SQL</strong>.  I mean it.  You get excellent SQL out of agents and
they can match the SQL they write with the SQL logs.  That beats them
min-maxing your ORM&#39;s capabilities and getting lost in the SQL output in
a log.</li>
<li><strong>Keep important checks local.</strong>  You really want to make sure that
permission checks are very clear to the AI, and that they are taking
place where it AI can see it.  Hiding permission checks in another file
or some config file will amost guarantee you that the AI will forget to
add permission checks in when adding new routes.</li>
</ul>
</div>
<div id="make-it-parallelizable">
<h2>Make It Parallelizable</h2>
<p>Agents aren&#39;t exceptionally fast individually, but parallelization boosts
overall efficiency.  Find a way to manage shared states like the file
system, databases, or Redis instances so that you can run more than one.
Avoid them, or find a way to quickly segment stuff out.</p>
<p>Your initial shared state is just the file system and a second check-out
will do.  But really I don&#39;t have an amazing solution yet.  There are some
good initial attempts.  For instance one of the tools to watch is
<a href="https://github.com/dagger/container-use">container-use</a>.  It&#39;s an MCP
server that instructs Claude or other agents to run their experiments
entirely in Docker.</p>
<p>Then there are tools like Cursor&#39;s background agents and Codex which are
moving this entire stuff into CI which will be interesting.  So far, I
don&#39;t this is working for me yet, but let&#39;s see again in a month.</p>
</div>
<div id="learn-to-refactor">
<h2>Learn To Refactor</h2>
<p>Agentic coding alters refactoring priorities.  Agents handle tasks
effectively until project complexity surpasses some manageable thresholds.
Too big here is defined by the total amount of stuff that it has to
consider.  So for instance you can vibe code your frontend together for a
while, but eventually you reach the point where you absolutely need to
tell it to make a component library.  Why?  Because if the total tailwind
class mess is splitered across 50 files you will find it very hard to get
the AI to make redesigns or extract components without major regressions.</p>
<p>An agentic workflow encourages good code maintenance and refactoring at
the right moment.  You don&#39;t want to do it too early and you definitely do
not want to do it too late.</p>
</div>
<div id="what-next">
<h2>What Next?</h2>
<p>Agentic coding is rapidly evolving, and my workflow today may look
dramatically different tomorrow.  What&#39;s clear though is that integrating
agents into your development process can unlock significant productivity
gains.  I encourage you to keep experimenting.  The tools and techniques
will evolve, but the core principles — simplicity, stability,
observability and smart parallelization — will remain essential.</p>
<p>Ultimately, the goal is not just to leverage agents to write code faster,
but to write better, more maintainable, and resilient code.  Already today
the code looks nothing like the terrible slop from a few months ago.  Stay
adaptable, and happy coding!</p>

</div>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/ai/">ai</a> and 
      <a href="https://lucumr.pocoo.org/tags/thoughts/">thoughts</a>
  

      </p></div></div>
  </body>
</html>
