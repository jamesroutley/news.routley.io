<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.bloomberg.com/news/features/2023-03-27/swedish-housing-market-crash-exposes-economic-divisions">Original</a>
    <h1>What broke Sweden? Real estate bust exposes big divide</h1>
    
    <div id="readability-page-1" class="page"><article>
  <ul>
<li>tigerbeetle
<ul>
<li>perf handover</li>
<li>compaction unchained</li>
<li>crash harder</li>
<li>sketching the query engine</li>
</ul>
</li>
<li>focus catchup</li>
<li>android update policies</li>
<li>legopunk</li>
<li>reading
<ul>
<li>a world without email</li>
<li>nobody cares about our concurrency control research</li>
<li>segcache</li>
<li>bloomRF</li>
<li>existential consistency</li>
<li>parameter-aware io management for ssds</li>
<li>fantastic ssd internals</li>
</ul>
</li>
</ul>
<p>The talks from systems distributed will be aired live over the next month or so. You can see the schedule <a href="https://systemsdistributed.com/#agenda">here</a>.</p>
<h2 id="perf-handover">perf handover</h2>
<p>Pretty soon I&#39;m going to have to switch from working on performance to designing the query engine. I figured the highest leverage thing I could do with my last few weeks is to make it easier for whoever is working on performance next to focus on the most important problems.</p>
<p>I started by massively expanding tracing in <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/552">#552</a> and <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/562">#562</a>. We now track queue depth and read/write latency throughout each part of the io pipeline, hit/miss counts for all caches and filters, and spans for every async task.</p>
<p>Having all of these on the same timeline, with easy searching, filtering and aggregation, makes it so much easier to understand the behaviour of the system by correlating different events. I spent a couple of days just browsing around in tracy and spotted a bunch of promising problems.</p>
<ul>
<li>After every checkpoint <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/558">there is a gap where the replica doesn&#39;t do any work</a>, probably caused by clients backing off too aggressively while the server is busy.</li>
<li>As the database grows, <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/560">checking that transfer ids are unique becomes the main bottleneck</a>. Almost all of these checks end at the bloom filter, but in larger databases the bloom filter blocks themselves don&#39;t all fit in the disk cache, so we end up doing a lot of sequential io to retrieve them. This is compounded by compaction trashing the cache but is still present to some degree even long after compaction, and even in cases where I&#39;d expect all the filter blocks to be resident in cache. So there may also be something funky with the eviction algorithm.</li>
<li>Relatedly, <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/551">compaction reads are mostly cache misses</a>. This one is potentially an opportunity. We have to copy blocks during compaction because they might be evicted from the cache while we&#39;re still compacting. If we mostly miss the cache anyway, we might as well read directly into the compaction block and both avoid the copy and avoid evicting index/filter blocks from the cache. I experimented with this in <a href="https://github.com/tigerbeetledb/tigerbeetle/commit/a8ab2c2b9268c6de9ba07c4de3b7fee4ba63f0e1">jamii-bypass-cache</a> and got a ~10% throughput boost, but I don&#39;t want to merge it in isolation - we need to sit down at some point and do some holistic tuning of our caching strategy after upcomping changes to compaction scheduling.</li>
<li>At the end of each compaction run we sort and flush our in-memory buffers. Each sort can take up to 250ms without yielding, during which <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/550">our io pipeline starves</a>. We know that the query engine is going to require in-memory buffers to be sorted more often, so this is only going to get worse. King is already making some headway on this in <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/594">#594</a> and <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/606">#606</a>.</li>
</ul>
<p>Between tracy and perf I assembled fairly convincing proof that <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/324">we&#39;re not remotely io-bound</a>. I don&#39;t have a solid enough understanding of cpu architecture to distinguish between being cpu-bound and memory-bound, but whether I sample by cpu cycles or by cache misses in perf we find the same bottlenecks - blake3 (hash-chaining to detect disk errors), wyhash (hashtables and bloom filters), sorting (converting in-memory buffers to on-disk data blocks). (Merging blocks during compaction was also on this list initially, but see below.)</p>
<p>It&#39;s difficult to disable the blake3 hashing because we rely on it for ids, but switching from the zig implementation of blake3 (~600mb/s) to <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/563">the highly-tuned c/asm implementation</a> (~6000mb/s) produced a 17% throughput boost! We probably won&#39;t merge that because it complicates cross-platform builds, but it definitely demonstrates that we have a lot to gain from improving the zig implementation.</p>
<p>I also tried <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/564">increasing the default disk cache size</a> to the size we expect to use in production, so that our other performance work isn&#39;t confounded by poor cache performance, but this is currently in limbo pending some decision about how the non-default cache size will be configured.</p>
<p>Lastly, I noticed in benchmarks with low load that the batch latency <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/588">never falls below 5ms</a>. Turns out that during my earlier changes to the benchmark I also introduced a minimum 5ms sleep between batches. Embarrassing. Fixing this required <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/589/files">converting the entire benchmark to continuation-passing style</a>, inverting most of the logic and adding ~100 lines of boilerplate. A really clear demonstration of why async/await is such a popular feature for programming languages.</p>
<h2 id="compaction-unchained">compaction unchained</h2>
<p>There are two major tasks in tigerbeetle. In the foreground, incoming operations are executed against a snapshot of the database and mutations are accumulated in new in-memory buffers. In the background, we flush the old in-memory buffers to disk and compact the lsm trees on disk. Every <code>constants.lsm_batch_multiple</code> operations these two tasks synchronize - we pick a new snapshot and swap the old and new buffers.</p>
<p>But both of these tasks are sharing the same thread. To prevent compaction from causing latency spikes, compaction work was divided into fixed-size <code>ticks</code> which were supposed to be divided evenly in the gaps between operations. But in practice, it&#39;s actually pretty hard to estimate how many ticks of work we need to do. It depends on how much compaction work is needed for each tree, how much overlap there is between tables on different levels, how many tombstones will be dropped during compaction etc. Also the tick interface was readiness-based - when you call tick it checks to see if enough io has completed that it can do any work. So the number of ticks required per operation wasn&#39;t even deterministic.</p>
<p>The overall effect of this was that all the compaction work tended to just pile up after the last operation, just before the two tasks synchronize.</p>
<p>My first step in fixing this was to <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/587">switch from a readiness-based interface to a completion-based interface</a>. Rather than repeatedly calling tick to move compaction work forwards, we just let them run at their own pace. I had planned to add some prioritization to the io system to ensure that operations didn&#39;t get stuck behind compaction, but our queues tend to be pretty shallow and quickly cleared so this ended up being unnecessary at the moment.</p>
<p>As a bonus, I was able to convert all the iterators and the merging to operate over large batches of values instead of individual values, resulting in some really tight loops. Merging completely vanished from the perf profile.</p>
<p>I was frustrated again though by the lack of async/await. Take <a href="https://github.com/tigerbeetledb/tigerbeetle/blob/6e971997b122050124a401135d95d24b34a51795/src/lsm/table_data_iterator.zig">table_data_iterator</a> for example. 120 lines of code to do this:</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(addresses</span><span>,</span><span> checksums) </span><span>|</span><span>address</span><span>,</span><span> checksum</span><span>| </span><span>{
</span><span>  </span><span>const</span><span> block </span><span>= </span><span>await</span><span> grid.</span><span>read</span><span>(address</span><span>,</span><span> checksum</span><span>,</span><span> .data</span><span>)</span><span>;
</span><span>  yield block</span><span>;
</span><span>}
</span></code></pre>
<p>A lot more work and a lot harder to read. The latter really bites - I spent two days debugging a crash that turned out to be caused by <a href="https://github.com/tigerbeetledb/tigerbeetle/commit/5aab967e22b5c18fb7698f6cf43d9caf1f22fdcc">misplacing a single line</a> while converting a complicated state machine from readiness to completion. Without all the cps noise the code might have fit on a single screen and I would be far less likely to misunderstand the control flow.</p>
<h2 id="crash-harder">crash harder</h2>
<p>While trying to debug the above I realized that the forest fuzzer only ever tests crashing and restarting the database (eg as if the power failed) when the io queue is empty, which is one of the less scary scenarios. I tweaked the fuzzer in <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/586/files">#586</a> to be able to crash in the middle of an operation. This didn&#39;t catch any new bugs - the vopr already had similar power - but it will give me more confidence as I continue to rework compaction scheduling.</p>
<h2 id="sketching-the-query-engine">sketching the query engine</h2>
<p>I started <a href="https://github.com/tigerbeetledb/tigerbeetle/blob/jamii-query-design/docs/design/queries.md">sketching out the design of the query engine</a>. </p>
<p>The constraints are severe - for every query we need to guarantee:</p>
<ul>
<li>Deterministic execution</li>
<li>Bounded memory usage</li>
<li>Bounded execution time</li>
<li>Bounded number of results</li>
</ul>
<p>Threading that needle is going to be an interesting challenge.</p>
<h2 id="focus-catchup">focus catchup</h2>
<p>I&#39;ve been delaying doing any more work on <a href="https://github.com/jamii/focus">focus</a> until I upgrade zig, and I&#39;ve been delaying doing that until I had time to figure out why it causes <a href="https://user-images.githubusercontent.com/109527915/227769931-70375b96-2725-4da2-9b8b-dbaaf076ee49.png">a weird rendering bug</a>.</p>
<p>But jakubvf kindly sent me <a href="https://github.com/jamii/focus/pull/6">a pr</a> for all the upgrades, so I finally got around to looking at the bug and it turned out to be a simple change in aligmnent/padding which was easy to track down and fix.</p>
<p>I still don&#39;t expect any big features in the near future, but with that blocker out the way I&#39;m back to my old steady stream of <a href="https://github.com/jamii/focus/commits/master">tiny tweaks and fixes</a>.</p>
<h2 id="android-update-policies">android update policies</h2>
<p>My phone stopped getting security updates two years, and stopped getting even <a href="https://lineageos.org/">lineage</a> updates this year (not to mention doing major version updates on lineage is painful). So I begrudgingly have to throw away a perfectly good piece of hardware.</p>
<p>I found AndroidAuthority has a <a href="https://www.androidauthority.com/phone-update-policies-1658633/">complete list</a> of the update policies for each major manufacturer. I was able to figure out that the best security-updates-for-money seems to be the pixel 6a - it will get updates <a href="https://support.google.com/pixelphone/answer/4457705?hl=en#zippy=%2Cpixel-phones">until at least July 2027</a> and open-box models can be found for 300 CAD.</p>
<h2 id="legopunk">legopunk</h2>
<p>In other news, framework announced a ton of new hardware, including a <a href="https://frame.work/ca/en/blog/introducing-the-framework-laptop-16">16 inch laptop with upgradeable gpu and hot-pluggable input devices</a>.</p>
<p>I was initially jealous that the new 13 inch laptop comes with a matte screen, but then I realized that this is framework we&#39;re talking about - I can just <a href="https://frame.work/ca/en/products/display-kit?v=FRANFX0001">upgrade to the new screen</a>!</p>
<p>What a contrast to my phone.</p>
<h2 id="a-world-without-email"><a href="https://www.goodreads.com/en/book/show/54326146">a world without email</a></h2>
<p>I&#39;m maybe the wrong audience for this. Cal spends most of the book on arguments that unstructured email/chat make for poor prioritization and fragmented concentration, which I&#39;m already sold on. What time he spends on actual alternatives is mostly devoted to talking about existing practices in software engineering (project boards, issue trackers, customer-support ticket systems etc).</p>
<p>Also misses what I think is one of the stickier contributors to use of chat - workplaces (especially remote) have stripped most social interaction out of the day. Chat is a meagre substitute, but it&#39;s the main option. I don&#39;t think it&#39;s a coincidence that methodologies like xp that focus the most on structured workflows also lean heavily on pair programming.</p>
<p>This was at least a nudge to update my leechblock filters to block slack and gmail for most of the day, as well as the notifications page on github (I still want to be able to review code that&#39;s in my queue, I just don&#39;t want to be distracted by seeing more stuff arriving in the queue when I haven&#39;t emptied it yet).</p>
<p>Idly fantasizing about an <a href="https://github.com/janestreet/iron">iron</a>-like tool for task management, that works offline and only syncs overnight.</p>
<h2 id="nobody-cares-about-our-concurrency-control-research"><a href="https://www.cs.cmu.edu/%7Epavlo/slides/pavlo-keynote-sigmod2017.pdf">nobody cares about our concurrency control research</a></h2>
<p>Academic work on concurrency control focuses on serializable execution of stored procedures. Andy Pavlo surveys DBAs and finds that most rarely use stored procedure or serializable transactions (the most common level seems to be read-committed).</p>
<h2 id="segcache"><a href="https://www.usenix.org/system/files/nsdi21-yang.pdf">segcache</a></h2>
<p>(Plus blog posts and videos at <a href="https://pelikan.io/">pelikan.io</a>)</p>
<p>Fun to compare to <a href="https://www.scattered-thoughts.net/log/0032#kangaroo-theory-and-practice-of-caching-billions-of-tiny-objects-on-flash">kangaroo</a>. Despite seemingly similar problems, different priorities lead to wildly different designs.</p>
<p>Segcache focuses on timely expiration and memory efficiency. Achieving both is hard - tracking which keys are about to expire usually requires some data-structure with entries per-key, which means high memory overhead when keys themselves are small. Memcached needs 56 bytes of metadata per key and doesn&#39;t guarantee timely eviction. Segcache needs 5 bytes (amortized) of metadata per key, <em>always</em> expires keys before their ttl and achieves a much lower miss ratio in production.</p>
<p>Segcache groups divides all possible ttls into &#39;buckets&#39;. Within each bucket keys are stored in a linked list of fixed-size append-only &#39;segments&#39;, sorted by creation time. This completely prevents internal fragmentation. To expire keys they only need to check the front of the linked list for each bucket.</p>
<p>When out of space, segcache picks a ttl bucket, takes the first n segments and merges them into 1 segment in a single pass, stochastically dropping the keys with lowest read frequency.</p>
<p>Keys are indexed in a bulk-chained hashtable. Locking is per-chain (&lt;= 6 keys). For each key the segment id+offset, frequency counter and the first 12 bits of the hash are packed into a 64 bit entry.</p>
<p>The frequency counter is a novel approximation which is accurate for counts &lt; 16 and increasingly less accurate for larger counts. Keys with large counts are always going to be retained anyway, so accuracy is most important when comparing keys which might be evicted. To avoid cache pollution by request spikes the counter is reset to 0 when segments are merged (to approximate a windowed count). To reduce locking costs, the counter is rate-limited by a per-chain last-access-timestamp.</p>
<h2 id="bloomrf"><a href="https://arxiv.org/pdf/2207.04789.pdf">bloomRF</a></h2>
<p>Proposes a bloom filter variant that performs well for range queries across a larger range of query parameters than previous designs, and that can be built online.</p>
<p>A regular bloom filter take k hashes of the key and sets <code>for (0..k) |i| filter[hash(i, key)] = true</code>. </p>
<p>bloomRF sets <code>hash(i, key)</code> to hash only a prefix of the key. So we might choose eg <code>hash(i, key) = hash(first_n_bits(i * 8, key))</code>. That means that we can look up a dyadic range like <code>[0xCC0,0xCCF]</code> by checking if <code>filter[hash(0xCC)]</code>. Arbitrary ranges can be built out of O(k) dyadic ranges.</p>
<p>That&#39;s still O(k) hash misses though, so to tame the constant factors we can tweak the hash function. When hashing the first eg 16 bits, we&#39;ll actually only hash the first 10 bits and then concatenate the last 6 bits. This means that sequences of 64 contiguous ranges will end up with hashes all on the same 64 bit word which we can check with a single instruction. So the actual number of cache accesses ends up being something like k+1.</p>
<p>So far pretty simple. But getting good performance out of this requires some amount of tweaking of number of hashes, size of prefixes and size of contiguous ranges that I didn&#39;t really follow on the first pass. </p>
<p>The performance evaluation in rocksdb seems promising, with better latency and false-positive ration for ranges up to 2^36. I&#39;d like to have seen what happens to the graph beyond that range though, since performance looks like it might just fall off a cliff.</p>
<h2 id="existential-consistency"><a href="https://scontent.fyvr3-1.fna.fbcdn.net/v/t39.8562-6/240845515_554551645666675_675892491003214182_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=1jI_KsBCOlIAX_xyHw4&amp;_nc_ht=scontent.fyvr3-1.fna&amp;oh=00_AfAjVcpgIIUE9uMxO0yYq4QtaYe7zndLPN0CtL4IOHl3sw&amp;oe=642BBA41">existential consistency</a></h2>
<p>Facebook uses a graph database which provides sequential and read-after-write consistency within a single cache and eventual consistency across caches. How much could they reduce errors by making that database linearizable? (They don&#39;t consider any stronger consistency model because they can&#39;t measure non-local consistency experimentally).</p>
<p>The first experiment samples all reads and writes for a subset of keys. They report a very low percentage of read anomalies.</p>
<p>But they note that the ratio of writes to reads is 450:1 and that changing their error margin for clock skew can increase the number of anomalies by 10x (switching from false negatives to false positives, so the real number is probably bounded somewhere between). If we combine those, we see that up to 0.00039% * 10 * 450 = 1.755% of writes cause violations of linearizable consistency. They note that writes aren&#39;t evenly distributed too, so it kinda sounds like the actual result here is that conversations on hot posts/photos see many read anomalies but read-only traffic on older posts sees none. </p>
<p>They also note that systems which are sensitive to read anomalies either set flags requiring stronger consistency or use other databases, so what we&#39;re measuring here is only those systems which either tolerate anomalies well or just happen to have workloads which don&#39;t cause many anomalies. So it&#39;s hard to generalize from these results.</p>
<p>The second experiment actively probes agreement between different caches. Disagreement can be caused by latency alone, so the actual percentage of disagreement is not very meaningful. But sudden spikes in disagreement are very strong alarm signals and usually indicate some operational failure.</p>
<p>Both experiments are very cool, even if I&#39;m somewhat skeptical of the interpretation of the first. It&#39;s very common to see people handwave about eventual consistency, and very rare to see anyone actually try to measure the impact, let alone systematically monitor it over time.</p>
<h2 id="parameter-aware-io-management-for-ssds"><a href="http://csl.skku.edu/papers/CS-TR-2010-329.pdf">parameter-aware io management for ssds</a></h2>
<p>SSDs pretend to be simple block devices but are much more complicated internally. Need to understand their layout to get the best performance, but manufacturers are secretive.</p>
<p>Shows how to experimentally discover internal parameters:</p>
<ul>
<li>Page size - write latency is lower when write size is an aligned multiple of page size, because doesn&#39;t require first reading from pages at the edge of the write.</li>
<li>Block size - when write size is lower than the block size, random writes get less throughput than sequential writes because they hit more blocks, requiring more gc work.</li>
<li>Read buffer - after polluting the cache with unrelated reads, issuing two reads for the same address has lower latency when the reads fit in the read buffer.</li>
<li>Write buffer - after flushing the cache, write latency is lower for writes that fit in the write buffer.</li>
</ul>
<p>Using these discovered parameters to tune the linux block layer and io scheduler leads to dramatic performance improvements on standard io benchmarks.</p>
<p>This paper is from 2010 and uses SATA though, so specific results may not still hold.</p>
<h2 id="fantastic-ssd-internals"><a href="https://huaicheng.github.io/p/systor22-queenie.pdf">fantastic ssd internals</a></h2>
<p>A more recent paper in the same vein as the above. They publish a tool that automatically infers parameters. </p>
<p>Highlighted findings:</p>
<ul>
<li>Write buffers are as small as 10mb, causing frequent latency spikes. </li>
<li>Flushes spike read latencies, not just write latencies. They demonstrate that a flush-aware io scheduler can reduce peak latencies.</li>
<li>Flushes are sometimes triggered by number of writes rather than by total write size.</li>
<li>When the write queue is empty and multiple writes arrive, some drives will only issue the first write and block until it returns.</li>
<li>Only 1 drive used dram read cache.</li>
<li>Almost all non-SLC drives use 4kb pages. SLC drives use 8kb or 16kb pages.</li>
<li>Larger ssds have stripe widths of 128..256. (I think this means that we should expect that larger drives need higher queue depth to saturate read bandwidth.)</li>
<li>Write parallelism is much lower, only 1..8. But writes are buffered by the write cache and can be pipelined with flushes, so not clear how this affects optimal queue depth.</li>
</ul>
<p>The authors were not able to validate any of their conclusions because manufacturers point-blank refuse to provide any information at all.</p>

</article></div>
  </body>
</html>
