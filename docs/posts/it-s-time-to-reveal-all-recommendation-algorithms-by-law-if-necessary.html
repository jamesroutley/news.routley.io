<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theregister.com/2023/04/13/reveal_all_recommendation_algorithms/">Original</a>
    <h1>It&#39;s time to reveal all recommendation algorithms – by law if necessary</h1>
    
    <div id="readability-page-1" class="page"><div id="body">
<p><span>Column</span> As it’s been about forty years since I’ve had pimples, it astounds me that YouTube’s recommendation engine recently served me videos of people with some really severe skin problems - generally on their noses. The preview images themselves are horrifying, and should really come with some sort of content warning. I immediately tell YouTube “I don’t want this” and “never recommend this channel.”</p>
<p>Yet it persists.</p>
<p>How? Why?</p>

    

<p>I’ve never watched a dermatology video in all of my years using YouTube, nor do I or anyone I know have an adjacent skin condition that might at some point suggest to the algorithm that I needed to be exposed to these truly can-not-be-unseen video images.</p>

        


        

<p>The essential nature of a recommendation algorithm is that it’s doing its best to anticipate your desires from whatever bits of data it can gather about you.</p>
<p>I defend myself from arbitrary data collection that fuels the algorithms using PiHole, the tracker-blocking Disconnect plugin, and Firefox, plus a few other tricks. In theory recommendation algorithms therefore have less to work with than if I simply journaled my every activity via some sort of oh-so-friendly-and-rapacious Android smartphone.</p>

        

<p>I make an active effort to resist data collection - and perhaps these horrors are one consequence.</p>
<p>At best that’s a guess, because I have no way to know what goes on at the heart of YouTube’s recommendation algorithm. If ever exposed, that closely guarded algorithm could be gamed - in a manner similar to the way so many marketing bottom feeders continually test and game search engine results.</p>
<p>There’s a profound commercial disincentive for YouTube to become transparent about how its algorithm works.</p>

        

<p>That leaves me and other privacy conscious folk with just one lever to pull offer - disliking a video and a channel and hoping that - somehow - the algorithm might be able to intuit a generalized case from a specific instance.</p>
<p>In the one example where we’ve been exposed to the inner workings of a recommendation engine - Twitter <a target="_blank" href="https://www.theregister.com/2023/04/07/twitter_code_cve_substack/">went public</a> with theirs at the end of March - we got an eyeful of a different kind of horror: an algorithm that promoted owner Elon Musk’s tweets above all others, promoted specific political interests - and that specifically <a target="_blank" href="https://www.axios.com/2023/04/03/elon-musks-twitter-free-speech-checkmarks-ranking">will not promote</a> tweets directly pointing to LGBTIQ+ words, concerns, or themes.</p>
<p>While we can all have a nice laugh at Elon’s profoundly public narcissism, the silencing of an entire community at a moment in history when forces around the world seek to roll back recent gains in civil rights for LGBTIQ+ individuals is not funny. Where it becomes harder to share one’s own story, that story can be framed as marginal, unimportant - even dangerous. It’s a profound ‘othering’ that can, thanks to an algorithm, be greatly amplified.</p>
<ul>

<li><a href="https://www.theregister.com/2023/03/08/generative_ai_viral_video/">Once AI can create endless viral videos, good luck switching off social media</a></li>

<li><a href="https://www.theregister.com/2023/02/08/ai_battle_microsoft_google/">Conversational AI tells us what we want to hear – a fib that the Web is reliable and friendly</a></li>

<li><a href="https://www.theregister.com/2023/01/18/chatgpt_vs_paperless_office/">AI may finally cure us of our data fetish</a></li>

<li><a href="https://www.theregister.com/2022/12/14/voice_assistants_failed/">Voice assistants failed because they serve their makers more than they help users</a></li>
</ul>
<p>The solution to both issues is obvious, technically easy, and yet commercially a nearly impossible proposition: open up all recommendation algorithms.</p>
<p>Make them completely transparent, and, for the individual being targeted by the recommendation engine, completely programmable.</p>
<p>I should not only be able to interrogate how I got a horrifying video of a very bad case of pimples, I should be able to get in there and tune things so that the algorithm no longer needs to guess my needs, because I have had the opportunity to make those needs clear.</p>
<p>Every algorithm that recommends things to us - music or movies or podcasts or stories or news reports - should be completely visible. There must be nothing secret behind the scenes, because we know now from countless examples - the biggest and ugliest being <a target="_blank" href="https://www.theregister.com/2018/03/28/cambridge_analytica_not_legitimate_business_says_whistleblower/">Cambridge Analytica</a> - how recommendations can be used to drive us to extremes of belief, emotion – even action.</p>
<p>That’s too much power to leave with an algorithm, and too much control to cede to those who tend those algorithms.</p>
<p>If recommendation algos aren’t shared then we need - by legislation, if necessary - a switch that turns the recommendation engine off.</p>
<p>That might leave us floating in a vast and unknowable sea of content, but it’s better to know you’re nowhere than to be led down a garden path. ®</p>                                
                    </div></div>
  </body>
</html>
