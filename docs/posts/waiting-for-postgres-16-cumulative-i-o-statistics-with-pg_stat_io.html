<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pganalyze.com/blog/pg-stat-io">Original</a>
    <h1>Waiting for Postgres 16: Cumulative I/O statistics with pg_stat_io</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>One of the most common questions I get from people running Postgres databases at scale is:</p>
<p>Historically, getting a complete picture of all the I/O produced by a Postgres server has been challenging. To start with, Postgres splits its I/O activity into writing the WAL stream, and reads/writes to the data directory. <strong>The real challenge is understanding second-order effects around writes</strong>: Typically the write to the data directory happens after the transaction commits, and understanding which process actually writes to the data directory (and when) is hard.</p>
<p>This whole situation has become an even bigger challenge in the cloud, when faced with provisioned IOPS, or worse, having to pay for individual I/Os like on Amazon Aurora. Often the solution has been to look at parts of the system that have instrumentation (such as individual queries), to get at least some sense for where the activity is happening.</p>
<p>Last weekend, a <strong>major improvement to the visibility into I/O activity</strong> <a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=a9c70b46dbe152e094f137f7e6ba9cd3a638ee25">was committed</a> to the upcoming Postgres 16 by Andres Freund, and authored by Melanie Plageman, with documentation contributed by Samay Sharma. My colleague Maciek Sakrejda and I have reviewed this patch through its various iterations, and we&#39;re very excited about what it brings to Postgres observability.</p>
<p>Welcome, <strong>pg_stat_io</strong>. Let&#39;s take a look:</p>

<h2 id="querying-system-wide-io-statistics-in-postgres"><a href="#querying-system-wide-io-statistics-in-postgres" aria-label="querying system wide io statistics in postgres permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Querying system-wide I/O statistics in Postgres</h2>
<p>Let&#39;s start by using a local Postgres built fresh from the development branch. Note that Postgres 16 is still under heavy development, not even at beta stage, and should definitely not be used on production. For this I followed the <a href="https://wiki.postgresql.org/wiki/Meson">new cheatsheet for using the Meson build system</a> (also new in Postgres 16), which significantly speeds up the build and test process.</p>
<p>We can start by querying <code>pg_stat_io</code> to get a sense for which information is tracked, omitting rows that are empty:</p>
<div data-language="sql"><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> pg_stat_io <span>WHERE</span> <span>reads</span> <span>&lt;&gt;</span> <span>0</span> <span>OR</span> writes <span>&lt;&gt;</span> <span>0</span> <span>OR</span> extends <span>&lt;&gt;</span> <span>0</span><span>;</span></code></pre></div>
<div data-language="text"><pre><code>    backend_type     | io_object | io_context |  reads   | writes  | extends | op_bytes | evictions |  reuses  | fsyncs |          stats_reset          
---------------------+-----------+------------+----------+---------+---------+----------+-----------+----------+--------+-------------------------------
 autovacuum launcher | relation  | normal     |       19 |       5 |         |     8192 |        13 |          |      0 | 2023-02-13 11:50:27.583875-08
 autovacuum worker   | relation  | normal     |    15972 |    2494 |    2894 |     8192 |     17430 |          |      0 | 2023-02-13 11:50:27.583875-08
 autovacuum worker   | relation  | vacuum     |  5754853 | 3006563 |       0 |     8192 |      2056 |  5752594 |        | 2023-02-13 11:50:27.583875-08
 client backend      | relation  | bulkread   | 25832582 |  626900 |         |     8192 |    753962 | 25074439 |        | 2023-02-13 11:50:27.583875-08
 client backend      | relation  | bulkwrite  |     4654 | 2858085 | 3259572 |     8192 |    998220 |  2209070 |        | 2023-02-13 11:50:27.583875-08
 client backend      | relation  | normal     |   960291 |  376524 |  159497 |     8192 |   1103707 |          |      0 | 2023-02-13 11:50:27.583875-08
 client backend      | relation  | vacuum     |   128710 |       0 |       0 |     8192 |      1221 |   127489 |        | 2023-02-13 11:50:27.583875-08
 background worker   | relation  | bulkread   | 39059938 |  590896 |         |     8192 |    802939 | 38253662 |        | 2023-02-13 11:50:27.583875-08
 background worker   | relation  | normal     |   257533 |  118972 |       0 |     8192 |    256437 |          |      0 | 2023-02-13 11:50:27.583875-08
 background writer   | relation  | normal     |          |  243142 |         |     8192 |           |          |      0 | 2023-02-13 11:50:27.583875-08
 checkpointer        | relation  | normal     |          |  390141 |         |     8192 |           |          |  18812 | 2023-02-13 11:50:27.583875-08
 standalone backend  | relation  | bulkwrite  |        0 |       0 |       8 |     8192 |         0 |        0 |        | 2023-02-13 11:50:27.583875-08
 standalone backend  | relation  | normal     |      689 |     983 |     470 |     8192 |         0 |          |      0 | 2023-02-13 11:50:27.583875-08
 standalone backend  | relation  | vacuum     |       10 |       0 |       0 |     8192 |         0 |        0 |        | 2023-02-13 11:50:27.583875-08
(14 rows)</code></pre></div>
<p>At a high level, this information can be interpreted as:</p>
<ul>
<li>Statistics are tracked for a given backend type, I/O object type (i.e. whether it&#39;s a temporary table), and I/O context (more on that later)</li>
<li>The main statistics are counting I/O operations: <strong>reads</strong>, <strong>writes</strong> and <strong>extends</strong> (a special kind of write to resize data files)</li>
<li>For each I/O operation the size in bytes is noted to help interpret the statistics (currently always block size, i.e., usually 8kB)</li>
<li>Additionally, the number of shared buffer evictions, ring buffer re-uses and fsync calls are tracked</li>
</ul>
<p><strong>On Postgres 16, this system-wide information will always available.</strong> You can find the complete details of each field in the <a href="https://www.postgresql.org/docs/devel/monitoring-stats.html#MONITORING-PG-STAT-IO-VIEW">Postgres documentation</a>.</p>
<p>Note that <code>pg_stat_io</code> shows logical I/O operations issued by Postgres. Whilst this often eventually maps to an actual I/O to a disk (especially in the case of writes), the operating system has its own caching and batching mechanism, and will for example often times split up an 8kB write to become two individual 4kB writes to the file system.</p>
<p>Generally we can assume that this captures all I/O issued by Postgres, except for:</p>
<ul>
<li>I/O for writing the Write-Ahead-Log (WAL)</li>
<li>Special cases such as tables being moved between tablespaces</li>
<li>Temporary files (such as used for sorts, or extensions like <code>pg_stat_statements</code>)</li>
</ul>
<p>Note that temporary relations are tracked (they are not the same as temporary files): In <code>pg_stat_io</code> these are marked as <code>io_object = &#34;temp relation&#34;</code> - you may otherwise be familiar with them being called &#34;local buffers&#34; in other statistics views.</p>
<p>With the basics in place, we can take a closer look at some use cases and learn why this matters.</p>
<h2 id="use-cases-for-pg_stat_io"><a href="#use-cases-for-pg_stat_io" aria-label="use cases for pg_stat_io permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use cases for pg_stat_io</h2>
<h3 id="tracking-write-io-activity-in-postgres"><a href="#tracking-write-io-activity-in-postgres" aria-label="tracking write io activity in postgres permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tracking Write I/O activity in Postgres</h3>
<figure>
<span>
      <a href="https://payments.posthaven.com/static/62bff7b564c8c03267aab13d95edf800/ca98b/write_lifecycle.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Lifecycle of a write in Postgres" title="Lifecycle of a write in Postgres" src="https://payments.posthaven.com/static/62bff7b564c8c03267aab13d95edf800/1d69c/write_lifecycle.png" srcset="/static/62bff7b564c8c03267aab13d95edf800/4dcb9/write_lifecycle.png 188w, /static/62bff7b564c8c03267aab13d95edf800/5ff7e/write_lifecycle.png 375w, /static/62bff7b564c8c03267aab13d95edf800/1d69c/write_lifecycle.png 750w, /static/62bff7b564c8c03267aab13d95edf800/78797/write_lifecycle.png 1125w, /static/62bff7b564c8c03267aab13d95edf800/aa440/write_lifecycle.png 1500w, /static/62bff7b564c8c03267aab13d95edf800/ca98b/write_lifecycle.png 1968w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy" decoding="async"/>
  </a>
    </span>
<figcaption>Lifecycle of a write in Postgres, and what is currently not visible in most statistics</figcaption>
</figure>
<p>When looking at a write in Postgres, we need to look beyond what a client sees as the query runtime, or something like pg_stat_statements can track. Postgres has a complex set of mechanisms that guarantee durability of writes, whilst allowing
clients to return quickly, trusting that the server has persisted the data in a crash safe manner.</p>
<p>The first thing that Postgres does to persist data, is to <strong>write it to the WAL log.</strong> Once this has succeeded, the client
will receive confirmation that the write has been successful. But what happens afterwards is where the additional
statistics tracking comes in handy.</p>
<p>For example, if you look at a given INSERT statement in pg_stat_statements, the <code>shared_blks_written</code> field is often going to tell you next to nothing, because the actual write to the data directory typically occurs at a later time, in order to batch writes for efficiency and to avoid I/O spikes.</p>
<p>In addition to writing the WAL, <strong>Postgres will also update the shared (or local) buffers for the write.</strong> Such an update
will mark the buffer page in question as &#34;dirty&#34;.</p>
<p>Then, in most cases, another process is responsible for actually
writing the dirty page to the data directory. There are three main process types to consider:</p>
<ol>
<li><strong>The background writer:</strong> Runs continuously in the background to write out (some) dirty pages</li>
<li><strong>The checkpointer:</strong> Runs on a scheduled basis, or based on amount of WAL written, and writes out all dirty pages not yet written</li>
<li><strong>All other process types</strong>, including regular client backends: Write out dirty pages if they need to evict the buffer page in question</li>
</ol>
<p>The main thing to understand is when the third case occurs - because <strong>it can drastically slow down queries</strong>. Even a simple &#34;SELECT&#34; might have to suddenly write to disk, before it has enough space in shared buffers to read in its data.</p>
<p>Historically you were already able to see some of this activity through the <code>pg_stat_bgwriter</code> view, specifically the fields named <code>buffers_</code>. However, this was incomplete, did not consider autovacuum activity explicitly, and did not let you understand the root cause of a write (e.g. a buffer eviction).</p>
<p>With <code>pg_stat_io</code> you can simply look at the <code>writes</code> field, and see both an accurate aggregate number, as well as exactly which process in Postgres actually ended up writing your data to disk.</p>

<p>One of the most important metrics that <code>pg_stat_io</code> helps give clarity on, is the situation where a buffer page in shared buffers is evicted. Since shared buffers is a fixed size pool of pages (each 8kb in size, on most Postgres systems), what is cached inside it matters a great deal - <strong>especially when your working set exceeds shared buffers</strong>.</p>
<p>By default, if you&#39;re on a self-managed Postgres, the <code>shared_buffers</code> setting is set to 128MB - or about 16,000 pages. Let&#39;s imagine you end up having loaded something through a very inefficient index scan, that ended up consuming all 128MB.</p>
<p><strong>What happens when you suddenly read something completely different?</strong> Postgres has to go and remove some of the old data from cache - also known as evicting a buffer page.</p>
<p>This eviction has two main effects:</p>
<ol>
<li>Data that was in Postgres buffer cache before, is no longer in the cache (note it may still be in the OS page cache)</li>
<li>If the page that was evicted was marked as &#34;dirty&#34;, the process evicting it also has to write the old page to disk</li>
</ol>
<p>Both of these aspects matter for sizing shared buffers, and <code>pg_stat_io</code> can clearly show this by tracking <code>evictions</code> for each backend type across the system. Further, if you see a sudden spike in evictions, and then suddenly a lot of <code>reads</code>, it can help you infer that the cached data that was evicted, was actually needed again shortly afterwards. If in doubt, you can use the <code>pg_buffercache</code> extension to look at the current shared buffers contents in detail.</p>
<p><a href="https://payments.posthaven.com/ebooks/optimizing-postgres-query-performance"><span>
      <span></span>
  <img alt="Download Free eBook: How To Get 3x Faster Postgres" title="Download Free eBook: How To Get 3x Faster Postgres" src="https://payments.posthaven.com/static/c15d0b3082bebd2680b86cc948555f76/acb04/ebook_promo_query_performance.jpg" srcset="/static/c15d0b3082bebd2680b86cc948555f76/bc01b/ebook_promo_query_performance.jpg 188w, /static/c15d0b3082bebd2680b86cc948555f76/bf173/ebook_promo_query_performance.jpg 375w, /static/c15d0b3082bebd2680b86cc948555f76/acb04/ebook_promo_query_performance.jpg 750w, /static/c15d0b3082bebd2680b86cc948555f76/ec605/ebook_promo_query_performance.jpg 1125w, /static/c15d0b3082bebd2680b86cc948555f76/c58a3/ebook_promo_query_performance.jpg 1500w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy" decoding="async"/>
    </span></a></p>
<h3 id="tracking-cumulative-io-activity-by-autovacuum-and-manual-vacuums"><a href="#tracking-cumulative-io-activity-by-autovacuum-and-manual-vacuums" aria-label="tracking cumulative io activity by autovacuum and manual vacuums permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tracking cumulative I/O activity by autovacuum and manual VACUUMs</h3>
<p>It&#39;s a fact that every Postgres server needs the occasional VACUUM - whether you schedule it manually, or have autovacuum take care of it for you. It helps clean up dead rows and makes space re-usable, and it freezes pages to prevent transaction ID wraparound.</p>
<p>But there is such a thing as VACUUMing too often. If not tuned correctly, VACUUM and autovacuum can have a dramatic effect on I/O activity. Historically the best bet was to look at the output of <code>log_autovacuum_min_duration</code>, which will give you information like this:</p>
<div data-language="text"><pre><code>  LOG:  automatic vacuum of table &#34;mydb.pg_toast.pg_toast_42593&#34;: index scans: 0
        pages: 0 removed, 13594 remain, 13594 scanned (100.00% of total)
        tuples: 0 removed, 54515 remain, 0 are dead but not yet removable
        removable cutoff: 11915, which was 6 XIDs old when operation ended
        new relfrozenxid: 11915, which is 4139 XIDs ahead of previous value
        frozen: 13594 pages from table (100.00% of total) had 54515 tuples frozen
        index scan not needed: 0 pages from table (0.00% of total) had 0 dead item identifiers removed
        avg read rate: 0.113 MB/s, avg write rate: 0.113 MB/s
        buffer usage: 13614 hits, 13602 misses, 13600 dirtied
        WAL usage: 40786 records, 13600 full page images, 113072608 bytes
        system usage: CPU: user: 0.26 s, system: 0.52 s, elapsed: 939.84 s</code></pre></div>
<p>From the <code>buffer usage</code> you can determine that this single VACUUM had to read 13602 pages, and marked 13600 pages as dirty. But what if we want to get a more complete picture, and across all our VACUUMs?</p>
<p>With <code>pg_stat_io</code>, you can now see a system-wide measurement of the impact of VACUUM, by looking at everything marked as <code>io_context = &#39;vacuum&#39;</code>, or associated to the <code>autovacuum worker</code> backend type:</p>
<div data-language="sql"><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> pg_stat_io <span>WHERE</span> backend_type <span>=</span> <span>&#39;autovacuum worker&#39;</span> <span>OR</span> <span>(</span>io_context <span>=</span> <span>&#39;vacuum&#39;</span> <span>AND</span> <span>(</span><span>reads</span> <span>&lt;&gt;</span> <span>0</span> <span>OR</span> writes <span>&lt;&gt;</span> <span>0</span> <span>OR</span> extends <span>&lt;&gt;</span> <span>0</span><span>)</span><span>)</span><span>;</span></code></pre></div>
<div data-language="text"><pre><code>    backend_type    | io_object | io_context |  reads  | writes  | extends | op_bytes | evictions | reuses  | fsyncs |          stats_reset          
--------------------+-----------+------------+---------+---------+---------+----------+-----------+---------+--------+-------------------------------
 autovacuum worker  | relation  | bulkread   |       0 |       0 |         |     8192 |         0 |       0 |        | 2023-02-13 11:50:27.583875-08
 autovacuum worker  | relation  | normal     |   16306 |    2494 |    2915 |     8192 |     17785 |         |      0 | 2023-02-13 11:50:27.583875-08
 autovacuum worker  | relation  | vacuum     | 5824251 | 3028684 |       0 |     8192 |      2588 | 5821460 |        | 2023-02-13 11:50:27.583875-08
 client backend     | relation  | vacuum     |  128710 |       0 |       0 |     8192 |      1221 |  127489 |        | 2023-02-13 11:50:27.583875-08
 standalone backend | relation  | vacuum     |      10 |       0 |       0 |     8192 |         0 |       0 |        | 2023-02-13 11:50:27.583875-08
(5 rows)</code></pre></div>
<p>In this particular example, in sum, the autovacuum worker has read 44.4 GB of data (5,824,251 buffer pages), and written 23.1GB (3,028,684 buffer pages).</p>
<p><strong>If you track these statistics over time</strong>, it will help you have a crystal-clear picture of whether autovacuum is to blame for an I/O spike during business hours. It will also help you make changes to tune autovacuum with more confidence, e.g. making autovacuum more aggressive to prevent bloat.</p>
<h3 id="visibility-into-bulk-readwrite-strategies-sequential-scans-and-copy"><a href="#visibility-into-bulk-readwrite-strategies-sequential-scans-and-copy" aria-label="visibility into bulk readwrite strategies sequential scans and copy permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Visibility into bulk read/write strategies (sequential scans and COPY)</h3>
<p>Have you ever used COPY in Postgres to load data? Or read data from a table using a sequential scan? You may not know that in most cases, this data does not pass through shared buffers in the regular way. Instead, Postgres uses a special dedicated ring buffer that ensures that most of shared buffers is undisturbed by such large activities.</p>
<p>Before <code>pg_stat_io</code>, it was near impossible to understand this activity in Postgres, as <strong>there was simply no tracking for it</strong>. Now, we can finally see both bulk reads (typically large sequential scans) and bulk writes (typically COPY in), and the I/O activity they cause.</p>
<p>You can simply filter for the new <code>bulkwrite</code> and <code>bulkread</code> values in <code>io_context</code>, and have visibility into this activity:</p>
<div data-language="sql"><pre><code><span>SELECT</span> <span>*</span> <span>FROM</span> pg_stat_io <span>WHERE</span> io_context <span>IN</span> <span>(</span><span>&#39;bulkread&#39;</span><span>,</span> <span>&#39;bulkwrite&#39;</span><span>)</span> <span>AND</span> <span>(</span><span>reads</span> <span>&lt;&gt;</span> <span>0</span> <span>OR</span> writes <span>&lt;&gt;</span> <span>0</span> <span>OR</span> extends <span>&lt;&gt;</span> <span>0</span><span>)</span><span>;</span></code></pre></div>
<div data-language="text"><pre><code>    backend_type    | io_object | io_context |  reads   | writes  | extends | op_bytes | evictions |  reuses  | fsyncs |          stats_reset          
--------------------+-----------+------------+----------+---------+---------+----------+-----------+----------+--------+-------------------------------
 client backend     | relation  | bulkread   | 25900458 |  627059 |         |     8192 |    754610 | 25141667 |        | 2023-02-13 11:50:27.583875-08
 client backend     | relation  | bulkwrite  |     4654 | 2858085 | 3259572 |     8192 |    998220 |  2209070 |        | 2023-02-13 11:50:27.583875-08
 background worker  | relation  | bulkread   | 39059938 |  590896 |         |     8192 |    802939 | 38253662 |        | 2023-02-13 11:50:27.583875-08
 standalone backend | relation  | bulkwrite  |        0 |       0 |       8 |     8192 |         0 |        0 |        | 2023-02-13 11:50:27.583875-08
(4 rows)</code></pre></div>
<p>In this example, there is 495 GB of bulk read activity, and 21 GB of bulk write activity we had no good way of identifying before. However, and most importantly, we don&#39;t have to worry about the <code>evictions</code> count here - these are all evictions from the special bulk read / bulk write ring buffer, not from regular shared buffers.</p>
<h2 id="sneak-peek-visualizing-pg_stat_io-in-pganalyze"><a href="#sneak-peek-visualizing-pg_stat_io-in-pganalyze" aria-label="sneak peek visualizing pg_stat_io in pganalyze permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sneak peek: Visualizing pg_stat_io in pganalyze</h2>
<p>It&#39;s still a while until Postgres 16 will be released (usually September or October each year), but to help test things (and because it&#39;s exciting!) <strong>I took a quick stab at updating pganalyze in an experimental branch</strong> to collect <code>pg_stat_io</code> metrics and visualize them over time.</p>
<p>Here is a very early look at how this may look like in the future:</p>
<figure>
<span>
      <a href="https://payments.posthaven.com/static/9643f0e4baa16706490fda146c7a3791/56fb6/pganalyze_pg_stat_io.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Screenshot of experimental pg_stat_io view in pganalyze" title="Screenshot of experimental pg_stat_io view in pganalyze" src="https://payments.posthaven.com/static/9643f0e4baa16706490fda146c7a3791/1d69c/pganalyze_pg_stat_io.png" srcset="/static/9643f0e4baa16706490fda146c7a3791/4dcb9/pganalyze_pg_stat_io.png 188w, /static/9643f0e4baa16706490fda146c7a3791/5ff7e/pganalyze_pg_stat_io.png 375w, /static/9643f0e4baa16706490fda146c7a3791/1d69c/pganalyze_pg_stat_io.png 750w, /static/9643f0e4baa16706490fda146c7a3791/78797/pganalyze_pg_stat_io.png 1125w, /static/9643f0e4baa16706490fda146c7a3791/aa440/pganalyze_pg_stat_io.png 1500w, /static/9643f0e4baa16706490fda146c7a3791/56fb6/pganalyze_pg_stat_io.png 2752w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy" decoding="async"/>
  </a>
    </span>
<figcaption>Experimental view of how pg_stat_io could look like when visualized over time</figcaption>
</figure>
<p>Even though this is just running locally on my laptop, already we can see a clear pattern where writes are done by the checkpointer and background writer processes, most of the time. We can also see my <code>checkpoint_timeout</code> being set to <code>5min</code> (the default), with both <strong>writes and fsyncs happening like clockwork</strong> - note the workload is periodic every 10 minutes, so every second checkpoint has less work to do.</p>
<p>However, we can also clearly see a spike in activity - and that spike can be easily explained: To generate more database activity, I triggered a big daily background process around 8:10pm UTC. The high amount of data read caused the working set to momentarily exceed shared buffers, and caused a large amount of buffer evictions, which then caused <strong>the client backend having to write out buffer pages unexpectedly</strong>.</p>
<p>On this system I have a very small <code>shared_buffers</code> setting (the default, 128 MB). I should probably increase shared_buffers...</p>
<h2 id="the-future-of-io-observability-in-postgres"><a href="#the-future-of-io-observability-in-postgres" aria-label="the future of io observability in postgres permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The future of I/O observability in Postgres</h2>
<p>A lot of the ground work for <code>pg_stat_io</code> actually happened previously in Postgres 15, through the new cumulative statistics system using shared memory.</p>
<p>Before Postgres 15, statistics tracking had to go through the statistics collector (an obscure process that received UDP packets from individual processes part of Postgres), which was slow and error prone. This historically limited the ability to collect more advanced statistics easily. As the addition of <code>pg_stat_io</code> shows, it is now much easier to track additional information about how Postgres operates.</p>
<p>Amongst the immediate improvements that are already being discussed are:</p>
<ul>
<li>Tracking of system-wide buffer cache hits (to allow calculating an accurate buffer cache hit ratio)</li>
<li>Cumulative system-wide I/O times (not just I/O counts as currently present in <code>pg_stat_io</code>)</li>
<li>Better cumulative WAL statistics (i.e. going beyond what pg_stat_wal offers)</li>
<li>Additional I/O tracking for tables and indexes</li>
</ul>
<p>Our team at pganalyze is excited to have helped shape the new <code>pg_stat_io</code> view, and we look forward to continue working with the community on making Postgres better.</p>
<p>Share this article: If you&#39;d like to share this article with your peers, you can <a href="(https://twitter.com/intent/tweet?text=Waiting%20for%20Postgres%2016:%20Cumulative%20I/O%20statistics%20with%20pg_stat_io%20-%20Check%20out%20this%20article%20by%20%40pganalyze%20%20and%20learn%20about%20querying%20system%20wide%20I/O%20statistics%20in%20Postgres%3A%20https%3A%2F%2Fpganalyze.com%2Fblog%2Fpg-stat-io)">tweet about it here</a>.</p>
<p><strong>PS:</strong> If you&#39;re interested in learning more about optimizing Postgres I/O performance and costs you can <a href="https://pganalyze.com/webinars/optimizing-postgres-io-performance-and-costs">check out our webinar recording</a>.</p></div></div><div><h3>Sign up for the pganalyze newsletter</h3><p>Receive infrequent emails about interesting Postgres content around the web, new pganalyze feature releases, and new pganalyze ebooks. No spam, we promise.</p></div></div>
  </body>
</html>
