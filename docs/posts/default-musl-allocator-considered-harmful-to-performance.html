<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nickb.dev/blog/default-musl-allocator-considered-harmful-to-performance/">Original</a>
    <h1>Default musl allocator considered harmful to performance</h1>
    
    <div id="readability-page-1" class="page"><div>
            <p><strong>TLDR</strong>: In a real world benchmark, the default musl allocator caused a 7x slowdown compared to other allocators. I recommend all Rust projects immediately add the following lines to their application’s <code>main.rs</code>:</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>// Avoid musl&#39;s default allocator due to lackluster performance
</span></span></span><span><span><span>// https://nickb.dev/blog/default-musl-allocator-considered-harmful-to-performance
</span></span></span><span><span><span></span><span>#[cfg(target_env = </span><span>&#34;musl&#34;</span><span>)]</span>
</span></span><span><span><span>#[global_allocator]</span>
</span></span><span><span><span>static</span> <span>GLOBAL</span>: <span>mimalloc</span>::MiMalloc <span>=</span> mimalloc::MiMalloc;
</span></span></code></pre></div><p>And <code>Cargo.toml</code></p>
<div><pre tabindex="0"><code data-lang="toml"><span><span>[<span>target</span>.<span>&#39;cfg(target_env = &#34;musl&#34;)&#39;</span>.<span>dependencies</span>]
</span></span><span><span><span>mimalloc</span> = <span>&#34;0.1.43&#34;</span>
</span></span></code></pre></div><p>The root cause is the contention between multiple threads when allocating memory, so the problem worsens as more threads or allocations are created.</p>
<p>I recommend swapping the allocator even if musl is not a compilation target today or if the program is single threaded. This is something you simply don’t want to forget.</p>
<p>Reader’s choice on what allocator they want to sub in. The code snippets use mimalloc, but jemalloc is also good.</p>
<p>Also reader’s choice if allocation substitution should only be restricted to musl environments (as shown) or if it should be done globally. I don’t mind conditionally compiling dependencies here as it serves as another form of documentation.</p>
<h2 id="why-musl">Why musl?</h2>
<p>If I feel so strongly about avoiding the default musl allocator, why even use musl in the first place?</p>
<p>Well, when an important customer is running a version of Red Hat Linux initially released before I started high school, you can bet that there will be glibc issues if you don’t have a build machine with the same version of Red Hat Linux. Corollary: hats off to Red Hat for supporting their distro releases for such a lengthy period of time.</p>
<p>So this is a love-hate relationship with musl. I love cross compiling and creating static executables where I can <code>scp</code> anywhere and just have everything work. I will continue to use musl and respect the hard work behind the team.</p>
<p>And while docker image size should never be a deciding factor, it can be tantalizing to leverage a 2MB distroless image with a static build with musl to minimize any possibility of cold starts for scalable workloads (though if the executable is much larger than 2MB than the difference between container images is negligible, so your mileage may vary).</p>
<div><pre tabindex="0"><code data-lang="plain"><span><span>REPOSITORY                              IMAGE ID       SIZE
</span></span><span><span>gcr.io/distroless/cc-debian12           6f09ff5d0af8   23.4MB
</span></span><span><span>gcr.io/distroless/base-nossl-debian12   ae4cc24e698d   14.8MB
</span></span><span><span>gcr.io/distroless/base-debian12         fab58a7ef52e   20.7MB
</span></span><span><span>gcr.io/distroless/static-debian12       5d7d2b425607   1.99MB
</span></span></code></pre></div><h2 id="scene-of-the-crime">Scene of the crime</h2>
<p>I first noticed performance issues when a server was processing data slower than my host machine, which needed to fetch the data over a 1 Gbit/s connection.</p>
<p>Here’s my thought process as I honed in:</p>
<ul>
<li>The server has an older CPU and older CPUs are typically slower CPUs. (true, but not 7x slower).</li>
<li>Maybe the server Proxmox VM has a CPU type of <a href="https://www.yinfor.com/2023/06/how-i-choose-vm-cpu-type-in-proxmox-ve.html">kvm64</a> (which I’ve been guilty of before), as it excludes SIMD instructions</li>
<li>Maybe musl’s CPU feature detection is wonky and not selecting the SIMD-enabled code? (Nope, not true). I wish there was a more <a href="https://stackoverflow.com/a/60329803">ergonomic way</a> to see if SIMD is being executed.</li>
</ul>
<p>Embarrassingly, it took me an hour until I ran the glibc and musl version side by side on the host machine and on the server and found the glibc was much faster in both. To be fair, 45 minutes of that hour was spent fiddling with a C++ dependency’s build system to test if the CPU feature detection was different on musl vs glibc (I never did figure it out).</p>
<p>I created a reduced benchmark on the host machine and compared the following:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span># glibc</span>
</span></span><span><span>/usr/bin/time -v ./target/release/compare
</span></span><span><span>
</span></span><span><span><span># musl</span>
</span></span><span><span>/usr/bin/time -v ./target/x86_64-unknown-linux-musl/release/compare
</span></span></code></pre></div><p>Below is a comparison table of just the important metrics:</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>glibc</th>
          <th>musl</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>User time (seconds)</td>
          <td>1.31</td>
          <td>2.72</td>
      </tr>
      <tr>
          <td>System time (seconds)</td>
          <td>0.37</td>
          <td>6.13</td>
      </tr>
      <tr>
          <td>Percent of CPU this job got</td>
          <td>943%</td>
          <td>745%</td>
      </tr>
      <tr>
          <td>Elapsed time (seconds)</td>
          <td>0.17</td>
          <td>1.18</td>
      </tr>
      <tr>
          <td>Voluntary context switches</td>
          <td>1196</td>
          <td>199786</td>
      </tr>
      <tr>
          <td>Involuntary context switches</td>
          <td>191</td>
          <td>794</td>
      </tr>
  </tbody>
</table>
<p>The musl version took 7x longer! But I didn’t know why. All I knew is that the differences in voluntary context switches stood out: musl had a 167x more of them! At 200k switches per second, we’re in thrashing territory.</p>
<p>My first instinct was to profile the executables with <code>callgrind</code> and visualize them with <code>kcachegrind</code>.</p>
<p>I was disappointed to see the cycle estimate results were within the margin of error of each other. How can that be when the run times were so different? Perhaps this is a case of observer effect due to how much slower apps run under valgrind. Or maybe this information is just not captured in callgrind. Looks like I have <a href="https://unix.stackexchange.com/questions/259710/understanding-linux-perf-sched-switch-and-context-switches">some homework</a> to hone my profiling tools.</p>
<p>Next, I looked at syscalls, a common source of context switching.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>strace -c ./target/release/compare
</span></span></code></pre></div><p>Both versions had the same number of syscalls. The main difference is the musl version spent 6.7 seconds in a <code>futex</code> while glibc only 0.5 seconds, a 13x penalty! This means there must be some contention for a shared lock in musl when allocating or de-allocating memory from multiple threads.</p>
<p>And to be honest, this is where I reached the end of my investigative skills, and verified that swapping the allocator fixed the performance issues.</p>
<h2 id="this-is-not-news">This is not news</h2>
<p>Just searching for “musl performance issues” will yield a myriad of results. It must be a rite of passage for programmers to stumble upon this performance pitfall.</p>
<ul>
<li>Chimera Linux uses <a href="https://chimera-linux.org/docs/configuration/musl">musl with mimalloc</a>, noting
<blockquote>
<p>the stock allocator is the primary reason for nearly all performance issues people generally have with musl</p>
</blockquote>
</li>
<li>The folks at tweag.io documented musl’s <a href="https://www.tweag.io/blog/2023-08-10-rust-static-link-with-mimalloc">malloc contention</a> results in a 20x slowdown</li>
<li>Projects like <a href="https://github.com/BurntSushi/ripgrep/issues/1268#issuecomment-486432534">Ripgrep</a> and <a href="https://andygrove.io/2020/05/why-musl-extremely-slow/">Apache DataFusion</a> needed to swap out the allocator (or abandon musl).</li>
<li>Chainguard <a href="https://edu.chainguard.dev/chainguard/chainguard-images/about/images-compiled-programs/glibc-vs-musl#runtime-performance">reported</a> a 2-4x slowdown with musl</li>
<li>Binaryen’s wasp-opt saw a musl allocator slowdown of 10x (and this blog was cited as a <a href="https://github.com/WebAssembly/binaryen/issues/5561#issuecomment-2729617274">reference</a>)</li>
<li>Lots of reddit threads and github issues</li>
</ul>
<p>There is a wide range of reported slowdowns, from 2x to 20x. Such a wide range is due to how many threads are contending for the memory allocator in the application. For applications that don’t allocate much or have fewer parallel threads, the slowdown is not as drastic.</p>
<p>The 7x slowdown I observed was on a 6 core machine. Since the slowdown is correlated with the amount of contention, I decided to do an experiment.</p>
<p>I ran down to the local VPS provider, nabbed a 48 core machine, and crushed its dreams of running anything meaningful with the following benchmark.</p>
<div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>main</span>() {
</span></span><span><span>    <span>let</span> num_threads <span>=</span> std::thread::available_parallelism().map_or(<span>8</span>, <span>|</span>x<span>|</span> x.get());
</span></span><span><span>
</span></span><span><span>    <span>let</span> <span>mut</span> handles <span>=</span> vec![];
</span></span><span><span>    <span>for</span> _ <span>in</span> <span>0</span><span>..</span>num_threads {
</span></span><span><span>        <span>let</span> handle <span>=</span> std::thread::spawn(<span>move</span> <span>||</span> {
</span></span><span><span>            <span>let</span> <span>mut</span> counter <span>=</span> <span>0</span>;
</span></span><span><span>            <span>for</span> _ <span>in</span> <span>0</span><span>..</span><span>100000</span> {
</span></span><span><span>                <span>let</span> data <span>=</span> vec![<span>1</span><span>u8</span>; counter];
</span></span><span><span>                counter <span>+=</span> <span>usize</span>::from(data.get(<span>100</span>).copied().unwrap_or(<span>1</span>));
</span></span><span><span>            }
</span></span><span><span>            println!(<span>&#34;counter: </span><span>{}</span><span>&#34;</span>, counter);
</span></span><span><span>        });
</span></span><span><span>        handles.push(handle);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>for</span> handle <span>in</span> handles {
</span></span><span><span>        handle.join().unwrap();
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>Using the not-so-scientific measurements of <code>time</code>, the default allocator yielded:</p>
<div><pre tabindex="0"><code data-lang="plain"><span><span>real    0m0.169s
</span></span><span><span>user    0m7.680s
</span></span><span><span>sys     0m0.025s
</span></span></code></pre></div><p>While the musl build yielded:</p>
<div><pre tabindex="0"><code data-lang="plain"><span><span>real    1m56.890s
</span></span><span><span>user    1m0.522s
</span></span><span><span>sys     7m3.542s
</span></span></code></pre></div><p>That’s nearly a 700x slowdown. Going from a blink of an eye to having time to get up and stretch, and we’re just getting started. 48 cores is now only a mid-tier instance at AWS, as one can rent instances with 192 cores.</p>
<p>I also ran the benchmark on an 8 core machine from same VPS provider and came away with the following learnings:</p>
<ul>
<li>Running the benchmark across 6x more cores resulted in a 4x slowdown.</li>
<li>The stock GNU allocator took the same amount of time regardless of 8 cores or 48 cores. Nicely done!</li>
</ul>
<p>I find a 700x difference in synthetic workloads and a 7x difference in application performance stemming from the memory allocator to be mind boggling.</p>
<h2 id="a-skill-issue">A skill issue?</h2>
<p>Andrew Kelley, Zig’s creator, brings up a great <a href="https://github.com/ziglang/zig/issues/12484#issuecomment-2465546911">discussion point</a> that the underperformance might not be such a big issue to experienced programmers:</p>
<blockquote>
<blockquote>
<p>it under performs <strong>a lot</strong></p>
</blockquote>
<p>It’s true. The funny thing is when you’re a beginner Zig programmer you need a good GPA [General Purpose Allocator] for performance reasons but you lack the skills to write an allocator implementation.</p>
<p>However, as you become an advanced programmer you start to learn about better memory management techniques that makes GPA performance irrelevant. Ironically, at this point you become capable of implementing a better GPA, but it’s low-key kind of useful that GPA’s poor performance helps highlight where you’re not batch allocating objects.</p>
<p>That’s the Zig Malloc Paradox.</p>
</blockquote>
<p>Andrew is, of course, completely right. If I had used musl’s allocator from the start, the poor performance would have caused me to structure the code base to minimize allocations by <a href="https://www.cs.cornell.edu/~asampson/blog/flattening.html">flattening data structures</a> and facilitating object reuse.</p>
<p>Or would I have abandoned the attempt and switched to another language or project? Unfortunately, I can’t rewind time and at this point, I’m reluctant to introduce breaking changes, as the code is already fast enough under <em>most</em> circumstances, and relied upon by others.</p>
<p>While there is probably a correlation between the number of allocations and experience, I would not use this as a measure of success.</p>
<p>And in an ironic twist, I’m working on a Rust library in a domain that differentiates itself by minimizing allocations through ergonomic sacrifices. Is it worth it? To me it is, as the performance benefits are tangible, but to others where the library may slot in as a tertiary afterthought, ergonomics would be more highly valued.</p>
<h2 id="musls-new-mallocng">Musl’s new mallocng</h2>
<p>I got excited when I read a <a href="https://www.reddit.com/r/rust/comments/gdycv8/comment/fpkvskl/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">reddit post from May 2020</a> about how an upcoming redesign of musl’s allocator should bring improvements.</p>
<p>Later that same year, <a href="https://musl.libc.org/releases.html">musl released</a> it in v1.2.1, but didn’t illustrate any performance implications. Perhaps some of the release notes are insinuating a performance improvement?</p>
<p>In May 2023, Rust bumped the musl <a href="https://blog.rust-lang.org/2023/05/09/Updating-musl-targets.html">target to 1.2.x</a>, and <code>cross</code> <a href="https://github.com/cross-rs/cross/pull/1346">followed suit</a> in October.</p>
<p>There hasn’t (yet) been a cross release that includes this change (much to the <a href="https://github.com/cross-rs/cross/issues/1613">chagrin of some</a>), but no worries, I can <code>cargo install</code> from git.</p>
<p>The excitement had built to a crescendo.</p>
<p>But the benchmark results remained unchanged.</p>
<p>I’m not the only one to have <a href="https://github.com/clux/muslrust/issues/142#issuecomment-2184935013">noticed this</a>.</p>
<blockquote>
<p>the new ng allocator in MUSL doesn’t make a dime of a difference</p>
</blockquote>
<p>In the end, no matter what musl allocator you are using, I recommend switching to a different one as shown at the start.</p>
<p>I love musl, but now I know it needs a little extra something to go with it, and that would be another allocator.</p>
<p><a href="https://www.reddit.com/r/rust/comments/1ihttiv/default_musl_allocator_considered_harmful_to/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">Discuss on Reddit</a></p>

          </div></div>
  </body>
</html>
