<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deep-floyd/IF">Original</a>
    <h1>DeepFloyd IF: open-source text-to-image model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://github.com/deep-floyd/IF/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/15fe4051d4655b865d0b30e0c023b557a1e8cc800b066f69a081e166c540b1c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f64655f4c6963656e73652d4d6f6469666965645f4d49542d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Code_License-Modified_MIT-blue.svg"/></a>
<a href="https://github.com/deep-floyd/IF/blob/main/LICENSE-MODEL"><img src="https://camo.githubusercontent.com/b463fc7c59518bd6d3233f7b49d8ab137045ad1e2e6508ddf36ad1417b6b3a25/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f576569676874735f4c6963656e73652d44656570466c6f79645f49462d6f72616e67652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/Weights_License-DeepFloyd_IF-orange.svg"/></a>
<a href="https://pepy.tech/project/deepfloyd_if" rel="nofollow"><img src="https://camo.githubusercontent.com/dd073c06174a9a149aa2606351c3bd853b839a3087a7af3a2aa746144271e341/68747470733a2f2f706570792e746563682f62616467652f64656570666c6f79645f6966" alt="Downloads" data-canonical-src="https://pepy.tech/badge/deepfloyd_if"/></a></p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/nabla.jpg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/nabla.jpg" width="100%"/></a>
</p>
<p dir="auto">We introduce DeepFloyd IF, a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. DeepFloyd IF is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules: a base model that generates 64x64 px image based on text prompt and two super-resolution models, each designed to generate images of increasing resolution: 256x256 px and 1024x1024 px. All stages of the model utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/deepfloyd_if_scheme.jpg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/deepfloyd_if_scheme.jpg" width="100%"/></a>
</p>
<p dir="auto"><em>Inspired by</em> <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow"><em>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</em></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-minimum-requirements-to-use-all-if-models" aria-label="Heading link" href="#minimum-requirements-to-use-all-if-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Minimum requirements to use all IF models:</h2>
<ul dir="auto">
<li>16GB vRAM for IF-I-XL (4.3B text to 64x64 base module) &amp; IF-II-L (1.2B to 256x256 upscaler module)</li>
<li>24GB vRAM for IF-I-XL (4.3B text to 64x64 base module) &amp; IF-II-L (1.2B to 256x256 upscaler module) &amp; Stable x4 (to 1024x1024 upscaler)</li>
<li><code>xformers</code> and set env variable <code>FORCE_MEM_EFFICIENT_ATTN=1</code></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-label="Heading link" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto"><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a>
<a href="https://huggingface.co/spaces/DeepFloyd/IF" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install deepfloyd_if==1.0.0
pip install xformers==0.0.16
pip install git+https://github.com/openai/CLIP.git --no-deps"><pre>pip install deepfloyd_if==1.0.0
pip install xformers==0.0.16
pip install git+https://github.com/openai/CLIP.git --no-deps</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-local-notebook-and-ui-demo" aria-label="Heading link" href="#local-notebook-and-ui-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Local notebook and UI demo</h2>
<p dir="auto">The Dream, Style Transfer, Super Resolution or Inpainting modes are avaliable as in a Jupyter Notebook at <code>IF/notebooks/pipes-DeepFloyd-IF.ipynb</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-integration-with--diffusers" aria-label="Heading link" href="#integration-with--diffusers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Integration with <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">🤗</g-emoji> Diffusers</h2>
<p dir="auto">IF is also integrated with the <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">🤗</g-emoji> Hugging Face <a href="https://github.com/huggingface/diffusers/">Diffusers library</a>.</p>
<p dir="auto">Diffusers runs each stage individually allowing the user to customize the image generation process as well as allowing to inspect intermediate results easily.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-example" aria-label="Heading link" href="#example"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Example</h3>
<p dir="auto">Before you can use IF, you need to accept its usage conditions. To do so:</p>
<ol dir="auto">
<li>Make sure to have a <a href="https://huggingface.co/join" rel="nofollow">Hugging Face account</a> and be loggin in</li>
<li>Accept the license on the model card of <a href="https://huggingface.co/DeepFloyd/IF-I-IF-v1.0" rel="nofollow">DeepFloyd/IF-I-IF-v1.0</a></li>
<li>Make sure to login locally. Install <code>huggingface_hub</code></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pip install huggingface_hub --upgrade"><pre>pip install huggingface_hub --upgrade</pre></div>
<p dir="auto">run the login function in a Python shell</p>
<div dir="auto" data-snippet-clipboard-copy-content="from huggingface_hub import login

login()"><pre><span>from</span> <span>huggingface_hub</span> <span>import</span> <span>login</span>

<span>login</span>()</pre></div>
<p dir="auto">and enter your <a href="https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens" rel="nofollow">Hugging Face Hub access token</a>.</p>
<p dir="auto">Next we install <code>diffusers</code> and dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install diffusers accelerate transformers safetensors"><pre>pip install diffusers accelerate transformers safetensors</pre></div>
<p dir="auto">And we can now run the model locally.</p>
<p dir="auto">By default <code>diffusers</code> makes use of <a href="https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings" rel="nofollow">model cpu offloading</a> to run the whole IF pipeline with as little as 14 GB of VRAM.</p>
<p dir="auto">If you are using <code>torch&gt;=2.0.0</code>, make sure to <strong>delete all</strong> <code>enable_xformers_memory_efficient_attention()</code>
functions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from diffusers import DiffusionPipeline
from diffusers.utils import pt_to_pil
import torch

# stage 1
stage_1 = DiffusionPipeline.from_pretrained(&#34;DeepFloyd/IF-I-IF-v1.0&#34;, variant=&#34;fp16&#34;, torch_dtype=torch.float16)
stage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &gt;= 2.0.0
stage_1.enable_model_cpu_offload()

# stage 2
stage_2 = DiffusionPipeline.from_pretrained(
    &#34;DeepFloyd/IF-II-L-v1.0&#34;, text_encoder=None, variant=&#34;fp16&#34;, torch_dtype=torch.float16
)
stage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &gt;= 2.0.0
stage_2.enable_model_cpu_offload()

# stage 3
safety_modules = {&#34;feature_extractor&#34;: stage_1.feature_extractor, &#34;safety_checker&#34;: stage_1.safety_checker, &#34;watermarker&#34;: stage_1.watermarker}
stage_3 = DiffusionPipeline.from_pretrained(&#34;stabilityai/stable-diffusion-x4-upscaler&#34;, **safety_modules, torch_dtype=torch.float16)
stage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ &gt;= 2.0.0
stage_3.enable_model_cpu_offload()

prompt = &#39;a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says &#34;very deep learning&#34;&#39;

# text embeds
prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)

generator = torch.manual_seed(0)

# stage 1
image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=&#34;pt&#34;).images
pt_to_pil(image)[0].save(&#34;./if_stage_I.png&#34;)

# stage 2
image = stage_2(
    image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=&#34;pt&#34;
).images
pt_to_pil(image)[0].save(&#34;./if_stage_II.png&#34;)

# stage 3
image = stage_3(prompt=prompt, image=image, generator=generator, noise_level=100).images
image[0].save(&#34;./if_stage_III.png&#34;)"><pre><span>from</span> <span>diffusers</span> <span>import</span> <span>DiffusionPipeline</span>
<span>from</span> <span>diffusers</span>.<span>utils</span> <span>import</span> <span>pt_to_pil</span>
<span>import</span> <span>torch</span>

<span># stage 1</span>
<span>stage_1</span> <span>=</span> <span>DiffusionPipeline</span>.<span>from_pretrained</span>(<span>&#34;DeepFloyd/IF-I-IF-v1.0&#34;</span>, <span>variant</span><span>=</span><span>&#34;fp16&#34;</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>)
<span>stage_1</span>.<span>enable_xformers_memory_efficient_attention</span>()  <span># remove line if torch.__version__ &gt;= 2.0.0</span>
<span>stage_1</span>.<span>enable_model_cpu_offload</span>()

<span># stage 2</span>
<span>stage_2</span> <span>=</span> <span>DiffusionPipeline</span>.<span>from_pretrained</span>(
    <span>&#34;DeepFloyd/IF-II-L-v1.0&#34;</span>, <span>text_encoder</span><span>=</span><span>None</span>, <span>variant</span><span>=</span><span>&#34;fp16&#34;</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>
)
<span>stage_2</span>.<span>enable_xformers_memory_efficient_attention</span>()  <span># remove line if torch.__version__ &gt;= 2.0.0</span>
<span>stage_2</span>.<span>enable_model_cpu_offload</span>()

<span># stage 3</span>
<span>safety_modules</span> <span>=</span> {<span>&#34;feature_extractor&#34;</span>: <span>stage_1</span>.<span>feature_extractor</span>, <span>&#34;safety_checker&#34;</span>: <span>stage_1</span>.<span>safety_checker</span>, <span>&#34;watermarker&#34;</span>: <span>stage_1</span>.<span>watermarker</span>}
<span>stage_3</span> <span>=</span> <span>DiffusionPipeline</span>.<span>from_pretrained</span>(<span>&#34;stabilityai/stable-diffusion-x4-upscaler&#34;</span>, <span>**</span><span>safety_modules</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>)
<span>stage_3</span>.<span>enable_xformers_memory_efficient_attention</span>()  <span># remove line if torch.__version__ &gt;= 2.0.0</span>
<span>stage_3</span>.<span>enable_model_cpu_offload</span>()

<span>prompt</span> <span>=</span> <span>&#39;a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says &#34;very deep learning&#34;&#39;</span>

<span># text embeds</span>
<span>prompt_embeds</span>, <span>negative_embeds</span> <span>=</span> <span>stage_1</span>.<span>encode_prompt</span>(<span>prompt</span>)

<span>generator</span> <span>=</span> <span>torch</span>.<span>manual_seed</span>(<span>0</span>)

<span># stage 1</span>
<span>image</span> <span>=</span> <span>stage_1</span>(<span>prompt_embeds</span><span>=</span><span>prompt_embeds</span>, <span>negative_prompt_embeds</span><span>=</span><span>negative_embeds</span>, <span>generator</span><span>=</span><span>generator</span>, <span>output_type</span><span>=</span><span>&#34;pt&#34;</span>).<span>images</span>
<span>pt_to_pil</span>(<span>image</span>)[<span>0</span>].<span>save</span>(<span>&#34;./if_stage_I.png&#34;</span>)

<span># stage 2</span>
<span>image</span> <span>=</span> <span>stage_2</span>(
    <span>image</span><span>=</span><span>image</span>, <span>prompt_embeds</span><span>=</span><span>prompt_embeds</span>, <span>negative_prompt_embeds</span><span>=</span><span>negative_embeds</span>, <span>generator</span><span>=</span><span>generator</span>, <span>output_type</span><span>=</span><span>&#34;pt&#34;</span>
).<span>images</span>
<span>pt_to_pil</span>(<span>image</span>)[<span>0</span>].<span>save</span>(<span>&#34;./if_stage_II.png&#34;</span>)

<span># stage 3</span>
<span>image</span> <span>=</span> <span>stage_3</span>(<span>prompt</span><span>=</span><span>prompt</span>, <span>image</span><span>=</span><span>image</span>, <span>generator</span><span>=</span><span>generator</span>, <span>noise_level</span><span>=</span><span>100</span>).<span>images</span>
<span>image</span>[<span>0</span>].<span>save</span>(<span>&#34;./if_stage_III.png&#34;</span>)</pre></div>
<p dir="auto">There are multiple ways to speed up the inference time and lower the memory consumption even more with <code>diffusers</code>. To do so, please have a look at the Diffusers docs:</p>
<ul dir="auto">
<li><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji> <a href="https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-speed" rel="nofollow">Optimizing for inference time</a></li>
<li><g-emoji alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png">⚙️</g-emoji> <a href="https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-memory" rel="nofollow">Optimizing for low memory during inference</a></li>
</ul>
<p dir="auto">For more in-detail information about how to use IF, please have a look at <a href="https://huggingface.co/blog/if" rel="nofollow">the IF blog post</a> <g-emoji alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png">📖</g-emoji>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-run-the-code-locally" aria-label="Heading link" href="#run-the-code-locally"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run the code locally</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-loading-the-models-into-vram" aria-label="Heading link" href="#loading-the-models-into-vram"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Loading the models into VRAM</h3>
<div dir="auto" data-snippet-clipboard-copy-content="from deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII
from deepfloyd_if.modules.t5 import T5Embedder

device = &#39;cuda:0&#39;
if_I = IFStageI(&#39;IF-I-IF-v1.0&#39;, device=device)
if_II = IFStageII(&#39;IF-II-L-v1.0&#39;, device=device)
if_III = StableStageIII(&#39;stable-diffusion-x4-upscaler&#39;, device=device)
t5 = T5Embedder(device=&#34;cpu&#34;)"><pre><span>from</span> <span>deepfloyd_if</span>.<span>modules</span> <span>import</span> <span>IFStageI</span>, <span>IFStageII</span>, <span>StableStageIII</span>
<span>from</span> <span>deepfloyd_if</span>.<span>modules</span>.<span>t5</span> <span>import</span> <span>T5Embedder</span>

<span>device</span> <span>=</span> <span>&#39;cuda:0&#39;</span>
<span>if_I</span> <span>=</span> <span>IFStageI</span>(<span>&#39;IF-I-IF-v1.0&#39;</span>, <span>device</span><span>=</span><span>device</span>)
<span>if_II</span> <span>=</span> <span>IFStageII</span>(<span>&#39;IF-II-L-v1.0&#39;</span>, <span>device</span><span>=</span><span>device</span>)
<span>if_III</span> <span>=</span> <span>StableStageIII</span>(<span>&#39;stable-diffusion-x4-upscaler&#39;</span>, <span>device</span><span>=</span><span>device</span>)
<span>t5</span> <span>=</span> <span>T5Embedder</span>(<span>device</span><span>=</span><span>&#34;cpu&#34;</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-i-dream" aria-label="Heading link" href="#i-dream"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>I. Dream</h3>
<p dir="auto">Dream is the text-to-image mode of the IF model</p>
<div dir="auto" data-snippet-clipboard-copy-content="from deepfloyd_if.pipelines import dream

prompt = &#39;ultra close-up color photo portrait of rainbow owl with deer horns in the woods&#39;
count = 4

result = dream(
    t5=t5, if_I=if_I, if_II=if_II, if_III=if_III,
    prompt=[prompt]*count,
    seed=42,
    if_I_kwargs={
        &#34;guidance_scale&#34;: 7.0,
        &#34;sample_timestep_respacing&#34;: &#34;smart100&#34;,
    },
    if_II_kwargs={
        &#34;guidance_scale&#34;: 4.0,
        &#34;sample_timestep_respacing&#34;: &#34;smart50&#34;,
    },
    if_III_kwargs={
        &#34;guidance_scale&#34;: 9.0,
        &#34;noise_level&#34;: 20,
        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,
    },
)

if_III.show(result[&#39;III&#39;], size=14)"><pre><span>from</span> <span>deepfloyd_if</span>.<span>pipelines</span> <span>import</span> <span>dream</span>

<span>prompt</span> <span>=</span> <span>&#39;ultra close-up color photo portrait of rainbow owl with deer horns in the woods&#39;</span>
<span>count</span> <span>=</span> <span>4</span>

<span>result</span> <span>=</span> <span>dream</span>(
    <span>t5</span><span>=</span><span>t5</span>, <span>if_I</span><span>=</span><span>if_I</span>, <span>if_II</span><span>=</span><span>if_II</span>, <span>if_III</span><span>=</span><span>if_III</span>,
    <span>prompt</span><span>=</span>[<span>prompt</span>]<span>*</span><span>count</span>,
    <span>seed</span><span>=</span><span>42</span>,
    <span>if_I_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>7.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;smart100&#34;</span>,
    },
    <span>if_II_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>4.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;smart50&#34;</span>,
    },
    <span>if_III_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>9.0</span>,
        <span>&#34;noise_level&#34;</span>: <span>20</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;75&#34;</span>,
    },
)

<span>if_III</span>.<span>show</span>(<span>result</span>[<span>&#39;III&#39;</span>], <span>size</span><span>=</span><span>14</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/dream-III.jpg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/dream-III.jpg" alt=""/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-ii-zero-shot-image-to-image-translation" aria-label="Heading link" href="#ii-zero-shot-image-to-image-translation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>II. Zero-shot Image-to-Image Translation</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/img_to_img_scheme.jpeg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/img_to_img_scheme.jpeg" alt=""/></a></p>
<p dir="auto">In Style Transfer mode, the output of your prompt comes out at the style of the <code>support_pil_img</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="from deepfloyd_if.pipelines import style_transfer

result = style_transfer(
    t5=t5, if_I=if_I, if_II=if_II,
    support_pil_img=raw_pil_image,
    style_prompt=[
        &#39;in style of professional origami&#39;,
        &#39;in style of oil art, Tate modern&#39;,
        &#39;in style of plastic building bricks&#39;,
        &#39;in style of classic anime from 1990&#39;,
    ],
    seed=42,
    if_I_kwargs={
        &#34;guidance_scale&#34;: 10.0,
        &#34;sample_timestep_respacing&#34;: &#34;10,10,10,10,10,10,10,10,0,0&#34;,
        &#39;support_noise_less_qsample_steps&#39;: 5,
    },
    if_II_kwargs={
        &#34;guidance_scale&#34;: 4.0,
        &#34;sample_timestep_respacing&#34;: &#39;smart50&#39;,
        &#34;support_noise_less_qsample_steps&#34;: 5,
    },
)
if_I.show(result[&#39;II&#39;], 1, 20)"><pre><span>from</span> <span>deepfloyd_if</span>.<span>pipelines</span> <span>import</span> <span>style_transfer</span>

<span>result</span> <span>=</span> <span>style_transfer</span>(
    <span>t5</span><span>=</span><span>t5</span>, <span>if_I</span><span>=</span><span>if_I</span>, <span>if_II</span><span>=</span><span>if_II</span>,
    <span>support_pil_img</span><span>=</span><span>raw_pil_image</span>,
    <span>style_prompt</span><span>=</span>[
        <span>&#39;in style of professional origami&#39;</span>,
        <span>&#39;in style of oil art, Tate modern&#39;</span>,
        <span>&#39;in style of plastic building bricks&#39;</span>,
        <span>&#39;in style of classic anime from 1990&#39;</span>,
    ],
    <span>seed</span><span>=</span><span>42</span>,
    <span>if_I_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>10.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;10,10,10,10,10,10,10,10,0,0&#34;</span>,
        <span>&#39;support_noise_less_qsample_steps&#39;</span>: <span>5</span>,
    },
    <span>if_II_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>4.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#39;smart50&#39;</span>,
        <span>&#34;support_noise_less_qsample_steps&#34;</span>: <span>5</span>,
    },
)
<span>if_I</span>.<span>show</span>(<span>result</span>[<span>&#39;II&#39;</span>], <span>1</span>, <span>20</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/deep_floyd_if_image_2_image.gif"><img src="https://github.com/deep-floyd/IF/raw/main/pics/deep_floyd_if_image_2_image.gif" alt="Alternative Text" data-animated-image=""/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-iii-super-resolution" aria-label="Heading link" href="#iii-super-resolution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>III. Super Resolution</h2>
<p dir="auto">For super-resolution, users can run <code>IF-II</code> and <code>IF-III</code> or &#39;Stable x4&#39; on an image that was not necessarely generated by IF (two cascades):</p>
<div dir="auto" data-snippet-clipboard-copy-content="from deepfloyd_if.pipelines import super_resolution

middle_res = super_resolution(
    t5,
    if_III=if_II,
    prompt=[&#39;woman with a blue headscarf and a blue sweaterp, detailed picture, 4k dslr, best quality&#39;],
    support_pil_img=raw_pil_image,
    img_scale=4.,
    img_size=64,
    if_III_kwargs={
        &#39;sample_timestep_respacing&#39;: &#39;smart100&#39;,
        &#39;aug_level&#39;: 0.5,
        &#39;guidance_scale&#39;: 6.0,
    },
)
high_res = super_resolution(
    t5,
    if_III=if_III,
    prompt=[&#39;&#39;],
    support_pil_img=middle_res[&#39;III&#39;][0],
    img_scale=4.,
    img_size=256,
    if_III_kwargs={
        &#34;guidance_scale&#34;: 9.0,
        &#34;noise_level&#34;: 20,
        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,
    },
)
show_superres(raw_pil_image, high_res[&#39;III&#39;][0])"><pre><span>from</span> <span>deepfloyd_if</span>.<span>pipelines</span> <span>import</span> <span>super_resolution</span>

<span>middle_res</span> <span>=</span> <span>super_resolution</span>(
    <span>t5</span>,
    <span>if_III</span><span>=</span><span>if_II</span>,
    <span>prompt</span><span>=</span>[<span>&#39;woman with a blue headscarf and a blue sweaterp, detailed picture, 4k dslr, best quality&#39;</span>],
    <span>support_pil_img</span><span>=</span><span>raw_pil_image</span>,
    <span>img_scale</span><span>=</span><span>4.</span>,
    <span>img_size</span><span>=</span><span>64</span>,
    <span>if_III_kwargs</span><span>=</span>{
        <span>&#39;sample_timestep_respacing&#39;</span>: <span>&#39;smart100&#39;</span>,
        <span>&#39;aug_level&#39;</span>: <span>0.5</span>,
        <span>&#39;guidance_scale&#39;</span>: <span>6.0</span>,
    },
)
<span>high_res</span> <span>=</span> <span>super_resolution</span>(
    <span>t5</span>,
    <span>if_III</span><span>=</span><span>if_III</span>,
    <span>prompt</span><span>=</span>[<span>&#39;&#39;</span>],
    <span>support_pil_img</span><span>=</span><span>middle_res</span>[<span>&#39;III&#39;</span>][<span>0</span>],
    <span>img_scale</span><span>=</span><span>4.</span>,
    <span>img_size</span><span>=</span><span>256</span>,
    <span>if_III_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>9.0</span>,
        <span>&#34;noise_level&#34;</span>: <span>20</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;75&#34;</span>,
    },
)
<span>show_superres</span>(<span>raw_pil_image</span>, <span>high_res</span>[<span>&#39;III&#39;</span>][<span>0</span>])</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/if_as_upscaler.jpg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/if_as_upscaler.jpg" alt=""/></a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-iv-zero-shot-inpainting" aria-label="Heading link" href="#iv-zero-shot-inpainting"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>IV. Zero-shot Inpainting</h3>
<div dir="auto" data-snippet-clipboard-copy-content="from deepfloyd_if.pipelines import inpainting

result = inpainting(
    t5=t5, if_I=if_I,
    if_II=if_II,
    if_III=if_III,
    support_pil_img=raw_pil_image,
    inpainting_mask=inpainting_mask,
    prompt=[
        &#39;oil art, a man in a hat&#39;,
    ],
    seed=42,
    if_I_kwargs={
        &#34;guidance_scale&#34;: 7.0,
        &#34;sample_timestep_respacing&#34;: &#34;10,10,10,10,10,0,0,0,0,0&#34;,
        &#39;support_noise_less_qsample_steps&#39;: 0,
    },
    if_II_kwargs={
        &#34;guidance_scale&#34;: 4.0,
        &#39;aug_level&#39;: 0.0,
        &#34;sample_timestep_respacing&#34;: &#39;100&#39;,
    },
    if_III_kwargs={
        &#34;guidance_scale&#34;: 9.0,
        &#34;noise_level&#34;: 20,
        &#34;sample_timestep_respacing&#34;: &#34;75&#34;,
    },
)
if_I.show(result[&#39;I&#39;], 2, 3)
if_I.show(result[&#39;II&#39;], 2, 6)
if_I.show(result[&#39;III&#39;], 2, 14)"><pre><span>from</span> <span>deepfloyd_if</span>.<span>pipelines</span> <span>import</span> <span>inpainting</span>

<span>result</span> <span>=</span> <span>inpainting</span>(
    <span>t5</span><span>=</span><span>t5</span>, <span>if_I</span><span>=</span><span>if_I</span>,
    <span>if_II</span><span>=</span><span>if_II</span>,
    <span>if_III</span><span>=</span><span>if_III</span>,
    <span>support_pil_img</span><span>=</span><span>raw_pil_image</span>,
    <span>inpainting_mask</span><span>=</span><span>inpainting_mask</span>,
    <span>prompt</span><span>=</span>[
        <span>&#39;oil art, a man in a hat&#39;</span>,
    ],
    <span>seed</span><span>=</span><span>42</span>,
    <span>if_I_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>7.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;10,10,10,10,10,0,0,0,0,0&#34;</span>,
        <span>&#39;support_noise_less_qsample_steps&#39;</span>: <span>0</span>,
    },
    <span>if_II_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>4.0</span>,
        <span>&#39;aug_level&#39;</span>: <span>0.0</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#39;100&#39;</span>,
    },
    <span>if_III_kwargs</span><span>=</span>{
        <span>&#34;guidance_scale&#34;</span>: <span>9.0</span>,
        <span>&#34;noise_level&#34;</span>: <span>20</span>,
        <span>&#34;sample_timestep_respacing&#34;</span>: <span>&#34;75&#34;</span>,
    },
)
<span>if_I</span>.<span>show</span>(<span>result</span>[<span>&#39;I&#39;</span>], <span>2</span>, <span>3</span>)
<span>if_I</span>.<span>show</span>(<span>result</span>[<span>&#39;II&#39;</span>], <span>2</span>, <span>6</span>)
<span>if_I</span>.<span>show</span>(<span>result</span>[<span>&#39;III&#39;</span>], <span>2</span>, <span>14</span>)</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/deep_floyd_if_inpainting.gif"><img src="https://github.com/deep-floyd/IF/raw/main/pics/deep_floyd_if_inpainting.gif" alt="" data-animated-image=""/></a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content--model-zoo-" aria-label="Heading link" href="#-model-zoo-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">🤗</g-emoji> Model Zoo <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">🤗</g-emoji></h3>
<p dir="auto">The link to download the weights as well as the model cards will be available soon on each model of the model zoo</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-original" aria-label="Heading link" href="#original"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Original</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Cascade</th>
<th>Params</th>
<th>FID</th>
<th>Batch size</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/DeepFloyd/IF-I-M-v1.0" rel="nofollow">IF-I-M</a></td>
<td>I</td>
<td>400M</td>
<td>8.86</td>
<td>3072</td>
<td>2.5M</td>
</tr>
<tr>
<td><a href="https://huggingface.co/DeepFloyd/IF-I-L-v1.0" rel="nofollow">IF-I-L</a></td>
<td>I</td>
<td>900M</td>
<td>8.06</td>
<td>3200</td>
<td>3.0M</td>
</tr>
<tr>
<td><a href="https://huggingface.co/DeepFloyd/IF-I-IF-v1.0" rel="nofollow">IF-I-XL</a>*</td>
<td>I</td>
<td>4.3B</td>
<td>6.66</td>
<td>3072</td>
<td>2.42M</td>
</tr>
<tr>
<td><a href="https://huggingface.co/DeepFloyd/IF-II-M-v1.0" rel="nofollow">IF-II-M</a></td>
<td>II</td>
<td>450M</td>
<td>-</td>
<td>1536</td>
<td>2.5M</td>
</tr>
<tr>
<td><a href="https://huggingface.co/DeepFloyd/IF-II-L-v1.0" rel="nofollow">IF-II-L</a>*</td>
<td>II</td>
<td>1.2B</td>
<td>-</td>
<td>1536</td>
<td>2.5M</td>
</tr>
<tr>
<td>IF-III-L* <em>(soon)</em></td>
<td>III</td>
<td>700M</td>
<td>-</td>
<td>3072</td>
<td>1.25M</td>
</tr>
</tbody>
</table>
<p dir="auto">*best modules</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-quantitative-evaluation" aria-label="Heading link" href="#quantitative-evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quantitative Evaluation</h3>
<p dir="auto"><code>FID = 6.66</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deep-floyd/IF/blob/main/pics/fid30k_if.jpg"><img src="https://github.com/deep-floyd/IF/raw/main/pics/fid30k_if.jpg" alt=""/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-label="Heading link" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">The code in this repository is released under the bespoke license (see added <a href="https://github.com/deep-floyd/IF/blob/main/LICENSE#L13">point two</a>).</p>
<p dir="auto">The weights will be available soon via <a href="https://huggingface.co/DeepFloyd" rel="nofollow">the DeepFloyd organization at Hugging Face</a> and have their own LICENSE.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-limitations-and-biases" aria-label="Heading link" href="#limitations-and-biases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Limitations and Biases</h2>
<p dir="auto">The models available in this codebase have known limitations and biases. Please refer to <a href="https://huggingface.co/DeepFloyd/IF-I-L-v1.0" rel="nofollow">the model card</a> for more information.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--deepfloyd-if-creators" aria-label="Heading link" href="#-deepfloyd-if-creators"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="mortar_board" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png">🎓</g-emoji> DeepFloyd IF creators:</h2>
<ul dir="auto">
<li><a href="https://github.com/shonenkov">Alex Shonenkov</a></li>
<li><a href="https://github.com/zeroshot-ai">Misha Konstantinov</a></li>
<li><a href="https://github.com/Gugutse">Daria Bakshandaeva</a></li>
<li><a href="https://github.com/christophschuhmann">Christoph Schuhmann</a></li>
<li><a href="https://github.com/ivksu">Ksenia Ivanova</a></li>
<li><a href="https://github.com/vauimpuls">Nadiia Klokova</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content--research-paper-soon" aria-label="Heading link" href="#-research-paper-soon"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="page_facing_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png">📄</g-emoji> Research Paper (Soon)</h2>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgements" aria-label="Heading link" href="#acknowledgements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">Special thanks to <a href="http://stability.ai" rel="nofollow">StabilityAI</a> and its CEO <a href="https://twitter.com/emostaque" rel="nofollow">Emad Mostaque</a> for invaluable support, providing GPU compute and infrastructure to train the models (our gratitude goes to <a href="https://github.com/rvencu">Richard Vencu</a>); thanks to <a href="https://laion.ai" rel="nofollow">LAION</a> and <a href="https://github.com/christophschuhmann">Christoph Schuhmann</a> in particular for contribution to the project and well-prepared datasets; thanks to <a href="https://huggingface.co" rel="nofollow">Huggingface</a> teams for optimizing models&#39; speed and memory consumption during inference, creating demos and giving cool advice!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--external-contributors-" aria-label="Heading link" href="#-external-contributors-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji> External Contributors <g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji></h2>
<ul dir="auto">
<li>The Biggest Thanks <a href="https://github.com/apolinario">@Apolinário</a>, for ideas, consultations, help and support on all stages to make IF available in open-source; for writing a lot of documentation and instructions; for creating a friendly atmosphere in difficult moments <g-emoji alias="owl" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f989.png">🦉</g-emoji>;</li>
<li>Thanks, <a href="https://github.com/patrickvonplaten">@patrickvonplaten</a>, for improving loading time of unet models by 80%;
for integration Stable-Diffusion-x4 as native pipeline <g-emoji alias="muscle" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4aa.png">💪</g-emoji>;</li>
<li>Thanks, <a href="https://github.com/williamberman">@williamberman</a> and <a href="https://github.com/patrickvonplaten">@patrickvonplaten</a> for diffusers integration <g-emoji alias="raised_hands" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64c.png">🙌</g-emoji>;</li>
<li>Thanks, <a href="https://github.com/hysts">@hysts</a> and <a href="https://github.com/apolinario">@Apolinário</a> for creating <a href="https://huggingface.co/spaces/DeepFloyd/IF" rel="nofollow">the best gradio demo with IF</a> <g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji>;</li>
<li>Thanks, <a href="https://github.com/Dango233">@Dango233</a>, for adaptation IF with xformers memory efficient attention <g-emoji alias="muscle" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4aa.png">💪</g-emoji>;</li>
</ul>
</article>
          </div></div>
  </body>
</html>
