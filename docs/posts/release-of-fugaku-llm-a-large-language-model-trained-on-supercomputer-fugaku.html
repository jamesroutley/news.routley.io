<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html">Original</a>
    <h1>Release of Fugaku-LLM – a large language model trained on supercomputer Fugaku</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p><img alt="" src="https://www.fujitsu.com/global/imagesgig5/top-r_tcm100-6314163_tcm100-2750236-32.png"/></p></div><!--Title--><!--Headline--><h2>Enhanced Japanese language ability, for use in research and business</h2><!--Business Organization--><h2>Tokyo Institute of Technology, Tohoku University, Fujitsu Limited, RIKEN, Nagoya University, CyberAgent Inc., Kotoba Technologies Inc.</h2><!--City, Press Release Date--><p><strong>Kawasaki, May 10, 2024</strong></p><h3>Summary</h3><ul><li>
Large language model with enhanced Japanese language ability was developed using Japanese supercomputing technology
</li><li>
Distributed parallel learning by maximizing the performance of the supercomputer “Fugaku”
</li><li>
Commercial use is permitted, which will lead to innovative research and business applications such as AI for Science
</li></ul><div><h3>Abstract</h3><p>
A team of researchers in Japan released Fugaku-LLM, a large language model <sup>(<a href="#footnote1">1</a>)</sup> with enhanced Japanese language capability, using the RIKEN supercomputer Fugaku. The team is led by Professor Rio Yokota of Tokyo Institute of Technology, Associate Professor Keisuke Sakaguchi of Tohoku University, Koichi Shirahata of Fujitsu Limited, Team Leader Mohamed Wahib of RIKEN, Associate Professor Koji Nishiguchi of Nagoya University, Shota Sasaki of CyberAgent, Inc, and Noriyuki Kojima of Kotoba Technologies Inc.
</p><p>
To train large language models on Fugaku, the researchers developed distributed training methods, including porting the deep learning framework Megatron-DeepSpeed to Fugaku in order to optimize the performance of Transformers on Fugaku. They accelerated the dense matrix multiplication library for Transformers, and optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication library on the Tofu interconnect D.
</p><p>
Fugaku-LLM has 13 billion parameters <sup>(<a href="#footnote2">2</a>)</sup> and is larger than the 7-billion-parameter models that have been developed widely in Japan. Fugaku-LLM has enhanced Japanese capabilities, with an average score of 5.5 on the Japanese MT-Bench <sup>(<a href="#footnote3">3</a>)</sup>, the highest performance among open models that are trained using original data produced in Japan. In particular, the benchmark performance for humanities and social sciences tasks reached a remarkably high score of 9.18.
</p><p>
Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data. The source code of Fugaku-LLM is available on GitHub <sup>(<a href="#footnote4">4</a>)</sup> and the model is available on Hugging Face <sup>(<a href="#footnote5">5</a>)</sup>. Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license.
</p><p>
In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.
</p></div><!--Paragraph - body--><h3>Background</h3><p>
In recent years, the development of large language models (LLMs) has been active, especially in the United States. In particular, the rapid spread of ChatGPT <sup>(<a href="#footnote6">6</a>)</sup>, developed by OpenAI, has profoundly impacted research and development, economic systems, and national security. Countries other than the U.S. are also investing enormous human and computational resources to develop LLMs in their own countries. Japan, too, needs to secure computational resources for AI research so as not to fall behind in this global race. There are high expectations for Fugaku, the flagship supercomputer system in Japan, and it is necessary to improve the computational environment for large-scale distributed training on Fugaku to meet these expectations.
</p><p>
 Therefore, Tokyo Institute of Technology, Tohoku University, Fujitsu, RIKEN, Nagoya University, CyberAgent, and Kotoba Technologies have started a joint research project on the development of large language models.
</p><h3>Role of each institution/company</h3><p><strong>Tokyo Institute of Technology:</strong> General oversight, parallelization and communication acceleration of large language models (optimization of communication performance by combining three types of parallelization, acceleration of collective communication on the Tofu interconnect D)
</p><p><strong>Tohoku University:</strong> Collection of training data and model selection
</p><p><strong>Fujitsu:</strong> Acceleration of computation and communication (acceleration of collective communication on Tofu interconnect D, performance optimization of pipeline parallelization) and implementation of pre-training and fine-tuning after training
</p><p><strong>RIKEN:</strong> Distributed parallelization and communication acceleration of large-scale language models (acceleration of collective communication on Tofu interconnect D)
</p><p><strong>Nagoya University:</strong> Study on application methods of Fugaku-LLM to 3D generative AI
</p><p><strong>CyberAgent:</strong> Provision of training data
</p><p><strong>Kotoba Technologies:</strong> Porting of deep learning framework to Fugaku
</p><!--Image--><div><p><img alt="Figure 1. RIKEN‘s supercomputer Fugaku ©RIKEN" src="https://www.fujitsu.com/global/imagesgig5/20240510-01a_tcm100-7566208_tcm100-2750236-32.jpg"/><span>Figure 1. RIKEN‘s supercomputer Fugaku ©RIKEN</span></p></div><h3>Research outcome</h3><h3>1. Significantly improved the computational performance of training large language models on the supercomputer Fugaku</h3><p>
 GPUs <sup>(<a href="#footnote7">7</a>)</sup> are the common choice of hardware for training large language models. However, there is a global shortage of GPUs due to the large investment from many countries to train LLMs. Under such circumstances, it is important to show that large language models can be trained using Fugaku, which uses CPUs instead of GPUs. The CPUs used in Fugaku are Japanese CPUs manufactured by Fujitsu, and play an important role in terms of revitalizing Japanese semiconductor technology.
</p><p>
By extracting the full potential of Fugaku, this study succeeded in increasing the computation speed of the matrix multiplication by a factor of 6, and the communication speed by a factor of 3. To maximize the distributed training performance on Fugaku, the deep learning framework Megatron-DeepSpeed was ported to Fugaku, and the dense matrix multiplication library was accelerated for Transformer. For communication acceleration, the researchers optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication on the Tofu interconnect D. The knowledge gained from these efforts can be utilized in the design of the next-generation computing infrastructure after Fugaku and will greatly enhance Japan&#39;s future advantage in the field of AI.
</p><h3>2. An easy-to-use, open, and secure, large language model with 13 billion parameters</h3><p>
In 2023, many large language models were developed by Japanese companies, but most of them have less than 7 billion parameters. Since the performance of large-scale language models generally improves as the number of parameters increases, the 13-billion-parameter model the research team developed is likely to be more powerful than other Japanese models. Although larger models have been developed outside of Japan, large language models also require large computational resources, making it difficult to use models with too many parameters. Fugaku-LLM is both high performance and well-balanced.
</p><p>
In addition, most models developed by Japanese companies employ continual learning <sup>(<a href="#footnote8">8</a>)</sup>, in which open models developed outside of Japan are continually trained on Japanese data. In contrast, Fugaku-LLM is trained from scratch using the team’s own data, so the entire learning process can be understood, which is superior in terms of transparency and safety.
</p><p>
Fugaku-LLM was trained on 380 billion tokens using 13,824 nodes of Fugaku, with about 60% of the training data being Japanese, combined with English, mathematics, and code. Compared to models that continually train on Japanese, Fugaku-LLM learned much of its information in Japanese. Fugaku-LLM is the best model among open models that are produced in Japan and trained with original data. In particular, it was confirmed that the model shows a high benchmark score of 9.18 in the humanities and social sciences tasks. It is expected that the model will be able to perform natural dialogue based on keigo (honorific speech) and other features of the Japanese language.
</p><h3>Future Development</h3><p>
The results from this research are being made public through GitHub and Hugging Face so that other researchers and engineers can use them to further develop large language models. Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license. Fugaku-LLM will be also offered to users via the Fujitsu Research Portal from May 10th, 2024.
</p><p>
In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.
</p><h3>Acknowledgement</h3><p>
This research was supported by the Fugaku policy-supporting proposal &#34;Development of Distributed Parallel Training for Large Language Models Using Fugaku&#34; (proposal number: hp230254).
</p><!--Paragraph - body end--><!--Footnotes--><hr/><ul><li><p><strong>[1]</strong></p></li><li><p><strong>[2]</strong></p></li><li><p><strong>[3]</strong></p></li><li><p><strong>[4]</strong></p></li><li><p><strong>[5]</strong></p></li><li><p><strong>[6]</strong></p></li><li><p><strong>[7]</strong></p></li><li><p><strong>[8]</strong></p></li></ul><!--Boilerplate(s)--><!--About Fujitsu--><!--Media Contacts--><!--Legal Notes--><!--Press Release Date, City, and Company Name--></div></div></div>
  </body>
</html>
