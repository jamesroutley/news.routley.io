<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/francisrstokes/githublog/blob/main/2024%2F5%2F29%2Ffast-inverse-sqrt.md">Original</a>
    <h1>Everything I know about the fast inverse square root algorithm</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><section aria-labelledby="file-name-id-wide file-name-id-mobile"><div data-hpc="true"><article itemprop="text">
<p dir="auto">The <strong>fast inverse square root algorithm</strong>, made famous (though not invented) by programming legend John Carmack in the Quake 3 source code, computes an inverse square root <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{1}{\sqrt{x}}$</math-renderer> with a bewildering handful of lines that interpret and manipulate the raw bits of float. It&#39;s <em>wild</em>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="float Q_rsqrt(float number) {
  long i;
  float x2, y;
  const float threehalfs = 1.5F;

  x2 = number * 0.5F;
  y  = number;
  i  = *(long*)&amp;y;                            // evil floating point bit level hacking
  i  = 0x5f3759df - ( i &gt;&gt; 1 );               // what the fuck?
  y  = *(float*)&amp;i;
  y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration
  // y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed

  return y;
}"><pre><span>float</span> <span>Q_rsqrt</span>(<span>float</span> <span>number</span>) {
  <span>long</span> <span>i</span>;
  <span>float</span> <span>x2</span>, <span>y</span>;
  <span>const</span> <span>float</span> <span>threehalfs</span> <span>=</span> <span>1.5F</span>;

  <span>x2</span> <span>=</span> <span>number</span> <span>*</span> <span>0.5F</span>;
  <span>y</span>  <span>=</span> <span>number</span>;
  <span>i</span>  <span>=</span> <span>*</span>(<span>long</span><span>*</span>)<span>&amp;</span><span>y</span>;                            <span>// evil floating point bit level hacking</span>
  <span>i</span>  <span>=</span> <span>0x5f3759df</span> <span>-</span> ( <span>i</span> &gt;&gt; <span>1</span> );               <span>// what the fuck?</span>
  <span>y</span>  <span>=</span> <span>*</span>(<span>float</span><span>*</span>)<span>&amp;</span><span>i</span>;
  <span>y</span>  <span>=</span> <span>y</span> <span>*</span> ( <span>threehalfs</span> <span>-</span> ( <span>x2</span> <span>*</span> <span>y</span> <span>*</span> <span>y</span> ) );   <span>// 1st iteration</span>
  <span>// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed</span>

  <span>return</span> <span>y</span>;
}</pre></div>
<p dir="auto">In this article, we&#39;ll get into what&#39;s actually happening at the mathematical level in quite a bit of detail, and by the end, with a little persistence, you&#39;ll come away actually <em>understanding</em> how it works.</p>
<p dir="auto">I&#39;m not the first to write about this algorithm, and I surely won&#39;t be the last, but my aim is to show <em>every</em> step of the process. A lot of really fantastic resources out there still skip over steps in derivations, or fail to highlight out apparently obvious points. My goal is to remove any and all magic from this crazy algorithm.</p>
<p dir="auto">It&#39;s important to note that this algorithm is very much <em>of its time</em>. Back when Quake 3 was released in 1999, computing an inverse square root was a slow, expensive process. The game had to compute hundreds or thousands of them per second in order to solve lighting equations, and other 3D vector calculations that rely on normalization. These days, on modern hardware, not only would a calculation like this not take place on the CPU, even if it did, it would be fast due to much more advanced dedicated floating point hardware.</p>

<div dir="auto" data-snippet-clipboard-copy-content="float Q_rsqrt(float number) {
  long i;
  float x2, y;
  const float threehalfs = 1.5F;

  x2 = number * 0.5F;
  y  = number;
  i  = *(long*)&amp;y;                            // evil floating point bit level hacking
  i  = 0x5f3759df - ( i &gt;&gt; 1 );               // what the fuck?
  y  = *(float*)&amp;i;
  y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration
  // y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed

  return y;
}"><pre><span>float</span> <span>Q_rsqrt</span>(<span>float</span> <span>number</span>) {
  <span>long</span> <span>i</span>;
  <span>float</span> <span>x2</span>, <span>y</span>;
  <span>const</span> <span>float</span> <span>threehalfs</span> <span>=</span> <span>1.5F</span>;

  <span>x2</span> <span>=</span> <span>number</span> <span>*</span> <span>0.5F</span>;
  <span>y</span>  <span>=</span> <span>number</span>;
  <span>i</span>  <span>=</span> <span>*</span>(<span>long</span><span>*</span>)<span>&amp;</span><span>y</span>;                            <span>// evil floating point bit level hacking</span>
  <span>i</span>  <span>=</span> <span>0x5f3759df</span> <span>-</span> ( <span>i</span> &gt;&gt; <span>1</span> );               <span>// what the fuck?</span>
  <span>y</span>  <span>=</span> <span>*</span>(<span>float</span><span>*</span>)<span>&amp;</span><span>i</span>;
  <span>y</span>  <span>=</span> <span>y</span> <span>*</span> ( <span>threehalfs</span> <span>-</span> ( <span>x2</span> <span>*</span> <span>y</span> <span>*</span> <span>y</span> ) );   <span>// 1st iteration</span>
  <span>// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed</span>

  <span>return</span> <span>y</span>;
}</pre></div>
<p dir="auto">This is the code, more or less exactly as it appears in the quake 3 source - including the comments. Personally I think <em>&#34;evil floating point bit level hacking, what the fuck&#34;</em> is a fantastic explanation, but I do want to dig in quite a bit further.</p>
<p dir="auto">One of the key ideas behind this algorithm, and the reason this works, is because the raw bit pattern of a float, when interpreted as 32-bit signed integer, essentially approximates a scaled and shifted <code>log2(x)</code> function.</p>
<p dir="auto">Logarithms have a bunch of rules, properties, and identities that can be exploited to make computing the inverse square root easy for a computer, using only simple operations like adds and shifts (though there are some supporting floating point multiplications, which we&#39;ll talk about later).</p>
<p dir="auto">In order to make sense of what it even means to interpret the bit pattern of a float, we need to look at how floats are represented in memory, and how the &#34;value&#34; of a float is derived from that representation.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">32-Bit Floats: Representation</h2><a id="user-content-32-bit-floats-representation" aria-label="Permalink: 32-Bit Floats: Representation" href="#32-bit-floats-representation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">An IEEE-754 32-bit float can be regarded as a struct, which holds 3 members. Using C&#39;s bit-field notation here:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct float_raw {
  int32_t mantissa : 23;
  int32_t exponent : 8;
  int32_t sign     : 1;
}"><pre><span>struct</span> <span>float_raw</span> {
  <span>int32_t</span> <span>mantissa</span> : <span>23</span>;
  <span>int32_t</span> <span>exponent</span> : <span>8</span>;
  <span>int32_t</span> <span>sign</span>     : <span>1</span>;
}</pre></div>
<p dir="auto"><strong>Sign:</strong> 1 bit which indicates whether or not the number is positive or negative
<strong>Exponent</strong>: 8 bits which are used to dictate the range that this number will fall into
<strong>Mantissa</strong>: 23 bits which linearly specifies where exactly in the range this number lives</p>
<p dir="auto">The following equation shows how the actual numerical value <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$N$</math-renderer> is conceptually derived from the 3 integer parts, where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$S$</math-renderer> is the sign bit, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E$</math-renderer> is the exponent value, and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$M$</math-renderer> is the mantissa.</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
N = -1^S \times 2^{E-127} \times (1 + \frac{M}{2^{23}})
$$</math-renderer></p>
<p dir="auto">Or, if we break some of the variables out:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
m = \frac{M}{2^{23}}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
B = 127
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
e = E-B
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
N = -1^S \times 2^e \times (1 + m)
$$</math-renderer></p>
<p dir="auto">Note the little trick to get the sign bit from a 0 or a 1 into a -1 or a 1. Also notice that instead of simply multiplying by m, we multiply by 1+m. This ensures that when <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$m$</math-renderer> is 0, we get <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$2^e$</math-renderer>, and when <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$m$</math-renderer> is 1, we get <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$2^{e+1}$</math-renderer> (i.e. the full range).</p>
<p dir="auto">Let&#39;s take an example like the (approximate) number <code>-1.724</code>. It&#39;s underlying representation would look like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/normal.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/normal.png"/></a></p>
<p dir="auto">One interesting thing is that the exponent is actually stored in a biased format. The actual exponent value is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$e = E - 127$</math-renderer>. This allows two floating point numbers to be compared as if they were unsigned integers, which is a rather large benefit when it comes to building hardware accelerated floating point units.</p>
<p dir="auto">The next complexity is that an exponent <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E$</math-renderer> of all zeros has a special meaning. All the numbers in this range are known as &#34;sub-normals&#34;, and are represented by a slightly modified equation:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
N = -1^S \times 2^{-126} \times m
$$</math-renderer></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/subnormal.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/subnormal.png"/></a></p>
<p dir="auto">The exponent is set to -126. The mantissa value doesn&#39;t have an added value of one (in fact it&#39;s implicitly <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$0 + m$</math-renderer>), so the range actually represents 0 to just less than <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$2^{-126}$</math-renderer>. Without this, it would be impossible to represent 0 or the very small numbers around 0, which can cause underflow errors when calculations on small numbers result in one of these impossible values.</p>
<p dir="auto">When the exponent <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E$</math-renderer> is all ones, then the floating point value is one of the two other (quite famous) special types: <code>NaN</code> and <code>Infinity/-Infinity</code>. If <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E = 255$</math-renderer>, and <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$M = 0$</math-renderer>, then the number represents an infinity, with the sign bit signifying positive or negative.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/infinity.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/infinity.png"/></a></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$M \neq 0$</math-renderer>, then the value is <code>NaN</code> (not a number), which is used to signify when an illegal operation has taken place, like <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$0/0$</math-renderer>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/nan.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/nan.png"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">32-Bit Floats: Interpreting the bits</h2><a id="user-content-32-bit-floats-interpreting-the-bits" aria-label="Permalink: 32-Bit Floats: Interpreting the bits" href="#32-bit-floats-interpreting-the-bits"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Of course, typically this internal representation is completely irrelevant to the programmer; They can just perform calculations and get results. Indeed, William Kahan notes in his <a href="https://people.eecs.berkeley.edu/~wkahan/JAVAhurt.pdf" rel="nofollow">1998 presentation &#34;How Javaâ€™s Floating-Point Hurts Everyone Everywhere&#34;:</a></p>
<blockquote>
<p dir="auto">Error-analysis tells us how to design floating-point arithmetic, like IEEE Standard 754, moderately tolerant of well-meaning ignorance among programmers</p>
</blockquote>
<p dir="auto">The idea being that &#34;numerical sophistication&#34; is not necessary to make effective use of floating point.</p>
<p dir="auto">But that said, an intimate familiarity of the format can lead to some clever designs. We looked at how the integer parts of a float translate to a decimal number, but we can also talk about those same parts in terms of an integer representation in mathematical form:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
I_x = 2^{31}S + 2^{23}E + M
$$</math-renderer></p>
<p dir="auto">That <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$2^{23}$</math-renderer> term is quite important, so let&#39;s break it out into its own variable:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
L = 2^{23}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
I_x = 2^{31}S + LE + M
$$</math-renderer></p>
<p dir="auto">And since we&#39;re mainly going to be talking about taking square roots of numbers, we can assume the sign is positive (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$S = 0$</math-renderer>), and use the simpler form:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
I_x = LE + M
$$</math-renderer></p>
<p dir="auto">If you take a closer look at the raw bits of a float, you can make some interesting observations.</p>
<p dir="auto">The first is that every <em>range</em> - that is the set of numbers which can be represented by any given exponent value - has approximately 50% less precision than the range before it.</p>
<p dir="auto">For example, take the exponent value <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E = 127, e = E - B = 0$</math-renderer>, which represents the range of representable numbers <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\pm[1, 2)$</math-renderer>. There are 8388607 (<code>(1 &lt;&lt; 23) - 1</code>) distinct steps from 1 to just below 2.</p>
<p dir="auto">Contrast that with exponent value <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$E = 128, e = E - B = 1$</math-renderer>, which represents the range of representable numbers <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\pm[2, 4)$</math-renderer>. It has the same 8388607 distinct steps, but it has to cover twice the distance on the number line.</p>

<p dir="auto">This relationship is <em>logarithmic</em>. If you take a series of evenly-spaced floating point numbers - say 256 of them - starting at 0, increasing by 0.25 each time, and interpret the bit pattern as an integer, you get the following graph:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/floats-as-ints.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/floats-as-ints.png"/></a></p>
<p dir="auto">Now if we plot the result of taking <code>log2(x)</code> of those same 256 float values, we get this curve.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/log2.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/log2.png"/></a></p>
<p dir="auto">Obviously the actual values on the graphs are wildly different, and the first one is much more <em>steppy</em>, but it&#39;s clear that the first is a close approximation of the second.</p>
<p dir="auto">The first graph is what you might call a <em>piecewise linear approximation</em>, which has been scaled and shifted by a specific amount. Perhaps unsurprisingly, the amount it&#39;s scaled and shifted by is related to the structure of a float!</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\log_2(x) \approx \frac{I_x}{L} - B
$$</math-renderer></p>
<p dir="auto">Here, <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$I_x$</math-renderer> is the raw bit pattern of a float in integer form. That is divided by by size of the mantissa, and the bias exponent is subtracted away. If we plot this directly against <code>log2(x)</code>, we get:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/francisrstokes/githublog/blob/main/assets/fast-inverse-sqrt/log2-vs-ints.png"><img src="https://github.com/francisrstokes/githublog/raw/main/assets/fast-inverse-sqrt/log2-vs-ints.png"/></a></p>
<p dir="auto">Again, not a perfect mapping, but a pretty good approximation! We can also sub in the floating point terms, assuming a positive sign bit and a <em>normal</em> number:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
I_x = log_2(1 + m_x) + B \times 2^{e_x}
$$</math-renderer></p>
<p dir="auto">Forgetting the integer representation for just a second, the log of a floating point number alone would be expressed as:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
log_2(1 + m_x) + e_x
$$</math-renderer></p>
<p dir="auto">Since we already know that the integer conversion is a <em>linear approximation</em>, we can make this approximate equivalence:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
log_2(1 + x) \approx x + \sigma
$$</math-renderer></p>
<p dir="auto">The sigma (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\sigma$</math-renderer>) term is essentially a fine adjustment parameter that can improve the approximation. To make it really concrete, the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$x$</math-renderer> term here will always be a number in the range <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$[0,1]$</math-renderer>, and represents a position in the exponent range <em>linearly</em>.</p>
<p dir="auto">With all of that in mind, we can focus our attention back on the thing we&#39;re (now indirectly) attempting to compute: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{1}{\sqrt{x}}$</math-renderer>.</p>
<p dir="auto">When work with the raw bits of a float, we are essentially operating on a logarithm of that value. Logarithms have been carefully studied for a long time, and they have many known properties and identities. For example:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\log_2(x^y) = y \times \log_2(x)
$$</math-renderer></p>
<p dir="auto">We can also note that:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\sqrt{x} = x^{0.5}
$$</math-renderer></p>
<p dir="auto">Since we&#39;re asking for an answer to the question:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y = \frac{1}{\sqrt{x}}
$$</math-renderer></p>
<p dir="auto">which we can reformulate as</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y = \frac{1}{x^{0.5}}
$$</math-renderer></p>
<p dir="auto">and even simpler:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y = x^{-0.5}
$$</math-renderer></p>
<p dir="auto">We can take at our log formulas from before, and state that:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\log_2(\frac{1}{\sqrt{x}}) = \log_2(x^{-0.5}) = -0.5 \times \log_2(x)
$$</math-renderer></p>
<p dir="auto">Plugging in the floating point values now:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\log_2(1 + m_y) + e_y \approx -0.5 \times \log_2(1 + m_x) + e_x
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
m_y + \sigma + e_y \approx -0.5 \times (m_x + \sigma + e_x)
$$</math-renderer></p>
<p dir="auto">And then getting the floating point constants back into their integer component form:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + \sigma + E_y - B \approx -0.5 \times (\frac{M_x}{L} + \sigma + E_x - B)
$$</math-renderer></p>
<p dir="auto">We can do some algebra on this expression to turn it into one that where both sides have something that looks a raw floating point bit pattern (integer) on both sides (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$LE + M$</math-renderer>). I&#39;m leaving every step in for clarity, though it&#39;s the last line here which is important:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + \sigma + E_y \approx -0.5 \times (\frac{M_x}{L} + \sigma + E_x - B) + B
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + E_y \approx -0.5 \times (\frac{M_x}{L} + \sigma + E_x - B) + B - \sigma
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + E_y \approx -\frac{1}{2}(\frac{M_x}{L} + \sigma + E_x - B) + B - \sigma
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + E_y \approx -\frac{1}{2}(\frac{M_x}{L} + E_x - B) + B - \frac{3}{2}\sigma
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{M_y}{L} + E_y \approx -\frac{1}{2}(\frac{M_x}{L} + E_x) - \frac{3}{2}(\sigma - B)
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
L(\frac{M_y}{L} + E_y) \approx L(-\frac{1}{2}(\frac{M_x}{L} + E_x) - \frac{3}{2}(\sigma - B))
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
L(\frac{M_y}{L} + E_y) \approx -\frac{1}{2}L(\frac{M_x}{L} + E_x) - \frac{3}{2}L(\sigma - B))
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
LE_y + M_y \approx -\frac{1}{2}(LE_x + M_x) - \frac{3}{2}L(\sigma - B))
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
LE_y + M_y \approx -\frac{1}{2}(LE_x + M_x) + \frac{3}{2}L(B - \sigma)
$$</math-renderer></p>
<p dir="auto">That is quite a mouthful - although all of the operations performed here are simple enough. With all the variable swapping done, and both sides containing groups that include proper, honest-to-goodness integer part floating point representations, we can group them back up and get:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
I_y \approx -\frac{1}{2}I_x + \frac{3}{2}L(B - \sigma)
$$</math-renderer></p>
<p dir="auto">This is quite a significant moment. On the left hand side, we&#39;ve got the <em>value</em> <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\log_2(\frac{1}{\sqrt{x}})$</math-renderer>, and on the right, we&#39;ve got a simple operation on the integer interpretation of the floating point input (multiplying by negative one half), plus a constant term, made up of constants related to floating point representation (as well as the sigma tuning parameter). <em>This is the famous line</em>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="i  = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck?"><pre><span>i</span>  <span>=</span> <span>0x5f3759df</span> <span>-</span> ( <span>i</span> &gt;&gt; <span>1</span> ); <span>// what the fuck?</span></pre></div>
<p dir="auto">A bit shift to the right multiplies by <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{1}{2}$</math-renderer>, which is subtracted from the constant  <code>0x5f3759df</code>. That hex constant is the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{3}{2}L(B - \sigma)$</math-renderer> term, but where exactly does <code>0x5f3759df</code> come from? Assuming a sigma value <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\sigma = 0$</math-renderer>, we can compute:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{3}{2}L(B - \sigma) = \frac{3}{2}2^{23} \times 127 = 1598029824
$$</math-renderer></p>
<p dir="auto"><code>1598029824</code> in hexadecimal is <code>0x5f400000</code>, which, as you&#39;ll note, is close to, but <em>not quite</em> the magic constant from Quake. It&#39;s off by <code>566817</code>, and we can use this information to compute the actual sigma value used in the game:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{3}{2}2^{23} \times 127 - \frac{3}{2}2^{23}(127 - \sigma) = 566817
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{3}{2}(2^{23} \times 127 - 2^{23}\times127 - 2^{23}(- \sigma)) = 566817
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{3}{2}(-2^{23}(- \sigma)) = 566817
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
-2^{23}(- \sigma) = \frac{566817}{1.5}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
2^{23}\sigma = 377878
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\sigma = \frac{377878}{2^{23}}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\sigma = 0.04504656
$$</math-renderer></p>
<p dir="auto">That sigma value was chosen by someone to optimise the approximation, but interestingly, it isn&#39;t actually the <em>optimal</em> value (more on that later), <em>and</em> it isn&#39;t actually known who came up with it! I&#39;ve left all the math in so as to remove any possibility of this being a &#34;magic&#34; constant; It&#39;s really anything but! In C:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int32_t compute_magic(void) {
  double sigma = 0.0450465;
  double expression = 1.5 * pow(2.0, 23.0) * (127.0 - sigma);
  int32_t i = expression;
  return i;
}

// -&gt; 0x5f3759df"><pre><span>int32_t</span> <span>compute_magic</span>(<span>void</span>) {
  <span>double</span> <span>sigma</span> <span>=</span> <span>0.0450465</span>;
  <span>double</span> <span>expression</span> <span>=</span> <span>1.5</span> <span>*</span> <span>pow</span>(<span>2.0</span>, <span>23.0</span>) <span>*</span> (<span>127.0</span> <span>-</span> <span>sigma</span>);
  <span>int32_t</span> <span>i</span> <span>=</span> <span>expression</span>;
  <span>return</span> <span>i</span>;
}

<span>// -&gt; 0x5f3759df</span></pre></div>
<p dir="auto">Note that doubles are used here not floats, and that the integer form is just a plain old cast, not an interpretation of the bit pattern.</p>
<div dir="auto" data-snippet-clipboard-copy-content="i  = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck?"><pre><span>i</span>  <span>=</span> <span>0x5f3759df</span> <span>-</span> ( <span>i</span> &gt;&gt; <span>1</span> ); <span>// what the fuck?</span></pre></div>
<p dir="auto">That single line computes an inverse square root approximation on a floating point number by realising that the raw bit pattern is an approximate log, and then exploiting identities and algebra, as well as extremely fast operations like shifting and addition.</p>
<p dir="auto">I&#39;ve often heard this algorithm referred to as a &#34;hack&#34;. Now, I&#39;m not one to put down a hacky solution, but a hack this is not. This is absolutely a solid, wel thought-out piece of engineering, employed to compute an expensive operation thousands of times per second on the under-powered hardware of the day.</p>
<p dir="auto">I&#39;ll make a quick note here that this algorithm will <em>only</em> work with &#34;normal&#34; floating point numbers. A &#34;sub-normal&#34; (that is, a tiny number <em>very</em> close to zero) will fall apart, because the log approximation assumes <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$log_2(1 + x) = x + \sigma$</math-renderer>, but what we&#39;d actually be plugging in would be <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$0 + x$</math-renderer>.</p>

<p dir="auto">The approximation described above is pretty good, but definitely contains measurable error. That&#39;s where the next line comes in.</p>
<div dir="auto" data-snippet-clipboard-copy-content="y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration"><pre><span>y</span> <span>=</span> <span>y</span> <span>*</span> ( <span>threehalfs</span> <span>-</span> ( <span>x2</span> <span>*</span> <span>y</span> <span>*</span> <span>y</span> ) ); <span>// 1st iteration</span></pre></div>
<p dir="auto">This line improves the approximation by a significant margin, by utilising an algorithm called Newtons method, or the <a href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="nofollow">Newton-Raphson method</a> . This is a generic, iterative mathematical technique for finding the roots (zeros) of function. You might wonder how that could be helpful here, since we aren&#39;t looking for a zero. Well, we already have our approximation <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$y$</math-renderer>, and we can create a new expression:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
f(y) = \frac{1}{y^2} - x = 0
$$</math-renderer></p>
<p dir="auto">Squaring the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$y$</math-renderer> term - which, remember, is <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{1}{\sqrt{x}}$</math-renderer> - gives us <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$\frac{1}{x} - x$</math-renderer>. Inverting that gives us <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$x - x$</math-renderer>, which is of course just 0. This expression is in a form that we can use for Newtons method.</p>
<p dir="auto">Newtons method works like this: Given an initial approximation <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$y_n$</math-renderer>, we can create a better approximation <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$y_{n+1}$</math-renderer> like this:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{f(y_n)}{f&#39;(y_n)}
$$</math-renderer></p>
<p dir="auto">Where <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$f&#39;(y)$</math-renderer> is the <a href="https://en.wikipedia.org/wiki/Derivative" rel="nofollow"><em>derivative</em></a> of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$f(y)$</math-renderer>. When we take the derivative of a function for a given input, we&#39;re determining the slope or gradient of function for that input. In other words, it&#39;s the rate of change. The improved approximation works by taking our current approximation (which we know is not yet correct), and nudging it along the slope towards the correct answer. It&#39;s kind of mind-boggling that this works, but there you go! I should note that this particular algorithm does not work for all circumstances, but it is a very powerful tool to throw at these kinds of problems!</p>
<p dir="auto">So what is the derivative of our <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$f(y)$</math-renderer> function? First let&#39;s rearrange the function a little:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
f(y) = y^{-2} - x
$$</math-renderer></p>
<p dir="auto">Taking the derivative for a function in this form works like:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{d}{dx}(x^n + c) = nx^{n-1}
$$</math-renderer></p>
<p dir="auto">So we get:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{d}{dy} f(y) = -2y^{-3}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{d}{dy} f(y) = -2y^{-3}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
\frac{d}{dy} f(y) = -\frac{2}{y^3}
$$</math-renderer></p>
<p dir="auto">That gives us this expression for the better approximation:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{\frac{1}{y_n^2} - x}{-\frac{2}{y_n^3}}
$$</math-renderer></p>
<p dir="auto">There is a problem with this form, however. A very large part of why this algorithm is fast is because it avoids floating point divisions, and the above equation has 3 of them! Fortunately, our very good friend algebra has our back again, and we can rearrange the expression into this form:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n(1.5 - 0.5x \cdot y_n^2)
$$</math-renderer></p>
<p dir="auto">No divisions, only multiplications! The exact steps to go from the first form to this one are <em>numerous</em> to say the least, but I&#39;ve included it in full for completeness. Feel free to skip over it and pick back up on the code below.</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{\frac{1 - xy_n^2}{y_n^2}}{-\frac{2}{y_n^3}}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{1 - xy_n^2}{y_n^2} \cdot -\frac{y_n^3}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{-1 - xy_n^2}{y_n^2} \cdot \frac{y_n^3}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - \frac{-1 - xy_n^2}{\cancel{y_n^2}} \cdot \frac{\cancel{y_n^2}y_n}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-1 - xy_n^2 \cdot \frac{y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-(1-xy_n^2) \cdot \frac{y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-1\cdot(1-xy_n^2) \cdot \frac{y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-1\cdot1 + -1\cdot(-xy_n^2) \cdot \frac{y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-1\frac{y_n}{2} + xy_n^2\frac{y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (-\frac{y_n}{2} + \frac{xy_n^2y_n}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (\frac{-y_n+xy_n^3}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (\frac{y_n \cdot -1 +xy_n^3)}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (\frac{y_n \cdot -1 + y_n(xy_n^2)}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n - (\frac{y_n(-1 + xy_n^2)}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n \cdot \frac{2}{2} - \frac{y_n(-1 + xy_n^2)}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} =  \frac{2y_n}{2} - \frac{y_n(-1 + xy_n^2)}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = \frac{2y_n - y_n(-1 + xy_n^2)}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = \frac{2y_n + y_n(-1(-1 + xy_n^2))}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = \frac{y_n(2 -1(-1 + xy_n^2))}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = \frac{y_n(2 + 1 -xy_n^2)}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = \frac{y_n(3 -xy_n^2)}{2}
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n(\frac{3}{2} - \frac{xy_n^2}{2})
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n(\frac{3}{2} - \frac{x}{2} y_n^2)
$$</math-renderer></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="1c36eeff0c5ea4fca34e867318c3811d">$$
y_{n+1} = y_n \cdot (1.5 - (0.5x \cdot y_n \cdot y_n))
$$</math-renderer></p>
<p dir="auto">So that is the last line of the function before the return:</p>
<div dir="auto" data-snippet-clipboard-copy-content="y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration"><pre><span>y</span> <span>=</span> <span>y</span> <span>*</span> ( <span>threehalfs</span> <span>-</span> ( <span>x2</span> <span>*</span> <span>y</span> <span>*</span> <span>y</span> ) ); <span>// 1st iteration</span></pre></div>
<p dir="auto">Amazingly, that ends up yielding a maximum absolute error of 0.175% (and often has a far lower error). Normally, Newtons method is applied iteratively to obtain closer and closer approximations, but in the case of the Quake code, only a single iteration was used. In the original source, a second iteration is present, but is commented out.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed"><pre><span>// y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed</span></pre></div>

<p dir="auto">This algorithm is outright astonishing. It builds on a deep knowledge of the internal mathematical details of the floating point number system, understanding what runs fast and slow on a computer, some nimble algebraic gymnastics, and a centuries old root finding method discovered by none other than Issac Newton himself, and solves a problem that was a computational bottleneck at a particular period of history.</p>
<p dir="auto">I mentioned that Carmack did not actually come up with this himself (though I wouldn&#39;t put it past him!). <a href="https://www.beyond3d.com/content/articles/8/" rel="nofollow">The truth is the exact origin is not 100% certain</a>. There&#39;s something kind of incredible about that, too.</p>
<p dir="auto">And believe it or not, this rabbit hole actually goes even deeper. Mathematician <a href="http://www.lomont.org/papers/2003/InvSqrt.pdf" rel="nofollow">Chris Lomont wrote up a paper</a> trying to find the optimal value for sigma in the log approximation step. It&#39;s definitely worth a look if this hasn&#39;t fully satisfied your curiosity about the subject.</p>
<p dir="auto">Lastly, <a href="https://github.com/francisrstokes/githublog/blob/main/2024/5/10/cordic.md">I recently wrote about CORDIC</a>, an algorithm for computing sines and cosines without floats, using only addition and bit shifting. Some folks had asked in the comments about its similarity to the fast inverse square root algorithm. I replied that it wasn&#39;t that similar, <em>really</em> - being all about floating point, bit level interpretations, and root-finding.</p>
<p dir="auto">But then I stopped to actually think about it, and while there are large differences in the details of the algorithm, there is a lot of <em>spirit</em> in common. Specifically, making clever mathematical observations, and bringing those to bear efficiently on the hardware constraints of the time.</p>
<p dir="auto">Some people look at algorithms like CORDIC and fast inverse square root, and think them only relics of the past; A technology with no utility in the modern world. I don&#39;t think I have to tell you that I disagree with that premise.</p>
<p dir="auto">A lot of us get into this field because, as kids, we loved to crack things open and see how they worked (even if, sometimes, we couldn&#39;t put them back together afterwards). Algorithms such as these live in that same space for me. I&#39;ve tried to keep that curious spark alive, and turn it on problems and technology that aren&#39;t immediately relevant to my everyday work. And the really crazy thing is that often the underlying elements <em>do</em> help me solve real problems! Knowledge is synthesisable, who would have thought.</p>
</article></div></section></div></div></div></div>
  </body>
</html>
