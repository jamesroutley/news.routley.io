<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pinecone.io/learn/langchain-retrieval-augmentation/">Original</a>
    <h1>Fixing Hallucination with Knowledge Bases</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels (LLMs) have a data freshness problem. Even some of the most powerful models, like GPT-4, have <em>no idea</em> about recent events.</p><p>The world, according to LLMs, is frozen in time. They only know the world as it appeared through their training data.</p><p>That creates problems for <em>any</em> use case that relies on up-to-date information or a particular dataset. For example, you may have internal company documents you’d like to interact with via an LLM.</p><p>The first challenge is adding those documents to the LLM, we could try training the LLM on these documents, but this is time-consuming and expensive. And what happens when a new document is added? Training for every new document added is beyond inefficient — it is simply impossible.</p><p>So, how do we handle this problem? We can use <em>retrieval augmentation</em>. This technique allows us to retrieve relevant information from an external knowledge base and give that information to our LLM.</p><p>The external knowledge base is our <em>“window”</em> into the world beyond the LLM’s training data. In this chapter, we will learn all about implementing retrieval augmentation for LLMs using LangChain.</p><p><iframe src="https://www.youtube-nocookie.com/embed/kvdVduIJsc8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p><hr/><h2 id="creating-the-knowledge-base">Creating the Knowledge Base</h2><p>We have two primary types of knowledge for LLMs. The <em>parametric knowledge</em> refers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM.</p><p>The second type of knowledge is <em>source knowledge</em>. This knowledge covers any information fed into the LLM via the input prompt. When we talk about <em>retrieval augmentation</em>, we’re talking about giving the LLM valuable source knowledge.</p><p><em>(You can follow along with the following sections using the <a href="https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb">Jupyter notebook here</a>!)</em></p><h3 id="getting-data-for-our-knowledge-base">Getting Data for our Knowledge Base</h3><p>To help our LLM, we need to give it access to relevant source knowledge. To do that, we need to create our knowledge base.</p><p>We start with a dataset. The dataset used naturally depends on the use case. It could be code documentation for an LLM that needs to help write code, company documents for an internal chatbot, or anything else.</p><p>In our example, we will use a subset of Wikipedia. To get that data, we will use Hugging Face datasets like so:</p><div><div><p>In[2]:</p><pre><code>from datasets import load_dataset

data = load_dataset(&#34;wikipedia&#34;, &#34;20220301.simple&#34;, split=&#39;train[:10000]&#39;)
data</code></pre></div><div><p>Out[2]:</p><div><pre><code>Downloading readme:   0%|          | 0.00/16.3k [00:00&lt;?, ?B/s]</code></pre><pre><code>Dataset({
</code></pre><pre><code>    features: [&#39;id&#39;, &#39;url&#39;, &#39;title&#39;, &#39;text&#39;],
</code></pre><pre><code>    num_rows: 10000
</code></pre><pre><code>})</code></pre></div></div><div><p>In[3]:</p><pre><code>data[6]</code></pre></div><div><p>Out[3]:</p><div><pre><code>{&#39;id&#39;: &#39;13&#39;,
</code></pre><pre><code> &#39;url&#39;: &#39;https://simple.wikipedia.org/wiki/Alan%20Turing&#39;,
</code></pre><pre><code> &#39;title&#39;: &#39;Alan Turing&#39;,
</code></pre><pre><code> &#39;text&#39;: &#39;Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\n\nEarly life and family \nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\n\nEducation \nTuring went to St. Michael\&#39;s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\n&#34;This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\n\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\n\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\n\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\n\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\n\nCareer \nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called &#34;intelligent&#34;. A computer could be said to &#34;think&#34; if a human talking with it could not tell it was a machine.\n\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&amp;CS) at Bletchley Park, Britain\&#39;s codebreaking centre that produced Ultra intelligence.\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\n\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was &#34;the first detailed design of a stored-program computer&#34;. Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\xa0May 1950.\n\nPrivate life \nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\n\nIn May 2012, a private member\&#39;s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\n\nDeath \nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\n\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\&#39;s treatment &#34;appalling&#34;.\n\nReferences\n\nOther websites \nJack Copeland 2012. Alan Turing: The codebreaker who saved \&#39;millions of lives\&#39;. BBC News / Technology \n\nEnglish computer scientists\nEnglish LGBT people\nEnglish mathematicians\nGay men\nLGBT scientists\nScientists from London\nSuicides by poison\nSuicides in the United Kingdom\n1912 births\n1954 deaths\nOfficers of the Order of the British Empire&#39;}</code></pre></div></div></div><p>Most datasets will contain records that include <em>a lot</em> of text. Because of this, our first task is usually to build a preprocessing pipeline that chunks those long bits of text into more <em>concise</em> chunks.</p><h3 id="creating-chunks">Creating Chunks</h3><p>Splitting our text into smaller chunks is essential for several reasons. Primarily we are looking to:</p><ul><li>Improve <em>“embedding accuracy”</em> — this will improve the relevance of results later.</li><li>Reduce the amount of text fed into our LLM as source knowledge. Limiting input improves the LLM’s ability to follow instructions, reduces generation costs, and helps us get faster responses.</li><li>Provide users with more precise information sources as we can narrow down the information source to a smaller chunk of text.</li><li>In the case of <em>very long</em> chunks of text, we will exceed the maximum context window of our embedding or completion models. Splitting these chunks makes it possible to add these longer documents to our knowledge base.</li></ul><p>To create these chunks, we first need a way of measuring the length of our text. LLMs don’t measure text by word or character — they measure it by <em>“tokens”</em>.</p><p>A token is typically the size of a word or sub-word and varies by LLM. The tokens themselves are built using a <em>tokenizer</em>. We will be using <code>gpt-3.5-turbo</code> as our completion model, and we can initialize the tokenizer for this model like so:</p><div><pre><code data-lang="python"><span>import</span> tiktoken  <span># !pip install tiktoken</span>

tokenizer <span>=</span> tiktoken<span>.</span>get_encoding(<span>&#39;p50k_base&#39;</span>)
</code></pre></div><p>Using the tokenizer, we can create tokens from plain text and count the number of tokens. We will wrap this into a function called <code>tiktoken_len</code>:</p><div><div><p>In[28]:</p><pre><code># create the length function
def tiktoken_len(text):
    tokens = tokenizer.encode(
        text,
        disallowed_special=()
    )
    return len(tokens)

tiktoken_len(&#34;hello I am a chunk of text and using the tiktoken_len function &#34;
             &#34;we can find the length of this chunk of text in tokens&#34;)</code></pre></div><div><p>Out[28]:</p><div><pre><code>28</code></pre></div></div></div><p>With our token counting function ready, we can initialize a LangChain <code>RecursiveCharacterTextSplitter</code> object. This object will allow us to split our text into chunks no longer than what we specify via the <code>chunk_size</code> parameter.</p><div><pre><code data-lang="python"><span>from</span> langchain.text_splitter <span>import</span> RecursiveCharacterTextSplitter

text_splitter <span>=</span> RecursiveCharacterTextSplitter(
    chunk_size<span>=</span><span>400</span>,
    chunk_overlap<span>=</span><span>20</span>,
    length_function<span>=</span>tiktoken_len,
    separators<span>=</span>[<span>&#34;</span><span>\n\n</span><span>&#34;</span>, <span>&#34;</span><span>\n</span><span>&#34;</span>, <span>&#34; &#34;</span>, <span>&#34;&#34;</span>]
)
</code></pre></div><p>Now we split the text like so:</p><div><div><p>In[6]:</p><pre><code>chunks = text_splitter.split_text(data[6][&#39;text&#39;])[:3]
chunks</code></pre></div><div><p>Out[6]:</p><div><pre><code>[&#39;Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\n\nEarly life and family \nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\n\nEducation \nTuring went to St. Michael\&#39;s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\n&#34;This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\n\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\n\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\n\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.&#39;,
</code></pre><pre><code> &#39;In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\n\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\n\nCareer \nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\n\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called &#34;intelligent&#34;. A computer could be said to &#34;think&#34; if a human talking with it could not tell it was a machine.\n\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&amp;CS) at Bletchley Park, Britain\&#39;s codebreaking centre that produced Ultra intelligence.\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.&#39;,
</code></pre><pre><code> &#39;From 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was &#34;the first detailed design of a stored-program computer&#34;. Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\xa0May 1950.\n\nPrivate life \nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\n\nIn May 2012, a private member\&#39;s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\n\nDeath \nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\n\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\&#39;s treatment &#34;appalling&#34;.\n\nReferences\n\nOther websites \nJack Copeland 2012. Alan Turing: The codebreaker who saved \&#39;millions of lives\&#39;. BBC News / Technology&#39;]</code></pre></div></div></div><p>None of these chunks are larger than the <code>400</code> chunk size limit we set earlier:</p><div><div><p>In[7]:</p><pre><code>tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])</code></pre></div><div><p>Out[7]:</p><div><pre><code>(397, 304, 399)</code></pre></div></div></div><p>With the <code>text_splitter</code>, we get nicely-sized chunks of text. We’ll use this functionality during the indexing process later. For now, let’s take a look at <em>embeddings</em>.</p><h3 id="creating-embeddings">Creating Embeddings</h3><p>The vector embeddings are vital to retrieving relevant context for our LLM. We take the chunks of text we’d like to store in our knowledge base and encode each chunk into a vector embedding.</p><p>These embeddings can act as “numerical representations” of the meaning of each chunk of text.
This is possible because we create the embeddings with another AI language model that has learned to translate human-readable text into AI-readable embeddings.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/cd74e72b53f29b372f75895b9153e3a56f5fea25/3c7bf/images/langchain-retrieval-augmentation-2.png" alt="Encoder translating plain text to embeddings" width="100%"/></p><p>We then store these embeddings in our vector database (more on this soon) and can find text chunks with similar meanings by calculating the distance between embeddings in vector space.</p><p><img loading="lazy" src="https://d33wubrfki0l68.cloudfront.net/a0bc9b1a9ab2e0ff22afb1d33ce1d9875bd0c64a/9af64/images/langchain-retrieval-augmentation-1.png" alt="Vector space with similar sentences in similar areas" width="100%"/></p><p>The embedding model we will use is another OpenAI model called <code>text-embedding-ada-002</code>. We can initialize it via LangChain like so:</p><div><pre><code data-lang="python"><span>from</span> langchain.embeddings.openai <span>import</span> OpenAIEmbeddings

model_name <span>=</span> <span>&#39;text-embedding-ada-002&#39;</span>

embed <span>=</span> OpenAIEmbeddings(
    document_model_name<span>=</span>model_name,
    query_model_name<span>=</span>model_name,
    openai_api_key<span>=</span>OPENAI_API_KEY
)
</code></pre></div><p>Now we can embed our text:</p><div><div><p>In[10]:</p><pre><code>texts = [
    &#39;this is the first chunk of text&#39;,
    &#39;then another second chunk of text is here&#39;
]

res = embed.embed_documents(texts)
len(res), len(res[0])</code></pre></div><div><p>Out[10]:</p><div><pre><code>(2, 1536)</code></pre></div></div></div><p>From this, we get <em>two</em> embeddings as we passed in two chunks of text. Each embedding is a <em>1536-dimensional</em> vector. This dimension is simply the output dimensionality of <code>text-embedding-ada-002</code>.</p><p>With that, we have our dataset, text splitter, and embedding model. We have everything needed to begin constructing our knowledge base.</p><h3 id="vector-database">Vector Database</h3><p>A vector database is a type of knowledge base that allows us to scale the search of similar embeddings to billions of records, manage our knowledge base by adding, updating, or removing records, and even do things like filtering.</p><p>We will be using the Pinecone vector database. To use it, we need a <a href="https://app.pinecone.io/">free API key</a>. Then we initialize our database index like so:</p><div><pre><code data-lang="python"><span>import</span> pinecone

index_name <span>=</span> <span>&#39;langchain-retrieval-augmentation&#39;</span>

pinecone<span>.</span>init(
        api_key<span>=</span><span>&#34;YOUR_API_KEY&#34;</span>,  <span># find api key in console at app.pinecone.io</span>
        environment<span>=</span><span>&#34;YOUR_ENV&#34;</span>  <span># find next to api key in console</span>
)

<span># we create a new index</span>
pinecone<span>.</span>create_index(
        name<span>=</span>index_name,
        metric<span>=</span><span>&#39;dotproduct&#39;</span>,
        dimension<span>=</span>len(res[<span>0</span>]) <span># 1536 dim of text-embedding-ada-002</span>
)
</code></pre></div><p>Then we connect to the new index:</p><div><div><p>In[12]:</p><pre><code>index = pinecone.GRPCIndex(index_name)

index.describe_index_stats()</code></pre></div><div><p>Out[12]:</p><div><pre><code>{&#39;dimension&#39;: 1536,
</code></pre><pre><code> &#39;index_fullness&#39;: 0.0,
</code></pre><pre><code> &#39;namespaces&#39;: {},
</code></pre><pre><code> &#39;total_vector_count&#39;: 0}</code></pre></div></div></div><p>We will see that the new Pinecone index has a <code>total_vector_count</code> of <code>0</code> because we haven’t added any vectors yet. Our next task is to do that.</p><p>The indexing process consists of us iterating through the data we’d like to add to our knowledge base, creating IDs, embeddings, and metadata — then adding these to the index.</p><p>We can do this in batches to speed up the process.</p><div><pre><code data-lang="python"><span>from</span> tqdm.auto <span>import</span> tqdm
<span>from</span> uuid <span>import</span> uuid4

batch_limit <span>=</span> <span>100</span>

texts <span>=</span> []
metadatas <span>=</span> []

<span>for</span> i, record <span>in</span> enumerate(tqdm(data)):
    <span># first get metadata fields for this record</span>
    metadata <span>=</span> {
        <span>&#39;wiki-id&#39;</span>: str(record[<span>&#39;id&#39;</span>]),
        <span>&#39;source&#39;</span>: record[<span>&#39;url&#39;</span>],
        <span>&#39;title&#39;</span>: record[<span>&#39;title&#39;</span>]
    }
    <span># now we create chunks from the record text</span>
    record_texts <span>=</span> text_splitter<span>.</span>split_text(record[<span>&#39;text&#39;</span>])
    <span># create individual metadata dicts for each chunk</span>
    record_metadatas <span>=</span> [{
        <span>&#34;chunk&#34;</span>: j, <span>&#34;text&#34;</span>: text, <span>**</span>metadata
    } <span>for</span> j, text <span>in</span> enumerate(record_texts)]
    <span># append these to current batches</span>
    texts<span>.</span>extend(record_texts)
    metadatas<span>.</span>extend(record_metadatas)
    <span># if we have reached the batch_limit we can add texts</span>
    <span>if</span> len(texts) <span>&gt;=</span> batch_limit:
        ids <span>=</span> [str(uuid4()) <span>for</span> _ <span>in</span> range(len(texts))]
        embeds <span>=</span> embed<span>.</span>embed_documents(texts)
        index<span>.</span>upsert(vectors<span>=</span>zip(ids, embeds, metadatas))
        texts <span>=</span> []
        metadatas <span>=</span> []
</code></pre></div><p>We’ve now indexed everything. To check the number of records in our index, we call <code>describe_index_stats</code> again:</p><div><div><p>In[14]:</p><pre><code>index.describe_index_stats()</code></pre></div><div><p>Out[14]:</p><div><pre><code>{&#39;dimension&#39;: 1536,
</code></pre><pre><code> &#39;index_fullness&#39;: 0.1,
</code></pre><pre><code> &#39;namespaces&#39;: {&#39;&#39;: {&#39;vector_count&#39;: 27437}},
</code></pre><pre><code> &#39;total_vector_count&#39;: 27437}</code></pre></div></div></div><p>Our index contains ~27K records. As mentioned earlier, we can scale this up to billions, but 27K is enough for our example.</p><h2 id="langchain-vector-store-and-querying">LangChain Vector Store and Querying</h2><p>We construct our index independently of LangChain. That’s because it’s a straightforward process, and it is faster to do this with the Pinecone client directly. However, we’re about to jump back into LangChain, so we should reconnect to our index via the LangChain library.</p><div><pre><code data-lang="python"><span>from</span> langchain.vectorstores <span>import</span> Pinecone

text_field <span>=</span> <span>&#34;text&#34;</span>

<span># switch back to normal index for langchain</span>
index <span>=</span> pinecone<span>.</span>Index(index_name)

vectorstore <span>=</span> Pinecone(
    index, embed<span>.</span>embed_query, text_field
)
</code></pre></div><p>We can use the <code>similarity search</code> method to make a query directly and return the chunks of text without any LLM generating the response.</p><div><div><p>In[16]:</p><pre><code>query = &#34;who was Benito Mussolini?&#34;

vectorstore.similarity_search(
    query,  # our search query
    k=3  # return 3 most relevant docs
)</code></pre></div><div><p>Out[16]:</p><div><pre><code>[Document(page_content=&#39;Benito Amilcare Andrea Mussolini KSMOM GCTE (29 July 1883 – 28 April 1945) was an Italian politician and journalist. He was also the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party.\n\nBiography\n\nEarly life\nBenito Mussolini was named after Benito Juarez, a Mexican opponent of the political power of the Roman Catholic Church, by his anticlerical (a person who opposes the political interference of the Roman Catholic Church in secular affairs) father. Mussolini\&#39;s father was a blacksmith. Before being involved in politics, Mussolini was a newspaper editor (where he learned all his propaganda skills) and elementary school teacher.\n\nAt first, Mussolini was a socialist, but when he wanted Italy to join the First World War, he was thrown out of the socialist party. He \&#39;invented\&#39; a new ideology, Fascism, much out of Nationalist\xa0and Conservative views.\n\nRise to power and becoming dictator\nIn 1922, he took power by having a large group of men, &#34;Black Shirts,&#34; march on Rome and threaten to take over the government. King Vittorio Emanuele III gave in, allowed him to form a government, and made him prime minister. In the following five years, he gained power, and in 1927 created the OVRA, his personal secret police force. Using the agency to arrest, scare, or murder people against his regime, Mussolini was dictator\xa0of Italy by the end of 1927. Only the King and his own Fascist party could challenge his power.&#39;, lookup_str=&#39;&#39;, metadata={&#39;chunk&#39;: 0.0, &#39;source&#39;: &#39;https://simple.wikipedia.org/wiki/Benito%20Mussolini&#39;, &#39;title&#39;: &#39;Benito Mussolini&#39;, &#39;wiki-id&#39;: &#39;6754&#39;}, lookup_index=0),
</code></pre><pre><code> Document(page_content=&#39;Fascism as practiced by Mussolini\nMussolini\&#39;s form of Fascism, &#34;Italian Fascism&#34;- unlike Nazism, the racist ideology that Adolf Hitler followed- was different and less destructive than Hitler\&#39;s. Although a believer in the superiority of the Italian nation and national unity, Mussolini, unlike Hitler, is quoted &#34;Race? It is a feeling, not a reality. Nothing will ever make me believe that biologically pure races can be shown to exist today&#34;.\n\nMussolini wanted Italy to become a new Roman Empire. In 1923, he attacked the island of Corfu, and in 1924, he occupied the city state of Fiume. In 1935, he attacked the African country Abyssinia (now called Ethiopia). His forces occupied it in 1936. Italy was thrown out of the League of Nations because of this aggression. In 1939, he occupied the country Albania. In 1936, Mussolini signed an alliance with Adolf Hitler, the dictator of Germany.\n\nFall from power and death\nIn 1940, he sent Italy into the Second World War on the side of the Axis countries. Mussolini attacked Greece, but he failed to conquer it. In 1943, the Allies landed in Southern Italy. The Fascist party and King Vittorio Emanuel III deposed Mussolini and put him in jail, but he was set free by the Germans, who made him ruler of the Italian Social Republic puppet state which was in a small part of Central Italy. When the war was almost over, Mussolini tried to escape to Switzerland with his mistress, Clara Petacci, but they were both captured and shot by partisans. Mussolini\&#39;s dead body was hanged upside-down, together with his mistress and some of Mussolini\&#39;s helpers, on a pole at a gas station in the village of Millan, which is near the border  between Italy and Switzerland.&#39;, lookup_str=&#39;&#39;, metadata={&#39;chunk&#39;: 1.0, &#39;source&#39;: &#39;https://simple.wikipedia.org/wiki/Benito%20Mussolini&#39;, &#39;title&#39;: &#39;Benito Mussolini&#39;, &#39;wiki-id&#39;: &#39;6754&#39;}, lookup_index=0),
</code></pre><pre><code> Document(page_content=&#39;Fascist Italy \nIn 1922, a new Italian government started. It was ruled by Benito Mussolini, the leader of Fascism in Italy. He became head of government and dictator, calling himself &#34;Il Duce&#34; (which means &#34;leader&#34; in Italian). He became friends with German dictator Adolf Hitler. Germany, Japan, and Italy became the Axis Powers. In 1940, they entered World War II together against France, Great Britain, and later the Soviet Union. During the war, Italy controlled most of the Mediterranean Sea.\n\nOn July 25, 1943, Mussolini was removed by the Great Council of Fascism. On September 8, 1943, Badoglio said that the war as an ally of Germany was ended. Italy started fighting as an ally of France and the UK, but Italian soldiers did not know whom to shoot. In Northern Italy, a movement called Resistenza started to fight against the German invaders. On April 25, 1945, much of Italy became free, while Mussolini tried to make a small Northern Italian fascist state called the Republic of Salò. The fascist state failed and Mussolini tried to flee to Switzerland and escape to Francoist Spain, but he was captured by Italian partisans. On 28 April 1945 Mussolini was executed by a partisan.\n\nAfter World War Two \n\nThe state became a republic on June 2, 1946. For the first time, women were able to vote. Italian people ended the Savoia dynasty and adopted a republic government.\n\nIn February 1947, Italy signed a peace treaty with the Allies. They lost all the colonies and some territorial areas (Istria and parts of Dalmatia).\n\nSince then Italy has joined NATO and the European Community (as a founding member). It is one of the seven biggest industrial economies in the world.\n\nTransportation \n\nThe railway network in Italy totals . It is the 17th longest in the world. High speed trains include ETR-class trains which travel at .&#39;, lookup_str=&#39;&#39;, metadata={&#39;chunk&#39;: 5.0, &#39;source&#39;: &#39;https://simple.wikipedia.org/wiki/Italy&#39;, &#39;title&#39;: &#39;Italy&#39;, &#39;wiki-id&#39;: &#39;363&#39;}, lookup_index=0)]</code></pre></div></div></div><p>All of these are relevant results, telling us that the retrieval component of our systems is functioning. The next step is adding our LLM to generatively answer our question using the information provided in these retrieved contexts.</p><h3 id="generative-question-answering">Generative Question Answering</h3><p>In generative question-answering (GQA), we pass our question to the LLM but instruct it to base the answer on the information returned from our knowledge base. We can do this in LangChain easily using the <code>RetrievalQA</code> chain.</p><div><pre><code data-lang="python"><span>from</span> langchain.chat_models <span>import</span> ChatOpenAI
<span>from</span> langchain.chains <span>import</span> RetrievalQA

<span># completion llm</span>
llm <span>=</span> ChatOpenAI(
    openai_api_key<span>=</span>OPENAI_API_KEY,
    model_name<span>=</span><span>&#39;gpt-3.5-turbo&#39;</span>,
    temperature<span>=</span><span>0.0</span>
)

qa <span>=</span> RetrievalQA<span>.</span>from_chain_type(
    llm<span>=</span>llm,
    chain_type<span>=</span><span>&#34;stuff&#34;</span>,
    retriever<span>=</span>vectorstore<span>.</span>as_retriever()
)
</code></pre></div><p>Let’s try this with our earlier query:</p><div><div><p>In[22]:</p><pre><code>qa.run(query)</code></pre></div><div><p>Out[22]:</p><div><pre><code>&#39;Benito Mussolini was an Italian politician and journalist who served as the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and invented the ideology of Fascism. Mussolini was a dictator of Italy by the end of 1927, and his form of Fascism, &#34;Italian Fascism,&#34; was different and less destructive than Hitler\&#39;s Nazism. Mussolini wanted Italy to become a new Roman Empire and attacked several countries, including Abyssinia (now called Ethiopia) and Greece. He was removed from power in 1943 and was executed by Italian partisans in 1945.&#39;</code></pre></div></div></div><p>The response we get this time is generated by our <code>gpt-3.5-turbo</code> LLM based on the retrieved information from our vector database.</p><p>We’re still not entirely protected from convincing yet false hallucinations by the model, they can happen, and it’s unlikely that we can eliminate the problem completely. However, we can do more to improve our trust in the answers provided.</p><p>An effective way of doing this is by adding citations to the response, allowing a user to see <em>where</em> the information is coming from. We can do this using a slightly different version of the <code>RetrievalQA</code> chain called <code>RetrievalQAWithSourcesChain</code>.</p><div><div><p>In[23]:</p><pre><code>from langchain.chains import RetrievalQAWithSourcesChain

qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type=&#34;stuff&#34;,
    retriever=vectorstore.as_retriever()
)</code></pre></div><div><p>In[24]:</p><pre><code>qa_with_sources(query)</code></pre></div><div><p>Out[24]:</p><div><pre><code>{&#39;question&#39;: &#39;who was Benito Mussolini?&#39;,
</code></pre><pre><code> &#39;answer&#39;: &#39;Benito Mussolini was an Italian politician and journalist who was the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and invented the ideology of Fascism. He became dictator of Italy by the end of 1927 and was friends with German dictator Adolf Hitler. Mussolini attacked Greece and failed to conquer it. He was removed by the Great Council of Fascism in 1943 and was executed by a partisan on April 28, 1945. After the war, several Neo-Fascist movements have had success in Italy, the most important being the Movimento Sociale Italiano. His granddaughter Alessandra Mussolini has outspoken views similar to Fascism. \n&#39;,
</code></pre><pre><code> &#39;sources&#39;: &#39;https://simple.wikipedia.org/wiki/Benito%20Mussolini, https://simple.wikipedia.org/wiki/Fascism&#39;}</code></pre></div></div></div><p>Now we have answered the question being asked but also included the <em>source</em> of this information being used by the LLM.</p><hr/><p>We’ve learned how to ground <strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels with source knowledge by using a vector database as our knowledge base. Using this, we can encourage accuracy in our LLM’s responses, keep source knowledge up to date, and improve trust in our system by providing citations with every answer.</p><p>We’re already seeing LLMs and knowledge bases paired together in huge products like Bing’s AI search, Google Bard, and <a href="https://youtu.be/hpePPqKxNq8">ChatGPT plugins</a>. Without a doubt, the future of LLMs is tightly coupled with high-performance, scalable, and reliable knowledge bases.</p><hr/><a href="https://blog.plover.com/learn/langchain-agents/"><span>Next Chapter:</span><p>Superpower LLMs with Conversational Agents in LangChain</p></a><section><hr/><h3>Comments</h3></section></div></div>
  </body>
</html>
