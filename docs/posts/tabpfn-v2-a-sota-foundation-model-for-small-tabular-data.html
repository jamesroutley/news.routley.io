<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41586-024-08328-6/link">Original</a>
    <h1>Show HN: TabPFN v2 – A SOTA foundation model for small tabular data</h1>
    
    <div id="readability-page-1" class="page"><div>
                    
                        <section data-title="Main"><div id="Sec1-section"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Throughout the history of artificial intelligence, manually created algorithmic components have been replaced with better-performing end-to-end learned ones. Hand-designed features in computer vision, such as SIFT (Scale Invariant Feature Transform)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 60, 91–110 (2004)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR10" id="ref-link-section-d87306886e547">10</a></sup> and HOG (Histogram of Oriented Gradients)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) 886–893 (IEEE, 2005)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR11" id="ref-link-section-d87306886e551">11</a></sup>, have been replaced by learned convolutions; grammar-based approaches in natural language processing have been replaced by learned transformers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d87306886e555">12</a></sup>; and the design of customized opening and end-game libraries in game playing has been superseded by end-to-end learned strategies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR3" id="ref-link-section-d87306886e559">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Silver, D. et al. Mastering the game of go without human knowledge. Nature 550, 354–359 (2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR13" id="ref-link-section-d87306886e562">13</a></sup>. Here we extend this end-to-end learning to the ubiquitous domain of tabular data.</p><p>The diversity of tabular data sets them apart from unprocessed modalities such as text and images. While in language modelling for example the meaning of a word is consistent across documents, in tabular datasets the same value can mean fundamentally different things. A drug discovery dataset, for example, might record chemical properties, whereas another dataset in materials science might document thermal and electric properties. This specialization leads to a proliferation of smaller, independent datasets and associated models. To illustrate, on the popular tabular benchmarking website openml.org, 76% of the datasets contain less than 10,000 rows at the time of writing.</p><p>Deep learning methods have traditionally struggled with tabular data, because of the heterogeneity between datasets and the heterogeneity of the raw data itself: Tables contain columns, also called features, with various scales and types (Boolean, categorical, ordinal, integer, floating point), imbalanced or missing data, unimportant features, outliers and so on. This made non-deep-learning methods, such as tree-based models, the strongest contender so far<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e572">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e575">15</a></sup>.</p><p>However, these traditional machine learning models have several drawbacks. Without substantial modifications, they yield poor out-of-distribution predictions and poor transfer of knowledge from one dataset to another<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goodfellow, I., Bengio, Y. &amp; Courville, A. Deep Learning (MIT Press, 2016)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR16" id="ref-link-section-d87306886e582">16</a></sup>. Finally, they are hard to combine with neural networks, as they do not propagate gradients.</p><p>As a remedy, we introduce TabPFN, a foundation model for small- to medium-sized tabular data. This new supervised tabular learning method can be applied to any small- to moderate-sized dataset and yields dominant performance for datasets with up to 10,000 samples and 500 features. In a single forward pass, TabPFN significantly outperforms state-of-the-art baselines on our benchmarks, including gradient-boosted decision trees, even when these are allowed 4 h of tuning, a speedup of 5,140× (classification) and 3,000× (regression). Finally, we demonstrate various foundation model characteristics of TabPFN, including fine-tuning, generative abilities and density estimation.</p></div></div></section><section data-title="Principled in-context learning"><div id="Sec2-section"><h2 id="Sec2">Principled in-context learning</h2><div id="Sec2-content"><p>TabPFN leverages in-context learning (ICL)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d87306886e598">17</a></sup>, the same mechanism that led to the astounding performance of large language models, to generate a powerful tabular prediction algorithm that is fully learned. Although ICL was first observed in large language models, recent work has shown that transformers can learn simple algorithms such as logistic regression through ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In Proc. Advances in Neural Information Processing Systems Vol. 35, 30583–30598 (ACM, 2022)." href="#ref-CR18" id="ref-link-section-d87306886e602">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="#ref-CR19" id="ref-link-section-d87306886e602_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Von Oswald, J. et al. Transformers learn in-context by gradient descent. In Proc. 40th International Conference on Machine Learning 35151–35174 (PMLR, 2023)." href="#ref-CR20" id="ref-link-section-d87306886e602_2">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In Proc. The Twelfth International Conference on Learning Representations (ICLR, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR21" id="ref-link-section-d87306886e605">21</a></sup>. Prior-data Fitted Networks (PFNs) have shown that even complex algorithms, such as Gaussian Processes and Bayesian Neural Networks, can be approximated with ICL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e609">22</a></sup>. ICL enables us to learn a wider space of possible algorithms, including cases for which a closed-form solution does not exist.</p><p>We build on a preliminary version of TabPFN<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d87306886e616">23</a></sup>, which demonstrated the applicability of in-context-learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural Information Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR17" id="ref-link-section-d87306886e620">17</a></sup> for tabular data in principle but had many limitations that rendered it inapplicable in most cases. Based on a series of improvements, the new TabPFN scales to 50× larger datasets; supports regression tasks, categorical data and missing values; and is robust to unimportant features and outliers.</p><p>The key idea behind TabPFN is to generate a large corpus of synthetic tabular datasets and then train a transformer-based<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d87306886e627">12</a></sup> neural network to learn to solve these synthetic prediction tasks. Although traditional approaches require hand-engineered solutions for data challenges such as missing values, our method autonomously learns effective strategies by solving synthetic tasks that include these challenges. This approach leverages ICL as a framework for exemplar-based declarative programming of algorithms. We design desired algorithmic behaviour by generating diverse synthetic datasets that demonstrate the desired behaviour and then train a model to encode an algorithm that satisfies it. This shifts the algorithm design process from writing explicit instructions to defining input–output examples, opening up possibilities for creating algorithms in various domains. Here, we apply this approach to the high-impact field of tabular learning, generating a powerful tabular prediction algorithm.</p><p>Our ICL approach differs fundamentally from standard supervised deep learning. Usually, models are trained per dataset, updating model parameters on individual samples or batches according to hand-crafted weight-updating algorithms, such as Adam<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d87306886e634">24</a></sup>. At inference time, the learned model is applied to test samples. By contrast, our approach is trained across datasets and is applied to entire datasets at inference time rather than individual samples. Before being applied to real-world datasets, the model is once pre-trained on millions of synthetic datasets representing different prediction tasks. At inference time, the model receives an unseen dataset with both labelled training and unlabelled test samples and performs training and prediction on this dataset in a single neural network forward pass.</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig2">2</a> outline our approach:</p><ol>
                <li>
                  <span>1.</span>
                  
                    <p>Data generation: we define a generative process (referred to as our prior) to synthesize diverse tabular datasets with varying relationships between features and targets, designed to capture a wide range of potential scenarios that our model might encounter. We sample millions of datasets from the generative process. For each dataset, a subset of samples has their target values masked, simulating a supervised prediction problem. Further details of our prior design are shown in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec4">Synthetic data based on causal models</a>’.</p>
                  
                </li>
                <li>
                  <span>2.</span>
                  
                    <p>Pre-training: we train a transformer model, our PFN, to predict the masked targets of all synthetic datasets, given the input features and the unmasked samples as context. This step is done only once during model development, learning a generic learning algorithm that can be used to predict any dataset.</p>
                  
                </li>
                <li>
                  <span>3.</span>
                  
                    <p>Real-world prediction: the resulting trained model can now be applied to arbitrary unseen real-world datasets. The training samples are provided as context to the model, which predicts the labels of these unseen datasets through ICL.</p>
                  
                </li>
              </ol><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Overview of the proposed method."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Overview of the proposed method.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="365"/></picture></a></div><p><b>a</b>, The high-level overview of TabPFN pre-training and usage. <b>b</b>, The TabPFN architecture. We train a model to solve more than 100 million synthetic tasks. Our architecture is an adaptation of the standard transformer encoder that is adapted for the two-dimensional data encountered in tables.</p></div></figure></div><p>Our approach also has a theoretical foundation as described in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e714">22</a></sup>. It can be viewed as approximating Bayesian prediction for a prior defined by the synthetic datasets. The trained PFN will approximate the posterior predictive distribution <span>\(p({\widehat{{\bf{y}}}}_{{\rm{test}}}| {{\bf{X}}}_{{\rm{test}}},{{\bf{X}}}_{{\rm{train}}},{{\bf{y}}}_{{\rm{train}}})\)</span>  and thus return a Bayesian prediction for the specified distribution over artificial datasets used during PFN pre-training.</p></div></div></section><section data-title="An architecture designed for tables"><div id="Sec3-section"><h2 id="Sec3">An architecture designed for tables</h2><div id="Sec3-content"><p>The transformer architecture is currently the favoured architecture for flexible deep learning and foundation models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583 – 589 (2021)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR4" id="ref-link-section-d87306886e834">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="OpenAI. GPT-4 Technical Report. Preprint at 
                  https://arxiv.org/abs/2303.08774
                  
                 (2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR5" id="ref-link-section-d87306886e837">5</a></sup>. Transformer models work on sequences and combine information between sequence items using so-called attention mechanisms<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In Proc. 3rd International Conference on Learning Representations (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR25" id="ref-link-section-d87306886e841">25</a></sup>, allowing them to effectively capture long-range dependencies and learn complex relationships in data. Although transformer-based models can be applied to tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In Proc. Advances in Neural Information Processing Systems 34 (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR26" id="ref-link-section-d87306886e845">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) 43181–43204 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR27" id="ref-link-section-d87306886e848">27</a></sup>, TabPFN addresses two key limitations inherent to them. First, as transformers are designed for sequences, they treat the input data as a single sequence, not using the tabular structure. Second, machine learning models are often used in a fit-predict model, in which a model is fitted on the training set once and then reused for multiple test datasets. Transformer-based ICL algorithms, however, receive train and test data in a single pass and thus perform training and prediction at once. Thus, when a fitted model is reused, it has to redo computations for the training set.</p><p>To better use the tabular structure, we propose an architecture that assigns a separate representation to each cell in the table, inspired by refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e855">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d87306886e858">28</a></sup>. Our architecture, visualized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig1">1b</a>, uses a two-way attention mechanism, with each cell attending to the other features in its row (that is, its sample) and then attending to the same feature across its column (that is, all other samples). This design enables the architecture to be invariant to the order of both samples and features and enables more efficient training and extrapolation to larger tables than those encountered during training, in terms of both the number of samples and features.</p><p>To mitigate repeating computations on the training set for each test sample in a fit-predict setting, our model can separate the inference on the training and test samples. This allows us to perform ICL on the training set once, save the resulting state and reuse it for multiple test set inferences. On datasets with 10,000 training samples and 10 features, our optimized train-state caching results in inference speedups of around 300× on CPU (from 32 s to 0.1 s) and 6× on GPU. With 10× more features (100), the speedups increase to 800× on CPU and 30× speedup on GPU. These measurements focus solely on the core inference process, excluding pre-processing and ensembling steps detailed in the section ‘Inference details’. The lower speedups on GPUs are because of an underutilization of their massively parallel architecture.</p><p>We further optimize the memory and compute requirements of the architecture by computing layer norms in half-precision, using flash attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR29" id="ref-link-section-d87306886e871">29</a></sup>, activation checkpointing and sequential computation of the state. Our optimizations reduce the memory requirements by a factor of four, resulting in less than 1,000 bytes per cell. This enables the prediction on datasets with up to 50 million cells (for example, 5 million rows × 10 features) on a single H100 GPU.</p><p>For regression tasks, we use a piece-wise constant output distribution, following refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e879">22</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Torgo, L. &amp; Gama, J. Regression using classification algorithms. Intell. Data Anal. 1, 275–292 (1997)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR30" id="ref-link-section-d87306886e882">30</a></sup>, which allows our models to predict a probability distribution of target values instead of a single value, including, for example, bimodal distributions.</p></div></div></section><section data-title="Synthetic data based on causal models"><div id="Sec4-section"><h2 id="Sec4">Synthetic data based on causal models</h2><div id="Sec4-content"><p>The performance of TabPFN relies on generating suitable synthetic training datasets that capture the characteristics and challenges of real-world tabular data. To generate such datasets, we developed an approach based on structural causal models (SCMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d87306886e894">31</a></sup>. SCMs provide a formal framework for representing causal relationships and generative processes underlying the data. By relying on synthetic data instead of large collections of public tabular data, we avoid common problems of foundational models, such as privacy and copyright infringements, contaminating our training data with test data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at 
                  https://arxiv.org/abs/2401.06059
                  
                 (2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR32" id="ref-link-section-d87306886e898">32</a></sup> or limited data availability.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig2">2</a>, our generative pipeline first samples high-level hyperparameters, such as dataset size, number of features and difficulty level, to govern the overall properties of each synthetic dataset. Guided by these hyperparameters, we construct a directed acyclic graph specifying the causal structure underlying the dataset.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Overview of the TabPFN prior."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Overview of the TabPFN prior.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="318"/></picture></a></div><p><b>a</b>, For each dataset, we first sample high-level hyperparameters. <b>b</b>, Based on these hyperparameters, we construct a structural causal model that encodes the computational function generating the dataset. Each node holds a vector and each edge in the computational graph implements a function according to one of the connection types. In step 1, using random noise variables we generate initialization data, which is fed into the root nodes of the graphs and propagated through the computational graph for each to-be-generated sample. In step 2, we randomly sample feature and target node positions in the graph, labelled F and T, respectively. In step 3, we extract the intermediate data representations at the sampled feature and target node positions. In step 4, we post-process the extracted data. <b>c,</b> We retrieve the final datasets. We plot interactions of feature pairs and the node colour represents the class of the sample.</p></div></figure></div><p>To generate each sample within a dataset, we propagate randomly generated noise, called our initialization data, through the root nodes of the causal graph. This initialization data are generated by sampling from a random normal or uniform distribution with varying degrees of non-independence between samples, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec23">Initialization data sampling</a>’. As these data traverse the edges of the computational graph, we apply a diverse set of computational mappings: small neural networks with linear or nonlinear activations (for example, sigmoid, ReLU (rectified linear unit), modulo, sine), discretization mechanisms for generating categorical features and decision tree structures to encode local, rule-based dependencies. At each edge, we add Gaussian noise, introducing uncertainty into the generated data. We save the intermediate data representations at each node to be retrieved later. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec22">Computational edge mappings</a>’ for details.</p><p>After traversing the causal graph, we extract the intermediate representations at the sampled feature and target nodes, yielding a sample consisting of feature values and an associated target value.</p><p>By incorporating various data challenges and complexities into the synthetic datasets, we create a training ground that allows TabPFN to develop strategies for handling similar issues in real-world datasets. For instance, consider the case of missing values, commonly present in tabular data. By exposing TabPFN to synthetic datasets with varying patterns and fractions of missing values in our synthetic data generation process, the model learns effective ways of handling missing values that generalize to real-world datasets. We apply post-processing techniques to further enhance the realism and challenge the robustness of the learned prediction algorithms. This includes warping with the Kumaraswamy distribution<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d87306886e952">33</a></sup>, introducing complex nonlinear distortions and quantization mimicking discretized features. See section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec24">Post-processing</a>’ for details.</p><p>Through this generative process, we created a massive corpus of around 100 million synthetic datasets per model training, each with a unique causal structure, feature types and functional characteristics.</p></div></div></section><section data-title="Qualitative analysis"><div id="Sec5-section"><h2 id="Sec5">Qualitative analysis</h2><div id="Sec5-content"><p>We first analyse the behaviour of TabPFN on toy problems to build intuition and disentangle the impact of various dataset characteristics. As regression problems are easier to visualize, we focus on these in our qualitative analysis. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig3">3a</a>, we compare TabPFN with a diverse set of standard predictors, with all methods using default settings.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="The behaviour of TabPFN and a set of baselines on simple functions."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: The behaviour of TabPFN and a set of baselines on simple functions.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="319"/></picture></a></div><p>In all plots, we use orange for the ground truth and blue for model predictions. <b>a</b>, Each column represents a different toy function, each having a single feature (along the <i>x</i>-axis) and a target (along the <i>y</i>-axis). TabPFN can model a lot of different functions, including noisy functions. <b>b</b>, TabPFN can model distributions over outputs out of the box, which is exemplified by predicting the light intensity pattern in a double-slit experiment after observing the positions of 1,000 photons.</p></div></figure></div><p>Linear (ridge) regression can naturally model only linear functions, leading to simple and interpretable predictions but catastrophic failure on many of the toy functions. Multilayer perceptrons (MLPs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d87306886e1008">34</a></sup> perform worse on datasets with highly non-smooth patterns<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e1012">14</a></sup>. This is especially apparent for the step function. TabPFN, by contrast, models either function type, smooth or non-smooth, out of the box. This includes a good approximation to step functions despite TabPFN being a neural network. CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d87306886e1016">9</a></sup>, representative of tree-based methods, fits only piece-wise constant functions. Although this leads to approximation errors and unintuitive predictions, it avoids catastrophic failures.</p><p>The main advantage of TabPFN over all baselines is its inherent ability to model uncertainty at no extra cost. Whereas classical regression methods output a single real-valued prediction, TabPFN returns a target distribution, capturing the uncertainty of predictions. These uncertainty modelling abilities of TabPFN extend beyond simple distributions and can handle complex, multi-modal distributions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig3">3b</a> shows this by modelling the density of light reaching a detector screen in a double-slit experiment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. Philos. Trans. R. Soc. Lond. 94, 1–16 (1804)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR35" id="ref-link-section-d87306886e1026">35</a></sup> for different slit distances and widths. In this classic experiment, photons are sent through two slits creating a multi-modal intensity pattern because of the wave-like interference behaviour of light. TabPFN predicts these intricate patterns in just a single forward pass, requiring only 1.2 s. By contrast, traditional methods such as CatBoost require training multiple quantile models at different quantiles and reconstructing the distribution from these predictions. Even after tuning CatBoost specifically for this task, it produced substantially worse predictions compared with TabPFN, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig3">3b</a>. With default settings, CatBoost requires 169.3 s and yields further deteriorated results. Qualitatively, we observe that TabPFN is more accurate in predicting very low densities and has fewer artefacts compared with CatBoost.</p></div></div></section><section data-title="Quantitative analysis"><div id="Sec6-section"><h2 id="Sec6">Quantitative analysis</h2><div id="Sec6-content"><p>We quantitatively evaluate TabPFN on two dataset collections: the AutoML Benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d87306886e1042">36</a></sup> and OpenML-CTR23<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d87306886e1046">37</a></sup>. These benchmarks comprise diverse real-world tabular datasets, curated for complexity, relevance and domain diversity. From these benchmarks, we use the 29 classification datasets and 28 regression datasets that have up to 10,000 samples, 500 features and 10 classes. We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e1050">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e1053">15</a></sup>, as well as five Kaggle competitions from the Tabular Playground Series.</p><p>We compared TabPFN against state-of-the-art baselines, including tree-based methods (random forest<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d87306886e1060">38</a></sup>, XGBoost (XGB)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d87306886e1064">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d87306886e1068">9</a></sup>, LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d87306886e1072">8</a></sup>), linear models, support vector machines (SVMs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d87306886e1076">39</a></sup> and MLPs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR34" id="ref-link-section-d87306886e1081">34</a></sup>.</p><p>Evaluation metrics include ROC AUC (area under the receiver operating characteristic curve; One-vs-Rest) and accuracy for classification, and <i>R</i><sup>2</sup> (coefficient of determination) and negative RMSE (root mean squared error) for regression. Scores were normalized per dataset, with 1.0 representing the best and 0.0 the worst performance with respect to all baselines.</p><p>For each dataset and method, we ran 10 repetitions with different random seeds and train–test splits (90% train, 10% test). We tuned hyperparameters using random search with five-fold cross-validation, with time budgets ranging from 30 s to 4 h. All methods were evaluated using eight CPU cores, with TabPFN additionally using a consumer-grade GPU (RTX 2080 Ti; other methods did not benefit from this, see Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig8">2d</a>). TabPFN was pre-trained once using eight NVIDIA RTX 2080 GPUs over 2 weeks, allowing for ICL on all new datasets in a single forward pass. These modest computational requirements make similar research accessible to academic labs. For details, refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec35">Detailed evaluation protocol</a>’.</p><h3 id="Sec7">Comparison with state-of-the-art baselines</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig4">4a</a> demonstrates the strong out-of-the-box performance of TabPFN compared with tuned and default configurations of XGBoost, CatBoost and a random forest. For classification tasks, TabPFN surpasses CatBoost, the strongest default baseline, by 0.187 (0.939 compared with 0.752) in normalized ROC AUC in the default setting and by 0.13 (0.952 compared with 0.822) in the tuned setting. For regression, TabPFN outperforms CatBoost in normalized RMSE by 0.051 (0.923 compared with 0.872) in the default setting and by 0.093 (0.968 compared with 0.875) in the tuned setting. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig4">4b</a>, we show per-dataset comparisons. Although for some datasets CatBoost outperforms TabPFN, TabPFN wins on most of the datasets.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Comparison of TabPFN on our test benchmarks, containing datasets with up to 10,000 samples and 500 features.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="304"/></picture></a></div><p>Performance was normalized per dataset before aggregation using all baselines; intervals represent the 95% confidence interval. Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d87306886e1133">54</a></sup>. <b>a</b>, Average performance of the default as well as the tuned versions of TabPFN and our baselines. All methods are tuned for ROC AUC or RMSE, respectively, thus decreasing the representativeness of the secondary metrics. LGBM, LightGBM; MLP, multilayer perceptron; SVM, support vector machines; RF, random forest; CB, CatBoost; XGB, XGBoost; Lin, logistic regression for classification and ridge regression for regression tasks. Plots on the right-hand side show a magnified analysis of the strongest baselines considered. <b>b</b>, A per-dataset comparison of TabPFN with its strongest baseline, CatBoost. Each dot is the average score on one dataset. <b>c</b>, The impact of hyperparameter tuning for the considered methods. The <i>x</i>-axis shows the average time required to fit and predict with the algorithm.</p></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig4">4c</a> shows how the performance of TabPFN and the baselines improve with more time spent on hyperparameter search. The default of TabPFN, taking 2.8 s on average for classification and 4.8 s for regression, outperforms all baselines, even when tuning them for 4 h—a speedup of 5,140× and 3,000×, respectively. We show comparisons on a larger number of metrics in Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab1">1</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab2">2</a>.</p><p>As shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig8">2</a>, similar to our primary benchmarks, TabPFN substantially outperformed all baselines on the benchmarks of refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e1176">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e1179">15</a></sup>. The benchmark of ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e1183">14</a></sup> is particularly noteworthy because on this benchmark, tree-based methods were previously found to excel. Moreover, we show in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab6">6</a> that default TabPFN outperforms default CatBoost on all five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h3 id="Sec8">Evaluating diverse data attributes</h3><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig5">5a,b</a>, we show the robustness of TabPFN to dataset characteristics that are traditionally hard to handle for neural-network-based approaches<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e1201">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In Proc. The Eleventh International Conference on Learning Representations (ICLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR23" id="ref-link-section-d87306886e1204">23</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Robustness across datasets and performance comparison with tuned ensembles."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Robustness across datasets and performance comparison with tuned ensembles.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="352"/></picture></a></div><p><b>a</b>, A comparison of modified datasets. We can see that TabPFN is not more vulnerable to the modifications compared with baselines. We also see that TabPFN reproduces the accuracy of CatBoost (default) with only half the training samples provided. Here we normalize scores per dataset (sharing one normalization across all modifications of one experiment) to avoid negative outliers. <b>b</b>, We split the test datasets by data characteristics and analyse the performance per subgroup. <b>c</b>, Classification performance. Left, the win rate of TabPFN (PHE) against AutoGluon (with one tie excluded); right, the ROC AUC score over time for tuning each method, with the first marker representing the default configuration for the non-ensembling methods. <b>d</b>, Regression performance presented as in <b>c</b> but using the RMSE metric. Intervals represent the 95% confidence interval and Wilcoxon <i>P</i> refers to the two-sided Wilcoxon signed-rank test <i>P</i> value<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR54" id="ref-link-section-d87306886e1241">54</a></sup>.</p></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig5">5a</a> provides an analysis of the performance of TabPFN across various dataset types. First, we add uninformative features (randomly shuffled features from the original dataset) and outliers (multiply each cell with 2% probability with a random number between 0 and the outlier factor). The results show that TabPFN is very robust to uninformative features and outliers, something typically hard for neural networks, as can be seen with the MLP baseline. Second, although dropping either samples or features hurts the performance of all methods, with half the samples TabPFN still performs as well as the next best method using all samples.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig5">5b</a>, we split our test datasets into subgroups and perform analyses per subgroup. We create subgroups based on the presence of categorical features, missing values, number of samples and number of features in the datasets. The sample- and feature-number subgroups are split such that a third of the datasets fall into each group. We can see that none of these characteristics strongly affect the performance of TabPFN relative to the other methods. However, we note that these results should not be taken as evidence that TabPFN scales well beyond the 10,000 samples and 500 features considered here. We show four further ablations in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig7">1</a>.</p><h3 id="Sec9">Comparison with tuned ensemble methods</h3><p>We compare the performance of TabPFN with AutoGluon 1.0 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d87306886e1276">40</a></sup>), which combines various machine learning models, including our baselines, into a stacked ensemble<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Wolpert, D. Stacked generalization. Neural Netw. 5, 241–259 (1992)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR41" id="ref-link-section-d87306886e1280">41</a></sup>, tunes their hyperparameters and then generates the final predictions using post hoc ensembling (PHE)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d87306886e1284">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d87306886e1287">43</a></sup>. It thus represents a different class of methods compared with individual baselines.</p><p>To assess whether TabPFN can also be improved by a tuned ensemble approach, we introduce TabPFN (PHE). TabPFN (PHE) automatically combines only TabPFN models with PHE and tunes their hyperparameters using a random portfolio from our search space. We detail this approach in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig5">5c–d</a> compares the performance of TabPFN, TabPFN (PHE), AutoGluon and CatBoost. For TabPFN (PHE) and AutoGluon, we start with a minimal budget of 300 s for tuning because AutoGluon otherwise does not reliably return results. In just 2.8 s, TabPFN (default) outperforms AutoGluon for classification tasks, even if AutoGluon is allowed up to 4 h, a 5.140× speedup. TabPFN (PHE) further improves performance leading to an average normalized ROC AUC score of 0.971, compared with 0.939 for TabPFN (default) and 0.914 for AutoGluon. For regression tasks, tuning hyperparameters is more important. Here, TabPFN (PHE) outperforms AutoGluon (allowed 4 h) after its minimal tuning budget of 300 s, a 48× speedup.</p></div></div></section><section data-title="Foundation model with interpretability"><div id="Sec10-section"><h2 id="Sec10">Foundation model with interpretability</h2><div id="Sec10-content"><p>Apart from its strong predictive performance, TabPFN exhibits key foundation model abilities, such as data generation, density estimation, learning reusable embeddings and fine-tuning. We showcase these abilities through proof-of-concept experiments on the German Credit Dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5NC77
                  
                 (1994)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR44" id="ref-link-section-d87306886e1312">44</a></sup>, which contains credit risk information and the mfeat-factors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Duin, R. Multiple Features. UCI Machine Learning Repository 
                  https://doi.org/10.24432/C5HC70
                  
                 (1998)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR45" id="ref-link-section-d87306886e1316">45</a></sup> dataset classifying handwritten digits based on a tabular representation.</p><p>TabPFN can estimate the probability density function of numerical features, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig6">6a</a>, and the probability mass function of categorical features. Computing the sample densities enables anomaly detection to identify issues such as fraud, equipment failures, medical emergencies or low-quality data.</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Showcase of the application of TabPFN as tabular foundation model."><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6: Showcase of the application of TabPFN as tabular foundation model.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://ldirer.com/articles/s41586-024-08328-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png?as=webp"/><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="180"/></picture></a></div><p><b>a</b>,<b>b</b>, On the German Credit Dataset, we perform data density estimation (<b>a</b>) and generation of new synthetic samples (<b>b</b>). <b>c</b>, We show our learned embeddings are useful representations of each sample on the handwritten digits dataset (mfeat-factors) with different classes forming different clusters. <b>d</b>, We demonstrate fine-tuning TabPFN for a specific set of tasks. Fine-tuned on a dataset containing various sine curves (top), we see the model makes more accurate predictions on another sine curve dataset.</p></div></figure></div><p>TabPFN also allows synthesizing new tabular data samples that mimic real-world dataset characteristics as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig6">6b</a>. This enables applications such as data augmentation or privacy-preserving data sharing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. iScience 25, 105331 (2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR46" id="ref-link-section-d87306886e1369">46</a></sup>.</p><p>The architecture of TabPFN yields meaningful feature representations that can be reused for downstream tasks such as data imputation and clustering. We extract and visualize learned embeddings from the mfeat-factors dataset in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig6">6c</a>, showing improved class separation compared with the raw data on the first two principal components.</p><p>Furthermore, we demonstrate the ability of TabPFN to improve performance through fine-tuning on related datasets. Unlike tree-based methods, the neural architecture of TabPFN enables fine-tuning on specific dataset classes. We conduct proof-of-concept experiments using sine curve datasets with varying offsets between fine-tuning and test data. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig6">6d</a> shows an example fine-tuning result. Our analysis across 50 runs (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig10">4</a>) shows that TabPFN successfully transfers knowledge even when labels differ significantly between fine-tuning and test tasks, with performance improving as distributions become more similar. This could, for example, enable fine-tuning for a range of datasets from medical studies to obtain an improved general model for medical diagnosis tasks. For details, refer to section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec31">Foundation model abilities</a>’.</p><p>Finally, we have developed a methodology to easily interpret the predictions of TabPFN. Interpretability is crucial for building trust and accountability when deploying models in high-stakes domains. We support the computation of feature importance through SHAP<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d87306886e1395">47</a></sup> (Shapley Additive Explanations), a game-theoretic approach to explain predictions. SHAP values represent the contribution of each feature to the output of the model. Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig9">3</a> compares the feature importance and impact for logistic regression, CatBoost and TabPFN. TabPFN achieves high accuracy while learning simple, interpretable feature relationships. By contrast, logistic regression is interpretable but less accurate, whereas CatBoost is accurate but qualitatively less interpretable because of complex, non-smooth decision boundaries.</p></div></div></section><section data-title="Conclusion"><div id="Sec11-section"><h2 id="Sec11">Conclusion</h2><div id="Sec11-content"><p>TabPFN represents a major change in tabular data modelling, leveraging ICL to autonomously discover a highly efficient algorithm that outperforms traditional human-designed approaches on datasets with up to 10,000 samples and 500 features. This shift towards foundation models trained on synthetic data opens up new possibilities for tabular data analysis across various domains.</p><p>Potential future directions include scaling to larger datasets<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR48" id="ref-link-section-d87306886e1413">48</a></sup>, handling data drift<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR49" id="ref-link-section-d87306886e1417">49</a></sup>, investigating fine-tuning abilities across related tabular tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning (ICML, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR50" id="ref-link-section-d87306886e1421">50</a></sup> and understanding the theoretical foundations of our approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Nagler, T. Statistical foundations of prior-data fitted networks. In Proc. 40th International Conference on Machine Learning (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR51" id="ref-link-section-d87306886e1425">51</a></sup>. Future work could also explore creating specialized priors to handle data types such as time series<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In Proc. 37th Conference on Advances in Neural Information Processing Systems (eds Oh, A. et al.) (NeurIPS, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR52" id="ref-link-section-d87306886e1429">52</a></sup> and multi-modal data, or specialized modalities such as ECG, neuroimaging data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 6217–6230 (IEEE, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR53" id="ref-link-section-d87306886e1434">53</a></sup> and genetic data. As the field of tabular data modelling continues to evolve, we believe that foundation models, such as TabPFN, will play a key part in empowering researchers. To facilitate the widespread use of TabPFN, in the section ‘User guide’ we discuss how to use it effectively.</p></div></div></section><section data-title="Methods"><div id="Sec12-section"><h2 id="Sec12">Methods</h2><div id="Sec12-content"><h3 id="Sec13">User guide</h3><h4 id="Sec14">When to use TabPFN</h4><p>TabPFN excels in handling small- to medium-sized datasets with up to 10,000 samples and 500 features (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig4">4</a> and Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab1">1</a>). For larger datasets and highly non-smooth regression datasets, approaches such as CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d87306886e1460">9</a></sup>, XGB<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d87306886e1464">7</a></sup> or AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d87306886e1468">40</a></sup> are likely to outperform TabPFN.</p><p>Although TabPFN provides a powerful drop-in replacement for traditional tabular data models such as CatBoost, similar to these models, it is intended to be only one component in the toolkit of a data scientist. Achieving top performance on real-world problems often requires domain expertise and the ingenuity of data scientists. As for other modelling approaches, data scientists should continue to apply their skills and insights in feature engineering, data cleaning and problem framing to get the most out of TabPFN. We hope that the training speed of TabPFN will facilitate faster iterations in the data science workflow.</p><h4 id="Sec15">Limitations of TabPFN</h4><p>The limitations of TabPFN are as follows: (1) the inference speed of TabPFN may be slower than highly optimized approaches such as CatBoost; (2) the memory usage of TabPFN scales linearly with dataset size, which can be prohibitive for very large datasets; and (3) our evaluation focused on datasets with up to 10,000 samples and 500 features; scalability to larger datasets requires further study.</p><h4 id="Sec16">Computational and time requirements</h4><p>TabPFN is computationally efficient and can run on consumer hardware for most datasets. However, training on a new dataset is recommended to run on a (consumer) GPU as this speeds it up by one to three orders of magnitude. Although TabPFN is very fast to train, it is not optimized for real-time inference tasks. For a dataset with 10,000 rows and 10 columns, our model requires 0.2 s (0.6 s without GPU) to perform a prediction for one sample, whereas CatBoost (default) can do the same in 0.0002 s. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at 
                  https://arxiv.org/abs/2312.08598
                  
                 (2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR55" id="ref-link-section-d87306886e1491">55</a></sup>, further optimizing TabPFN specifically for inference tasks has already been explored, resulting in four times faster inference performance compared with even XGBoost, but so far also reducing predictive quality. Refer to the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec19">Details on the neural architecture</a>’ for details on the memory usage and runtime complexity of TabPFN.</p><h4 id="Sec17">Data preparation</h4><p>TabPFN can handle raw data with minimal pre-processing. If we simply provide the data in a tabular format (NumPy matrix), TabPFN will automatically handle missing values, encode categorical variables and normalize features. Although TabPFN works well out of the box, we can further improve the performance using dataset-specific pre-processing. This can also be partly done automatically with our PHE technique or manually by modifying the default settings. When manually pre-processing data, we should keep in mind that the neural network of TabPFN expects roughly normally distributed features and targets after all pre-processing steps. If we, for example, know that a feature follows a log distribution, it might help to exponentiate it before feeding it to TabPFN. As TabPFN does <i>z</i>-normalization of all inputs, scaling does not affect the predictions. As for all algorithms, however, using domain knowledge to combine or remove features can increase performance.</p><h4 id="Sec18">Hyperparameter tuning</h4><p>TabPFN provides strong performance out of the box without extensive hyperparameter tuning (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec7">Comparison with state-of-the-art baselines</a>’). If we have additional computational resources, we can further optimize the performance of TabPFN using hyperparameter optimization (HPO) or the PHE technique described in the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec30">TabPFN (PHE)</a>’. Our implementation directly provides HPO with random search and PHE.</p><h3 id="Sec19">Details on the neural architecture</h3><p>Our architecture is a variation of the original transformer encoder<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR12" id="ref-link-section-d87306886e1533">12</a></sup> and the original PFN architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e1537">22</a></sup>, but it treats each cell in the table as a separate time position, similar to that in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In Proc. Advances in Neural Information Processing Systems (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR28" id="ref-link-section-d87306886e1541">28</a></sup>. Therefore, it can generalize to more training samples as well as features than seen during training.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig1">1b</a> details our new architecture. All features that go into our architecture are first mapped to floating point values, that is, categoricals are transformed to integers. These values are subjected to <i>z</i>-normalization using the mean and standard deviation for each feature separately across the whole training set. These values are now encoded with simple linear encoders. Each layer first has an attention over features, followed by an attention over samples, both of which operate separately on each column or row, respectively. These two sub-layers are followed by an MLP sublayer. Each sublayer is followed by a residual addition and a half-precision layer norm.</p><p>We found that encoding groups of features can be even more effective compared with encoding one value per representation. For our hyperparameter search space, we selected six architectures for classification and five for regression. In three of the six classification models and four of the five regression models, including the TabPFN default, a transformer position encodes two features of one example; in others, it represents one value.</p><p>Although the inter-feature attention is a classical fully connected attention, our inter-sample attention does not allow the test samples to attend to each other but only to the training data. Therefore, we make sure that the test samples do not influence each other or the training set representations. To allow our model to differentiate features more easily that have the same statistics, for example, two features that have the same entries just in different orders, we use random feature embeddings that we add to all embeddings before the first layer. We generate one embedding per feature by projecting a random vector of one-fourth the size of our embeddings through a learned linear layer and add this to all embeddings representing an instance of that feature.</p><p>As the representations of training samples are not influenced by the test set, we cache the keys and values of the training samples to allow splitting training and inference. We use a special variant of multi-query attention for our inter-sample attention from test samples<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at 
                  https://arxiv.org/abs/1911.02150
                  
                 (2019)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR56" id="ref-link-section-d87306886e1564">56</a></sup> to save memory when caching representations. In our variant, we use all keys and values for the attention between samples of the training set, but repeatedly use the first key and value for attention from the test samples. This allows caching only one key or value vector pair per cell in the training set that is fed into our inter-sample attention of new test samples.</p><p>The compute requirements of this architecture scale quadratically with the number of samples (<i>n</i>) and the number of features (<i>m</i>), that is <i>O</i>(<i>n</i><sup>2</sup> + <i>m</i><sup>2</sup>), and the memory requirements scale linearly in the dataset size, <i>O</i>(<i>n</i> <span>⋅</span> <i>m</i>).</p><p>Finally, we found that pre-processing inputs can help performance, thus we can perform <i>z</i>-normalization of all inputs across the sample dimension and add an extra input for each cell that indicates whether the input was missing; the input itself is set to 0 in these cases. All inputs are finally linearly encoded into the embedding dimension of TabPFN.</p><h3 id="Sec20">Details on the causal generative process</h3><p>An SCM <span>\({\mathcal{G}}:= (Z,{\epsilon })\)</span> consists of a collection <i>Z</i> <span>≔</span> (<i>z</i><sub>1</sub>, …, <i>z</i><sub><i>k</i></sub>) of structural assignments (called mechanisms): <span>\({z}_{i}={f}_{i}({z}_{{\rm{PA}}{\mathcal{G}}(i)},{{\epsilon }}_{i})\,,\)</span> where <span>\({\rm{PA}}\,{\mathcal{G}}(i)\)</span> is the set of parents of node <i>i</i> (its direct causes) in the underlying directed acyclic graph (DAG) <span>\({\mathcal{G}}\)</span> (the causal graph), <i>f</i><sub><i>i</i></sub> is a (potentially nonlinear) deterministic function and <i>ϵ</i><sub><i>i</i></sub> is a noise variable. Causal relationships in <span>\({\mathcal{G}}\)</span> are represented by edges pointing from causes to effects<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR31" id="ref-link-section-d87306886e1864">31</a></sup>. As our prior is a sampling procedure, we can make a lot of choices on, for example, the graph size or complexity. By defining a probability distribution over these hyperparameters in the prior, the posterior predictive distribution approximated by TabPFN at inference time implicitly represents a Bayesian ensemble, jointly integrating over a weighted hyperparameter space. The specific hyperparameter ranges and sampling strategies are chosen to cover a diverse set of scenarios that we expect to encounter in real-world tabular data.</p><h4 id="Sec21">Graph structure sampling</h4><p>The structural causal models underlying each dataset are based on a DAG <span>\({\mathcal{G}}\)</span>. We sample these graphs using the growing network with redirection sampling method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d87306886e1893">57</a></sup>, a preferential attachment process that generates random scale-free networks. We either sample a single connected component or merge multiple disjoint subgraphs. Disjoint subgraphs lead to features that are marginally independent of the target if they are not connected to the target node, reflecting real-world scenarios with uninformative predictors.</p><p>To control the complexity of the sampled DAGs, we use two hyperparameters: the number of nodes <i>N</i> and the redirection probability <i>P</i>. <i>N</i> is sampled from a log-uniform distribution, <span>\(\log N \sim {\mathcal{U}}(a,b)\)</span>, where <i>a</i> and <i>b</i> are hyperparameters controlling the range of the graph size. The redirection probability <i>P</i> is sampled from a gamma distribution, <i>P</i> ~ <i>Γ</i>(<i>α</i>, <i>β</i>), where <i>α</i> and <i>β</i> are shape and rate parameters, respectively. Larger values of <i>N</i> yield graphs with more nodes, whereas smaller values of <i>P</i> lead to denser graphs with more edges on average<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. Phys. Rev. E 63, 066123 (2001)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR57" id="ref-link-section-d87306886e1987">57</a></sup>.</p><h4 id="Sec22">Computational edge mappings</h4><p>In our implementation, each SCM node and sample is represented as a vector in <span>\({{\mathbb{R}}}^{d}\)</span>. When propagating data through the SCM, the deterministic functions <i>f</i><sub><i>i</i></sub> at each edge map the input vectors to an output vector using four types of computational modules:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Small neural networks: here we initialize weight matrices <span>\(W\in {{\mathbb{R}}}^{d\times d}\)</span> using Xavier initialization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. 13th International Conference on Artificial Intelligence and Statistics 249–256 (JMLR, 2010)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR58" id="ref-link-section-d87306886e2084">58</a></sup> and apply a linear transformation <i>W</i><i>x</i> + <i>b</i> to the input vectors <span>\(x\in {{\mathbb{R}}}^{d}\)</span>, where <span>\(b\in {{\mathbb{R}}}^{d}\)</span> is a bias vector. After the linear projection, we apply element-wise nonlinear activation functions <span>\(\sigma :{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{d}\)</span>, randomly sampled from a set, including identity, logarithm, sigmoid, absolute value, sine, hyperbolic tangent, rank operation, squaring, power functions, smooth ReLU<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In Proc. 27th International Conference on Machine Learning (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR59" id="ref-link-section-d87306886e2221">59</a></sup>, step function and modulo operation.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Categorical feature discretization: to generate categorical features from the numerical vectors at each node, we map the vector to the index of the nearest neighbour in a set of per node randomly sampled vectors {<i>p</i><sub>1</sub>, …, <i>p</i><sub><i>K</i></sub>} for a feature with <i>K</i> categories. This discrete index will be observed in the feature set as a categorical feature. We sample the number of categories <i>K</i> from a rounded gamma distribution with an offset of 2 to yield a minimum number of classes of 2. To further use these discrete class assignments in the computational graph, they need to be embedded as continuous values. We sample a second set of embedding vectors <span>\(\{{p}_{1}^{{\prime} },\ldots ,{p}_{K}^{{\prime} }\}\)</span> for each class and transform the classes to these embeddings.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Decision trees: to incorporate structured, rule-based dependencies, we implement decision trees in the SCMs. At certain edges, we select a subset of features and apply decision boundaries on their values to determine the output<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Quinlan, J. R. Induction of decision trees. Mach. Learn. 1, 81–106 (1986)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR60" id="ref-link-section-d87306886e2331">60</a></sup>. The decision tree parameters (feature splits, thresholds) are randomly sampled per edge.</p>
                      
                    </li>
                    <li>
                      <span>4.</span>
                      
                        <p>Noise injection: at each edge, we add random normal noise from the normal distribution <span>\({\mathcal{N}}(0,{\sigma }^{2}I)\)</span>.</p>
                      
                    </li>
                  </ol><h4 id="Sec23">Initialization data sampling</h4><p>For each to-be-generated sample, we randomly generate initialization data <i>ϵ</i> that is inserted at the DAG root nodes and then propagated through the computational graph. The noise variables <i>ϵ</i> are generated according to one of three sampling mechanisms:</p><ol>
                    <li>
                      <span>1.</span>
                      
                        <p>Normal: <span>\({\epsilon } \sim {\mathcal{N}}(0,{\sigma }_{{\epsilon }}^{2})\)</span>, where <span>\({\sigma }_{{\epsilon }}^{2}\)</span> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>2.</span>
                      
                        <p>Uniform: <span>\({\epsilon } \sim {\mathcal{U}}(-a,a)\)</span>, where <i>a</i> is a hyperparameter.</p>
                      
                    </li>
                    <li>
                      <span>3.</span>
                      
                        <p>Mixed: for each root node, we randomly select either a normal or uniform distribution to sample the initialization noise <i>ϵ</i> from.</p>
                      
                    </li>
                  </ol><p>Furthermore, we sample input data with varying degrees of non-independence for some datasets. Here we first sample a random fraction <i>ρ</i> of samples to serve as prototypes <span>\({x}_{1}^{* },\ldots ,{x}_{M}^{* }\)</span>, where <i>M</i> = <i>ρ</i><i>n</i> and <i>n</i> is the dataset size. Then, for each input vector <i>x</i><sub><i>i</i></sub> to be sampled, we assign weights <i>α</i><sub><i>i</i><i>j</i></sub> to the prototypes and linearly mix the final input as</p><div id="Equ1"><p><span>$${x}_{i}=\mathop{\sum }\limits_{j=1}^{M}{\alpha }_{ij}{x}_{j}^{* },$$</span></p><p>
                    (1)
                </p></div><p>where ∑<sub><i>j</i></sub><i>α</i><sub><i>i</i><i>j</i></sub> = 1. The weights <i>α</i><sub><i>i</i><i>j</i></sub> are sampled from a multinomial distribution, <i>α</i><sub><i>i</i></sub> ~ Multinomial(<i>β</i>), where <i>β</i> is a temperature hyperparameter controlling the degree of non-independence: larger <i>β</i> yields more uniform weights, whereas smaller <i>β</i> concentrates the weights on fewer prototypes per sample.</p><h4 id="Sec24">Post-processing</h4><p>Each dataset is post-processed randomly with one or more of the following post-processings: (1) For some datasets, we use the Kumaraswamy feature warping, introducing nonlinear distortions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Kumaraswamy, P. A generalized probability density function for double-bounded random processes. J. Hydrol. 46, 79–88 (1980)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR33" id="ref-link-section-d87306886e2824">33</a></sup> to features as done in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In Proc. 40th International Conference on Machine Learning 25444–25470 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR61" id="ref-link-section-d87306886e2828">61</a></sup>. (2) We quantize some continuous features into buckets of randomly sampled cardinality <i>K</i>, mimicking binned or discretized features commonly encountered in datasets. We map a feature value <i>x</i> to the index of the bucket it falls into, determined by <i>K</i> + 1 bin edges sampled from the set of values this feature takes. (3) To introduce scenarios for dynamic imputation and handling of incomplete datasets, a common challenge in data science, we randomly designate a fraction <i>ρ</i><sub>miss</sub> of the data as missing according to the missing completely at random strategy. Each value is masked as missing with probability <i>ρ</i><sub>miss</sub>, independently of the data values.</p><h4 id="Sec25">Target generation</h4><p>To generate target labels for regression tasks, we select a randomly chosen continuous feature without post-processing. For classification labels, we select a random categorical feature that contains up to 10 classes. Thus, natively our method is limited to predicting at most 10 classes. This number can be increased by pre-training on datasets with a larger number of classes or by using approaches such as building a one-vs-one classifier, one-vs-rest classifier or building on approaches such as error-correcting output codes (ECOC)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. J. Artif. Intell. Res. 2, 263–286 (1994)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR62" id="ref-link-section-d87306886e2858">62</a></sup>.</p><h3 id="Sec26">Training details</h3><p>The training loss of any PFN is the cross-entropy between the targets of held-out samples of synthetic datasets and the model prediction. For a test set (<b>X</b><sub>test</sub>, <b><i>y</i></b><sub>test</sub>) = <i>D</i><sub>test</sub>, the training loss is given by <span>\({{\mathcal{L}}}_{{\rm{P}}{\rm{F}}{\rm{N}}}={{\bf{E}}}_{(({{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}})\cup {D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})\sim p(D)}[-\log {q}_{\theta }({{\boldsymbol{y}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}}|{{\boldsymbol{X}}}_{{\rm{t}}{\rm{e}}{\rm{s}}{\rm{t}}},{D}_{{\rm{t}}{\rm{r}}{\rm{a}}{\rm{i}}{\rm{n}}})]\)</span>. By minimizing this loss, the PFN learns to approximate the true Bayesian posterior predictive distribution for a chosen prior over datasets (and potentially their latent variables) <i>D</i>, as shown in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e3210">22</a></sup>.</p><p>We trained our final models for approximately 2,000,000 steps with a batch size of 64 datasets. That means the models used for TabPFN are trained on around 130,000,000 synthetically generated datasets each. One training run requires around 2 weeks on one node with eight Nvidia RTX 2080 Ti GPUs. We sample the number of training samples for each dataset uniformly up to 2,048 and use a fixed validation set size of 128. We sample the number of features using a beta distribution (<i>k</i> = 0.95, <i>b</i> = 8.0) that we linearly scale to the range 1–160. To avoid peaks in memory usage, the total size of each table was restricted to be below 75,000 cells by decreasing the number of samples for large numbers of features.</p><p>We chose the hyperparameters for the prior based on random searches, in which we use only a single GPU per training and evaluate on our development set, see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec6">Quantitative analysis</a>’. We used the Adam optimizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR, 2015)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR24" id="ref-link-section-d87306886e3229">24</a></sup> with linear warmup and cosine annealing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In Proc. 5th International Conference on Learning Representations (ICLR, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR63" id="ref-link-section-d87306886e3233">63</a></sup> and tested a set of learning rates in [0.0001, 0.0005], using the one with the lowest final training loss.</p><h3 id="Sec27">Inference details</h3><p>To get the most performance out of TabPFN, it is crucial to optimize its inference pipeline. We generally always apply TabPFN in a small ensemble, in which we perform pre-processing or post-processing of the data differently for each ensemble member.</p><p>As our models are not fully permutation invariant, for each ensemble member, we shuffle the feature order, approximating order invariance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d87306886e3249">64</a></sup>. For classification tasks, we additionally randomly permute the labels. We also apply a temperature to the softmax distribution of our model outputs for calibration.</p><p>Apart from the above, we use a subset of the following for each of our default ensemble members:</p><ol>
                  <li>
                    <span>1.</span>
                    
                      <p>Quantile + Id: we quantize the inputs to equally spaced values between 0 and 1, but keep a copy of each original feature. This effectively doubles the number of features passed to TabPFN.</p>
                    
                  </li>
                  <li>
                    <span>2.</span>
                    
                      <p>Category shuffling: the labels of categorical features with low cardinality are shuffled.</p>
                    
                  </li>
                  <li>
                    <span>3.</span>
                    
                      <p>SVD: an SVD compression of the features is appended to the features.</p>
                    
                  </li>
                  <li>
                    <span>4.</span>
                    
                      <p>Outlier removal: all outliers, more than 12 standard deviations from the mean, are removed.</p>
                    
                  </li>
                  <li>
                    <span>5.</span>
                    
                      <p>Power transform: each feature (or the label for regression) is transformed using a Yeo–Johnson transformation to stabilize the variance and make the data more normally distributed.</p>
                    
                  </li>
                  <li>
                    <span>6.</span>
                    
                      <p>One-hot encoding: categorical features are encoded using one-hot encoding, in which each category is represented as a binary vector.</p>
                    
                  </li>
                </ol><p>For PHE and hyperparameter tuning of TabPFN, we use a larger set of pre-processing techniques that additionally include a logarithmic, an exponential and a KDI transformation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="McCarter, C. The kernel density integral transformation. Transact. Mach. Learn. Res. 
                  https://openreview.net/pdf?id=6OEcDKZj5j
                  
                 (2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR65" id="ref-link-section-d87306886e3328">65</a></sup>. These transformations help address nonlinear relationships, skewed distributions and varying scales among features.</p><p>To calibrate prediction uncertainty, we apply a softmax temperature (default <i>T</i> = 0.9) by dividing logits before the softmax calculation:</p><div id="Equ2"><p><span>$$P({y}_{i}| x)=\frac{\exp ({z}_{i}/T)}{{\sum }_{j}\exp ({z}_{j}/T)},$$</span></p><p>
                    (2)
                </p></div><p>where <i>z</i><sub><i>i</i></sub> are the logits, <i>T</i> is the temperature and <i>P</i>(<i>y</i><sub><i>i</i></sub><span>∣</span><i>x</i>) is the calibrated probability. We offer the option to generate second-order polynomial features by multiplying up to 50 randomly selected feature pairs:</p><div id="Equ3"><p><span>$${f}_{ij}={x}_{i}\cdot {x}_{j},\quad \,{\rm{for}}\,(i,j)\in {\mathcal{S}},$$</span></p><p>
                    (3)
                </p></div><p>where <span>\({\mathcal{S}}\)</span> is the set of randomly chosen feature pairs. This can capture nonlinear interactions between features. This option is disabled by default. To ensure proper handling of duplicate samples given the sample permutation invariance of our architecture, we add a unique sample identifier feature. This is a random number drawn from a standard normal distribution, ensuring each sample is treated distinctly in the attention mechanism. We also provide an option for subsampling in each estimator, to increase ensemble diversity, which performs random sampling without replacement. This option is disabled by default.</p><h4 id="Sec28">Regression details</h4><p>To enable our model to do classification on a large range of scales and target distributions, we use the following approach. During pre-training, we rescale our regression targets to have zero mean and a standard deviation of 1 (<i>z</i>-score). To decide where the borders between our features lie, we draw a large sample of datasets from our prior and choose the 1/5,000 quantiles from this distribution. At inference time, we bring the real-world data to a similar range by again applying <i>z</i>-score normalization. Furthermore, we allow applying a range of transforms, including a power transform as part of our default. All of the transforms, including the <i>z</i>-score are inverted at prediction time by applying the inverse of the transform to the borders between buckets. This is equivalent to applying the inverse of the transform to the random variable represented by our output distribution but for the half-normals used on the sides for full support<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In Proc. The Tenth International Conference on Learning Representations (ICLR, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR22" id="ref-link-section-d87306886e3630">22</a></sup>. This is because all transforms are strictly monotone and the borders represent positions on the cumulative distribution function.</p><h4 id="Sec29">Data grouping based on random forest</h4><p>To perform well on very heterogeneous datasets, we also propose to use random trees to split the training data into smaller more homogeneous datasets. This technique is used only when performing HPO or PHE for TabPFN. It is especially useful for TabPFN as our model performs best on small datasets.</p><p>The pre-processing for a single ensemble member, that is, a single tree, works as follows: we use a standard random tree with feature and sample bootstrapping and Gini impurity loss. For each leaf node of the decision tree, we store the subset of training samples that fall into that node and train a TabPFN on these. To predict the class label for a test sample <i>x</i>, we determine the TabPFN to use by passing <i>x</i> through the decision tree. We set the minimal leaf size to be large (500–2,000) such that the resulting data groups are large enough to train a strong model.</p><h3 id="Sec30">TabPFN (PHE)</h3><p>To further enhance the inference performance of TabPFN, in TabPFN (PHE), we use PHE for a fixed portfolio of TabPFN configurations from our search space detailed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab5">5</a>. For TabPFN (PHE), we first use holdout validation to sequentially evaluate models from the portfolio until a time limit is reached. After all models are evaluated once, we repeat holdout validation with new data splits until the time limit is reached. Then, we ensemble all evaluated TabPFN models by aggregating their predictions with a weighted arithmetic mean. We learn the weights using greedy ensemble selection (GES)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) (Omnipress, 2004)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR42" id="ref-link-section-d87306886e3663">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In Proc. 6th IEEE International Conference on Data Mining (eds Clifton, C. et al.) 828–833 (IEEE, 2006)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR66" id="ref-link-section-d87306886e3666">66</a></sup> with 25 iterations on prediction data from holdout validation. Finally, we prune each zero-weighted model, refit all remaining models on all data and return the weighted average of their predictions.</p><p>Following standard practice in AutoML, we use GES because its predictive performance is often superior to the best individual model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d87306886e3673">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="#ref-CR67" id="ref-link-section-d87306886e3676">67</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In Proc. First International Conference on Automated Machine Learning (AutoML, 2022)." href="#ref-CR68" id="ref-link-section-d87306886e3676_1">68</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d87306886e3679">69</a></sup>. Owing to its ICL, we expect TabPFN to overfit the training data less than predictions of traditionally trained algorithms; thus, we opt for (repeated) holdout validation (as in Auto-Sklearn 1; ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges (eds Hutter, F. et al.) Ch. 6 (Springer, 2019)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR67" id="ref-link-section-d87306886e3683">67</a></sup>) instead of (repeated) cross-validation (as in AutoGluon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d87306886e3687">40</a></sup>). Moreover, as GES usually produces sparse weight vectors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In Proc. International Conference on Automated Machine Learning Vol. 224 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR43" id="ref-link-section-d87306886e3691">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In Proc. International Conference on Automated Machine Learning Vol. 224, 1–23 (PMLR, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR69" id="ref-link-section-d87306886e3694">69</a></sup>, we expect the final ensemble after pruning each zero-weighted model to consist of a smaller number of models than for other ensembling approaches, such as bagging. Consequently, PHE can also improve the inference efficiency of a TabPFN ensemble compared with other ensembling approaches.</p><h3 id="Sec31">Foundation model abilities</h3><h4 id="Sec32">Density estimation</h4><p>The combination of a regression and a classification TabPFN can be used as a generative model for tabular data, not only modelling targets but features as well. Let <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span> denote the original dataset, where <span>\({{\bf{x}}}_{i}\in {{\mathbb{R}}}^{d}\)</span> is a <i>d</i>-dimensional feature vector and <i>y</i><sub><i>i</i></sub> is the corresponding target value, and let <i>q</i><sub><i>θ</i></sub> represent our trained TabPFN model, either a regression or classification model depending on the target type. We aim to approximate the joint distribution of a new example and its label <span>\(p({\bf{x}},y| {\mathcal{D}})\)</span>. To do this, we factorize the joint distribution as</p><div id="Equ4"><p><span>$$p({\bf{x}},y| {\mathcal{D}})=\mathop{\prod }\limits_{j=1}^{d}p({x}_{j}| {{\bf{x}}}_{ &lt; j},{\mathcal{D}})\cdot p(\,y| {\bf{x}},{\mathcal{D}})$$</span></p><p>
                    (4)
                </p></div><div id="Equ5"><p><span>$$\approx \mathop{\prod }\limits_{j=1}^{d}{q}_{\theta }({x}_{j}| {{\boldsymbol{x}}}_{ &lt; j},{{\mathcal{D}}}_{:, &lt; j})\cdot {q}_{\theta }(\,y| {\boldsymbol{x}},{\mathcal{D}}),$$</span></p><p>
                    (5)
                </p></div><p>where we only condition on a subset of the features in the training set (<span>\({{\mathcal{D}}}_{:, &lt; j}\)</span>). The feature order of the joint density factorization influences the estimated densities. To reduce variance from this source, we apply a permutation sampling approximation of Janossy Pooling at inference time, in which we average the outputs of <i>N</i><sub><i>j</i></sub> feature permutations, with <i>N</i><sub><i>j</i></sub> = 24 in our experiments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In Proc. 7th International Conference on Learning Representations (ICLR, 2019)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR64" id="ref-link-section-d87306886e4262">64</a></sup>.</p><p>As we cannot condition on an empty feature set for technical reasons, we condition the prediction of the first feature <i>x</i><sub>1</sub>, on a feature with random noise, that is, no information.</p><p>The above factorization of the density of a sample (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Equ5">5</a>)) is completely tractable and we thus use it to estimate the likelihood for data points. This enables tasks such as anomaly detection and outlier identification.</p><h4 id="Sec33">Synthetic data generation</h4><p>We can leverage the generative abilities of TabPFN (see section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec32">Density estimation</a>’) to synthesize new tabular data samples that mimic the characteristics of a given real-world dataset, by simply following the factorization in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Equ5">5</a>) and sampling each feature step by step. The generated synthetic samples (<b>x</b><sup>*</sup>, <i>y</i><sup>*</sup>) can be used for various purposes, such as data augmentation, privacy-preserving data sharing and scenario simulation.</p><h4 id="Sec34">Embeddings</h4><p>TabPFN can be used to retrieve meaningful feature representations or embeddings. Given a dataset <span>\({\mathcal{D}}={\{({{\bf{x}}}_{i},{y}_{i})\}}_{i=1}^{N}\)</span>, the goal is to learn a mapping <span>\({f}_{\theta }:{{\mathbb{R}}}^{d}\to {{\mathbb{R}}}^{k}\)</span> that transforms the original <i>d</i>-dimensional feature vectors <b>x</b><sub><i>i</i></sub> into an embedding space of dimension <i>k</i>. The resulting embeddings <span>\({f}_{\theta }({{\bf{x}}}_{i})\in {{\mathbb{R}}}^{k}\)</span> capture the learned relationships between features and can be used for downstream tasks. To use TabPFN for this problem, we simply use the target-column representations of its final layer as embeddings.</p><h3 id="Sec35">Detailed evaluation protocol</h3><p>To rigorously assess the performance and robustness of TabPFN, we conduct a comprehensive quantitative evaluation on standard tabular dataset benchmarks, comparing against state-of-the-art baselines under a standardized protocol.</p><h4 id="Sec36">Default configuration of TabPFN</h4><p>Unlike traditional algorithms, in-context-learned algorithms do not have hyperparameters that directly control their training procedure. Instead, hyperparameters for inference of TabPFN only control the pre-processing of data and post-processing of predictions (for example, feature scaling or softmax temperature). Our default configuration (TabPFN (default)) for both classification and regression is optimized for accurate predictions with minimal fitting time. Here, we apply the same model multiple times with different pre- and post-processors and take the average over the predictions, yielding a four-way (eight-way for regression) ensemble. The settings for our data processing were obtained through a hyperparameter search optimized on our development datasets. The exact settings chosen are listed in Extended Data Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab5">5</a>. We emphasize that, as for other foundation models (such as GPT), we trained our TabPFN model once and used the same model to perform ICL in a forward pass on all new datasets.</p><h4 id="Sec37">Baselines</h4><p>We compare with tree-based methods, such as random forests<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR38" id="ref-link-section-d87306886e4575">38</a></sup>, XGBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR7" id="ref-link-section-d87306886e4579">7</a></sup>, CatBoost<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR9" id="ref-link-section-d87306886e4583">9</a></sup> and LightGBM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th International Conference on Advances in Neural Information Processing Systems (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR8" id="ref-link-section-d87306886e4587">8</a></sup>, the state of the art for experts to perform predictions on tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e4591">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e4594">15</a></sup>. We also compare with simpler methods, such as ridge regression<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 55–67 (1970)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR70" id="ref-link-section-d87306886e4599">70</a></sup>, logistic regression and SVMs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Cortes, C. &amp; Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR39" id="ref-link-section-d87306886e4603">39</a></sup>. Although standard neural networks, which unlike TabPFN do not use ICL, were shown to underperform for small (&lt;10,000 samples) tabular data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Borisov, V. et al. Deep neural networks and tabular data: a survey. IEEE Trans. Neural Netw. Learn. Syst. 35, 7499–7519 (2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR1" id="ref-link-section-d87306886e4607">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e4610">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. Inf. Fusion 81, 84–90 (2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR71" id="ref-link-section-d87306886e4613">71</a></sup>, as a point of reference, we still consider a simple neural network, the MLP.</p><h4 id="Sec38">Tabular dataset benchmarks</h4><p>We perform our analysis on two widely used and publicly available benchmark suites: the standard AutoML benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d87306886e4625">36</a></sup> and the recent regression benchmark OpenML-CTR23 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR37" id="ref-link-section-d87306886e4629">37</a></sup>). Both benchmarks comprise a diverse set of real-world tabular datasets, carefully curated to be representative of various domains and data characteristics. The authors of the benchmark suite selected these datasets based on criteria such as sufficient complexity, real-world relevance, absence of free-form text features and diversity of problem domains.</p><p>For our quantitative analysis of TabPFN for classification tasks, we use a set of test datasets comprising all 29 datasets from the AutoML benchmark with up to 10,000 samples, 500 features and 10 classes. For regression tasks, the AutoML benchmark contains only 16 datasets matching these constraints. To increase statistical power, we augmented this set with all datasets matching our constraints from the recent OpenML-CTR23 benchmark, yielding a test set of 28 unique regression datasets in total. Extended Data Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab3">3</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Tab4">4</a> provide full details for our test sets of classification and regression datasets, respectively.</p><p>We further evaluated additional benchmark suites from refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e4645">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e4648">15</a></sup>. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proc. 36th International Conference on Neural Information Processing Systems Vol. 35, 507–520 (ACM, 2022)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR14" id="ref-link-section-d87306886e4652">14</a></sup>, there are 22 tabular classification datasets selected based on criteria such as heterogeneous columns, moderate dimensionality and sufficient difficulty. In ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In Proc. 37th International Conference on Neural Information Processing System Vol. 36, 76336–76369 (ACM, 2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR15" id="ref-link-section-d87306886e4656">15</a></sup>, there is a collection of 176 classification datasets, representing one of the largest tabular data benchmarks. However, the curation process for these datasets may not be as rigorous or quality controlled as for AutoML Benchmark and OpenML-CTR23. We also evaluated five Kaggle competitions with less than 10,000 training samples from the latest completed Tabular Playground Series.</p><h4 id="Sec39">Development datasets</h4><p>To decide on the hyperparameters of TabPFN, as well as our hyperparameter search spaces, we considered another set of datasets, our development datasets. We carefully selected datasets to be non-overlapping with our test datasets described above. The list of development datasets can be found in Supplementary Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://ldirer.com/articles/s41586-024-08328-6#MOESM2">5</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://ldirer.com/articles/s41586-024-08328-6#MOESM2">6</a>. We considered the mean of normalized scores (ROC/RMSE) and rank quantiles and chose the best model configurations on these development datasets.</p><h4 id="Sec40">Metrics and cross-validation</h4><p>To obtain scores for classification tasks, we use two widely adopted evaluation metrics: ROC AUC (One-vs-Rest) and accuracy. ROC AUC averages performance over different sensitivity–specificity trade-offs, and accuracy measures the fraction of samples labelled correctly.</p><p>For regression tasks, we use <i>R</i><sup>2</sup> and negative RMSE as evaluation metrics. <i>R</i><sup>2</sup> represents the proportion of variance in the target column that the model can predict. RMSE is the root of the average squared magnitude of the errors between the predicted and actual values. As we use negative RMSE, for all our four metrics higher values indicate a better fit.</p><p>To increase statistical validity, for each dataset and method in our test datasets, we evaluated 10 repetitions, each with a different random seed and train–test split (90% train and 10% test samples; all methods used the same cross-validation splits, defined by OpenML<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. SIGKDD Explor. 15, 49–60 (2014)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR72" id="ref-link-section-d87306886e4697">72</a></sup>). We average the scores of all repetitions per dataset. Then, to average scores across datasets, we normalize per dataset following previous benchmarks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR36" id="ref-link-section-d87306886e4701">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at 
                  https://arxiv.org/abs/2003.06505
                  
                 (2020)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR40" id="ref-link-section-d87306886e4704">40</a></sup>. The absolute scores are linearly scaled such that a score of 1.0 corresponds to the highest value achieved by any method on that dataset, whereas a score of 0 represents the lowest result. This normalization allows for building meaningful averages across datasets with very different score ranges. We provide absolute performance numbers in Supplementary Data Tables <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://ldirer.com/articles/s41586-024-08328-6#MOESM1">1</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://ldirer.com/articles/s41586-024-08328-6#MOESM1">2</a>. All confidence intervals shown are 95% confidence intervals.</p><p>We tuned all methods with a random search using five-fold cross-validation with ROC AUC/RMSE up to a given time budget, ranging from half a minute to 4 h. The first candidate in the random search was the default setting supplied in the implementation of the method and was also used if not a single cross-validation run finished before the time budget was consumed. See the section ‘<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Sec5">Qualitative analysis</a>’ for the used search spaces per method. All methods were evaluated using 8 CPU cores. Moreover, TabPFN makes use of a 5-year-old consumer-grade GPU (RTX 2080 Ti). We also tested GPU acceleration for the baselines. However, as Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig8">2</a> shows, this did not improve performance, probably because of the small dataset sizes.</p></div></div></section>
                    
                </div><div>
                <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><p>All datasets evaluated are publicly available on <a href="https://openml.org">openml.org</a> or <a href="https://kaggle.com">kaggle.com</a>. We have provided scripts in our code repository that automate the process of downloading and evaluating the datasets. These scripts contain dataset identifiers, as well as exact data splitting and processing procedures.</p></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><p>Our code is available at <a href="https://priorlabs.ai/tabpfn-nature/">https://priorlabs.ai/tabpfn-nature/</a> (<a href="https://doi.org/10.5281/zenodo.13981285">https://doi.org/10.5281/zenodo.13981285</a>). We also provide an API that allows users to run TabPFN with minimal coding experience or without the availability of specific computing hardware such as a GPU. The code is designed to be modular and easily installable in a standard Python environment. The code to generate synthetic pre-training data has not been released with our models. We aim to enable researchers and practitioners to easily integrate TabPFN into their workflows and apply it to their specific tabular data tasks. We encourage users to provide feedback, report issues, and contribute to the further development of TabPFN. This open release aims to facilitate collaboration and accelerate the adoption and advancement of TabPFN in various research and application domains.</p></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Borisov, V. et al. Deep neural networks and tabular data: a survey. <i>IEEE Trans. Neural Netw. Learn. Syst.</i> <b>35</b>, 7499–7519 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TNNLS.2022.3229161" data-track-item_id="10.1109/TNNLS.2022.3229161" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTNNLS.2022.3229161" aria-label="Article reference 1" data-doi="10.1109/TNNLS.2022.3229161">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37015381" aria-label="PubMed reference 1">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1543.93250" aria-label="MATH reference 1">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20and%20tabular%20data%3A%20a%20survey&amp;journal=IEEE%20Trans.%20Neural%20Netw.%20Learn.%20Syst.&amp;doi=10.1109%2FTNNLS.2022.3229161&amp;volume=35&amp;pages=7499-7519&amp;publication_year=2024&amp;author=Borisov%2CV">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">van Breugel, B. &amp; van der Schaar, M. Position: why tabular foundation models should be a research priority. In <i>Proc. 41st International Conference on Machine Learning</i> 48976–48993 (PMLR, 2024).</p></li><li data-counter="3."><p id="ref-CR3">Silver, D. et al. Mastering the game of go with deep neural networks and tree search. <i>Nature</i> <b>529</b>, 484–489 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature16961" data-track-item_id="10.1038/nature16961" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature16961" aria-label="Article reference 3" data-doi="10.1038/nature16961">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016Natur.529..484S" aria-label="ADS reference 3">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://ldirer.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xhs12is7w%3D" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26819042" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0153.49402" aria-label="MATH reference 3">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20with%20deep%20neural%20networks%20and%20tree%20search&amp;journal=Nature&amp;doi=10.1038%2Fnature16961&amp;volume=529&amp;pages=484-489&amp;publication_year=2016&amp;author=Silver%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. <i>Nature</i> <b>596</b>, 583 – 589 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-021-03819-2" data-track-item_id="10.1038/s41586-021-03819-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-021-03819-2" aria-label="Article reference 4" data-doi="10.1038/s41586-021-03819-2">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34265844" aria-label="PubMed reference 4">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605" aria-label="PubMed Central reference 4">PubMed Central</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0849.90143" aria-label="MATH reference 4">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Highly%20accurate%20protein%20structure%20prediction%20with%20AlphaFold&amp;journal=Nature&amp;doi=10.1038%2Fs41586-021-03819-2&amp;volume=596&amp;publication_year=2021&amp;author=Jumper%2CJM">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">OpenAI. GPT-4 Technical Report. Preprint at <a href="https://arxiv.org/abs/2303.08774" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a> (2023).</p></li><li data-counter="6."><p id="ref-CR6">Friedman, J. H. Greedy function approximation: a gradient boosting machine. <i>Ann. Stat</i>. 1189–1232 (2001).</p></li><li data-counter="7."><p id="ref-CR7">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In <i>Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (eds Krishnapuram, B. et al.) 785–794 (ACM Press, 2016).</p></li><li data-counter="8."><p id="ref-CR8">Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017).</p></li><li data-counter="9."><p id="ref-CR9">Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. &amp; Gulin, A. CatBoost: unbiased boosting with categorical features. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Bengio, S. et al.) 6639–6649 (Curran Associates, 2018).</p></li><li data-counter="10."><p id="ref-CR10">Lowe, D. G. Distinctive image features from scale-invariant keypoints. <i>Int. J. Comput. Vis.</i> <b>60</b>, 91–110 (2004).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/B:VISI.0000029664.99615.94" data-track-item_id="10.1023/B:VISI.0000029664.99615.94" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="Article reference 10" data-doi="10.1023/B:VISI.0000029664.99615.94">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1066.68568" aria-label="MATH reference 10">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int.%20J.%20Comput.%20Vis.&amp;doi=10.1023%2FB%3AVISI.0000029664.99615.94&amp;volume=60&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In <i>Proc. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR</i>’<i>05)</i> 886–893 (IEEE, 2005).</p></li><li data-counter="12."><p id="ref-CR12">Vaswani, A. et al. Attention is all you need. In <i>Proc. 30th International Conference on Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) 6000–6010 (Curran Associates, 2017).</p></li><li data-counter="13."><p id="ref-CR13">Silver, D. et al. Mastering the game of go without human knowledge. <i>Nature</i> <b>550</b>, 354–359 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature24270" data-track-item_id="10.1038/nature24270" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature24270" aria-label="Article reference 13" data-doi="10.1038/nature24270">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017Natur.550..354S" aria-label="ADS reference 13">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://ldirer.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhs12ltLvM" aria-label="CAS reference 13">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29052630" aria-label="PubMed reference 13">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1443.97024" aria-label="MATH reference 13">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Mastering%20the%20game%20of%20go%20without%20human%20knowledge&amp;journal=Nature&amp;doi=10.1038%2Fnature24270&amp;volume=550&amp;pages=354-359&amp;publication_year=2017&amp;author=Silver%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="14."><p id="ref-CR14">Grinsztajn, L., Oyallon, E. &amp; Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In <i>Proc. 36th International Conference on Neural Information Processing Systems</i> Vol. 35, 507–520 (ACM, 2022).</p></li><li data-counter="15."><p id="ref-CR15">McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In <i>Proc. 37th International Conference on Neural Information Processing System</i> Vol. 36, 76336–76369 (ACM, 2024).</p></li><li data-counter="16."><p id="ref-CR16">Goodfellow, I., Bengio, Y. &amp; Courville, A. <i>Deep Learning</i> (MIT Press, 2016).</p></li><li data-counter="17."><p id="ref-CR17">Brown, T. et al. Language models are few-shot learners. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran Associates, 2020).</p></li><li data-counter="18."><p id="ref-CR18">Garg, S., Tsipras, D., Liang, P. S. &amp; Valiant, G. What can transformers learn in-context? A case study of simple function classes. In <i>Proc. Advances in Neural Information Processing Systems</i> Vol. 35, 30583–30598 (ACM, 2022).</p></li><li data-counter="19."><p id="ref-CR19">Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. &amp; Zhou, D. What learning algorithm is in-context learning? Investigations with linear models. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="20."><p id="ref-CR20">Von Oswald, J. et al. Transformers learn in-context by gradient descent. In <i>Proc. 40th International Conference on Machine Learning</i> 35151–35174 (PMLR, 2023).</p></li><li data-counter="21."><p id="ref-CR21">Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. In <i>Proc. The Twelfth International Conference on Learning Representations</i> (ICLR, 2024).</p></li><li data-counter="22."><p id="ref-CR22">Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. &amp; Hutter, F. Transformers can do Bayesian inference. In <i>Proc.</i> <i>The Tenth International Conference on Learning Representations</i> (ICLR, 2022).</p></li><li data-counter="23."><p id="ref-CR23">Hollmann, N., Müller, S., Eggensperger, K. &amp; Hutter, F. TabPFN: a transformer that solves small tabular classification problems in a second. In <i>Proc. The Eleventh International Conference on Learning Representations</i> (ICLR, 2023).</p></li><li data-counter="24."><p id="ref-CR24">Kingma, D. &amp; Ba, J. Adam: A method for stochastic optimization. In <i>Proc. International Conference on Learning Representations</i> (ICLR, 2015).</p></li><li data-counter="25."><p id="ref-CR25">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. In <i>Proc. 3rd International Conference on Learning Representations</i> (eds Bengio, Y. &amp; LeCun, Y.) (ICLR, 2015).</p></li><li data-counter="26."><p id="ref-CR26">Gorishniy, Y., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Revisiting deep learning models for tabular data. In <i>Proc. Advances in Neural Information Processing Systems 34</i> (eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021).</p></li><li data-counter="27."><p id="ref-CR27">Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) 43181–43204 (PMLR, 2023).</p></li><li data-counter="28."><p id="ref-CR28">Lorch, L., Sussex, S., Rothfuss, J., Krause, A. &amp; Schölkopf, B. Amortized inference for causal structure learning. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022).</p></li><li data-counter="29."><p id="ref-CR29">Dao, T., Fu, D., Ermon, S., Rudra, A. &amp; Ré, C. Flashattention: fast and memory-efficient exact attention with io-awareness. In <i>Proc. Advances in Neural Information Processing Systems</i> (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022).</p></li><li data-counter="30."><p id="ref-CR30">Torgo, L. &amp; Gama, J. Regression using classification algorithms. <i>Intell. Data Anal.</i> <b>1</b>, 275–292 (1997).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3233/IDA-1997-1405" data-track-item_id="10.3233/IDA-1997-1405" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3233%2FIDA-1997-1405" aria-label="Article reference 30" data-doi="10.3233/IDA-1997-1405">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.62065" aria-label="MATH reference 30">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Regression%20using%20classification%20algorithms&amp;journal=Intell.%20Data%20Anal.&amp;doi=10.3233%2FIDA-1997-1405&amp;volume=1&amp;pages=275-292&amp;publication_year=1997&amp;author=Torgo%2CL&amp;author=Gama%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="31."><p id="ref-CR31">Pearl, J. <i>Causality</i> 2nd edn (Cambridge Univ. Press, 2009).</p></li><li data-counter="32."><p id="ref-CR32">Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. Preprint at <a href="https://arxiv.org/abs/2401.06059" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2401.06059">https://arxiv.org/abs/2401.06059</a> (2024).</p></li><li data-counter="33."><p id="ref-CR33">Kumaraswamy, P. A generalized probability density function for double-bounded random processes. <i>J. Hydrol.</i> <b>46</b>, 79–88 (1980).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/0022-1694(80)90036-0" data-track-item_id="10.1016/0022-1694(80)90036-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2F0022-1694%2880%2990036-0" aria-label="Article reference 33" data-doi="10.1016/0022-1694(80)90036-0">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1980JHyd...46...79K" aria-label="ADS reference 33">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0253.73040" aria-label="MATH reference 33">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20generalized%20probability%20density%20function%20for%20double-bounded%20random%20processes&amp;journal=J.%20Hydrol.&amp;doi=10.1016%2F0022-1694%2880%2990036-0&amp;volume=46&amp;pages=79-88&amp;publication_year=1980&amp;author=Kumaraswamy%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="34."><p id="ref-CR34">Rosenblatt, F. <i>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</i>. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961).</p></li><li data-counter="35."><p id="ref-CR35">Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. <i>Philos. Trans. R. Soc. Lond.</i> <b>94</b>, 1–16 (1804).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1804RSPT...94....1Y" aria-label="ADS reference 35">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1032.68554" aria-label="MATH reference 35">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=I.%20The%20bakerian%20lecture.%20experiments%20and%20calculations%20relative%20to%20physical%20optics.&amp;journal=Philos.%20Trans.%20R.%20Soc.%20Lond.&amp;volume=94&amp;pages=1-16&amp;publication_year=1804&amp;author=Young%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="36."><p id="ref-CR36">Gijsbers, P. et al. AMLB: an AutoML benchmark. <i>J. Mach. Learn. Res.</i> <b>25</b>, 1–65 (2024).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=AMLB%3A%20an%20AutoML%20benchmark&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=25&amp;pages=1-65&amp;publication_year=2024&amp;author=Gijsbers%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="37."><p id="ref-CR37">Fischer, S. F., Feurer, M. &amp; Bischl, B. OpenML-CTR23 – a curated tabular regression benchmarking suite. In <i>Proc. AutoML Conference 2023 (Workshop)</i> (AutoML, 2023).</p></li><li data-counter="38."><p id="ref-CR38">Breimann, L. Random forests. <i>Mach. Learn.</i> <b>45</b>, 5–32 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/A:1010933404324" data-track-item_id="10.1023/A:1010933404324" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1010933404324" aria-label="Article reference 38" data-doi="10.1023/A:1010933404324">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1007.68152" aria-label="MATH reference 38">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20forests&amp;journal=Mach.%20Learn.&amp;doi=10.1023%2FA%3A1010933404324&amp;volume=45&amp;pages=5-32&amp;publication_year=2001&amp;author=Breimann%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="39."><p id="ref-CR39">Cortes, C. &amp; Vapnik, V. Support-vector networks. <i>Mach. Learn.</i> <b>20</b>, 273–297 (1995).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00994018" data-track-item_id="10.1007/BF00994018" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00994018" aria-label="Article reference 39" data-doi="10.1007/BF00994018">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0831.68098" aria-label="MATH reference 39">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Support-vector%20networks&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00994018&amp;volume=20&amp;pages=273-297&amp;publication_year=1995&amp;author=Cortes%2CC&amp;author=Vapnik%2CV">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. Preprint at <a href="https://arxiv.org/abs/2003.06505" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2003.06505">https://arxiv.org/abs/2003.06505</a> (2020).</p></li><li data-counter="41."><p id="ref-CR41">Wolpert, D. Stacked generalization. <i>Neural Netw.</i> <b>5</b>, 241–259 (1992).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0893-6080(05)80023-1" data-track-item_id="10.1016/S0893-6080(05)80023-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0893-6080%2805%2980023-1" aria-label="Article reference 41" data-doi="10.1016/S0893-6080(05)80023-1">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0792.68144" aria-label="MATH reference 41">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Stacked%20generalization&amp;journal=Neural%20Netw.&amp;doi=10.1016%2FS0893-6080%2805%2980023-1&amp;volume=5&amp;pages=241-259&amp;publication_year=1992&amp;author=Wolpert%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="42."><p id="ref-CR42">Caruana, R., Niculescu-Mizil, A., Crew, G. &amp; Ksikes, A. Ensemble selection from libraries of models. In <i>Proc. 21st International Conference on Machine Learning</i> (ed. Greiner, R.) (Omnipress, 2004).</p></li><li data-counter="43."><p id="ref-CR43">Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post hoc ensemble selection in AutoML. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224 (PMLR, 2023).</p></li><li data-counter="44."><p id="ref-CR44">Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5NC77" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5NC77">https://doi.org/10.24432/C5NC77</a> (1994).</p></li><li data-counter="45."><p id="ref-CR45">Duin, R. Multiple Features. UCI Machine Learning Repository <a href="https://doi.org/10.24432/C5HC70" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.24432/C5HC70">https://doi.org/10.24432/C5HC70</a> (1998).</p></li><li data-counter="46."><p id="ref-CR46">Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in medicine. <i>iScience</i> <b>25</b>, 105331 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.isci.2022.105331" data-track-item_id="10.1016/j.isci.2022.105331" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.isci.2022.105331" aria-label="Article reference 46" data-doi="10.1016/j.isci.2022.105331">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2022iSci...25j5331R" aria-label="ADS reference 46">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://ldirer.com/articles/cas-redirect/1:CAS:528:DC%2BB38XivVSitb%2FM" aria-label="CAS reference 46">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=36325058" aria-label="PubMed reference 46">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9619172" aria-label="PubMed Central reference 46">PubMed Central</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1471.91288" aria-label="MATH reference 46">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthetic%20data%20as%20an%20enabler%20for%20machine%20learning%20applications%20in%20medicine&amp;journal=iScience&amp;doi=10.1016%2Fj.isci.2022.105331&amp;volume=25&amp;publication_year=2022&amp;author=Rajotte%2CJ-F">
                    Google Scholar</a> 
                </p></li><li data-counter="47."><p id="ref-CR47">Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In <i>Proc.</i> <i>Advances in Neural Information Processing Systems</i> (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017).</p></li><li data-counter="48."><p id="ref-CR48">Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks. In <i>Proc.</i> <i>38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="49."><p id="ref-CR49">Helli, K., Schnurr, D., Hollmann, N., Müller, S. &amp; Hutter, F. Drift-resilient tabPFN: In-context learning temporal distribution shifts on tabular data. In <i>Proc. 38th Conference on Neural Information Processing Systems</i> (NeurIPS, 2024).</p></li><li data-counter="50."><p id="ref-CR50">Thomas, V. et al. Retrieval &amp; fine-tuning for in-context tabular models. In <i>Proc. 1st Workshop on In-Context Learning at the 41st International Conference on Machine Learning</i> (ICML, 2024).</p></li><li data-counter="51."><p id="ref-CR51">Nagler, T. Statistical foundations of prior-data fitted networks. In <i>Proc. 40th International Conference on Machine Learning</i> (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, 2023).</p></li><li data-counter="52."><p id="ref-CR52">Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. &amp; White, C. ForecastPFN: synthetically-trained zero-shot forecasting. In <i>Proc. 37th Conference on Advances in Neural Information Processing Systems</i> (eds Oh, A. et al.) (NeurIPS, 2023).</p></li><li data-counter="53."><p id="ref-CR53">Czolbe, S. &amp; Dalca, A. V. Neuralizer: General neuroimage analysis without re-training. In <i>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> 6217–6230 (IEEE, 2023).</p></li><li data-counter="54."><p id="ref-CR54">Wilcoxon, F. in <i>Breakthroughs in Statistics: Methodology and Distribution</i> (eds Kotz, S. &amp; Johnson, N. L.) 196–202 (Springer, 1992).</p></li><li data-counter="55."><p id="ref-CR55">Müller, A., Curino, C. &amp; Ramakrishnan, R. Mothernet: a foundational hypernetwork for tabular classification. Preprint at <a href="https://arxiv.org/abs/2312.08598" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2312.08598">https://arxiv.org/abs/2312.08598</a> (2023).</p></li><li data-counter="56."><p id="ref-CR56">Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at <a href="https://arxiv.org/abs/1911.02150" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1911.02150">https://arxiv.org/abs/1911.02150</a> (2019).</p></li><li data-counter="57."><p id="ref-CR57">Krapivsky, P. L. &amp; Redner, S. Organization of growing random networks. <i>Phys. Rev. E</i> <b>63</b>, 066123 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1103/PhysRevE.63.066123" data-track-item_id="10.1103/PhysRevE.63.066123" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1103%2FPhysRevE.63.066123" aria-label="Article reference 57" data-doi="10.1103/PhysRevE.63.066123">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2001PhRvE..63f6123K" aria-label="ADS reference 57">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://ldirer.com/articles/cas-redirect/1:STN:280:DC%2BD38%2FhsF2isA%3D%3D" aria-label="CAS reference 57">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1109.92301" aria-label="MATH reference 57">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Organization%20of%20growing%20random%20networks&amp;journal=Phys.%20Rev.%20E&amp;doi=10.1103%2FPhysRevE.63.066123&amp;volume=63&amp;publication_year=2001&amp;author=Krapivsky%2CPL&amp;author=Redner%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="58."><p id="ref-CR58">Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In <i>Proc. 13th International Conference on Artificial Intelligence and Statistics</i> 249–256 (JMLR, 2010).</p></li><li data-counter="59."><p id="ref-CR59">Nair, V. &amp; Hinton, G. Rectified linear units improve restricted Boltzmann machines. In <i>Proc. 27th International Conference on Machine Learning</i> (eds Fürnkranz, J. &amp; Joachims, T.) 807–814 (Omnipress, 2010).</p></li><li data-counter="60."><p id="ref-CR60">Quinlan, J. R. Induction of decision trees. <i>Mach. Learn.</i> <b>1</b>, 81–106 (1986).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF00116251" data-track-item_id="10.1007/BF00116251" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF00116251" aria-label="Article reference 60" data-doi="10.1007/BF00116251">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0887.73077" aria-label="MATH reference 60">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Induction%20of%20decision%20trees&amp;journal=Mach.%20Learn.&amp;doi=10.1007%2FBF00116251&amp;volume=1&amp;pages=81-106&amp;publication_year=1986&amp;author=Quinlan%2CJR">
                    Google Scholar</a> 
                </p></li><li data-counter="61."><p id="ref-CR61">Müller, S., Feurer, M., Hollmann, N. &amp; Hutter, F. PFNS4BO: in-context learning for Bayesian optimization. In <i>Proc. 40th International Conference on Machine Learning</i> 25444–25470 (PMLR, 2023).</p></li><li data-counter="62."><p id="ref-CR62">Dietterich, T. G. &amp; Bakiri, G. Solving multiclass learning problems via error-correcting output codes. <i>J. Artif. Intell. Res.</i> <b>2</b>, 263–286 (1994).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1613/jair.105" data-track-item_id="10.1613/jair.105" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1613%2Fjair.105" aria-label="Article reference 62" data-doi="10.1613/jair.105">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0900.68358" aria-label="MATH reference 62">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20multiclass%20learning%20problems%20via%20error-correcting%20output%20codes&amp;journal=J.%20Artif.%20Intell.%20Res.&amp;doi=10.1613%2Fjair.105&amp;volume=2&amp;pages=263-286&amp;publication_year=1994&amp;author=Dietterich%2CTG&amp;author=Bakiri%2CG">
                    Google Scholar</a> 
                </p></li><li data-counter="63."><p id="ref-CR63">Loshchilov, I. &amp; Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In <i>Proc. 5th International Conference on Learning Representations</i> (ICLR, 2017).</p></li><li data-counter="64."><p id="ref-CR64">Murphy, R. L., Srinivasan, B., Rao, V. A. &amp; Ribeiro, B. Janossy pooling: learning deep permutation-invariant functions for variable-size inputs. In <i>Proc. 7th International Conference on Learning Representations</i> (ICLR, 2019).</p></li><li data-counter="65."><p id="ref-CR65">McCarter, C. The kernel density integral transformation. <i>Transact. Mach. Learn. Res.</i> <a href="https://openreview.net/pdf?id=6OEcDKZj5j" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://openreview.net/pdf?id=6OEcDKZj5j">https://openreview.net/pdf?id=6OEcDKZj5j</a> (2023).</p></li><li data-counter="66."><p id="ref-CR66">Caruana, R., Munson, A. &amp; Niculescu-Mizil, A. Getting the most out of ensemble selection. In <i>Proc. 6th IEEE International Conference on Data Mining</i> (eds Clifton, C. et al.) 828–833 (IEEE, 2006).</p></li><li data-counter="67."><p id="ref-CR67">Feurer, M. et al. in <i>Automated Machine Learning: Methods, Systems, Challenges</i> (eds Hutter, F. et al.) Ch. 6 (Springer, 2019).</p></li><li data-counter="68."><p id="ref-CR68">Purucker, L. &amp; Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles in AutoML with OpenML. In <i>Proc. First International Conference on Automated Machine Learning</i> (AutoML, 2022).</p></li><li data-counter="69."><p id="ref-CR69">Purucker, L. &amp; Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and salvageable failure. In <i>Proc. International Conference on Automated Machine Learning</i> Vol. 224, 1–23 (PMLR, 2023).</p></li><li data-counter="70."><p id="ref-CR70">Hoerl, A. E. &amp; Kennard, R. W. Ridge regression: biased estimation for nonorthogonal problems. <i>Technometrics</i> <b>12</b>, 55–67 (1970).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1080/00401706.1970.10488634" data-track-item_id="10.1080/00401706.1970.10488634" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F00401706.1970.10488634" aria-label="Article reference 70" data-doi="10.1080/00401706.1970.10488634">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0202.17205" aria-label="MATH reference 70">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 70" href="http://scholar.google.com/scholar_lookup?&amp;title=Ridge%20regression%3A%20biased%20estimation%20for%20nonorthogonal%20problems&amp;journal=Technometrics&amp;doi=10.1080%2F00401706.1970.10488634&amp;volume=12&amp;pages=55-67&amp;publication_year=1970&amp;author=Hoerl%2CAE&amp;author=Kennard%2CRW">
                    Google Scholar</a> 
                </p></li><li data-counter="71."><p id="ref-CR71">Shwartz-Ziv, R. &amp; Armon, A. Tabular data: deep learning is not all you need. <i>Inf. Fusion</i> <b>81</b>, 84–90 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2021.11.011" data-track-item_id="10.1016/j.inffus.2021.11.011" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2021.11.011" aria-label="Article reference 71" data-doi="10.1016/j.inffus.2021.11.011">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 71" href="http://scholar.google.com/scholar_lookup?&amp;title=Tabular%20data%3A%20deep%20learning%20is%20not%20all%20you%20need&amp;journal=Inf.%20Fusion&amp;doi=10.1016%2Fj.inffus.2021.11.011&amp;volume=81&amp;pages=84-90&amp;publication_year=2022&amp;author=Shwartz-Ziv%2CR&amp;author=Armon%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="72."><p id="ref-CR72">Vanschoren, J., van Rijn, J. N., Bischl, B. &amp; Torgo, L. OpenML: networked science in machine learning. <i>SIGKDD Explor.</i> <b>15</b>, 49–60 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1145/2641190.2641198" data-track-item_id="10.1145/2641190.2641198" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1145%2F2641190.2641198" aria-label="Article reference 72" data-doi="10.1145/2641190.2641198">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1505.62090" aria-label="MATH reference 72">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 72" href="http://scholar.google.com/scholar_lookup?&amp;title=OpenML%3A%20networked%20science%20in%20machine%20learning&amp;journal=SIGKDD%20Explor.&amp;doi=10.1145%2F2641190.2641198&amp;volume=15&amp;pages=49-60&amp;publication_year=2014&amp;author=Vanschoren%2CJ&amp;author=Rijn%2CJN&amp;author=Bischl%2CB&amp;author=Torgo%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="73."><p id="ref-CR73">Fix, E. &amp; Hodges, J. L. Discriminatory analysis. Nonparametric discrimination: consistency properties. <i>Int. Stat. Rev.</i> <b>57</b>, 238–247 (1989).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.2307/1403797" data-track-item_id="10.2307/1403797" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2307%2F1403797" aria-label="Article reference 73" data-doi="10.2307/1403797">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0715.62080" aria-label="MATH reference 73">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 73" href="http://scholar.google.com/scholar_lookup?&amp;title=Discriminatory%20analysis.%20Nonparametric%20discrimination%3A%20consistency%20properties&amp;journal=Int.%20Stat.%20Rev.&amp;doi=10.2307%2F1403797&amp;volume=57&amp;pages=238-247&amp;publication_year=1989&amp;author=Fix%2CE&amp;author=Hodges%2CJL">
                    Google Scholar</a> 
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>We express our gratitude to the following individuals for their valuable contributions and support. We thank E. Bergman for his assistance with the evaluation of TabPFN, for helping implement the random forest pre-processing, and for his efforts in improving the code quality and documentation. His contributions were instrumental in benchmarking TabPFN and ensuring the reproducibility of our results. We thank A. Gupta and D. Otte for their work on the Inference Server, which enables the fast deployment of TabPFN without the need for a local GPU. Their efforts have greatly enhanced the accessibility and usability of TabPFN. We thank L. Schweizer for his work on exploring the random forest pre-processing for TabPFN further. We thank D. Schnurr and K. Helli for their work on visualization, and D. Schnurr for his specific contributions related to handling missing values. We thank S. M. Lundberg for the collection of visualization methods for feature attribution that we adapted for our work. We thank A. Müller for the insightful discussions related to TabPFN training and for his guidance on identifying and mitigating biases in the prior. His expertise has been invaluable in refining the TabPFN methodology. We are very grateful to C. Langenberg and M. Pietzner for providing insights on medical applications, interpreting model results and offering general advice. Their continued support has been instrumental in shaping this work. We thank S. Stäglich for his outstanding maintenance and support with the cluster infrastructure. We thank B. Lake for his general paper writing advice. We are grateful for the computational resources that were available for this research. Specifically, we acknowledge support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant no. 417962828. We acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant no. 499552394, and by the European Union (through ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. F.H. acknowledges the financial support of the Hector Foundation.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Noah Hollmann, Samuel Müller</p></li></ol><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Machine Learning Lab, University of Freiburg, Freiburg, Germany</p><p>Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo &amp; Frank Hutter</p></li><li id="Aff2"><p>Computational Medicine, Berlin Institute of Health at Charité, Universitätsmedizin Berlin, Berlin, Germany</p><p>Noah Hollmann</p></li><li id="Aff3"><p>Prior Labs, Freiburg, Germany</p><p>Noah Hollmann &amp; Frank Hutter</p></li><li id="Aff4"><p>Neuromedical AI Lab, Department of Neurosurgery, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff5"><p>Medical Physics, Department of Diagnostic and Interventional Radiology, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany</p><p>Robin Tibor Schirrmeister</p></li><li id="Aff6"><p>ELLIS Institute Tübingen, Tübingen, Germany</p><p>Frank Hutter</p></li></ol><h3 id="contributions">Contributions</h3><p>N.H. improved the prior of the model; added regression support, unsupervised capabilities and inference optimizations; and contributed to the experiments and wrote the paper. S.M. improved the neural network architecture, training and efficiency; added inference optimizations; and contributed to experiments and wrote the paper. L.P. improved the inference interface of the model; contributed to hyperparameter tuning; added post hoc ensembling of TabPFN models; contributed to benchmarking; and wrote the paper. A.K. added inference optimizations and Kaggle experiments. M.K. contributed to inference optimizations. S.B.H. contributed to the usability of our code. R.T.S. contributed to preliminary architectural experiments to speed up inference and helped revise the first draft of the paper. F.H. contributed technical advice and ideas, contributed to the random forest pre-processing, managed collaborations and funding, and wrote the paper.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:noah@priorlabs.ai">Noah Hollmann</a>, <a id="corresp-c2" href="mailto:samuelgabrielmuller@gmail.com">Samuel Müller</a> or <a id="corresp-c3" href="mailto:fh@cs.uni-freiburg.de">Frank Hutter</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interests</h3>
                <p>The following patent applications invented by S.M. and F.H. and filed by R. Bosch are related to this work: DE202021105192U1 and DE102021210775A1. The authors do not have any ownership rights to these patent applications. F.H. and N.H. are affiliated with PriorLabs, a company focused on developing tabular foundation models. The authors declare no other competing interests.</p>
              
            </div></div></section><section data-title="Peer review"><div id="peer-review-section"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Nature</i> thanks Duncan McElfresh, Oleksandr Shchur and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
              
            </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Extended data figures and tables"><div id="Sec42-section"><h2 id="Sec42">Extended data figures and tables</h2><div id="Sec42-content"><div data-test="supplementary-info"><div data-test="supp-item" id="Fig7"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 performance comparison across" href="https://ldirer.com/articles/s41586-024-08328-6/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig7_ESM.jpg">Extended Data Fig. 1 Performance comparison across additional dataset characteristics, extending Fig. </a><a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://ldirer.com/articles/s41586-024-08328-6#Fig5">5</a>.</h3><p>This figure shows the relative performance of different methods when datasets are split based on specific attributes. Error bars represent 95% confidence intervals. While performance differences are generally subtle across these splits, the most notable variation is observed for datasets with outliers in the target variable, though confidence intervals still overlap.</p></div><div data-test="supp-item" id="Fig8"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 performance comparisons of ta" href="https://ldirer.com/articles/s41586-024-08328-6/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig8_ESM.jpg">Extended Data Fig. 2 Performance comparisons of TabPFN and baselines on additional benchmark datasets and with GPU support.</a></h3><p>(a) Classification performance on the Grinsztajn medium-sized benchmark with categorical features, across 7 datasets. (b) Classification performance on the Grinsztajn medium-sized benchmark with numerical features, across its 15 datasets. (c) Classification performance on the TabZilla benchmark, consisting of 102 datasets with fewer than 10,000 rows of data, 500 features, and 10 classes. Duplicated datasets and those with fewer than 5 samples per class were removed to enable 5-fold cross-validation. (d) Performance Over Time Comparison with CPU vs. GPU Hardware: The performance over time when running our strongest baselines with eight CPUs (CPU) vs. eight CPUs and on one GPU (+GPU) on our classification test benchmark. AutoGluon automatically decides which models to train with what resources. For CatBoost and XGB, we specified that the models should train with GPU. Intervals represent 95% CI.</p></div><div data-test="supp-item" id="Fig9"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 comparing shap (shapley addit" href="https://ldirer.com/articles/s41586-024-08328-6/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig9_ESM.jpg">Extended Data Fig. 3 Comparing SHAP (SHapley Additive exPlanations) summary plots between TabPFN and baselines.</a></h3><p>We compare SHAP feature importance and impact for Logistic Regression, TabPFN, and CatBoost on the “Default of Credit Card Clients” dataset. The top features visualized are credit amount, age, and duration. Each point represents a single instance, with the color indicating the value of the checking status feature (blue for low, red for high), illustrating its interaction with the respective feature on the x-axis. We see that Logistic Regression is most interpretable due to the simple underlying functions. However, Logistic Regression has poor predictive accuracy, and the learned functions are unintuitive when looking at the outer bounds of features. TabPFN has good predictive accuracy and learns simple, interpretable functions. CatBoost is the least interpretable, with unclear patterns and wide variation in SHAP values per sample. This figure is adapted from Lundberg et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lundberg, S. M. &amp; Lee, S.-I. A unified approach to interpreting model predictions. In Proc. Advances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 (Curran Associates, 2017)." href="https://ldirer.com/articles/s41586-024-08328-6#ref-CR47" id="ref-link-section-d87306886e4935">47</a></sup>.</p></div><div data-test="supp-item" id="Fig10"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 finetuning tabpfn on 2-dimens" href="https://ldirer.com/articles/s41586-024-08328-6/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_Fig10_ESM.jpg">Extended Data Fig. 4 Finetuning TabPFN on 2-dimensional sine curve datasets.</a></h3><p>(a) Examples of 2D sine curve datasets with different offsets. (b) Finetuning loss curves for 50 runs with random train-test offsets. Colors indicate the offset between train and test. TabPFN shows positive transfer, with better performance for more similar distributions. For a dataset shift of <i>π</i>, the inverse label needs to be predicted in the test set, compared to the finetuning data. However, TabPFN still generalizes when finetuned on this data.</p></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption><b id="Tab1" data-test="table-caption">Extended Data Table 1 Aggregated results on the 29 AMLB classification Benchmark datasets</b></figcaption></figure></div></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption><b id="Tab2" data-test="table-caption">Extended Data Table 2 Aggregated results on the 28 AMLB and OpenML-CTR23 regression Benchmark datasets</b></figcaption></figure></div></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption><b id="Tab3" data-test="table-caption">Extended Data Table 3 List of test datasets used for primary evaluation of classification tasks</b></figcaption></figure></div></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption><b id="Tab4" data-test="table-caption">Extended Data Table 4 List of test datasets used for primary evaluation of regression tasks</b></figcaption></figure></div></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption><b id="Tab5" data-test="table-caption">Extended Data Table 5 Hyperparameter defaults and search space for TabPFN and our baselines</b></figcaption></figure></div></div><div data-test="supp-item"><div data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption><b id="Tab6" data-test="table-caption">Extended Data Table 6 Performance on Kaggle Data Science Challenges</b></figcaption></figure></div></div></div></div></div></section><section data-title="Supplementary information"><div id="Sec43-section"><h2 id="Sec43">Supplementary information</h2><div id="Sec43-content"><div data-test="supplementary-info"><div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 1–4" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Tables 1–4</a></h3><p> Unnormalized per dataset results: per dataset ROC AUC scores for our model and baselines on the four evaluated benchmarks.</p></div><div data-test="supp-item" id="MOESM2"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary tables 5 and 6" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-024-08328-6/MediaObjects/41586_2024_8328_MOESM2_ESM.pdf" data-supp-info-image="">Supplementary Tables 5 and 6</a></h3><p>Meta-information on development datasets: meta-information of the development dataset is used to validate the performance of our models for regression and classification.</p></div></div></div></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Accurate%20predictions%20on%20small%20data%20with%20a%20tabular%20foundation%20model&amp;author=Noah%20Hollmann%20et%20al&amp;contentID=10.1038%2Fs41586-024-08328-6&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2025-01-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41586-024-08328-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-024-08328-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Hollmann, N., Müller, S., Purucker, L. <i>et al.</i> Accurate predictions on small data with a tabular foundation model.
                    <i>Nature</i> <b>637</b>, 319–326 (2025). https://doi.org/10.1038/s41586-024-08328-6</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-024-08328-6?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-05-17">17 May 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2024-10-31">31 October 2024</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2025-01-08">08 January 2025</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2025-01-09">09 January 2025</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-024-08328-6</span></p></li></ul></div></div></div></div></section>
            </div></div>
  </body>
</html>
