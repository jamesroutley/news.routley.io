<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reasonablypolymorphic.com/blog/solving-lcsa/">Original</a>
    <h1>Using obscure graph theory to solve programming languages problems</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>Usually I write about <em>solutions</em> to problems I’ve worked out, but I’ve found myself increasingly becoming interesting in <em>where solutions come from.</em> Maybe it’s because I’ve been reading Boorstin’s excellent <a href="https://en.wikipedia.org/wiki/The_Discoverers">The Discoverers</a>, which I’d strongly recommend.</p>
<p>Regardless of why, I thought I’d switch up the usual dance step today, and discuss what solving my most-recent-big-problem actually looked like, in terms of what I tried, where I looked, and what the timeline was.</p>
<h2 id="the-problem">The Problem</h2>
<p>The problem is to serialize a program graph into a series of let-bindings. For example, given the following graph:</p>
<pre><code>      +
    /   \
  f ---&gt; g
  |     / \
  a     \ /
      expensive</code></pre>
<p>which represents the program:</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>f a (g expensive expensive) <span>+</span> g expensive expensive</span></code></pre></div>
<p>Unfortunately, this is a naive representation of the program, since it duplicates the work required to compute <code>expensive</code> four times, and <code>g expensive expensive</code> twice. Instead, we would prefer to generate the equivalent-but-more-efficient program:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>let</span> <span>$</span><span>0</span> <span>=</span> expensive</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span>$</span><span>1</span> <span>=</span> g <span>$</span><span>0</span> <span>$</span><span>0</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a> <span>in</span> f a <span>$</span><span>1</span> <span>+</span> <span>$</span><span>1</span></span></code></pre></div>
<p>This transformation is affectionately known as <em>sharing</em>, since it shares the computed answer whenever there is repeated work to be done.</p>
<p>So this is what we’re trying to do. Given the original graph, determine the best place to insert these let-bindings, for some reasonable definition of “best.” We can assume there are no side effects involved, so any place that an expression is well-scoped is an acceptable solution.</p>
<p>In order to understand some of my attempted solutions, it’s worth noting that our final solution should build something of type <code>Expr</code>, and the original graph is represented as a <code>IntMap (ExprF Int)</code>. <code>ExprF</code> is the <a href="https://hackage.haskell.org/package/recursion-schemes-5.2.3/docs/Data-Functor-Foldable.html#t:Base"><code>Base</code></a> functor of <code>Expr</code>, with all of its self-references replaced by some type variable, in this case <code>Int</code>. Thus, the graph above looks much more like:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>_ <span>:</span> <span>IntMap</span> (<span>ExprF</span> <span>Int</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>_ <span>=</span> IM.fromList</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  [ (<span>0</span>, <span>Apply</span> <span>&#34;+&#34;</span> [<span>1</span>, <span>3</span>])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  , (<span>1</span>, <span>Apply</span> <span>&#34;f&#34;</span> [<span>2</span>, <span>3</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  , (<span>2</span>, <span>...</span>)  <span>-- a</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  , (<span>3</span>, <span>Apply</span> <span>&#34;g&#34;</span> [<span>4</span>, <span>4</span>])</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  , (<span>4</span>, <span>...</span>)  <span>-- expensive</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  ]</span></code></pre></div>
<h2 id="the-original-solution">The Original Solution</h2>
<p>I spent over a year trying to solve this problem, with various mostly-working solutions during that time. My strategy here was to think really hard, write up some algorithm that seemed plausible, and then run it against our (small) battery of integration tests to make sure it got the same answer as before.</p>
<p>Why not property test it? I tried, but found it very challenging to implement well-typed generators that would reliably introduce shared thunks. But maybe there’s a different lesson to be learned here about writing good generators.</p>
<p>Anyway. For eight months, one of these think-really-hard algorithms fit the bill and didn’t give us any problems. It was a weird, bespoke solution to the problem that independetly kept track of all of the free variables in every graph fragment, and tried to let-bind a fragment as soon as we landed in a context where all of the free variables were in scope. It seemed to work, but it was extremely messy and unmaintainable.</p>
<p>At the time of writing, this sharing algorithm was the only source of let-binds in our entire language, which meant that it didn’t need to account for let-binds <em>in</em> the program.</p>
<p>Of course, that invariant eventually changed. We added a way in the source langauge to introduce <code>let</code>s, which meant my algorithm was wrong. And I had written it sufficiently long ago that I no longer remembered <em>exactly why it worked.</em> Which meant the <a href="https://pages.cs.wisc.edu/~remzi/Naur.pdf">theory of my program was lost, and thus that we ought to rewrite it.</a></p>
<h2 id="unfolding-a-solution">Unfolding a Solution</h2>
<p>I went back to the problem statement, and stared at it for a long time (back to the think-really-hard algorithm!) Upon staring at the problem, I realized that what I was really trying to do was determine where diamond patterns arose in the propgram graph.</p>
<p>Recall our original graph:</p>
<pre><code>      +
    /   \
  f ---&gt; g
  |     / \
  a     \ /
      expensive</code></pre>
<p>If we redraw it such that <code>g</code> is on a different rank than <code>f</code>, then the two diamond patterns become much clearer:</p>
<pre><code>      +
    /  \
  f     |
  | \   |
  a  \ /
      g
     / \
     \ /
   expensive</code></pre>
<p>The insight I came up with is that if a node <code>n</code> is the source of a diamond, then we must let-bind the sink of the diamond immediately before inlining the definition of <code>n</code>.</p>
<p>This gives rise to the question of “how do we identify a diamond?” What we can do is give a mapping from each node to its reachable set of nodes. For example, in the above, we’d compute the map:</p>
<pre><code>+         -&gt; {+, f, a, g, expensive}
f         -&gt; {f, a, g, expensive}
a         -&gt; {a}
g         -&gt; {g, expensive}
expensive -&gt; {expensive}</code></pre>
<p>Then when we go to inline a node, say, <code>+</code>, we can look for any nodes that are reachable via more than one of its immediate subterms. Since the immediate subterms of <code>+</code> are <code>f</code> and <code>g</code>, we can take the intersections of their reachable sets:</p>
<pre><code>{f, a, g, expensive} union {g, expensive}</code></pre>
<p>giving us</p>
<pre><code>{g, expensive}</code></pre>
<p>which is exactly the set of nodes that we need to perform sharing on. If you topologically sort this set, it gives you the order that you should perform your let bindings.</p>
<p>EXCEPT there’s a kink in the whole thing. What happens if one of the terms in this diamond contains free variables? In particular, we might have something like this:</p>
<pre><code>      +
    /  \
  f     |
  | \   |
  a  \ /
      λx
     / \
     \ /
   expensive
      |
      x</code></pre>
<p>This gives us an analogous set of reachable nodes when we look at <code>+</code>, but we obviously can’t lift <code>expensive x</code> above the lambda.</p>
<p>Resolving this problem required giving up on the notion of memoizing the entire reachable set of nodes, and to instead crawl the graph ensuring that everything is well-scoped.</p>
<h2 id="performance-woes">Performance Woes</h2>
<p>My algorithm looked fine, and, importantly, got the right answer in a reasonable amount of time on our (small) battery of integration tests. So I shipped it, commended myself on a job well done, and thought nothing more about it. For about a week, until a bug report came in saying that our compiler now seemed to hang on big programs.</p>
<p>Which was something I hadn’t noticed, since we didn’t have any big programs in our integration tests.</p>
<p>Damn!</p>
<p>Upon digging in to what exactly was so slow, I noticed that my algorithm was <a href="https://accidentallyquadratic.tumblr.com/">accidentally quadratic</a>. I needed to fold over every node in the graph, and that required looking at the entire reachable set underneath it. I had put in some of the obvious safeguards, hoping that they would prune the search tree early, but it wasn’t enough sacrifice for the Great God of Asymptotes.</p>
<p>Did I mention that at this point in the story, having this algorithm working fast was on the critical path of the company? Everybody else was blocked on me figuring this out. Talk about pressure!</p>
<p>Anyway. You’ll notice above that in my description of the algorithm, everything sounds fine. But the juice is in the details, as the common saying goes. Computing reachability isn’t quite the right thing to be using here, as it gave us the wrong answer for the lambda example above. Which is unfortunate because reachability is something we can do in linear time.</p>
<p>And then when reachability didn’t work, I just threw away the fast performance and hoped my bespoke algorithm would do the job. My only redemption comes from the fact that at least it got the right answer, even if it did so very slowly.</p>
<h2 id="finding-the-kernel">Finding the Kernel</h2>
<p>Back to the drawing board.</p>
<p>Whenever I have graph theory problems, I call up my boy Vikrem. He’s good at nerd stuff like this.</p>
<p>We <a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubberducked</a> the problem, and tried to reframe the problem in the language of graph theory. We had a Merkiv–Maguire moment where we indepdently realized that the goal was somehow related to finding the <em>lowest common ancestor</em> (LCA) of a node.</p>
<p>Which is to say, roughly, that we are looking for forks in the diamond diagram. Which we already knew, but it was nice to have some language for.</p>
<p>Our new problem is that LCA is defined only over trees. There are some extensions to DAGs, but none of them seem to be particularly well founded. However, searching for exactly that brought me to <a href="https://stackoverflow.com/questions/14865081/algorithm-to-find-lowest-common-ancestor-in-directed-acyclic-graph">this stackoverflow question</a>, where nestled in the comments is someone suggesting that the poster isn’t looking for LCA, but instead for a related notion the <em>lowest <strong>single</strong> common ancestor.</em> LSCA is defined in a 2010 paper <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020019010000487">New common ancestor problems in trees and directed acyclic graphs</a>.</p>
<p>The standard definition of <code>LCA(x, y) = l</code> is that “<code>l</code> is an ancestor of <code>x</code> and of <code>y</code>, and that no descendent of <code>l</code> has this property.”</p>
<p>But the definition of <code>LSCA(x, y) = l</code> is that “<code>l</code> lies on all root-to-<code>x</code> paths, and that <code>l</code> lies on all root-to-<code>y</code> paths, and that no descendent of <code>l</code> has this property.”</p>
<p>The distinction between the two is easily seen in the following graph:</p>
<pre><code>  0
 / \
1   2
| X |
3   4</code></pre>
<p>Under the standard definition, LCA is not uniquely defined for DAGs. That is, <code>LCA(3, 4) = {1, 2}</code>. But neither 1 nor 2 lies on <em>all</em> paths from the root. Under LSCA therefore we get <code>LSCA(3, 4) = 0</code>, which is the obviously-correct place to let-bind 3 and 4.</p>
<p>The paper gives a preprocessing scheme for computing LSCA by building a “lowest single ancestor” (LSA) tree. The LSA of a node is the LSCA of all of its in-edges. This definition cashes out to mean “the most immediate diamond above any node.” Finally! This is exactly what we’re looking for, since this is where we must insert our let-bindings! Even better, the paper gives us an algorithm for computing the LSA tree in linear time!</p>
<h2 id="the-first-implementer">The First Implementer</h2>
<p>Of course, I’m lazy and would prefer not to implement this thing. So instead I searched on hackage for <code>lsca</code>, and found nothing. But then I searched for <code>lca</code> and found that, like always, <a href="https://hackage.haskell.org/package/lca">Ed Kmett was 13 years ahead of me.</a></p>
<p>The <code>lca</code> package implements an <span>\(O(log n)\)</span> algorithm for computing the LCA of any two nodes in a graph. Which is very convenient for me, since the LSCA algorithm requires being able to do this.</p>
<p>Time to roll up the sleeves and get cracking I suppose.</p>
<p>The paper was surprisingly straightforward, and my first attempt implemented the (imperative) algorithms as given (imperatively.) The first step is to do a topological sort on the DAG in order to know in which order one ought to unfold the LSA tree.</p>
<p>But as is so often the case, this topological sort isn’t actually relevant to the algorithm; it’s just an encoding detail of expressing the algorithm imperatively. But you don’t need that when you’ve got laziness on your side! Instead you can just tie the know and do something cool like this:</p>
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span>lsaTree ::</span> <span>Ord</span> v <span>=&gt;</span> <span>Map</span> v (<span>Set</span> v) <span>-&gt;</span> <span>Map</span> v (<span>Path</span> v)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>lsaTree input <span>=</span> fix <span>$</span> \result <span>-&gt;</span> M.fromList <span>$</span> <span>do</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  (node, parents) <span>&lt;-</span> M.toList input</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span>let</span> parentResults <span>=</span> <span>fmap</span> (result <span>M.!</span>) parents</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span>...</span></span></code></pre></div>
<p>Notice how we use <code>fix</code> to bind the eventual result of the final computation. Then we can chase pointers by looking them up in <code>result</code>—even though it’s not yet “computed.” Who cares what order the computer does it in. Why is that a thing I should need to specify?</p>
<p>Anyway. The exact details of implementing LSA are not particularly important for the remainder of this blog post. If you’re interested, you can peep the PR, which is <a href="https://github.com/ekmett/lca/pull/8">delightfully small</a>.</p>
<h2 id="tying-it-all-back-together">Tying It All Back Together</h2>
<p>Equipped with my LSA tree, I was now ready to go back and solve the original problem of figuring out where to stick let-bindings. It’s easy now. Given the original program graph, find the LSA for each node. The LSA is the place you should insert the let binding.</p>
<p>So given the map of nodes to their LSAs, invert that map and get back a map of nodes to descendents who have this node as an LSA. Now when you go to inline a node, just look up everything in this map and inline it first.</p>
<p>It turns out to be a very elegant solution. It’s one third of the length of my horrible ad-hoc implementations, and it runs in linear time of the number of nodes in the graph. All in all, very good.</p>
<p>More often than I’m comfortable about, people will ask me how I can have so many good ideas. And what I like about this story is that it’s pretty typical of how I actually “have” “good” ideas. I’m reminded of the fact that <a href="https://fs.blog/great-talks/richard-hamming-your-research/">luck favors the prepared mind</a>. Attentive readers will notice that <em>none</em> of this process was due to brilliance on my part. I happened to know Vikrem who’s a genius. Together we pulled at some ancient graph theory strings and remembered a fact that someone else had thought important to teach us. That wasn’t actually the right path, but it lead us to stackoverflow where someone had linked to a relevant paper. I implemented the paper using a library that someone else had done the heavy lifting on, and simplified the implementation using this knot-tying trick I picked up somewhere along the way.</p>
<p>Also, I’m just really pleased that the solution came from trying to reverse engineer the relevant graph-theory search terms. Maybe that’s the actual takeaway here.</p>

<p>
    <span>
        <a href="https://reasonablypolymorphic.com/blog/bidirectional-instance-contexts">←</a>
    </span>
    <span>
        <a href="https://reasonablypolymorphic.com/blog/api-analysis">→</a>
    </span>
</p>

</div></div>
  </body>
</html>
