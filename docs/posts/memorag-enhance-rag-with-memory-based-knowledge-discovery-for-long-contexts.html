<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/qhjqhj00/MemoRAG">Original</a>
    <h1>MemoRAG â€“ Enhance RAG with memory-based knowledge discovery for long contexts</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
<p dir="auto"><strong>Empowering RAG with a memory-based data interface for all-purpose applications!</strong></p>
<p><a href="https://arxiv.org/abs/2409.05591" rel="nofollow"><img src="https://camo.githubusercontent.com/e77bc7a7bb6e94ececde002fe9566c05e16a43ce2505e3312f593c35d4478f49/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d6235323132662e7376673f6c6f676f3d6172786976" data-canonical-src="https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv"/></a>
<a href="https://huggingface.co/TommyChien/memorag-qwen2-7b-inst" rel="nofollow"><img src="https://camo.githubusercontent.com/b9cfd536e732f588fb294c166d0cf0445280e1277da0992d910d08046c71d394/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67466163652532304d6f64656c2d3237623362342e737667" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Model-27b3b4.svg"/></a>
<a href="https://github.com/"><img alt="License" src="https://camo.githubusercontent.com/f2d7ec897487f1f15b8aa0becf58575e8864664c20296ca5ad838c94a6aee34e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c4943454e53452d4d49542d677265656e" data-canonical-src="https://img.shields.io/badge/LICENSE-MIT-green"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/39dc05a0a44228ca43023e33f606d774bfbc34ed0aa0be2538308b2038d534c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d6164655f776974682d507974686f6e2d626c7565"><img alt="Static Badge" src="https://camo.githubusercontent.com/39dc05a0a44228ca43023e33f606d774bfbc34ed0aa0be2538308b2038d534c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d6164655f776974682d507974686f6e2d626c7565" data-canonical-src="https://img.shields.io/badge/made_with-Python-blue"/></a>
</p></div>

<p dir="auto"><strong>MemoRAG</strong> is an innovative RAG framework built on top of a highly efficient, super-long memory model. Unlike standard RAG, which primarily handles queries with explicit information needs, MemoRAG leverages its memory model to achieve a global understanding of the entire database. By recalling query-specific clues from memory, MemoRAG enhances evidence retrieval, resulting in more accurate and contextually rich response generation.â€‹</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/qhjqhj00/MemoRAG/blob/main/asset/tech_case.jpg"><img src="https://github.com/qhjqhj00/MemoRAG/raw/main/asset/tech_case.jpg"/></a>
</p>

<p dir="auto">We will provide a toy demo to demonstrate MemoRAG, you can try with the following scripts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="streamlit run demo/demo.py"><pre><span>streamlit</span> <span>run</span> <span>demo</span><span>/</span><span>demo</span>.<span>py</span></pre></div>
<p dir="auto">Afterwards, you can view the demo as below:</p>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/qhjqhj00/MemoRAG/blob/main/asset/demo.gif"><img src="https://github.com/qhjqhj00/MemoRAG/raw/main/asset/demo.gif" data-animated-image=""/></a>
  </p>
</div>

<p dir="auto">[13/09/24] MemoRAG adds <code>Meta-Llama-3.1-8B-Instruct</code> and <code>Llama3.1-8B-Chinese-Chat</code> as the Memory Model, see <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/longllm_as_memory.ipynb"><code>examples</code></a>.</p>
<p dir="auto">[10/09/24] We release MemoRAG&#39;s <a href="https://arxiv.org/pdf/2409.05591" rel="nofollow"><code>Technical Report</code></a>.</p>
<p dir="auto">[09/09/24] You can try MemoRAG on <a href="https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX?usp=sharing" rel="nofollow"><code>Google Colab</code></a> for free.</p>
<p dir="auto">[05/09/24] A Qwen2-based memory model is available at <a href="https://huggingface.co/TommyChien/memorag-qwen2-7b-inst" rel="nofollow"><code>TommyChien/memorag-qwen2-7b-inst</code></a>.</p>
<p dir="auto">[03/09/24] A Mistral-based memory model is available at <a href="https://huggingface.co/TommyChien/memorag-mistral-7b-inst" rel="nofollow"><code>TommyChien/memorag-mistral-7b-inst</code></a>.</p>
<p dir="auto">[01/09/24] The project launched!</p>

<ul dir="auto">
<li><strong>Global Memory</strong>: Handles up to <strong>1 million tokens</strong> in a single context, providing comprehensive understanding across massive datasets.</li>
<li><strong>Optimizable &amp; Flexible</strong>: Adapts to new tasks with ease, achieving optimized performance with just a few hours of additional training.</li>
<li><strong>Contextual Clues</strong>: Generates precise clues from global memory, bridging raw input to answers and unlocking <strong>hidden insights</strong> from complex data.</li>
<li><strong>Efficient Caching</strong>: Speeds up context pre-filling by <strong>up to 30x</strong>, with support for caching chunking, indexing, and encoding.</li>
<li><strong>Context Reuse</strong>: Encodes long contexts <strong>once</strong> and supports repeated usage, boosting efficiency in tasks that require recurring data access.</li>
</ul>

<p dir="auto"><strong>MemoRAG</strong>  is currently under active development, with resources and prototypes continuously being published at this repository.</p>
<ul>
<li> Codes / Models / Dataset Release</li>
<li> Support OpenAI/Azure models</li>
<li> Technical Report Release</li>
<li> Support Chinese</li>
<li> Demo Codes Release</li>
<li> Training Codes for Memory model Release</li>
<li> <strong>Light-Weight Optimization</strong></li>
<li> <strong>Speed Up Inference</strong></li>
<li> <strong>Integrate Any Retrieval Methods</strong></li>
<li> <strong>Enrich the Memory Ability</strong></li>
</ul>
<p dir="auto">Note: The <strong>recent goals</strong> of MemoRAG are to achieve <strong>light-weight optimization</strong> through engineering improvements and to <strong>enhance its memory capabilities</strong>, enabling it to adapt to a wider range of applications and <strong>support longer context</strong> (e.g., more than one million tokens).</p>

<p dir="auto">ðŸ†“ <strong>You can directly try MemoRAG on <a href="https://colab.research.google.com/drive/1fPMXKyi4AwWSBkC7Xr5vBdpPpx9gDeFX?usp=sharing" rel="nofollow"><code>Google Colab</code></a> for free.</strong></p>
<p dir="auto">In this notebook, we run the complete MemoRAG pipeline (Memory Model + Retriever + Generation Model) on a single T4 GPU with 15GiB of memory provided by Google Colab. Despite the limited resources, MemoRAG can process half of the content from <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/harry_potter.txt">the example book</a> (~68K tokens) and perform all of its functions.</p>

<p dir="auto">To use Memorizer and MemoRAG, you need to have Python installed along with the required libraries. You can install the necessary dependencies using the following command:</p>
<p dir="auto"><strong>Install Dependencies</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install torch==2.3.1
conda install -c pytorch -c nvidia faiss-gpu=1.8.0"><pre>pip install torch==2.3.1
conda install -c pytorch -c nvidia faiss-gpu=1.8.0</pre></div>
<p dir="auto"><strong>Install from source</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone this repo first
cd MemoRAG
pip install -e ."><pre><span><span>#</span> clone this repo first</span>
<span>cd</span> MemoRAG
pip install -e <span>.</span></pre></div>
<p dir="auto"><strong>Install via pip</strong></p>

<p dir="auto">For <strong>Quick Start</strong>,
We provide a notebook to illustrate all functions of MemoRAG <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/example.ipynb">here</a>.</p>

<p dir="auto">MemoRAG is easy to use and can be initialized with HuggingFace models directly. By using the <code>MemoRAG.memorize()</code> method, the memory model builds a global memory over a long input context. Empirically, with default parameter settings, <code>TommyChien/memorag-qwen2-7b-inst</code> can handle contexts of up to 400K tokens, while <code>TommyChien/memorag-mistral-7b-inst</code> can manage contexts up to 128K tokens. By increasing the <code>beacon_ratio</code> parameter, the modelâ€™s capacity to handle longer contexts can be extended. For example, <code>TommyChien/memorag-qwen2-7b-inst</code> can process up to one million tokens with <code>beacon_ratio=16</code>.</p>

<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG

# Initialize MemoRAG pipeline
pipe = MemoRAG(
    mem_model_name_or_path=&#34;TommyChien/memorag-mistral-7b-inst&#34;,
    ret_model_name_or_path=&#34;BAAI/bge-m3&#34;, 
    gen_model_name_or_path=&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;, # Optional: if not specify, use memery model as the generator
    cache_dir=&#34;path_to_model_cache&#34;,  # Optional: specify local model cache directory
    access_token=&#34;hugging_face_access_token&#34;,  # Optional: Hugging Face access token
    beacon_ratio=4
)

context = open(&#34;examples/harry_potter.txt&#34;).read()
query = &#34;How many times is the Chamber of Secrets opened in the book?&#34;

# Memorize the context and save to cache
pipe.memorize(context, save_dir=&#34;cache/harry_potter/&#34;, print_stats=True)

# Generate response using the memorized context
res = pipe(context=context, query=query, task_type=&#34;memorag&#34;, max_new_tokens=256)
print(f&#34;MemoRAG generated answer: \n{res}&#34;)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>

<span># Initialize MemoRAG pipeline</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>&#34;TommyChien/memorag-mistral-7b-inst&#34;</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>&#34;BAAI/bge-m3&#34;</span>, 
    <span>gen_model_name_or_path</span><span>=</span><span>&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>, <span># Optional: if not specify, use memery model as the generator</span>
    <span>cache_dir</span><span>=</span><span>&#34;path_to_model_cache&#34;</span>,  <span># Optional: specify local model cache directory</span>
    <span>access_token</span><span>=</span><span>&#34;hugging_face_access_token&#34;</span>,  <span># Optional: Hugging Face access token</span>
    <span>beacon_ratio</span><span>=</span><span>4</span>
)

<span>context</span> <span>=</span> <span>open</span>(<span>&#34;examples/harry_potter.txt&#34;</span>).<span>read</span>()
<span>query</span> <span>=</span> <span>&#34;How many times is the Chamber of Secrets opened in the book?&#34;</span>

<span># Memorize the context and save to cache</span>
<span>pipe</span>.<span>memorize</span>(<span>context</span>, <span>save_dir</span><span>=</span><span>&#34;cache/harry_potter/&#34;</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Generate response using the memorized context</span>
<span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>query</span><span>=</span><span>query</span>, <span>task_type</span><span>=</span><span>&#34;memorag&#34;</span>, <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>f&#34;MemoRAG generated answer: <span>\n</span><span><span>{</span><span>res</span><span>}</span></span>&#34;</span>)</pre></div>
<p dir="auto">When running the above code, <strong>the encoded key-value (KV) cache, Faiss index, and chunked passages are stored</strong> in the specified <code>save_dir</code>. Afterward, if the same context is used again, the data can be quickly loaded from the disk:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipe.load(&#34;cache/harry_potter/&#34;, print_stats=True)"><pre><span>pipe</span>.<span>load</span>(<span>&#34;cache/harry_potter/&#34;</span>, <span>print_stats</span><span>=</span><span>True</span>)</pre></div>
<p dir="auto">Typically, loading cached weights is highly efficient. For example, <strong>encoding, chunking, and indexing a 200K-token context takes approximately 35 seconds</strong> using <code>TommyChien/memorag-qwen2-7b-inst</code> as the memory model, <strong>but only 1.5 seconds when loading from cached files.</strong></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Using Long LLMs as Memory Model ðŸ†• ðŸ†• ðŸ†•</h3><a id="user-content-using-long-llms-as-memory-model-new-new-new" aria-label="Permalink: Using Long LLMs as Memory Model :new: :new: :new:" href="#using-long-llms-as-memory-model-new-new-new"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Recent LLMs have become effective memory models due to their expanding context windows. MemoRAG now supports leveraging these long-context LLMs as memory models, utilizing <a href="https://github.com/microsoft/MInference"><code>MInference</code></a> to optimize context prefilling. We have tested <code>Meta-Llama-3.1-8B-Instruct</code> and <code>Llama3.1-8B-Chinese-Chat</code> as memory models, both of which natively support a 128K context length. We are currently exploring additional suitable LLMs and optimizing strategies to enhance the memory mechanisms and context length further. For detailed usage instructions, please refer to the provided scripts and the <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/examples/longllm_as_memory.ipynb"><code>notebook</code></a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG
model = MemoRAG(
    mem_model_name_or_path=&#34;shenzhi-wang/Llama3.1-8B-Chinese-Chat&#34;,    # For Chinese
    # mem_model_name_or_path=&#34;meta-llama/Meta-Llama-3.1-8B-Instruct&#34;,  # For English
    ret_model_name_or_path=&#34;BAAI/bge-m3&#34;,
    # cache_dir=&#34;path_to_model_cache&#34;,  # to specify local model cache directory (optional)
    # access_token=&#34;hugging_face_access_token&#34;  # to specify local model cache directory (optional)
    )"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>
<span>model</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>&#34;shenzhi-wang/Llama3.1-8B-Chinese-Chat&#34;</span>,    <span># For Chinese</span>
    <span># mem_model_name_or_path=&#34;meta-llama/Meta-Llama-3.1-8B-Instruct&#34;,  # For English</span>
    <span>ret_model_name_or_path</span><span>=</span><span>&#34;BAAI/bge-m3&#34;</span>,
    <span># cache_dir=&#34;path_to_model_cache&#34;,  # to specify local model cache directory (optional)</span>
    <span># access_token=&#34;hugging_face_access_token&#34;  # to specify local model cache directory (optional)</span>
    )</pre></div>
<p dir="auto">Afterward, you can use MemoRAG&#39;s functions as usual.</p>

<p dir="auto">To perform summarization tasks, use the following script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="res = pipe(context=context, task_type=&#34;summarize&#34;, max_new_tokens=512)
print(f&#34;MemoRAG summary of the full book:\n {res}&#34;)"><pre><span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>task_type</span><span>=</span><span>&#34;summarize&#34;</span>, <span>max_new_tokens</span><span>=</span><span>512</span>)
<span>print</span>(<span>f&#34;MemoRAG summary of the full book:<span>\n</span> <span><span>{</span><span>res</span><span>}</span></span>&#34;</span>)</pre></div>

<p dir="auto">If you want to use APIs as a generator, refer to the script below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import Agent, MemoRAG

# API configuration
api_dict = {
    &#34;endpoint&#34;: &#34;&#34;,
    &#34;api_version&#34;: &#34;2024-02-15-preview&#34;,
    &#34;api_key&#34;: &#34;&#34;
}
model = &#34;gpt-35-turbo-16k&#34;
source = &#34;azure&#34;

# Initialize Agent with the API
agent = Agent(model, source, api_dict)
print(agent.generate(&#34;hi!&#34;))  # Test the API

# Initialize MemoRAG pipeline with a customized generator model
pipe = MemoRAG(
    mem_model_name_or_path=&#34;TommyChien/memorag-qwen2-7b-inst&#34;,
    ret_model_name_or_path=&#34;BAAI/bge-m3&#34;,
    cache_dir=&#34;path_to_model_cache&#34;,  # Optional: specify local model cache directory
    customized_gen_model=agent,
)

# Load previously cached context
pipe.load(&#34;cache/harry_potter_qwen/&#34;, print_stats=True)

# Use the loaded context for question answering
query = &#34;How are the mutual relationships between the main characters?&#34;
context = open(&#34;harry_potter.txt&#34;).read()

res = pipe(context=context, query=query, task_type=&#34;memorag&#34;, max_new_tokens=256)
print(f&#34;MemoRAG with GPT-3.5 generated answer: \n{res}&#34;)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>Agent</span>, <span>MemoRAG</span>

<span># API configuration</span>
<span>api_dict</span> <span>=</span> {
    <span>&#34;endpoint&#34;</span>: <span>&#34;&#34;</span>,
    <span>&#34;api_version&#34;</span>: <span>&#34;2024-02-15-preview&#34;</span>,
    <span>&#34;api_key&#34;</span>: <span>&#34;&#34;</span>
}
<span>model</span> <span>=</span> <span>&#34;gpt-35-turbo-16k&#34;</span>
<span>source</span> <span>=</span> <span>&#34;azure&#34;</span>

<span># Initialize Agent with the API</span>
<span>agent</span> <span>=</span> <span>Agent</span>(<span>model</span>, <span>source</span>, <span>api_dict</span>)
<span>print</span>(<span>agent</span>.<span>generate</span>(<span>&#34;hi!&#34;</span>))  <span># Test the API</span>

<span># Initialize MemoRAG pipeline with a customized generator model</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>&#34;TommyChien/memorag-qwen2-7b-inst&#34;</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>&#34;BAAI/bge-m3&#34;</span>,
    <span>cache_dir</span><span>=</span><span>&#34;path_to_model_cache&#34;</span>,  <span># Optional: specify local model cache directory</span>
    <span>customized_gen_model</span><span>=</span><span>agent</span>,
)

<span># Load previously cached context</span>
<span>pipe</span>.<span>load</span>(<span>&#34;cache/harry_potter_qwen/&#34;</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Use the loaded context for question answering</span>
<span>query</span> <span>=</span> <span>&#34;How are the mutual relationships between the main characters?&#34;</span>
<span>context</span> <span>=</span> <span>open</span>(<span>&#34;harry_potter.txt&#34;</span>).<span>read</span>()

<span>res</span> <span>=</span> <span>pipe</span>(<span>context</span><span>=</span><span>context</span>, <span>query</span><span>=</span><span>query</span>, <span>task_type</span><span>=</span><span>&#34;memorag&#34;</span>, <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>f&#34;MemoRAG with GPT-3.5 generated answer: <span>\n</span><span><span>{</span><span>res</span><span>}</span></span>&#34;</span>)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Supported APIs for Generators</h3><a id="user-content-supported-apis-for-generators" aria-label="Permalink: Supported APIs for Generators" href="#supported-apis-for-generators"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The built-in <code>Agent</code> object supports models from both <code>openai</code> and <code>deepseek</code>. Below are the configurations for initializing these models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using deepseek models
model = &#34;&#34;
source = &#34;deepseek&#34;
api_dict = {
    &#34;base_url&#34;: &#34;&#34;,
    &#34;api_key&#34;: &#34;&#34;
}

# Using openai models
model = &#34;&#34;
source = &#34;openai&#34;
api_dict = {
    &#34;api_key&#34;: &#34;&#34;
}"><pre><span># Using deepseek models</span>
<span>model</span> <span>=</span> <span>&#34;&#34;</span>
<span>source</span> <span>=</span> <span>&#34;deepseek&#34;</span>
<span>api_dict</span> <span>=</span> {
    <span>&#34;base_url&#34;</span>: <span>&#34;&#34;</span>,
    <span>&#34;api_key&#34;</span>: <span>&#34;&#34;</span>
}

<span># Using openai models</span>
<span>model</span> <span>=</span> <span>&#34;&#34;</span>
<span>source</span> <span>=</span> <span>&#34;openai&#34;</span>
<span>api_dict</span> <span>=</span> {
    <span>&#34;api_key&#34;</span>: <span>&#34;&#34;</span>
}</pre></div>

<p dir="auto">The Memory model can be used independently to store, recall, and interact with the context. Hereâ€™s an example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import Memory

# Initialize the Memory model
memo_model = Memory(
    &#34;TommyChien/memorag-qwen2-7b-inst&#34;,
    cache_dir=&#34;path_to_model_cache&#34;,  # Optional: specify local model cache directory
    beacon_ratio=4  # Adjust beacon ratio for handling longer contexts
)

# Load and memorize the context
context = open(&#34;harry_potter.txt&#34;).read()
memo_model.memorize(context)

# Save the memorized context to disk
memo_model.save(&#34;cache/harry_potter/memory.bin&#34;)

# Query the model for answers
query = &#34;How are the mutual relationships between the main characters?&#34;

res = memo_model.answer(query)
print(&#34;Using memory to answer the query:\n&#34;, res)

# Recall text clues for evidence retrieval
res = memo_model.recall(query)
print(&#34;Using memory to recall text clues to support evidence retrieval:\n&#34;, res)

# Rewrite the query into more specific surrogate queries
res = memo_model.rewrite(query)
print(&#34;Using memory to rewrite the input query into more specific surrogate queries:\n&#34;, res)"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>Memory</span>

<span># Initialize the Memory model</span>
<span>memo_model</span> <span>=</span> <span>Memory</span>(
    <span>&#34;TommyChien/memorag-qwen2-7b-inst&#34;</span>,
    <span>cache_dir</span><span>=</span><span>&#34;path_to_model_cache&#34;</span>,  <span># Optional: specify local model cache directory</span>
    <span>beacon_ratio</span><span>=</span><span>4</span>  <span># Adjust beacon ratio for handling longer contexts</span>
)

<span># Load and memorize the context</span>
<span>context</span> <span>=</span> <span>open</span>(<span>&#34;harry_potter.txt&#34;</span>).<span>read</span>()
<span>memo_model</span>.<span>memorize</span>(<span>context</span>)

<span># Save the memorized context to disk</span>
<span>memo_model</span>.<span>save</span>(<span>&#34;cache/harry_potter/memory.bin&#34;</span>)

<span># Query the model for answers</span>
<span>query</span> <span>=</span> <span>&#34;How are the mutual relationships between the main characters?&#34;</span>

<span>res</span> <span>=</span> <span>memo_model</span>.<span>answer</span>(<span>query</span>)
<span>print</span>(<span>&#34;Using memory to answer the query:<span>\n</span>&#34;</span>, <span>res</span>)

<span># Recall text clues for evidence retrieval</span>
<span>res</span> <span>=</span> <span>memo_model</span>.<span>recall</span>(<span>query</span>)
<span>print</span>(<span>&#34;Using memory to recall text clues to support evidence retrieval:<span>\n</span>&#34;</span>, <span>res</span>)

<span># Rewrite the query into more specific surrogate queries</span>
<span>res</span> <span>=</span> <span>memo_model</span>.<span>rewrite</span>(<span>query</span>)
<span>print</span>(<span>&#34;Using memory to rewrite the input query into more specific surrogate queries:<span>\n</span>&#34;</span>, <span>res</span>)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Usage for Memory-Augmented Retrieval</h3><a id="user-content-usage-for-memory-augmented-retrieval" aria-label="Permalink: Usage for Memory-Augmented Retrieval" href="#usage-for-memory-augmented-retrieval"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In addition to the standalone Memory Model, MemoRAG provides memory-augmented retrieval functionality. This allows for improved evidence retrieval based on recalled clues from memory.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from memorag import MemoRAG

# Initialize MemoRAG pipeline
pipe = MemoRAG(
    mem_model_name_or_path=&#34;TommyChien/memorag-qwen2-7b-inst&#34;,
    ret_model_name_or_path=&#34;BAAI/bge-m3&#34;,
    cache_dir=&#34;path_to_model_cache&#34;,  # Optional: specify local model cache directory
    access_token=&#34;hugging_face_access_token&#34;  # Optional: Hugging Face access token
)

# Load and memorize the context
test_txt = open(&#34;harry_potter.txt&#34;).read()
pipe.memorize(test_txt, save_dir=&#34;cache/harry_potter/&#34;, print_stats=True)

# Define the query
query = &#34;How are the mutual relationships between the main characters?&#34;

# Recall clues from memory
clues = pipe.mem_model.recall(query).split(&#34;\n&#34;)
clues = [q for q in clues if len(q.split()) &gt; 3]  # Filter out short or irrelevant clues
print(&#34;Clues generated from memory:\n&#34;, clues)

# Retrieve relevant passages based on the recalled clues
retrieved_passages = pipe._retrieve(clues)
print(&#34;\n======\n&#34;.join(retrieved_passages[:3]))"><pre><span>from</span> <span>memorag</span> <span>import</span> <span>MemoRAG</span>

<span># Initialize MemoRAG pipeline</span>
<span>pipe</span> <span>=</span> <span>MemoRAG</span>(
    <span>mem_model_name_or_path</span><span>=</span><span>&#34;TommyChien/memorag-qwen2-7b-inst&#34;</span>,
    <span>ret_model_name_or_path</span><span>=</span><span>&#34;BAAI/bge-m3&#34;</span>,
    <span>cache_dir</span><span>=</span><span>&#34;path_to_model_cache&#34;</span>,  <span># Optional: specify local model cache directory</span>
    <span>access_token</span><span>=</span><span>&#34;hugging_face_access_token&#34;</span>  <span># Optional: Hugging Face access token</span>
)

<span># Load and memorize the context</span>
<span>test_txt</span> <span>=</span> <span>open</span>(<span>&#34;harry_potter.txt&#34;</span>).<span>read</span>()
<span>pipe</span>.<span>memorize</span>(<span>test_txt</span>, <span>save_dir</span><span>=</span><span>&#34;cache/harry_potter/&#34;</span>, <span>print_stats</span><span>=</span><span>True</span>)

<span># Define the query</span>
<span>query</span> <span>=</span> <span>&#34;How are the mutual relationships between the main characters?&#34;</span>

<span># Recall clues from memory</span>
<span>clues</span> <span>=</span> <span>pipe</span>.<span>mem_model</span>.<span>recall</span>(<span>query</span>).<span>split</span>(<span>&#34;<span>\n</span>&#34;</span>)
<span>clues</span> <span>=</span> [<span>q</span> <span>for</span> <span>q</span> <span>in</span> <span>clues</span> <span>if</span> <span>len</span>(<span>q</span>.<span>split</span>()) <span>&gt;</span> <span>3</span>]  <span># Filter out short or irrelevant clues</span>
<span>print</span>(<span>&#34;Clues generated from memory:<span>\n</span>&#34;</span>, <span>clues</span>)

<span># Retrieve relevant passages based on the recalled clues</span>
<span>retrieved_passages</span> <span>=</span> <span>pipe</span>.<span>_retrieve</span>(<span>clues</span>)
<span>print</span>(<span>&#34;<span>\n</span>======<span>\n</span>&#34;</span>.<span>join</span>(<span>retrieved_passages</span>[:<span>3</span>]))</pre></div>

<p dir="auto">Below are experiments results for the memory model, incorporating with three generation models.</p>
<markdown-accessiblity-table>
    We test MemoRAG on three benchmarks. The best results of each block are in bold.
    <table><thead>
        <tr>
            <th>Dataset</th>
            <th>NarrativeQA</th>
            <th>Qasper</th>
            <th>MultifieldQA</th>
            <th>Musique</th>
            <th>2Wiki</th>
            <th>HotpotQA</th>
            <th>MultiNews</th>
            <th>GovReport</th>
            <th>En.sum</th>
            <th>En.qa</th>
            <th>Fin</th>
            <th>Legal</th>
            <th>Mix</th>
        </tr>
        <tr>
            <td></td>
            <td colspan="8"><strong>LongBench</strong></td>
            <td colspan="2"><strong>InfBench</strong></td>
            <td colspan="3"><strong>UltraDomain</strong></td>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td colspan="13"><strong>Generator: Llama3-8B-Instruct-8K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>21.3</td>
            <td>43.4</td>
            <td>46.6</td>
            <td>23.5</td>
            <td>38.2</td>
            <td>47.1</td>
            <td>24.6</td>
            <td>23.6</td>
            <td>13.1</td>
            <td>6.7</td>
            <td>34.2</td>
            <td>33.2</td>
            <td>42.7</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>22.1</td>
            <td>44.3</td>
            <td>50.2</td>
            <td>22.2</td>
            <td>36.7</td>
            <td>48.4</td>
            <td>22.1</td>
            <td>20.1</td>
            <td>12.1</td>
            <td>15.1</td>
            <td>41.4</td>
            <td>40.6</td>
            <td>46.4</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>12.3</td>
            <td>35.2</td>
            <td>44.4</td>
            <td>22.1</td>
            <td>33.3</td>
            <td>41.9</td>
            <td>22.1</td>
            <td>20.7</td>
            <td>11.7</td>
            <td>14.8</td>
            <td>41.9</td>
            <td>33.7</td>
            <td>44.9</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>20.2</td>
            <td>43.9</td>
            <td>49.1</td>
            <td>22.7</td>
            <td>36.1</td>
            <td>44.5</td>
            <td>20.6</td>
            <td>21.0</td>
            <td>12.0</td>
            <td>13.3</td>
            <td>39.5</td>
            <td>36.8</td>
            <td>44.5</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>22.1</td>
            <td>44.3</td>
            <td>50.2</td>
            <td>22.2</td>
            <td>36.7</td>
            <td>48.4</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td><strong>19.1</strong></td>
            <td>41.4</td>
            <td>40.6</td>
            <td>46.4</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>22.8</strong></td>
            <td><strong>45.7</strong></td>
            <td><strong>50.7</strong></td>
            <td><strong>28.4</strong></td>
            <td><strong>51.4</strong></td>
            <td><strong>57.0</strong></td>
            <td><strong>27.4</strong></td>
            <td><strong>27.9</strong></td>
            <td><strong>14.1</strong></td>
            <td>16.1</td>
            <td><strong>47.8</strong></td>
            <td><strong>47.9</strong></td>
            <td><strong>55.5</strong></td>
        </tr>
        <tr>
            <td colspan="13"><strong>Generator: Phi-3-mini-128K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>21.4</td>
            <td>35.0</td>
            <td>47.3</td>
            <td>19.0</td>
            <td>35.5</td>
            <td>42.1</td>
            <td>25.6</td>
            <td>23.7</td>
            <td>13.0</td>
            <td>15.2</td>
            <td>44.8</td>
            <td>40.5</td>
            <td>44.7</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>20.3</td>
            <td>33.0</td>
            <td>44.3</td>
            <td>21.1</td>
            <td>35.4</td>
            <td>42.1</td>
            <td>17.7</td>
            <td>19.8</td>
            <td>9.6</td>
            <td>16.3</td>
            <td>41.7</td>
            <td>41.2</td>
            <td>43.7</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>13.7</td>
            <td>32.4</td>
            <td>43.5</td>
            <td>21.0</td>
            <td>35.6</td>
            <td>40.6</td>
            <td>20.3</td>
            <td>18.2</td>
            <td>10.0</td>
            <td>19.5</td>
            <td>42.8</td>
            <td>35.1</td>
            <td>43.9</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>19.6</td>
            <td>34.1</td>
            <td>46.5</td>
            <td>21.9</td>
            <td>36.1</td>
            <td>41.7</td>
            <td>20.1</td>
            <td>18.6</td>
            <td>10.4</td>
            <td>16.1</td>
            <td>41.8</td>
            <td>40.9</td>
            <td>43.2</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>18.7</td>
            <td>36.0</td>
            <td>47.5</td>
            <td>20.5</td>
            <td>36.8</td>
            <td>42.7</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>19.6</td>
            <td>43.1</td>
            <td>41.6</td>
            <td>44.2</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>27.5</strong></td>
            <td><strong>43.9</strong></td>
            <td><strong>52.2</strong></td>
            <td><strong>33.9</strong></td>
            <td><strong>54.1</strong></td>
            <td><strong>54.8</strong></td>
            <td><strong>32.9</strong></td>
            <td><strong>26.3</strong></td>
            <td><strong>15.7</strong></td>
            <td><strong>22.9</strong></td>
            <td><strong>51.5</strong></td>
            <td><strong>51.0</strong></td>
            <td><strong>55.6</strong></td>
        </tr>
        <tr>
            <td colspan="13"><strong>Generator: Mistral-7B-Instruct-v0.2-32K</strong></td>
        </tr>
        <tr>
            <td>Full</td>
            <td>20.8</td>
            <td>29.2</td>
            <td>46.3</td>
            <td>18.9</td>
            <td>20.6</td>
            <td>37.6</td>
            <td>23.0</td>
            <td>20.4</td>
            <td>12.4</td>
            <td>12.3</td>
            <td>36.5</td>
            <td>35.8</td>
            <td>42.1</td>
        </tr>
        <tr>
            <td>BGE-M3</td>
            <td>17.3</td>
            <td>29.5</td>
            <td>46.3</td>
            <td>18.5</td>
            <td>20.3</td>
            <td>36.2</td>
            <td>24.3</td>
            <td>26.1</td>
            <td>13.5</td>
            <td>12.2</td>
            <td>40.5</td>
            <td>42.0</td>
            <td>41.1</td>
        </tr>
        <tr>
            <td>Stella-v5</td>
            <td>13.5</td>
            <td>23.7</td>
            <td>42.1</td>
            <td>18.6</td>
            <td>22.2</td>
            <td>31.9</td>
            <td>21.1</td>
            <td>18.5</td>
            <td>13.2</td>
            <td>9.7</td>
            <td>40.9</td>
            <td>34.9</td>
            <td>42.1</td>
        </tr>
        <tr>
            <td>RQ-RAG</td>
            <td>17.1</td>
            <td>29.2</td>
            <td>47.0</td>
            <td>19.1</td>
            <td>21.5</td>
            <td>37.0</td>
            <td>22.1</td>
            <td>18.6</td>
            <td>13.1</td>
            <td>12.7</td>
            <td>44.3</td>
            <td>44.6</td>
            <td>43.4</td>
        </tr>
        <tr>
            <td>HyDE</td>
            <td>17.4</td>
            <td>29.5</td>
            <td>46.3</td>
            <td>18.5</td>
            <td>20.1</td>
            <td>36.2</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>12.2</td>
            <td>42.8</td>
            <td>35.1</td>
            <td>43.9</td>
        </tr>
        <tr>
            <td><strong>MemoRAG</strong></td>
            <td><strong>23.1</strong></td>
            <td>31.2</td>
            <td><strong>50.0</strong></td>
            <td>26.9</td>
            <td>30.3</td>
            <td>42.9</td>
            <td><strong>27.1</strong></td>
            <td><strong>31.6</strong></td>
            <td><strong>17.9</strong></td>
            <td>15.4</td>
            <td>48.0</td>
            <td>51.2</td>
            <td><strong>53.6</strong></td>
        </tr>
        <tr>
            <td><strong>MemoRAG-qwen2</strong></td>
            <td>22.2</td>
            <td><strong>32.7</strong></td>
            <td>49.6</td>
            <td><strong>31.4</strong></td>
            <td><strong>33.7</strong></td>
            <td><strong>44.4</strong></td>
            <td>27.0</td>
            <td>31.5</td>
            <td>16.8</td>
            <td><strong>17.6</strong></td>
            <td><strong>48.7</strong></td>
            <td><strong>52.3</strong></td>
            <td>48.6</td>
        </tr>
    </tbody>
</table></markdown-accessiblity-table>

<p dir="auto">To evaluate MemoRAG, use the following script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd examples
bash longbench/eval.sh"><pre><span>cd</span> examples
bash longbench/eval.sh</pre></div>
<p dir="auto">We will update other evaluation scripts soon.</p>

<p dir="auto">UltraDomain Benchmark: <a href="https://huggingface.co/datasets/TommyChien/UltraDomain" rel="nofollow">this repo</a>.</p>
<p dir="auto">Other Evaluation Data: <a href="https://huggingface.co/datasets/TommyChien/MemoRAG-data/" rel="nofollow">this repo</a>.</p>


<p dir="auto">MemoRAG is licensed under the <a href="https://github.com/qhjqhj00/MemoRAG/blob/main/LICENSE">MIT License</a>.</p>

<p dir="auto">If you use MemoRAG in your research, please cite our paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{qian2024memorag,
      title={MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery}, 
      author={Hongjin Qian and Peitian Zhang and Zheng Liu and Kelong Mao and Zhicheng Dou},
      year={2024},
      eprint={2409.05591},
      url={https://arxiv.org/abs/2409.05591}, 
}"><pre><span>@misc</span>{<span>qian2024memorag</span>,
      <span>title</span>=<span><span>{</span>MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Hongjin Qian and Peitian Zhang and Zheng Liu and Kelong Mao and Zhicheng Dou<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2409.05591<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://arxiv.org/abs/2409.05591<span>}</span></span>, 
}</pre></div>
</article></div></div>
  </body>
</html>
