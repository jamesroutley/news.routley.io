<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://theorem.dev/blog/anthropic-bug-test/">Original</a>
    <h1>Systematically generating tests that would have caught Anthropic&#39;s top‑K bug</h1>
    
    <div id="readability-page-1" class="page"><div>
    
<article>
    
    <div>
        <p>Most testing strategies miss rare edge cases until customers find them in production. Our system automatically generates targeted unit tests for rare bugs, including the one that would have caught Anthropic’s recent <a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues">approximate top-K bug</a>. In this blog post, we’ll provide a brief overview of how it works.</p>
<figure>
  <img src="https://theorem.dev/images/Fractional%20Proofs%20-%20venn.svg" alt="Comparison of testing approaches"/>
  <figcaption><strong>Figure 1:</strong> Unit-level PBTs are fast but miss edge cases. Proofs offer exhaustive coverage but require extensive reasoning and code refactoring. End-to-end PBTs have coverage but are not compute efficient. Fractional proofs sit at the intersection, using proof decomposition to generate targeted unit tests that balance compute efficiency, developer accuracy, and speed.</figcaption>
</figure>
<h2 id="catching-the-rare-bug-in-top-k-sampling">Catching the rare bug in top-K sampling</h2>
<p>A bug in the TPU implementation of approximate top-K resulted in the most likely token sometimes being excluded. Rare bugs like this frequently slip through to production because covering every behavior with testing is infeasible in practice. After discovery, Anthropic provided a simple reproducer of the bug, but it is the sort of test you only manage to write after a laborious bug minimization process.</p>
<p>We used fractional proof decomposition to automatically generate the unit test without relying on Anthropic’s bug reproducer code. You can run the unit test on <a href="https://colab.research.google.com/drive/1u3gUEb-_POObIU5wtR-UX7OG8hlOkoRu?usp=sharing">colab</a>. For any code, if testing is done via fractional proof decomposition, bugs can be systematically found without the benefit of hindsight.</p>
<figure>
  <!-- <img src="/images/Figure 2.png" alt="Top-K sampling property verification" style="max-width: 80%; height: auto;" /> -->
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@given</span>(k<span>=</span>st<span>.</span>integers(min_value<span>=</span><span>0</span>, max_value<span>=</span>TOP_K_RANGE), arr<span>=</span>arr_strategy)
</span></span><span><span><span>def</span> <span>test_approx_max_k</span>(k, arr):
</span></span><span><span>    N <span>=</span> len(arr)
</span></span><span><span>    k <span>=</span> int(k <span>%</span> min(N <span>-</span> MIN_TOP_K, TOP_K_RANGE)) <span>+</span> MIN_TOP_K
</span></span><span><span>
</span></span><span><span>    approx_values, _ <span>=</span> lax<span>.</span>approx_max_k(arr, k<span>=</span>k)
</span></span><span><span>    <span>assert</span> jnp<span>.</span>max(approx_values) <span>==</span> jnp<span>.</span>max(arr), \
</span></span></code></pre></div>  <figcaption><strong>Figure 2:</strong> Top-K sampling should always have some chance of picking the most likely token. We encode this property with a PBT (property-based test) for <code>max(approximate_top_k(arr, k=k)) == max(arr)</code>. If the implementation of lax.approx_max_k is correct, we should expect the test to pass because the approximate top-K algorithm is <a href="https://arxiv.org/abs/2206.14286">implemented</a> by dividing data points into L bins and computing the true max in each bin. L is chosen based on the desired average recall r as .</figcaption>
</figure>
<h2 id="systematically-generating-tests-via-fractional-proof-decomposition">Systematically generating tests via fractional proof decomposition</h2>
<figure>
  <img src="https://theorem.dev/images/Fractional%20Proofs%20Writing%20Systematic%20PBTs.svg" alt="Fractional proof decomposition process"/>
  <figcaption><strong>Figure 3:</strong> We encode theorems as property-based tests (PBTs), then recursively decompose them into smaller sub-theorems using reasoning, and fuzz the theorems, aka run PBTs, once the decomposition creates compute efficient unit-level PBTs.</figcaption>
</figure>
<p><strong>Step 1:</strong> Identify the theorem, which is the property that your implementation must satisfy. Then, encode it as a PBT using the <a href="https://hypothesis.readthedocs.io/en/latest/">Hypothesis</a> framework. We call the top-level theorem an end-to-end PBT because it corresponds to the end-to-end behavior of the function.</p>

<p>The theorem for the top-K bug is:</p><p>
$$\forall\ \text{prompt}, k,\ LLM_{\text{top-1}}(\text{prompt}) \in LLM_{\text{top-}k}(\text{prompt})$$</p><p>For any $\text{prompt}$, $LLM_{\text{top-k}}(\text{prompt})$ represents the model’s prediction of the top-K most likely next tokens. Now, we can encode this as an end-to-end PBT. Since the end-to-end PBT does not need to run on TPU, we set up a different <a href="https://colab.research.google.com/drive/1EHTacAGfL-od6j8gy2AUdoIX4ivu1g0J?usp=sharing">colab</a>.</p>
<figure>
  <!-- <img src="/images/Figure 4.png" alt="Top-K theorem encoded as PBT" style="max-width: 80%; height: auto;" /> -->
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@given</span>(prompt<span>=</span>st<span>.</span>text(min_size<span>=</span><span>1</span>, max_size<span>=</span>MAX_SIZE), k<span>=</span>st<span>.</span>integers(<span>1</span>, VOCAB_SIZE))
</span></span><span><span><span>@settings</span>(max_examples<span>=</span><span>50</span>, deadline<span>=</span><span>None</span>)
</span></span><span><span><span>def</span> <span>test_top_token_present</span>(prompt, k):
</span></span><span><span>  greedy_params <span>=</span> SamplingParams(temperature<span>=</span><span>0.0</span>, max_tokens<span>=</span><span>1</span>, logprobs<span>=</span><span>1</span>)
</span></span><span><span>  topk_params   <span>=</span> SamplingParams(temperature<span>=</span><span>1.0</span>, max_tokens<span>=</span><span>1</span>, logprobs<span>=</span>MAX_LOGPROBS, top_k<span>=</span>k)
</span></span><span><span>  most_likely_token <span>=</span> llm<span>.</span>generate([prompt], greedy_params, use_tqdm<span>=</span><span>False</span>)[<span>0</span>]<span>.</span>outputs[<span>0</span>]<span>.</span>token_ids[<span>0</span>]
</span></span><span><span>  logprobs          <span>=</span> llm<span>.</span>generate([prompt],   topk_params, use_tqdm<span>=</span><span>False</span>)[<span>0</span>]<span>.</span>outputs[<span>0</span>]<span>.</span>logprobs[<span>0</span>]
</span></span><span><span>
</span></span><span><span>  <span>assert</span> most_likely_token <span>in</span> logprobs
</span></span></code></pre></div>  <figcaption><strong>Figure 4:</strong> The theorem stating that the most likely token should always be included in the top k tokens, encoded as a PBT.</figcaption>
</figure>
<p>Although the end-to-end PBT has comprehensive coverage, to catch rare bugs would take an excessively large number of tokens. The rarer the bug, the more compute is required.</p>
<figure>
  <img src="https://theorem.dev/images/Fractional%20Proofs%20Formula%204.svg" alt="Compute requirements for rare bugs"/>
</figure>
<p><strong>Step 2:</strong> Recursively decompose the theorems into a collection of smaller theorems, which are also encoded as PBTs. These smaller theorems are intermediate results that compose together to establish the original end-to-end PBT.</p>
<p>Decomposing the end-to-end PBT for the top-K bug yields, by construction, three theorems:</p>
<ol>
<li><code>max(approximate_top_k(arr, k=k)) == max(arr)</code> (true max always included) <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li>On any input tokens, the logits are finite (not ∞ and not NaN)</li>
<li>In vLLM, the token ids are the same as the logprobs dict keys</li>
</ol>
<p>You can think of PBTs as fractional components of the brute-force proof. Just as the brute-force proof is optimized by decomposing properties into logical sub-properties via reasoning, better known as <a href="https://en.wikipedia.org/wiki/Partial_evaluation">partial evaluation</a>, we’re applying reasoning to decompose the  fractional brute-force proofs. The reasoning bootstraps trust in PBT coverage–so even though you don’t exhaustively check every single input like a formal proof, you get systematic understanding of your programs, and control over how you spend your testing compute. We’re calling this sampling technique fractional proofs.</p>
<p><strong>Step 3:</strong> Continue decomposing until the input space is small enough to be compute efficient at catching rare bugs. You stop decomposing when:</p>
<ol>
<li>Each sub-test runs sufficiently quickly,</li>
<li>Each sub-test tests a sufficient fraction of its input distribution so as to catch most bugs that would end up in code of similar complexity to that being tested, and</li>
<li>The sub-properties provably compose to cover the full end-to-end property</li>
</ol>
<p>While we found the top-K bug in about 10m of sampling, we were able to find the XLA:TPU bug (also discussed in Anthropic’s post), involving an issue with excess-precision, in just a few seconds. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<h2 id="fractional-proofs-for-efficient-oversight">Fractional proofs for efficient oversight</h2>
<p>Systematic decomposition catches rare bugs without sacrificing developer speed or compute efficiency. Instead of scaling compute proportional to the rarity of the bug, fractional proofs scale compute as the logarithm of rarity.</p>
<figure>
  <img src="https://theorem.dev/images/Fractional%20Proofs%20Formula%205.svg" alt="Compute scaling comparison"/>
</figure>
<p>We can straightforwardly extend the approximate top-K example in this post to real world codebases. For example, top-K can be decomposed into a sequence of PBTs testing how libtpu implements the algorithm described in its reference <a href="https://arxiv.org/abs/2206.14286">paper</a>. Or, we can use this reasoning to establish how single-TPU behavior composes into cluster behavior.</p>
<p>At Theorem, we’re training models that can automatically reason about program correctness. If you want to catch bugs earlier and make your devs happy, <a href="https://theorem.dev/cdn-cgi/l/email-protection#88e2e9fbe7e6c8fce0ede7faede5a6ecedfe">send me an email</a>.</p>

    </div>
</article>


  </div></div>
  </body>
</html>
