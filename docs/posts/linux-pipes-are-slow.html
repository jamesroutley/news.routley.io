<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qsantos.fr/2024/08/25/linux-pipes-are-slow/">Original</a>
    <h1>Linux Pipes Are Slow</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<h2><code>vmsplice</code> is <em>too</em> fast</h2>



<p>Some programs use a particular system call “<code>vmsplice</code>” to move data faster through a pipe. Francesco already did <a href="https://mazzo.li/posts/fast-pipes.html">a deep dive on using <code>vmsplice</code> to make things fast</a>. However, while experimenting with it, I noticed that, when not using <code>vmsplice</code>, Linux pipes are slower than what I would have expected. Since you cannot always use it, I wanted to understand exactly why that was, and whether it could be improved.</p>



<p>The reason I want to move data through pipes is that I am writing a program <a href="https://github.com/qsantos/ripmors/">encode/decode Morse code blazingly fast</a>.</p>



<p>To get a point of reference, the obvious candidate is the <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/">Fizz Buzz throughput competition at the Code Golf StackExchange</a>. There are two kinds of solutions:</p>



<ol>
<li>the ones that manage to reach up to a few gigabytes per second, with <a href="https://codegolf.stackexchange.com/a/215231">neil’s</a> reaching 8.4 GiB/s;</li>



<li>the ones which largely surpass that, from <a href="https://codegolf.stackexchange.com/a/256115">tkluck’s</a> at 15.5 GiB/s to <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630">ais523’s</a> at 60.8 GiB/s, to <a href="https://codegolf.stackexchange.com/a/269772">david’s</a> at 208.3 GiB/s using multiple cores.</li>
</ol>



<p>The difference between the first and the second group is that the second is using <code>vmsplice</code>, while the first is not<sup data-fn="25bc32bc-4b79-41b9-87b3-0db6adcecbbf"><a href="#25bc32bc-4b79-41b9-87b3-0db6adcecbbf" id="25bc32bc-4b79-41b9-87b3-0db6adcecbbf-link">1</a></sup>. But how can using <code>vmsplice</code> enable such a large gain in performance? My intuition about <code>vmsplice</code> is that it allows you to avoid copying data to and from kernel space. Surely, copying data cannot be slower than generating it? Even assuming it is not faster, and that you have to copy the data twice to get it through the pipe, you would assume a throughput gain of 3×, at best. But here, we have 7, even just looking at single-core solutions.</p>



<p>Something is missing in my mental model, I want to know what.</p>



<p>First, I’ll need to perform my own measurements to easily compare with what I’ll do afterward. Compiling and running aie523’s solution on my computer<sup data-fn="9f242c74-a611-49a2-9265-07d5945c4750"><a href="#9f242c74-a611-49a2-9265-07d5945c4750" id="9f242c74-a611-49a2-9265-07d5945c4750-link">2</a></sup>, I get:</p>



<pre><code>$ ./fizzbuzz | pv &gt;/dev/null
96.4GiB 0:00:01 [96.4GiB/s]</code></pre>



<p>With david’s solution, I reach 277 GB/s when using 7 cores (40 GB/s per core).</p>



<p>Now, to understand what’s going on, we need to find the answer to these questions:</p>



<ol>
<li>How fast can we write data <em>ideally</em>?</li>



<li>How fast can we <em>actually</em> write data to a pipe?</li>



<li>How does <code>vmsplice</code> help?</li>
</ol>



<h2>Writing Data in the Ideal Wonderland</h2>



<p>First, let’s consider the program below, which just copies data without doing any system call. I use <code><a href="https://doc.rust-lang.org/std/hint/fn.black_box.html">std::hint::black_box</a></code> to stop the compiler from noticing that we are not using the result. Without this, the compiler would optimize the program to nothing.</p>



<pre><code lang="rust">fn main() {
    let dst = [0u8; 1 &lt;&lt; 15];
    let src = [0u8; 1 &lt;&lt; 15];
    let mut copied = 0;
    while copied &lt; (1000 &lt;&lt; 30) {
        std::hint::black_box(dst).copy_from_slice(&amp;src);
        copied += src.len();
    }
}</code></pre>



<p>On my system, this runs at 167 GB/s. This is consistent with the speed of writing to L1 cache for my CPU<sup data-fn="e22c8070-2784-40d4-81ae-6916566ce03a"><a href="#e22c8070-2784-40d4-81ae-6916566ce03a" id="e22c8070-2784-40d4-81ae-6916566ce03a-link">3</a></sup>.</p>



<p>When <a href="https://jvns.ca/blog/2017/03/19/getting-started-with-ftrace/">profiling this with <code>ftrace</code></a>, we see that 99.9% of the time is spent in <code>__memset_avx512_unaligned_erms</code>, directly called by <code>main</code>, and calling no other functions. The flamegraph is pretty much flat. If you do not feel like running a full-fledged profiler, you can just <a href="https://yosefk.com/blog/profiling-with-ctrl-c.html">use <code>gdb</code> and hit Ctrl+C at a random time</a>:</p>



<pre><code lang="bash">$ cargo build --release
$ gdb target/release/copy 
…
(gdb) run
…
^C (hitting Ctrl+C)
Program received signal SIGINT, Interrupt.
__memset_avx512_unaligned_erms () at ../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:236
…
=&gt; 0x00007ffff7f15dba    f3 aa    rep stos %al,%es:(%rdi)</code></pre>



<p>In any case, note that we are using AVX-512. The reference to <code>memset</code> in the names can be surprising, but this is just because part of the logic is common with <code>memcpy</code>. The implementation is in a <a href="https://github.com/lattera/glibc/blob/master/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S">generic file dedicated to  SIMD <strong>vec</strong>torization</a> that supports SSE, AVX2 and AVX-512. In our case, <a href="https://github.com/lattera/glibc/blob/895ef79e04a953cac1493863bcae29ad85657ee1/sysdeps/x86_64/multiarch/memset-avx512-unaligned-erms.S#L4">the AVX-512 specialization</a> is used.</p>



<p>As an aside, note that the implementation of <code>memcpy</code> in glibc uses <a href="https://web.mit.edu/darwin/src/modules/xnu/osfmk/man/vm_copy.html">vm_copy</a> to copy pages directly on Mach-based systems (mostly Apply products) <a href="https://github.com/lattera/glibc/blob/master/string/memcpy.c#L44">uses a kernel feature to copy pages directly</a>.</p>



<p>However, AVX-512 is quite niche. According to <a href="https://store.steampowered.com/hwsurvey">Steam’s hardware survey</a> (section “Other Settings”), only about 12% of Steam users have it. In fact, Intel only included AVX-512 for consumer-grade processors in the 11th generation; and now reserves it for servers. AMD CPUs support AVX-512 since the Ryzen 7000 series (Zen 4).</p>



<p>So I tested this same program while disabling AVX-512. For this, I used the Linux kernel option <code><a href="https://www.phoronix.com/review/amd-zen4-avx512">clearcpuid=304</a></code>. I was able to check that it used <code>__memset_avx2_unaligned_erms</code> using the <code>gdb</code> and Ctrl+C trick. I then did the same to disable AVX2 with <code><a href="https://www.phoronix.com/news/Linux-5.19-Better-clearcpuid">clearcpuid=304,avx2,avx</a></code>, making it use <code>__memset_sse2_unaligned_erms</code>.</p>



<p>Although SSE2 is always available on x86-64, I also disabled the <code>cpuid</code> bit for SSE2 and SSE to see if it could nudge <code>glibc</code> into using scalar registers to copy data. I immediately got a kernel panic. Ah, well.</p>



<p>When using AVX2, the throughput was… 167 GB/s. When using only SSE2, the throughput was… still 167 GB/s. To an extent, it makes sense: even SSE2 is quite enough to fully use the bus and saturate L1 bandwidth. Using wider registers only helps when performing ALU operations.</p>



<p>The conclusion from this experiment is that, as long as vectorization is used, I should reach 167 GB/s.</p>



<h2>Actually Writing Data to a Pipe</h2>



<p>So, let’s look at what happen when we write to a pipe instead of to user space memory:</p>



<pre><code lang="rust">use std::io::Write;
use std::os::fd::FromRawFd;
fn main() {
    let vec = vec![b&#39;\0&#39;; 1 &lt;&lt; 15];
    let mut total_written = 0;
    let mut stdout = unsafe { std::fs::File::from_raw_fd(1) };
    while let Ok(n) = stdout.write(&amp;vec) {
        total_written += n;
        if total_written &gt;= (100 &lt;&lt; 30) {
            break;
        }
    }
}</code></pre>



<p>We then measure the throughput using:</p>



<pre><code lang="bash">cargo run --release | pv &gt;/dev/null</code></pre>



<p>On my computer, this reaches 17 GB/s. This is 10 times as slow as just writing to a buffer! How can a system call which basically writes to a kernel buffer be so much slower? And no, context switches don’t take that much time.</p>



<p>So let’s do some profiling of this program.</p>



<svg version="1.1" onload="init(evt)" viewBox="0 0 1200 646" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"></svg></div></div>
  </body>
</html>
