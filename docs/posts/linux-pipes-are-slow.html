<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://qsantos.fr/2024/08/25/linux-pipes-are-slow/">Original</a>
    <h1>Linux Pipes Are Slow</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<h2><code>vmsplice</code> is <em>too</em> fast</h2>



<p>Some programs use a particular system call “<code>vmsplice</code>” to move data faster through a pipe. Francesco already did <a href="https://mazzo.li/posts/fast-pipes.html">a deep dive on using <code>vmsplice</code> to make things fast</a>. However, while experimenting with it, I noticed that, when not using <code>vmsplice</code>, Linux pipes are slower than what I would have expected. Since you cannot always use it, I wanted to understand exactly why that was, and whether it could be improved.</p>



<p>The reason I want to move data through pipes is that I am writing a program <a href="https://github.com/qsantos/ripmors/">encode/decode Morse code blazingly fast</a>.</p>



<p>To get a point of reference, the obvious candidate is the <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/">Fizz Buzz throughput competition at the Code Golf StackExchange</a>. There are two kinds of solutions:</p>



<ol>
<li>the ones that manage to reach up to a few gigabytes per second, with <a href="https://codegolf.stackexchange.com/a/215231">neil’s</a> reaching 8.4 GiB/s;</li>



<li>the ones which largely surpass that, from <a href="https://codegolf.stackexchange.com/a/256115">tkluck’s</a> at 15.5 GiB/s to <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630">ais523’s</a> at 60.8 GiB/s, to <a href="https://codegolf.stackexchange.com/a/269772">david’s</a> at 208.3 GiB/s using multiple cores.</li>
</ol>



<p>The difference between the first and the second group is that the second is using <code>vmsplice</code>, while the first is not<sup data-fn="25bc32bc-4b79-41b9-87b3-0db6adcecbbf"><a href="#25bc32bc-4b79-41b9-87b3-0db6adcecbbf" id="25bc32bc-4b79-41b9-87b3-0db6adcecbbf-link">1</a></sup>. But how can using <code>vmsplice</code> enable such a large gain in performance? My intuition about <code>vmsplice</code> is that it allows you to avoid copying data to and from kernel space. Surely, copying data cannot be slower than generating it? Even assuming it is not faster, and that you have to copy the data twice to get it through the pipe, you would assume a throughput gain of 3×, at best. But here, we have 7, even just looking at single-core solutions.</p>



<p>Something is missing in my mental model, I want to know what.</p>



<p>First, I’ll need to perform my own measurements to easily compare with what I’ll do afterward. Compiling and running aie523’s solution on my computer<sup data-fn="9f242c74-a611-49a2-9265-07d5945c4750"><a href="#9f242c74-a611-49a2-9265-07d5945c4750" id="9f242c74-a611-49a2-9265-07d5945c4750-link">2</a></sup>, I get:</p>



<pre><code>$ ./fizzbuzz | pv &gt;/dev/null
96.4GiB 0:00:01 [96.4GiB/s]</code></pre>



<p>With david’s solution, I reach 277 GB/s when using 7 cores (40 GB/s per core).</p>



<p>Now, to understand what’s going on, we need to find the answer to these questions:</p>



<ol>
<li>How fast can we write data <em>ideally</em>?</li>



<li>How fast can we <em>actually</em> write data to a pipe?</li>



<li>How does <code>vmsplice</code> help?</li>
</ol>



<h2>Writing Data in the Ideal Wonderland</h2>



<p>First, let’s consider the program below, which just copies data without doing any system call. I use <code><a href="https://doc.rust-lang.org/std/hint/fn.black_box.html">std::hint::black_box</a></code> to stop the compiler from noticing that we are not using the result. Without this, the compiler would optimize the program to nothing.</p>



<pre><code lang="rust">fn main() {
    let dst = [0u8; 1 &lt;&lt; 15];
    let src = [0u8; 1 &lt;&lt; 15];
    let mut copied = 0;
    while copied &lt; (1000 &lt;&lt; 30) {
        std::hint::black_box(dst).copy_from_slice(&amp;src);
        copied += src.len();
    }
}</code></pre>



<p>On my system, this runs at 167 GB/s. This is consistent with the speed of writing to L1 cache for my CPU<sup data-fn="e22c8070-2784-40d4-81ae-6916566ce03a"><a href="#e22c8070-2784-40d4-81ae-6916566ce03a" id="e22c8070-2784-40d4-81ae-6916566ce03a-link">3</a></sup>.</p>



<p>When <a href="https://jvns.ca/blog/2017/03/19/getting-started-with-ftrace/">profiling this with <code>ftrace</code></a>, we see that 99.9% of the time is spent in <code>__memset_avx512_unaligned_erms</code>, directly called by <code>main</code>, and calling no other functions. The flamegraph is pretty much flat. If you do not feel like running a full-fledged profiler, you can just <a href="https://yosefk.com/blog/profiling-with-ctrl-c.html">use <code>gdb</code> and hit Ctrl+C at a random time</a>:</p>



<pre><code lang="bash">$ cargo build --release
$ gdb target/release/copy 
…
(gdb) run
…
^C (hitting Ctrl+C)
Program received signal SIGINT, Interrupt.
__memset_avx512_unaligned_erms () at ../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:236
…
=&gt; 0x00007ffff7f15dba    f3 aa    rep stos %al,%es:(%rdi)</code></pre>



<p>In any case, note that we are using AVX-512. The reference to <code>memset</code> in the names can be surprising, but this is just because part of the logic is common with <code>memcpy</code>. The implementation is in a <a href="https://github.com/lattera/glibc/blob/master/sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S">generic file dedicated to  SIMD <strong>vec</strong>torization</a> that supports SSE, AVX2 and AVX-512. In our case, <a href="https://github.com/lattera/glibc/blob/895ef79e04a953cac1493863bcae29ad85657ee1/sysdeps/x86_64/multiarch/memset-avx512-unaligned-erms.S#L4">the AVX-512 specialization</a> is used.</p>



<p>As an aside, note that the implementation of <code>memcpy</code> in glibc uses <a href="https://web.mit.edu/darwin/src/modules/xnu/osfmk/man/vm_copy.html">vm_copy</a> to copy pages directly on Mach-based systems (mostly Apply products) <a href="https://github.com/lattera/glibc/blob/master/string/memcpy.c#L44">uses a kernel feature to copy pages directly</a>.</p>



<p>However, AVX-512 is quite niche. According to <a href="https://store.steampowered.com/hwsurvey">Steam’s hardware survey</a> (section “Other Settings”), only about 12% of Steam users have it. In fact, Intel only included AVX-512 for consumer-grade processors in the 11th generation; and now reserves it for servers. AMD CPUs support AVX-512 since the Ryzen 7000 series (Zen 4).</p>



<p>So I tested this same program while disabling AVX-512. For this, I used the Linux kernel option <code><a href="https://www.phoronix.com/review/amd-zen4-avx512">clearcpuid=304</a></code>. I was able to check that it used <code>__memset_avx2_unaligned_erms</code> using the <code>gdb</code> and Ctrl+C trick. I then did the same to disable AVX2 with <code><a href="https://www.phoronix.com/news/Linux-5.19-Better-clearcpuid">clearcpuid=304,avx2,avx</a></code>, making it use <code>__memset_sse2_unaligned_erms</code>.</p>



<p>Although SSE2 is always available on x86-64, I also disabled the <code>cpuid</code> bit for SSE2 and SSE to see if it could nudge <code>glibc</code> into using scalar registers to copy data. I immediately got a kernel panic. Ah, well.</p>



<p>When using AVX2, the throughput was… 167 GB/s. When using only SSE2, the throughput was… still 167 GB/s. To an extent, it makes sense: even SSE2 is quite enough to fully use the bus and saturate L1 bandwidth. Using wider registers only helps when performing ALU operations.</p>



<p>The conclusion from this experiment is that, as long as vectorization is used, I should reach 167 GB/s.</p>



<h2>Actually Writing Data to a Pipe</h2>



<p>So, let’s look at what happen when we write to a pipe instead of to user space memory:</p>



<pre><code lang="rust">use std::io::Write;
use std::os::fd::FromRawFd;
fn main() {
    let vec = vec![b&#39;\0&#39;; 1 &lt;&lt; 15];
    let mut total_written = 0;
    let mut stdout = unsafe { std::fs::File::from_raw_fd(1) };
    while let Ok(n) = stdout.write(&amp;vec) {
        total_written += n;
        if total_written &gt;= (100 &lt;&lt; 30) {
            break;
        }
    }
}</code></pre>



<p>We then measure the throughput using:</p>



<pre><code lang="bash">cargo run --release | pv &gt;/dev/null</code></pre>



<p>On my computer, this reaches 17 GB/s. This is 10 times as slow as just writing to a buffer! How can a system call which basically writes to a kernel buffer be so much slower? And no, context switches don’t take that much time.</p>



<p>So let’s do some profiling of this program.</p>



<svg version="1.1" onload="init(evt)" viewBox="0 0 1200 646" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<!-- Flame graph stack visualization. See https://github.com/brendangregg/FlameGraph for latest version, and http://www.brendangregg.com/flamegraphs.html for examples. -->
<!-- NOTES:  -->
<defs>
	<linearGradient id="background" y1="0" y2="1" x1="0" x2="0">
		<stop stop-color="#eeeeee" offset="5%"></stop>
		<stop stop-color="#eeeeb0" offset="95%"></stop>
	</linearGradient>
</defs>


<rect x="0.0" y="0" width="1200.0" height="646.0" fill="url(#background)"></rect>
<text id="title" x="600.00" y="24">./zeroes | pv &gt;/dev/null</text>
<text id="subtitle" x="600.00" y="48">Profiling of ./zeroes</text>
<text id="details" x="10.00" y="629"> </text>
<text id="unzoom" x="10.00" y="24">Reset Zoom</text>
<text id="search" x="1090.00" y="24">Search</text>
<text id="ignorecase" x="1174.00" y="24">ic</text>
<text id="matched" x="1090.00" y="629"> </text>
<g id="frames">
<g>
<title>__x86_return_thunk (4,202,612 samples, 0.01%)</title><rect x="213.3" y="325" width="0.1" height="23.0" fill="rgb(253,221,53)" rx="2" ry="2"></rect>
<text x="216.26" y="339.5"></text>
</g>
<g>
<title>exit_to_user_mode_prepare (23,309,116 samples, 0.07%)</title><rect x="1187.8" y="397" width="0.8" height="23.0" fill="rgb(228,108,25)" rx="2" ry="2"></rect>
<text x="1190.80" y="411.5"></text>
</g>
<g>
<title>_raw_spin_lock_irqsave (25,962,963 samples, 0.08%)</title><rect x="504.7" y="277" width="0.9" height="23.0" fill="rgb(247,195,46)" rx="2" ry="2"></rect>
<text x="507.67" y="291.5"></text>
</g>
<g>
<title>__x86_return_thunk (32,381,871 samples, 0.09%)</title><rect x="259.4" y="301" width="1.1" height="23.0" fill="rgb(253,221,53)" rx="2" ry="2"></rect>
<text x="262.38" y="315.5"></text>
</g>
<g>
<title>try_charge_memcg (846,277,863 samples, 2.46%)</title><rect x="184.2" y="301" width="29.1" height="23.0" fill="rgb(210,27,6)" rx="2" ry="2"></rect>
<text x="187.24" y="315.5">tr..</text>
</g>
<g>
<title>_raw_spin_unlock_irq (288,030,674 samples, 0.84%)</title><rect x="892.0" y="349" width="9.9" height="23.0" fill="rgb(215,47,11)" rx="2" ry="2"></rect>
<text x="895.00" y="363.5"></text>
</g>
<g>
<title>check_new_pages (310,465,336 samples, 0.90%)</title><rect x="505.7" y="277" width="10.6" height="23.0" fill="rgb(249,202,48)" rx="2" ry="2"></rect>
<text x="508.70" y="291.5"></text>
</g>
<g>
<title>osq_lock (117,185,036 samples, 0.34%)</title><rect x="803.0" y="325" width="4.1" height="23.0" fill="rgb(214,43,10)" rx="2" ry="2"></rect>
<text x="806.04" y="339.5"></text>
</g>
<g>
<title>__rcu_read_lock (226,040,208 samples, 0.66%)</title><rect x="145.4" y="301" width="7.7" height="23.0" fill="rgb(220,69,16)" rx="2" ry="2"></rect>
<text x="148.37" y="315.5"></text>
</g>
<g>
<title>__GI___libc_write (34,368,952,857 samples, 99.89%)</title><rect x="11.3" y="493" width="1178.7" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="14.30" y="507.5">__GI___libc_write</text>
</g>
<g>
<title>&lt;std::os::unix::net::stream::UnixStream as std::io::Write&gt;::write (12,137,904 samples, 0.04%)</title><rect x="10.0" y="517" width="0.4" height="23.0" fill="rgb(239,160,38)" rx="2" ry="2"></rect>
<text x="13.00" y="531.5"></text>
</g>
<g>
<title>__hrtimer_run_queues (11,254,762 samples, 0.03%)</title><rect x="802.6" y="205" width="0.3" height="23.0" fill="rgb(237,150,35)" rx="2" ry="2"></rect>
<text x="805.56" y="219.5"></text>
</g>
<g>
<title>_raw_spin_unlock_irqrestore (16,388,541 samples, 0.05%)</title><rect x="830.2" y="325" width="0.5" height="23.0" fill="rgb(228,106,25)" rx="2" ry="2"></rect>
<text x="833.16" y="339.5"></text>
</g>
<g>
<title>fpregs_assert_state_consistent (4,142,862 samples, 0.01%)</title><rect x="1188.5" y="373" width="0.1" height="23.0" fill="rgb(228,109,26)" rx="2" ry="2"></rect>
<text x="1191.45" y="387.5"></text>
</g>
<g>
<title>vfs_write (33,501,533,082 samples, 97.36%)</title><rect x="36.1" y="397" width="1148.9" height="23.0" fill="rgb(250,209,50)" rx="2" ry="2"></rect>
<text x="39.07" y="411.5">vfs_write</text>
</g>
<g>
<title>__x86_return_thunk (15,219,222 samples, 0.04%)</title><rect x="183.7" y="253" width="0.5" height="23.0" fill="rgb(253,221,53)" rx="2" ry="2"></rect>
<text x="186.72" y="267.5"></text>
</g>
<g>
<title>aa_file_perm (30,051,576 samples, 0.09%)</title><rect x="1183.1" y="325" width="1.0" height="23.0" fill="rgb(229,110,26)" rx="2" ry="2"></rect>
<text x="1186.09" y="339.5"></text>
</g>
<g>
<title>_find_first_bit (57,390,916 samples, 0.17%)</title><rect x="908.8" y="301" width="2.0" height="23.0" fill="rgb(252,216,51)" rx="2" ry="2"></rect>
<text x="911.79" y="315.5"></text>
</g>
<g>
<title>__list_del_entry_valid (120,783,301 samples, 0.35%)</title><rect x="255.2" y="301" width="4.2" height="23.0" fill="rgb(239,158,37)" rx="2" ry="2"></rect>
<text x="258.24" y="315.5"></text>
</g>
<g>
<title>hrtimer_interrupt (12,658,212 samples, 0.04%)</title><rect x="802.6" y="229" width="0.4" height="23.0" fill="rgb(228,109,26)" rx="2" ry="2"></rect>
<text x="805.56" y="243.5"></text>
</g>
<g>
<title>_raw_spin_trylock (959,948,236 samples, 2.79%)</title><rect x="260.5" y="301" width="32.9" height="23.0" fill="rgb(222,80,19)" rx="2" ry="2"></rect>
<text x="263.49" y="315.5">_r..</text>
</g>
<g>
<title>file_update_time (4,074,965 samples, 0.01%)</title><rect x="1151.6" y="349" width="0.1" height="23.0" fill="rgb(210,27,6)" rx="2" ry="2"></rect>
<text x="1154.60" y="363.5"></text>
</g>
<g>
<title>__mod_zone_page_state (32,134,035 samples, 0.09%)</title><rect x="503.6" y="277" width="1.1" height="23.0" fill="rgb(221,74,17)" rx="2" ry="2"></rect>
<text x="506.57" y="291.5"></text>
</g>
<g>
<title>_raw_spin_unlock_irqrestore (4,140,183 samples, 0.01%)</title><rect x="505.6" y="277" width="0.1" height="23.0" fill="rgb(228,106,25)" rx="2" ry="2"></rect>
<text x="508.56" y="291.5"></text>
</g>
<g>
<title>syscall_exit_to_user_mode (30,099,876 samples, 0.09%)</title><rect x="1187.7" y="421" width="1.0" height="23.0" fill="rgb(251,211,50)" rx="2" ry="2"></rect>
<text x="1190.66" y="435.5"></text>
</g>
<g>
<title>all (34,408,358,444 samples, 100%)</title><rect x="10.0" y="589" width="1180.0" height="23.0" fill="rgb(213,39,9)" rx="2" ry="2"></rect>
<text x="13.00" y="603.5"></text>
</g>
<g>
<title>__x86_return_thunk (16,313,413 samples, 0.05%)</title><rect x="906.1" y="325" width="0.5" height="23.0" fill="rgb(253,221,53)" rx="2" ry="2"></rect>
<text x="909.05" y="339.5"></text>
</g>
<g>
<title>entry_SYSCALL_64 (134,570,535 samples, 0.39%)</title><rect x="16.9" y="469" width="4.6" height="23.0" fill="rgb(239,156,37)" rx="2" ry="2"></rect>
<text x="19.92" y="483.5"></text>
</g>
<g>
<title>__sysvec_apic_timer_interrupt (12,658,212 samples, 0.04%)</title><rect x="802.6" y="253" width="0.4" height="23.0" fill="rgb(242,173,41)" rx="2" ry="2"></rect>
<text x="805.56" y="267.5"></text>
</g>
<g>
<title>asm_sysvec_apic_timer_interrupt (16,786,362 samples, 0.05%)</title><rect x="802.4" y="301" width="0.6" height="23.0" fill="rgb(232,127,30)" rx="2" ry="2"></rect>
<text x="805.41" y="315.5"></text>
</g>
<g>
<title>update_process_times (7,029,218 samples, 0.02%)</title><rect x="802.7" y="133" width="0.2" height="23.0" fill="rgb(250,209,50)" rx="2" ry="2"></rect>
<text x="805.70" y="147.5"></text>
</g>
<g>
<title>policy_nodemask (33,222,227 samples, 0.10%)</title><rect x="910.8" y="325" width="1.1" height="23.0" fill="rgb(221,74,17)" rx="2" ry="2"></rect>
<text x="913.76" y="339.5"></text>
</g>
<g>
<title>cgroup_rstat_updated (79,560,370 samples, 0.23%)</title><rect x="181.0" y="229" width="2.7" height="23.0" fill="rgb(244,180,43)" rx="2" ry="2"></rect>
<text x="183.99" y="243.5"></text>
</g>
<g>
<title>_copy_from_iter (6,966,591,966 samples, 20.25%)</title><rect x="912.7" y="325" width="238.9" height="23.0" fill="rgb(227,104,24)" rx="2" ry="2"></rect>
<text x="915.69" y="339.5">_copy_from_iter</text>
</g>
<g>
<title>__list_del_entry_valid (400,133,559 samples, 1.16%)</title><rect x="489.8" y="277" width="13.8" height="23.0" fill="rgb(239,158,37)" rx="2" ry="2"></rect>
<text x="492.84" y="291.5"></text>
</g>
<g>
<title>clear_page_erms (4,655,903,634 samples, 13.53%)</title><rect x="296.0" y="301" width="159.7" height="23.0" fill="rgb(229,113,27)" rx="2" ry="2"></rect>
<text x="299.01" y="315.5">clear_page_erms</text>
</g>
<g>
<title>update_wall_time (4,225,544 samples, 0.01%)</title><rect x="802.6" y="133" width="0.1" height="23.0" fill="rgb(207,11,2)" rx="2" ry="2"></rect>
<text x="805.56" y="147.5"></text>
</g>
<g>
<title>__rcu_read_unlock (6,832,345 samples, 0.02%)</title><rect x="1183.9" y="301" width="0.2" height="23.0" fill="rgb(253,224,53)" rx="2" ry="2"></rect>
<text x="1186.89" y="315.5"></text>
</g>
<g>
<title>refill_stock (17,620,376 samples, 0.05%)</title><rect x="212.7" y="277" width="0.6" height="23.0" fill="rgb(238,153,36)" rx="2" ry="2"></rect>
<text x="215.66" y="291.5"></text>
</g>
<g>
<title>ksys_write (33,653,152,549 samples, 97.81%)</title><rect x="30.9" y="421" width="1154.1" height="23.0" fill="rgb(205,4,1)" rx="2" ry="2"></rect>
<text x="33.87" y="435.5">ksys_write</text>
</g>
<g>
<title>sysvec_apic_timer_interrupt (13,980,036 samples, 0.04%)</title><rect x="802.5" y="277" width="0.5" height="23.0" fill="rgb(220,69,16)" rx="2" ry="2"></rect>
<text x="805.51" y="291.5"></text>
</g>
<g>
<title>__memcg_kmem_charge_page (2,947,262,302 samples, 8.57%)</title><rect x="112.2" y="325" width="101.1" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="115.19" y="339.5">__memcg_kmem..</text>
</g>
<g>
<title>sysvec_apic_timer_interrupt (4,214,752 samples, 0.01%)</title><rect x="455.5" y="253" width="0.2" height="23.0" fill="rgb(220,69,16)" rx="2" ry="2"></rect>
<text x="458.53" y="267.5"></text>
</g>
<g>
<title>get_page_from_freelist (8,833,731,051 samples, 25.67%)</title><rect x="213.4" y="325" width="302.9" height="23.0" fill="rgb(252,218,52)" rx="2" ry="2"></rect>
<text x="216.40" y="339.5">get_page_from_freelist</text>
</g>
<g>
<title>rw_verify_area (10,800,291 samples, 0.03%)</title><rect x="1170.5" y="373" width="0.4" height="23.0" fill="rgb(218,64,15)" rx="2" ry="2"></rect>
<text x="1173.54" y="387.5"></text>
</g>
<g>
<title>tick_sched_handle (7,029,218 samples, 0.02%)</title><rect x="802.7" y="157" width="0.2" height="23.0" fill="rgb(219,68,16)" rx="2" ry="2"></rect>
<text x="805.70" y="171.5"></text>
</g>
<g>
<title>tick_sched_do_timer (4,225,544 samples, 0.01%)</title><rect x="802.6" y="157" width="0.1" height="23.0" fill="rgb(227,104,25)" rx="2" ry="2"></rect>
<text x="805.56" y="171.5"></text>
</g>
<g>
<title>__get_obj_cgroup_from_memcg (460,978,258 samples, 1.34%)</title><rect x="129.6" y="301" width="15.8" height="23.0" fill="rgb(209,21,5)" rx="2" ry="2"></rect>
<text x="132.56" y="315.5"></text>
</g>
<g>
<title>asm_sysvec_apic_timer_interrupt (4,105,206 samples, 0.01%)</title><rect x="295.9" y="301" width="0.1" height="23.0" fill="rgb(232,127,30)" rx="2" ry="2"></rect>
<text x="298.87" y="315.5"></text>
</g>
<g>
<title>__mutex_lock.constprop.0 (8,703,282,714 samples, 25.29%)</title><rect x="516.3" y="349" width="298.5" height="23.0" fill="rgb(225,95,22)" rx="2" ry="2"></rect>
<text x="519.35" y="363.5">__mutex_lock.constprop.0</text>
</g>
<g>
<title>timekeeping_advance (4,225,544 samples, 0.01%)</title><rect x="802.6" y="109" width="0.1" height="23.0" fill="rgb(227,104,25)" rx="2" ry="2"></rect>
<text x="805.56" y="123.5"></text>
</g>
<g>
<title>security_file_permission (410,026,776 samples, 1.19%)</title><rect x="1170.9" y="373" width="14.1" height="23.0" fill="rgb(225,96,23)" rx="2" ry="2"></rect>
<text x="1173.91" y="387.5"></text>
</g>
<g>
<title>apparmor_file_permission (317,347,965 samples, 0.92%)</title><rect x="1173.3" y="349" width="10.9" height="23.0" fill="rgb(218,60,14)" rx="2" ry="2"></rect>
<text x="1176.29" y="363.5"></text>
</g>
<g>
<title>zeroes::main (34,396,128,845 samples, 99.96%)</title><rect x="10.4" y="517" width="1179.6" height="23.0" fill="rgb(205,1,0)" rx="2" ry="2"></rect>
<text x="13.42" y="531.5">zeroes::main</text>
</g>
<g>
<title>alloc_pages (292,194,676 samples, 0.85%)</title><rect x="901.9" y="349" width="10.0" height="23.0" fill="rgb(210,25,6)" rx="2" ry="2"></rect>
<text x="904.88" y="363.5"></text>
</g>
<g>
<title>copy_page_from_iter (6,986,960,281 samples, 20.31%)</title><rect x="912.0" y="349" width="239.6" height="23.0" fill="rgb(209,22,5)" rx="2" ry="2"></rect>
<text x="914.99" y="363.5">copy_page_from_iter</text>
</g>
<g>
<title>mutex_lock (389,788,556 samples, 1.13%)</title><rect x="1151.8" y="349" width="13.4" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="1154.79" y="363.5"></text>
</g>
<g>
<title>&lt;std::os::unix::net::stream::UnixStream as std::io::Write&gt;::write (21,805,012 samples, 0.06%)</title><rect x="10.6" y="493" width="0.7" height="23.0" fill="rgb(239,160,38)" rx="2" ry="2"></rect>
<text x="13.56" y="507.5"></text>
</g>
<g>
<title>_raw_spin_lock_irqsave (431,015,735 samples, 1.25%)</title><rect x="815.4" y="325" width="14.8" height="23.0" fill="rgb(247,195,46)" rx="2" ry="2"></rect>
<text x="818.37" y="339.5"></text>
</g>
<g>
<title>policy_node (119,598,949 samples, 0.35%)</title><rect x="906.7" y="325" width="4.1" height="23.0" fill="rgb(236,143,34)" rx="2" ry="2"></rect>
<text x="909.66" y="339.5"></text>
</g>
<g>
<title>__sysvec_apic_timer_interrupt (4,214,752 samples, 0.01%)</title><rect x="455.5" y="229" width="0.2" height="23.0" fill="rgb(242,173,41)" rx="2" ry="2"></rect>
<text x="458.53" y="243.5"></text>
</g>
<g>
<title>mutex_spin_on_owner (7,651,102,789 samples, 22.24%)</title><rect x="540.7" y="325" width="262.3" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="543.65" y="339.5">mutex_spin_on_owner</text>
</g>
<g>
<title>mutex_unlock (154,311,955 samples, 0.45%)</title><rect x="1165.2" y="349" width="5.2" height="23.0" fill="rgb(251,212,50)" rx="2" ry="2"></rect>
<text x="1168.15" y="363.5"></text>
</g>
<g>
<title>entry_SYSRETQ_unsafe_stack (4,116,333 samples, 0.01%)</title><rect x="1188.8" y="469" width="0.2" height="23.0" fill="rgb(206,7,1)" rx="2" ry="2"></rect>
<text x="1191.83" y="483.5"></text>
</g>
<g>
<title>__wake_up_common_lock (463,608,242 samples, 1.35%)</title><rect x="814.8" y="349" width="15.9" height="23.0" fill="rgb(238,155,37)" rx="2" ry="2"></rect>
<text x="817.82" y="363.5"></text>
</g>
<g>
<title>__mod_memcg_state (156,325,243 samples, 0.45%)</title><rect x="178.4" y="253" width="5.3" height="23.0" fill="rgb(205,1,0)" rx="2" ry="2"></rect>
<text x="181.36" y="267.5"></text>
</g>
<g>
<title>__fdget_pos (131,186,857 samples, 0.38%)</title><rect x="31.6" y="397" width="4.5" height="23.0" fill="rgb(216,55,13)" rx="2" ry="2"></rect>
<text x="34.57" y="411.5"></text>
</g>
<g>
<title>__rcu_read_unlock (256,610,337 samples, 0.75%)</title><rect x="136.5" y="277" width="8.8" height="23.0" fill="rgb(253,224,53)" rx="2" ry="2"></rect>
<text x="139.48" y="291.5"></text>
</g>
<g>
<title>update_vsyscall (4,225,544 samples, 0.01%)</title><rect x="802.6" y="61" width="0.1" height="23.0" fill="rgb(254,229,54)" rx="2" ry="2"></rect>
<text x="805.56" y="75.5"></text>
</g>
<g>
<title>__alloc_pages (12,488,057,959 samples, 36.29%)</title><rect x="88.1" y="349" width="428.2" height="23.0" fill="rgb(233,129,30)" rx="2" ry="2"></rect>
<text x="91.08" y="363.5">__alloc_pages</text>
</g>
<g>
<title>copy_user_enhanced_fast_string (6,911,972,451 samples, 20.09%)</title><rect x="914.6" y="301" width="237.0" height="23.0" fill="rgb(238,155,37)" rx="2" ry="2"></rect>
<text x="917.56" y="315.5">copy_user_enhanced_fast_string</text>
</g>
<g>
<title>mod_memcg_state (483,429,269 samples, 1.40%)</title><rect x="167.7" y="277" width="16.5" height="23.0" fill="rgb(232,127,30)" rx="2" ry="2"></rect>
<text x="170.66" y="291.5"></text>
</g>
<g>
<title>__wake_up_common (7,952,263 samples, 0.02%)</title><rect x="815.1" y="325" width="0.3" height="23.0" fill="rgb(248,197,47)" rx="2" ry="2"></rect>
<text x="818.10" y="339.5"></text>
</g>
<g>
<title>page_counter_try_charge (100,471,631 samples, 0.29%)</title><rect x="209.2" y="277" width="3.5" height="23.0" fill="rgb(233,132,31)" rx="2" ry="2"></rect>
<text x="212.21" y="291.5"></text>
</g>
<g>
<title>__x64_sys_write (103,208,079 samples, 0.30%)</title><rect x="27.3" y="421" width="3.6" height="23.0" fill="rgb(246,189,45)" rx="2" ry="2"></rect>
<text x="30.33" y="435.5"></text>
</g>
<g>
<title>syscall_enter_from_user_mode (78,405,130 samples, 0.23%)</title><rect x="1185.0" y="421" width="2.7" height="23.0" fill="rgb(254,229,54)" rx="2" ry="2"></rect>
<text x="1187.97" y="435.5"></text>
</g>
<g>
<title>asm_sysvec_apic_timer_interrupt (5,629,654 samples, 0.02%)</title><rect x="455.5" y="277" width="0.2" height="23.0" fill="rgb(232,127,30)" rx="2" ry="2"></rect>
<text x="458.49" y="291.5"></text>
</g>
<g>
<title>osq_unlock (226,312,628 samples, 0.66%)</title><rect x="807.1" y="325" width="7.7" height="23.0" fill="rgb(248,198,47)" rx="2" ry="2"></rect>
<text x="810.06" y="339.5"></text>
</g>
<g>
<title>timekeeping_update (4,225,544 samples, 0.01%)</title><rect x="802.6" y="85" width="0.1" height="23.0" fill="rgb(238,152,36)" rx="2" ry="2"></rect>
<text x="805.56" y="99.5"></text>
</g>
<g>
<title>zeroes (34,408,355,195 samples, 100.00%)</title><rect x="10.0" y="565" width="1180.0" height="23.0" fill="rgb(224,88,21)" rx="2" ry="2"></rect>
<text x="13.00" y="579.5">zeroes</text>
</g>
<g>
<title>__list_add_valid (119,183,921 samples, 0.35%)</title><rect x="485.8" y="277" width="4.0" height="23.0" fill="rgb(211,28,6)" rx="2" ry="2"></rect>
<text x="488.76" y="291.5"></text>
</g>
<g>
<title>__fget_light (122,843,196 samples, 0.36%)</title><rect x="31.9" y="373" width="4.2" height="23.0" fill="rgb(233,132,31)" rx="2" ry="2"></rect>
<text x="34.85" y="387.5"></text>
</g>
<g>
<title>propagate_protected_usage (5,505,752 samples, 0.02%)</title><rect x="212.5" y="253" width="0.2" height="23.0" fill="rgb(206,5,1)" rx="2" ry="2"></rect>
<text x="215.47" y="267.5"></text>
</g>
<g>
<title>__cond_resched (11,051,020 samples, 0.03%)</title><rect x="540.3" y="325" width="0.4" height="23.0" fill="rgb(217,58,14)" rx="2" ry="2"></rect>
<text x="543.27" y="339.5"></text>
</g>
<g>
<title>_raw_spin_unlock (71,553,131 samples, 0.21%)</title><rect x="293.4" y="301" width="2.5" height="23.0" fill="rgb(223,85,20)" rx="2" ry="2"></rect>
<text x="296.42" y="315.5"></text>
</g>
<g>
<title>do_syscall_64 (33,995,562,642 samples, 98.80%)</title><rect x="22.8" y="445" width="1165.9" height="23.0" fill="rgb(209,20,4)" rx="2" ry="2"></rect>
<text x="25.85" y="459.5">do_syscall_64</text>
</g>
<g>
<title>memcg_account_kmem (495,700,356 samples, 1.44%)</title><rect x="167.2" y="301" width="17.0" height="23.0" fill="rgb(230,115,27)" rx="2" ry="2"></rect>
<text x="170.24" y="315.5"></text>
</g>
<g>
<title>entry_SYSCALL_64_safe_stack (4,103,934 samples, 0.01%)</title><rect x="1188.7" y="469" width="0.1" height="23.0" fill="rgb(231,120,28)" rx="2" ry="2"></rect>
<text x="1191.69" y="483.5"></text>
</g>
<g>
<title>__rcu_read_lock (131,786,573 samples, 0.38%)</title><rect x="132.0" y="277" width="4.5" height="23.0" fill="rgb(220,69,16)" rx="2" ry="2"></rect>
<text x="134.96" y="291.5"></text>
</g>
<g>
<title>tick_sched_timer (11,254,762 samples, 0.03%)</title><rect x="802.6" y="181" width="0.3" height="23.0" fill="rgb(254,227,54)" rx="2" ry="2"></rect>
<text x="805.56" y="195.5"></text>
</g>
<g>
<title>entry_SYSCALL_64_after_hwframe (34,033,680,043 samples, 98.91%)</title><rect x="21.5" y="469" width="1167.2" height="23.0" fill="rgb(218,63,15)" rx="2" ry="2"></rect>
<text x="24.54" y="483.5">entry_SYSCALL_64_after_hwframe</text>
</g>
<g>
<title>__get_task_ioprio (65,612,447 samples, 0.19%)</title><rect x="41.6" y="373" width="2.3" height="23.0" fill="rgb(230,119,28)" rx="2" ry="2"></rect>
<text x="44.64" y="387.5"></text>
</g>
<g>
<title>rmqueue_bulk (1,769,082,103 samples, 5.14%)</title><rect x="455.7" y="301" width="60.6" height="23.0" fill="rgb(235,138,33)" rx="2" ry="2"></rect>
<text x="458.68" y="315.5">rmqueu..</text>
</g>
<g>
<title>__cond_resched (5,443,361 samples, 0.02%)</title><rect x="1164.9" y="325" width="0.2" height="23.0" fill="rgb(217,58,14)" rx="2" ry="2"></rect>
<text x="1167.92" y="339.5"></text>
</g>
<g>
<title>__rcu_read_unlock (394,985,353 samples, 1.15%)</title><rect x="153.1" y="301" width="13.6" height="23.0" fill="rgb(253,224,53)" rx="2" ry="2"></rect>
<text x="156.12" y="315.5"></text>
</g>
<g>
<title>__x86_indirect_thunk_array (11,044,363 samples, 0.03%)</title><rect x="16.5" y="469" width="0.3" height="23.0" fill="rgb(231,123,29)" rx="2" ry="2"></rect>
<text x="19.45" y="483.5"></text>
</g>
<g>
<title>pipe_write (32,852,709,554 samples, 95.48%)</title><rect x="43.9" y="373" width="1126.6" height="23.0" fill="rgb(236,146,35)" rx="2" ry="2"></rect>
<text x="46.89" y="387.5">pipe_write</text>
</g>
<g>
<title>bpf_lsm_file_permission (23,314,251 samples, 0.07%)</title><rect x="1184.2" y="349" width="0.8" height="23.0" fill="rgb(238,154,36)" rx="2" ry="2"></rect>
<text x="1187.17" y="363.5"></text>
</g>
<g>
<title>main (34,408,266,749 samples, 100.00%)</title><rect x="10.0" y="541" width="1180.0" height="23.0" fill="rgb(243,179,42)" rx="2" ry="2"></rect>
<text x="13.00" y="555.5">main</text>
</g>
<g>
<title>__x86_return_thunk (16,610,051 samples, 0.05%)</title><rect x="166.7" y="301" width="0.5" height="23.0" fill="rgb(253,221,53)" rx="2" ry="2"></rect>
<text x="169.67" y="315.5"></text>
</g>
<g>
<title>syscall_return_via_sysret (28,605,764 samples, 0.08%)</title><rect x="1189.0" y="469" width="1.0" height="23.0" fill="rgb(229,113,27)" rx="2" ry="2"></rect>
<text x="1191.97" y="483.5"></text>
</g>
<g>
<title>_raw_spin_lock_irq (1,785,614,251 samples, 5.19%)</title><rect x="830.8" y="349" width="61.2" height="23.0" fill="rgb(231,122,29)" rx="2" ry="2"></rect>
<text x="833.76" y="363.5">_raw_s..</text>
</g>
<g>
<title>__cond_resched (43,770,418 samples, 0.13%)</title><rect x="110.7" y="325" width="1.5" height="23.0" fill="rgb(217,58,14)" rx="2" ry="2"></rect>
<text x="113.69" y="339.5"></text>
</g>
</g>
</svg>



<p>Note that <code>__GI___libc_write</code> is the <code>glibc</code> wrapper that performs the system call. It and everything below is in user land. Everything above is in the kernel.</p>



<p>As expected, we are spending virtually all our time calling <code>write</code>. In particular, we are spending 95% of our time inside <code>pipe_write</code>. Inside this function, we are spending 36% of our total time in <code>__alloc_pages</code>, which provisions new memory pages for the pipe. We cannot just reuse a handful of pages in a loop because <code>pv</code> moves these pages using <code>splice</code> to <code>/dev/null</code>, which consume them.</p>



<p>Next to it are <code>__mutex_lock.constprop.0</code> that takes 25% of the time and <code>_raw_spin_lock_irq</code> that takes 5%. They lock the pipe for writing.</p>



<p>This leaves just 20% of the time for the copying of data itself in <code>copy_user_enhanced_fast_string</code>. But, even with only 20% of the CPU time, we would expect to be able to move 167 GB/s * 20% = 33 GB/s. It means that, even taken separately, this function is still twice as slow as <code>__memset_avx512_unaligned_erms</code>, which was used in the program that just wrote to user space memory.</p>



<p>What is <code>copy_user_enhanced_fast_string</code> doing to be so slow? We need to dig deeper. For this, <a href="https://blog.packagecloud.io/how-to-extract-and-disassmble-a-linux-kernel-image-vmlinuz/">I disassembled my Linux kernel</a><sup data-fn="ce4dcb58-ec87-4707-ba7b-fd4c521c97da"><a href="#ce4dcb58-ec87-4707-ba7b-fd4c521c97da" id="ce4dcb58-ec87-4707-ba7b-fd4c521c97da-link">4</a></sup>, and looked at that function.</p>



<pre><code>$ grep -w copy_user_enhanced_fast_string /usr/lib/debug/boot/System.map-6.1.0-18-amd64 
ffffffff819d3d90 T copy_user_enhanced_fast_string
$ objdump -d --start-address=0xffffffff819d3d90 vmlinuz | less   
    
vmlinuz:     file format elf64-x86-64


Disassembly of section .text:

ffffffff819d3d90 &lt;.text+0x9d3d90&gt;:

ffffffff819d3d90:       90                      nop
ffffffff819d3d91:       90                      nop
ffffffff819d3d92:       90                      nop
ffffffff819d3d93:       83 fa 40                cmp    $0x40,%edx
ffffffff819d3d96:       72 48                   jb     0xffffffff819d3de0
ffffffff819d3d98:       89 d1                   mov    %edx,%ecx
ffffffff819d3d9a:       f3 a4                   rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff819d3d9c:       31 c0                   xor    %eax,%eax
ffffffff819d3d9e:       90                      nop
ffffffff819d3d9f:       90                      nop
ffffffff819d3da0:       90                      nop
ffffffff819d3da1:       e9 9a dd 42 00          jmp    0xffffffff81e01b40
...
ffffffff81e01b40:       c3                      ret
</code></pre>



<p>The <code>NOP</code> instructions at the beginning and at the end of the function allow <code>ftrace</code> to insert tracing instructions when needed. This lets it collect data about specific kernel function calls without inducing any slow down for kernel functions that are not being profiled. The CPU instruction decoding pipeline takes care of <code>NOP</code> early, so they have basically no impact on performance (other than taking room in the L1i cache).</p>



<p>I do not know why the <code>JMP</code> is not just a <code>RET</code>, however.</p>



<p>In any case, the <code>CMP</code> test and <code>JB</code> jump handle the case of buffers that are smaller than 64 bytes by jumping to another function that copy 8 bytes at a time with 64-bit registers, then 1 byte at a time with 8 bit register in two loops. For large buffers, the copying is handled by a <code>REP MOV</code> instruction. That’s definitely not vectorized code.</p>



<p>In fact, <a href="https://github.com/torvalds/linux/blob/830b3c68c1fb1e9176028d02ef86f3cf76aa2476/arch/x86/lib/copy_user_64.S#L161">this function is not implemented in C but directly in Assembly</a>! This means that there is no need to look at the result of compilation; we can just look at the source code. And it’s not just a missed optimization when compiling, it was written like that.</p>



<p>But is the lack of vector instruction the only reason why <code>copy_user_enhanced_fast_string</code> is twice as slow as <code>__memset_avx512_unaligned_erms</code>? To check this, I adapted the initial Rust program to explicitly use <code>REP MOVS</code>:</p>



<pre><code lang="rust">use std::arch::asm;

fn main() {
    let src = [0u8; 1 &lt;&lt; 15];
    let mut dst = [0u8; 1 &lt;&lt; 15];
    let mut copied = 0;
    while copied &lt; (1000u64 &lt;&lt; 30) {
        unsafe {
            asm!(
                &#34;rep movsb&#34;,
                inout(&#34;rsi&#34;) src.as_ptr() =&gt; _,
                inout(&#34;rdi&#34;) dst.as_mut_ptr() =&gt; _,
                inout(&#34;ecx&#34;) 1 &lt;&lt; 15 =&gt; _,
            );
        }
        copied += 1 &lt;&lt; 15;
    }
}</code></pre>



<p>The throughput is 80 GB/s. This is a factor 2 slow down, exactly what we observe with the kernel function!</p>



<p>Now, we know that the Linux kernel is not using SIMD to copy memory and that this makes <code>copy_user_enhanced_fast_string</code> twice as slow as it could be.</p>



<p>But why is that? Over at Stack Overflow, <a href="https://stackoverflow.com/questions/59525762/why-doesnt-copy-user-enhanced-fast-string-use-avx-if-it-is-available">Peter Cordes explains that using SSE/AVX instructions is not worth it in most cases</a>, because of the cost of saving and restoring the SIMD context.</p>



<p>In summary: the kernel is spending quite a bit of time on managing memory, and it is not even using SIMD when actually copying the bytes. This is the source of the 10× slow-down we see when comparing with the ideal case.</p>



<h2><code>vmsplice</code> to the Rescue</h2>



<p>We now have an upper bound (167 GB/s to write the data in memory once) and a lower bound (17 GB/s when using <code>write</code> on a pipe). Let’s look in details at the effect of usng <code>vmsplice</code>. It mitigates the cost of using a pipe by moving entire buffers from user space to the kernel without copying them.</p>



<p>To understand how it works, again, read <a href="https://mazzo.li/posts/fast-pipes.html">the excellent article by Francesco</a>. We’ll be using the <a href="https://github.com/bitonic/pipes-speed-test"><code>./write</code> program</a> from that article to get a minimal example of using <code>vmsplice</code>. This program just writes an infinite number of <code>&#39;X&#39;</code>s. This will simplify the profiling by not having any time dedicated to compute Fizz Buzz data or something else.</p>



<p><code>./write</code> actually achieves 210 GB/s, well above our upper bound, but that’s because the program is kind of cheating by reusing the same buffers to pass to <code>vmsplice</code>. For anything other than a stream of constant bytes, we will actually have to fill the buffers with new data, which is where the upper bound actually applies. In any case, we only care about what <code>vmsplice</code> does:</p>



<svg version="1.1" onload="init(evt)" viewBox="0 0 1200 430" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<!-- Flame graph stack visualization. See https://github.com/brendangregg/FlameGraph for latest version, and http://www.brendangregg.com/flamegraphs.html for examples. -->
<!-- NOTES:  -->
<defs>
	<linearGradient id="background" y1="0" y2="1" x1="0" x2="0">
		<stop stop-color="#eeeeee" offset="5%"></stop>
		<stop stop-color="#eeeeb0" offset="95%"></stop>
	</linearGradient>
</defs>


<rect x="0.0" y="0" width="1200.0" height="430.0" fill="url(#background)"></rect>
<text id="title" x="600.00" y="24">./write –write_with_vmsplice –huge_page –busy_loop | ./read –read_with_splice –busy_loop</text>
<text id="subtitle" x="600.00" y="48">Profiling of ./write</text>
<text id="details" x="10.00" y="413"> </text>
<text id="unzoom" x="10.00" y="24">Reset Zoom</text>
<text id="search" x="1090.00" y="24">Search</text>
<text id="ignorecase" x="1174.00" y="24">ic</text>
<text id="matched" x="1090.00" y="413"> </text>
<g id="frames">
<g>
<title>__iov_iter_get_pages_alloc (3,949,231,497 samples, 14.88%)</title><rect x="963.5" y="205" width="175.6" height="23.0" fill="rgb(250,210,50)" rx="2" ry="2"></rect>
<text x="966.49" y="219.5">__iov_iter_get_pages_a..</text>
</g>
<g>
<title>__fget_light (62,561,929 samples, 0.24%)</title><rect x="243.3" y="229" width="2.8" height="23.0" fill="rgb(233,132,31)" rx="2" ry="2"></rect>
<text x="246.34" y="243.5"></text>
</g>
<g>
<title>internal_get_user_pages_fast (3,133,200,714 samples, 11.81%)</title><rect x="975.8" y="181" width="139.3" height="23.0" fill="rgb(253,223,53)" rx="2" ry="2"></rect>
<text x="978.83" y="195.5">internal_get_user..</text>
</g>
<g>
<title>write (26,541,239,090 samples, 100.00%)</title><rect x="10.0" y="349" width="1180.0" height="23.0" fill="rgb(228,107,25)" rx="2" ry="2"></rect>
<text x="13.00" y="363.5">write</text>
</g>
<g>
<title>__check_object_size (130,746,843 samples, 0.49%)</title><rect x="911.9" y="157" width="5.8" height="23.0" fill="rgb(226,98,23)" rx="2" ry="2"></rect>
<text x="914.91" y="171.5"></text>
</g>
<g>
<title>osq_lock (197,341,424 samples, 0.74%)</title><rect x="634.9" y="205" width="8.8" height="23.0" fill="rgb(214,43,10)" rx="2" ry="2"></rect>
<text x="637.91" y="219.5"></text>
</g>
<g>
<title>import_iovec (1,716,063,086 samples, 6.47%)</title><rect x="886.6" y="229" width="76.3" height="23.0" fill="rgb(241,168,40)" rx="2" ry="2"></rect>
<text x="889.58" y="243.5">import_i..</text>
</g>
<g>
<title>__import_iovec (1,666,122,831 samples, 6.28%)</title><rect x="888.8" y="205" width="74.1" height="23.0" fill="rgb(214,42,10)" rx="2" ry="2"></rect>
<text x="891.80" y="219.5">__import..</text>
</g>
<g>
<title>do_mmap (2,892,862 samples, 0.01%)</title><rect x="10.0" y="229" width="0.1" height="23.0" fill="rgb(228,107,25)" rx="2" ry="2"></rect>
<text x="13.00" y="243.5"></text>
</g>
<g>
<title>iov_iter_advance (538,480,611 samples, 2.03%)</title><rect x="1115.1" y="181" width="24.0" height="23.0" fill="rgb(248,197,47)" rx="2" ry="2"></rect>
<text x="1118.13" y="195.5">i..</text>
</g>
<g>
<title>pipe_lock (150,603,536 samples, 0.57%)</title><rect x="1147.6" y="229" width="6.7" height="23.0" fill="rgb(226,99,23)" rx="2" ry="2"></rect>
<text x="1150.62" y="243.5"></text>
</g>
<g>
<title>syscall_enter_from_user_mode (4,006,940 samples, 0.02%)</title><rect x="1187.3" y="253" width="0.2" height="23.0" fill="rgb(254,229,54)" rx="2" ry="2"></rect>
<text x="1190.34" y="267.5"></text>
</g>
<g>
<title>kill_fasync (2,756,081 samples, 0.01%)</title><rect x="1139.1" y="229" width="0.1" height="23.0" fill="rgb(224,91,21)" rx="2" ry="2"></rect>
<text x="1142.07" y="243.5"></text>
</g>
<g>
<title>do_syscall_64 (2,892,862 samples, 0.01%)</title><rect x="10.0" y="277" width="0.1" height="23.0" fill="rgb(209,20,4)" rx="2" ry="2"></rect>
<text x="13.00" y="291.5"></text>
</g>
<g>
<title>vm_mmap_pgoff (2,892,862 samples, 0.01%)</title><rect x="10.0" y="253" width="0.1" height="23.0" fill="rgb(237,150,35)" rx="2" ry="2"></rect>
<text x="13.00" y="267.5"></text>
</g>
<g>
<title>do_syscall_64 (26,020,596,200 samples, 98.04%)</title><rect x="32.6" y="277" width="1156.8" height="23.0" fill="rgb(209,20,4)" rx="2" ry="2"></rect>
<text x="35.59" y="291.5">do_syscall_64</text>
</g>
<g>
<title>arch_get_unmapped_area_topdown (2,892,862 samples, 0.01%)</title><rect x="10.0" y="181" width="0.1" height="23.0" fill="rgb(250,208,49)" rx="2" ry="2"></rect>
<text x="13.00" y="195.5"></text>
</g>
<g>
<title>mutex_lock (30,726,118 samples, 0.12%)</title><rect x="1139.2" y="229" width="1.4" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="1142.19" y="243.5"></text>
</g>
<g>
<title>entry_SYSRETQ_unsafe_stack (8,363,819 samples, 0.03%)</title><rect x="1189.4" y="301" width="0.4" height="23.0" fill="rgb(206,7,1)" rx="2" ry="2"></rect>
<text x="1192.44" y="315.5"></text>
</g>
<g>
<title>mutex_spin_on_owner (5,495,045,988 samples, 20.70%)</title><rect x="390.6" y="205" width="244.3" height="23.0" fill="rgb(217,57,13)" rx="2" ry="2"></rect>
<text x="393.60" y="219.5">mutex_spin_on_owner</text>
</g>
<g>
<title>update_process_times (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="61" width="0.1" height="23.0" fill="rgb(250,209,50)" rx="2" ry="2"></rect>
<text x="1190.40" y="75.5"></text>
</g>
<g>
<title>__cond_resched (5,668,335 samples, 0.02%)</title><rect x="390.4" y="205" width="0.2" height="23.0" fill="rgb(217,58,14)" rx="2" ry="2"></rect>
<text x="393.35" y="219.5"></text>
</g>
<g>
<title>__fdget (18,709,007 samples, 0.07%)</title><rect x="242.5" y="229" width="0.8" height="23.0" fill="rgb(214,41,10)" rx="2" ry="2"></rect>
<text x="245.51" y="243.5"></text>
</g>
<g>
<title>osq_unlock (817,653,660 samples, 3.08%)</title><rect x="643.7" y="205" width="36.3" height="23.0" fill="rgb(248,198,47)" rx="2" ry="2"></rect>
<text x="646.68" y="219.5">osq..</text>
</g>
<g>
<title>syscall_exit_to_user_mode (43,203,238 samples, 0.16%)</title><rect x="1187.5" y="253" width="1.9" height="23.0" fill="rgb(251,211,50)" rx="2" ry="2"></rect>
<text x="1190.52" y="267.5"></text>
</g>
<g>
<title>__sysvec_apic_timer_interrupt (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="181" width="0.1" height="23.0" fill="rgb(242,173,41)" rx="2" ry="2"></rect>
<text x="1190.40" y="195.5"></text>
</g>
<g>
<title>_copy_from_user (1,015,660,918 samples, 3.83%)</title><rect x="917.7" y="157" width="45.2" height="23.0" fill="rgb(206,7,1)" rx="2" ry="2"></rect>
<text x="920.72" y="171.5">_cop..</text>
</g>
<g>
<title>get_pipe_info (88,389,900 samples, 0.33%)</title><rect x="882.7" y="229" width="3.9" height="23.0" fill="rgb(226,98,23)" rx="2" ry="2"></rect>
<text x="885.65" y="243.5"></text>
</g>
<g>
<title>tick_sched_handle (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="85" width="0.1" height="23.0" fill="rgb(219,68,16)" rx="2" ry="2"></rect>
<text x="1190.40" y="99.5"></text>
</g>
<g>
<title>asm_sysvec_apic_timer_interrupt (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="229" width="0.1" height="23.0" fill="rgb(232,127,30)" rx="2" ry="2"></rect>
<text x="1190.40" y="243.5"></text>
</g>
<g>
<title>vm_unmapped_area (2,892,862 samples, 0.01%)</title><rect x="10.0" y="157" width="0.1" height="23.0" fill="rgb(235,138,33)" rx="2" ry="2"></rect>
<text x="13.00" y="171.5"></text>
</g>
<g>
<title>try_grab_folio (289,286,952 samples, 1.09%)</title><rect x="1102.3" y="157" width="12.8" height="23.0" fill="rgb(238,153,36)" rx="2" ry="2"></rect>
<text x="1105.27" y="171.5"></text>
</g>
<g>
<title>get_user_pages_fast (7,086,063 samples, 0.03%)</title><rect x="975.5" y="181" width="0.3" height="23.0" fill="rgb(229,111,26)" rx="2" ry="2"></rect>
<text x="978.52" y="195.5"></text>
</g>
<g>
<title>page_cache_pipe_buf_release (43,300,058 samples, 0.16%)</title><rect x="880.7" y="205" width="2.0" height="23.0" fill="rgb(224,89,21)" rx="2" ry="2"></rect>
<text x="883.73" y="219.5"></text>
</g>
<g>
<title>__mutex_lock.constprop.0 (9,759,850,719 samples, 36.77%)</title><rect x="246.1" y="229" width="433.9" height="23.0" fill="rgb(225,95,22)" rx="2" ry="2"></rect>
<text x="249.12" y="243.5">__mutex_lock.constprop.0</text>
</g>
<g>
<title>with_vmsplice (4,241,758 samples, 0.02%)</title><rect x="1189.8" y="325" width="0.2" height="23.0" fill="rgb(230,119,28)" rx="2" ry="2"></rect>
<text x="1192.81" y="339.5"></text>
</g>
<g>
<title>exit_to_user_mode_prepare (30,683,997 samples, 0.12%)</title><rect x="1188.0" y="229" width="1.3" height="23.0" fill="rgb(228,108,25)" rx="2" ry="2"></rect>
<text x="1190.95" y="243.5"></text>
</g>
<g>
<title>pud_huge (4,099,890 samples, 0.02%)</title><rect x="1102.1" y="157" width="0.2" height="23.0" fill="rgb(229,110,26)" rx="2" ry="2"></rect>
<text x="1105.09" y="171.5"></text>
</g>
<g>
<title>mas_empty_area_rev (2,892,862 samples, 0.01%)</title><rect x="10.0" y="133" width="0.1" height="23.0" fill="rgb(244,181,43)" rx="2" ry="2"></rect>
<text x="13.00" y="147.5"></text>
</g>
<g>
<title>copy_user_enhanced_fast_string (76,785,562 samples, 0.29%)</title><rect x="959.5" y="133" width="3.4" height="23.0" fill="rgb(238,155,37)" rx="2" ry="2"></rect>
<text x="962.46" y="147.5"></text>
</g>
<g>
<title>entry_SYSCALL_64_after_hwframe (26,054,316,296 samples, 98.17%)</title><rect x="31.1" y="301" width="1158.3" height="23.0" fill="rgb(218,63,15)" rx="2" ry="2"></rect>
<text x="34.09" y="315.5">entry_SYSCALL_64_after_hwframe</text>
</g>
<g>
<title>__do_sys_vmsplice (25,963,594,680 samples, 97.82%)</title><rect x="33.0" y="253" width="1154.3" height="23.0" fill="rgb(227,104,24)" rx="2" ry="2"></rect>
<text x="36.02" y="267.5">__do_sys_vmsplice</text>
</g>
<g>
<title>wait_for_space (742,762,247 samples, 2.80%)</title><rect x="1154.3" y="229" width="33.0" height="23.0" fill="rgb(215,49,11)" rx="2" ry="2"></rect>
<text x="1157.32" y="243.5">wa..</text>
</g>
<g>
<title>__hrtimer_run_queues (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="133" width="0.1" height="23.0" fill="rgb(237,150,35)" rx="2" ry="2"></rect>
<text x="1190.40" y="147.5"></text>
</g>
<g>
<title>check_stack_object (58,579,278 samples, 0.22%)</title><rect x="915.1" y="133" width="2.6" height="23.0" fill="rgb(215,49,11)" rx="2" ry="2"></rect>
<text x="918.12" y="147.5"></text>
</g>
<g>
<title>get_unmapped_area (2,892,862 samples, 0.01%)</title><rect x="10.0" y="205" width="0.1" height="23.0" fill="rgb(234,134,32)" rx="2" ry="2"></rect>
<text x="13.00" y="219.5"></text>
</g>
<g>
<title>entry_SYSCALL_64 (259,954,397 samples, 0.98%)</title><rect x="19.5" y="301" width="11.6" height="23.0" fill="rgb(239,156,37)" rx="2" ry="2"></rect>
<text x="22.53" y="315.5"></text>
</g>
<g>
<title>syscall_exit_to_user_mode_prepare (2,750,376 samples, 0.01%)</title><rect x="1189.3" y="229" width="0.1" height="23.0" fill="rgb(235,142,34)" rx="2" ry="2"></rect>
<text x="1192.32" y="243.5"></text>
</g>
<g>
<title>entry_SYSCALL_64_after_hwframe (2,892,862 samples, 0.01%)</title><rect x="10.0" y="301" width="0.1" height="23.0" fill="rgb(218,63,15)" rx="2" ry="2"></rect>
<text x="13.00" y="315.5"></text>
</g>
<g>
<title>add_to_pipe (4,557,388,711 samples, 17.17%)</title><rect x="680.0" y="229" width="202.7" height="23.0" fill="rgb(223,84,20)" rx="2" ry="2"></rect>
<text x="683.04" y="243.5">add_to_pipe</text>
</g>
<g>
<title>__mmap (2,892,862 samples, 0.01%)</title><rect x="10.0" y="325" width="0.1" height="23.0" fill="rgb(227,104,25)" rx="2" ry="2"></rect>
<text x="13.00" y="339.5"></text>
</g>
<g>
<title>iovec_from_user.part.0 (1,403,501,367 samples, 5.29%)</title><rect x="900.5" y="181" width="62.4" height="23.0" fill="rgb(242,171,40)" rx="2" ry="2"></rect>
<text x="903.48" y="195.5">iovec_..</text>
</g>
<g>
<title>hrtimer_interrupt (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="157" width="0.1" height="23.0" fill="rgb(228,109,26)" rx="2" ry="2"></rect>
<text x="1190.40" y="171.5"></text>
</g>
<g>
<title>all (26,541,242,666 samples, 100%)</title><rect x="10.0" y="373" width="1180.0" height="23.0" fill="rgb(213,39,9)" rx="2" ry="2"></rect>
<text x="13.00" y="387.5"></text>
</g>
<g>
<title>tick_sched_timer (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="109" width="0.1" height="23.0" fill="rgb(254,227,54)" rx="2" ry="2"></rect>
<text x="1190.40" y="123.5"></text>
</g>
<g>
<title>sysvec_apic_timer_interrupt (2,662,881 samples, 0.01%)</title><rect x="1187.4" y="205" width="0.1" height="23.0" fill="rgb(220,69,16)" rx="2" ry="2"></rect>
<text x="1190.40" y="219.5"></text>
</g>
<g>
<title>iov_iter_get_pages2 (3,963,033,797 samples, 14.93%)</title><rect x="962.9" y="229" width="176.2" height="23.0" fill="rgb(247,194,46)" rx="2" ry="2"></rect>
<text x="965.88" y="243.5">iov_iter_get_pages2</text>
</g>
<g>
<title>mutex_unlock (158,874,728 samples, 0.60%)</title><rect x="1140.6" y="229" width="7.0" height="23.0" fill="rgb(251,212,50)" rx="2" ry="2"></rect>
<text x="1143.56" y="243.5"></text>
</g>
<g>
<title>vmsplice (26,534,006,381 samples, 99.97%)</title><rect x="10.1" y="325" width="1179.7" height="23.0" fill="rgb(212,35,8)" rx="2" ry="2"></rect>
<text x="13.13" y="339.5">vmsplice</text>
</g>
</g>
</svg>



<p>Like with <code>write</code>, we are spending a significant amount of time (37%) in <code>__mutex_lock.constprop.0</code>. However, there is no <code>_alloc_pages</code> and no <code>_raw_spin_lock_irq</code>. And, instead of <code>copy_user_enhanced_fast_string</code>, we find <code>add_to_pipe</code>, <code>import_iovec</code> and <code>iov_iter_get_pages2</code>. From this, we can see that how <code>vmsplice</code> bypasses the expensive parts of the <code>write</code> system call.</p>



<p>As an aside, I was a bit surprised about the effect of the buffer size, especially when not using <code>vmsplice</code>. It looks like minimizing the number of system calls is not always the most important thing to do.</p>



<figure><table><thead><tr><th>What</th><th data-align="right">Buffer size</th><th data-align="right">Throughput (GB/s)</th><th data-align="right">System calls</th><th data-align="right">Instructions</th><th data-align="right">ins/syscall</th></tr></thead><tbody><tr><td>./write</td><td data-align="right">32768</td><td data-align="right">99</td><td data-align="right">3276822</td><td data-align="right">7373684904</td><td data-align="right">2250</td></tr><tr><td>./write</td><td data-align="right">65536</td><td data-align="right">150</td><td data-align="right">1638466</td><td data-align="right">5438514152</td><td data-align="right">3319</td></tr><tr><td>./write</td><td data-align="right">131072</td><td data-align="right">207</td><td data-align="right">819270</td><td data-align="right">4288897413</td><td data-align="right">5235</td></tr><tr><td>zeroes</td><td data-align="right">32768</td><td data-align="right">17</td><td data-align="right">3276800</td><td data-align="right">31859864089</td><td data-align="right">9723</td></tr><tr><td>zeroes</td><td data-align="right">65536</td><td data-align="right">13</td><td data-align="right">1638400</td><td data-align="right">31750857264</td><td data-align="right">19379</td></tr><tr><td>zeroes</td><td data-align="right">131072</td><td data-align="right">12</td><td data-align="right">819200</td><td data-align="right">35002733773</td><td data-align="right">42728</td></tr></tbody></table></figure>







<p>There you have it. Writing to a pipe is ten times slower than writing to raw memory. And this is because, when writing to a pipe, we need to spend a lot of time taking a lock, and we cannot use vector instructions efficiently.</p>



<p>In principle, we could move data at 167 GB/s, but we need to avoid the cost of locking the buffer, and the cost of saving and restoring the SIMD context. This is exactly what <code>splice</code> and <code>vmsplice</code> do. They are often described as avoiding copying data between buffers, and this is true, but, most importantly, they completely bypass the conservative kernel code with extensive procedures and scalar code.</p>


<ol><li id="25bc32bc-4b79-41b9-87b3-0db6adcecbbf">Of course, they need to write code fast enough for exploit what <code>vmsplice</code> enables, but the point is that the first group’s performance is limited by not using <code>vmsplice</code>. <a href="#25bc32bc-4b79-41b9-87b3-0db6adcecbbf-link" aria-label="Jump to footnote reference 1">↩︎</a></li><li id="9f242c74-a611-49a2-9265-07d5945c4750">All benchmarks were performed on my personal desktop computer, which features a 7950X3D and DDR5 RAM overclocked to 6000T/s. And I am running Debian 12 with a 6.1.0-18-amd64 Linux kernel. CPU mitigations were disabled using the Linux kernel option <code>mitigations=off</code>.</li><li id="e22c8070-2784-40d4-81ae-6916566ce03a">See “L1 Cache write” in the last row of the second table from the bottom of the <a href="https://lanoc.org/review/cpus/8673-amd-ryzen-9-7950x3d?start=2">LanOC review</a>. This gives 2,518.4 GB/s for all 16 physical cores, or 157.4 GB/s per physical core.  <a href="#e22c8070-2784-40d4-81ae-6916566ce03a-link" aria-label="Jump to footnote reference 3">↩︎</a></li><li id="ce4dcb58-ec87-4707-ba7b-fd4c521c97da">I had to install <code>linux-image-6.1.0-18-amd64-dbg</code> to get the file <code>/usr/lib/debug/boot/System.map-6.1.0-18-amd64</code> with the symbols. <a href="#ce4dcb58-ec87-4707-ba7b-fd4c521c97da-link" aria-label="Jump to footnote reference 4">↩︎</a></li></ol>	</div></div>
  </body>
</html>
