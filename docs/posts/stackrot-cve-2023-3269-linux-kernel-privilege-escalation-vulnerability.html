<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lrh2000/StackRot">Original</a>
    <h1>StackRot (CVE-2023-3269): Linux kernel privilege escalation vulnerability</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>This serves as an &#34;early&#34; disclosure of the StackRot vulnerability, in
compliance with <a href="https://oss-security.openwall.org/wiki/mailing-lists/distros" rel="nofollow">the policy of the linux-distros list</a>. While all the
essential vulnerability details have been provided here, the complete exploit
code and a comprehensive write-up will be made publicly available no later than
the end of July. <a href="https://www.openwall.com/lists/oss-security/2023/07/05/1" rel="nofollow">The oss-security thread</a> will be notified, and any
updates will be reflected in this GitHub repository.</strong></p>
<p dir="auto">A flaw was found in the handling of stack expansion in the Linux kernel 6.1
through 6.4, aka &#34;Stack Rot&#34;. The maple tree, responsible for managing virtual
memory areas, can undergo node replacement without properly acquiring the MM
write lock, leading to use-after-free issues. An unprivileged local user could
use this flaw to compromise the kernel and escalate their privileges.</p>
<p dir="auto">As StackRot is a Linux kernel vulnerability found in the memory management
subsystem, it affects almost all kernel configurations and requires minimal
capabilities to trigger. However, it should be noted that maple nodes are freed
using RCU callbacks, delaying the actual memory deallocation until after the
RCU grace period. Consequently, exploiting this vulnerability is considered
challenging.</p>
<p dir="auto">To the best of my knowledge, there are currently no publicly available exploits
targeting use-after-free-by-RCU (UAFBR) bugs. This marks the first instance
where UAFBR bugs have been proven to be exploitable, even without the presence
of CONFIG_PREEMPT or CONFIG_SLAB_MERGE_DEFAULT settings. Notably, this exploit
has been successfully demonstrated in the environment provided by <a href="https://google.github.io/kctf/vrp.html" rel="nofollow">Google kCTF
VRP</a> (<a href="https://storage.googleapis.com/kctf-vrp-public-files/bzImage_upstream_6.1.25" rel="nofollow">bzImage_upstream_6.1.25</a>, <a href="https://storage.googleapis.com/kctf-vrp-public-files/bzImage_upstream_6.1.25_config" rel="nofollow">config</a>).</p>
<p dir="auto">The StackRot vulnerability has been present in the Linux kernel since version
6.1 when the VMA tree structure was <a href="https://lore.kernel.org/lkml/20220906194824.2110408-1-Liam.Howlett@oracle.com/" rel="nofollow">changed</a> from red-black trees to maple
trees.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-background" aria-hidden="true" href="#background"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Background</h2>
<p dir="auto">Whenever the <code>mmap()</code> system call is utilized to establish a memory mapping,
the kernel generates a structure called <code>vm_area_struct</code> to represent the
corresponding virtual memory area (VMA). This structure stores various
information including flags, properties, and other pertinent details related to
the mapping.</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct vm_area_struct {
        long unsigned int          vm_start;             /*     0     8 */
        long unsigned int          vm_end;               /*     8     8 */
        struct mm_struct *         vm_mm;                /*    16     8 */
        pgprot_t                   vm_page_prot;         /*    24     8 */
        long unsigned int          vm_flags;             /*    32     8 */
        union {
                struct {
                        struct rb_node rb __attribute__((__aligned__(8))); /*    40    24 */
                        /* --- cacheline 1 boundary (64 bytes) --- */
                        long unsigned int rb_subtree_last; /*    64     8 */
                } __attribute__((__aligned__(8))) shared __attribute__((__aligned__(8))); /*    40    32 */
                struct anon_vma_name * anon_name;        /*    40     8 */
        } __attribute__((__aligned__(8)));               /*    40    32 */
        /* --- cacheline 1 boundary (64 bytes) was 8 bytes ago --- */
        struct list_head           anon_vma_chain;       /*    72    16 */
        struct anon_vma *          anon_vma;             /*    88     8 */
        const struct vm_operations_struct  * vm_ops;     /*    96     8 */
        long unsigned int          vm_pgoff;             /*   104     8 */
        struct file *              vm_file;              /*   112     8 */
        void *                     vm_private_data;      /*   120     8 */
        /* --- cacheline 2 boundary (128 bytes) --- */
        atomic_long_t              swap_readahead_info;  /*   128     8 */
        struct vm_userfaultfd_ctx  vm_userfaultfd_ctx;   /*   136     0 */

        /* size: 136, cachelines: 3, members: 14 */
        /* forced alignments: 1 */
        /* last cacheline: 8 bytes */
} __attribute__((__aligned__(8)));"><pre><span>struct</span> <span>vm_area_struct</span> {
        <span>long unsigned <span>int</span></span>          <span>vm_start</span>;             <span>/*     0     8 */</span>
        <span>long unsigned <span>int</span></span>          <span>vm_end</span>;               <span>/*     8     8 */</span>
        <span>struct</span> <span>mm_struct</span> <span>*</span>         <span>vm_mm</span>;                <span>/*    16     8 */</span>
        <span>pgprot_t</span>                   <span>vm_page_prot</span>;         <span>/*    24     8 */</span>
        <span>long unsigned <span>int</span></span>          <span>vm_flags</span>;             <span>/*    32     8 */</span>
        <span>union</span> {
                <span>struct</span> {
                        <span>struct</span> <span>rb_node</span> <span>rb</span> __attribute__((<span>__aligned__</span>(<span>8</span>)))<span></span>; <span>/*    40    24 */</span>
                        <span>/* --- cacheline 1 boundary (64 bytes) --- */</span>
                        <span>long unsigned <span>int</span></span> <span>rb_subtree_last</span>; <span>/*    64     8 */</span>
                } __attribute__((<span>__aligned__</span>(<span>8</span>))) <span>shared</span> __attribute__((<span>__aligned__</span>(<span>8</span>))); <span>/*    40    32 */</span>
                <span>struct</span> <span>anon_vma_name</span> <span>*</span> <span>anon_name</span>;        <span>/*    40     8 */</span>
        } __attribute__((<span>__aligned__</span>(<span>8</span>)));               <span>/*    40    32 */</span>
        <span>/* --- cacheline 1 boundary (64 bytes) was 8 bytes ago --- */</span>
        <span>struct</span> <span>list_head</span>           <span>anon_vma_chain</span>;       <span>/*    72    16 */</span>
        <span>struct</span> <span>anon_vma</span> <span>*</span>          <span>anon_vma</span>;             <span>/*    88     8 */</span>
        <span>const</span> <span>struct</span> <span>vm_operations_struct</span>  <span>*</span> <span>vm_ops</span>;     <span>/*    96     8 */</span>
        <span>long unsigned <span>int</span></span>          <span>vm_pgoff</span>;             <span>/*   104     8 */</span>
        <span>struct</span> <span>file</span> <span>*</span>              <span>vm_file</span>;              <span>/*   112     8 */</span>
        <span>void</span> <span>*</span>                     <span>vm_private_data</span>;      <span>/*   120     8 */</span>
        <span>/* --- cacheline 2 boundary (128 bytes) --- */</span>
        <span>atomic_long_t</span>              <span>swap_readahead_info</span>;  <span>/*   128     8 */</span>
        <span>struct</span> <span>vm_userfaultfd_ctx</span>  <span>vm_userfaultfd_ctx</span>;   <span>/*   136     0 */</span>

        <span>/* size: 136, cachelines: 3, members: 14 */</span>
        <span>/* forced alignments: 1 */</span>
        <span>/* last cacheline: 8 bytes */</span>
} __attribute__((<span>__aligned__</span>(<span>8</span>)))<span></span>;</pre></div>
<p dir="auto">Subsequently, when the kernel encounters page faults or other memory-related
system calls, it requires fast lookup of the VMA solely based on the address.
Previously, the VMAs were managed using red-black trees. However, starting from
Linux kernel version 6.1, the migration to maple trees took place. <a href="https://docs.kernel.org/6.4/core-api/maple_tree.html" rel="nofollow">Maple
trees</a> are RCU-safe B-tree data structures optimized for storing
non-overlapping ranges. Nonetheless, their intricate nature adds complexity to
the codebase and introduces the StackRot vulnerability.</p>
<p dir="auto">In essence, the maple tree is composed of maple nodes. Throughout this article,
it is assumed that the maple tree has only a single root node, which can
contain a maximum of 16 intervals. Each interval can either represent a gap or
point to a VMA. Thus, no gaps are allowed between two intervals.</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct maple_range_64 {
        struct maple_pnode *       parent;               /*     0     8 */
        long unsigned int          pivot[15];            /*     8   120 */
        /* --- cacheline 2 boundary (128 bytes) --- */
        union {
                void *             slot[16];             /*   128   128 */
                struct {
                        void *     pad[15];              /*   128   120 */
                        /* --- cacheline 3 boundary (192 bytes) was 56 bytes ago --- */
                        struct maple_metadata meta;      /*   248     2 */
                };                                       /*   128   128 */
        };                                               /*   128   128 */

        /* size: 256, cachelines: 4, members: 3 */
};"><pre><span>struct</span> <span>maple_range_64</span> {
        <span>struct</span> <span>maple_pnode</span> <span>*</span>       <span>parent</span>;               <span>/*     0     8 */</span>
        <span>long unsigned <span>int</span></span>          <span>pivot</span>[<span>15</span>];            <span>/*     8   120 */</span>
        <span>/* --- cacheline 2 boundary (128 bytes) --- */</span>
        <span>union</span> {
                <span>void</span> <span>*</span>             <span>slot</span>[<span>16</span>];             <span>/*   128   128 */</span>
                <span>struct</span> {
                        <span>void</span> <span>*</span>     <span>pad</span>[<span>15</span>];              <span>/*   128   120 */</span>
                        <span>/* --- cacheline 3 boundary (192 bytes) was 56 bytes ago --- */</span>
                        <span>struct</span> <span>maple_metadata</span> <span>meta</span>;      <span>/*   248     2 */</span>
                };                                       <span>/*   128   128 */</span>
        };                                               <span>/*   128   128 */</span>

        <span>/* size: 256, cachelines: 4, members: 3 */</span>
};</pre></div>
<p dir="auto">The structure <code>maple_range_64</code> represents a maple node in the following manner.
The pivots indicate the endpoints of 16 intervals, while the slots are used to
reference the VMA structure when the node is considered a leaf node. The layout
of pivots and slots can be visualized as shown below:</p>
<div data-snippet-clipboard-copy-content=" Slots -&gt; | 0 | 1 | 2 | ... | 12 | 13 | 14 | 15 |
          ┬   ┬   ┬   ┬     ┬    ┬    ┬    ┬    ┬
          │   │   │   │     │    │    │    │    └─ Implied maximum
          │   │   │   │     │    │    │    └─ Pivot 14
          │   │   │   │     │    │    └─ Pivot 13
          │   │   │   │     │    └─ Pivot 12
          │   │   │   │     └─ Pivot 11
          │   │   │   └─ Pivot 2
          │   │   └─ Pivot 1
          │   └─ Pivot 0
          └─  Implied minimum"><pre><code> Slots -&gt; | 0 | 1 | 2 | ... | 12 | 13 | 14 | 15 |
          ┬   ┬   ┬   ┬     ┬    ┬    ┬    ┬    ┬
          │   │   │   │     │    │    │    │    └─ Implied maximum
          │   │   │   │     │    │    │    └─ Pivot 14
          │   │   │   │     │    │    └─ Pivot 13
          │   │   │   │     │    └─ Pivot 12
          │   │   │   │     └─ Pivot 11
          │   │   │   └─ Pivot 2
          │   │   └─ Pivot 1
          │   └─ Pivot 0
          └─  Implied minimum
</code></pre></div>
<p dir="auto">Regarding concurrent modification, the maple tree imposes restrictions,
requiring an exclusive lock to be held by writers. In the case of the VMA tree,
the exclusive lock corresponds to the MM write lock. As for readers, two
options are available. The first option involves holding the MM read lock,
which results in the writer being blocked by the MM read-write lock.
Alternatively, the second option is to enter the RCU critical section. By doing
so, the writer is not blocked, and readers can continue their operations since
the maple tree is RCU-safe. While most existing VMA accesses opt for the first
option, the second option is employed in a few performance-critical scenarios,
such as lockless page faults.</p>
<p dir="auto">However, there is an additional aspect that requires particular attention,
which pertains to stack expansion. The stack represents a memory area that is
mapped with the MAP_GROWSDOWN flag, indicating automatic expansion when an
address below the region is accessed. In such cases, the start address of the
corresponding VMA is adjusted, as well as the associated interval within the
maple tree. Notably, these adjustments are made without holding the MM write
lock.</p>
<div dir="auto" data-snippet-clipboard-copy-content="static inline
void do_user_addr_fault(struct pt_regs *regs,
                        unsigned long error_code,
                        unsigned long address)
{
	// ...

	if (unlikely(!mmap_read_trylock(mm))) {
		// ...
	}
	// ...
	if (unlikely(expand_stack(vma, address))) {
		// ...
	}

	// ...
}"><pre><span>static</span> <span>inline</span>
<span>void</span> <span>do_user_addr_fault</span>(<span>struct</span> <span>pt_regs</span> <span>*</span><span>regs</span>,
                        <span>unsigned long</span> <span>error_code</span>,
                        <span>unsigned long</span> <span>address</span>)
{
	<span>// ...</span>

	<span>if</span> (<span>unlikely</span>(!<span>mmap_read_trylock</span>(<span>mm</span>))) {
		<span>// ...</span>
	}
	<span>// ...</span>
	<span>if</span> (<span>unlikely</span>(<span>expand_stack</span>(<span>vma</span>, <span>address</span>))) {
		<span>// ...</span>
	}

	<span>// ...</span>
}</pre></div>
<p dir="auto">Typically, a gap exists between the stack VMA and its neighboring VMA, as the
kernel enforces a stack guard. In this scenario, when expanding the stack, only
the pivot value in the maple node needs updating, a process that can be
performed atomically. However, if the neighboring VMA also possesses the
MAP_GROWSDOWN flag, no stack guard is enforced. Consequently, the stack
expansion can eliminate the gap. In such situations, the gap interval within
the maple node must be removed. As the maple tree is RCU-safe, overwriting the
node in-place is not possible. Instead, a new node is created, triggering node
replacement, and the old node is subsequently destroyed using an RCU callback.</p>
<div dir="auto" data-snippet-clipboard-copy-content="int expand_downwards(struct vm_area_struct *vma, unsigned long address)
{
	// ...

	if (prev) {
		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;
		    vma_is_accessible(prev) &amp;&amp;
		    (address - prev-&gt;vm_end &lt; stack_guard_gap))
			return -ENOMEM;
	}

	// ...
}"><pre><span>int</span> <span>expand_downwards</span>(<span>struct</span> <span>vm_area_struct</span> <span>*</span><span>vma</span>, <span>unsigned long</span> <span>address</span>)
{
	<span>// ...</span>

	<span>if</span> (<span>prev</span>) {
		<span>if</span> (!(<span>prev</span><span>-&gt;</span><span>vm_flags</span> <span>&amp;</span> <span>VM_GROWSDOWN</span>) <span>&amp;&amp;</span>
		    <span>vma_is_accessible</span>(<span>prev</span>) <span>&amp;&amp;</span>
		    (<span>address</span> <span>-</span> <span>prev</span><span>-&gt;</span><span>vm_end</span> <span>&lt;</span> <span>stack_guard_gap</span>))
			<span>return</span> <span>-</span><span>ENOMEM</span>;
	}

	<span>// ...</span>
}</pre></div>
<p dir="auto">The RCU callback is invoked only after all pre-existing RCU critical sections
have concluded. However, the issue arises when accessing VMAs, as only the MM
read lock is held, and it does not enter the RCU critical section.
Consequently, in theory, the callback could be invoked at any time, resulting
in the freeing of the old maple node. However, pointers to the old node may
have already been fetched, leading to a use-after-free bug when attempting
subsequent access to it.</p>
<p dir="auto">The backtrace where use-after-free (UAF) occurs is shown below:</p>
<div data-snippet-clipboard-copy-content="  - CPU 0 -                                        - CPU 1 -

  mm_read_lock()                                    mm_read_lock()
  expand_stack()                                    find_vma_prev()
    expand_downwards()                                mas_walk()
      mas_store_prealloc()                              mas_state_walk()
        mas_wr_story_entry()                              mas_start()
          mas_wr_modify()                                   mas_root()
            mas_wr_store_node()                               node = rcu_dereference_check()
              mas_replace()                                   [ The node pointer is recorded ]
                mas_free()
                  ma_free_rcu()
                    call_rcu(&amp;mt_free_rcu)
                    [ The node is dead ]
  mm_read_unlock()

  [ Wait for the next RCU grace period.. ]
  rcu_do_batch()                                      mas_prev()
    mt_free_rcu()                                       mas_prev_entry()
      kmem_cache_free()                                   mas_prev_nentry()
      [ The node is freed ]                                 mas_slot()
                                                              mt_slot()
                                                                rcu_dereference_check(node-&gt;..)
                                                                [ UAF occurs here ]
                                                    mm_read_unlock()"><pre><code>  - CPU 0 -                                        - CPU 1 -

  mm_read_lock()                                    mm_read_lock()
  expand_stack()                                    find_vma_prev()
    expand_downwards()                                mas_walk()
      mas_store_prealloc()                              mas_state_walk()
        mas_wr_story_entry()                              mas_start()
          mas_wr_modify()                                   mas_root()
            mas_wr_store_node()                               node = rcu_dereference_check()
              mas_replace()                                   [ The node pointer is recorded ]
                mas_free()
                  ma_free_rcu()
                    call_rcu(&amp;mt_free_rcu)
                    [ The node is dead ]
  mm_read_unlock()

  [ Wait for the next RCU grace period.. ]
  rcu_do_batch()                                      mas_prev()
    mt_free_rcu()                                       mas_prev_entry()
      kmem_cache_free()                                   mas_prev_nentry()
      [ The node is freed ]                                 mas_slot()
                                                              mt_slot()
                                                                rcu_dereference_check(node-&gt;..)
                                                                [ UAF occurs here ]
                                                    mm_read_unlock()
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-fix" aria-hidden="true" href="#fix"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Fix</h2>
<p dir="auto">I reported this vulnerability to the Linux kernel security team on June 15th.
Following that, the process of addressing this bug was led by Linus Torvalds.
Given its complexity, it took nearly two weeks to develop a set of patches that
received consensus.</p>
<p dir="auto">On June 28th, during the merge window for Linux kernel 6.5, the fix was merged
into Linus&#39; tree. Linus provided a <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9471f1f2f50282b9e8f59198ec6bb738b4ccc009" rel="nofollow">comprehensive merge message</a> to
elucidate the patch series from a technical perspective.</p>
<p dir="auto">These patches were subsequently backported to stable kernels (<a href="https://lore.kernel.org/stable/2023070133-create-stainless-9a8c@gregkh/T/" rel="nofollow">6.1.37</a>,
<a href="https://lore.kernel.org/stable/2023070146-endearing-bounding-d21a@gregkh/T/" rel="nofollow">6.3.11</a>, and <a href="https://lore.kernel.org/stable/2023070140-eldercare-landlord-133c@gregkh/T/" rel="nofollow">6.4.1</a>), effectively resolving the &#34;Stack Rot&#34; bug on
July 1st.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-exploit" aria-hidden="true" href="#exploit"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Exploit</h2>
<p dir="auto"><strong>The complete exploit code and a comprehensive write-up will be made publicly
available no later than the end of July.</strong></p>
</article>
          </div></div>
  </body>
</html>
