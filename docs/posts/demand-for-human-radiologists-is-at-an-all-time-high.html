<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.worksinprogress.news/p/why-ai-isnt-replacing-radiologists">Original</a>
    <h1>Demand for human radiologists is at an all-time high</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><p><em><span>Works in Progress is becoming a print magazine. Our first print issue, Issue 21, will land in November. If you live in the United States or the United Kingdom, you can subscribe </span><a href="https://worksinprogress.co/print/" rel="">here</a><span>. If you live outside the US or UK and want to be notified as soon as subscriptions are live in your country, </span><a href="https://airtable.com/appYLHseqbAs4aOV9/paglNkIcTXDSx6Bz5/form" rel="">leave your details here</a><span>.</span></em></p><p><span>CheXNet can detect pneumonia with greater accuracy than a panel of board-certified radiologists. It is an AI model released in 2017, trained on more than 100,000 chest X-rays. It is fast, free, and can run on a </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10607847/#:~:text=The%20usage%20of%20the%20transfer%2Dlearning%20technique%20and%20traditional%20deep%20learning%20engineering%20techniques%20was%20shown%20to%20enable%20us%20to%20obtain%20such%20results%20on%20consumer%2Dclass%20GPUs%20(graphics%20processing%20units)." rel="">single consumer-grade GPU</a><span>. A hospital can use it to classify a new scan in under a second.</span></p><p><span>Since then, companies like Annalise.ai, Lunit, Aidoc, and Qure.ai have released models that can detect hundreds of diseases across multiple types of scans with greater accuracy and speed than human radiologists in benchmark tests. Some products can reorder radiologist worklists to prioritize critical cases, suggest next steps for care teams, or generate structured draft reports that fit into hospital record systems. A few, like </span><a href="https://www.healthvisors.com/en/idx-dr/" rel="">IDx-DR</a><span>, are even cleared to operate without a physician reading the image at all. In total, there are </span><a href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-enabled-medical-devices" rel="">over 700 FDA-cleared</a><span> radiology models, which account for roughly three-quarters of all medical AI devices.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_Gah!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_Gah!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 424w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 848w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 1272w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!_Gah!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png" width="1240" height="1018" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1018,&#34;width&#34;:1240,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!_Gah!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 424w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 848w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 1272w, https://substackcdn.com/image/fetch/$s_!_Gah!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27136d01-b2a8-4e09-92dc-c3d228110dc4_1240x1018.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><p><span>Radiology is a field optimized for human replacement, where digital inputs, pattern recognition tasks, and clear benchmarks predominate.</span><em> </em><span>In 2016, Geoffrey Hinton – computer scientist and Turing Award winner – </span><a href="https://www.youtube.com/watch?v=2HMPRXstSvQ" rel="">declared</a><span> that ‘people should stop training radiologists now’. If the most extreme predictions about the effect of AI on employment and wages were true, then radiology should be the canary in the coal mine.</span></p><p><span>But demand for human labor is higher than ever. In 2025, American diagnostic radiology residency programs offered a </span><a href="https://www.nrmp.org/wp-content/uploads/2025/03/Advance_Data_Tables_2025.pdf" rel="">record 1,208 positions</a><span> across all radiology specialties, a four percent increase from 2024, and the field’s </span><a href="https://www.asrt.org/docs/default-source/research/staffing-surveys/radiologic-sciences-workplace-and-staffing-survey-2023.pdf" rel="">vacancy rates are at all-time highs</a><span>. In 2025, radiology was the second-highest-paid medical specialty in the country, with an </span><a href="https://radiologybusiness.com/topics/healthcare-management/radiologist-salary/radiology-rises-no-2-highest-paid-specialty-surpassing-cardiology-and-plastic-surgery-medscape#:~:text=On%20average%2C%20full,rounded%20out%20the%20top%205" rel="">average</a><span> income of $520,000, over 48 percent higher than the average salary in </span><a href="https://radiologybusiness.com/topics/healthcare-management/radiologist-salary/average-radiologist-pay-has-leapt-nearly-38-2015#" rel="">2015</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZRYf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZRYf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 424w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 848w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 1272w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!ZRYf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png" width="1240" height="1104" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1104,&#34;width&#34;:1240,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:115248,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.worksinprogress.news/i/174517364?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZRYf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 424w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 848w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 1272w, https://substackcdn.com/image/fetch/$s_!ZRYf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c5c5c55-c95a-4edb-ac67-4925ad6f07c7_1240x1104.png 1456w" sizes="100vw"/></picture><div><div></div></div></div></a></figure></div><p>Three things explain this. First, while models beat humans on benchmarks, the standardized tests designed to measure AI performance, they struggle to replicate this performance in hospital conditions. Most tools can only diagnose abnormalities that are common in training data, and models often don’t work as well outside of their test conditions. Second, attempts to give models more tasks have run into legal hurdles: regulators and medical insurers so far are reluctant to approve or cover fully autonomous radiology models. Third, even when they do diagnose accurately, models replace only a small share of a radiologist’s job. Human radiologists spend a minority of their time on diagnostics and the majority on other activities, like talking to patients and fellow clinicians.</p><p>Artificial intelligence is rapidly spreading across the economy and society. But radiology shows us that it will not necessarily dominate every field in its first years of diffusion — at least until these common hurdles are overcome. Exploiting all of its benefits will involve adapting it to society, and society’s rules to it.</p><p><span>All AIs are functions or algorithms, called models, that take in inputs and spit out outputs. Radiology models are trained to detect a finding, which is a measurable piece of evidence that helps identify or rule out a disease or condition. </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12044510/" rel="">Most radiology models</a><span> detect a single finding or condition in one type of image. For example, a model might look at a chest CT and answer whether there are </span><a href="https://www.qure.ai/news_press_coverages/qure.ai-launches-FDA-cleared-AI-solution-for-advanced-lung-nodule-quantification-on-CT-scans-at-AABIP-2024" rel="">lung nodules</a><span>, </span><a href="https://www.signifyresearch.net/insights/ai-in-medical-imaging-news-round-up-february-2025/" rel="">rib fractures</a><span>, or what the </span><a href="https://www.signifyresearch.net/insights/ai-in-medical-imaging-news-round-up-february-2025/" rel="">coronary arterial calcium score</a><span> is.</span></p><p>For every individual question, a new model is required. In order to cover even a modest slice of what they see in a day, a radiologist would need to switch between dozens of models and ask the right questions of each one. Several platforms manage, run, and interpret outputs from dozens or even hundreds of separate AI models across vendors, but each model operates independently, analyzing for one finding or disease at a time. The final output is a list of separate answers to specific questions, rather than a single description of an image.</p><p><span>Even with hundreds of imaging algorithms approved by the Food and Drug Administration (FDA) on the market, the combined footprint of today’s radiology AI models still cover only a small fraction of real-world imaging tasks. Many cluster around a few use cases: stroke, breast cancer, and lung cancer together account for </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10546456/" rel="">about 60 percent</a><span> of models, but only a </span><a href="https://media.market.us/medical-imaging-statistics/" rel="">minority of the actual radiology imaging</a><span> volume that is carried out in the US. Other subspecialties, such as vascular, head and neck, spine, and thyroid imaging currently have </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11816879/#:~:text=subspecialties%20including%20vascular%2C%20head/neck%2C%20spine%2C%20thyroid%2C%20and%20FDG%20PET%2DCT%20imaging%20have%20relatively%20few%20AI%20products%20(between%201%20and%202)" rel="">relatively few AI products</a><span>. This is in part due to data availability: the scan needs to be common enough for there to be many annotated examples that can be used to train models. Some scans are also inherently more complicated than others. For example, ultrasounds are taken from multiple angles and do not have standard imaging planes, unlike X-rays.</span></p><p><span>Once deployed outside of the hospital where they were initially trained, models can struggle. In a standard clinical trial, samples are taken from multiple hospitals to ensure exposure to a broad range of patients and to avoid site-specific effects, such as a single doctor’s technique or how a hospital chooses to calibrate its diagnostic equipment.</span><span> But when an algorithm is undergoing regulatory approval in the US, its developers will normally test it on a relatively narrow dataset. Out of the models in 2024 that reported the number of sites where they were tested, </span><a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2833324" rel="">38 percent</a><span> were tested on data from a single hospital. Public benchmarks tend to rely on multiple datasets from the same hospital.</span></p><p><span>The performance of a tool can </span><a href="https://journals.plos.org/plosmedicine/article?id=10.1371%2Fjournal.pmed.1002683" rel="">drop</a><span> as much as 20 percentage points when it is tested out of sample, on data from other hospitals. </span><a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683" rel="">In one study</a><span>, a pneumonia detection model trained on chest X-rays from a single hospital performed substantially worse when tested at a different hospital. Some of </span><a href="https://www.nature.com/articles/s41746-024-01037-4" rel="">these challenges</a><span> stemmed from avoidable experimental issues like overfitting, but others are indicative of deeper problems like differences in how hospitals record and generate data, such as using slightly different imaging equipment. This means that individual hospitals or departments would need to retrain or revalidate today’s crop of tools before adopting them, even if they have been proven elsewhere.</span></p><p><span>The limitations of radiology models stem from deeper problems with building medical AI. Training datasets come with strict inclusion criteria, where the diagnosis must be unambiguous (typically </span><a href="https://blog.mozilla.ai/the-importance-of-ground-truth-data-in-ai-applications-an-overview/" rel="">confirmed</a><span> by a consensus of two to three experts or a pathology result) and without images that are shot at an odd angle, look too dark, or are blurry. This skews performance towards the easiest cases, which doctors are already best at diagnosing, and away from real-world images. In one 2022 </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9221818/" rel="">study</a><span>, an algorithm that was meant to spot pneumonia on chest X-rays faltered when the disease presented in subtle or mild forms, or when other lung conditions resembled pneumonia, such as pleural effusions, where fluid builds up in lungs, or in atelectasis (collapsed lung). Humans also benefit from context: one radiologist told me about a model they use that labels surgical staples as hemorrhages, because of the bright streaks they create in the image.</span></p><p><span>Medical imaging datasets used for training also tend to have fewer cases from </span><a href="https://www.medrxiv.org/content/10.1101/2025.06.06.25328913v1" rel="">children</a><span>, women, and ethnic minorities, making their performance generally worse for these demographics. Many </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9712398/" rel="">lack</a><span> </span><a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(21)00252-1/fulltext" rel="">information</a><span> about the gender or race of cases at all, making it difficult to adjust for these issues and address the problem of bias. The result is that radiology models often predict only a narrow slice of the world,</span><span> though there are scenarios where AI models do perform well, including identifying common diseases like pneumonia or certain tumors.</span></p><p><span>The problems don’t stop there. Even a model for the precise question you need and in the hospital where it was trained is unlikely to perform as well in clinical practice as it did in the benchmark. In benchmark studies, researchers isolate a cohort of scans, define goals in quantitative metrics, such as the sensitivity (the percentage of people with the condition who are correctly identified by the test) and specificity (the percentage of people </span><em>without</em><span> the condition who are correctly identified as such), and compare the performance of a model to the score of another reviewer, typically a human doctor. Clinical studies, on the other hand, show how well the model performs in a real healthcare setting without controls. Since the earliest days of computer-aided diagnosis, there has been a gulf between benchmark and clinical performance.</span></p><p><span>In the 1990s, </span><a href="https://laurenoakdenrayner.com/2019/01/21/medical-ai-safety-doing-it-wrong/" rel="">computer-aided diagnosis,</a><span> effectively rudimentary AI systems, were developed to screen mammograms, or X-rays of breasts that are performed to look for breast cancer. In trials, the combination of humans and computer-aided diagnosis systems </span><a href="https://pubmed.ncbi.nlm.nih.gov/2079409/" rel="">outperformed</a><span> humans alone in accuracy when evaluating mammograms. </span><a href="https://pubmed.ncbi.nlm.nih.gov/11274556/" rel="">More</a><span> </span><a href="https://ajronline.org/doi/full/10.2214/AJR.07.2812" rel="">controlled experiments</a><span> followed, which pointed to computer-aided diagnosis helping radiologists pick up more cancer with minimal costs.</span></p><p><span>The FDA approved mammography computer-aided diagnosis in 1998, and Medicare started to reimburse the use of computer-aided diagnosis in 2001. The US government </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6670274/#:~:text=There%20is%20a%20gap%20in,considered%20when%20advancing%20CAD%20use." rel="">paid radiologists $7 more</a><span> to report a screening mammogram if they used the technology; by 2010, approximately </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6670274/" rel="">74 percent of mammograms</a><span> in the country were read by computer-aided diagnosis alongside a clinician.</span></p><p><span>But computer-aided diagnosis turned out to be a disappointment. Between 1998 and 2002 researchers </span><a href="https://pubmed.ncbi.nlm.nih.gov/17409321/" rel="">analyzed</a><span> 430,000 screening mammograms from 200,000 women at 43 community clinics in Colorado, New Hampshire, and Washington. Among the seven clinics that turned to computer-aided detection software, the machines flagged more images, leading to clinicians conducting 20 percent more biopsies, but </span><a href="https://www.lrb.co.uk/the-paper/v36/n11/paul-taylor/breast-cancer-screening" rel="">uncovering no more cancer than before</a><span>. </span><a href="https://pubmed.ncbi.nlm.nih.gov/26414882/" rel="">Several</a><span> other large clinical studies had similar findings.</span></p><p><span>Another way to measure performance is to compare having computerized help to a second clinician reading every film, called ‘double reading’. Across </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0959804908001287#:~:text=CAD%20does%20not%20have%20a,in%20both%20sets%20of%20studies." rel="">ten trials and seventeen studies of double reading</a><span>, researchers found that computer aids did not raise the cancer detection rate but led to patients being called back an additional ten percent more often. In contrast, having two readers caught more cancers while slightly lowering callbacks. Computer-aided detection was worse than standard care, and much worse than another pair of eyes. In </span><a href="https://www.gehealthcare.com/-/media/1f6a7cd9ac304842bb8c04fc9c76dd9f.pdf?la=en&amp;hash=6A" rel="">2018</a><span>, Medicare stopped reimbursing doctors more for mammograms read with computer-aided diagnosis than those read by a radiologist alone.</span></p><p><span>One explanation for this gap is that people </span><a href="https://pubmed.ncbi.nlm.nih.gov/10812315/" rel="">behave differently</a><span> if they are treating patients day to day than when they are part of laboratory studies or other controlled experiments.</span><span> In particular, doctors appear to defer excessively to assistive AI tools in clinical settings in a way that they do not in lab settings. They did this even with much more primitive tools than we have today: one </span><a href="https://pubmed.ncbi.nlm.nih.gov/15354301/" rel="">clinical trial</a><span> all the way back in 2004 asked 20 breast screening specialists to read mammogram cases with the computer prompts switched on, then brought in a new group to read the identical films without the software. When guided by computer aids, doctors identified barely half of the malignancies, while those reviewing without the model caught 68 percent. The gap was largest when computer aids failed to recognize the malignancy itself; many doctors seemed to treat an absence of prompts as reassurance that a film was clean. Another </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/" rel="">review</a><span>, this time from 2011, found that when a system gave incorrect guidance, clinicians were 26 percent more likely to make a wrong decision than unaided peers.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2RNw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2RNw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 424w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 848w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 1272w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!2RNw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png" width="3900" height="2600" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:2600,&#34;width&#34;:3900,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:9032197,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.worksinprogress.news/i/174517364?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d6a172c-33aa-4ff9-bb3c-dfda071bff29_3900x2600.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2RNw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 424w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 848w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 1272w, https://substackcdn.com/image/fetch/$s_!2RNw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ce83619-5c80-4838-b4cb-9ae099d1369b_3900x2600.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption><span>Works in Progress is becoming a print magazine – you can subscribe </span><a href="https://worksinprogress.co/print/" rel="">here</a><span>.</span></figcaption></figure></div><p>It would seem as if better models and more automation could together fix the problems of current-day AI for radiology. Without a doctor involved whose behavior might change we might expect real-world results to match benchmark scores. But regulatory requirements and insurance policies are slowing the adoption of fully autonomous radiology AI.</p><p><span>The FDA </span><a href="https://www.fda.gov/media/160125/download?utm_" rel="">splits imaging software</a><span> into two regulatory lanes: assistive or triage tools, which require a licensed physician to read the scan and sign the chart, and autonomous tools, which do not. Makers of assistive tools simply have to show that their software can match the performance of tools that are already on the market. Autonomous tools have to clear a much higher bar: they must demonstrate that the AI tool will refuse to read any scan that is blurry, uses an unusual scanner, or is outside its competence. The bar is higher because, once the human disappears, a latent software defect could harm thousands before anyone notices.</span></p><p><span>Meeting that criteria is difficult. Even state-of-the-art vision networks falter with images that lack contrast, have unexpected angles, or lots of different artefacts. IDx-DR, a diabetic retinopathy screener and one of the few cleared to operate autonomously, comes with </span><a href="https://www.accessdata.fda.gov/cdrh_docs/reviews/DEN180001.pdf?utm_" rel="">guardrails</a><span>: the patient must be an adult with no prior retinopathy diagnosis; there must be two macula-centred photographs of the fundus (the rear of the eye) with a resolution of at least 1,000 times 1,000 pixels; if glare, small pupils or poor focus degrade quality, the software must self-abort and refer the patient to an eye care professional.</span></p><p>Stronger evidence and improved performance could eventually clear both hurdles, but other requirements would still delay widespread use. For example, if you retrain a model, you are required to receive new approval even if the previous model was approved. This contributes to the market generally lagging behind frontier capabilities.</p><p><span>And when autonomous models are approved, malpractice insurers are not eager to cover them. Diagnostic error is the costliest mistake in American medicine, resulting in </span><a href="https://www.hopkinsmedicine.org/-/media/armstrong-institute/documents/news/2013-4-23-diagnostic-errors-more-common.pdf" rel="">roughly a third</a><span> of all malpractice payouts, and radiologists are perennial defendants. Insurers believe that software makes catastrophic payments more likely than a human clinician, as a broken algorithm can harm many patients at once. Standard contract language now often includes phrases such as: ‘Coverage applies solely to interpretations reviewed and authenticated by a licensed physician; no indemnity is afforded for diagnoses generated autonomously by software’. One insurer, Berkley, even </span><a href="https://www.hunton.com/hunton-insurance-recovery-blog/the-continued-proliferation-of-ai-exclusions#:~:text=Berkley&#39;s%20%E2%80%9CAbsolute%E2%80%9D%20AI%20Exclusion,practices%2C%20procedures%2C%20or%20training;" rel="">carries</a><span> the blunter label ‘Absolute AI Exclusion’.</span></p><p><span>Without malpractice coverage, hospitals cannot afford to let algorithms sign reports. In the case of IDx-DR, the vendor, Digital Diagnostics, includes a </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8651697/" rel="">product liability policy</a><span> and an indemnity clause. This means that if the clinic used the device exactly as the FDA label prescribes, with adult patients, good-quality images, and no prior retinopathy, then the company will reimburse the clinic for damages traceable to algorithmic misclassification.</span></p><p><span>Today, if American hospitals wanted to adopt AI for fully independent diagnostic reads, they would need to believe that autonomous models deliver enough cost savings or throughput gains to justify pushing for exceptions to credentialing and billing norms. For now, usage is too sparse to make a difference. One 2024 investigation </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11458846/" rel="">estimated</a><span> that 48 percent of radiologists are using AI at all in their practice. A </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12202002/#:~:text=Similarly%2C%20while%20most%20organizations%20have%20deployed%20AI%20in%20imaging%20and%20radiology%2C%20only%2019%25%20of%20them%20report%20a%20high%20degree%20of%20success%20in%20this%20area." rel="">2025 survey</a><span> reported that only 19 percent of respondents who have started piloting or deploying AI use cases in radiology reported a ‘high’ degree of success.</span></p><p>Even if AI models become accurate enough to read scans on their own and are cleared to do so, radiologists may still find themselves busier, rather than out of a career.</p><p><span>Radiologists are useful for more than reading scans; a </span><a href="https://pubmed.ncbi.nlm.nih.gov/23763878/" rel="">study</a><span> that followed staff radiologists in three different hospitals in 2012 found that only 36 percent of their time was dedicated to direct image interpretation. More time is spent on overseeing imaging examinations, communicating results and recommendations to the treating clinicians and occasionally directly to patients, teaching radiology residents and technologists who conduct the scans, and reviewing imaging orders and changing scanning protocols.</span><span> This means that, if AI were to get better at interpreting scans, radiologists may simply shift their time toward other tasks. This would reduce the substitution effect of AI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!n5yi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!n5yi!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 424w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 848w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!n5yi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png" width="1240" height="1002" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1002,&#34;width&#34;:1240,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!n5yi!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 424w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 848w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 1272w, https://substackcdn.com/image/fetch/$s_!n5yi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e08d19-012e-497d-bf5c-bcb96a2f0170_1240x1002.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>As tasks get faster or cheaper to perform, we may also do more of them. In some cases, especially if lower costs or faster turnaround times open the door to new uses, the increase in demand can </span><a href="https://books.google.com/books?id=gAAKAAAAIAAJ&amp;q=editions:AAotKDT6KKcC&amp;pg=PR3#v=onepage&amp;q=editions%3AAAotKDT6KKcC&amp;f=false" rel="">outweigh</a><span> the increase in efficiency, a phenomenon known as Jevons paradox. This has historical precedent in the field: in the early 2000s hospitals swapped film jackets for digital systems. Hospitals that digitized improved radiologist productivity, and time to read an individual scan went down. A </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3043948/" rel="">study</a><span> at Vancouver General found that the switch boosted radiologist productivity 27 percent for plain radiography and 98 percent for CT within a year of going filmless. This occurred alongside other advancements in imaging technology that made scans faster to execute. Yet, no radiologists were laid off.</span></p><p><span>Instead, the overall American utilization rate per 1,000 insured patients for all imaging </span><a href="https://pubmed.ncbi.nlm.nih.gov/21962785/" rel="">increased by 60 percent</a><span> from 2000 to 2008. This is </span><a href="https://www.gao.gov/assets/gao-09-559.pdf" rel="">not explained</a><span> by a commensurate increase in physician visits. Instead, each visit was associated with more imaging on average. Before digitization, the nonmonetary price of imaging was high: the </span><a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4586334/" rel="">median reporting turnaround time</a><span> for x-rays was 76 hours for patients discharged from emergency departments, and 84 hours for admitted patients. After departments digitized, these times dropped to 38 hours and 35 hours, respectively.</span></p><p><span>Faster scans give doctors more options. Until the early 2000s, only exceptional trauma cases would receive whole-body CT scans; the increased speed of CT turnaround times mean that they are now a </span><a href="https://journals.sagepub.com/doi/10.1177/00031348211061330?icid=int.sj-abstract.similar-articles.8" rel="">common</a><span> choice. This is a reflection of elastic demand, a concept in economics that describes when demand for a product or service is very sensitive to changes in price. In this case, when these scans got cheaper in terms of waiting time, demand for those scans increased.</span></p><p><span>Over the past decade, improvements in image interpretation have run far ahead of their diffusion. Hundreds of models can spot bleeds, nodules, and clots, yet AI is often limited to assistive use on a small subset of scans in any given practice. And despite </span><a href="https://www.youtube.com/watch?v=2HMPRXstSvQ" rel="">predictions</a><span> to the contrary, head counts and salaries have continued to rise. The promise of AI in radiology is overstated by benchmarks alone.</span></p><p>Multi‑task foundation models may widen coverage, and different training sets could blunt data gaps. But many hurdles cannot be removed with better models alone: the need to counsel the patient, shoulder malpractice risk, and receive accreditation from regulators. Each hurdle makes full substitution the expensive, risky option and human plus machine the default. Sharp increases in AI capabilities could certainly alter this dynamic, but it is a useful model for the first years of AI models that benchmark well at tasks associated with a particular career.</p><p><span>However, there are industries where conditions are different. Large platforms </span><a href="https://www.cbsnews.com/news/facebook-artificial-intelligence-harmful-content-misinformation/" rel="">rely heavily</a><span> on AI systems to triage or remove harmful or policy-violating content. At Facebook and Instagram, 94 percent and 98 percent of moderation decisions respectively are </span><a href="https://www.lemonde.fr/en/pixels/article/2023/11/14/content-moderation-key-facts-to-learn-from-facebook-instagram-x-and-tiktok-transparency-reports_6252988_13.html" rel="">made by machines</a><span>. But many of the more sophisticated knowledge jobs look more like radiology.</span></p><p>In many jobs, tasks are diverse, stakes are high, and demand is elastic. When this is the case, we should expect software to, at least initially, lead to more human work, not less. The lesson from a decade of radiology models is neither optimism about increased output nor dread about replacement. Models can lift productivity, but their implementation depends on behavior, institutions and incentives. For now, the paradox has held: the better the machines, the busier radiologists have become.</p><p><em>Deena Mousa</em><span> </span><em><span>is a lead researcher at Open Philanthropy. Follow her on </span><a href="https://x.com/deenamousa" rel="">Twitter</a><span>.</span></em></p></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
