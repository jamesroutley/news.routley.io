<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.nvidia.com/labs/par/Perfusion/">Original</a>
    <h1>Nvidia Launches a 100kb text-to-image model called Perfusion</h1>
    
    <div id="readability-page-1" class="page">

<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span><sup>1</sup>NVIDIA,</span>
            <span><sup>2</sup>Tel Aviv University</span>
          </p>

          <p><span> </span>
            <span> Accepted to SIGGRAPH 2023 </span>
          </p>

          
        </div>
      </div>
    </div>
  </div>
</section>

<!--teaser-->
<section>
  <div>
    <div>
      <h2>
<!--        We present <span style="color: #a537fd"><b>Perfusion</b></span>, a new text-to-image personalization method.-->
<!--        With only a <b><u>100KB model size</u></b>, Perfusion can creatively portray personalized objects, allowing significant changes in their appearance,-->
<!--        while maintaining their identity, using a novel mechanism we call <b>‚Äú<u>Key-Locking</u>‚Äù</b>.-->
<!--        Perfusion can also <b><u>combine individually learned concepts</u></b> into a single generated image.-->
<!--        Finally, it enables <b><u>controlling the trade-off</u></b> between visual and textual alignment at inference time, covering the entire Pareto front with just a single trained model.-->

        We present <span><b>Perfusion</b></span>, a new text-to-image personalization method.
        With only a <b><u>100KB model size per concept</u></b> (excluding the pretrained model, which is a few GBs), trained for roughly <b><u>4 minutes</u></b>, Perfusion can creatively portray personalized objects.
        It allows significant changes in their appearance,
        while maintaining their identity, using a novel mechanism we call <b>‚Äú<u>Key-Locking</u>‚Äù</b>.
        Perfusion can also <b><u>combine individually learned concepts</u></b> into a single generated image.
        Finally, it enables <b><u>controlling the trade-off</u></b> between visual and textual alignment at inference time, covering the entire Pareto front with just a single trained model.

      </h2>
      <p><img src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/Teaser.png" alt="Teaser."/></p><!--      <h2 class="subtitle has-text-centered">-->
<!--        Perfusion enables image generation for personalized concepts with large changes in their appearance, pose, and context,-->
<!--        using a compact model of only 100KB per concept, without compromising identity.-->
<!--        We can combine learned concepts at inference time, creating scenes which portray multiple concepts-->
<!--        side-by-side, or even create interactions between them.-->
<!--      </h2>-->
    </div>
  </div>
</section>

<!--<!‚ÄìPerfusion‚Äì>-->
<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->

<!--      <div class="section-title">-->
<!--        <h2 class="title is-2">Perfusion</h2>-->
<!--      </div>-->

<!--    <div class="columns is-centered">-->
<!--      <div class="column">-->
<!--        <div class="content">-->
<!--          <p style="font-size: 20px">-->
<!--            We introduce Perfusion, a method to "teach" a text-to-image model about user-provided concepts, allowing it to generate new images of the concepts guided by text.-->
<!--Perfusion enables image generation for personalized concepts with large changes in their appearance, pose, and context, using a compact model of only 100KB per concept, without compromising identity.-->
<!--            We can combine learned concepts at inference time, creating scenes which portray multiple concepts side-by-side, or even create interactions between them.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<!--examples-->
<section>
  <div>
    <div>
      <h2> Perfusion can easily create appealing images.
        </h2>

      
    </div>
  </div>


<!--<style>-->
<!--.carousel-container {-->
<!--  display: flex;-->
<!--  justify-content: center;-->
<!--  margin: auto 50px;-->
<!--}-->
<!--</style>-->

</section>



<!--Abstract-->
<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
Text-to-image models (T2I) offer a new level of flexibility by allowing users
to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a
challenging problem. The task of T2I personalization poses multiple hard
challenges, such as maintaining high visual fidelity while allowing creative
control, combining multiple personalized concepts in a single image, and
keeping a small model size. We present <em>Perfusion</em>, a T2I personalization
method that addresses these challenges using dynamic rank-1 updates to
the underlying T2I model. Perfusion avoids overfitting by introducing a new
mechanism that ‚Äúlocks‚Äù new concepts‚Äô cross-attention Keys to their superordinate category. Additionally, we develop a <em>gated</em> rank-1 approach that
enables us to control the influence of a learned concept during inference time
and to combine multiple concepts. This allows runtime-efficient balancing
of visual-fidelity and textual-alignment with a single 100KB trained model, which is five orders of magnitude smaller than the current state of the art. Moreover, it can span different operating points across the Pareto front
without additional training.
Finally, we show that Perfusion outperforms strong baselines in both qualitative and quantitative terms. Importantly, key-locking leads to novel results compared to traditional approaches, allowing to portray personalized object interactions in unprecedented ways, even in one-shot settings.
          </p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Architecture -->
<section>
  <div>
    <div>
        <!--/ Architecture. -->
        <p>
          <h2>How does it work?</h2>
        </p>
        <div>
          <div>
            <p><img id="architecture" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/Architecture.png"/>
            </p>
          </div>
        </div>
        <p>
          <b>Architecture outline (A)</b>: A prompt is transformed into a sequence of encodings. Each encoding is fed to a set of cross-attention modules (purple
          blocks) of a diffusion U-Net denoiser. Zoomed-in purple module shows how the Key and Value pathways are conditioned on the text encoding. The Key drives
          the attention map, which then modulates the Value pathway. <b>Gated Rank-1 Edit (B)</b>: Top: The K pathway is locked so any encoding of ùëí_Hugsy that reaches
          ùëäùëò is mapped to the key of the supre-category ùêæ_teddy. Bottom: Any encoding of ùëí_Hugsy that reaches ùëäùë£ , is mapped to ùëâ_Hugsy, which is learned.
          The gated aspect of this update allows to selectively apply it to only the necessary encodings and provides means for regulating the strength of learned concept, as expressed in the output images.
        </p>
    </div>
  </div>
</section>

<!-- Baseline compare -->
<section>
  <div>

      <p>
        <h2>Comparison To Current Methods</h2>
      </p>

    <div>

      <div>
        <div>

          <p>
            Perfusion can enable more animate results, with better prompt-matching and less susceptibility to background traits from the original image.
            For each concept, we show exemplars from our training set, along with generated images,
            their conditioning texts and comparisons to Custom-Diffusion, Dreambooth and Textual-Inversion baselines.  <br/>
          </p>
            <p><img id="style_transfer" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/baseline_comparison_single_concept.png"/>
            </p>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Compositions -->
<section>
  <div>

      <p>
        <h2>Compositions</h2>
      </p>

    <div>

      <!-- Cross domain. -->
      <div>
        <div>

          <p>
            Our method enables us to combine multiple learned concept into a single generated image, using a textual prompt.  The concepts are individually learned and merged only during the runtime process to produce the final image. </p>
            <p><img id="compositions" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/compositions.png"/>
            </p>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Pareto -->
<section>
  <div>

      

    <div>

      <!-- Cross domain. -->
      <div>
        <div>

          <p>
            Our method enables to control the trade-off between visual-fidelity and the textual-alignment at inference time.
            A high bias value reduces the concept‚Äôs effect, while a low bias value makes it more influential.
            <b>With just a single 100KB trained  model and run-time parameter choices, Perfusion (blue and cyan) spans the Pareto front.</b>  <br/>
          </p>
            <div>
              <p><img id="pareto" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/pareto.png"/></p></div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- One shot -->
<section>
  <div>

      

    <div>

      <!-- Cross domain. -->
      <div>
        <div>

          <p>
            When training with a single image, our
method can generate images with both high visual-fidelity and textual-
alignment.   <br/>
          </p>
            <div>
              <p><img id="bias" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/one_shot.png"/></p></div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- key-locking -->
<section>
  <div>

      

    <div>

      <!-- Cross domain. -->
      <div>
        <div>

          <p>
            We present 3 variations of key-locking: </p>
            <div>
              <p><img id="key_locking" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/key_locking.png"/></p></div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Downstream models -->
<section>
  <div>

      

    <div>

      <!-- Cross domain. -->
      <div>
        <div>

          <p>
            A Perfusion concept  trained using a vanilla diffusion-model can generalize to fine-tuned variants. <br/>
          </p>
            <div>
              <p><img id="blended" src="https://research.nvidia.com/labs/par/Perfusion/static/images_perfusion/zero_shot_transfer.png"/></p></div>
        </div>
      </div>
      <!--/ Cross domain. -->

      <!-- New domain editing. -->
    </div>
    <!--/ New domain editing. -->
  </div>
</section>

<!-- Cite -->
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <p>If you find our work useful, please cite our paper:</p>
    <pre><code>@inproceedings{tewel2023keylocked,
      author = {Tewel, Yoad and Gal, Rinon and Chechik, Gal and Atzmon, Yuval},
      title = {Key-Locked Rank One Editing for Text-to-Image Personalization},
      year = {2023},
      booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
      location = {Los Angeles, CA, USA},
      series = {SIGGRAPH &#39;23}
}</code></pre>
  </div>
</section>





</div>
  </body>
</html>
