<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://justine.lol/matmul/">Original</a>
    <h1>LLaMA Now Goes Faster on CPUs</h1>
    
    <div id="readability-page-1" class="page">

<p>
Mar 31<sup>st</sup>, 2024 @ <a href="https://justine.lol/index.html">justine&#39;s web page</a>
</p>

<div>
<p><a href="https://justine.lol/matmul/llamafile.png"><img src="https://justine.lol/matmul/llamafile.png" alt="[line drawing of llama animal head in front of slightly open manilla folder filled with files]" width="320" height="320"/></a></p></div>

<p>
I just wrote 84 new matrix multiplication kernels for
<a href="https://github.com/mozilla-Ocho/llamafile">llamafile</a> which
enable it to read prompts / images faster. Compared to llama.cpp, prompt
eval time with llamafile should go anywhere between 30% and 500% faster
when using F16 and Q8_0 weights on CPU. The improvements are most
dramatic for ARMv8.2+ (e.g. RPI 5), Intel (e.g. Alderlake), and AVX512
(e.g. Zen 4) computers. My kernels go 2x faster than MKL for matrices
that fit in L2 cache, which makes them a work in progress, since the
speedup works best for prompts having fewer than 1,000 tokens.

</p><h3>Background</h3>

<p>
llamafile is a local LLM project I started with Mozilla back in Nov
2023. We&#39;re using
<a href="https://justine.lol/cosmo3/">Cosmopolitan Libc</a> to package
<a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a> as a
single-file cross-platform binary that runs on six OSes for AMD64 and
ARM64, while making gentle modifications. I believe that by improving
the core technology, we can give our users the best possible llama.cpp
experience, while helping both projects reach a broader audience.
Mozilla has been giving me the resources to do this.

</p><h3 id="enterprise">
  <a href="#enterprise">Performance Gains on Enterprise Hardware</a>
</h3>

<p>
When I first got into LLMs, my workstation was an austere Hewlett
Packard running Alpine with a spinning disk, slow RAM, an AVX2
processor, and no GPU. What I liked about llama.cpp is they were the
first LLM project that cared about people like me. So I started
volunteering full time and collaborated with guys like Slaren to
<a href="https://github.com/ggerganov/llama.cpp/pull/613">introduce
mmap()</a> support, which made weights load instantly using half as much
RAM. It was a leap forward for local LLMs at the time, but did little to
improve evaluation speed. Most of the inference code was written by
Georgi Gerganov himself, and it&#39;s so good that it&#39;d take me another year
to finally improve upon. Now that I have, let&#39;s see how much faster
things go on my old Hewlett Packard.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/hp.jpg"><img src="https://justine.lol/matmul/hp.jpg" alt="[photo of an HP
     ProDesk 600 G5 workstation]" width="510" height="222"/></a><p> LLM
     Performance on HP Intel® Core™ i9-9900 ($439) w/ 2200 MT/s RAM c. 2020
</p></th></tr><tr>
<th>prompt</th><th>eval</th><th>model
</th><th>weights</th><th>hardware
</th><th>software
</th></tr><tr><td>   28</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   17</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   12</td><td>   7</td><td>Mistral 7b</td><td> q4_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>   32</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   22</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   16</td><td>   4</td><td>Mistral 7b</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>   23</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>   15</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   14</td><td>   2</td><td>Mistral 7b</td><td> f16</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>  205</td><td>  26</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>  144</td><td>  26</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   91</td><td>  23</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr><tr><td>  171</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llamafile-0.7
</td></tr><tr><td>  118</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  101</td><td>  15</td><td>TinyLlama 1.1B</td><td> f16</td><td>Skylake</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Here we see that, on Skylake, llamafile users can expect to see a 2x
speedup and llama.cpp users can expect 50% better performance. Please
note this only applies to certain weights. So far, I&#39;ve only written
optimized kernels for the q8_0, f16, q4_1, q4_0, and f32 data types. I
think both q8_0 and f16 are really solid choices. Possibly even f32 if
you&#39;ve got plenty of RAM. That&#39;s because my new kernels change the
rules. They&#39;re doing such a good job fixing the memory bandwidth quants
always solved, that quantization could become the bigger bottleck. That
would be great news for the future of local language models, since it
means less need to trade away knowledge for speed.

</p><h3 id="hobbyist">
  <a href="#hobbyist">Performance Gains on Hobbyist Hardware</a>
</h3>

<p>
You don&#39;t need a large computer to run a large language model. One of
the best personal computers available in stores today is the Raspberry Pi. They
deliver good performance at a great price and consume very little power.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/pi5.jpg"><img src="https://justine.lol/matmul/pi5-small.jpg" alt="[photo of a raspberry pi 5 computer board]" width="320" height="243"/></a><p>
LLM Performance on $100 Raspberry Pi v5 (ARMv8.2) and v4 (ARMv8.0)
</p></th></tr><tr>
<th>prompt</th><th>eval</th><th>model
</th><th>weights</th><th>hardware
</th><th>software
</th></tr><tr><td>   62</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llamafile-0.7
</td></tr><tr><td>   28</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    8</td><td>   5</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI5</td><td> llamafile-0.6.2
</td></tr><tr><td>   45</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llamafile-0.7
</td></tr><tr><td>   35</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   20</td><td>   9</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI5</td><td> llamafile-0.6.2
</td></tr><tr><td>   10</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llamafile-0.7
</td></tr><tr><td>   10</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    9</td><td>   3</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>RPI4</td><td> llamafile-0.6.2
</td></tr><tr><td>    3</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llamafile-0.7
</td></tr><tr><td>    3</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>    4</td><td>   2</td><td>TinyLlama 1.1B</td><td> f16</td><td>RPI4</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Raspberry Pi released their fifth edition a few months ago and it&#39;s
outrageously fast compared to their previous model. They also introduced
support for the ARMv8.2 dotprod and fp16 arithmetic ISAs, which are very
useful for LLMs. Those two features alone enabled llama.cpp to achieve a
10x performance boost for f16 weights last year. This week I teased out
another 2x performance boost on top of that, by using a kernel that I
originally intended for AVX512. You wouldn&#39;t think a kernel designed for
beefy data center equipment would work out for a teensy tiny little
Raspberry Pi, but it actually fit hand in glove since both CPUs have 32
vector registers.

</p><p>
It&#39;s worthwhile to note that the new ARMv8.2 fp16 ISA may introduce more
errors than usual, since it causes llamafile to use fp16 words and we
aren&#39;t using techniques like <a href="https://justine.lol/matmul/kahan.gif">Kahan summation</a> for
computing dot products. So Q8_0 weights actually end up having slightly
better perplexity, because it uses the dotprod ISA which lets us updot
signed 8-bit integers into a 32-bit compute type which absorbs errors.
However this doesn&#39;t mean the faster fp16 weights can&#39;t be useful. Many
developers in this field view the differences as negligible.

</p><p>
For example, let&#39;s say you want to setup an email server on your pihole
and have TinyLLaMA filter spam. It&#39;s possible to
<a href="https://www.postfix.org/FILTER_README.html">configure Postfix
to filter content using a shell script</a> which lets you run the
llamafile command.

</p><pre>llamafile -m TinyLlama-1.1B-Chat-v1.0.f16.gguf \
          --grammar <span>&#39;root ::= &#34;yes&#34; | &#34;no&#34;&#39;</span> --temp 0 -c 0 \
          --no-display-prompt --log-disable -p <span>&#34;&lt;|user|&gt;
Can you say for certain that the following email is spam?

To: jtunney@gmail.com
From: Federal-Tax-DebtHelp &lt;ConfirmationEmail.inzk@janents.com&gt;
Subject: Reduce your payments to what you can afford

Reduce your payments to what you can afford 
 
 [IMG] 
 [IMG] 
 
 [IMG] 
&lt;/s&gt;
&lt;|assistant|&gt;&#34;</span>
</pre>

<p>
When I run the shell script above on my RPI5, it takes 3 seconds.

</p><pre>jart@pi5:~/scratch$ time ./spam.sh
yes

real    0m3.168s
user    0m10.851s
sys     0m0.541s
</pre>

<p>
There are several important things happening here:

</p><ul>

  <li>
    <code>--temp 0</code> turns off the random number generator (we
    don&#39;t want improvisation for a spam filter)

  </li><li>
    Here we see why prompt eval time is king. Token generation speed
    (eval time) doesn&#39;t matter for this use case, since we&#39;re using
    the <code>--grammar</code> flag to force the LLM to only print a
    single &#34;yes\n&#34; or &#34;no\n&#34; token.

  </li><li>
    I piped the original email through <code>links -codepage utf-8
    -force-html -width 400 -dump /dev/stdin</code> which reduced the
    number of tokens and removed hidden HTML content that was put there
    by the spammer to make the email look like ham to naive Bayesian
    filters.

  </li><li>
    The <code>-c 0</code> flag configures TinyLLaMA to use the maximum
    context size, which is 2048 tokens. That&#39;s the largest prompt we can
    give it. To help avoid feeding in too much text, you can pipe the
    email through <code>sed &#39;s/   */ /g&#39; | dd bs=1
    count=7000</code> to remove superfluous spaces and place an upper
    limit on its size.

</li></ul>

<p>
Please note that spam filtering is just the tip of the iceberg. I&#39;ve
always thought that &#34;generative ai&#34; is a misnomer, because language
models (more commonly known as &#34;The Algorithm&#34;) have always been used by
tech companies in the past to extract knowledge, curate information, and
rank content. That requires reading rather than writing. AI has never
traditionally needed to talk that much, because the English language
just isn&#39;t that useful at the scale tech companies operate. Even if you
could generate English summaries for exabytes of text a millionth its
size, that would still be more words than any team could hope to read in
a lifetime.

</p><h3 id="gaming">
  <a href="#gaming">Performance Gains on Gaming Hardware</a>
</h3>

<p>
Gamers have the highest quality expectations of any value consumer, so
any hardware built for them is usually pretty good. In the machine
learning industry, we have thrived for years repurposing hardware that
was intended for gamers. If it weren&#39;t for their important contribution,
the AI Winter may have needed to last another ten years. So a few months
ago, I asked a gamer to build me a computer that can replace my old
Hewlett Packard.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/meatball-pc.png"><img src="https://justine.lol/matmul/meatball-pc-1200.jpg" alt="[photo of computer in black box with metal panels and rgb fans]" width="600" height="413"/></a><p> LLM Performance on Intel® Core™
     i9-14900K ($600) w/ 6400 MT/s RAM
<!-- <th colspan="6"><a href="raptorlake.jpg"><img src="raptorlake.jpg" -->
<!--      alt="[photo of an Intel Core i9 14900K unlocked CPU box]" -->
<!--      width="600" height="450"></a><p> LLM Performance on Intel® Core™ -->
<!--      i9-14900K ($600) w/ 6400 MT/s RAM -->
</p></th></tr><tr>
<th>prompt</th><th>eval</th><th>model
</th><th>weights</th><th>hardware
</th><th>software
</th></tr><tr><td>   63</td><td>  12</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   40</td><td>   9</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   19</td><td>   7</td><td>Mistral 7b</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>   50</td><td>   7</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   13</td><td>   5</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   10</td><td>   4</td><td>Mistral 7b</td><td> f16</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>  406</td><td>  67</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>  273</td><td>  53</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  114</td><td>  43</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr><tr><td>  407</td><td>  42</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llamafile-0.7
</td></tr><tr><td>   90</td><td>  31</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   68</td><td>  30</td><td>TinyLlama 1.1B</td><td> f16</td><td>Alderlake</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
I think Alderlake is a great CPU but it&#39;s popularly misunderstood, as
evidenced by how easily I quintupled its float16 performance. Unlike
ARMv8.2, I was able to do that without introducing rounding errors,
since my x86 kernels use a float32 compute type internally. This means I
can have an even smarter spam filter. For example, when I run
my <code>spam.sh</code> shell script, it only takes 420 milliseconds,
which is 7x faster than my Raspberry Pi 5. That&#39;s right, when it comes
to small workloads, this chip is able to finish before CUDA even gets
started.

</p><p>
Alderlake owners can also look forward to the fact that llamafile takes
special care to not run on your efficiency cores. This is one of the
things that helps llamafile to go faster than llama.cpp. It also means
you can run LLMs around the clock and there&#39;s still plenty of resources
leftover for the other programs on your computer. The reason why that&#39;s
important is because llama.cpp dispatches threads in lockstep, which
would have meant that if any <code>1</code> core takes longer than the
others to do its job, then all other <code>n</code> cores would need to
busy loop until it completed.

</p><p>
However the greatest feature of this microprocessor is how quickly it
can build all 2.6 million lines of code in the Cosmopolitan monorepo. My
Hewlett Packard always took 64 seconds, but this gaming computer does it
in 20. It actually took 35 seconds originally; what made it faster is was applying
<a href="https://www.thermal-grizzly.com/en/conductonaut/s-tg-c-001-r">liquid
metal</a> and AI overclocking. Another reason systems code is so fast on
the Alderlake is there was a fire fight between the hackers and
scientists in the creation of this CPU, and the hackers won. I hope
they&#39;ll strike out a better compromise on AVX512 in the future, but
overall I&#39;m very happy with this chip, since I believe it represents
significant progress over previous models.

</p><h3 id="apple">
  <a href="#apple">Performance Gains on Apple Hardware</a>
</h3>

<p>
If there&#39;s a personal computer with the most class, it would definitely
be the Mac Studio. Gaining the performance advantage here was harder for
me, because it&#39;s the hardware platform the llama.cpp developers care
about most, plus I&#39;m working with a handicap due to my choice to use
Stallman&#39;s compiler instead of Apple&#39;s proprietary tools.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/mac-studio.png"><img src="https://justine.lol/matmul/mac-studio-800x800.jpg" alt="[photo of a silvery Mac Studio computer cube]" width="400" height="400"/></a><p> LLM Performance on Mac Studio CPU w/ 24-core
     M2 Ultra ($5000)
</p></th></tr><tr>
<th>prompt</th><th>eval</th><th>model
</th><th>weights</th><th>hardware
</th><th>software
</th></tr><tr><td>   90</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>   90</td><td>  27</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   37</td><td>  24</td><td>Mistral 7b</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>   79</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>   57</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>   21</td><td>  15</td><td>Mistral 7b</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>  457</td><td>  95</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>  564</td><td> 108</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  236</td><td>  95</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr><tr><td>  419</td><td>  66</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.7
</td></tr><tr><td>  400</td><td>  67</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llama.cpp 2024-03-26
</td></tr><tr><td>  141</td><td>  66</td><td>TinyLlama 1.1B</td><td> f16</td><td>M2 Ultra</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
I wouldn&#39;t want to pick a fight with an Apple user, because their M2
microprocessor turns llamafile into a firehose of synthetic content. The
trick Apple used to do it is leveraging their vertical integration. If
you buy a Mac Studio and look inside, you&#39;ll discover that they put the
RAM DIMMs <em>inside</em> the CPU. It makes latency-bound operations
like token generation go much faster, because the CPU no longer needs to
make all these long distance phone calls. However, in terms of sheer
flops (as measured by prompt tok/sec), we can see that compared to my
much cheaper Intel computer, the M2 Ultra only exposes 30% more compute
via the ARM ISA. You need to go through their proprietary frameworks
like Metal and Accelerate if you want to access anything more. If you
have xcode installed, then llamafile by default will compile a small
stub module which does just that, since despite my values I&#39;m happy to
help you get in front of any closed source library standing between you
and your silicon.

</p><p>
One important thing to know if you&#39;re considering buying a Mac Studio is
that, like the Windows Executive, XNU does a really good job keeping
your desktop stable, and that means protecting your system from you. It
takes me 45 seconds on Mac Studio to compile the Cosmo monorepo, due to
all these safety features; but if I fork bombed it, I&#39;d be surprised if
Netflix skipped a single frame. My <code>spam.sh</code> script also goes
430ms, which is slower than Intel. However none of this concerns me,
since I&#39;ve seen the way Asahi Linux is able to unleash the M2&#39;s full
potential.

</p><h3 id="professional">
  <a href="#professional">Performance Gains on Professional Hardware</a>
</h3>

<p>
While llamafile cares deeply about helping the GPU poor, it offers a
first-class experience to the 1% too. The AMD Ryzen Threadripper PRO
7995WX was just launched several months ago and it&#39;s the most expensive
CPU money can buy right now. It&#39;ll set you back $10,000 but you get 96
cores of AVX512, based on the Zen4 architecture.

</p><div>
<table>
<tbody><tr>
<th colspan="6"><a href="https://justine.lol/matmul/threadripper-pro.jpg"><img src="https://justine.lol/matmul/threadripper-pro-800x800.jpg" alt="[photo of AMD Ryzen 4 ThreadRipper Pro box]" width="400" height="400"/></a><p> LLM Performance on AMD Ryzen Threadripper PRO 7995WX w/ 96 cores ($10,000)
</p></th></tr><tr>
<th>prompt</th><th>eval</th><th>model
</th><th>weights</th><th>hardware
</th><th>software
</th></tr><tr><td>  557</td><td>  17</td><td>Mistral 7b</td><td> bf16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  485</td><td>  17</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  197</td><td>  16</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>   52</td><td>  18</td><td>Mistral 7b</td><td> f16</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td>  480</td><td>  10</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  221</td><td>  10</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llama.cpp 2024-03-30
</td></tr><tr><td>   38</td><td>   9</td><td>Mistral 7b</td><td> f32</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td>  382</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  283</td><td>  24</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>   37</td><td>  25</td><td>Mistral 7b</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td> 1929</td><td>  52</td><td>TinyLlama 1.1B</td><td> bf16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td> 1819</td><td>  52</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td>  824</td><td>  51</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>  295</td><td>  89</td><td>TinyLlama 1.1B</td><td> f16</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr><tr><td> 1268</td><td>  60</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.7
</td></tr><tr><td> 1127</td><td>  60</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llama.cpp 2024-03-29
</td></tr><tr><td>  169</td><td>  93</td><td>TinyLlama 1.1B</td><td> q8_0</td><td>7995WX</td><td> llamafile-0.6.2
</td></tr></tbody></table>
</div>

<p>
Here we see that, despite only being twice the price, the 7995WX x86 ISA
offers 7x more raw compute power than the M2 Ultra ARM ISA, and nearly
the same token generation speed, which is likely thanks to its 384mb L3
cache. When I bought this chip, I had to expand support in llama.cpp for
bfloat16 and AVX512 before I could fully test its capabilities. My work
means you can now run LLaMA 2.8x faster on Zen4 than you could before.

</p><p>
One thing I like about AVX512 is that Google&#39;s Gemma model can
<a href="https://github.com/google/gemma.cpp/issues/23">solve math
riddles on AVX512 but not on AVX2</a> because the bigger vectors usually
make it easier to reduce rounding errors. Its <code>VDPBF16PS</code>
instruction helps us updot bf16 similar to VNNI and ARM dotprod. Having
native support for bf16 is nice, since models like Mistral and TinyLLaMA
distribute weights using bfloat16 as their canonical format. If we were
to convert bf16 to fp16, then only 13% of the numbers that are possible
can be accurately represented. In practice, it matters little, since
99.71% of the numbers Mistral 7b uses are among that 13%. However I
believe that llamafile should deliver, to the best of its ability,
whatever number of bits are being claimed. Especially when doing so also
enables us to better exploit the capabilities of our hardware. Adding
bf16 support is my first big step towards improving that.

</p><p>
Please be warned that a lot of people who bought this Threadripper ran
into issues with sketchy RAM. I had to RMA the first DIMMs I bought for
this computer, because most of them died and I was getting 5 eval tokens
per second with Mistral. I&#39;ve been having better luck with a new full
kit of eight sticks that just arrived today. When I run <code>sysbench
memory run</code> it reports 10,033,424 mops, which is oddly faster than
my Mac Studio where 9,892,584 mops is reported, however my Intel
computer does 14,490,952. I expected my Threadripper&#39;s RAM to have that
speed since both set of components advertised 6400 MT/s with the same
timings, but I&#39;m told that I traded this away to have 256GB of ECC. As
for disk speed, <code>dd if=/dev/zero of=/tmp/output bs=128k count=50k;
rm -f /tmp/output</code> reports 1.6 GB/s which is 3.6x slower than my
Mac Studio, and 3x slower than my Intel (which has the same M.2 stick).
I&#39;m told that Intel and Apple are just better at this, but I wish I
understood why.

</p><p>
Last but not least, it runs my <code>spam.sh</code> script in 323ms and
builds the whole Cosmo monorepo in 13 seconds. It&#39;s actually capable of
building it faster, since this is the first time I&#39;ve ever seen my build
config being constrained by an individual artifact blocking the critical
path. I never thought I&#39;d live to see the day. I&#39;m also puzzled that
llamafile v0.6.2 is somehow managing to do 93 tokens per second; that&#39;s
40% faster than my M2. It&#39;s exciting news, since after reviewing the
breadth of this blog post, I would have wept if there were no more
optimizations possible.

</p><h2 id="source">
  <a href="#source">Source Code</a>
</h2>

<p>
The source code for my matrix multiplication kernels can be found at:

</p><ul>
<li><a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp">https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp</a>
</li></ul>

<p>
Both Mozilla and myself felt it would be worthwhile to contribute these
improvements to the upstream project. Doing that required adapting the
code to their preferred method of handling portability at compile-time.
We also took the liberty of changing the license from Apache 2.0 to MIT,
since the latter is what the llama.cpp developers prefer. Here are links
to the most recent pull requests I&#39;ve sent them:

</p><ul>
<li><a href="https://github.com/ggerganov/llama.cpp/pull/6414">https://github.com/ggerganov/llama.cpp/pull/6414</a>
</li><li><a href="https://github.com/ggerganov/llama.cpp/pull/6412">https://github.com/ggerganov/llama.cpp/pull/6412</a>
</li></ul>

<h2 id="details">
  <a href="#details">Technical Details</a>
</h2>

<h3 id="howitworks">
  <a href="#howitworks">How I Improved Prompt Eval Time on CPUs</a>
</h3>

<p>
There are dozens of mathematical operations a transformer model needs to
perform in order to generate text, e.g. rope, transpose, reshape,
softmax, rms_norm, etc. All of the performance improvements I described
above, were achieved by focusing exclusively on a single one, which
is <code>GGML_OP_MUL_MAT</code>, because that&#39;s what my Linux Perf
profiler told me llamafile spends 95% of its time doing.

</p><p>
So what is this matrix multiplication thing? We shall start by defining
the most important algorithm in the world using the pythonic dialect of
Python that is most popular with developers today:

</p><pre><span>def</span> <span>matmul</span>(A, B):
  <span>assert</span> <span>len</span>(B) == <span>len</span>(A[0])
  <span>return</span> [[<span>sum</span>(A[i][l] * B[l][j]
               <span>for</span> l <span>in</span> <span>range</span>(<span>len</span>(B)))
           <span>for</span> j <span>in</span> <span>range</span>(<span>len</span>(B[0]))]
          <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(A))]
</pre>

<p>
As we can see, it&#39;s just three for loops and a multiply-add. How hard
could it be?

</p><p>
On my workstation (which I call meatball), the code above goes a
screeching 0.042 gigaflops. Most Python programmers are smart enough to
know that they should delegate tasks like these to a library
like <code>np.matmul</code>, which goes 29 gigaflops. NumPy achieves its
speed using FORTRAN which for generations has been favored
by <a href="https://justine.lol/dox/pascal.txt">real programmers</a>
who&#39;ve led us to believe these libraries are something mysterious
chiseled in stone by the hand of Moses himself; but if we look at the
FORTRAN code NumPy actually uses, then it really isn&#39;t all that
complicated and could clearly benefit from some revision.

</p><pre>      <span>SUBROUTINE</span> <span>SGEMM</span>(TRANSA,TRANSB,M,N,K,ALPHA,A,LDA,B,LDB,BETA,C,LDC)
<span>*     .. Scalar Arguments ..</span>
      <span>REAL</span> ALPHA,BETA
      <span>INTEGER</span> K,LDA,LDB,LDC,M,N
      <span>CHARACTER</span> TRANSA,TRANSB
<span>*     .. Array Arguments ..</span>
      <span>REAL</span> A(LDA,*),B(LDB,*),C(LDC,*)
      [...]
<span>*
*           Form  C := alpha*A*B + beta*C.
*</span>
              <span>DO</span> <span>90</span> J = 1,N
                  <span>IF</span> (BETA.<span>EQ</span>.ZERO) <span>THEN</span>
                      <span>DO</span> <span>50</span> I = 1,M
                          C(I,J) = ZERO
   <span>50</span>                 <span>CONTINUE</span>
                  <span>ELSE IF</span> (BETA.<span>NE</span>.ONE) <span>THEN</span>
                      <span>DO</span> <span>60</span> I = 1,M
                          C(I,J) = BETA*C(I,J)
   <span>60</span>                 <span>CONTINUE</span>
                  <span>END IF</span>
                  <span>DO</span> <span>80</span> L = 1,K
                      <span>IF</span> (B(L,J).<span>NE</span>.ZERO) <span>THEN</span>
                          TEMP = ALPHA*B(L,J)
                          <span>DO</span> <span>70</span> I = 1,M
                              C(I,J) = C(I,J) + TEMP*A(I,L)
   <span>70</span>                     <span>CONTINUE</span>
                      <span>END IF</span>
   <span>80</span>             <span>CONTINUE</span>
   <span>90</span>         <span>CONTINUE</span>
      [...]
      <span>RETURN</span>
      <span>END</span>
</pre>

<p>
I like to define my subroutines using a modern language like C++, which
goes 47 gigaflops. This means C++ is three orders of a magnitude faster
than Python. That&#39;s twenty years of progress per Moore&#39;s law.

</p><pre><span>// multiplies matrices on cpu
// with column major ordering
//
//     m×k * k×n → m×n
//     k×m * k×n → m×n if aᵀ
//     m×k * n×k → m×n if bᵀ
//     k×m * n×k → m×n if aᵀ and bᵀ
//</span>
<span>template</span> &lt;<span>typename</span> T,  <span>typename</span> TA,
          <span>typename</span> TB, <span>typename</span> TC&gt;
<span>void</span> <span>GEMM</span>(<span>bool</span> aᵀ, <span>bool</span> bᵀ,
          <span>int</span> m, <span>int</span> n, <span>int</span> k, <span>T</span> α,
          <span>const</span> <span>TA</span> *A, <span>int</span> lda,
          <span>const</span> <span>TB</span> *B, <span>int</span> ldb, <span>T</span> β,
          <span>TC</span> *C, <span>int</span> ldc) {
    <span>assert</span>(m &gt;= 0 &amp;&amp; n &gt;= 0 &amp;&amp; k &gt;= 0);
    <span>assert</span>(lda &gt;= <span>std</span>::max(1, aᵀ ? k : m));
    <span>assert</span>(ldb &gt;= <span>std</span>::max(1, bᵀ ? n : k));
    <span>assert</span>(ldc &gt;= <span>std</span>::max(1, m));
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>T</span> d = 0;
            <span>for</span> (<span>int</span> l = 0; l &lt; k; ++l) {
                <span>T</span> a = A[aᵀ ? lda * i + l : lda * l + i];
                <span>T</span> b = B[bᵀ ? ldb * l + j : ldb * j + l];
                d += a * b;
            }
            <span>if</span> (β) {
                <span>T</span> c = C[ldc * j + i];
                C[ldc * j + i] = α * d + β * c;
            } <span>else</span> {
                C[ldc * j + i] = α * d;
            }
        }
}
</pre>

<p>
In order to do better than 47 gigaflops on CPU, most C++ developers are
smart enough to know they should use a BLAS library. Mightiest of the
open source BLAS is <a href="https://github.com/flame/blis/">BLIS</a>
which is funded by Microsoft, Intel, Texas Instruments, AMD, HPE,
Oracle, Huawei, Facebook, ARM, and the National Science Foundation.

</p><blockquote>
&#34;Any time somebody outside Intel beats MKL by a nontrivial amount, I
report it to the MKL team. It is fantastic for any open-source project
to get within 10% of MKL... [T]his is why Intel funds BLIS development.&#34;
(@jeffhammond) <a href="https://github.com/flame/blis/issues/264#issuecomment-428673275">blis/issues/264</a>
</blockquote>

<p>
That&#39;s very impressive. Matrix multiplication is the practical
application of hardware that hardware makers care about optimizing most.
Since nobody knows more about Intel hardware than Intel, I imagine it&#39;s
not everday that somebody manages to challenge Intel for supremacy on
their own platform. Based on my own evaluation, what BLIS says is true.
However that is only true for single-threaded performance. Their
multithreading mode is still experimental, but if I use
a <code>./configure</code> flag to turn it on, then I&#39;m able to boost
performance to 85 gigaflops.

</p><p>
llama.cpp had the important insight that less is more when it comes to
linear algebra. The alpha and beta parameters are never used, so they&#39;re
always set to to 1 and 0. The op graph for LLMs are designed in such a
way that the A matrix is almost always transposed and B is almost never
transposed, which means inner dimension dot product can vectorize over
contiguous memory. The m/k dimensions are usually evenly divisible by
64. While generating tokens, n=1 is usually the case, which makes matmul
a de facto matvec for the performance most people care about. BLAS
libraries usually hurt more than they help for matrix-vector
multiplication, because it&#39;s so computationally simple by comparison.
Sort of like the difference between downloading a movie and pinging a
server. Matrix vector multiplication is an operation where latency (not
throughput) is the bottleneck, and the bloat of fancy libraries has a
measurable impact. So llama.cpp does something like this, which goes 233
gigaflops.

</p><pre><span>template</span> &lt;<span>typename</span> T&gt;
<span>void</span> <span>LLMM</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
          <span>const</span> <span>T</span> *A, <span>int</span> lda,
          <span>const</span> <span>T</span> *B, <span>int</span> ldb,
          <span>T</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>T</span> d = 0;
            <span>for</span> (<span>int</span> l = 0; l &lt; k; ++l)
                d += A[lda * i + l] * B[ldb * j + l];
            C[ldc * j + i] = d;
        }
}
</pre>

<p>
This gives us the best possible token generation speeds. However
llama.cpp&#39;s Achilles heel on CPU has always been prompt processing
speed, which goes much slower. That&#39;s because chewing through prompts
requires bona fide matrix-matrix multiplication. Being able to do this
fast is important if you care about text summarization and LLaVA image
processing. That&#39;s the reason why support for countless BLAS libraries
has been added to llama.cpp over the past year. The most formidable of
them is Intel&#39;s Math Kernel Library (MKL) which goes 384 gigaflops.

</p><p>
The difference between 233 versus 384 gigaflops may not seem like much,
at least not compared to Python, but it&#39;s a tremendous gulf. MKL is also
closed source and proprietary. We aren&#39;t even allowed to disassemble it
and try to reverse engineer how it works. Intel has been developing math
kernels for fifty years and they hold the secrets they&#39;ve acquired very
close to their chest. But even if our desire for performance was so
great that we were willing to overlook the ethics of an open source
project spending the majority of its time inside a proprietary blob, the
simple fact of the matter is that integrating foreign BLAS libraries
into llama.cpp isn&#39;t that practical, due to the way its threading model
works. In order to improve prompt processing speed, we must figure out
the trick BLAS libraries use, and implement it in a scrappy
dependency-free way that stays true to llama.cpp&#39;s roots.

</p><p>
I believe the trick with CPU math kernels is exploiting instruction
level parallelism with fewer memory references. If you compile the
example above with <code>-O3 -ffast-math -march=native</code> then the
code your compiler generates should look like this:

</p><pre><span>void</span> <span>SLLMM</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
           <span>const</span> <span>float</span> *A, <span>int</span> lda,
           <span>const</span> <span>float</span> *B, <span>int</span> ldb,
           <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>__m256</span> c = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8)
                c = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l),
                                    _mm256_loadu_ps(B + ldb * j + l), c);
            C[ldc * j + i] = hsum(c);
        }
}
</pre>

<p>
So what llama.cpp usually does when it wants to improve things, is it&#39;ll
unroll the innermost loop like this:

</p><pre><span>void</span> <span>SLLMM2</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
           <span>const</span> <span>float</span> *A, <span>int</span> lda,
           <span>const</span> <span>float</span> *B, <span>int</span> ldb,
           <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; ++j) {
            <span>__m256</span> c0 = _mm256_setzero_ps();
            <span>__m256</span> c1 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 16) {
                c0 = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l + 0),
                                     _mm256_loadu_ps(B + ldb * j + l + 0), c0);
                c1 = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l + 8),
                                     _mm256_loadu_ps(B + ldb * j + l + 8), c1);
            }
            C[ldc * j + i] = hsum(c0) + hsum(c1);
        }
}
</pre>

<p>
That may slightly improve numerical stability, but it does very little
to enhance performance, since modern CPUs are perfectly capable of
speculatively executing future loop iterations on their own. What we
want to do instead is unroll the <em>outer</em> loop. The advantage of
doing this becomes clear if we consider how it enables us to share
the <code>a0</code> register load across multiple floating point
operations.

</p><pre><span>void</span> <span>SLLMM4</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
            <span>const</span> <span>float</span> *A, <span>int</span> lda,
            <span>const</span> <span>float</span> *B, <span>int</span> ldb,
            <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; ++i)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; j += 4) {
            <span>__m256</span> c0 = _mm256_setzero_ps();
            <span>__m256</span> c1 = _mm256_setzero_ps();
            <span>__m256</span> c2 = _mm256_setzero_ps();
            <span>__m256</span> c3 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8) {
                <strong><span>__m256</span> a0 = _mm256_loadu_ps(A + lda * (i + 0) + l);</strong>
                <span>__m256</span> k0 = _mm256_loadu_ps(B + ldb * (j + 0) + l);
                <span>__m256</span> k1 = _mm256_loadu_ps(B + ldb * (j + 1) + l);
                <span>__m256</span> k2 = _mm256_loadu_ps(B + ldb * (j + 2) + l);
                <span>__m256</span> k3 = _mm256_loadu_ps(B + ldb * (j + 3) + l);
                c0 = _mm256_fmadd_ps(a0, k0, c0);
                c1 = _mm256_fmadd_ps(a0, k1, c1);
                c2 = _mm256_fmadd_ps(a0, k2, c2);
                c3 = _mm256_fmadd_ps(a0, k3, c3);
            }
            C[ldc * (j + 0) + (i + 0)] = hsum(c0);
            C[ldc * (j + 1) + (i + 0)] = hsum(c1);
            C[ldc * (j + 2) + (i + 0)] = hsum(c2);
            C[ldc * (j + 3) + (i + 0)] = hsum(c3);
        }
}
</pre>

<p>
If we unroll both outer loops, the effect is compounded.

</p><pre><span>void</span> <span>SLLMM3X4</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
              <span>const</span> <span>float</span> *A, <span>int</span> lda,
              <span>const</span> <span>float</span> *B, <span>int</span> ldb,
              <span>float</span> *C, <span>int</span> ldc) {
<span>#pragma omp parallel for collapse(2) if (m * n * k &gt; 300000)</span>
    <span>for</span> (<span>int</span> i = 0; i &lt; m; i += 3)
        <span>for</span> (<span>int</span> j = 0; j &lt; n; j += 4) {
            <span>__m256</span> c00 = _mm256_setzero_ps();
            <span>__m256</span> c01 = _mm256_setzero_ps();
            <span>__m256</span> c02 = _mm256_setzero_ps();
            <span>__m256</span> c03 = _mm256_setzero_ps();
            <span>__m256</span> c10 = _mm256_setzero_ps();
            <span>__m256</span> c11 = _mm256_setzero_ps();
            <span>__m256</span> c12 = _mm256_setzero_ps();
            <span>__m256</span> c13 = _mm256_setzero_ps();
            <span>__m256</span> c20 = _mm256_setzero_ps();
            <span>__m256</span> c21 = _mm256_setzero_ps();
            <span>__m256</span> c22 = _mm256_setzero_ps();
            <span>__m256</span> c23 = _mm256_setzero_ps();
            <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8) {
                <span>__m256</span> k0 = _mm256_loadu_ps(B + ldb * (j + 0) + l);
                <span>__m256</span> k1 = _mm256_loadu_ps(B + ldb * (j + 1) + l);
                <span>__m256</span> k2 = _mm256_loadu_ps(B + ldb * (j + 2) + l);
                <span>__m256</span> k3 = _mm256_loadu_ps(B + ldb * (j + 3) + l);
                <span>__m256</span> a0 = _mm256_loadu_ps(A + lda * (i + 0) + l);
                c00 = _mm256_fmadd_ps(a0, k0, c00);
                c01 = _mm256_fmadd_ps(a0, k1, c01);
                c02 = _mm256_fmadd_ps(a0, k2, c02);
                c03 = _mm256_fmadd_ps(a0, k3, c03);
                <span>__m256</span> a1 = _mm256_loadu_ps(A + lda * (i + 1) + l);
                c10 = _mm256_fmadd_ps(a1, k0, c10);
                c11 = _mm256_fmadd_ps(a1, k1, c11);
                c12 = _mm256_fmadd_ps(a1, k2, c12);
                c13 = _mm256_fmadd_ps(a1, k3, c13);
                <span>__m256</span> a2 = _mm256_loadu_ps(A + lda * (i + 2) + l);
                c20 = _mm256_fmadd_ps(a2, k0, c20);
                c21 = _mm256_fmadd_ps(a2, k1, c21);
                c22 = _mm256_fmadd_ps(a2, k2, c22);
                c23 = _mm256_fmadd_ps(a2, k3, c23);
            }
            C[ldc * (j + 0) + (i + 0)] = hsum(c00);
            C[ldc * (j + 1) + (i + 0)] = hsum(c01);
            C[ldc * (j + 2) + (i + 0)] = hsum(c02);
            C[ldc * (j + 3) + (i + 0)] = hsum(c03);
            C[ldc * (j + 0) + (i + 1)] = hsum(c10);
            C[ldc * (j + 1) + (i + 1)] = hsum(c11);
            C[ldc * (j + 2) + (i + 1)] = hsum(c12);
            C[ldc * (j + 3) + (i + 1)] = hsum(c13);
            C[ldc * (j + 0) + (i + 2)] = hsum(c20);
            C[ldc * (j + 1) + (i + 2)] = hsum(c21);
            C[ldc * (j + 2) + (i + 2)] = hsum(c22);
            C[ldc * (j + 3) + (i + 2)] = hsum(c23);
        }
}
</pre>

<p>
Vectorized outer product with OpenMP goes 810 gigaflops on my Alderlake
i9-14900K with 6400 MT/s RAM when multiplying a 513×512 with a 512×512
matrix. That is twenty eight years of progress per Moore&#39;s law compared
to Python. It&#39;s clearly optimal since my CPU is listed as only being
capable of going
<a href="https://nanoreview.net/en/cpu/intel-core-i9-14900k">780
gigaflops</a>. Yes, I overclocked it with liquid metal. On the other
hand, MKL processes this matrix size at 295 gigaflops on my machine.

</p><pre><span>1</span>:        <span>vmovups</span>      (<span>%r10</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vmovups</span>      (<span>%rsi</span>,<span>%r9</span>,4),<span>%ymm4</span>
          <span>vmovups</span>      (<span>%rcx</span>,<span>%r9</span>,4),<span>%ymm2</span>
          <span>vmovups</span>      (<span>%rdx</span>,<span>%r9</span>,4),<span>%ymm1</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm6</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm15</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm12</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm9</span>
          <span>vmovups</span>      (<span>%rdi</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm5</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm14</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm11</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm8</span>
          <span>vmovups</span>      (<span>%rbx</span>,<span>%r9</span>,4),<span>%ymm0</span>
          <span>vfmadd231ps</span>  (<span>%r11</span>,<span>%r9</span>,4),<span>%ymm0</span>,<span>%ymm3</span>
          <span>add</span>          <span>$8</span>,<span>%r9</span>
          <span>vfmadd231ps</span>  <span>%ymm4</span>,<span>%ymm0</span>,<span>%ymm13</span>
          <span>vfmadd231ps</span>  <span>%ymm2</span>,<span>%ymm0</span>,<span>%ymm10</span>
          <span>vfmadd231ps</span>  <span>%ymm1</span>,<span>%ymm0</span>,<span>%ymm7</span>
          <span>cmp</span>          <span>%r9d</span>,<span>%r14d</span>
          <span>jg</span>          1<span>b</span>
</pre>

<p>
But does the C function above generalize to all matrix sizes? Nope. If I
bump the complexity up from 512 to 1024, then I&#39;m pretty much back at
square one, not doing much better than a naive kernel, and MKL wins once
more. I personally don&#39;t view this as too problematic, since llama.cpp
by default processes prompts in modestly sized batches, and a kernel
should only need to be good for its intended size. It&#39;s also only a
matter of time until I unriddle the tricks needed for optimal tiling and
cache locality that can make my kernels scale.

</p><h3 id="threads">
  <a href="#threads">How I Got Multiple Threads to Work</a>
</h3>

<p>
Now to incorporate this into llamafile, we can&#39;t use OpenMP for the same
reason we can&#39;t use BLAS libraries. The kernel must be harmonized with
the way llama.cpp works. Its threading model is very similar to GPUs.
Ops in the model graph are processed one by one. A thread is spawned for
each core. Threads are restrained by a spinlock barrier and then set
loose to compute different parts of an output matrix in parallel as soon
as the next op is ready for execution. The id of each thread is
called <code>ith</code> and the number of threads is
called <code>nth</code>. There are no futexes or semaphores, because
kernel scheduling would greatly reduce tokens/sec. If we were to have
the <code>ith=0</code> thread call a BLAS API that spawned threads of
its own, then they&#39;d be immediately starved of resources by all
the <code>ith&gt;0</code> threads returning to the spinlock barrier. We
can work within this model by defining a new kernel framework.

</p><pre><span>#define</span> <span>BEGIN_KERNEL</span>(RM, RN) \
    <span>int</span> ytiles = (m - m0) / RM; \
    <span>int</span> xtiles = (n - n0) / RN; \
    <span>int</span> tiles = ytiles * xtiles; \
    <span>int</span> duty = (tiles + nth - 1) / nth; \
    <span>if</span> (duty &lt; 1) \
        duty = 1; \
    <span>int</span> start = duty * ith; \
    <span>int</span> end = start + duty; \
    <span>if</span> (end &gt; tiles) \
        end = tiles; \
    <span>for</span> (<span>int</span> job = start; job &lt; end; ++job) { \
        <span>int</span> i = m0 + job / xtiles * RM; \
        <span>int</span> j = n0 + job % xtiles * RN;

<span>#define</span> <span>END_KERNEL</span>() }
</pre>

<p>
Along with a solution for packing tiles.

</p><pre><span>template</span> &lt;<span>typename</span> T&gt; <span>class</span> <span>GEMMER</span> {
  <span>public:</span>
    <span>GEMMER</span>(<span>int</span> k, <span>const</span> <span>T</span> *A, <span>int</span> lda, <span>const</span> <span>T</span> *B, <span>int</span> ldb, <span>float</span> *C, <span>int</span> ldc,
           <span>int</span> ith, <span>int</span> nth)
        : k(k), A(A), lda(lda), B(B), ldb(ldb), C(C), ldc(ldc), ith(ith), nth(nth) {
    }

    <span>void</span> <span>llmm</span>(<span>int</span> m, <span>int</span> n) {
        mnpack(0, m, 0, n);
    }

  <span>private:</span>
    <span>void</span> <span>mnpack</span>(<span>int</span> m0, <span>int</span> m, <span>int</span> n0, <span>int</span> n) {
        <span>if</span> (m - m0 &lt;= 0 || n - n0 &lt;= 0)
            return;
        <span>int</span> mc, nc, mp, np;
        <span>if</span> (m - m0 &gt;= 3 &amp;&amp; n - n0 &gt;= 4) {
            mc = 3;
            nc = 4;
            llmm3x4(m0, m, n0, n);
        } <span>else if</span> (m - m0 &gt;= 4 &amp;&amp; n - n0 &gt;= 1) {
            mc = 4;
            nc = 1;
            llmm4x1(m0, m, n0, n);
        } <span>else if</span> (m - m0 &gt;= 1 &amp;&amp; n - n0 &gt;= 4) {
            mc = 1;
            nc = 4;
            llmm1x4(m0, m, n0, n);
        } <span>else</span> {
            mc = 1;
            nc = 1;
            llmm1x1(m0, m, n0, n);
        }
        mp = m0 + (m - m0) / mc * mc;
        np = n0 + (n - n0) / nc * nc;
        mnpack(mp, m, n0, np);
        mnpack(m0, mp, np, n);
        mnpack(mp, m, np, n);
    }

    <span>// ...</span>

    <span>void</span> <span>llmm1x1</span>(<span>int</span> m0, <span>int</span> m, <span>int</span> n0, <span>int</span> n) {
        BEGIN_KERNEL(1, 1)
        <span>__m256</span> c = _mm256_setzero_ps();
        <span>for</span> (<span>int</span> l = 0; l &lt; k; l += 8)
            c = _mm256_fmadd_ps(_mm256_loadu_ps(A + lda * i + l),
                                _mm256_loadu_ps(B + ldb * j + l), c);
        C[ldc * j + i] = hsum(c);
        END_KERNEL()
    }

    <span>const</span> <span>int</span> k;
    <span>const</span> <span>T</span> *<span>const</span> A;
    <span>const</span> <span>int</span> lda;
    <span>const</span> <span>T</span> *<span>const</span> B;
    <span>const</span> <span>int</span> ldb;
    <span>float</span> *<span>const</span> C;
    <span>const</span> <span>int</span> ldc;
    <span>const</span> <span>int</span> ith;
    <span>const</span> <span>int</span> nth;
};
</pre>

<p>
We can now export nice friendly C APIs to GGML that go 790 gigaflops
while incurring none of the latency disadvantages associated with
traditional BLAS libraries.

</p><pre><span>void</span> <span>SLLMMT</span>(<span>int</span> m, <span>int</span> n, <span>int</span> k,
            <span>const</span> <span>float</span> *A, <span>int</span> lda,
            <span>const</span> <span>float</span> *B, <span>int</span> ldb,
            <span>float</span> *C, <span>int</span> ldc,
            <span>int</span> ith, <span>int</span> nth) {
    <span>if</span> (nth) {
        <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, ith, nth};
        tb.llmm(m, n);
    } <span>else if</span> (!HAVE_OPENMP || n * m * k &lt; THRESHOLD) {
        <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, 0, 1};
        tb.llmm(m, n);
    } <span>else</span> {
        nth = sysconf(_SC_NPROCESSORS_ONLN);
<span>#pragma omp parallel for</span>
        <span>for</span> (ith = 0; ith &lt; nth; ++ith) {
            <span>GEMMER</span>&lt;<span>float</span>&gt; tb{k, A, lda, B, ldb, C, ldc, ith, nth};
            tb.llmm(m, n);
        }
    }
}
</pre>

<h2 id="methodology">
  <a href="#methodology">Methodology</a>
</h2>

<p>
You need to run the following command on Linux in order to benchmark
llamafile reliably. It also helps a little bit with timings to run as
root, but that shouldn&#39;t be necessary.

</p><pre><span>echo</span> performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
</pre>

<p>
On Apple Silicon, I needed to build llama.cpp using the following
command in order to get it to run in CPU mode.

</p><pre>make -j32 LLAMA_NO_ACCELERATE=1 LLAMA_NO_METAL=1
</pre>

<p>
Some of you might be surprised that I didn&#39;t write my kernels in
assembly like BLIS does, especially given my history of projects
like <a href="https://justine.lol/matmul/github.com/jart/blink">blink</a>, <a href="https://justine.lol/sectorlisp2/">SectorLISP</a>
and <a href="https://justine.lol/sectorlambda2/">SectorLAMBDA</a>. The truth is I&#39;ve been
coding in assembly this whole time. I configured Emacs so I can push a
button, and the disassembly for the C++ code I&#39;m working on will pop up
on the screen in a few milliseconds. I know anyone whose codebase has
slow build times doesn&#39;t possess this advantage, which has made me
famous. Once I figure out how to do that for .cu files, I&#39;ll be
unstoppable.

</p><h2 id="credits">
  <a href="#credits">Credits</a>
</h2>

<p>
I learned how to write math kernels by renting
<a href="https://vast.ai/">Vast</a> VMs and watching
<a href="https://ahgamut.github.io/">Gautham Venkatasubramanian</a>
and
<a href="https://github.com/mrdomino">mrdomino</a> develop CUDA kernels
in a tmux session. They&#39;ve been focusing on solving a much more
important challenge for llamafile, which is helping it not have a
mandatory dependency on the cuBLAS: the reigning supreme linear algebra
library of such speed, accuracy, and ferocity that it could only have
been written by the prince of darkness himself. You&#39;re encouraged to
follow our ongoing progress on GitHub.

</p><h2 class="page" id="discord"><a href="#discord">Discord</a></h2>

Congratulations on reading this far. I&#39;d like to extend an invitation
for you to join us on the
<a href="https://discord.gg/Va7WsS56x3">Mozilla AI Discord</a>, where
you can ask questions and hang out with me, folks from Mozilla, and
others in the llamafile community.

<h2 class="page" id="funding"><a href="#funding">Funding</a></h2>

<p>
  <a href="https://justine.lol/lemuria.png">
    <picture>
      <source srcset="//worker.jart.workers.dev/sectorlisp2/lemuria.webp" type="image/webp"/>
      <img src="https://worker.jart.workers.dev/sectorlisp2/lemuria.png" width="850" height="360" alt="[United States of Lemuria - two dollar bill - all debts public and primate]"/>
    </picture>
  </a>

</p><p>
My full-time work on open source projects like llamafile is funded
thanks to the generous support of Mozilla,
my <a href="https://github.com/sponsors/jart">GitHub sponsors</a>, and
<a href="https://www.patreon.com/jart">Patreon subscribers</a>. Thank
you everyone, for helping me have the opportunity to serve you these
last four years. Your support made it possible for high-quality math
kernels to be shared with the commons.

</p>
</div>
  </body>
</html>
