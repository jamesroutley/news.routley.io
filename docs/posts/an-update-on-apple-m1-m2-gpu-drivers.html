<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/995383/34dc5950cab5e739/">Original</a>
    <h1>An Update on Apple M1/M2 GPU Drivers</h1>
    
    <div id="readability-page-1" class="page"><div>
<center>
<table>
<tbody><tr><td>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the discount offer on the right.  Thank you
for visiting LWN.net!
</p></td><td>
<div>
<h3>Special discount offer</h3>
           <p>
           <a href="https://lwn.net/Promo/sl-discount-3/claim">Subscribe to LWN now</a> at the
           &#34;professional hacker&#34; level for at least six months,
           and you will
           receive a special discount of 25%.
           
</p></div>
</td>
</tr>

</tbody></table>
</center>

<p>
The kernel graphics driver for the Apple M1 and M2 GPUs is, rather
famously, written in Rust, but it has achieved conformance with
various graphics standards, which is also noteworthy.  At the <a href="https://indico.freedesktop.org/event/6/">X.Org Developers Conference
(XDC) 2024</a>, Alyssa Rosenzweig gave an update on the status of the
driver, along with some news about the kinds of games it can support (<a href="https://www.youtube.com/watch?v=TtLP5sAXYKo">YouTube video</a>, <a href="https://indico.freedesktop.org/event/6/contributions/284/attachments/230/310/slides.pdf ">slides</a>).
There has been lots of progress since her talk at XDC last year (<a href="https://www.youtube.com/watch?v=O36VFNdQHsE">YouTube video</a>),
with, of course, still more to come.
</p>

<p>
It is something of an XDC tradition, since she began it in Montreal in 2019
(<a href="https://www.youtube.com/watch?v=PqAAWzchHvk">YouTube video</a>),
for Rosenzweig to give her presentations dressed like a witch.
This year&#39;s edition was no exception, though this time she started her talk in
French, which resulted in some nervous chuckles from attendees. After a few
sentences, she switched to English, &#34;<q>I&#39;m just kidding</q>&#34;, and
continued with her talk.
</p>

<h4>Updates and tessellation</h4>

<p>
Last year at XDC, she and Asahi Lina reported that the driver had reached
<a href="https://en.wikipedia.org/wiki/OpenGL_ES">OpenGL ES</a> 3.1
conformance.  They also talked about <a href="https://www.khronos.org/opengl/wiki/Geometry_Shader">geometry
shaders</a>, because &#34;<q>that was the next step</q>&#34;.  Since then, the
driver has become <a href="https://en.wikipedia.org/wiki/OpenGL">OpenGL</a> 4.6
conformant. That meant she was going to turn to talking about <a href="https://www.khronos.org/opengl/wiki/tessellation">tessellation</a>
shaders, &#34;<q>as I threatened to do at the end of last year&#39;s talk</q>&#34;.
</p>

<p>
<a href="https://en.wikipedia.org/wiki/Tessellation_(computer_graphics)">Tessellation</a>,
which is a technique that &#34;<q>allows detail to be dynamically added and
subtracted</q>&#34; from a scene, is required for OpenGL 4.0, and there is
a hardware tessellator on the Apple GPU—but, &#34;<q>we can&#39;t use it</q>&#34;.  The
hardware is too limited to implement any of the standards; &#34;<q>it is
missing features that are hard required for OpenGL, <a href="https://www.vulkan.org/">Vulkan</a>, and <a href="https://en.wikipedia.org/wiki/Direct3D">Direct3D</a></q>&#34;.  That
makes it &#34;<q>pretty much useless to anybody who is not implementing <a href="https://en.wikipedia.org/wiki/Metal_(API)">Metal</a></q>&#34;.  Apple
supports OpenGL 4.1, though it is not conformant, but if you use any
of the features that the hardware does not support, it simply falls back to
software; &#34;<q>we are not going to do that</q>&#34;.
</p>

<p><a href="https://lwn.net/Articles/996249/">
<img src="https://static.lwn.net/images/2024/xdc-rosenzweig-sm.png" alt="[Alyssa Rosenzweig]" title="Alyssa Rosenzweig" width="225" height="280"/>
</a></p><p>
As far as Rosenzweig is aware, the hardware lacks <a href="https://docs.vulkan.org/spec/latest/chapters/tessellation.html#tessellation-point-mode">point mode</a>, where points
are used instead of the usual triangles; it also lacks <a href="https://docs.vulkan.org/spec/latest/chapters/tessellation.html#tessellation-isoline-tessellation">isoline</a>
support, but those two things can be emulated. The real problem comes
with <a href="https://www.khronos.org/opengl/wiki/Transform_Feedback">transform
feedback</a> and geometry shaders, neither of which is supported by the
hardware, but the driver emulates them with <a href="https://www.khronos.org/opengl/wiki/Compute_Shader">compute
shaders</a>.  However, the hardware tessellator cannot be used at all when
those are being emulated because minute differences in the tessellation algorithms used
by the hardware and the emulation would result in <a href="https://docs.vulkan.org/spec/latest/appendices/invariance.html">invariance</a>
failures.  She is not sure whether that is a problem in practice or not,
&#34;<q>but the spec says not to do it</q>&#34;, so she is hoping not to have to go
that route.
</p>

<p>
Instead, &#34;<q>we use software</q>&#34;.  In particular, Microsoft released a
reference tessellator a decade or more ago, which was meant to show
hardware vendors what they were supposed to implement when tessellation was
first introduced.  It is &#34;<q>a giant pile of 2000 lines of C++</q>&#34; that
she does not understand, despite trying multiple times; &#34;<q>it is
inscrutable</q>&#34;. The code will tessellate a single <a href="https://www.khronos.org/opengl/wiki/tessellation#Patches">patch</a>, which gave the
driver developers an idea: &#34;<q>if we can run that code, we can get the
tessellation outputs and then we can just draw the triangles or the lines
with this <a href="https://vulkan-tutorial.com/Vertex_buffers/Index_buffer">index buffer</a></q>&#34;.
</p>

<p>
There are some problems with that approach, however, starting with the fact
that the developers are writing a GPU driver; &#34;<q>famously, GPUs do not like running
2000 lines of C++</q>&#34;.  But, she announced, &#34;<q>we have conformant <a href="https://www.khronos.org/opencl/">OpenCL</a> 3.0 support</q>&#34;
thanks to Karol Herbst, though it has not yet been released.  OpenCL C is
&#34;<q>pretty much the same as regular CPU C</q>&#34;, though it has a few
limitations and some extensions for GPUs.  So the idea would be to turn the
C++ tessellation code into OpenCL C code; &#34;<q>we don&#39;t have to understand
any of it, we just need to not break anything when we do the port</q>&#34;.
</p>

<p>
That works, but &#34;<q><tt>tessellator.cl</tt> is the most unhinged file of my
career</q>&#34;; doing things that way was also the most unhinged thing she has done in her career
&#34;<q>and I&#39;m standing up here in a witch hat for the fifth year in a
row</q>&#34;. The character debuted in the exact same room in 2019 when she
was 17 years old, she recalled.
</p>

<p>
The CPU tessellator only operates on a single patch at a time, but a scene
might have 10,000 patches—doing them all serially will be a real problem.
GPUs are massively parallel, though, so having multiple threads each doing
tessellation is &#34;<q>pretty easy to arrange</q>&#34;.  There is a problem with
memory allocation; the CPU tessellator just allocates for each operation
sequentially, but that will not work for parallel processing.  Instead, the
driver uses the GPU atomic instructions to manage the allocation of output buffers.
</p>

<p>
In order to draw the output of the tessellators, though, there is a need to
use draw instructions with packed data structures as specified by the GPU.
That is normally done from the C driver code using functions that are <a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/25498">generated
by the GenXML tool</a>.  Since the tessellators are simply C code,
&#34;<q>thanks to OpenCL</q>&#34;, the generated functions can be included into the
code that runs on the GPU.  Rosenzweig went into more detail, which fills
in the holes (and likely inaccuracies) of the above description; those
interested in the details should look at the presentation video and her
slides.
</p>

<p>
&#34;<q>Does it work?  Yes, it does.</q>&#34;
She showed an image of terrain tessellation from a Vulkan demo.  It was run
on an M2 Mac with &#34;<q>entirely OpenCL-based tessellation</q>&#34;. There is also
the question of &#34;<q>how is the performance of this abomination?</q>&#34;  The
answer is that it is &#34;<q>okay</q>&#34;.  On the system, software-only terrain tessellation runs at
less than one frame-per-second (fps), which &#34;<q>is not very fun for playing
games</q>&#34;; for OpenCL, it runs at 265fps, which is &#34;<q>pretty good</q>&#34;
and is unlikely to be the bottleneck for real games.  The hardware
can do
820fps; &#34;<q>I did wire up the hardware tessellator just to get a number for
this talk.</q>&#34;   There is still room for improvement on the driver&#39;s
numbers, she said.
</p>

<h4>Vulkan and gaming</h4>

<p>
She also announced Vulkan 1.3 conformance for the Honeykrisp M1/M2 GPU
driver. It <a href="https://rosenzweig.io/blog/vk13-on-the-m1-in-1-month.html">started</a>
by copying the <a href="https://docs.mesa3d.org/drivers/nvk.html">NVK
Vulkan driver for NVIDIA GPUs</a>, &#34;<q>smashed against the [Open]GL 4.6
[driver]</q>&#34;, which started passing the conformance test suite
&#34;<q>in about a month</q>&#34;.  That was six months ago and, since then, she
has added geometry and tessellation shaders, transform feedback, and <a href="https://docs.vulkan.org/spec/latest/chapters/shaders.html#shaders-objects">shader
objects</a>.  The driver now supports every feature needed for multiple
DirectX versions.
</p>

<p>
There are a lot of problems &#34;<q>if we want to run triple-A (AAA) games on
this system</q>&#34;, however.  A target game runs on DirectX and Windows on an x86 CPU with
4KB pages, but &#34;<q>our target hardware is running literally none of those
things</q>&#34;. What is needed is to somehow translate DirectX to Vulkan,
Windows to Linux, x86 to Arm64, and 4KB pages to 16KB pages.  The first two
have a well-known solution in the form of the <a href="https://github.com/doitsujin/dxvk/wiki">DXVK driver</a> and <a href="https://www.winehq.org/">Wine</a>, which are &#34;<q>generally packaged
into <a href="https://en.wikipedia.org/wiki/Proton_(software)">Proton</a>
for <a href="https://en.wikipedia.org/wiki/Steam_(service)">Steam</a> gaming</q>&#34;.
Going from x86 to Arm64 also has off-the-shelf solutions: <a href="https://fex-emu.com/">FEX-Emu</a> or <a href="https://box86.org/">Box64</a>.  She has a bias toward FEX-Emu;
&#34;<q>when I am not committing <a href="https://www.mesa3d.org/">Mesa</a>
crimes, I am committing FEX-Emulation crimes</q>&#34;.  The big problem,
though, is the page-size difference.
</p>

<p>
FEX-Emu requires 4KB pages; Box64 has a &#34;<q>hack to use 16KB pages, but it
doesn&#39;t work for Wine, so it doesn&#39;t help us here</q>&#34;.  MacOS can use 4KB
pages for the x86 emulation, but &#34;<q>this requires very invasive kernel
support</q>&#34;;  <a href="https://asahilinux.org/">Asahi Linux</a> already has around 1000 patches that are making
their way toward the mainline kernel, but &#34;<q>every one of those thousand
is a challenge</q>&#34;. Making changes like &#34;<q>rewriting the Linux memory
manager</q>&#34; is not a reasonable path.
</p>

<p>
It turns out that, even though Linux does not support heterogeneous page
sizes between different processes, it does support them between different
kernels; &#34;<q>what I mean by that is virtualization</q>&#34;.  A KVM guest
kernel can have a different page size than the host kernel.  So, &#34;<q>this
entire mess</q>&#34;, consisting of FEX-Emu, Wine, DXVK, Honeykrisp, Steam, and
the game, &#34;<q>we are going to throw that into a virtual machine, which is
running a 4KB guest kernel</q>&#34;.
</p>

<p>
There is some overhead, of course, but it is hardware virtualization, so
that should have low CPU overhead.  The problem lies with the peripherals,
she said.  So, instead of having Honeykrisp in the host kernel, it runs in
the guest using <a href="https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf">virtgpu</a>
native contexts;  all of the work to create the final GPU command buffer is done
in the guest and handed to the host, rather than making all of the Vulkan
calls individually traverse the virtual-machine boundary.  The <a href="https://docs.mesa3d.org/drivers/virgl.html">VirGL</a> renderer on the
host then hands that to the GPU, which &#34;<q>is not 100% native speed, but
definitely well above 90%</q>&#34;, Rosenzweig said.
</p>

<p>
The good news is that the overheads for the CPU and GPU do not stack, since
the two run in parallel. &#34;<q>So all the crap overhead we have in the CPU is
actually crap that is running in parallel to the crap overhead on the GPU,
so we only pay the cost once.</q>&#34;
</p>

<p>
&#34;<q>&#39;Does it work?&#39; is the question you all want to know.</q>&#34;  It does,
she said, it runs
games like <a href="https://en.wikipedia.org/wiki/Portal_(video_game)">Portal</a> and <a href="https://en.wikipedia.org/wiki/Portal_2">Portal 2</a>. She also listed a number of
others: <a href="https://en.wikipedia.org/wiki/Castle_Crashers">Castle Crashers</a>,
<a href="https://en.wikipedia.org/wiki/The_Witcher_3:_Wild_Hunt">The
Witcher 3</a>, <a href="https://en.wikipedia.org/wiki/Fallout_4">Fallout 4</a>, <a href="https://en.wikipedia.org/wiki/Control_(video_game)">Control</a>, <a href="https://en.wikipedia.org/wiki/Ghostrunner">Ghostrunner</a>, and <a href="https://en.wikipedia.org/wiki/Cyberpunk_2077">Cyberpunk 2077</a>.
</p>

<p>
All of the different pieces that she mentioned were <a href="https://rosenzweig.io/blog/aaa-gaming-on-m1.html">made available</a>
on October 10, the day of the talk.  For those running the <a href="https://asahilinux.org/fedora/">Fedora Asahi Remix</a> distribution,
she suggested immediately updating to pick up the pieces that
she had described.  Before taking questions, she launched Steam, which took
some time to come up, in part because of the virtual machine and the x86
emulation.  Once it came up, she launched Control, which ran at 45fps on an
M1 MAX system.
</p>

<p>
There was a question about resources from someone who has a Mac with 8GB of
RAM.  Rosenzweig said that the high-end gaming titles are only likely to
work on systems with 16GB or more.  She noted that she was playing Castle
Crashers on an 8GB system during the conference, so some games will play;
Portal will also work on that system.  She hopes that the resources
required will drop over time.
</p>

<p>
Another question was about <a href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)">ray-tracing</a>
support, since Control can use that <a href="https://www.khronos.org/blog/ray-tracing-in-vulkan">feature</a>.
Rosenzweig suggested that patches were welcome but that she did not see
that as a high priority (&#34;<q>frankly, I think ray tracing is a bit of a
gimmick feature</q>&#34;).  Apple hardware only supports it with the M3 and
the current driver is for M1 and M2 GPUs, though she plans to start working
on M3 before long.  The session concluded soon after that, though
Rosenzweig played Control, admittedly poorly, as time ran down.
</p>

<p>
[ I would like to thank LWN&#39;s travel sponsor, the Linux Foundation, for
travel assistance to Montreal for XDC. ]
</p></div></div>
  </body>
</html>
