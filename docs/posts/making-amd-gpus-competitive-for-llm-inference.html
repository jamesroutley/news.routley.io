<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference">Original</a>
    <h1>Making AMD GPUs competitive for LLM inference</h1>
    
    <div id="readability-page-1" class="page"><div>
<div>
  <div>
    <div>
      <p>
        <time datetime="2023-08-09T13:30:00+00:00" itemprop="datePublished">
          Aug 9, 2023
        </time>
        
        • <span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <span itemprop="name">MLC Community</span>
        </span>
        
      </p>
      
    <!-- |                  | AMD Radeon™ RX 7900 XTX | NVIDIA ® GeForce RTX™ 4090 | NVIDIA ® GeForce RTX™ 3090 Ti |
|:----------------:|:-----------------------:|:--------------------------:|:-----------------------------:|
|       Cores      |  6144 stream processors |      16384 CUDA cores      |        10752 CUDA cores       |
|      Memory      |        24GB GDDR6       |         24GB GDDR6X        |          24GB GDDR6X          |
|  FP16 Performance|         123 TFLOPS      |         330 TFLOPS         |          160 TFLOPS           |
| Memory Bandwidth |         960 GB/s        |          1008 GB/s         |           1008 GB/s           |
|        TDP       |           320W          |            450W            |              450W             |
|       Price      |           999$          |            1599$           |             1999$             | -->


















<!-- MLC-LLM leverages machine learning compilation, an emerging technology that compiles and automates
optimization of machine learning programs. Specifically, we build a solution on Apache TVM unity, a deep-learning compiler that utilizes a unified IR to represent the DL model at both graph and operator levels throughout the compilation stages. It allows *customizable model construction, composable model transformation, and transferable kernel optimization* for ML engineers to effectively customize and reuse ML compilation pipelines, reducing the effort of repeatedly implementing the same mechanism for different models or backends. TVM Unity also implements universal deployment runtime that enables developers to deploy the solution to the programming language and platform of their choice.

What makes TVM Unity different and even more productive is the Python-first development flow, where we can

* Inspect and modify the computational graph in Python
* Compose IR transformations in Python
* Inspect and write self-defined operators in Python, and compile them with other pre-defined operators composable in the same computational graph
* Write the kernel optimization generically in Python and the compiler generates shader language codes for different backends accordingly, which allows us to transfer the kernel optimization techniques across backends

We are leveraging the Python-first development, and universal deployment solution to quickly enable high-performance AMD GPU
support less than one human week's effort. -->













<!-- |             | AMD Radeon™ RX 7900 XTX | NVIDIA ® GeForce RTX™ 4090 | NVIDIA ® GeForce RTX™ 3090 Ti |
|:-----------:|:-----------------------:|:--------------------------:|:-----------------------------:|
|  Llama 2 7B |       130.9 toks/s      |        159.4 toks/s        |          138.5 toks/s         |
| Llama 2 13B |       74.7 toks/s       |         90.7 toks/s        |          80.3 toks/s          | -->



















<!-- ```
(deck@steamdeck mlc-llm)$ ./build/mlc_chat_cli --local-id Llama-2-7b-chat-hf-q4f16_1
Use MLC config: "/home/deck/mlc-llm/dist/Llama-2-7b-chat-hf-q4f16_1/params/mlc-chat-config.json"
Use model weights: "/home/deck/mlc-llm/dist/Llama-2-7b-chat-hf-q4f16_1/params/ndarray-cache.json"
Use model library: "/home/deck/mlc-llm/dist/Llama-2-7b-chat-hf-q4f16_1/Llama-2-7b-chat-hf-q4f16_1-Vulkan.so"
You can use the following special commands:
  /help               print the special commands
  /exit               quit the cli
  /stats              print out the latest stats (token/sec)
  /reset              restart a fresh chat
  /reload [local_id]  reload model `local_id` from disk, or reload the current model if `local_id` is not specified

Loading model...
Loading finished
Running system prompts...
System prompts finished
[INST]: Hi
[/INST]: Hello! I'm here to help you with any questions or concerns you may have. However, I must inform you that I cannot provide advice or suggestions that promote or facilitate harmful or illegal activities. It is important to always act in a safe and responsible manner, and to respect the laws and well-being of yourself and others. Is there anything else I can help you with?
[INST]: /stats
prefill: 48.3 tok/s, decode: 13.2 tok/s
``` -->






















    </div>
  </div>
</div>
</div></div>
  </body>
</html>
