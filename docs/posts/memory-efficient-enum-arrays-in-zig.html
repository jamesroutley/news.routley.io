<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alic.dev/blog/dense-enums">Original</a>
    <h1>Memory-efficient enum arrays in Zig</h1>
    
    <div id="readability-page-1" class="page"><div id="blog-content"><p>
					
					Enums (or tagged unions) whose variants vary in size are prone to significant 
					memory fragmentation in Rust. That&#39;s because we need to allocate enough 
					data to accommodate the largest variant.

					</p><figure>
						<div>
						<p><b>Figure 1:</b> Consider the following enum:
</p><div>
<pre>pub enum Foo { 
  A(u8), 
  B(u16),
  C(u32),
  D(u64),
}

</pre></div><p>
						Because of the space needed for tagging and alignment, this type is 16 bytes long.
						</p></div>
						<p><img alt="A visualization of an enum with variants of different sizes, and their respective memory fragmentation." src="https://alic.dev/static/dense-enums/img1.webp"/>
						</p>
					</figure>

					<p>
					This presents real pain when collecting a large number of them into a 
					<span>Vec</span> or <span>HashMap</span>. 
					The padding can be dealt with using some form of 
					<i>struct of arrays</i> (SoA)
					transformation that stores the tag in a separate allocation. 
					However, reducing the variant fragmentation is not so trivial. 
					</p>

					<p>
					You could hand-roll specialized data structures for <i>a particular enum</i>
					that reduce fragmentation to a minimum; but doing this generically 
					for an arbitrary enum with maximum memory efficiency is close to 
					impossible in Rust. The only options we have are proc-macros, which 
					compose poorly (no <span>#[derive]</span> on third-party code or type aliases) 
					and are not type aware (unless using workarounds based on 
					<span>generic_const_expr</span>, which infect the call graph with verbose trait 
					bounds and don&#39;t work with generic type parameters). Zig on the 
					other hand let&#39;s us perform the wildest data structure transformations 
					in a generic and concise way. 
					</p>

					<p>
					Before I go into the implementation details, I&#39;d like to explain why 
					reducing the aforementioned memory fragmentation is useful in practice. 
					</p>

					<p>Background</p>
					<p>
					To me, one of the biggest motivators for efficient enum arrays 
					has been compilers. One problem that keeps coming up when designing 
					an AST is figuring out how to reduce its memory footprint. Big ASTs 
					can incur a hefty performance penalty during compilation, because 
					memory bandwidth and latency are a frequent bottleneck in compiler 
					frontends. Chandler Carruth&#39;s 
					<a href="https://www.youtube.com/watch?v=ZI198eFghJk">video on the Carbon compiler</a> has been 
					making the rounds on language forums. In it he describes how a 
					parsed clang AST regularly consumes 50x more memory than the 
					original source code!
					</p>

					<p>
					Alright, so what does this have to do with enums? Well, the 
					most common way of representing syntax tree nodes is via some kind 
					of recursive (or recursive-like) data structure. Let&#39;s define a node 
					for expressions in Rust, using newtype indices for indirection:
					</p>
					<div>
<div>
<pre>enum Expr {
    Unit,
    Number,
    Binary(Operation, ExprId, ExprId),
    Ident(Symbol),
    Eval(ExprId, ExprSlice),
    BlockExpression(ExprId, StatementSlice)
}
</pre></div></div>

					<div>
					<p><b>Note:</b> We can write an AST node in OCaml for comparison:
</p><div>
<pre>type expr = 
  | Unit
  | Number
  | Binary of op * expr * expr
  | Ident of symbol
  | Eval of expr * stmt list

</pre></div><p>
					A big difference compared to Rust is that we can express truly 
					recursive data types without any form of explicit indirection. 
					That&#39;s because the runtime system and garbage collector take care 
					of the memory bookkeeping for us.
					</p></div><p>

					The problem we have now is that we want to improve the <i>packing efficiency</i>
					of those enums. A simple <span>Vec(Expr)</span> will consume <span>sizeof(Enum)</span> 
					amount of memory for every element, which corresponds to the size of 
					the largest variant + tag + padding. Luckily, there are some ways 
					of dealing with this.

					</p><p>Reducing Fragmentation</p><p>
					Let&#39;s take a simple example of a 3-variant enum with member sizes 
					8, 16 and 32 bits. Storing those in a regular <span>Vec</span> will look like this:

					</p><figure>
						<p><b>Figure 2:</b> 
						Here every element reserves a large amount of space to accommodate the 32-bit variant and to satisfy its alignment. 
						</p>
						<p><img alt="A visualization of an array of enum values, with varying fragmentation levels per element" src="https://alic.dev/static/dense-enums/layout1.webp"/>
						</p>
					</figure>

					<p>
					The most common way to improve packing efficiency is by just keeping the enum variants as small as possible using tagged indices (*). 
					</p><p>
					(*): For examples in Rust, take a look at the <a href="https://doc.rust-lang.org/stable/nightly-rustc/rustc_data_structures/tagged_ptr/index.html#">tagged_index crate</a> used in the compiler 
					or check out this recent <a href="https://mcyoung.xyz/2023/08/09/yarns/">blog post on small-string optimization</a>. 
					You&#39;ll find these optimizations all the time in high-performance code 
					like language runtimes, garbage collectors, compilers, game engines or OS kernels.
					</p>
					

					<p>
					Unfortunately, that doesn&#39;t completely solve the fragmentation issue. The other way is to tackle the container type directly! We could use a struct-of-arrays approach to store discriminant and value in two separate allocations. In fact, that&#39;s what the self-hosted Zig compiler actually does.
					</p>

					<figure>
						<p><b>Figure 3:</b> 
						The tags and union values are stored in two separate allocations, so we&#39;re not paying for padding anymore. However, the union collection still has variant fragmentation. 
						</p>
						<p><img alt="A struct-of-arrays transformation of an enum array" src="https://alic.dev/static/dense-enums/layout2.webp"/>
						</p>
					</figure>

					<p>
					Because of Zig&#39;s staged compilation, we can have container types that 
					perform this SoA transformation generically for any type. In Rust, 
					we&#39;re constrained to proc-macros like <span>soa_derive</span> 
					which has several downsides (e.g. we can&#39;t place <span>#[derive]</span> 
					on third-party types without changing their source).
					</p>

					<p>Reducing Variant Fragmentation</p><p>
					This SoA transformation reduces a lot of wasted padding introduced by 
					the enum tag, but still isn&#39;t optimal. To really get rid of fragmentation 
					in the values, we can create one vector <i>per variant</i>. 

					</p><figure>
						<p><b>Figure 4:</b> 
						Compared to the SoA layout from before, we have a partial order instead of a total order. So upon insertion, we get back a tagged index that holds both the enum tag and the index in the particular variant array. 
						</p>
						<p><img alt="Visualization of a simple AoVA layout" src="https://alic.dev/static/dense-enums/layout6.webp"/>
						</p>
					</figure><p>

					I don&#39;t think there&#39;s a name for this collection, so I call it <i>array of variant arrays</i> (or AoVA). This can be implemented in Rust and Zig, using proc-macros and comptime respectively.

					</p><p>Size Equivalence Classes</p>
					<p>
					We could stop here, but let&#39;s consider enums that have <i>lots</i> of variants that can be grouped into a small number of clusters with the same type size:
					</p><div>
<div>
<pre>enum Foo {
    A(u8, u8),
    B(u16),
    C(u16),
    D([u8; 2]),
    E(u32),
    F(u16, u16),
    G(u32),
    H(u32),
    I([u8; 4]),
    J(u32, u32),
    K(u32, (u16, u16)),
    L(u64),
    M(u64),
    N(u32, u16, u16),
    O([u8; 8])
}
</pre></div>
					<figure>
						<p><img alt="Naive AoVA layout causes us to create 15 different vectors - one per enum variant" src="https://alic.dev/static/dense-enums/layout15.webp"/>
						</p>
					</figure>
					</div>

					<p>
					As you can see, the one-vec-per-variant approach would add 15 vectors. 
					It&#39;s likely that the number of (re)allocations and system calls would
					increase substantially, and require a lot of memory to amortize
					compared to the naive <span>Vec<foo></foo></span>. 
					The vectors may also be arbitrarily spread in memory, 
					leading to a higher chance of cache conflicts. The AoVA collection itself
					also consumes a lot of memory, bloating any structure it&#39;s embedded in. 
					</p>

					<p>
					Now, if we group every variant by size, we get three clusters: 2, 4, and 8 bytes. Such clusters can be allocated together into the same vector - thereby reducing the number of total vectors we have in our container by 80%. So we could realistically store variants of <span>Foo</span> in three clusters:
					</p>
					<div>
<div>
<pre>struct FooVec {
    c_2: Vec&lt;[u8; 2]&gt;, // A - D
    c_4: Vec&lt;[u8; 4]&gt;, // E - I
    c_8: Vec&lt;[u8; 8]&gt;, // J - O
}
</pre></div>
					<figure>
						<p><img alt="The dense AoVA version reduces our vector count to 3" src="https://alic.dev/static/dense-enums/layout5.webp"/>
						</p>
					</figure>
					</div>

					<p>
					You could say this is a <i>dense</i> version of our AoVA pattern. 
					However, once we colocate different variants in the <i>same allocation</i>,
					we lose the ability to iterate through the vector in a type-safe way. 
					The only way to access elements in such a container is via the tagged 
					pointer that was created upon insertion. If your access pattern does 
					not require blind iteration (which can be the case for flattened, 
					index-based tree structures), this might be a worthwhile trade-off.  
					</p>
					
					<p>
					I&#39;ve implemented a <a href="https://github.com/dist1ll/osmium">prototype of this data structure in Zig</a>. 
					The most important pieces are the compiler built-ins that allow reflection on
					field types, byte and bit sizes, as well as inspecting the discriminant.
					</p>

					<figure>
					<p><b>Snippet:</b> At its core, it performs straightforward compile-time 
					reflection to compute the clusters and field-to-cluster mappings. We do
					pseudo-dynamic allocation using a stack-allocated vector. 
					The cluster information is used to construct the AoVA data structure.  
					Exact <a href="https://github.com/dist1ll/osmium/blob/127145584fb22b48c05b5dbb14c670001318a81c/src/osmium.zig#L54-L71">source of the snippet is here</a>.
					</p>
<div>
<pre>// determine kind of type (i.e. struct, union, etc.)
switch (@typeInfo(inner)) {
    .Union =&gt; |u| {
        // store mapping from union field -&gt; cluster index
        var field_map = [_]u8{0} ** u.fields.len;

        // iterate over union fields
        for (u.fields, 0..) |field, idx| {
            // compute size
            const space = @max(field.alignment, @sizeOf(field.type));

            // insert into hashtable 
            if (!svec.contains_slow(space)) {
                svec.push(space) catch @compileError(ERR_01);
            }

            field_map[idx] = svec.len - 1;
        }

        // return clusters
        return .{ .field_map = field_map, .sizes = svec };
    },
    else =&gt; @compileError(&#34;only unions allowed&#34;),
}
</pre></div>
					</figure>

					<p>
					If you do want type-safe iteration, you could pay the cost of padding, and add the tag back in:
					</p>

					<figure>
						<p><b>Figure 5:</b> 
						We&#39;ve essentially partitioned the enum on the <i>data-level</i>, 
						leaving the interpretation at the type-level untouched
						</p>
						<p><img alt="Dense AoVA layout with additional tag and padding" src="https://alic.dev/static/dense-enums/layout3.webp"/>
						</p>
					</figure>

					<p>
					If the padding is too much, you can do an SoA transformation on each of the variant arrays.
					</p>

					<figure>
						<p><b>Figure 6:</b> Here we have a similar partitioning, but without
						the padding. The downside is that we&#39;re doubling the vector count.
						</p>
						<p><img alt="Dense AoVA layout, with tag and per-cluster SoA transformation" src="https://alic.dev/static/dense-enums/layout4.webp"/>
						</p>
					</figure>

					<p>
					So as you can see, there&#39;s quite a few trade-offs we can make in this space - 
					and they all depend on the concrete memory layout of our enum. 
					</p>

					<p>
					While creating such data structures is pretty straightforward in Zig, 
					creating any of these examples in Rust using proc macros is basically 
					impossible - the reason being that proc macros don&#39;t have access to 
					type information like size or alignment. While you could have a proc 
					macro generate a <span>const fn</span> that computes the clusters for a particular 
					enum, this function cannot be used to specify the length of an array 
					for a <i>generic</i> type.
					</p>

					<p>
					Another limit to Rust&#39;s generics is that the implementation of a 
					generic container cannot be conditioned on whether the given type is 
					an enum or a struct. 
					In Zig, we can effectively do something like this:
					</p>

					<figure>
<div>
<pre>// this is pseudocode
struct EfficientContainer<t> {
    if(T.isEnum()) {
        x: EfficientStructArray<t>,
    } else {
        x: EfficientEnumArray<t>,
    }
}
</t></t></t></pre></div>
					</figure>
					<p>
					We can also specialize the flavor of our AoVA implementation based on the enum. Maybe the benefits of colocating different variants only starts to make sense if we reduce the number of vectors by more than 90%. 
					</p>
					
					<p>
					So ultimately we gain a lot of fine-grained control over data structure selection. And if we have good heuristics, we can let the type-aware staging mechanism select the best implementation for us. To me, this represents a huge step in composability for high-performance systems software. 
					</p>

					<p>Bonus: Determining Index Bitwidth at Compile Time</p>
					<p>
					There&#39;s another pretty crazy feature that Zig&#39;s staged compilation 
					model gets you: if you know the maximum capacity of your data structure 
					at compile time, you can pass that information to the type-constructing function
					and let it determine the bitwidth of the returned tagged index.
					</p>

					<p>
					When this tagged index is included in a subsequent data structure, let&#39;s say another enum, this information carries over naturally, and the bits that we didn&#39;t need can be used for the discriminant!
					</p>

					<p>
					So what Zig gives you is <i>composable</i> memory efficiency. By being 
					specific about the number of bits you need, different parts of the code 
					can take advantage of that. And with implicit widening integer coercion, 
					dealing with APIs of different bitwidths stays ergonomic. In a way, 
					this reminds me a lot of refinement typing and ranged integers, so this 
					ties in a lot with my <a href="https://alic.dev/blog/custom-bitwidth">post on custom bitwidth integers</a>. 
					</p>
					
					<p>Conclusion</p><p>
					Writing extremely efficient generic data structures in Rust is not 
					always easy - in some cases they incur lots of accidental complexity, 
					in some others they&#39;re essentially impossible to implement. I think 
					one of the biggest takeaways for me with regards to staged compilation 
					was the ability to be composable on a memory layout level. If you&#39;re 
					developing a systems programming language that embraces efficiency and 
					zero-cost abstractions, you should absolutely take another look at 
					staged programming and in particular Zig&#39;s comptime.
					</p><!-- +++++++++ -->
				</div></div>
  </body>
</html>
