<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.exe.dev/expensively-quadratic">Original</a>
    <h1>Expensively Quadratic: The LLM Agent Cost Curve</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      <article>
<p>Pop quiz: at what point in the context length of a coding agent are cached
reads costing you half of the next API call? By 50,000 tokens, your
conversation’s costs are probably being dominated by cache reads.</p>
<p>Let’s take a step back. We’ve <a href="https://sketch.dev/blog/agent-loop">previously
written</a> about how coding agents work:
they post the conversation thus far to the LLM, and continue doing that in
a loop as long as the LLM is requesting tool calls. When there are no
more tools to run, the loop waits for user input, and the whole cycle starts
over. Visually:</p>

<p>Or, in code form:</p>
<pre><code>def loop(llm):
    msg = user_input()
    while True:
        output, tool_calls = llm(msg)
        print(&#34;Agent: &#34;, output)
        if tool_calls:
            msg = [handle_tool_call(tc)
                   for tc in tool_calls]
        else:
            msg = user_input()
</code></pre><p>The LLM providers charge for input tokens, cache writes, output tokens, and cache reads.
It&#39;s a little tricky: you indicate in your prompt to cache up to a
certain point (usually the end), and you get charged as “cache write” and not input.
The previous turn&#39;s output becomes the next turn&#39;s cache write. Visually:</p>

<p>Here, the colors and numbers indicate the costs making up the <em>n</em>th call to the
LLM. Every subsequent call reads the story so far from the cache, writes the
previous call’s output to the cache (as well as any new input), and gets an
output. The area represents the cost, though in this diagram, it&#39;s not quite
drawn to scale. Add up all the rectangles, and that&#39;s the total cost.</p>
<p>That triangle emerging for cache reads? That&#39;s the scary quadratic!</p>
<p>How scary is the quadratic? Pretty squarey! I took a rather ho-hum feature
implementation conversation, and visualized it like the diagram above. The area
corresponds to cost: the width of every rectangle is the number of tokens and
the height is the cost per token. As the conversation evolves, more and more
of the cost is the long thin lines across the bottom that correspond to cache
reads.</p>






<p>The whole conversation cost $12.93 total or so. You can see that as the
conversation continues, the cache reads dominate. At the end of the
conversation, cache reads are 87% of the total cost. They were half the cost at
27,500 tokens!</p>


<p>This conversation is just one example. Does this happen generally? exe.dev&#39;s
LLM gateway keeps track of the costs we&#39;re incurring. We do not store the
messages themselves going past, but we do keep track of the number of tokens. The
following graph shows the &#34;cumulative cost&#34; visualization for many Shelley
conversations, not just my own. I sampled 250 conversations from the data
randomly.</p>
<p>The x-axis is the context length, and the y-axis is the cumulative cost up to
that point. The left graph is all the costs and the right graph is just the
cache reads. You can mouse over to find a given conversation on both graphs.
The box plots below show the distribution of input tokens and output tokens.</p>


<p>The cost curves are all different because every conversation is different. Some
conversations write a lot of code, so spend more money on expensive output
tokens. Some conversations read lots of the code base, so spend money on tool
call outputs, which look like cache writes. Some conversations waited for the
user while the cache expired, so have to re-write data to the cache. In our
data, the median input was about 285 tokens and the median output was about
100, but the distribution is pretty wide.</p>
<p>Let&#39;s look at how some conversations got to 100,000 tokens. We sampled from the
same data set, but excluded short conversations under 20 calls and also
excluded conversations that didn&#39;t get to 100,000 tokens.
The number of LLM calls in the conversation matters quite a bit. The cache read
cost isn&#39;t really the number of tokens squared; it&#39;s the number of tokens times
the number of calls, and different conversations have very different numbers of
LLM calls!</p>


<p>To go back to our original question, we can build a little simulator.
Anthropic&#39;s rates are <em>x</em> for input, 1.25<em>x</em> for cache write, 5<em>x</em> for output,
and <em>x</em>/10 for cache read, where <em>x</em> = $5 per million tokens for Opus 4.5.  In
the default settings of the simulator, it only takes 20,000 tokens to get to
the point where cache reads dominate.</p>


</article>
      
    </div></div>
  </body>
</html>
