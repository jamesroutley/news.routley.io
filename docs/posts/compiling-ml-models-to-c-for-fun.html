<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bernsteinbear.com/blog/compiling-ml-models/">Original</a>
    <h1>Compiling ML models to C for fun</h1>
    
    <div id="readability-page-1" class="page"><div>
            <blockquote>
  <p><em>NOTE: This post is going to be a compiler post, not a machine learning
tutorial, so please treat it as such. Maybe it will still help you understand
ML through a compilers lens.</em></p>
</blockquote>

<p>I had a nice chat with my friend <a href="https://www.chrisgregory.me/">Chris</a>
recently.</p>

<p>He walked me through the basics of machine learning while I was looking at
<a href="https://karpathy.ai/">Andrej Karpathy</a>’s
<a href="https://github.com/karpathy/micrograd/">micrograd</a>.</p>

<p>If you are unfamiliar, micrograd is a very small implementation of a
scalar-valued neural network (as opposed to vectors or matrices as the
computational unit) in pure Python, which uses no libraries.</p>

<p>Micrograd is a combination of a couple of different and complementary parts:</p>

<ul>
  <li>a little graph-based expression builder and evaluator</li>
  <li>reverse-mode automatic differentiation on that same computation graph</li>
  <li>neural net building blocks for a multi-layer perceptron (MLP)</li>
</ul>

<p>(If you don’t know what a MLP is, don’t worry too much. This post should give
you a bit of background, especially if you are already comfortable with Python.
You may want to go through and read and think about the micrograd source code
before coming back. Or not! Your call. Playing with it helped me a lot. Chris
suggested trying to make a network learn XOR.)</p>

<p>Together, these three major components let you write code that looks like this:</p>

<div><div><pre><code><span>from</span> <span>micrograd.nn</span> <span>import</span> <span>MLP</span>
<span>model</span> <span>=</span> <span>MLP</span><span>(</span><span>2</span><span>,</span> <span>[</span><span>4</span><span>,</span> <span>1</span><span>])</span>
</code></pre></div></div>

<p>And summon a neural network from thin air.</p>

<p>The thing that got me the first time I read it was that I thought the building
blocks <em>were</em> the network. In this library, no. Using a building analogy, they
are more like blueprints or scaffolding. With each evaluation of the network,
the connective tissue (intermediate computation graph) is constructed anew. In
compiler terms, the building blocks are kind of like the front-end and the
expression graph is a sort of intermediate representation (IR).</p>

<p>You may be sitting there wondering why I am telling you this. I normally blog
about compilers. What’s this?</p>

<p>It’s because once I untangled and understood the three pieces in micrograd, I
realized:</p>

<ul>
  <li>ML models are graphs</li>
  <li>forward and backward passes are graph traversals</li>
  <li>the graph structure does not change over time</li>
  <li>performance is important</li>
</ul>

<p>Which means… it sounds like a great opportunity for a compiler! This is why
projects like PyTorch and TensorFlow have compilers
(TorchScript/TorchDynamo/AOT Autograd/PrimTorch/TorchInductor/Glow, XLA, etc).
Compiling your model speeds up both training and inference. So this post will
not contain anything novel—it’s hopefully a quick sketch of a small example
of what the Big Projects do.</p>

<p>We’re going to compile micrograd neural nets into C. In order, we will</p>

<ul>
  <li>do a brief overview of neural networks</li>
  <li>look at how micrograd does forward and backward passes</li>
  <li>review the chain rule</li>
  <li>learn why micrograd is slow</li>
  <li>write a small compiler</li>
  <li>see micrograd go zoom</li>
</ul>

<p>Let’s go!</p>

<h2 id="how-micrograd-does-neural-networks">How micrograd does neural networks</h2>

<p>First, a bit about multi-layer perceptrons. MLPs are densely connected neural
networks where input flows in one direction through the network. As it exists
in the upstream repository, micrograd only supports MLPs.</p>

<p>In case visual learning is your thing, here is a small diagram:</p>

<figure>
  
  <figcaption>Fig. 1 - Multi-layer Perceptron diagram. Well, a layer of one,
  anyway. I made this in Excalidraw. I love Excalidraw.</figcaption>
</figure>

<p>In this image, circles represent data (input or intermediate computation
results) and arrows are weights and operations on the data. In this case, the
<code>x</code>, <code>y</code>, and <code>z</code> circles are input data. The arrows going right are
multiplications with weights. The meeting of the arrows represents an addition
(forming a dot product) followed by addition of the bias (kind of like another
weight), all fed into an activation function (in this case <code>ReLU</code>, for
“rectified linear unit”)<sup id="fnref:activation" role="doc-noteref"><a href="#fn:activation" rel="footnote">1</a></sup>. The circles on the right are the results
of the first layer.</p>

<p>Karpathy implements this pretty directly, with each neuron being an instance of
the <code>Neuron</code> class and having a <code>__call__</code> method do the dot product. After
each dot product is an activation, in this case <code>ReLU</code>, which is equivalent to
<code>max(x, 0)</code>. I think the <code>0</code> is an arbitrary threshold but I am not certain.</p>

<p>Below is the entire blueprint code for a multilayer perceptron in micrograd
(we’ll come back to the <code>Value</code> class later):</p>

<div><div><pre><code><span>import</span> <span>random</span>
<span>from</span> <span>micrograd.engine</span> <span>import</span> <span>Value</span>

<span>class</span> <span>Module</span><span>:</span>

    <span>def</span> <span>zero_grad</span><span>(</span><span>self</span><span>):</span>
        <span>for</span> <span>p</span> <span>in</span> <span>self</span><span>.</span><span>parameters</span><span>():</span>
            <span>p</span><span>.</span><span>grad</span> <span>=</span> <span>0</span>

    <span>def</span> <span>parameters</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>[]</span>

<span>class</span> <span>Neuron</span><span>(</span><span>Module</span><span>):</span>

    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>nin</span><span>,</span> <span>nonlin</span><span>=</span><span>True</span><span>):</span>
        <span>self</span><span>.</span><span>w</span> <span>=</span> <span>[</span><span>Value</span><span>(</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>-</span><span>1</span><span>,</span><span>1</span><span>))</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>nin</span><span>)]</span>
        <span>self</span><span>.</span><span>b</span> <span>=</span> <span>Value</span><span>(</span><span>0</span><span>)</span>
        <span>self</span><span>.</span><span>nonlin</span> <span>=</span> <span>nonlin</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>act</span> <span>=</span> <span>sum</span><span>((</span><span>wi</span><span>*</span><span>xi</span> <span>for</span> <span>wi</span><span>,</span><span>xi</span> <span>in</span> <span>zip</span><span>(</span><span>self</span><span>.</span><span>w</span><span>,</span> <span>x</span><span>)),</span> <span>self</span><span>.</span><span>b</span><span>)</span>
        <span>return</span> <span>act</span><span>.</span><span>relu</span><span>()</span> <span>if</span> <span>self</span><span>.</span><span>nonlin</span> <span>else</span> <span>act</span>

    <span>def</span> <span>parameters</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>self</span><span>.</span><span>w</span> <span>+</span> <span>[</span><span>self</span><span>.</span><span>b</span><span>]</span>

    <span>def</span> <span>__repr__</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>f</span><span>&#34;</span><span>{</span><span>&#39;ReLU&#39;</span> <span>if</span> <span>self</span><span>.</span><span>nonlin</span> <span>else</span> <span>&#39;Linear&#39;</span><span>}</span><span>Neuron(</span><span>{</span><span>len</span><span>(</span><span>self</span><span>.</span><span>w</span><span>)</span><span>}</span><span>)&#34;</span>

<span>class</span> <span>Layer</span><span>(</span><span>Module</span><span>):</span>

    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>nin</span><span>,</span> <span>nout</span><span>,</span> <span>**</span><span>kwargs</span><span>):</span>
        <span>self</span><span>.</span><span>neurons</span> <span>=</span> <span>[</span><span>Neuron</span><span>(</span><span>nin</span><span>,</span> <span>**</span><span>kwargs</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>nout</span><span>)]</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>out</span> <span>=</span> <span>[</span><span>n</span><span>(</span><span>x</span><span>)</span> <span>for</span> <span>n</span> <span>in</span> <span>self</span><span>.</span><span>neurons</span><span>]</span>
        <span>return</span> <span>out</span><span>[</span><span>0</span><span>]</span> <span>if</span> <span>len</span><span>(</span><span>out</span><span>)</span> <span>==</span> <span>1</span> <span>else</span> <span>out</span>

    <span>def</span> <span>parameters</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>[</span><span>p</span> <span>for</span> <span>n</span> <span>in</span> <span>self</span><span>.</span><span>neurons</span> <span>for</span> <span>p</span> <span>in</span> <span>n</span><span>.</span><span>parameters</span><span>()]</span>

    <span>def</span> <span>__repr__</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>f</span><span>&#34;Layer of [</span><span>{</span><span>&#39;, &#39;</span><span>.</span><span>join</span><span>(</span><span>str</span><span>(</span><span>n</span><span>)</span> <span>for</span> <span>n</span> <span>in</span> <span>self</span><span>.</span><span>neurons</span><span>)</span><span>}</span><span>]&#34;</span>

<span>class</span> <span>MLP</span><span>(</span><span>Module</span><span>):</span>

    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>nin</span><span>,</span> <span>nouts</span><span>):</span>
        <span>sz</span> <span>=</span> <span>[</span><span>nin</span><span>]</span> <span>+</span> <span>nouts</span>
        <span>self</span><span>.</span><span>layers</span> <span>=</span> <span>[</span><span>Layer</span><span>(</span><span>sz</span><span>[</span><span>i</span><span>],</span> <span>sz</span><span>[</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>nonlin</span><span>=</span><span>i</span><span>!=</span><span>len</span><span>(</span><span>nouts</span><span>)</span><span>-</span><span>1</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>len</span><span>(</span><span>nouts</span><span>))]</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span><span>:</span>
            <span>x</span> <span>=</span> <span>layer</span><span>(</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span>

    <span>def</span> <span>parameters</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>[</span><span>p</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span> <span>for</span> <span>p</span> <span>in</span> <span>layer</span><span>.</span><span>parameters</span><span>()]</span>

    <span>def</span> <span>__repr__</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>f</span><span>&#34;MLP of [</span><span>{</span><span>&#39;, &#39;</span><span>.</span><span>join</span><span>(</span><span>str</span><span>(</span><span>layer</span><span>)</span> <span>for</span> <span>layer</span> <span>in</span> <span>self</span><span>.</span><span>layers</span><span>)</span><span>}</span><span>]&#34;</span>
</code></pre></div></div>

<p>You can ignore some of the clever coding in <code>MLP.__init__</code>. This ensures that
all of the layers match up end-to-end dimension-wise. It also ensures the last
layer is linear, meaning the neurons do not have an activation function
attached.</p>

<p>But this neural network is not built just with floating point numbers. Instead
Karpathy uses this <code>Value</code> thing. What’s that about?</p>

<h2 id="intro-to-the-expression-builder">Intro to the expression builder</h2>

<p>I said that one of micrograd’s three components is an expression graph builder.</p>

<p>Using the expression builder looks like a slightly more complicated way of
doing math in Python:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span><span> </span><span>from</span> <span>micrograd.engine</span> <span>import</span> <span>Value</span>
<span>&gt;&gt;&gt;</span><span> </span><span>a</span> <span>=</span> <span>Value</span><span>(</span><span>2</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>b</span> <span>=</span> <span>Value</span><span>(</span><span>3</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>c</span> <span>=</span> <span>Value</span><span>(</span><span>4</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>d</span> <span>=</span> <span>(</span><span>a</span> <span>+</span> <span>b</span><span>)</span> <span>*</span> <span>c</span>
<span>&gt;&gt;&gt;</span><span> </span><span>d</span>
<span>Value(data=20, grad=0)
</span><span>&gt;&gt;&gt;</span><span>
</span></code></pre></div></div>

<p>The <code>Value</code> class even implements all the operator methods like <code>__add__ </code> to
make the process painless and look as much like normal Python math as possible.</p>

<p>But it’s a little different than normal math. It’s different first because it
has this <code>grad</code> field—which we’ll talk more about later—but also because as
it does the math it also builds up an graph (you can kind of think of it as an
abstract syntax tree, or AST).</p>

<p>It’s not visible in the normal string representation, though. <code>Value</code> instances
have a hidden field called <code>_prev</code> that stores the constituent parts that make
up an expression:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span><span> </span><span>d</span><span>.</span><span>_prev</span>
<span>{Value(data=5, grad=0), Value(data=4, grad=0)}
</span><span>&gt;&gt;&gt;</span><span>
</span></code></pre></div></div>

<p>They also have a hidden operator field:</p>



<p>This means that we have two operands to the <code>*</code> node <code>d</code>: <code>c</code> (4) and <code>a + b</code>
(5).</p>

<p>I said you could think about it like an AST but it’s not quite an AST because
it’s not a tree. It’s expected and normal to have more of a directed acyclic
graph (DAG)-like structure.</p>

<div><div><pre><code><span>&gt;</span><span>&gt;&gt;</span> <span>w</span> <span>=</span> <span>Value</span><span>(</span><span>2</span><span>)</span>
<span>&gt;</span><span>&gt;&gt;</span> <span>x</span> <span>=</span> <span>1</span> <span>+</span> <span>w</span>
<span>&gt;</span><span>&gt;&gt;</span> <span>y</span> <span>=</span> <span>3</span> <span>*</span> <span>w</span>
<span>&gt;</span><span>&gt;&gt;</span> <span>z</span> <span>=</span> <span>x</span> <span>+</span> <span>y</span>
<span>&gt;</span><span>&gt;&gt;</span> <span>z</span>
<span>Value(data=9, grad=0)
</span><span>&gt;</span><span>&gt;&gt;</span>
</code></pre></div></div>

<p>Here <code>x</code> and <code>y</code> both use <code>w</code> and then are both used by <code>z</code>, forming a diamond
pattern.</p>

<figure>
<svg width="204pt" height="188pt" viewBox="0.00 0.00 203.57 188.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="#ffffff" stroke="transparent" points="-4,4 -4,-184 199.5659,-184 199.5659,4 -4,4"></polygon>
<!-- z -->
<g id="node1">
<title>z</title>
<ellipse fill="none" stroke="#000000" cx="98.7986" cy="-162" rx="40.9005" ry="18"></ellipse>
<text text-anchor="middle" x="98.7986" y="-157.8" font-family="Times,serif" font-size="14.00" fill="#000000">z = x+y</text>
</g>
<!-- x -->
<g id="node2">
<title>x</title>
<ellipse fill="none" stroke="#000000" cx="45.7986" cy="-90" rx="45.5981" ry="18"></ellipse>
<text text-anchor="middle" x="45.7986" y="-85.8" font-family="Times,serif" font-size="14.00" fill="#000000">x = 1+w </text>
</g>
<!-- z->x -->
<g id="edge1">
<title>z-&gt;x</title>
<path fill="none" stroke="#000000" d="M85.9688,-144.5708C79.5712,-135.8797 71.7098,-125.2001 64.6429,-115.5998"></path>
<polygon fill="#000000" stroke="#000000" points="67.4409,-113.4968 58.694,-107.5182 61.8035,-117.6465 67.4409,-113.4968"></polygon>
</g>
<!-- y -->
<g id="node3">
<title>y</title>
<ellipse fill="none" stroke="#000000" cx="152.7986" cy="-90" rx="42.5359" ry="18"></ellipse>
<text text-anchor="middle" x="152.7986" y="-85.8" font-family="Times,serif" font-size="14.00" fill="#000000">y = 3*w</text>
</g>
<!-- z->y -->
<g id="edge2">
<title>z-&gt;y</title>
<path fill="none" stroke="#000000" d="M111.8705,-144.5708C118.4371,-135.8153 126.5173,-125.0418 133.7586,-115.3867"></path>
<polygon fill="#000000" stroke="#000000" points="136.6471,-117.3687 139.8471,-107.2687 131.0471,-113.1687 136.6471,-117.3687"></polygon>
</g>
<!-- w -->
<g id="node4">
<title>w</title>
<ellipse fill="none" stroke="#000000" cx="98.7986" cy="-18" rx="33.2211" ry="18"></ellipse>
<text text-anchor="middle" x="98.7986" y="-13.8" font-family="Times,serif" font-size="14.00" fill="#000000">w = 2</text>
</g>
<!-- x->w -->
<g id="edge4">
<title>x-&gt;w</title>
<path fill="none" stroke="#000000" d="M58.6284,-72.5708C65.1682,-63.6866 73.2376,-52.7245 80.4242,-42.9615"></path>
<polygon fill="#000000" stroke="#000000" points="83.3424,-44.9012 86.4519,-34.7729 77.705,-40.7514 83.3424,-44.9012"></polygon>
</g>
<!-- y->w -->
<g id="edge3">
<title>y-&gt;w</title>
<path fill="none" stroke="#000000" d="M139.7267,-72.5708C133.0636,-63.6866 124.842,-52.7245 117.5197,-42.9615"></path>
<polygon fill="#000000" stroke="#000000" points="120.1783,-40.6729 111.3783,-34.7729 114.5783,-44.8729 120.1783,-40.6729"></polygon>
</g>
</g>
</svg>
  <figcaption>Fig. 2 - A dependency graph that has diamond-shaped dependencies
  in it, making it a directed graph instead of a tree.</figcaption>
</figure>

<p>It is assumed that the graph won’t have cycles in it<sup id="fnref:rnn-unrolling" role="doc-noteref"><a href="#fn:rnn-unrolling" rel="footnote">2</a></sup>.</p>

<p>So what does creating the graph look like in code? well, the <code>Value.__mul__</code>
function, called on the left hand side of an <code>x*y</code> operation<sup id="fnref:binop" role="doc-noteref"><a href="#fn:binop" rel="footnote">3</a></sup>, looks
like this:</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>__mul__</span><span>(</span><span>self</span><span>,</span> <span>other</span><span>):</span>
        <span># create a transient value if the right hand side is a constant int or
</span>        <span># float, like v * 3
</span>        <span>other</span> <span>=</span> <span>other</span> <span>if</span> <span>isinstance</span><span>(</span><span>other</span><span>,</span> <span>Value</span><span>)</span> <span>else</span> <span>Value</span><span>(</span><span>other</span><span>)</span>
        <span># pass in new data, children, and operation
</span>        <span>out</span> <span>=</span> <span>Value</span><span>(</span><span>self</span><span>.</span><span>data</span> <span>*</span> <span>other</span><span>.</span><span>data</span><span>,</span> <span>(</span><span>self</span><span>,</span> <span>other</span><span>),</span> <span>&#39;*&#39;</span><span>)</span>
        <span># ... we&#39;ll come back to this hidden part later ...
</span>        <span>return</span> <span>out</span>
</code></pre></div></div>

<p>The <code>children</code> tuple <code>(self, other)</code> are the pointers to the other nodes in the
graph.</p>

<p>But why do we have these expression graphs? Why not just use math? Who
cares about all the back pointers?</p>

<h2 id="lets-talk-about-gradient">Let’s talk about grad(ient)</h2>

<p>Training a neural network is a process of shaping your function (the neural
network) over time to output the results you want. Inside your function are a
bunch of coefficients (“weights”) which get iteratively adjusted during
training.</p>

<p>The standard training process involves your neural network structure and also
another function that tells you how far off your output is from some expected
value (a “loss function”). A simple example of a loss function is
<code>loss(actual, expected) = (expected - actual)**2</code> (where <code>**</code> is exponentiation
in Python). If you use this particular function across multiple inputs at a
time, it’s called Mean Squared Error (MSE)<sup id="fnref:other-loss" role="doc-noteref"><a href="#fn:other-loss" rel="footnote">4</a></sup>.</p>

<p>If you are trying to get some expected output, you want to minimize the value
of your loss function as much as possible. In order to minimze your loss, you
have to update the weights.</p>

<p>To figure out which weights to update and by how much, you need to know how
much each weight contributes to the final loss. Not every weight is equal; some
have significantly more impact than others.</p>

<p>The question “how much did this weight contribute to the loss this round” is
answered by the value of the grad (gradient) of that weight—the first
derivative—the slope at a point. For example, in <code>y = mx + b</code>, the equation
that describes a line, the derivative with respect to <code>x</code> is <code>m</code>, because the
value of <code>x</code> is scaled by <code>m</code> (and <code>b</code> is a constant).</p>

<p>To compute the grad, you need to traverse backwards from the loss<sup id="fnref:forward" role="doc-noteref"><a href="#fn:forward" rel="footnote">5</a></sup> to
do something called reverse mode automatic differentiation (reverse mode AD).
This sounds scary. Every article online about it has scary notation and
squiggly lines. But it’s pretty okay, actually, so keep on reading.</p>

<p>Fortunately for us, reverse mode AD, like evaluating an AST top to bottom, it
is a graph traversal with some local state. If you can write a tree-walking
interpreter, you can do reverse mode automatic differentiation.</p>

<h3 id="reverse-mode-ad-and-backpropagation">Reverse mode AD and backpropagation</h3>

<p>Instead of building up a parallel graph of derivatives (a sort of “dual” to the
normal expression graph), reverse mode AD computes local derivatives at each
node in the <code>grad</code> (gradient) field. Then you can propagate these gradients
backward through the graph from the loss all the way to the
weights—backpropagation.</p>

<p>But how do you compose all those local derivatives? There’s no way it’s simple,
right? Taking derivatives of big math expressions is scary…</p>

<p>It turns out, calculus already has the answer in something called <em>the chain
rule</em>.</p>

<h2 id="the-chain-rule">The chain rule</h2>

<p>I am not going to pretend that I am a math person. Aside from what I re-learned
in the last couple of weeks, I only vaguely remember the chain rule from 10
years ago. Most of what I remember is my friend Julia figuring it out
instantaneously and wondering why I didn’t get it yet. That’s about it. So
please look elsewhere for details if this section doesn’t do it for you. I
won’t be offended.</p>

<h3 id="a-quick-overview">A quick overview</h3>

<p>The chain rule tells you how to compute derivatives of function composition.
Using the example from Wikipedia, if you have some function <code>h(x) = f(g(x))</code>,
then <code>h&#39;(x) = f&#39;(g(x)) * g&#39;(x)</code> (where <code>f&#39;</code> and <code>h&#39;</code> and <code>g&#39;</code> are the
derivatives of <code>f</code> and <code>h</code> and <code>g</code>, respectively). This rule is nice, because
you don’t need to do anything tricky when you start composing functions, as
long as you understand how to take the derivative of each of the component
parts.</p>

<p>For example, if you have <code>sin(x**2)</code>, you only need to know the derivative of
the component functions <code>x**2</code> (it’s <code>2*x</code>) and <code>sin(x)</code> (it’s <code>cos(x)</code>) to
find out the answer: <code>cos(x**2) * 2x</code>.</p>

<p>To take a look at the proof of this and also practice a bit, take a look at
<a href="https://web.auburn.edu/holmerr/1617/Textbook/chainrule-screen.pdf">this short slide
deck</a> (PDF)
from Auburn University. Their course page <a href="https://web.auburn.edu/holmerr/1617/Textbook/contents.aspx">table of
contents</a> has more
slide decks<sup id="fnref:pdf-nav" role="doc-noteref"><a href="#fn:pdf-nav" rel="footnote">6</a></sup>.</p>

<p>Also make sure to check out the <a href="https://en.wikipedia.org/wiki/Differentiation_rules">list of differentiation
rules</a> on Wikipedia.</p>

<p>It turns out that the chain rule comes in handy for taking derivatives of
potentially enormous expression graphs. Nobody needs to sit down and work out
how to take the derivative of your huge and no doubt overly complex function…
you just have your building blocks that you already understand, and they are
composed.</p>

<p>So let’s apply the chain rule to expression graphs.</p>

<h3 id="applying-this-to-the-graph">Applying this to the graph</h3>

<p>We’ll start with one <code>Value</code> node at a time. For a given node, we can do one
step of the chain rule (in pseudocode):</p>

<div><div><pre><code><span># pseudocode
</span><span>def</span> <span>backward</span><span>(</span><span>node</span><span>):</span>
    <span>for</span> <span>child</span> <span>in</span> <span>node</span><span>.</span><span>_prev</span><span>:</span>
        <span>child</span><span>.</span><span>grad</span> <span>+=</span> <span>derivative_wrt_child</span><span>(</span><span>child</span><span>)</span> <span>*</span> <span>node</span><span>.</span><span>grad</span>
</code></pre></div></div>

<p>Where <code>wrt</code> means “with respect to”. It’s important that we take the derivative
of each child <em>with respect to the child</em>.</p>

<p>Instead of just setting <code>child.grad</code>, we are increasing it for two reasons:</p>

<ul>
  <li>one child may be shared with other parents, in which case it affects both</li>
  <li>batching, but that’s not important right now</li>
</ul>

<p>To make this more concrete, let’s take a look at Karpathy’s implementation of
the derivative of <code>*</code>, for example. In math, if you have <code>f(x,y) = x*y</code>, then
<code>f&#39;(x, y) = 1*y</code> (with respect to <code>x</code>) and <code>f&#39;(x, y) = x*1</code> (with respect to
<code>y</code>). In code, that looks like:</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>__mul__</span><span>(</span><span>self</span><span>,</span> <span>other</span><span>):</span>
        <span>other</span> <span>=</span> <span>other</span> <span>if</span> <span>isinstance</span><span>(</span><span>other</span><span>,</span> <span>Value</span><span>)</span> <span>else</span> <span>Value</span><span>(</span><span>other</span><span>)</span>
        <span>out</span> <span>=</span> <span>Value</span><span>(</span><span>self</span><span>.</span><span>data</span> <span>*</span> <span>other</span><span>.</span><span>data</span><span>,</span> <span>(</span><span>self</span><span>,</span> <span>other</span><span>),</span> <span>&#39;*&#39;</span><span>)</span>

        <span># The missing snippet from earlier!
</span>        <span>def</span> <span>_backward</span><span>():</span>
            <span>self</span><span>.</span><span>grad</span> <span>+=</span> <span>other</span><span>.</span><span>data</span> <span>*</span> <span>out</span><span>.</span><span>grad</span>
            <span>other</span><span>.</span><span>grad</span> <span>+=</span> <span>self</span><span>.</span><span>data</span> <span>*</span> <span>out</span><span>.</span><span>grad</span>
        <span>out</span><span>.</span><span>_backward</span> <span>=</span> <span>_backward</span>

        <span>return</span> <span>out</span>
</code></pre></div></div>

<p>This means that for each of the children, we will use the <em>other child</em>’s data
and (because of the chain rule) multiply it by the parent expression’s <code>grad</code>.
That is, <code>self</code>’s grad (the left hand side) is adjusted using <code>other</code>’s data
(the right hand side) and vice versa. See what a nice translation of the math
that is? Get the derivative, apply the chain rule, add to the child’s grad.</p>

<p>Now we have a function to do one derivative step for one operation node, but we
need to do the whole graph.</p>

<p>But traversing a graph is not as simple as traversing a tree. You need to avoid
visiting a node more than once and also guarantee that you visit child nodes
before parent nodes (in forward mode) or parent nodes before children nodes (in
reverse mode). The tricky thing is that while we don’t <em>visit</em> a node more than
once, visiting updates the node’s children (not the node itself), and nodes may
share children, so children’s <code>grad</code>s may be updated multiple times. This is
expected and normal!</p>

<p>For that reason, we have topological sort.</p>

<h2 id="topological-sort-and-graph-transformations">Topological sort and graph transformations</h2>

<p>A topological sort on a graph is an order where children are always visited
before their parents. In general this only works if the graph does not have
cycles, but—thankfully—we already assume above that the graph does not have
cycles.</p>

<p>Here is a sample topological sort on the <code>Value</code> graph. It uses the nested
function <code>build_topo</code> for terseness, but that is not strictly necessary.</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>topo</span><span>(</span><span>self</span><span>):</span>
        <span># modified from Value.backward, which builds a topological sort
</span>        <span># internally
</span>        <span>topo</span> <span>=</span> <span>[]</span>
        <span>visited</span> <span>=</span> <span>set</span><span>()</span>
        <span>def</span> <span>build_topo</span><span>(</span><span>v</span><span>):</span>
            <span>if</span> <span>v</span> <span>not</span> <span>in</span> <span>visited</span><span>:</span>
                <span>visited</span><span>.</span><span>add</span><span>(</span><span>v</span><span>)</span>
                <span>for</span> <span>child</span> <span>in</span> <span>v</span><span>.</span><span>_prev</span><span>:</span>
                    <span>build_topo</span><span>(</span><span>child</span><span>)</span>
                <span>topo</span><span>.</span><span>append</span><span>(</span><span>v</span><span>)</span>
        <span>build_topo</span><span>(</span><span>self</span><span>)</span>
        <span>return</span> <span>topo</span>
</code></pre></div></div>

<p>To get a feel for how this works, we can do a topological sort of a very simple
expression graph, <code>1*2</code>.</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span><span> </span><span>from</span> <span>micrograd.engine</span> <span>import</span> <span>Value</span>
<span>&gt;&gt;&gt;</span><span> </span><span>x</span> <span>=</span> <span>Value</span><span>(</span><span>1</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>y</span> <span>=</span> <span>Value</span><span>(</span><span>2</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>z</span> <span>=</span> <span>x</span> <span>*</span> <span>y</span>
<span>&gt;&gt;&gt;</span><span> </span><span>z</span><span>.</span><span>topo</span><span>()</span>
<span>[Value(data=1, grad=0), Value(data=2, grad=0), Value(data=3, grad=0)]
</span><span>&gt;&gt;&gt;</span><span>
</span></code></pre></div></div>

<p>The topological sort says that in order to calculate the value <code>3</code>, we must
first calculate the values <code>1</code> and <code>2</code>. It doesn’t matter in what order we do
<code>1</code> and <code>2</code>, but they both have to come before <code>3</code>.</p>

<p>Now that we have a way to get a graph traversal order, we can start doing some
backpropagation.</p>

<h3 id="applying-this-to-backpropagation">Applying this to backpropagation</h3>

<p>If we take what we know now about the chain rule and topological sort, we can
do backpropagation on the graph. Below is the code straight from micrograd. It
first builds a topological sort and then operates on it in reverse, applying
the chain rule to each <code>Value</code> one at a time.</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>backward</span><span>(</span><span>self</span><span>):</span>

        <span># topological order all of the children in the graph
</span>        <span>topo</span> <span>=</span> <span>[]</span>
        <span>visited</span> <span>=</span> <span>set</span><span>()</span>
        <span>def</span> <span>build_topo</span><span>(</span><span>v</span><span>):</span>
            <span>if</span> <span>v</span> <span>not</span> <span>in</span> <span>visited</span><span>:</span>
                <span>visited</span><span>.</span><span>add</span><span>(</span><span>v</span><span>)</span>
                <span>for</span> <span>child</span> <span>in</span> <span>v</span><span>.</span><span>_prev</span><span>:</span>
                    <span>build_topo</span><span>(</span><span>child</span><span>)</span>
                <span>topo</span><span>.</span><span>append</span><span>(</span><span>v</span><span>)</span>
        <span>build_topo</span><span>(</span><span>self</span><span>)</span>

        <span># --- the new bit ---
</span>        <span># go one variable at a time and apply the chain rule to get its gradient
</span>        <span>self</span><span>.</span><span>grad</span> <span>=</span> <span>1</span>
        <span>for</span> <span>v</span> <span>in</span> <span>reversed</span><span>(</span><span>topo</span><span>):</span>
            <span>v</span><span>.</span><span>_backward</span><span>()</span>
</code></pre></div></div>

<p>The <code>Value.backward</code> function is normally called on the result <code>Value</code> of the
loss function.</p>

<p>If you are wondering why we set <code>self.grad</code> to <code>1</code> here before doing
backpropagation, take a moment and wonder to yourself. Maybe it’s worth drawing
a picture!</p>

<!--
linearize the operations both for forward and backward passes

wengert list is kind of like TAC or bytecode or IR
-->

<h2 id="putting-it-all-together">Putting it all together</h2>

<p>I am not going to get into the specifics, but here is what a rough sketch of
very simplified training loop might look like for MLP-based classifier for the
<a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST digit recognition
problem</a>. <strong>This code is not
runnable as-is.</strong> It needs the image loading support code and a loss function.
The hyperparameters (batch size, etc) are completely arbitrary and untuned.
<a href="https://github.com/tekknolagi/micrograd/blob/534ab3c884e66c8a325e0a8f3ed278656a616002/mnist.py">The full training
code</a>
and <a href="https://github.com/tekknolagi/micrograd/blob/534ab3c884e66c8a325e0a8f3ed278656a616002/micrograd/engine.py">corresponding engine
modifications</a>
to add <code>exp</code>/<code>log</code>/<code>Max</code> are available in the GitHub repo.</p>

<div><div><pre><code><span>import</span> <span>random</span>
<span>from</span> <span>micrograd.nn</span> <span>import</span> <span>MLP</span>
<span># ...
</span><span>NUM_DIGITS</span> <span>=</span> <span>10</span>
<span>LEARNING_RATE</span> <span>=</span> <span>0.1</span>
<span># Each image is 28x28. Hidden layer of width 50. Output 10 digits.
</span><span>model</span> <span>=</span> <span>MLP</span><span>(</span><span>28</span><span>*</span><span>28</span><span>,</span> <span>[</span><span>50</span><span>,</span> <span>NUM_DIGITS</span><span>])</span>
<span># Pretend there is some kind of function that loads the labeled training images
# into memory.
</span><span>db</span> <span>=</span> <span>list</span><span>(</span><span>images</span><span>(</span><span>&#34;train-images-idx3-ubyte&#34;</span><span>,</span> <span>&#34;train-labels-idx1-ubyte&#34;</span><span>))</span>
<span>num_epochs</span> <span>=</span> <span>100</span>
<span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>num_epochs</span><span>):</span>
    <span>for</span> <span>image</span> <span>in</span> <span>db</span><span>:</span>
        <span># zero grad
</span>        <span>for</span> <span>p</span> <span>in</span> <span>model</span><span>.</span><span>parameters</span><span>():</span>
            <span>p</span><span>.</span><span>grad</span> <span>=</span> <span>0.0</span>
        <span># forward
</span>        <span>output</span> <span>=</span> <span>model</span><span>(</span><span>image</span><span>.</span><span>pixels</span><span>)</span>
        <span>loss</span> <span>=</span> <span>compute_loss</span><span>(</span><span>output</span><span>)</span>
        <span># backward
</span>        <span>loss</span><span>.</span><span>backward</span><span>()</span>
        <span># update
</span>        <span>for</span> <span>p</span> <span>in</span> <span>model</span><span>.</span><span>parameters</span><span>():</span>
            <span>p</span><span>.</span><span>data</span> <span>-=</span> <span>LEARNING_RATE</span> <span>*</span> <span>p</span><span>.</span><span>grad</span>
</code></pre></div></div>

<p>In this snippet, constructing the <code>MLP</code> (<code>model = MLP(...)</code>) builds a bunch of
<code>Neuron</code>s in <code>Layer</code>s and initializes some weights as <code>Value</code>s, but it does not
construct the graph yet. Only when it is called (as in <code>model(image.pixels)</code>)
does it construct the graph and do all of the dot products. Then we construct
more of the graph on top of that when calculating the loss. This is the forward
pass!</p>

<p>Then we have the backward pass, where we call <code>backward()</code> on the loss, as I
explained above.</p>

<p>Then we adjust all of the weights by their gradients.</p>

<p>And remember to zero your gradients, folks!</p>

<picture>
  <source srcset="/assets/img/nn-mistakes-dark.png" media="(prefers-color-scheme: dark)"/>
  <img src="https://www.datadoodad.com/assets/img/nn-mistakes.png" alt="Tweet from Andrej Karpathy: most common neural net mistakes: 1) you didn&#39;t try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)"/>
</picture>

<p>This is nice and simple—thank you, Andrej—but is it fast enough to be
usable? Let’s find out.</p>

<h2 id="performance-problems">Performance problems</h2>

<p>Uh oh, running this with CPython is slow. It looks like computing a forward
pass for one image takes about a second. And then we have to do a backward
pass, too. And we have to do several epochs of all 60,000 images. That is going
to take way too long!</p>

<p>Well, let’s do what everyone always suggests: try with
<a href="https://www.pypy.org/">PyPy</a>. Oh neat, a couple images per second.
Unfortunately, that is still not fast enough.</p>

<blockquote>
  <p>By the way, our old project
<a href="https://github.com/tekknolagi/skybison">Skybison</a> is way faster than both
CPython and PyPy here! What a fun fact. After some profiling, its major
performance pain point was function creation (that is a bit slow in Skybison
right now), but if you lift the <code>_backward</code> inner functions to the top level,
the problem goes away. Then it’s very clear that set lookup from topo sort is
the slowest bit in the profile. After that it’s garbage collection from all
the transient <code>Value</code> objects.</p>

  <p>Incidentally, hoisting the inner functions to be top-level functions <em>also</em>
massively speeds up PyPy and it becomes faster than Skybison.</p>
</blockquote>

<p>If I had to guess, my hypothesis for the pain points for all of the runtimes
is:</p>

<ul>
  <li>recreating the graph with every forward pass, because of excessive allocation
of <code>Value</code>s and all of their <code>_backward</code> functions
    <ul>
      <li>there’s also a ton of allocation and iteration overhead with the <code>zip</code> in
<code>Neuron.__call__</code></li>
    </ul>
  </li>
  <li>doing a topological sort with every backward pass, because of the pointer
chasing, function calls, and <code>set</code>/<code>list</code> allocation and operations</li>
  <li>normal Python interpreter overhead</li>
</ul>

<p>But if I have learned anything at all over the years, instead of optimizing
blindly in the dark, we should first measure.</p>

<h3 id="checking-with-a-profiler">Checking with a profiler</h3>

<p>Emery Berger and his team have released an excellent Python profiling tool
called <a href="https://github.com/plasma-umass/scalene">Scalene</a>. To use it, you can
run <code>scalene yourprogram.py</code> instead of <code>python3 progam.py</code> and when it is
finished (or you hit Control-C), a little locally-hosted website will pop up
with profiling information.</p>

<p>I ran Scalene on our little micrograd MNIST and this is what it looks like.</p>

<figure>
  <a href="https://www.datadoodad.com/assets/img/scalene-nn.png"><img src="https://www.datadoodad.com/assets/img/scalene-nn.png"/></a>
  <figcaption>
    <p>Fig. 3 - A screenshot of the Scalene profiler’s view of micrograd. It looks
  like there is a lot of <code>Value</code> allocation and <code>self._prev</code> being a set could
  even be a leak somehow! You can especially see there are a lot of <code>+</code> and <code>*</code>
  operations because <code>__add__</code> and <code>__mul__</code> allocate a lot.</p>
  </figcaption>
</figure>

<p>It looks like in the memory column, the line is going up and to the right,
which is not what we want. It also looks like a huge amount of time is being
spent in creating the <code>set</code> of <code>_prev</code> elements for each <code>Value</code></p>

<p>If you are old school and don’t trust new profiling tools, you can even confirm
these observations using <code>perf</code>. You’ll need to install the debug symbols for
your Python distribution, probably (in my case it was <code>python3.10-dbg</code> for
Ubuntu) and then you can run <code>perf record python3 yourprogram.py</code>. Here’s what
that view looks like for me (cut off below <code>0.5%</code>):</p>

<div><div><pre><code>Samples: 138K of event &#39;cpu_core/cycles/&#39;, Event count (approx.): 64926188565
Overhead  Command  Shared Object     Symbol
  37.41%  python3  python3.10        [.] gc_collect_main.lto_priv.0
  27.85%  python3  python3.10        [.] deduce_unreachable
   9.91%  python3  python3.10        [.] visit_reachable.lto_priv.0
   3.58%  python3  python3.10        [.] set_traverse.lto_priv.0
   3.29%  python3  python3.10        [.] dict_traverse.lto_priv.0
   2.65%  python3  python3.10        [.] _PyEval_EvalFrameDefault
   2.04%  python3  python3.10        [.] func_traverse.lto_priv.0
   1.67%  python3  python3.10        [.] subtype_traverse.lto_priv.0
   1.16%  python3  python3.10        [.] tupletraverse.lto_priv.0
   0.73%  python3  python3.10        [.] _PyObject_GenericSetAttrWithDict
   0.54%  python3  python3.10        [.] cell_traverse.lto_priv.0
   0.52%  python3  python3.10        [.] insertdict.lto_priv.0
</code></pre></div></div>

<p><code>gc_collect_main</code> being 37% of the profile is a massive red flag. Then the
other functions below (<code>deduce_unreachable</code> and all the <code>_traverse</code> functions)
also look GC-related… that means the program is just drowning in allocations.
So Scalene and <code>perf</code> seem to agree.</p>

<p>If you remove the <code>set(_children)</code> and just leave it as a tuple (this seems to
not affect correctness), the profile is a little more spread out.</p>

<p>Another easy enough fix is to add <code>__slots__</code> to the <code>Value</code> class. Attribute
dicts are the only place I can think of where we are allocating dicts, so maybe
we can take care of that. After adding <code>__slots__</code>, sure enough,
<code>dict_traverse</code> goes away.</p>

<p>Last, we could also try to remove the nested function allocation (as we tried
above for Skybison/PyPy). This will remove <code>func_traverse</code>, too. That’s a
little more work than the previous two micro-optimizations, though.</p>

<p>And none of these little fixes changes the overall architecture of the program,
which involves doing <em>so much work</em> to do a little math and a little graph
walking<sup id="fnref:hotspot-fails" role="doc-noteref"><a href="#fn:hotspot-fails" rel="footnote">7</a></sup>.</p>

<p>So what’s to be done?</p>

<h3 id="solutions">Solutions</h3>

<p>As I like to say, the best way to make a program faster is to <em>do less</em>.
Too much GC? Allocate less. Too much recursion? Topo sort less. Too much
overhead? Interpret less. In more detail, my proposed solutions are:</p>

<ul>
  <li>Re-use the graph structure between inputs. Instead of building a <code>Value</code>
graph anew every time, copy in new inputs and propagate them forward and
backward.</li>
  <li>Since you aren’t changing the graph anymore, no need to re-topo-sort. Keep
the ordering around. This helps for both forward and backward passes.</li>
  <li>At the end of the day, the <code>Value</code> abstraction does not matter too much. If
we know what order to traverse in and are using IEEE-754 doubles, we should
compile the topo sort with its operations to C or something more
bare-bones<sup id="fnref:asm" role="doc-noteref"><a href="#fn:asm" rel="footnote">8</a></sup>.</li>
</ul>

<p>This checks out with what we already know about compilers: if you can freeze
some of the dynamism in the allowable semantics of a program, you get a
performance benefit. Since the graph shape is static, this sounds like a fine
idea.</p>

<!--
TODO: parallelization of work in the graph? is that possible? it looks like in
MNIST you can't do anything for tensor version, but maybe for scalar.
-->

<h2 id="lets-write-a-compiler">Let’s write a compiler</h2>

<p>The goal with this compiler is to write something very small that fits
reasonably cleanly into micrograd as it already is—not to re-architect
anything.</p>

<p>We could write a compiler to a kind of bytecode. This would get rid of all of
the function calls and repeated tree traversals and pointer chasing. It would
probably be faster. But unfortunately we would still have an interpreter loop,
and that interpreter would be written in Python—would have a lot of overhead.</p>

<p>Instead, we will go further and compile that straight line code to C. The end
goal is to make a Python C extension that we can <code>import</code> and use in place of
the interpreted version of micrograd.</p>

<p>The original version of this project compiled the <code>MLP</code> and <code>Layer</code> and
<code>Neuron</code> classes directly into C, but that unfortunately is not very
extensible: making architectural changes to your model would then require
writing new compilers. It also did not support backpropagation, so it only
helped inference.</p>

<p>For this reason, we are writing compilers for <code>Value</code> graphs. This means
anybody can get a compiler for free as long as their machine learning
architecture uses <code>Value</code>s. You need only write an interpreter for it!</p>

<h3 id="forward">Forward</h3>

<p>Since we have a topological sort, we might as well use it both forward and
backward. Then we only need to write a compiler that works one <code>Value</code> at a
time. We can drive it like this:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span><span> </span><span>from</span> <span>micrograd.engine</span> <span>import</span> <span>Value</span>
<span>&gt;&gt;&gt;</span><span> </span><span>x</span> <span>=</span> <span>Value</span><span>(</span><span>1</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>y</span> <span>=</span> <span>Value</span><span>(</span><span>2</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>z</span> <span>=</span> <span>x</span> <span>*</span> <span>y</span>
<span>&gt;&gt;&gt;</span><span> </span><span>order</span> <span>=</span> <span>z</span><span>.</span><span>topo</span><span>()</span>
<span>&gt;&gt;&gt;</span><span> </span><span>for</span> <span>v</span> <span>in</span> <span>order</span><span>:</span>
<span>...</span><span>     </span><span>print</span><span>(</span><span>v</span><span>.</span><span>compile</span><span>())</span>
<span>...
</span><span>data[1] = 2;
data[0] = 1;
data[2] = data[1]*data[0];
</span><span>&gt;&gt;&gt;</span><span>
</span></code></pre></div></div>

<p>(Where it is assumed that <code>data</code> is some properly-sized array of <code>double</code>s that
we will create later.)</p>

<p>Look, there it is! A neat little linearization of the graph. It’s kind of like
the topo sort we saw earlier, but in C code. This strategy works because we
don’t have loops and we don’t have re-definitions of <code>Value</code>s. Each value is
set once<sup id="fnref:ssa" role="doc-noteref"><a href="#fn:ssa" rel="footnote">9</a></sup>. and this code, even with all its memory loads and stores,
should be much faster than pointer chasing and function calls in Python-land.</p>

<p>We could have done this similarly to the interpreted version, where each kind
of operation has its own method (<code>__add__</code>, <code>__mul__</code>, etc), but it’s easier to
present the compiler all in one method. For that reason I am adding a <code>compile</code>
function. See for example the implementation of constant values (<code>op==&#39;&#39;</code>) and
addition (<code>op==&#39;+&#39;</code>):</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>var</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>f</span><span>&#34;data[</span><span>{</span><span>self</span><span>.</span><span>_id</span><span>}</span><span>]&#34;</span>

    <span>def</span> <span>set</span><span>(</span><span>self</span><span>,</span> <span>val</span><span>):</span>
        <span>return</span> <span>f</span><span>&#34;</span><span>{</span><span>self</span><span>.</span><span>var</span><span>()</span><span>}</span><span> = </span><span>{</span><span>val</span><span>}</span><span>;&#34;</span>

    <span>def</span> <span>compile</span><span>(</span><span>self</span><span>):</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>in</span> <span>(</span><span>&#39;weight&#39;</span><span>,</span> <span>&#39;bias&#39;</span><span>,</span> <span>&#39;input&#39;</span><span>):</span>
            <span># Not calculated; set elsewhere
</span>            <span>return</span> <span>&#34;&#34;</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>==</span> <span>&#39;&#39;</span><span>:</span>
            <span>return</span> <span>self</span><span>.</span><span>set</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>self</span><span>.</span><span>data</span><span>}</span><span>&#34;</span><span>)</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>==</span> <span>&#39;*&#39;</span><span>:</span>
            <span>c0</span><span>,</span> <span>c1</span> <span>=</span> <span>self</span><span>.</span><span>_prev</span>
            <span>return</span> <span>self</span><span>.</span><span>set</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>c0</span><span>.</span><span>var</span><span>()</span><span>}</span><span>*</span><span>{</span><span>c1</span><span>.</span><span>var</span><span>()</span><span>}</span><span>&#34;</span><span>)</span>
        <span>raise</span> <span>RuntimeError</span><span>(</span><span>f</span><span>&#34;op </span><span>{</span><span>self</span><span>.</span><span>_op</span><span>}</span><span> left as an exercise for the reader&#34;</span><span>)</span>
</code></pre></div></div>

<p>The other operators are not so different. See if you can figure out how to
implement <code>**</code> or <code>exp</code>, for example. Note that <code>**</code> requires either storing
additional data or a kind of gross hack.</p>

<p>You may notice that this compilation strategy requires assigning identifiers to
<code>Value</code>s. To do that, I have added an <code>_id</code> field that is an auto-incrementing
counter in the <code>__init__</code> function. The implementation does not matter so much;
just know that every <code>Value</code> object has a unique <code>_id</code>.</p>

<p>My complete compiler implementation for all of the operations is about 40 lines
and it even includes some small on-the-fly optimizations. But this compiler
does forward passes. What about backward passes? We need to train faster, too.
Backward has to be much more complicated, right?</p>

<h3 id="backward">Backward</h3>

<p>Actually, it’s about the same complexity. We need only do a line-by-line
translation of the backpropagation functions (all the <code>_backward</code>
implementations).</p>

<p>For example, we can revisit the backpropagation for <code>*</code>. I added some helper
functions to make the code shorter and look more like the interpreted version.
Like the forward version, all the operators are in one method:
<code>backward_compile</code>.</p>

<div><div><pre><code><span>class</span> <span>Value</span><span>:</span>
    <span># ...
</span>    <span>def</span> <span>getgrad</span><span>(</span><span>self</span><span>):</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>in</span> <span>(</span><span>&#39;&#39;</span><span>,</span> <span>&#39;input&#39;</span><span>):</span>
            <span>raise</span> <span>RuntimeError</span><span>(</span><span>&#34;Grad for constants and input data not stored&#34;</span><span>)</span>
        <span>return</span> <span>f</span><span>&#34;grad[</span><span>{</span><span>self</span><span>.</span><span>_id</span><span>}</span><span>]&#34;</span>

    <span>def</span> <span>setgrad</span><span>(</span><span>self</span><span>,</span> <span>val</span><span>):</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>in</span> <span>(</span><span>&#39;&#39;</span><span>,</span> <span>&#39;input&#39;</span><span>):</span>
            <span># We don&#39;t care about setting gradients for constants or input
</span>            <span># data.
</span>            <span>return</span> <span>[]</span>
        <span>return</span> <span>[</span><span>f</span><span>&#34;</span><span>{</span><span>self</span><span>.</span><span>getgrad</span><span>()</span><span>}</span><span> += </span><span>{</span><span>val</span><span>}</span><span>;&#34;</span><span>]</span>

    <span>def</span> <span>backward_compile</span><span>(</span><span>self</span><span>):</span>
        <span>if</span> <span>not</span> <span>self</span><span>.</span><span>_prev</span><span>:</span>
            <span>assert</span> <span>self</span><span>.</span><span>_op</span> <span>in</span> <span>(</span><span>&#39;&#39;</span><span>,</span> <span>&#39;weight&#39;</span><span>,</span> <span>&#39;bias&#39;</span><span>,</span> <span>&#39;input&#39;</span><span>)</span>
            <span># Nothing to propagate to children.
</span>            <span>assert</span> <span>not</span> <span>self</span><span>.</span><span>_prev</span>
            <span>return</span> <span>[]</span>
        <span>if</span> <span>self</span><span>.</span><span>_op</span> <span>==</span> <span>&#39;*&#39;</span><span>:</span>
            <span>left</span><span>,</span> <span>right</span> <span>=</span> <span>self</span><span>.</span><span>_prev</span>
            <span>return</span> <span>left</span><span>.</span><span>setgrad</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>right</span><span>.</span><span>var</span><span>()</span><span>}</span><span>*</span><span>{</span><span>self</span><span>.</span><span>getgrad</span><span>()</span><span>}</span><span>&#34;</span><span>)</span> <span>+</span>\
                    <span>right</span><span>.</span><span>setgrad</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>left</span><span>.</span><span>var</span><span>()</span><span>}</span><span>*</span><span>{</span><span>self</span><span>.</span><span>getgrad</span><span>()</span><span>}</span><span>&#34;</span><span>)</span>
        <span>raise</span> <span>RuntimeError</span><span>(</span><span>f</span><span>&#34;op </span><span>{</span><span>self</span><span>.</span><span>_op</span><span>}</span><span> left as an exercise for the reader&#34;</span><span>)</span>
</code></pre></div></div>

<p>(Like the forward version, assume for now that <code>grad</code> is some properly-sized
array of <code>double</code>s that we will create later.)</p>

<p>Let’s see how it works in practice.</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span><span> </span><span>x</span> <span>=</span> <span>Value</span><span>(</span><span>1</span><span>,</span> <span>_op</span><span>=</span><span>&#39;weight&#39;</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>y</span> <span>=</span> <span>Value</span><span>(</span><span>2</span><span>,</span> <span>_op</span><span>=</span><span>&#39;weight&#39;</span><span>)</span>
<span>&gt;&gt;&gt;</span><span> </span><span>z</span> <span>=</span> <span>x</span> <span>*</span> <span>y</span>
<span>&gt;&gt;&gt;</span><span> </span><span>order</span> <span>=</span> <span>z</span><span>.</span><span>topo</span><span>()</span>
<span>&gt;&gt;&gt;</span><span> </span><span>for</span> <span>v</span> <span>in</span> <span>order</span><span>:</span>
<span>...</span><span>     </span><span>print</span><span>(</span><span>v</span><span>.</span><span>backward_compile</span><span>())</span>
<span>...
</span><span>[]
[]
[&#39;grad[6] += data[7]*grad[8];&#39;, &#39;grad[7] += data[6]*grad[8];&#39;]
</span><span>&gt;&gt;&gt;</span><span>
</span></code></pre></div></div>

<p>Huh, that’s weird. Why is there no backpropagation code for <code>x</code> (<code>grad[6]</code>) and
<code>y</code> (<code>grad[7]</code>)? That’s because they don’t have any children of their own;
instead, they are adjusted by their parent node, <code>z</code> (<code>grad[8]</code>). This is what
I meant earlier when I said that visiting a node adjusts the node’s children.</p>

<p>My complete backward pass compiler implementation is about 30 lines! Shorter
than the forward pass, even. That’s pretty neat.</p>

<p><strong>You have just finished writing a compiler.</strong> Congratulations! Seriously, the
most involved and complicated bit is over. The rest is small details and Python
C-API specifics that you can skip if you like. All we’re missing is <code>update</code>
and <code>set_input</code> and some wrapper code, which are not nearly as interesting.</p>

<h3 id="update">Update</h3>

<p>Once we have done the backward pass (potentially multiple in a row), we need to
adjust the weights by their gradients. Code generation for this is a fairly
mechanical translation of the Python code into C. For comparison, here is the
interpreted version:</p>

<div><div><pre><code><span>def</span> <span>update</span><span>(</span><span>model</span><span>)</span>
    <span>for</span> <span>p</span> <span>in</span> <span>model</span><span>.</span><span>parameters</span><span>():</span>
        <span>p</span><span>.</span><span>data</span> <span>-=</span> <span>LEARNING_RATE</span> <span>*</span> <span>p</span><span>.</span><span>grad</span>
</code></pre></div></div>

<p>It loops over the model parameters at run-time and adjusts them. By contrast,
the compiled version does the iteration at compile-time and has straight-line
subtractions at run-time:</p>

<div><div><pre><code><span>def</span> <span>gen_update</span><span>(</span><span>f</span><span>,</span> <span>model</span><span>,</span> <span>learning_rate</span><span>):</span>
    <span>for</span> <span>o</span> <span>in</span> <span>model</span><span>.</span><span>parameters</span><span>():</span>
        <span>assert</span> <span>o</span><span>.</span><span>_op</span> <span>in</span> <span>(</span><span>&#39;weight&#39;</span><span>,</span> <span>&#39;bias&#39;</span><span>),</span> <span>repr</span><span>(</span><span>o</span><span>.</span><span>_op</span><span>)</span>
        <span>print</span><span>(</span><span>f</span><span>&#34;data[</span><span>{</span><span>o</span><span>.</span><span>_id</span><span>}</span><span>] -= </span><span>{</span><span>learning_rate</span><span>}</span><span> * </span><span>{</span><span>o</span><span>.</span><span>getgrad</span><span>()</span><span>}</span><span>;&#34;</span><span>,</span> <span>file</span><span>=</span><span>f</span><span>)</span>
<span># data[0] -= 0.01 * grad[0];
# data[1] -= 0.01 * grad[1];
# data[2] -= 0.01 * grad[2];
# ...
</span></code></pre></div></div>

<p>It’s even the same length as the Python equivalent, if you exclude the
<code>assert</code>.</p>

<h3 id="setting-the-input">Setting the input</h3>

<p>Getting input from Python code into C++ is a little tricky when it’s not simple
data types like integers and floats. Ideally our generated ML code would be
able to share memory with Python to avoid copying data back and forth, but that
wouldn’t be as simple an implementation<sup id="fnref:zero-copy" role="doc-noteref"><a href="#fn:zero-copy" rel="footnote">10</a></sup>, so we’re doing something
slightly sillier.</p>

<p>We’re going to have a function <code>set_input</code> that takes its black and white pixel
data in an array of bytes and copies each pixel to its respective slot in the
<code>data</code> array. While this is pretty slow compared to, say, <em>not</em> copying, it is
certainly not the bottleneck in the pipeline.</p>

<div><div><pre><code><span>def</span> <span>gen_set_input</span><span>(</span><span>inp</span><span>):</span>
    <span>result</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>idx</span><span>,</span> <span>o</span> <span>in</span> <span>enumerate</span><span>(</span><span>inp</span><span>):</span>
        <span>result</span><span>.</span><span>append</span><span>(</span><span>f</span><span>&#34;data[</span><span>{</span><span>o</span><span>.</span><span>_id</span><span>}</span><span>] = buf[</span><span>{</span><span>idx</span><span>}</span><span>];</span><span>\n</span><span>&#34;</span><span>)</span>
    <span>return</span> <span>&#34;&#34;</span><span>.</span><span>join</span><span>(</span><span>result</span><span>)</span>
</code></pre></div></div>

<p>In this case, <code>inp</code> is the array of inputs. Unlike with the interpreted version
of micrograd, we are not creating new input <code>Value</code>s with every iteration. This
means we have to pre-allocate the range of IDs used for input to and output
from the ML model:</p>

<div><div><pre><code><span>NUM_PIXELS</span> <span>=</span> <span>28</span><span>*</span><span>28</span>
<span>NUM_DIGITS</span> <span>=</span> <span>10</span>
<span>inp</span> <span>=</span> <span>[</span><span>Value</span><span>(</span><span>0</span><span>,</span> <span>(),</span> <span>&#34;input&#34;</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>NUM_PIXELS</span><span>)]</span>
<span>exp</span> <span>=</span> <span>[</span><span>Value</span><span>(</span><span>0</span><span>,</span> <span>(),</span> <span>&#34;input&#34;</span><span>)</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>NUM_DIGITS</span><span>)]</span>
<span>out</span> <span>=</span> <span>model</span><span>(</span><span>inp</span><span>)</span>  <span># create the compile-time Value graph
</span><span>loss</span> <span>=</span> <span>compute_loss</span><span>(</span><span>out</span><span>,</span> <span>exp</span><span>)</span>

<span>gen_set_input</span><span>(</span><span>inp</span><span>)</span>
</code></pre></div></div>

<p>Note that the <code>data</code> or <code>grad</code> fields of each <code>Value</code> node contain garbage data
since <code>inp</code> and <code>exp</code> are arbitrarily chosen. However, the generated C code does
not actually use these Python values. All we care about is the graph structure
represented by the <code>_op</code> and <code>_prev</code> fields.</p>

<p>In order to use this C code from Python, we’ll have to make a Python C
extension using the C-API.</p>

<h3 id="a-python-c-extension">A Python C extension</h3>

<p>Having a bunch of free-floating code to update <code>data</code> and <code>grad</code> arrays is fun,
and it’s a complete compiler, but it’s not useful yet. We need to wrap that
code in functions (I called them <code>forward</code>, <code>backward</code>, <code>update</code>, and
<code>set_input</code>) and make them accessible to our Python driver program. We don’t
want to have to completely move to C!</p>

<p>Most of this is straightforward (literally <code>print(&#34;void forward() {&#34;)</code> and so
on), but some of this requires knowledge of Python internals.</p>

<p>For example, here is a snippet of the wrapper code around the <code>forward</code>
function.</p>

<div><div><pre><code><span>PyObject</span><span>*</span> <span>forward_wrapper</span><span>(</span><span>PyObject</span> <span>*</span><span>module</span><span>,</span> <span>PyObject</span> <span>*</span><span>const</span> <span>*</span><span>args</span><span>,</span> <span>Py_ssize_t</span> <span>nargs</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>nargs</span> <span>!=</span> <span>2</span><span>)</span> <span>{</span>
        <span>PyErr_Format</span><span>(</span><span>PyExc_TypeError</span><span>,</span> <span>&#34;expected 2 args: label, pixels&#34;</span><span>);</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>PyObject</span><span>*</span> <span>label_obj</span> <span>=</span> <span>args</span><span>[</span><span>0</span><span>];</span>
    <span>PyObject</span><span>*</span> <span>pixels_obj</span> <span>=</span> <span>args</span><span>[</span><span>1</span><span>];</span>
    <span>if</span> <span>(</span><span>!</span><span>PyLong_CheckExact</span><span>(</span><span>label_obj</span><span>))</span> <span>{</span>
        <span>PyErr_Format</span><span>(</span><span>PyExc_TypeError</span><span>,</span> <span>&#34;expected int&#34;</span><span>);</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>if</span> <span>(</span><span>!</span><span>PyBytes_CheckExact</span><span>(</span><span>pixels_obj</span><span>))</span> <span>{</span>
        <span>PyErr_Format</span><span>(</span><span>PyExc_TypeError</span><span>,</span> <span>&#34;expected bytes&#34;</span><span>);</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>if</span> <span>(</span><span>PyBytes_Size</span><span>(</span><span>pixels_obj</span><span>)</span> <span>!=</span> <span>28</span><span>*</span><span>28</span><span>)</span> <span>{</span>
        <span>PyErr_Format</span><span>(</span><span>PyExc_TypeError</span><span>,</span> <span>&#34;expected bytes of size 28*28&#34;</span><span>);</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>// ...</span>
<span>}</span>
</code></pre></div></div>

<p>It is an example of a <em>fastcall</em> C-API function, meaning it takes its arguments
in an array. We have to register it as such:</p>

<div><div><pre><code><span>static</span> <span>PyMethodDef</span> <span>nn_methods</span><span>[]</span> <span>=</span> <span>{</span>
    <span>{</span> <span>&#34;forward&#34;</span><span>,</span> <span>(</span><span>PyCFunction</span><span>)</span><span>forward_wrapper</span><span>,</span> <span>METH_FASTCALL</span><span>,</span> <span>&#34;doc goes here&#34;</span> <span>},</span>
    <span>// ...</span>
<span>};</span>
</code></pre></div></div>

<p>And then make a Python-importable module description so that we can create a
<code>module</code> object at import-time:</p>

<div><div><pre><code><span>static</span> <span>struct</span> <span>PyModuleDef</span> <span>nnmodule</span> <span>=</span> <span>{</span>
    <span>PyModuleDef_HEAD_INIT</span><span>,</span>
    <span>&#34;nn&#34;</span><span>,</span>
    <span>&#34;doc goes here&#34;</span><span>,</span>
    <span>-</span><span>1</span><span>,</span>
    <span>nn_methods</span><span>,</span>
    <span>NULL</span><span>,</span>
    <span>NULL</span><span>,</span>
    <span>NULL</span><span>,</span>
    <span>NULL</span>
<span>};</span>
</code></pre></div></div>

<p>And then we can create this magic <code>PyInit_nn</code> function. If the Python native
importer finds a module in a <code>.so</code> and it has a <code>PyInit_XYZ</code> function, it will
call it to create the module object.</p>

<div><div><pre><code><span>// Some good keywords are &#34;PEP 384&#34; and &#34;PEP 489&#34;.</span>
<span>PyObject</span><span>*</span> <span>PyInit_nn</span><span>()</span> <span>{</span>
    <span>PyObject</span><span>*</span> <span>m</span> <span>=</span> <span>PyState_FindModule</span><span>(</span><span>&amp;</span><span>nnmodule</span><span>);</span>
    <span>if</span> <span>(</span><span>m</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>m</span><span>;</span>
    <span>}</span>
    <span>// ...</span>
    <span>return</span> <span>PyModule_Create</span><span>(</span><span>&amp;</span><span>nnmodule</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>That’s mostly it! Now we can use all of our hard work in model training and
inference.</p>

<h2 id="did-it-work-is-it-faster">Did it work? Is it faster?</h2>

<p>These are two separate questions and performance doesn’t mean anything if your
code produces wrong output.</p>

<h3 id="correctness">Correctness</h3>

<p>Testing compilers can be tricky. There are a lot of parts and they all have to
work on their own and also together. Thankfully in this case, we have a very
small compiler with very few basic operations. This makes it not too difficult
to write unit tests about the generated C code.</p>

<p>It’s also probably worth having some side-by-side tests on the output <em>numbers</em>
of the interpreted and compiled versions of the same code. If they are with
some error margin, we can consider the compiler correct. I don’t recommend
doing MNIST, though; the interpreted version is too slow and unit tests should
be fast. Maybe try XOR.</p>

<p>Thankfully, CPython uses the host system floating point implementation for its
<code>float</code>s, so we get the same numeric behavior as C for no additional effort.</p>

<h3 id="performance">Performance</h3>

<p>On my machine, training goes from 1 image per second (interpreted) to &gt;1000
images per second (compiled). This is at least a THOUSAND TIMES speed increase!
It comes with an up-front cost, though; you have to compile the C code. If you
use TCC, a very fast C compiler, you get pretty reasonable performance. I saw
about half second compile times and 45 seconds per epoch. If you use Clang, a
much slower C compiler, you get even better performance. Take a look at this
handy dandy tradeoff table:</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Compile time (s)</td>
      <td>Time per epoch (s)</td>
      <td>Speedup</td>
    </tr>
    <tr>
      <td>Interpreted</td>
      <td>0</td>
      <td>60,000</td>
      <td>1x</td>
    </tr>
    <tr>
      <td>TCC</td>
      <td>0.5</td>
      <td>45</td>
      <td>1333x</td>
    </tr>
    <tr>
      <td>Clang <code>-O0</code></td>
      <td>~30</td>
      <td>30</td>
      <td>2000x</td>
    </tr>
    <tr>
      <td>Clang <code>-O1</code></td>
      <td>~350</td>
      <td>8</td>
      <td>7500x</td>
    </tr>
  </tbody>
</table>

<p>Either way, this is a pretty big win. I think we did it! Check out the full
<a href="https://github.com/tekknolagi/micrograd/blob/c15b6b8fd373c48014be369c4f7bd0917932a53b/micrograd/engine.py">compiler
code</a>
and <a href="https://github.com/tekknolagi/micrograd/blob/c15b6b8fd373c48014be369c4f7bd0917932a53b/test.py">compiler wrapper and training
code</a>
on GitHub.</p>

<!-- TODO note the perf of real world ML software for comparison? -->

<!--

### more optimizations

TODO: is this true? does it work?

i added `-O1` but Clang took forever so I eventually killed the process. so i
tried to figure out what was taking so long using gdb and repeatedly attaching
and it looked like SROA (scalar replacement of aggregates). so i took
everything out of arrays and put them in individual global variables. then
regalloc took a long time (but not forever!) and eventually, in 373s instead of
30s, i got a binary. that binary runs 2x as fast (~4000 images/s).

TODO: make the arrays static?

-->

<h2 id="conclusion">Conclusion</h2>

<p>Neural networks are represented by static data-flow graphs which are executed
in both forward and backward directions. This means they are kind of like
tree-walking interpreters. It also means that compiling the tree to a
lower-level representation makes the program faster.</p>

<p>On a more serious note: I have traditionally been very uninterested in applying
ML because it is oft-used to either harm people (surveillance, recommender
systems siloing people, etc) or make software worse (several large companies
recently mucked up their chronological feeds, etc).</p>

<p>I learned about machine learning and wrote this post to understand the theory
my friends geek out about regularly. I implore you, potential ML practitioner,
to use your skills for Good.</p>

<p><em>Massive thanks to <a href="https://www.chrisgregory.me/">Chris</a> and
<a href="http://www.biancacapretta.com/">Bianca</a> for providing significant feedback on
this post and to <a href="https://tchebb.me/">Tom</a> for (naturally) finding and fixing a
bug.</em></p>

<h2 id="more-thoughts-and-further-reading">More thoughts and further reading</h2>

<p>There’s a lot more work to do if you are interested and have the time. I might
follow-up on these later. I might not.</p>

<h3 id="fused-multiply-add">Fused multiply add</h3>

<p>There are a lot of instances of <code>x * y + z</code> in the <code>Value</code> graph.
Unfortunately, due to the way the graph is laid out and traversed, forward and
backward code for these nodes does not often colocate <code>x * y</code> and <code>xy + z</code>.
This means that it is difficult or unlikely for the C compiler to generate a
fused multiply add (FMA) instruction, which may be faster and shorter than
separate mul/add. FMA also might require storing less intermediate data. But I
could be wrong here! It’s worth experimenting.</p>

<h3 id="compiling-for-training-vs-inference">Compiling for training vs inference</h3>

<p>Right now our compilation strategy works for both training and inference. This
is great, because it <em>does</em> make both of them faster than before, but it comes
with a tradeoff: inference is slightly slower.</p>

<p>If, post training, you freeze the weights and make their immutability known,
things get a lot more efficient. Right now we have so many memory loads and
stores and it’s hard for the C compiler to prove anything about the properties
of the numbers when it is trying to optimize. It probably also prevents use of
SIMD instructions. If we can inline the weights as double constants in the
generated C code, we can probably get much better machine code.</p>

<!-- does the lack of locality hurt too? -->

<h3 id="scalar-valued-is-less-efficient-than-tensor-valued">Scalar-valued is less efficient than tensor-valued</h3>

<p>We managed to remove a lot of the overhead <em>for the program we had</em>, but the
overall architecture did not improve much. To do that, we need to move from
scalar-valued to tensor-valued <code>Value</code>s.</p>

<p>It’s kind of like programming in assembly (scalar) vs a higher level (tensor)
language. It’s much harder to make optimizations directly on the assembly. If
you have semantically bigger and more descriptive operations in your AST
(<code>matmul</code>, etc), the compiler can better understand what you mean and optimize
that.</p>

<p>It also brings better data locality (matrix is stored densely and in either
row-major or column-major order) and we can get some vectorized math instead of
millions of <code>mulsd</code>.</p>

<p>From what I can tell, optimizing linear algebra IRs is an ongoing area of
research.</p>

<!--
TODO:
* fuse matmul with addition of bias (`W @ x + b`)
  * https://discuss.tvm.apache.org/t/operator-fusion-for-rnn/11966
  * https://github.com/pytorch/pytorch/issues/39661
* fuse matmul/add with activation function (`relu(W @ x + b)`)
  * https://github.com/pytorch/pytorch/issues/77171
* matmul associativity (and commutativity with einstein notation??) to reduce
  size of intermediate values

TODO: parallelization of matmul? GEMM?
-->

<h3 id="using-pypy">Using PyPy</h3>

<p>PyPy is a JIT compiler for Python, but it also includes a general-purpose
programming language called RPython. The neat thing is, if you write an
interpreter in RPython, PyPy will turn your interpreter into a tracing JIT
compiler. So this brings up some questions:</p>

<p><strong>What if you wrote micrograd in RPython?</strong> Would PyPy make an effective JIT
out of the tree-walking interpreter… even if it allocated all the AST nodes
on the fly? Would it blow the trace length limit?</p>

<p><strong>What if you generated Python code or bytecode?</strong> This doesn’t even require
writing the interpreter in RPython, but it does require writing a compiler from
<code>Value</code> graphs to Python (bytecode). Could PyPy compile <em>this</em> effectively?</p>


        </div></div>
  </body>
</html>
