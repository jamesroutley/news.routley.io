<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/langmanus/langmanus">Original</a>
    <h1>LangManus: An Open-Source Manus Agent with LangChain &#43; LangGraph</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/d6d32de04dc2f1ea0900c9cb1387eda9d8a5898eeff33592c9059a04377cddda/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31322b2d626c75652e737667" alt="Python 3.12+" data-canonical-src="https://img.shields.io/badge/python-3.12+-blue.svg"/></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"/></a>
<a href="https://github.com/langmanus/langmanus/blob/main/assets/wechat_community.jpg"><img src="https://camo.githubusercontent.com/f4e2b8212fde89761601afacc1f18ac76fa02c3db8c5122f8d186f2a57080428/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5765436861742d4c616e676d616e75732d627269676874677265656e3f6c6f676f3d776563686174266c6f676f436f6c6f723d7768697465" alt="WeChat" data-canonical-src="https://img.shields.io/badge/WeChat-Langmanus-brightgreen?logo=wechat&amp;logoColor=white"/></a>
<a href="https://discord.gg/m3MszDcn" rel="nofollow"><img src="https://camo.githubusercontent.com/cfb3aecb5fd07e2c45e231529772587afee9c04acf394dc9b08aad4551abb6fd/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f6d334d737a44636e3f7374796c653d666c6174" alt="Discord Follow" data-canonical-src="https://dcbadge.vercel.app/api/server/m3MszDcn?style=flat"/></a></p>
<p dir="auto"><a href="https://github.com/langmanus/langmanus/blob/main/README.md">English</a> | <a href="https://github.com/langmanus/langmanus/blob/main/README_zh.md">简体中文</a> | <a href="https://github.com/langmanus/langmanus/blob/main/README_ja.md">日本語</a></p>
<blockquote>
<p dir="auto">Come From Open Source, Back to Open Source</p>
</blockquote>
<p dir="auto">LangManus is a community-driven AI automation framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.</p>

<p dir="auto"><strong>Task</strong>: Calculate the influence index of DeepSeek R1 on HuggingFace. This index can be designed using a weighted sum of factors such as followers, downloads, and likes.</p>
<p dir="auto"><strong>LangManus&#39;s Fully Automated Plan and Solution</strong>:</p>
<ol dir="auto">
<li>Gather the latest information about &#34;DeepSeek R1&#34;, &#34;HuggingFace&#34;, and related topics through online searches.</li>
<li>Interact with a Chromium instance to visit the HuggingFace official website, search for &#34;DeepSeek R1&#34; and retrieve the latest data, including followers, likes, downloads, and other relevant metrics.</li>
<li>Find formulas for calculating model influence using search engines and web scraping.</li>
<li>Use Python to compute the influence index of DeepSeek R1 based on the collected data.</li>
<li>Present a comprehensive report to the user.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/langmanus/langmanus/blob/main/assets/demo.gif"><img src="https://github.com/langmanus/langmanus/raw/main/assets/demo.gif" alt="Demo" data-animated-image=""/></a></p>
<ul dir="auto">
<li><a href="https://youtu.be/sZCHqrQBUGk" rel="nofollow">View on YouTube</a></li>
</ul>

<ul dir="auto">
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#project-statement">Project Statement</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#why-langmanus">Why LangManus?</a></li>
<li><a href="#setup">Setup</a>
<ul dir="auto">
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#configuration">Configuration</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a></li>
<li><a href="#docker">Docker</a></li>
<li><a href="#web-ui">Web UI</a></li>
<li><a href="#development">Development</a></li>
<li><a href="#faq">FAQ</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/langmanus/langmanus.git
cd langmanus

# Install dependencies, uv will take care of the python interpreter and venv creation
uv sync

# Playwright install to use Chromium for browser-use by default
uv run playwright install

# Configure environment
# Windows: copy .env.example .env
cp .env.example .env
# Edit .env with your API keys

# Run the project
uv run main.py"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/langmanus/langmanus.git
<span>cd</span> langmanus

<span><span>#</span> Install dependencies, uv will take care of the python interpreter and venv creation</span>
uv sync

<span><span>#</span> Playwright install to use Chromium for browser-use by default</span>
uv run playwright install

<span><span>#</span> Configure environment</span>
<span><span>#</span> Windows: copy .env.example .env</span>
cp .env.example .env
<span><span>#</span> Edit .env with your API keys</span>

<span><span>#</span> Run the project</span>
uv run main.py</pre></div>

<p dir="auto">This is an academically driven open-source project, developed by a group of former colleagues in our spare time. It aims to explore and exchange ideas in the fields of Multi-Agent and DeepResearch.</p>
<ul dir="auto">
<li><strong>Purpose</strong>: The primary purpose of this project is academic research, participation in the GAIA leaderboard, and the future publication of related papers.</li>
<li><strong>Independence Statement</strong>: This project is entirely independent and unrelated to our primary job responsibilities. It does not represent the views or positions of our employers or any organizations.</li>
<li><strong>No Association</strong>: This project has no association with Manus (whether it refers to a company, organization, or any other entity).</li>
<li><strong>Clarification Statement</strong>: We have not promoted this project on any social media platforms. Any inaccurate reports related to this project are not aligned with its academic spirit.</li>
<li><strong>Contribution Management</strong>: Issues and PRs will be addressed during our free time and may experience delays. We appreciate your understanding.</li>
<li><strong>Disclaimer</strong>: This project is open-sourced under the MIT License. Users assume all risks associated with its use. We disclaim any responsibility for any direct or indirect consequences arising from the use of this project.</li>
</ul>

<p dir="auto">本项目是一个学术驱动的开源项目，由一群前同事在业余时间开发，旨在探索和交流 Multi-Agent 和 DeepResearch 相关领域的技术。</p>
<ul dir="auto">
<li><strong>项目目的</strong>：本项目的主要目的是学术研究、参与 GAIA 排行榜，并计划在未来发表相关论文。</li>
<li><strong>独立性声明</strong>：本项目完全独立，与我们的本职工作无关，不代表我们所在公司或任何组织的立场或观点。</li>
<li><strong>无关联声明</strong>：本项目与 Manus（无论是公司、组织还是其他实体）无任何关联。</li>
<li><strong>澄清声明</strong>：我们未在任何社交媒体平台上宣传过本项目，任何与本项目相关的不实报道均与本项目的学术精神无关。</li>
<li><strong>贡献管理</strong>：Issue 和 PR 将在我们空闲时间处理，可能存在延迟，敬请谅解。</li>
<li><strong>免责声明</strong>：本项目基于 MIT 协议开源，使用者需自行承担使用风险。我们对因使用本项目产生的任何直接或间接后果不承担责任。</li>
</ul>

<p dir="auto">LangManus implements a hierarchical multi-agent system where a supervisor coordinates specialized agents to accomplish complex tasks:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/langmanus/langmanus/blob/main/assets/architecture.png"><img src="https://github.com/langmanus/langmanus/raw/main/assets/architecture.png" alt="LangManus Architecture"/></a></p>
<p dir="auto">The system consists of the following agents working together:</p>
<ol dir="auto">
<li><strong>Coordinator</strong> - The entry point that handles initial interactions and routes tasks</li>
<li><strong>Planner</strong> - Analyzes tasks and creates execution strategies</li>
<li><strong>Supervisor</strong> - Oversees and manages the execution of other agents</li>
<li><strong>Researcher</strong> - Gathers and analyzes information</li>
<li><strong>Coder</strong> - Handles code generation and modifications</li>
<li><strong>Browser</strong> - Performs web browsing and information retrieval</li>
<li><strong>Reporter</strong> - Generates reports and summaries of the workflow results</li>
</ol>


<ul dir="auto">
<li>🤖 <strong>LLM Integration</strong>
<ul dir="auto">
<li>It supports the integration of most models through <a href="https://docs.litellm.ai/docs/providers" rel="nofollow">litellm</a>.</li>
<li>Support for open source models like Qwen</li>
<li>OpenAI-compatible API interface</li>
<li>Multi-tier LLM system for different task complexities</li>
</ul>
</li>
</ul>

<ul dir="auto">
<li>🔍 <strong>Search and Retrieval</strong>
<ul dir="auto">
<li>Web search via Tavily API</li>
<li>Neural search with Jina</li>
<li>Advanced content extraction</li>
</ul>
</li>
</ul>

<ul dir="auto">
<li>🐍 <strong>Python Integration</strong>
<ul dir="auto">
<li>Built-in Python REPL</li>
<li>Code execution environment</li>
<li>Package management with uv</li>
</ul>
</li>
</ul>

<ul dir="auto">
<li>📊 <strong>Visualization and Control</strong>
<ul dir="auto">
<li>Workflow graph visualization</li>
<li>Multi-agent orchestration</li>
<li>Task delegation and monitoring</li>
</ul>
</li>
</ul>

<p dir="auto">We believe in the power of open source collaboration. This project wouldn&#39;t be possible without the amazing work of projects like:</p>
<ul dir="auto">
<li><a href="https://github.com/QwenLM/Qwen">Qwen</a> for their open source LLMs</li>
<li><a href="https://tavily.com/" rel="nofollow">Tavily</a> for search capabilities</li>
<li><a href="https://jina.ai/" rel="nofollow">Jina</a> for crawl search technology</li>
<li><a href="https://pypi.org/project/browser-use/" rel="nofollow">Browser-use</a> for control browser</li>
<li>And many other open source contributors</li>
</ul>
<p dir="auto">We&#39;re committed to giving back to the community and welcome contributions of all kinds - whether it&#39;s code, documentation, bug reports, or feature suggestions.</p>


<ul dir="auto">
<li><a href="https://github.com/astral-sh/uv">uv</a> package manager</li>
</ul>

<p dir="auto">LangManus leverages <a href="https://github.com/astral-sh/uv">uv</a> as its package manager to streamline dependency management.
Follow the steps below to set up a virtual environment and install the necessary dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Step 1: Create and activate a virtual environment through uv
uv python install 3.12
uv venv --python 3.12

source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Step 2: Install project dependencies
uv sync"><pre><span><span>#</span> Step 1: Create and activate a virtual environment through uv</span>
uv python install 3.12
uv venv --python 3.12

<span>source</span> .venv/bin/activate  <span><span>#</span> On Windows: .venv\Scripts\activate</span>

<span><span>#</span> Step 2: Install project dependencies</span>
uv sync</pre></div>
<p dir="auto">By completing these steps, you&#39;ll ensure your environment is properly configured and ready for development.</p>

<p dir="auto">LangManus uses a three-layer LLM system, which are respectively used for reasoning, basic tasks, and vision-language tasks. Configuration is done using the <code>conf.yaml</code> file in the root directory of the project. You can copy <code>conf.yaml.example</code> to <code>conf.yaml</code> to start the configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cp conf.yaml.example conf.yaml"><pre>cp conf.yaml.example conf.yaml</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)
USE_CONF: true

# LLM Config
## Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples
REASONING_MODEL:
  model: &#34;volcengine/ep-xxxx&#34;
  api_key: $REASONING_API_KEY # Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY
  api_base: $REASONING_BASE_URL

BASIC_MODEL:
  model: &#34;azure/gpt-4o-2024-08-06&#34;
  api_base: $AZURE_API_BASE
  api_version: $AZURE_API_VERSION
  api_key: $AZURE_API_KEY

VISION_MODEL:
  model: &#34;azure/gpt-4o-2024-08-06&#34;
  api_base: $AZURE_API_BASE
  api_version: $AZURE_API_VERSION
  api_key: $AZURE_API_KEY"><pre><span><span>#</span> Setting it to true will read the conf.yaml configuration, and setting it to false will use the original .env configuration. The default is false (compatible with existing configurations)</span>
<span>USE_CONF</span>: <span>true</span>

<span><span>#</span> LLM Config</span>
<span><span>#</span># Follow the litellm configuration parameters: https://docs.litellm.ai/docs/providers. You can click on the specific provider document to view the completion parameter examples</span>
<span>REASONING_MODEL</span>:
  <span>model</span>: <span><span>&#34;</span>volcengine/ep-xxxx<span>&#34;</span></span>
  <span>api_key</span>: <span>$REASONING_API_KEY </span><span><span>#</span> Supports referencing the environment variable ENV_KEY in the.env file through $ENV_KEY</span>
  <span>api_base</span>: <span>$REASONING_BASE_URL</span>

<span>BASIC_MODEL</span>:
  <span>model</span>: <span><span>&#34;</span>azure/gpt-4o-2024-08-06<span>&#34;</span></span>
  <span>api_base</span>: <span>$AZURE_API_BASE</span>
  <span>api_version</span>: <span>$AZURE_API_VERSION</span>
  <span>api_key</span>: <span>$AZURE_API_KEY</span>

<span>VISION_MODEL</span>:
  <span>model</span>: <span><span>&#34;</span>azure/gpt-4o-2024-08-06<span>&#34;</span></span>
  <span>api_base</span>: <span>$AZURE_API_BASE</span>
  <span>api_version</span>: <span>$AZURE_API_VERSION</span>
  <span>api_key</span>: <span>$AZURE_API_KEY</span></pre></div>
<p dir="auto">You can create a .env file in the root directory of the project and configure the following environment variables. You can copy the.env.example file as a template to start:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Tool API Key
TAVILY_API_KEY=your_tavily_api_key
JINA_API_KEY=your_jina_api_key  # Optional

# Browser Configuration
CHROME_INSTANCE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # Optional, the path to the Chrome executable file
CHROME_HEADLESS=False  # Optional, the default is False
CHROME_PROXY_SERVER=http://127.0.0.1:10809  # Optional, the default is None
CHROME_PROXY_USERNAME=  # Optional, the default is None
CHROME_PROXY_PASSWORD=  # Optional, the default is None"><pre><span><span>#</span> Tool API Key</span>
<span>TAVILY_API_KEY</span>=your_tavily_api_key
<span>JINA_API_KEY</span>=your_jina_api_key  <span><span>#</span> Optional</span>

<span><span>#</span> Browser Configuration</span>
<span>CHROME_INSTANCE_PATH</span>=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  <span><span>#</span> Optional, the path to the Chrome executable file</span>
<span>CHROME_HEADLESS</span>=False  <span><span>#</span> Optional, the default is False</span>
<span>CHROME_PROXY_SERVER</span>=http://127.0.0.1:10809  <span><span>#</span> Optional, the default is None</span>
<span>CHROME_PROXY_USERNAME</span>=  <span><span>#</span> Optional, the default is None</span>
<span>CHROME_PROXY_PASSWORD</span>=  <span><span>#</span> Optional, the default is None</span></pre></div>
<blockquote>
<p dir="auto"><strong>Note:</strong></p>
<ul dir="auto">
<li>The system uses different models for different types of tasks:
<ul dir="auto">
<li>The reasoning LLM is used for complex decision-making and analysis.</li>
<li>The basic LLM is used for simple text tasks.</li>
<li>The vision-language LLM is used for tasks involving image understanding.</li>
</ul>
</li>
<li>The configuration of all LLMs can be customized independently.</li>
<li>The Jina API key is optional. Providing your own key can obtain a higher rate limit (you can obtain this key at <a href="https://jina.ai/" rel="nofollow">jina.ai</a>).</li>
<li>The default configuration for Tavily search is to return up to 5 results (you can obtain this key at <a href="https://app.tavily.com/" rel="nofollow">app.tavily.com</a>).</li>
</ul>
</blockquote>
<div dir="auto"><h3 tabindex="-1" dir="auto">Configure Pre-commit Hook</h3><a id="user-content-configure-pre-commit-hook" aria-label="Permalink: Configure Pre-commit Hook" href="#configure-pre-commit-hook"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">LangManus includes a pre-commit hook that runs linting and formatting checks before each commit. To set it up:</p>
<ol dir="auto">
<li>Make the pre-commit script executable:</li>
</ol>

<ol start="2" dir="auto">
<li>Install the pre-commit hook:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="ln -s ../../pre-commit .git/hooks/pre-commit"><pre>ln -s ../../pre-commit .git/hooks/pre-commit</pre></div>
<p dir="auto">The pre-commit hook will automatically:</p>
<ul dir="auto">
<li>Run linting checks (<code>make lint</code>)</li>
<li>Run code formatting (<code>make format</code>)</li>
<li>Add any reformatted files back to staging</li>
<li>Prevent commits if there are any linting or formatting errors</li>
</ul>


<p dir="auto">To run LangManus with default settings:</p>


<p dir="auto">LangManus provides a FastAPI-based API server with streaming support:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start the API server
make serve

# Or run directly
uv run server.py"><pre><span><span>#</span> Start the API server</span>
make serve

<span><span>#</span> Or run directly</span>
uv run server.py</pre></div>
<p dir="auto">The API server exposes the following endpoints:</p>
<ul dir="auto">
<li><code>POST /api/chat/stream</code>: Chat endpoint for LangGraph invoke with streaming support
<ul dir="auto">
<li>Request body:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;messages&#34;: [{ &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Your query here&#34; }],
  &#34;debug&#34;: false
}"><pre>{
  <span>&#34;messages&#34;</span>: [{ <span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>, <span>&#34;content&#34;</span>: <span><span>&#34;</span>Your query here<span>&#34;</span></span> }],
  <span>&#34;debug&#34;</span>: <span>false</span>
}</pre></div>
<ul dir="auto">
<li>Returns a Server-Sent Events (SSE) stream with the agent&#39;s responses</li>
</ul>
</li>
</ul>

<p dir="auto">LangManus can be customized through various configuration files in the <code>src/config</code> directory:</p>
<ul dir="auto">
<li><code>env.py</code>: Configure LLM models, API keys, and base URLs</li>
<li><code>tools.py</code>: Adjust tool-specific settings (e.g., Tavily search results limit)</li>
<li><code>agents.py</code>: Modify team composition and agent system prompts</li>
</ul>

<p dir="auto">LangManus uses a sophisticated prompting system in the <code>src/prompts</code> directory to define agent behaviors and responsibilities:</p>

<ul dir="auto">
<li>
<p dir="auto"><strong>Supervisor (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/supervisor.md"><code>src/prompts/supervisor.md</code></a>)</strong>: Coordinates the team and delegates tasks by analyzing requests and determining which specialist should handle them. Makes decisions about task completion and workflow transitions.</p>
</li>
<li>
<p dir="auto"><strong>Researcher (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/researcher.md"><code>src/prompts/researcher.md</code></a>)</strong>: Specializes in information gathering through web searches and data collection. Uses Tavily search and web crawling capabilities while avoiding mathematical computations or file operations.</p>
</li>
<li>
<p dir="auto"><strong>Coder (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/coder.md"><code>src/prompts/coder.md</code></a>)</strong>: Professional software engineer role focused on Python and bash scripting. Handles:</p>
<ul dir="auto">
<li>Python code execution and analysis</li>
<li>Shell command execution</li>
<li>Technical problem-solving and implementation</li>
</ul>
</li>
<li>
<p dir="auto"><strong>File Manager (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/file_manager.md"><code>src/prompts/file_manager.md</code></a>)</strong>: Handles all file system operations with a focus on properly formatting and saving content in markdown format.</p>
</li>
<li>
<p dir="auto"><strong>Browser (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/browser.md"><code>src/prompts/browser.md</code></a>)</strong>: Web interaction specialist that handles:</p>
<ul dir="auto">
<li>Website navigation</li>
<li>Page interaction (clicking, typing, scrolling)</li>
<li>Content extraction from web pages</li>
</ul>
</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">Prompt System Architecture</h4><a id="user-content-prompt-system-architecture" aria-label="Permalink: Prompt System Architecture" href="#prompt-system-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The prompts system uses a template engine (<a href="https://github.com/langmanus/langmanus/blob/main/src/prompts/template.py"><code>src/prompts/template.py</code></a>) that:</p>
<ul dir="auto">
<li>Loads role-specific markdown templates</li>
<li>Handles variable substitution (e.g., current time, team member information)</li>
<li>Formats system prompts for each agent</li>
</ul>
<p dir="auto">Each agent&#39;s prompt is defined in a separate markdown file, making it easy to modify behavior and responsibilities without changing the underlying code.</p>

<p dir="auto">LangManus can be run in a Docker container. default serve api on port 8000.</p>
<p dir="auto">Before run docker, you need to prepare environment variables in <code>.env</code> file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -t langmanus .
docker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus"><pre>docker build -t langmanus <span>.</span>
docker run --name langmanus -d --env-file .env -e CHROME_HEADLESS=True -p 8000:8000 langmanus</pre></div>
<p dir="auto">You can also just run the cli with docker.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -t langmanus .
docker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py"><pre>docker build -t langmanus <span>.</span>
docker run --rm -it --env-file .env -e CHROME_HEADLESS=True langmanus uv run python main.py</pre></div>

<p dir="auto">LangManus provides a default web UI.</p>
<p dir="auto">Please refer to the <a href="https://github.com/langmanus/langmanus-web">langmanus/langmanus-web-ui</a> project for more details.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Docker Compose (include both backend and frontend)</h2><a id="user-content-docker-compose-include-both-backend-and-frontend" aria-label="Permalink: Docker Compose (include both backend and frontend)" href="#docker-compose-include-both-backend-and-frontend"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">LangManus provides a docker-compose setup to easily run both the backend and frontend together:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start both backend and frontend
docker-compose up -d

# The backend will be available at http://localhost:8000
# The frontend will be available at http://localhost:3000, which could be accessed through web browser"><pre><span><span>#</span> Start both backend and frontend</span>
docker-compose up -d

<span><span>#</span> The backend will be available at http://localhost:8000</span>
<span><span>#</span> The frontend will be available at http://localhost:3000, which could be accessed through web browser</span></pre></div>
<p dir="auto">This will:</p>
<ol dir="auto">
<li>Build and start the LangManus backend container</li>
<li>Build and start the LangManus web UI container</li>
<li>Connect them using a shared network</li>
</ol>
<p dir="auto">** Make sure you have your <code>.env</code> file prepared with the necessary API keys before starting the services. **</p>


<p dir="auto">Run the test suite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run all tests
make test

# Run specific test file
pytest tests/integration/test_workflow.py

# Run with coverage
make coverage"><pre><span><span>#</span> Run all tests</span>
make <span>test</span>

<span><span>#</span> Run specific test file</span>
pytest tests/integration/test_workflow.py

<span><span>#</span> Run with coverage</span>
make coverage</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Run linting
make lint

# Format code
make format"><pre><span><span>#</span> Run linting</span>
make lint

<span><span>#</span> Format code</span>
make format</pre></div>

<p dir="auto">Please refer to the <a href="https://github.com/langmanus/langmanus/blob/main/docs/FAQ.md">FAQ.md</a> for more details.</p>

<p dir="auto">We welcome contributions of all kinds! Whether you&#39;re fixing a typo, improving documentation, or adding a new feature, your help is appreciated. Please see our <a href="https://github.com/langmanus/langmanus/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details on how to get started.</p>

<p dir="auto">This project is open source and available under the <a href="https://github.com/langmanus/langmanus/blob/main/LICENSE">MIT License</a>.</p>

<p dir="auto"><a href="https://www.star-history.com/#langmanus/langmanus&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/d7548b13ba3d8abd8d5730c8b7ed0f60800993548325afaafe70c065473cb5f5/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c616e676d616e75732f6c616e676d616e757326747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=langmanus/langmanus&amp;type=Date"/></a></p>

<p dir="auto">Special thanks to all the open source projects and contributors that make LangManus possible. We stand on the shoulders of giants.</p>
<p dir="auto">In particular, we want to express our deep appreciation for:</p>
<ul dir="auto">
<li><a href="https://github.com/langchain-ai/langchain">LangChain</a> for their exceptional framework that powers our LLM interactions and chains</li>
<li><a href="https://github.com/langchain-ai/langgraph">LangGraph</a> for enabling our sophisticated multi-agent orchestration</li>
<li><a href="https://pypi.org/project/browser-use/" rel="nofollow">Browser-use</a> for control browser</li>
</ul>
<p dir="auto">These amazing projects form the foundation of LangManus and demonstrate the power of open source collaboration.</p>
</article></div></div>
  </body>
</html>
