<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ServiceStack/llms">Original</a>
    <h1>OSS Alternative to Open WebUI – ChatGPT-Like UI, API and CLI</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Lightweight CLI, API and ChatGPT-like alternative to Open WebUI for accessing multiple LLMs, entirely offline, with all data kept private in browser storage.</p>
<p dir="auto">Configure additional providers and models in <a href="https://seinwave.com/ServiceStack/llms/blob/main/llms/llms.json">llms.json</a></p>
<ul dir="auto">
<li>Mix and match local models with models from different API providers</li>
<li>Requests automatically routed to available providers that supports the requested model (in defined order)</li>
<li>Define free/cheapest/local providers first to save on costs</li>
<li>Any failures are automatically retried on the next available provider</li>
</ul>

<ul dir="auto">
<li><strong>Lightweight</strong>: Single <a href="https://github.com/ServiceStack/llms/blob/main/llms/main.py">llms.py</a> Python file with single <code>aiohttp</code> dependency (Pillow optional)</li>
<li><strong>Multi-Provider Support</strong>: OpenRouter, Ollama, Anthropic, Google, OpenAI, Grok, Groq, Qwen, Z.ai, Mistral</li>
<li><strong>OpenAI-Compatible API</strong>: Works with any client that supports OpenAI&#39;s chat completion API</li>
<li><strong>Built-in Analytics</strong>: Built-in analytics UI to visualize costs, requests, and token usage</li>
<li><strong>GitHub OAuth</strong>: Optionally Secure your web UI and restrict access to specified GitHub Users</li>
<li><strong>Configuration Management</strong>: Easy provider enable/disable and configuration management</li>
<li><strong>CLI Interface</strong>: Simple command-line interface for quick interactions</li>
<li><strong>Server Mode</strong>: Run an OpenAI-compatible HTTP server at <code>http://localhost:{PORT}/v1/chat/completions</code></li>
<li><strong>Image Support</strong>: Process images through vision-capable models
<ul dir="auto">
<li>Auto resizes and converts to webp if exceeds configured limits</li>
</ul>
</li>
<li><strong>Audio Support</strong>: Process audio through audio-capable models</li>
<li><strong>Custom Chat Templates</strong>: Configurable chat completion request templates for different modalities</li>
<li><strong>Auto-Discovery</strong>: Automatically discover available Ollama models</li>
<li><strong>Unified Models</strong>: Define custom model names that map to different provider-specific names</li>
<li><strong>Multi-Model Support</strong>: Support for over 160+ different LLMs</li>
</ul>

<p dir="auto">Access all your local all remote LLMs with a single ChatGPT-like UI:</p>
<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow"><img src="https://camo.githubusercontent.com/b15da41dc087a03ed362b120a1691f94aded5aced272ef2f17e389cc29aa3955/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f62672e77656270" alt="" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/bg.webp"/></a></p>

<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow"><img src="https://camo.githubusercontent.com/e877364274f90a1f181161ae6004c3363d7118fb80bba73143c772c82d861042/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f6461726b2d6174746163682d696d6167652e776562703f" alt="" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/dark-attach-image.webp?"/></a></p>

<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow"><img src="https://camo.githubusercontent.com/92f63276a00506f31f2bf49e9b24c74be6886bde117b5a22975cf468711a6480/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f616e616c79746963732d636f7374732e77656270" alt="" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/analytics-costs.webp"/></a></p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Monthly Token Usage (Dark Mode)</h4><a id="user-content-monthly-token-usage-dark-mode" aria-label="Permalink: Monthly Token Usage (Dark Mode)" href="#monthly-token-usage-dark-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow"><img src="https://camo.githubusercontent.com/afceb19fbdb9ec74603e5392c66f4d2158c45c6025759c81b8e25904f07ad25b/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f6461726b2d616e616c79746963732d746f6b656e732e776562703f" alt="" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/dark-analytics-tokens.webp?"/></a></p>

<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow"><img src="https://camo.githubusercontent.com/65f62f32417b3cce0fd3fb28f2537387dee9e570410ab883adcc006641f7d030/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f616e616c79746963732d61637469766974792e77656270" alt="" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/analytics-activity.webp"/></a></p>
<p dir="auto"><a href="https://servicestack.net/posts/llms-py-ui" rel="nofollow">More Features and Screenshots</a>.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Check Provider Reliability and Response Times</h4><a id="user-content-check-provider-reliability-and-response-times" aria-label="Permalink: Check Provider Reliability and Response Times" href="#check-provider-reliability-and-response-times"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Check the status of configured providers to test if they&#39;re configured correctly, reachable and what their response times is for the simplest <code>1+1=</code> request:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Check all models for a provider:
llms --check groq

# Check specific models for a provider:
llms --check groq kimi-k2 llama4:400b gpt-oss:120b"><pre><span><span>#</span> Check all models for a provider:</span>
llms --check groq

<span><span>#</span> Check specific models for a provider:</span>
llms --check groq kimi-k2 llama4:400b gpt-oss:120b</pre></div>
<p dir="auto"><a href="https://servicestack.net/img/posts/llms-py-ui/llms-check.webp" rel="nofollow"><img src="https://camo.githubusercontent.com/9d0eb09ed2372f6b08c90b4e91fef06897b5462f57b43a5a09a32e727b75c410/68747470733a2f2f73657276696365737461636b2e6e65742f696d672f706f7374732f6c6c6d732d70792d75692f6c6c6d732d636865636b2e77656270" alt="llms-check.webp" data-canonical-src="https://servicestack.net/img/posts/llms-py-ui/llms-check.webp"/></a></p>
<p dir="auto">As they&#39;re a good indicator for the reliability and speed you can expect from different providers we&#39;ve created a
<a href="https://github.com/ServiceStack/llms/actions/workflows/test-providers.yml">test-providers.yml</a> GitHub Action to
test the response times for all configured providers and models, the results of which will be frequently published to
<a href="https://github.com/ServiceStack/llms/blob/main/docs/checks/latest.txt">/checks/latest.txt</a></p>


<ul dir="auto">
<li>Improved Responsive Layout with collapsible Sidebar</li>
<li>Watching config files for changes and auto-reloading</li>
<li>Add cancel button to cancel pending request</li>
<li>Return focus to textarea after request completes</li>
<li>Clicking outside model or system prompt selector will collapse it</li>
<li>Clicking on selected item no longer deselects it</li>
<li>Support <code>VERBOSE=1</code> for enabling <code>--verbose</code> mode (useful in Docker)</li>
</ul>

<ul dir="auto">
<li>Dark Mode</li>
<li>Drag n&#39; Drop files in Message prompt</li>
<li>Copy &amp; Paste files in Message prompt</li>
<li>Support for GitHub OAuth and optional restrict access to specified Users</li>
<li>Support for Docker and Docker Compose</li>
</ul>
<p dir="auto"><a href="https://github.com/ServiceStack/llms/releases">llms.py Releases</a></p>



<ul dir="auto">
<li><a href="#using-docker">Using Docker</a></li>
</ul>


<p dir="auto">Set environment variables for the providers you want to use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENROUTER_API_KEY=&#34;...&#34;"><pre><span>export</span> OPENROUTER_API_KEY=<span><span>&#34;</span>...<span>&#34;</span></span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Provider</th>
<th>Variable</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>openrouter_free</td>
<td><code>OPENROUTER_API_KEY</code></td>
<td>OpenRouter FREE models API key</td>
<td><code>sk-or-...</code></td>
</tr>
<tr>
<td>groq</td>
<td><code>GROQ_API_KEY</code></td>
<td>Groq API key</td>
<td><code>gsk_...</code></td>
</tr>
<tr>
<td>google_free</td>
<td><code>GOOGLE_FREE_API_KEY</code></td>
<td>Google FREE API key</td>
<td><code>AIza...</code></td>
</tr>
<tr>
<td>codestral</td>
<td><code>CODESTRAL_API_KEY</code></td>
<td>Codestral API key</td>
<td><code>...</code></td>
</tr>
<tr>
<td>ollama</td>
<td>N/A</td>
<td>No API key required</td>
<td></td>
</tr>
<tr>
<td>openrouter</td>
<td><code>OPENROUTER_API_KEY</code></td>
<td>OpenRouter API key</td>
<td><code>sk-or-...</code></td>
</tr>
<tr>
<td>google</td>
<td><code>GOOGLE_API_KEY</code></td>
<td>Google API key</td>
<td><code>AIza...</code></td>
</tr>
<tr>
<td>anthropic</td>
<td><code>ANTHROPIC_API_KEY</code></td>
<td>Anthropic API key</td>
<td><code>sk-ant-...</code></td>
</tr>
<tr>
<td>openai</td>
<td><code>OPENAI_API_KEY</code></td>
<td>OpenAI API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>grok</td>
<td><code>GROK_API_KEY</code></td>
<td>Grok (X.AI) API key</td>
<td><code>xai-...</code></td>
</tr>
<tr>
<td>qwen</td>
<td><code>DASHSCOPE_API_KEY</code></td>
<td>Qwen (Alibaba) API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>z.ai</td>
<td><code>ZAI_API_KEY</code></td>
<td>Z.ai API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>mistral</td>
<td><code>MISTRAL_API_KEY</code></td>
<td>Mistral API key</td>
<td><code>...</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Start the UI and an OpenAI compatible API on port <strong>8000</strong>:</p>

<p dir="auto">Launches UI at <code>http://localhost:8000</code> and OpenAI Endpoint at <code>http://localhost:8000/v1/chat/completions</code>.</p>
<p dir="auto">To see detailed request/response logging, add <code>--verbose</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="llms --serve 8000 --verbose"><pre>llms --serve 8000 --verbose</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="llms &#34;What is the capital of France?&#34;"><pre>llms <span><span>&#34;</span>What is the capital of France?<span>&#34;</span></span></pre></div>

<p dir="auto">Any providers that have their API Keys set and enabled in <code>llms.json</code> are automatically made available.</p>
<p dir="auto">Providers can be enabled or disabled in the UI at runtime next to the model selector, or on the command line:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Disable free providers with free models and free tiers
llms --disable openrouter_free codestral google_free groq

# Enable paid providers
llms --enable openrouter anthropic google openai grok z.ai qwen mistral"><pre><span><span>#</span> Disable free providers with free models and free tiers</span>
llms --disable openrouter_free codestral google_free groq

<span><span>#</span> Enable paid providers</span>
llms --enable openrouter anthropic google openai grok z.ai qwen mistral</pre></div>

<div dir="auto"><h4 tabindex="-1" dir="auto">a) Simple - Run in a Docker container:</h4><a id="user-content-a-simple---run-in-a-docker-container" aria-label="Permalink: a) Simple - Run in a Docker container:" href="#a-simple---run-in-a-docker-container"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Run the server on port <code>8000</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY ghcr.io/servicestack/llms:latest"><pre>docker run -p 8000:8000 -e GROQ_API_KEY=<span>$GROQ_API_KEY</span> ghcr.io/servicestack/llms:latest</pre></div>
<p dir="auto">Get the latest version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull ghcr.io/servicestack/llms:latest"><pre>docker pull ghcr.io/servicestack/llms:latest</pre></div>
<p dir="auto">Use custom <code>llms.json</code> and <code>ui.json</code> config files outside of the container (auto created if they don&#39;t exist):</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8000:8000 -e GROQ_API_KEY=$GROQ_API_KEY \
  -v ~/.llms:/home/llms/.llms \
  ghcr.io/servicestack/llms:latest"><pre>docker run -p 8000:8000 -e GROQ_API_KEY=<span>$GROQ_API_KEY</span> \
  -v <span>~</span>/.llms:/home/llms/.llms \
  ghcr.io/servicestack/llms:latest</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">b) Recommended - Use Docker Compose:</h4><a id="user-content-b-recommended---use-docker-compose" aria-label="Permalink: b) Recommended - Use Docker Compose:" href="#b-recommended---use-docker-compose"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Download and use <a href="https://raw.githubusercontent.com/ServiceStack/llms/refs/heads/main/docker-compose.yml" rel="nofollow">docker-compose.yml</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -O https://raw.githubusercontent.com/ServiceStack/llms/refs/heads/main/docker-compose.yml"><pre>curl -O https://raw.githubusercontent.com/ServiceStack/llms/refs/heads/main/docker-compose.yml</pre></div>
<p dir="auto">Update API Keys in <code>docker-compose.yml</code> then start the server:</p>

<div dir="auto"><h4 tabindex="-1" dir="auto">c) Build and run local Docker image from source:</h4><a id="user-content-c-build-and-run-local-docker-image-from-source" aria-label="Permalink: c) Build and run local Docker image from source:" href="#c-build-and-run-local-docker-image-from-source"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/ServiceStack/llms

docker-compose -f docker-compose.local.yml up -d --build"><pre>git clone https://github.com/ServiceStack/llms

docker-compose -f docker-compose.local.yml up -d --build</pre></div>
<p dir="auto">After the container starts, you can access the UI and API at <code>http://localhost:8000</code>.</p>
<p dir="auto">See <a href="https://seinwave.com/ServiceStack/llms/blob/main/DOCKER.md">DOCKER.md</a> for detailed instructions on customizing configuration files.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">GitHub OAuth Authentication</h2><a id="user-content-github-oauth-authentication" aria-label="Permalink: GitHub OAuth Authentication" href="#github-oauth-authentication"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">llms.py supports optional GitHub OAuth authentication to secure your web UI and API endpoints. When enabled, users must sign in with their GitHub account before accessing the application.</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;auth&#34;: {
        &#34;enabled&#34;: true,
        &#34;github&#34;: {
            &#34;client_id&#34;: &#34;$GITHUB_CLIENT_ID&#34;,
            &#34;client_secret&#34;: &#34;$GITHUB_CLIENT_SECRET&#34;,
            &#34;redirect_uri&#34;: &#34;http://localhost:8000/auth/github/callback&#34;,
            &#34;restrict_to&#34;: &#34;$GITHUB_USERS&#34;
        }
    }
}"><pre>{
    <span>&#34;auth&#34;</span>: {
        <span>&#34;enabled&#34;</span>: <span>true</span>,
        <span>&#34;github&#34;</span>: {
            <span>&#34;client_id&#34;</span>: <span><span>&#34;</span>$GITHUB_CLIENT_ID<span>&#34;</span></span>,
            <span>&#34;client_secret&#34;</span>: <span><span>&#34;</span>$GITHUB_CLIENT_SECRET<span>&#34;</span></span>,
            <span>&#34;redirect_uri&#34;</span>: <span><span>&#34;</span>http://localhost:8000/auth/github/callback<span>&#34;</span></span>,
            <span>&#34;restrict_to&#34;</span>: <span><span>&#34;</span>$GITHUB_USERS<span>&#34;</span></span>
        }
    }
}</pre></div>
<p dir="auto"><code>GITHUB_USERS</code> is optional but if set will only allow access to the specified users.</p>
<p dir="auto">See <a href="https://seinwave.com/ServiceStack/llms/blob/main/GITHUB_OAUTH_SETUP.md">GITHUB_OAUTH_SETUP.md</a> for detailed setup instructions.</p>

<p dir="auto">The configuration file <a href="https://seinwave.com/ServiceStack/llms/blob/main/llms/llms.json">llms.json</a> is saved to <code>~/.llms/llms.json</code> and defines available providers, models, and default settings. If it doesn&#39;t exist, <code>llms.json</code> is auto created with the latest
configuration, so you can re-create it by deleting your local config (e.g. <code>rm -rf ~/.llms</code>).</p>
<p dir="auto">Key sections:</p>

<ul dir="auto">
<li><code>headers</code>: Common HTTP headers for all requests</li>
<li><code>text</code>: Default chat completion request template for text prompts</li>
<li><code>image</code>: Default chat completion request template for image prompts</li>
<li><code>audio</code>: Default chat completion request template for audio prompts</li>
<li><code>file</code>: Default chat completion request template for file prompts</li>
<li><code>check</code>: Check request template for testing provider connectivity</li>
<li><code>limits</code>: Override Request size limits</li>
<li><code>convert</code>: Max image size and length limits and auto conversion settings</li>
</ul>

<p dir="auto">Each provider configuration includes:</p>
<ul dir="auto">
<li><code>enabled</code>: Whether the provider is active</li>
<li><code>type</code>: Provider class (OpenAiProvider, GoogleProvider, etc.)</li>
<li><code>api_key</code>: API key (supports environment variables with <code>$VAR_NAME</code>)</li>
<li><code>base_url</code>: API endpoint URL</li>
<li><code>models</code>: Model name mappings (local name → provider name)</li>
<li><code>pricing</code>: Pricing per token (input/output) for each model</li>
<li><code>default_pricing</code>: Default pricing if not specified in <code>pricing</code></li>
<li><code>check</code>: Check request template for testing provider connectivity</li>
</ul>


<div dir="auto" data-snippet-clipboard-copy-content="# Simple question
llms &#34;Explain quantum computing&#34;

# With specific model
llms -m gemini-2.5-pro &#34;Write a Python function to sort a list&#34;
llms -m grok-4 &#34;Explain this code with humor&#34;
llms -m qwen3-max &#34;Translate this to Chinese&#34;

# With system prompt
llms -s &#34;You are a helpful coding assistant&#34; &#34;How do I reverse a string in Python?&#34;

# With image (vision models)
llms --image image.jpg &#34;What&#39;s in this image?&#34;
llms --image https://example.com/photo.png &#34;Describe this photo&#34;

# Display full JSON Response
llms &#34;Explain quantum computing&#34; --raw"><pre><span><span>#</span> Simple question</span>
llms <span><span>&#34;</span>Explain quantum computing<span>&#34;</span></span>

<span><span>#</span> With specific model</span>
llms -m gemini-2.5-pro <span><span>&#34;</span>Write a Python function to sort a list<span>&#34;</span></span>
llms -m grok-4 <span><span>&#34;</span>Explain this code with humor<span>&#34;</span></span>
llms -m qwen3-max <span><span>&#34;</span>Translate this to Chinese<span>&#34;</span></span>

<span><span>#</span> With system prompt</span>
llms -s <span><span>&#34;</span>You are a helpful coding assistant<span>&#34;</span></span> <span><span>&#34;</span>How do I reverse a string in Python?<span>&#34;</span></span>

<span><span>#</span> With image (vision models)</span>
llms --image image.jpg <span><span>&#34;</span>What&#39;s in this image?<span>&#34;</span></span>
llms --image https://example.com/photo.png <span><span>&#34;</span>Describe this photo<span>&#34;</span></span>

<span><span>#</span> Display full JSON Response</span>
llms <span><span>&#34;</span>Explain quantum computing<span>&#34;</span></span> --raw</pre></div>

<p dir="auto">By default llms uses the <code>defaults/text</code> chat completion request defined in <a href="https://seinwave.com/ServiceStack/llms/blob/main/llms/llms.json">llms.json</a>.</p>
<p dir="auto">You can instead use a custom chat completion request with <code>--chat</code>, e.g:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Load chat completion request from JSON file
llms --chat request.json

# Override user message
llms --chat request.json &#34;New user message&#34;

# Override model
llms -m kimi-k2 --chat request.json"><pre><span><span>#</span> Load chat completion request from JSON file</span>
llms --chat request.json

<span><span>#</span> Override user message</span>
llms --chat request.json <span><span>&#34;</span>New user message<span>&#34;</span></span>

<span><span>#</span> Override model</span>
llms -m kimi-k2 --chat request.json</pre></div>
<p dir="auto">Example <code>request.json</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;model&#34;: &#34;kimi-k2&#34;,
  &#34;messages&#34;: [
    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;},
    {&#34;role&#34;: &#34;user&#34;,   &#34;content&#34;: &#34;&#34;}
  ],
  &#34;temperature&#34;: 0.7,
  &#34;max_tokens&#34;: 150
}"><pre>{
  <span>&#34;model&#34;</span>: <span><span>&#34;</span>kimi-k2<span>&#34;</span></span>,
  <span>&#34;messages&#34;</span>: [
    {<span>&#34;role&#34;</span>: <span><span>&#34;</span>system<span>&#34;</span></span>, <span>&#34;content&#34;</span>: <span><span>&#34;</span>You are a helpful assistant.<span>&#34;</span></span>},
    {<span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>,   <span>&#34;content&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>}
  ],
  <span>&#34;temperature&#34;</span>: <span>0.7</span>,
  <span>&#34;max_tokens&#34;</span>: <span>150</span>
}</pre></div>

<p dir="auto">Send images to vision-capable models using the <code>--image</code> option:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Use defaults/image Chat Template (Describe the key features of the input image)
llms --image ./screenshot.png

# Local image file
llms --image ./screenshot.png &#34;What&#39;s in this image?&#34;

# Remote image URL
llms --image https://example.org/photo.jpg &#34;Describe this photo&#34;

# Data URI
llms --image &#34;data:image/png;base64,$(base64 -w 0 image.png)&#34; &#34;Describe this image&#34;

# With a specific vision model
llms -m gemini-2.5-flash --image chart.png &#34;Analyze this chart&#34;
llms -m qwen2.5vl --image document.jpg &#34;Extract text from this document&#34;

# Combined with system prompt
llms -s &#34;You are a data analyst&#34; --image graph.png &#34;What trends do you see?&#34;

# With custom chat template
llms --chat image-request.json --image photo.jpg"><pre><span><span>#</span> Use defaults/image Chat Template (Describe the key features of the input image)</span>
llms --image ./screenshot.png

<span><span>#</span> Local image file</span>
llms --image ./screenshot.png <span><span>&#34;</span>What&#39;s in this image?<span>&#34;</span></span>

<span><span>#</span> Remote image URL</span>
llms --image https://example.org/photo.jpg <span><span>&#34;</span>Describe this photo<span>&#34;</span></span>

<span><span>#</span> Data URI</span>
llms --image <span><span>&#34;</span>data:image/png;base64,<span><span>$(</span>base64 -w 0 image.png<span>)</span></span><span>&#34;</span></span> <span><span>&#34;</span>Describe this image<span>&#34;</span></span>

<span><span>#</span> With a specific vision model</span>
llms -m gemini-2.5-flash --image chart.png <span><span>&#34;</span>Analyze this chart<span>&#34;</span></span>
llms -m qwen2.5vl --image document.jpg <span><span>&#34;</span>Extract text from this document<span>&#34;</span></span>

<span><span>#</span> Combined with system prompt</span>
llms -s <span><span>&#34;</span>You are a data analyst<span>&#34;</span></span> --image graph.png <span><span>&#34;</span>What trends do you see?<span>&#34;</span></span>

<span><span>#</span> With custom chat template</span>
llms --chat image-request.json --image photo.jpg</pre></div>
<p dir="auto">Example of <code>image-request.json</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;model&#34;: &#34;qwen2.5vl&#34;,
    &#34;messages&#34;: [
        {
            &#34;role&#34;: &#34;user&#34;,
            &#34;content&#34;: [
                {
                    &#34;type&#34;: &#34;image_url&#34;,
                    &#34;image_url&#34;: {
                        &#34;url&#34;: &#34;&#34;
                    }
                },
                {
                    &#34;type&#34;: &#34;text&#34;,
                    &#34;text&#34;: &#34;Caption this image&#34;
                }
            ]
        }
    ]
}"><pre>{
    <span>&#34;model&#34;</span>: <span><span>&#34;</span>qwen2.5vl<span>&#34;</span></span>,
    <span>&#34;messages&#34;</span>: [
        {
            <span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>,
            <span>&#34;content&#34;</span>: [
                {
                    <span>&#34;type&#34;</span>: <span><span>&#34;</span>image_url<span>&#34;</span></span>,
                    <span>&#34;image_url&#34;</span>: {
                        <span>&#34;url&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>
                    }
                },
                {
                    <span>&#34;type&#34;</span>: <span><span>&#34;</span>text<span>&#34;</span></span>,
                    <span>&#34;text&#34;</span>: <span><span>&#34;</span>Caption this image<span>&#34;</span></span>
                }
            ]
        }
    ]
}</pre></div>
<p dir="auto"><strong>Supported image formats</strong>: PNG, WEBP, JPG, JPEG, GIF, BMP, TIFF, ICO</p>
<p dir="auto"><strong>Image sources</strong>:</p>
<ul dir="auto">
<li><strong>Local files</strong>: Absolute paths (<code>/path/to/image.jpg</code>) or relative paths (<code>./image.png</code>, <code>../image.jpg</code>)</li>
<li><strong>Remote URLs</strong>: HTTP/HTTPS URLs are automatically downloaded</li>
<li><strong>Data URIs</strong>: Base64-encoded images (<code>data:image/png;base64,...</code>)</li>
</ul>
<p dir="auto">Images are automatically processed and converted to base64 data URIs before being sent to the model.</p>

<p dir="auto">Popular models that support image analysis:</p>
<ul dir="auto">
<li><strong>OpenAI</strong>: GPT-4o, GPT-4o-mini, GPT-4.1</li>
<li><strong>Anthropic</strong>: Claude Sonnet 4.0, Claude Opus 4.1</li>
<li><strong>Google</strong>: Gemini 2.5 Pro, Gemini Flash</li>
<li><strong>Qwen</strong>: Qwen2.5-VL, Qwen3-VL, QVQ-max</li>
<li><strong>Ollama</strong>: qwen2.5vl, llava</li>
</ul>
<p dir="auto">Images are automatically downloaded and converted to base64 data URIs.</p>

<p dir="auto">Send audio files to audio-capable models using the <code>--audio</code> option:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Use defaults/audio Chat Template (Transcribe the audio)
llms --audio ./recording.mp3

# Local audio file
llms --audio ./meeting.wav &#34;Summarize this meeting recording&#34;

# Remote audio URL
llms --audio https://example.org/podcast.mp3 &#34;What are the key points discussed?&#34;

# With a specific audio model
llms -m gpt-4o-audio-preview --audio interview.mp3 &#34;Extract the main topics&#34;
llms -m gemini-2.5-flash --audio interview.mp3 &#34;Extract the main topics&#34;

# Combined with system prompt
llms -s &#34;You&#39;re a transcription specialist&#34; --audio talk.mp3 &#34;Provide a detailed transcript&#34;

# With custom chat template
llms --chat audio-request.json --audio speech.wav"><pre><span><span>#</span> Use defaults/audio Chat Template (Transcribe the audio)</span>
llms --audio ./recording.mp3

<span><span>#</span> Local audio file</span>
llms --audio ./meeting.wav <span><span>&#34;</span>Summarize this meeting recording<span>&#34;</span></span>

<span><span>#</span> Remote audio URL</span>
llms --audio https://example.org/podcast.mp3 <span><span>&#34;</span>What are the key points discussed?<span>&#34;</span></span>

<span><span>#</span> With a specific audio model</span>
llms -m gpt-4o-audio-preview --audio interview.mp3 <span><span>&#34;</span>Extract the main topics<span>&#34;</span></span>
llms -m gemini-2.5-flash --audio interview.mp3 <span><span>&#34;</span>Extract the main topics<span>&#34;</span></span>

<span><span>#</span> Combined with system prompt</span>
llms -s <span><span>&#34;</span>You&#39;re a transcription specialist<span>&#34;</span></span> --audio talk.mp3 <span><span>&#34;</span>Provide a detailed transcript<span>&#34;</span></span>

<span><span>#</span> With custom chat template</span>
llms --chat audio-request.json --audio speech.wav</pre></div>
<p dir="auto">Example of <code>audio-request.json</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;model&#34;: &#34;gpt-4o-audio-preview&#34;,
    &#34;messages&#34;: [
        {
            &#34;role&#34;: &#34;user&#34;,
            &#34;content&#34;: [
                {
                    &#34;type&#34;: &#34;input_audio&#34;,
                    &#34;input_audio&#34;: {
                        &#34;data&#34;: &#34;&#34;,
                        &#34;format&#34;: &#34;mp3&#34;
                    }
                },
                {
                    &#34;type&#34;: &#34;text&#34;,
                    &#34;text&#34;: &#34;Please transcribe this audio&#34;
                }
            ]
        }
    ]
}"><pre>{
    <span>&#34;model&#34;</span>: <span><span>&#34;</span>gpt-4o-audio-preview<span>&#34;</span></span>,
    <span>&#34;messages&#34;</span>: [
        {
            <span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>,
            <span>&#34;content&#34;</span>: [
                {
                    <span>&#34;type&#34;</span>: <span><span>&#34;</span>input_audio<span>&#34;</span></span>,
                    <span>&#34;input_audio&#34;</span>: {
                        <span>&#34;data&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>,
                        <span>&#34;format&#34;</span>: <span><span>&#34;</span>mp3<span>&#34;</span></span>
                    }
                },
                {
                    <span>&#34;type&#34;</span>: <span><span>&#34;</span>text<span>&#34;</span></span>,
                    <span>&#34;text&#34;</span>: <span><span>&#34;</span>Please transcribe this audio<span>&#34;</span></span>
                }
            ]
        }
    ]
}</pre></div>
<p dir="auto"><strong>Supported audio formats</strong>: MP3, WAV</p>
<p dir="auto"><strong>Audio sources</strong>:</p>
<ul dir="auto">
<li><strong>Local files</strong>: Absolute paths (<code>/path/to/audio.mp3</code>) or relative paths (<code>./audio.wav</code>, <code>../recording.m4a</code>)</li>
<li><strong>Remote URLs</strong>: HTTP/HTTPS URLs are automatically downloaded</li>
<li><strong>Base64 Data</strong>: Base64-encoded audio</li>
</ul>
<p dir="auto">Audio files are automatically processed and converted to base64 data before being sent to the model.</p>

<p dir="auto">Popular models that support audio processing:</p>
<ul dir="auto">
<li><strong>OpenAI</strong>: gpt-4o-audio-preview</li>
<li><strong>Google</strong>: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite</li>
</ul>
<p dir="auto">Audio files are automatically downloaded and converted to base64 data URIs with appropriate format detection.</p>

<p dir="auto">Send documents (e.g. PDFs) to file-capable models using the <code>--file</code> option:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Use defaults/file Chat Template (Summarize the document)
llms --file ./docs/handbook.pdf

# Local PDF file
llms --file ./docs/policy.pdf &#34;Summarize the key changes&#34;

# Remote PDF URL
llms --file https://example.org/whitepaper.pdf &#34;What are the main findings?&#34;

# With specific file-capable models
llms -m gpt-5               --file ./policy.pdf   &#34;Summarize the key changes&#34;
llms -m gemini-flash-latest --file ./report.pdf   &#34;Extract action items&#34;
llms -m qwen2.5vl           --file ./manual.pdf   &#34;List key sections and their purpose&#34;

# Combined with system prompt
llms -s &#34;You&#39;re a compliance analyst&#34; --file ./policy.pdf &#34;Identify compliance risks&#34;

# With custom chat template
llms --chat file-request.json --file ./docs/handbook.pdf"><pre><span><span>#</span> Use defaults/file Chat Template (Summarize the document)</span>
llms --file ./docs/handbook.pdf

<span><span>#</span> Local PDF file</span>
llms --file ./docs/policy.pdf <span><span>&#34;</span>Summarize the key changes<span>&#34;</span></span>

<span><span>#</span> Remote PDF URL</span>
llms --file https://example.org/whitepaper.pdf <span><span>&#34;</span>What are the main findings?<span>&#34;</span></span>

<span><span>#</span> With specific file-capable models</span>
llms -m gpt-5               --file ./policy.pdf   <span><span>&#34;</span>Summarize the key changes<span>&#34;</span></span>
llms -m gemini-flash-latest --file ./report.pdf   <span><span>&#34;</span>Extract action items<span>&#34;</span></span>
llms -m qwen2.5vl           --file ./manual.pdf   <span><span>&#34;</span>List key sections and their purpose<span>&#34;</span></span>

<span><span>#</span> Combined with system prompt</span>
llms -s <span><span>&#34;</span>You&#39;re a compliance analyst<span>&#34;</span></span> --file ./policy.pdf <span><span>&#34;</span>Identify compliance risks<span>&#34;</span></span>

<span><span>#</span> With custom chat template</span>
llms --chat file-request.json --file ./docs/handbook.pdf</pre></div>
<p dir="auto">Example of <code>file-request.json</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;model&#34;: &#34;gpt-5&#34;,
  &#34;messages&#34;: [
    {
      &#34;role&#34;: &#34;user&#34;,
      &#34;content&#34;: [
        {
          &#34;type&#34;: &#34;file&#34;,
          &#34;file&#34;: {
            &#34;filename&#34;: &#34;&#34;,
            &#34;file_data&#34;: &#34;&#34;
          }
        },
        {
          &#34;type&#34;: &#34;text&#34;,
          &#34;text&#34;: &#34;Please summarize this document&#34;
        }
      ]
    }
  ]
}"><pre>{
  <span>&#34;model&#34;</span>: <span><span>&#34;</span>gpt-5<span>&#34;</span></span>,
  <span>&#34;messages&#34;</span>: [
    {
      <span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>,
      <span>&#34;content&#34;</span>: [
        {
          <span>&#34;type&#34;</span>: <span><span>&#34;</span>file<span>&#34;</span></span>,
          <span>&#34;file&#34;</span>: {
            <span>&#34;filename&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>,
            <span>&#34;file_data&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>
          }
        },
        {
          <span>&#34;type&#34;</span>: <span><span>&#34;</span>text<span>&#34;</span></span>,
          <span>&#34;text&#34;</span>: <span><span>&#34;</span>Please summarize this document<span>&#34;</span></span>
        }
      ]
    }
  ]
}</pre></div>
<p dir="auto"><strong>Supported file formats</strong>: PDF</p>
<p dir="auto">Other document types may work depending on the model/provider.</p>
<p dir="auto"><strong>File sources</strong>:</p>
<ul dir="auto">
<li><strong>Local files</strong>: Absolute paths (<code>/path/to/file.pdf</code>) or relative paths (<code>./file.pdf</code>, <code>../file.pdf</code>)</li>
<li><strong>Remote URLs</strong>: HTTP/HTTPS URLs are automatically downloaded</li>
<li><strong>Base64/Data URIs</strong>: Inline <code>data:application/pdf;base64,...</code> is supported</li>
</ul>
<p dir="auto">Files are automatically downloaded (for URLs) and converted to base64 data URIs before being sent to the model.</p>

<p dir="auto">Popular multi-modal models that support file (PDF) inputs:</p>
<ul dir="auto">
<li>OpenAI: gpt-5, gpt-5-mini, gpt-4o, gpt-4o-mini</li>
<li>Google: gemini-flash-latest, gemini-2.5-flash-lite</li>
<li>Grok: grok-4-fast (OpenRouter)</li>
<li>Qwen: qwen2.5vl, qwen3-max, qwen3-vl:235b, qwen3-coder, qwen3-coder-flash (OpenRouter)</li>
<li>Others: kimi-k2, glm-4.5-air, deepseek-v3.1:671b, llama4:400b, llama3.3:70b, mai-ds-r1, nemotron-nano:9b</li>
</ul>

<p dir="auto">Run as an OpenAI-compatible HTTP server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start server on port 8000
llms --serve 8000"><pre><span><span>#</span> Start server on port 8000</span>
llms --serve 8000</pre></div>
<p dir="auto">The server exposes a single endpoint:</p>
<ul dir="auto">
<li><code>POST /v1/chat/completions</code> - OpenAI-compatible chat completions</li>
</ul>
<p dir="auto">Example client usage:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X POST http://localhost:8000/v1/chat/completions \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{
    &#34;model&#34;: &#34;kimi-k2&#34;,
    &#34;messages&#34;: [
      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}
    ]
  }&#39;"><pre>curl -X POST http://localhost:8000/v1/chat/completions \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -d <span><span>&#39;</span>{</span>
<span>    &#34;model&#34;: &#34;kimi-k2&#34;,</span>
<span>    &#34;messages&#34;: [</span>
<span>      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}</span>
<span>    ]</span>
<span>  }<span>&#39;</span></span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# List enabled providers and models
llms --list
llms ls

# List specific providers
llms ls ollama
llms ls google anthropic

# Enable providers
llms --enable openrouter
llms --enable anthropic google_free groq

# Disable providers
llms --disable ollama
llms --disable openai anthropic

# Set default model
llms --default grok-4"><pre><span><span>#</span> List enabled providers and models</span>
llms --list
llms ls

<span><span>#</span> List specific providers</span>
llms ls ollama
llms ls google anthropic

<span><span>#</span> Enable providers</span>
llms --enable openrouter
llms --enable anthropic google_free groq

<span><span>#</span> Disable providers</span>
llms --disable ollama
llms --disable openai anthropic

<span><span>#</span> Set default model</span>
llms --default grok-4</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="pip install llms-py --upgrade"><pre>pip install llms-py --upgrade</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Use custom config file
llms --config /path/to/config.json &#34;Hello&#34;

# Get raw JSON response
llms --raw &#34;What is 2+2?&#34;

# Enable verbose logging
llms --verbose &#34;Tell me a joke&#34;

# Custom log prefix
llms --verbose --logprefix &#34;[DEBUG] &#34; &#34;Hello world&#34;

# Set default model (updates config file)
llms --default grok-4

# Pass custom parameters to chat request (URL-encoded)
llms --args &#34;temperature=0.7&amp;seed=111&#34; &#34;What is 2+2?&#34;

# Multiple parameters with different types
llms --args &#34;temperature=0.5&amp;max_completion_tokens=50&#34; &#34;Tell me a joke&#34;

# URL-encoded special characters (stop sequences)
llms --args &#34;stop=Two,Words&#34; &#34;Count to 5&#34;

# Combine with other options
llms --system &#34;You are helpful&#34; --args &#34;temperature=0.3&#34; --raw &#34;Hello&#34;"><pre><span><span>#</span> Use custom config file</span>
llms --config /path/to/config.json <span><span>&#34;</span>Hello<span>&#34;</span></span>

<span><span>#</span> Get raw JSON response</span>
llms --raw <span><span>&#34;</span>What is 2+2?<span>&#34;</span></span>

<span><span>#</span> Enable verbose logging</span>
llms --verbose <span><span>&#34;</span>Tell me a joke<span>&#34;</span></span>

<span><span>#</span> Custom log prefix</span>
llms --verbose --logprefix <span><span>&#34;</span>[DEBUG] <span>&#34;</span></span> <span><span>&#34;</span>Hello world<span>&#34;</span></span>

<span><span>#</span> Set default model (updates config file)</span>
llms --default grok-4

<span><span>#</span> Pass custom parameters to chat request (URL-encoded)</span>
llms --args <span><span>&#34;</span>temperature=0.7&amp;seed=111<span>&#34;</span></span> <span><span>&#34;</span>What is 2+2?<span>&#34;</span></span>

<span><span>#</span> Multiple parameters with different types</span>
llms --args <span><span>&#34;</span>temperature=0.5&amp;max_completion_tokens=50<span>&#34;</span></span> <span><span>&#34;</span>Tell me a joke<span>&#34;</span></span>

<span><span>#</span> URL-encoded special characters (stop sequences)</span>
llms --args <span><span>&#34;</span>stop=Two,Words<span>&#34;</span></span> <span><span>&#34;</span>Count to 5<span>&#34;</span></span>

<span><span>#</span> Combine with other options</span>
llms --system <span><span>&#34;</span>You are helpful<span>&#34;</span></span> --args <span><span>&#34;</span>temperature=0.3<span>&#34;</span></span> --raw <span><span>&#34;</span>Hello<span>&#34;</span></span></pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Custom Parameters with <code>--args</code></h4><a id="user-content-custom-parameters-with---args" aria-label="Permalink: Custom Parameters with --args" href="#custom-parameters-with---args"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>--args</code> option allows you to pass URL-encoded parameters to customize the chat request sent to LLM providers:</p>
<p dir="auto"><strong>Parameter Types:</strong></p>
<ul dir="auto">
<li><strong>Floats</strong>: <code>temperature=0.7</code>, <code>frequency_penalty=0.2</code></li>
<li><strong>Integers</strong>: <code>max_completion_tokens=100</code></li>
<li><strong>Booleans</strong>: <code>store=true</code>, <code>verbose=false</code>, <code>logprobs=true</code></li>
<li><strong>Strings</strong>: <code>stop=one</code></li>
<li><strong>Lists</strong>: <code>stop=two,words</code></li>
</ul>
<p dir="auto"><strong>Common Parameters:</strong></p>
<ul dir="auto">
<li><code>temperature</code>: Controls randomness (0.0 to 2.0)</li>
<li><code>max_completion_tokens</code>: Maximum tokens in response</li>
<li><code>seed</code>: For reproducible outputs</li>
<li><code>top_p</code>: Nucleus sampling parameter</li>
<li><code>stop</code>: Stop sequences (URL-encode special chars)</li>
<li><code>store</code>: Whether or not to store the output</li>
<li><code>frequency_penalty</code>: Penalize new tokens based on frequency</li>
<li><code>presence_penalty</code>: Penalize new tokens based on presence</li>
<li><code>logprobs</code>: Include log probabilities in response</li>
<li><code>parallel_tool_calls</code>: Enable parallel tool calls</li>
<li><code>prompt_cache_key</code>: Cache key for prompt</li>
<li><code>reasoning_effort</code>: Reasoning effort (low, medium, high, *minimal, *none, *default)</li>
<li><code>safety_identifier</code>: A string that uniquely identifies each user</li>
<li><code>seed</code>: For reproducible outputs</li>
<li><code>service_tier</code>: Service tier (free, standard, premium, *default)</li>
<li><code>top_logprobs</code>: Number of top logprobs to return</li>
<li><code>top_p</code>: Nucleus sampling parameter</li>
<li><code>verbosity</code>: Verbosity level (0, 1, 2, 3, *default)</li>
<li><code>enable_thinking</code>: Enable thinking mode (Qwen)</li>
<li><code>stream</code>: Enable streaming responses</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Default Model Configuration</h3><a id="user-content-default-model-configuration" aria-label="Permalink: Default Model Configuration" href="#default-model-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>--default MODEL</code> option allows you to set the default model used for all chat completions. This updates the <code>defaults.text.model</code> field in your configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Set default model to gpt-oss
llms --default gpt-oss:120b

# Set default model to Claude Sonnet
llms --default claude-sonnet-4-0

# The model must be available in your enabled providers
llms --default gemini-2.5-pro"><pre><span><span>#</span> Set default model to gpt-oss</span>
llms --default gpt-oss:120b

<span><span>#</span> Set default model to Claude Sonnet</span>
llms --default claude-sonnet-4-0

<span><span>#</span> The model must be available in your enabled providers</span>
llms --default gemini-2.5-pro</pre></div>
<p dir="auto">When you set a default model:</p>
<ul dir="auto">
<li>The configuration file (<code>~/.llms/llms.json</code>) is automatically updated</li>
<li>The specified model becomes the default for all future chat requests</li>
<li>The model must exist in your currently enabled providers</li>
<li>You can still override the default using <code>-m MODEL</code> for individual requests</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="pip install llms-py --upgrade"><pre>pip install llms-py --upgrade</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Beautiful rendered Markdown</h3><a id="user-content-beautiful-rendered-markdown" aria-label="Permalink: Beautiful rendered Markdown" href="#beautiful-rendered-markdown"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Pipe Markdown output to <a href="https://github.com/charmbracelet/glow">glow</a> to beautifully render it in the terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="llms &#34;Explain quantum computing&#34; | glow"><pre>llms <span><span>&#34;</span>Explain quantum computing<span>&#34;</span></span> <span>|</span> glow</pre></div>

<p dir="auto">Any OpenAI-compatible providers and their models can be added by configuring them in <a href="https://seinwave.com/ServiceStack/llms/blob/main/llms.json">llms.json</a>. By default only AI Providers with free tiers are enabled which will only be &#34;available&#34; if their API Key is set.</p>
<p dir="auto">You can list the available providers, their models and which are enabled or disabled with:</p>

<p dir="auto">They can be enabled/disabled in your <code>llms.json</code> file or with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="llms --enable &lt;provider&gt;
llms --disable &lt;provider&gt;"><pre>llms --enable <span>&lt;</span>provider<span>&gt;</span>
llms --disable <span>&lt;</span>provider<span>&gt;</span></pre></div>
<p dir="auto">For a provider to be available, they also require their API Key configured in either your Environment Variables
or directly in your <code>llms.json</code>.</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Provider</th>
<th>Variable</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>openrouter_free</td>
<td><code>OPENROUTER_API_KEY</code></td>
<td>OpenRouter FREE models API key</td>
<td><code>sk-or-...</code></td>
</tr>
<tr>
<td>groq</td>
<td><code>GROQ_API_KEY</code></td>
<td>Groq API key</td>
<td><code>gsk_...</code></td>
</tr>
<tr>
<td>google_free</td>
<td><code>GOOGLE_FREE_API_KEY</code></td>
<td>Google FREE API key</td>
<td><code>AIza...</code></td>
</tr>
<tr>
<td>codestral</td>
<td><code>CODESTRAL_API_KEY</code></td>
<td>Codestral API key</td>
<td><code>...</code></td>
</tr>
<tr>
<td>ollama</td>
<td>N/A</td>
<td>No API key required</td>
<td></td>
</tr>
<tr>
<td>openrouter</td>
<td><code>OPENROUTER_API_KEY</code></td>
<td>OpenRouter API key</td>
<td><code>sk-or-...</code></td>
</tr>
<tr>
<td>google</td>
<td><code>GOOGLE_API_KEY</code></td>
<td>Google API key</td>
<td><code>AIza...</code></td>
</tr>
<tr>
<td>anthropic</td>
<td><code>ANTHROPIC_API_KEY</code></td>
<td>Anthropic API key</td>
<td><code>sk-ant-...</code></td>
</tr>
<tr>
<td>openai</td>
<td><code>OPENAI_API_KEY</code></td>
<td>OpenAI API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>grok</td>
<td><code>GROK_API_KEY</code></td>
<td>Grok (X.AI) API key</td>
<td><code>xai-...</code></td>
</tr>
<tr>
<td>qwen</td>
<td><code>DASHSCOPE_API_KEY</code></td>
<td>Qwen (Alibaba) API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>z.ai</td>
<td><code>ZAI_API_KEY</code></td>
<td>Z.ai API key</td>
<td><code>sk-...</code></td>
</tr>
<tr>
<td>mistral</td>
<td><code>MISTRAL_API_KEY</code></td>
<td>Mistral API key</td>
<td><code>...</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: GPT-5, GPT-5 Codex, GPT-4o, GPT-4o-mini, o3, etc.</li>
<li><strong>Features</strong>: Text, images, function calling</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENAI_API_KEY=&#34;your-key&#34;
llms --enable openai"><pre><span>export</span> OPENAI_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable openai</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Claude Opus 4.1, Sonnet 4.0, Haiku 3.5, etc.</li>
<li><strong>Features</strong>: Text, images, large context windows</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export ANTHROPIC_API_KEY=&#34;your-key&#34;
llms --enable anthropic"><pre><span>export</span> ANTHROPIC_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable anthropic</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>GoogleProvider</code></li>
<li><strong>Models</strong>: Gemini 2.5 Pro, Flash, Flash-Lite</li>
<li><strong>Features</strong>: Text, images, safety settings</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export GOOGLE_API_KEY=&#34;your-key&#34;
llms --enable google_free"><pre><span>export</span> GOOGLE_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable google_free</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: 100+ models from various providers</li>
<li><strong>Features</strong>: Access to latest models, free tier available</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENROUTER_API_KEY=&#34;your-key&#34;
llms --enable openrouter"><pre><span>export</span> OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable openrouter</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Grok-4, Grok-3, Grok-3-mini, Grok-code-fast-1, etc.</li>
<li><strong>Features</strong>: Real-time information, humor, uncensored responses</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export GROK_API_KEY=&#34;your-key&#34;
llms --enable grok"><pre><span>export</span> GROK_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable grok</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Llama 3.3, Gemma 2, Kimi K2, etc.</li>
<li><strong>Features</strong>: Fast inference, competitive pricing</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export GROQ_API_KEY=&#34;your-key&#34;
llms --enable groq"><pre><span>export</span> GROQ_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable groq</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OllamaProvider</code></li>
<li><strong>Models</strong>: Auto-discovered from local Ollama installation</li>
<li><strong>Features</strong>: Local inference, privacy, no API costs</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# Ollama must be running locally
llms --enable ollama"><pre><span><span>#</span> Ollama must be running locally</span>
llms --enable ollama</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Qwen3-max, Qwen-max, Qwen-plus, Qwen2.5-VL, QwQ-plus, etc.</li>
<li><strong>Features</strong>: Multilingual, vision models, coding, reasoning, audio processing</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export DASHSCOPE_API_KEY=&#34;your-key&#34;
llms --enable qwen"><pre><span>export</span> DASHSCOPE_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable qwen</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: GLM-4.6, GLM-4.5, GLM-4.5-air, GLM-4.5-x, GLM-4.5-airx, GLM-4.5-flash, GLM-4:32b</li>
<li><strong>Features</strong>: Advanced language models with strong reasoning capabilities</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export ZAI_API_KEY=&#34;your-key&#34;
llms --enable z.ai"><pre><span>export</span> ZAI_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable z.ai</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Mistral Large, Codestral, Pixtral, etc.</li>
<li><strong>Features</strong>: Code generation, multilingual</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export MISTRAL_API_KEY=&#34;your-key&#34;
llms --enable mistral"><pre><span>export</span> MISTRAL_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable mistral</pre></div>

<ul dir="auto">
<li><strong>Type</strong>: <code>OpenAiProvider</code></li>
<li><strong>Models</strong>: Codestral</li>
<li><strong>Features</strong>: Code generation</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="export CODESTRAL_API_KEY=&#34;your-key&#34;
llms --enable codestral"><pre><span>export</span> CODESTRAL_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span>
llms --enable codestral</pre></div>

<p dir="auto">The tool automatically routes requests to the first available provider that supports the requested model. If a provider fails, it tries the next available provider with that model.</p>
<p dir="auto">Example: If both OpenAI and OpenRouter support <code>kimi-k2</code>, the request will first try OpenRouter (free), then fall back to Groq than OpenRouter (Paid) if requests fails.</p>


<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;defaults&#34;: {
    &#34;headers&#34;: {&#34;Content-Type&#34;: &#34;application/json&#34;},
    &#34;text&#34;: {
      &#34;model&#34;: &#34;kimi-k2&#34;,
      &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;&#34;}]
    }
  },
  &#34;providers&#34;: {
    &#34;groq&#34;: {
      &#34;enabled&#34;: true,
      &#34;type&#34;: &#34;OpenAiProvider&#34;,
      &#34;base_url&#34;: &#34;https://api.groq.com/openai&#34;,
      &#34;api_key&#34;: &#34;$GROQ_API_KEY&#34;,
      &#34;models&#34;: {
        &#34;llama3.3:70b&#34;: &#34;llama-3.3-70b-versatile&#34;,
        &#34;llama4:109b&#34;: &#34;meta-llama/llama-4-scout-17b-16e-instruct&#34;,
        &#34;llama4:400b&#34;: &#34;meta-llama/llama-4-maverick-17b-128e-instruct&#34;,
        &#34;kimi-k2&#34;: &#34;moonshotai/kimi-k2-instruct-0905&#34;,
        &#34;gpt-oss:120b&#34;: &#34;openai/gpt-oss-120b&#34;,
        &#34;gpt-oss:20b&#34;: &#34;openai/gpt-oss-20b&#34;,
        &#34;qwen3:32b&#34;: &#34;qwen/qwen3-32b&#34;
      }
    }
  }
}"><pre>{
  <span>&#34;defaults&#34;</span>: {
    <span>&#34;headers&#34;</span>: {<span>&#34;Content-Type&#34;</span>: <span><span>&#34;</span>application/json<span>&#34;</span></span>},
    <span>&#34;text&#34;</span>: {
      <span>&#34;model&#34;</span>: <span><span>&#34;</span>kimi-k2<span>&#34;</span></span>,
      <span>&#34;messages&#34;</span>: [{<span>&#34;role&#34;</span>: <span><span>&#34;</span>user<span>&#34;</span></span>, <span>&#34;content&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>}]
    }
  },
  <span>&#34;providers&#34;</span>: {
    <span>&#34;groq&#34;</span>: {
      <span>&#34;enabled&#34;</span>: <span>true</span>,
      <span>&#34;type&#34;</span>: <span><span>&#34;</span>OpenAiProvider<span>&#34;</span></span>,
      <span>&#34;base_url&#34;</span>: <span><span>&#34;</span>https://api.groq.com/openai<span>&#34;</span></span>,
      <span>&#34;api_key&#34;</span>: <span><span>&#34;</span>$GROQ_API_KEY<span>&#34;</span></span>,
      <span>&#34;models&#34;</span>: {
        <span>&#34;llama3.3:70b&#34;</span>: <span><span>&#34;</span>llama-3.3-70b-versatile<span>&#34;</span></span>,
        <span>&#34;llama4:109b&#34;</span>: <span><span>&#34;</span>meta-llama/llama-4-scout-17b-16e-instruct<span>&#34;</span></span>,
        <span>&#34;llama4:400b&#34;</span>: <span><span>&#34;</span>meta-llama/llama-4-maverick-17b-128e-instruct<span>&#34;</span></span>,
        <span>&#34;kimi-k2&#34;</span>: <span><span>&#34;</span>moonshotai/kimi-k2-instruct-0905<span>&#34;</span></span>,
        <span>&#34;gpt-oss:120b&#34;</span>: <span><span>&#34;</span>openai/gpt-oss-120b<span>&#34;</span></span>,
        <span>&#34;gpt-oss:20b&#34;</span>: <span><span>&#34;</span>openai/gpt-oss-20b<span>&#34;</span></span>,
        <span>&#34;qwen3:32b&#34;</span>: <span><span>&#34;</span>qwen/qwen3-32b<span>&#34;</span></span>
      }
    }
  }
}</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;providers&#34;: {
    &#34;openrouter&#34;: {
      &#34;enabled&#34;: false,
      &#34;type&#34;: &#34;OpenAiProvider&#34;,
      &#34;base_url&#34;: &#34;https://openrouter.ai/api&#34;,
      &#34;api_key&#34;: &#34;$OPENROUTER_API_KEY&#34;,
      &#34;models&#34;: {
        &#34;grok-4&#34;: &#34;x-ai/grok-4&#34;,
        &#34;glm-4.5-air&#34;: &#34;z-ai/glm-4.5-air&#34;,
        &#34;kimi-k2&#34;: &#34;moonshotai/kimi-k2&#34;,
        &#34;deepseek-v3.1:671b&#34;: &#34;deepseek/deepseek-chat&#34;,
        &#34;llama4:400b&#34;: &#34;meta-llama/llama-4-maverick&#34;
      }
    },
    &#34;anthropic&#34;: {
      &#34;enabled&#34;: false,
      &#34;type&#34;: &#34;OpenAiProvider&#34;,
      &#34;base_url&#34;: &#34;https://api.anthropic.com&#34;,
      &#34;api_key&#34;: &#34;$ANTHROPIC_API_KEY&#34;,
      &#34;models&#34;: {
        &#34;claude-sonnet-4-0&#34;: &#34;claude-sonnet-4-0&#34;
      }
    },
    &#34;ollama&#34;: {
      &#34;enabled&#34;: false,
      &#34;type&#34;: &#34;OllamaProvider&#34;,
      &#34;base_url&#34;: &#34;http://localhost:11434&#34;,
      &#34;models&#34;: {},
      &#34;all_models&#34;: true
    }
  }
}"><pre>{
  <span>&#34;providers&#34;</span>: {
    <span>&#34;openrouter&#34;</span>: {
      <span>&#34;enabled&#34;</span>: <span>false</span>,
      <span>&#34;type&#34;</span>: <span><span>&#34;</span>OpenAiProvider<span>&#34;</span></span>,
      <span>&#34;base_url&#34;</span>: <span><span>&#34;</span>https://openrouter.ai/api<span>&#34;</span></span>,
      <span>&#34;api_key&#34;</span>: <span><span>&#34;</span>$OPENROUTER_API_KEY<span>&#34;</span></span>,
      <span>&#34;models&#34;</span>: {
        <span>&#34;grok-4&#34;</span>: <span><span>&#34;</span>x-ai/grok-4<span>&#34;</span></span>,
        <span>&#34;glm-4.5-air&#34;</span>: <span><span>&#34;</span>z-ai/glm-4.5-air<span>&#34;</span></span>,
        <span>&#34;kimi-k2&#34;</span>: <span><span>&#34;</span>moonshotai/kimi-k2<span>&#34;</span></span>,
        <span>&#34;deepseek-v3.1:671b&#34;</span>: <span><span>&#34;</span>deepseek/deepseek-chat<span>&#34;</span></span>,
        <span>&#34;llama4:400b&#34;</span>: <span><span>&#34;</span>meta-llama/llama-4-maverick<span>&#34;</span></span>
      }
    },
    <span>&#34;anthropic&#34;</span>: {
      <span>&#34;enabled&#34;</span>: <span>false</span>,
      <span>&#34;type&#34;</span>: <span><span>&#34;</span>OpenAiProvider<span>&#34;</span></span>,
      <span>&#34;base_url&#34;</span>: <span><span>&#34;</span>https://api.anthropic.com<span>&#34;</span></span>,
      <span>&#34;api_key&#34;</span>: <span><span>&#34;</span>$ANTHROPIC_API_KEY<span>&#34;</span></span>,
      <span>&#34;models&#34;</span>: {
        <span>&#34;claude-sonnet-4-0&#34;</span>: <span><span>&#34;</span>claude-sonnet-4-0<span>&#34;</span></span>
      }
    },
    <span>&#34;ollama&#34;</span>: {
      <span>&#34;enabled&#34;</span>: <span>false</span>,
      <span>&#34;type&#34;</span>: <span><span>&#34;</span>OllamaProvider<span>&#34;</span></span>,
      <span>&#34;base_url&#34;</span>: <span><span>&#34;</span>http://localhost:11434<span>&#34;</span></span>,
      <span>&#34;models&#34;</span>: {},
      <span>&#34;all_models&#34;</span>: <span>true</span>
    }
  }
}</pre></div>

<div data-snippet-clipboard-copy-content="usage: llms [-h] [--config FILE] [-m MODEL] [--chat REQUEST] [-s PROMPT] [--image IMAGE] [--audio AUDIO] [--file FILE]
            [--args PARAMS] [--raw] [--list] [--check PROVIDER] [--serve PORT] [--enable PROVIDER] [--disable PROVIDER]
            [--default MODEL] [--init] [--root PATH] [--logprefix PREFIX] [--verbose]

llms v2.0.24

options:
  -h, --help            show this help message and exit
  --config FILE         Path to config file
  -m, --model MODEL     Model to use
  --chat REQUEST        OpenAI Chat Completion Request to send
  -s, --system PROMPT   System prompt to use for chat completion
  --image IMAGE         Image input to use in chat completion
  --audio AUDIO         Audio input to use in chat completion
  --file FILE           File input to use in chat completion
  --args PARAMS         URL-encoded parameters to add to chat request (e.g. &#34;temperature=0.7&amp;seed=111&#34;)
  --raw                 Return raw AI JSON response
  --list                Show list of enabled providers and their models (alias ls provider?)
  --check PROVIDER      Check validity of models for a provider
  --serve PORT          Port to start an OpenAI Chat compatible server on
  --enable PROVIDER     Enable a provider
  --disable PROVIDER    Disable a provider
  --default MODEL       Configure the default model to use
  --init                Create a default llms.json
  --root PATH           Change root directory for UI files
  --logprefix PREFIX    Prefix used in log messages
  --verbose             Verbose output"><pre><code>usage: llms [-h] [--config FILE] [-m MODEL] [--chat REQUEST] [-s PROMPT] [--image IMAGE] [--audio AUDIO] [--file FILE]
            [--args PARAMS] [--raw] [--list] [--check PROVIDER] [--serve PORT] [--enable PROVIDER] [--disable PROVIDER]
            [--default MODEL] [--init] [--root PATH] [--logprefix PREFIX] [--verbose]

llms v2.0.24

options:
  -h, --help            show this help message and exit
  --config FILE         Path to config file
  -m, --model MODEL     Model to use
  --chat REQUEST        OpenAI Chat Completion Request to send
  -s, --system PROMPT   System prompt to use for chat completion
  --image IMAGE         Image input to use in chat completion
  --audio AUDIO         Audio input to use in chat completion
  --file FILE           File input to use in chat completion
  --args PARAMS         URL-encoded parameters to add to chat request (e.g. &#34;temperature=0.7&amp;seed=111&#34;)
  --raw                 Return raw AI JSON response
  --list                Show list of enabled providers and their models (alias ls provider?)
  --check PROVIDER      Check validity of models for a provider
  --serve PORT          Port to start an OpenAI Chat compatible server on
  --enable PROVIDER     Enable a provider
  --disable PROVIDER    Disable a provider
  --default MODEL       Configure the default model to use
  --init                Create a default llms.json
  --root PATH           Change root directory for UI files
  --logprefix PREFIX    Prefix used in log messages
  --verbose             Verbose output
</code></pre></div>


<p dir="auto">The easiest way to run llms-py is using Docker:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using docker-compose (recommended)
docker-compose up -d

# Or pull and run directly
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest"><pre><span><span>#</span> Using docker-compose (recommended)</span>
docker-compose up -d

<span><span>#</span> Or pull and run directly</span>
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>

<p dir="auto">Pre-built Docker images are automatically published to GitHub Container Registry:</p>
<ul dir="auto">
<li><strong>Latest stable</strong>: <code>ghcr.io/servicestack/llms:latest</code></li>
<li><strong>Specific version</strong>: <code>ghcr.io/servicestack/llms:v2.0.24</code></li>
<li><strong>Main branch</strong>: <code>ghcr.io/servicestack/llms:main</code></li>
</ul>

<p dir="auto">Pass API keys as environment variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=&#34;sk-or-...&#34; \
  -e GROQ_API_KEY=&#34;gsk_...&#34; \
  -e GOOGLE_FREE_API_KEY=&#34;AIza...&#34; \
  -e ANTHROPIC_API_KEY=&#34;sk-ant-...&#34; \
  -e OPENAI_API_KEY=&#34;sk-...&#34; \
  ghcr.io/servicestack/llms:latest"><pre>docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>sk-or-...<span>&#34;</span></span> \
  -e GROQ_API_KEY=<span><span>&#34;</span>gsk_...<span>&#34;</span></span> \
  -e GOOGLE_FREE_API_KEY=<span><span>&#34;</span>AIza...<span>&#34;</span></span> \
  -e ANTHROPIC_API_KEY=<span><span>&#34;</span>sk-ant-...<span>&#34;</span></span> \
  -e OPENAI_API_KEY=<span><span>&#34;</span>sk-...<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>

<p dir="auto">Create a <code>docker-compose.yml</code> file (or use the one in the repository):</p>
<div dir="auto" data-snippet-clipboard-copy-content="version: &#39;3.8&#39;

services:
  llms:
    image: ghcr.io/servicestack/llms:latest
    ports:
      - &#34;8000:8000&#34;
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - GOOGLE_FREE_API_KEY=${GOOGLE_FREE_API_KEY}
    volumes:
      - llms-data:/home/llms/.llms
    restart: unless-stopped

volumes:
  llms-data:"><pre><span>version</span>: <span><span>&#39;</span>3.8<span>&#39;</span></span>

<span>services</span>:
  <span>llms</span>:
    <span>image</span>: <span>ghcr.io/servicestack/llms:latest</span>
    <span>ports</span>:
      - <span><span>&#34;</span>8000:8000<span>&#34;</span></span>
    <span>environment</span>:
      - <span>OPENROUTER_API_KEY=${OPENROUTER_API_KEY}</span>
      - <span>GROQ_API_KEY=${GROQ_API_KEY}</span>
      - <span>GOOGLE_FREE_API_KEY=${GOOGLE_FREE_API_KEY}</span>
    <span>volumes</span>:
      - <span>llms-data:/home/llms/.llms</span>
    <span>restart</span>: <span>unless-stopped</span>

<span>volumes</span>:
  <span>llms-data</span>:</pre></div>
<p dir="auto">Create a <code>.env</code> file with your API keys:</p>
<div dir="auto" data-snippet-clipboard-copy-content="OPENROUTER_API_KEY=sk-or-...
GROQ_API_KEY=gsk_...
GOOGLE_FREE_API_KEY=AIza..."><pre>OPENROUTER_API_KEY=sk-or-...
GROQ_API_KEY=gsk_...
GOOGLE_FREE_API_KEY=AIza...</pre></div>
<p dir="auto">Start the service:</p>


<p dir="auto">Build the Docker image from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using the build script
./docker-build.sh

# Or manually
docker build -t llms-py:latest .

# Run your local build
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  llms-py:latest"><pre><span><span>#</span> Using the build script</span>
./docker-build.sh

<span><span>#</span> Or manually</span>
docker build -t llms-py:latest <span>.</span>

<span><span>#</span> Run your local build</span>
docker run -p 8000:8000 \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  llms-py:latest</pre></div>

<p dir="auto">To persist configuration and analytics data between container restarts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using a named volume (recommended)
docker run -p 8000:8000 \
  -v llms-data:/home/llms/.llms \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest

# Or mount a local directory
docker run -p 8000:8000 \
  -v $(pwd)/llms-config:/home/llms/.llms \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest"><pre><span><span>#</span> Using a named volume (recommended)</span>
docker run -p 8000:8000 \
  -v llms-data:/home/llms/.llms \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest

<span><span>#</span> Or mount a local directory</span>
docker run -p 8000:8000 \
  -v <span><span>$(</span>pwd<span>)</span></span>/llms-config:/home/llms/.llms \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Custom Configuration Files</h3><a id="user-content-custom-configuration-files" aria-label="Permalink: Custom Configuration Files" href="#custom-configuration-files"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Customize llms-py behavior by providing your own <code>llms.json</code> and <code>ui.json</code> files:</p>
<p dir="auto"><strong>Option 1: Mount a directory with custom configs</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create config directory with your custom files
mkdir -p config
# Add your custom llms.json and ui.json to config/

# Mount the directory
docker run -p 8000:8000 \
  -v $(pwd)/config:/home/llms/.llms \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest"><pre><span><span>#</span> Create config directory with your custom files</span>
mkdir -p config
<span><span>#</span> Add your custom llms.json and ui.json to config/</span>

<span><span>#</span> Mount the directory</span>
docker run -p 8000:8000 \
  -v <span><span>$(</span>pwd<span>)</span></span>/config:/home/llms/.llms \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>
<p dir="auto"><strong>Option 2: Mount individual config files</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8000:8000 \
  -v $(pwd)/my-llms.json:/home/llms/.llms/llms.json:ro \
  -v $(pwd)/my-ui.json:/home/llms/.llms/ui.json:ro \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest"><pre>docker run -p 8000:8000 \
  -v <span><span>$(</span>pwd<span>)</span></span>/my-llms.json:/home/llms/.llms/llms.json:ro \
  -v <span><span>$(</span>pwd<span>)</span></span>/my-ui.json:/home/llms/.llms/ui.json:ro \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>
<p dir="auto"><strong>With docker-compose:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="volumes:
  # Use local directory
  - ./config:/home/llms/.llms

  # Or mount individual files
  # - ./my-llms.json:/home/llms/.llms/llms.json:ro
  # - ./my-ui.json:/home/llms/.llms/ui.json:ro"><pre><span>volumes</span>:
  <span><span>#</span> Use local directory</span>
  - <span>./config:/home/llms/.llms</span>

  <span><span>#</span> Or mount individual files</span>
  <span><span>#</span> - ./my-llms.json:/home/llms/.llms/llms.json:ro</span>
  <span><span>#</span> - ./my-ui.json:/home/llms/.llms/ui.json:ro</span></pre></div>
<p dir="auto">The container will auto-create default config files on first run if they don&#39;t exist. You can customize these to:</p>
<ul dir="auto">
<li>Enable/disable specific providers</li>
<li>Add or remove models</li>
<li>Configure API endpoints</li>
<li>Set custom pricing</li>
<li>Customize chat templates</li>
<li>Configure UI settings</li>
</ul>
<p dir="auto">See <a href="https://seinwave.com/ServiceStack/llms/blob/main/DOCKER.md">DOCKER.md</a> for detailed configuration examples.</p>

<p dir="auto">Change the port mapping to run on a different port:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run on port 3000 instead of 8000
docker run -p 3000:8000 \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest"><pre><span><span>#</span> Run on port 3000 instead of 8000</span>
docker run -p 3000:8000 \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest</pre></div>

<p dir="auto">You can also use the Docker container for CLI commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run a single query
docker run --rm \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest \
  llms &#34;What is the capital of France?&#34;

# List available models
docker run --rm \
  -e OPENROUTER_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest \
  llms --list

# Check provider status
docker run --rm \
  -e GROQ_API_KEY=&#34;your-key&#34; \
  ghcr.io/servicestack/llms:latest \
  llms --check groq"><pre><span><span>#</span> Run a single query</span>
docker run --rm \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest \
  llms <span><span>&#34;</span>What is the capital of France?<span>&#34;</span></span>

<span><span>#</span> List available models</span>
docker run --rm \
  -e OPENROUTER_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest \
  llms --list

<span><span>#</span> Check provider status</span>
docker run --rm \
  -e GROQ_API_KEY=<span><span>&#34;</span>your-key<span>&#34;</span></span> \
  ghcr.io/servicestack/llms:latest \
  llms --check groq</pre></div>

<p dir="auto">The Docker image includes a health check that verifies the server is responding:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Check container health
docker ps

# View health check logs
docker inspect --format=&#39;{{json .State.Health}}&#39; llms-server"><pre><span><span>#</span> Check container health</span>
docker ps

<span><span>#</span> View health check logs</span>
docker inspect --format=<span><span>&#39;</span>{{json .State.Health}}<span>&#39;</span></span> llms-server</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Multi-Architecture Support</h3><a id="user-content-multi-architecture-support" aria-label="Permalink: Multi-Architecture Support" href="#multi-architecture-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The Docker images support multiple architectures:</p>
<ul dir="auto">
<li><code>linux/amd64</code> (x86_64)</li>
<li><code>linux/arm64</code> (ARM64/Apple Silicon)</li>
</ul>
<p dir="auto">Docker will automatically pull the correct image for your platform.</p>


<p dir="auto"><strong>Config file not found</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Initialize default config
llms --init

# Or specify custom path
llms --config ./my-config.json"><pre><span><span>#</span> Initialize default config</span>
llms --init

<span><span>#</span> Or specify custom path</span>
llms --config ./my-config.json</pre></div>
<p dir="auto"><strong>No providers enabled</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Check status
llms --list

# Enable providers
llms --enable google anthropic"><pre><span><span>#</span> Check status</span>
llms --list

<span><span>#</span> Enable providers</span>
llms --enable google anthropic</pre></div>
<p dir="auto"><strong>API key issues</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Check environment variables
echo $ANTHROPIC_API_KEY

# Enable verbose logging
llms --verbose &#34;test&#34;"><pre><span><span>#</span> Check environment variables</span>
<span>echo</span> <span>$ANTHROPIC_API_KEY</span>

<span><span>#</span> Enable verbose logging</span>
llms --verbose <span><span>&#34;</span>test<span>&#34;</span></span></pre></div>
<p dir="auto"><strong>Model not found</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# List available models
llms --list

# Check provider configuration
llms ls openrouter"><pre><span><span>#</span> List available models</span>
llms --list

<span><span>#</span> Check provider configuration</span>
llms ls openrouter</pre></div>

<p dir="auto">Enable verbose logging to see detailed request/response information:</p>
<div dir="auto" data-snippet-clipboard-copy-content="llms --verbose --logprefix &#34;[DEBUG] &#34; &#34;Hello&#34;"><pre>llms --verbose --logprefix <span><span>&#34;</span>[DEBUG] <span>&#34;</span></span> <span><span>&#34;</span>Hello<span>&#34;</span></span></pre></div>
<p dir="auto">This shows:</p>
<ul dir="auto">
<li>Enabled providers</li>
<li>Model routing decisions</li>
<li>HTTP request details</li>
<li>Error messages with stack traces</li>
</ul>


<ul dir="auto">
<li><code>llms/main.py</code> - Main script with CLI and server functionality</li>
<li><code>llms/llms.json</code> - Default configuration file</li>
<li><code>llms/ui.json</code> - UI configuration file</li>
<li><code>requirements.txt</code> - Python dependencies, required: <code>aiohttp</code>, optional: <code>Pillow</code></li>
</ul>

<ul dir="auto">
<li><code>OpenAiProvider</code> - Generic OpenAI-compatible provider</li>
<li><code>OllamaProvider</code> - Ollama-specific provider with model auto-discovery</li>
<li><code>GoogleProvider</code> - Google Gemini with native API format</li>
<li><code>GoogleOpenAiProvider</code> - Google Gemini via OpenAI-compatible endpoint</li>
</ul>

<ol dir="auto">
<li>Create a provider class inheriting from <code>OpenAiProvider</code></li>
<li>Implement provider-specific authentication and formatting</li>
<li>Add provider configuration to <code>llms.json</code></li>
<li>Update initialization logic in <code>init_llms()</code></li>
</ol>

<p dir="auto">Contributions are welcome! Please submit a PR to add support for any missing OpenAI-compatible providers.</p>
</article></div></div>
  </body>
</html>
