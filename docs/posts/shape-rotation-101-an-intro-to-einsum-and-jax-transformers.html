<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sankalp.bearblog.dev/einsum-new/">Original</a>
    <h1>Shape Rotation 101: An Intro to Einsum and Jax Transformers</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
<i>
<time datetime="2024-06-22T04:50Z">
22 Jun, 2024
</time>
</i>
</p>
<h2 id="acknowledgements">Acknowledgements</h2><p>First, I would like to acknowledge my friends and kind internet strangers who helped me with this post.</p>
<p>This post heavily adapts from the following -</p>
<ol>
<li>Tested Jax Transformer code via <a href="https://x.com/_xjdr">xjdr</a> | <a href="https://github.com/xjdr-alt/simple_transformer/blob/main/simple_transformer.py">Github Link</a></li>
<li><a href="https://rockt.github.io/2018/04/30/einsum">Einstein summation in pytorch</a></li>
<li><a href="https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/">Einstein summation in numpy</a></li>
<li><a href="https://ajcr.net/Basic-guide-to-einsum/">Basic guide to einsum</a></li>
</ol>
<p>Big thanks to <a href="https://x.com/_xjdr">_xjdr</a>, <a href="https://x.com/felix_red_panda">Felix</a>, <a href="https://x.com/thepushkarp">Pushkar</a> and <a href="https://x.com/4evaBehindSOTA">Tokenbender</a> for proof-reading.</p>
<h2 id="intro">Intro</h2><p>I have been “delving” into jax and einsum notation lately in my quest to become a shape-rotator.</p>
<p>
<img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719001786.gif" alt="twirl"/>
</p>
<p>This post is divided into two parts. In the first part, we go through einsum notation basics. The second part is about understanding simple transformer code in jax which uses a lot of einsum.</p>
<p>From your end, I want some brains (no I am not a zombie, I just want your attention). I also assume knowledge of numpy basics, matrix multiplication brute force algorithm and transformers basics (only part 2).</p>
<p>In the case I spectacularly fail to explain einsum, you can refer the posts 2, 3 and 4 mentioned above. 2. is einsum in pytorch and builds up on 3 and 4. 3 goes into internals while 4 focuses on the notations with examples.</p>
<figure>
<img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719001941.png" width="75%" alt="Notation reminder"/>
<figcaption>Reminder</figcaption>
</figure>
<h2 id="but-what-is-einsum">But what is Einsum</h2><p>Einsum is an alternative API for tensor/numerical array manipulation provided by several libraries. NumPy (since v1.6), PyTorch, and other scientific computing libraries offer an <code>einsum</code> function.</p>
<blockquote>
<p>Einsum notation was introduced by… you guessed it right Albert Einstein [<a href="https://en.wikipedia.org/wiki/Einstein_notation">wikipedia</a>]</p>
</blockquote>
<p>This function leverages Einstein summation notation to simplify complex linear algebraic operations on multi-dimensional arrays - <strong>tensor contractions</strong> (more on this later) and summations. The syntax is mostly consistent across NumPy, Torch, Jax etc.</p>
<p><strong>numpy.einsum</strong></p>
<p><strong><code>numpy.einsum(*subscripts, *operands, out=None, dtype=None, order=&#39;K&#39;, casting=&#39;safe&#39;, optimize=False)</code></strong><a href="https://github.com/numpy/numpy/blob/v2.0.0/numpy/_core/einsumfunc.py#L1057-L1505">[SOURCE]</a></p>
<p><strong>torch.einsum</strong></p>
<p><strong><code>torch.einsum(equation, *operands) → [Tensor]</code></strong>
<a href="https://pytorch.org/docs/stable/_modules/torch/functional.html#einsum">[SOURCE]</a></p>
<h2 id="three-reasons-to-learn-einsum">Three reasons to learn einsum</h2><p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719029187.jpeg" alt="shape"/></p>
<p>Learning einsum is worth your time. Many deep learning researchers use it in their work.</p>
<p>To become a true shape rotator.</p>
<p>It can <em>outperform familiar array functions in terms of speed and memory efficiency, thanks to its expressive power and smart loops.</em> It’s self-documenting too. Only downside is the notation can be tricky to understand initially.</p>
<h2 id="ok-show-me-an-example-of-einsum">Ok show me an example of einsum</h2><p>Let’s say we have two matrices <strong>A</strong> and <strong>B that</strong> we want to multiply them <strong>element-wise and then take sum for axis = 1 (row wise)</strong></p>
<div><pre><span></span><span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>])</span> <span># shape (3,)</span>

<span>B</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span> <span>0</span><span>,</span>  <span>1</span><span>,</span>  <span>2</span><span>,</span>  <span>3</span><span>],</span> <span># (3, 4)</span>
              <span>[</span> <span>4</span><span>,</span>  <span>5</span><span>,</span>  <span>6</span><span>,</span>  <span>7</span><span>],</span>
              <span>[</span> <span>8</span><span>,</span>  <span>9</span><span>,</span> <span>10</span><span>,</span> <span>11</span><span>]])</span>
</pre></div>
<p>Using einsum notation,</p>
<div><pre><span></span><span>&gt;&gt;&gt;</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>&#39;i,ij-&gt;i&#39;</span><span>,</span> <span>A</span><span>,</span> <span>B</span><span>)</span>
<span>array</span><span>([</span> <span>0</span><span>,</span> <span>22</span><span>,</span> <span>76</span><span>])</span>
</pre></div>
<p>Without einsum, this would look like -</p>
<p>Multiply them.</p>
<div><pre><span></span><span>&gt;&gt;&gt;</span> <span>A</span> <span>*</span> <span>B</span>
<span>Traceback</span> <span>(</span><span>most</span> <span>recent</span> <span>call</span> <span>last</span><span>):</span>
  <span>File</span> <span>&#34;&lt;stdin&gt;&#34;</span><span>,</span> <span>line</span> <span>1</span><span>,</span> <span>in</span> <span>&lt;</span><span>module</span><span>&gt;</span>
<span>ValueError</span><span>:</span> <span>operands</span> <span>could</span> <span>not</span> <span>be</span> <span>broadcast</span> <span>together</span> <span>with</span> <span>shapes</span> <span>(</span><span>3</span><span>,)</span> <span>(</span><span>3</span><span>,</span><span>4</span><span>)</span>
</pre></div>
<p>But alas, my silly self forgot to reshape. You require the matrices to have same dimensions in order for broadcasting. Convert A from (3,) → (3, 1) (essentially a column vector)</p>
<div><pre><span></span><span>&gt;&gt;&gt;</span> <span>A</span> <span>=</span> <span>A</span><span>[:,</span> <span>np</span><span>.</span><span>newaxis</span><span>]</span>
<span>&gt;&gt;&gt;</span> <span>A</span>
<span>array</span><span>([[</span><span>0</span><span>],</span>
       <span>[</span><span>1</span><span>],</span>
       <span>[</span><span>2</span><span>]])</span>
</pre></div>
<p>Now you can perform</p>
<div><pre><span></span><span>&gt;&gt;&gt;</span> <span>(</span><span>A</span> <span>*</span> <span>B</span><span>)</span><span>.</span><span>sum</span><span>(</span><span>axis</span> <span>=</span> <span>1</span><span>)</span>
<span>array</span><span>([</span> <span>0</span><span>,</span> <span>22</span><span>,</span> <span>76</span><span>])</span>

<span>#  A gets broadcasted from (3, 1) to (3, 3) before multiplication</span>
<span># [0 0 0]</span>
<span># [1 1 1]</span>
<span># [2 2 2]</span>
</pre></div>
<p>Reiterating, with einsum all you <del>need</del> did was <strong>np.einsum(&#39;i,ij-&gt;i&#39;, A, B).</strong></p>
<p>Let’s try to understand how it works.</p>
<h2 id="but-how-does-it-work">But how does it work</h2><p><code>np.einsum(&#39;string specifying indices and operation &#39;, matrix1, matrix2 ...)</code></p>
<p>The string looks like <code>i, ij-&gt;i</code> - <code>input indices -&gt; output indices</code></p>
<p><code>i, ij</code> - input specification (the dimensions/axis of the matrices to which we do operations. The comma separates the indices of different matrices.</p>
<p><code>i</code> → output specification (desired shape)</p>
<p><strong>i</strong> corresponds to row of matrix A
<strong>ij</strong> corresponds to row, column respectively for matrix B.</p>

<p>The specific letters that you can use in the string are arbitrary. You could have used something like <code>a, ab-&gt;a</code> . Just make sure that there is one label/index to represent each axis/dimension of the matrix.</p>
<p>Each letter/label e.g i, j represents the axis of the matrix/tensor that will be iterated over and can be expressed as a deeply nested set of for loops. There are a few important rules that you need to know after which it’s easy to understand einsum.</p>
<h2 id="some-rules">Some rules</h2><p><strong>Note</strong>: I just write A[i][j] (pseudocode) as A[i, j] (numpy notation) for convenience</p>
<p>[1] <strong>Repeating letters between input arrays means that values along those axes will be multiplied together. The products make up the values for the output array.</strong></p>
<p><code>i, ij-&gt;i</code></p>
<p>The result is going to be sum along axis = 1 for element wise product of A and B which means it’s going to be a row vector.</p>
<p><code>product[i] = A[i] * B[i][j]</code></p>
<p>If our einsum would have been something like <code>bmhk, bhlm -&gt; blhk</code> then</p>
<p><code>product[b, l, h, k] = A[b, m, h, k] * B[b, h, l, m]</code></p>
<p>[2] <strong>Omitting a letter from the output means that values along that axis will be summed.</strong></p>
<p>In simple words, any letter/index that doesn’t appear on the right hand side of the string is summed up over. We don’t put <strong>j</strong> on RHS since we want the sum along that dimension (column-wise)</p>
<p><code>output[i] += A[i] * B[i, j]</code> # this is a tensor contraction</p>
<hr/>
<h3 id="tensor-contraction">Tensor contraction</h3><p>Slight digression here. What we just did above is a tensor contraction.</p>
<p>It generalizes the concept of matrix multiplication to higher-dimensional arrays, or tensors. Summing over the product of paired indices between two tensors, resulting in a new tensor with reduced dimensionality. This is what <strong>einsum does.</strong></p>
<p>Mathematically, the above operation can be expressed as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mtext>result</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>j</mi></mrow></munder><mi>A</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mi>·</mi><mi>B</mi><mo stretchy="false">[</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mrow></math><ol>
<li>For each value of <code>i</code>, the elements of <code>A[i]</code> are multiplied with the corresponding elements of <code>B[i,j]</code> along the <code>j</code> axis.</li>
<li>The products are summed over the <code>j</code> axis, effectively reducing the dimensionality of the result.</li>
<li>The resulting tensor has shape <code>[i]</code>, as specified by the output indices.</li>
</ol>
<hr/>
<p>Below is how above einsum would look if we wrote it in the form of nested for loops (summations are for inner most for loops).</p>
<div><pre><span></span><span># for loop for above einsum</span>
<span>result</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>A</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>])</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>A</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>B</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>result</span><span>[</span><span>i</span><span>]</span> <span>+=</span> <span>A</span><span>[</span><span>i</span><span>]</span> <span>*</span> <span>B</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span>
</pre></div>
<p>But why is it faster? It didn’t require reshaping hence avoiding overhead of creating a temporary array <code>A[:, np.newaxis] * B</code>. It simply sums the products along the rows as it goes. That comples explanation for our first example.</p>
<p>[3] <strong>We can return the unsummed axes in any order we like.</strong></p>
<p>This is sort of equivalent to reshaping/rearrange.</p>
<p>For example, transpose will be <code>np.einsum(&#39;ij-&gt;ji&#39;, A)</code></p>
<div><pre><span></span><span>&gt;&gt;&gt;</span> <span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>],</span>
<span>...</span>               <span>[</span><span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>],</span>
<span>...</span>               <span>[</span><span>7</span><span>,</span> <span>8</span><span>,</span> <span>9</span><span>]])</span>
<span>&gt;&gt;&gt;</span>
<span>&gt;&gt;&gt;</span> <span># Perform transpose using einsum</span>
<span>&gt;&gt;&gt;</span> <span>A_transpose</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>&#39;ij-&gt;ji&#39;</span><span>,</span> <span>A</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>A_transpose</span>
<span>array</span><span>([[</span><span>1</span><span>,</span> <span>4</span><span>,</span> <span>7</span><span>],</span>
       <span>[</span><span>2</span><span>,</span> <span>5</span><span>,</span> <span>8</span><span>],</span>
       <span>[</span><span>3</span><span>,</span> <span>6</span><span>,</span> <span>9</span><span>]])</span>
</pre></div>
<hr/>
<h3 id="sum-of-all-elements">Sum of all elements</h3><p><code>np.einsum(&#39;ij-&gt;&#39;, A)</code> - omitting both i, j means summation happens along these dimensions.</p>
<div><pre><span></span><span># Perform summation using for loop</span>
<span>sum_loop</span> <span>=</span> <span>0</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>A</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>A</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>sum_loop</span> <span>+=</span> <span>A</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span> <span>*</span> <span>1</span>
</pre></div>
<hr/>
<h3 id="trace">Trace</h3><p><code>np.einsum(’ii-&gt;’, A)</code> For trace of matrix (sum of diagonal elements)</p>
<h3 id="matrix-multiplication-in-einsum">Matrix multiplication in einsum</h3><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>C</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi></mrow></munder><msub><mi>A</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><msub><mi>B</mi><mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow></math><p>A better example to demonstrate einsum is matrix multiplication. Above can be expressed in for three nested for loops (brute force matrix multiplication algorithm). Here’s an <a href="https://notesbylex.com/matrix-multiplication">animation</a>.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719002673.png" alt="Untitled"/></p>
<p>k is repeated which means product happens along it. k is not in the output specification summation. k is called <strong>summation index.</strong></p>
<blockquote>
<p>All indices in einsum format string can be partitioned in two sets: free indices and summation indices</p>
<ul>
<li><em>Free indices</em> are the indices used in the output specification (right hand side of string). They are associated with the <em>outer</em> <code>for</code>loops.</li>
<li><em>Summation indices</em> are all other indices: those that appear in the argument specifications but <strong>not</strong> in the output specification. They are so called because they are <em>summed out</em> when computing the output tensor. <strong>They are associated with the <em>inner</em> <code>for</code>loops.</strong></li>
</ul>
</blockquote>
<p>You could also mention matrix multiplication as <code>np.einsum(’ij,jk→ik’, A, B)</code> and it would be still valid (as I mentioned earlier that the letters are arbitrary).</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719002762.png" alt="Untitled 1"/>
</p>
<h3 id="matrix-product-tranpose">Matrix product tranpose</h3><p>Let’s say you want to get transpose of matrix product i.e (A @ B).T</p>
<p><code>np.einsum(&#39;ij,jk-&gt;ki&#39;, A, B)</code> Note how we just rearranged <code>ik</code> to <code>ki</code> and that’s a transpose.</p>
<h2 id="observations-for-nested-loops">Observations for nested loops</h2><p><code>ij, jk-&gt;ik</code> - the number of unique indices in the string = number of nested loops</p>
<p>The order of nested loops will follow the order of the right hand side/output specification of the string.</p>
<p>index not present on right side - summation index, always present in the innermost loop</p>
<hr/>
<h2 id="more-examples">More examples</h2><p>Here’s a list of operations to practice mentally (image stolen from <a href="https://ajcr.net/Basic-guide-to-einsum/">post 4</a>)</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719002888.png" alt="Screenshot_2024-06-20_at_12"/></p>
<p>Ok, that’s a lot to digest. Take a break fellow shape rotator for in the next section, we shall dive deep into a simple Jax transformer implementation and witness einsum in action on the frontlines of deep learning.</p>
<p><strong>Shoutout again to Mr. <a href="https://x.com/_xjdr">xjdr</a></strong> for open source contribution of the jax transformer code.</p>
<p>It’s cleanly written and tested by him. He also clarified some of my doubts.</p>
<h2 id="about-jax">About jax</h2><p><a href="https://jax.readthedocs.io/en/latest/">Jax</a> is somewhere middle in between of Numpy and Pytorch. Researchers mainly use Pytorch for research but for production loads, people are moving to Jax for being faster. You will see that it’s syntax is similar to numpy (but there is a huge emphasis on functional programming concepts like pure functions, immutable arrays etc.). It uses JIT (just in time compilation to fasten things up.</p>
<p>Next we try to decode this simple transformer implementation in Jax.</p>
<hr/>
<h2 id="simple-jax-transformer">Simple Jax Transformer</h2><p>According to Mr. _xjdr, “this is a decoder only, from the early noam era pre RoPE transformer”</p>
<div><pre><span></span><span>from</span> <span>typing</span> <span>import</span> <span>List</span><span>,</span> <span>NamedTuple</span>

<span>import</span> <span>jax</span>
<span>import</span> <span>jax.numpy</span> <span>as</span> <span>jnp</span>

<span>class</span> <span>LayerWeights</span><span>(</span><span>NamedTuple</span><span>):</span>
  <span>attn_norm</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>ffn_norm</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w_q_dhk</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w_k_dhk</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w_v_dhk</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w_o_hkd</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w1</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w2</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>w3</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>

<span>class</span> <span>XfmrWeights</span><span>(</span><span>NamedTuple</span><span>):</span>
  <span>tok_embeddings</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>layer_weights</span><span>:</span> <span>List</span><span>[</span><span>LayerWeights</span><span>]</span>
  <span>norm</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>
  <span>output</span><span>:</span> <span>jax</span><span>.</span><span>Array</span>

<span>def</span> <span>norm</span><span>(</span><span>x</span><span>,</span> <span>w</span><span>,</span> <span>eps</span><span>:</span> <span>float</span> <span>=</span> <span>1e-6</span><span>):</span>
    <span>return</span> <span>w</span> <span>*</span> <span>(</span><span>x</span> <span>*</span> <span>jax</span><span>.</span><span>lax</span><span>.</span><span>rsqrt</span><span>(</span><span>jax</span><span>.</span><span>lax</span><span>.</span><span>pow</span><span>(</span><span>x</span><span>,</span> <span>2</span><span>)</span><span>.</span><span>mean</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span> <span>+</span> <span>eps</span><span>))</span>

<span>def</span> <span>attention</span><span>(</span><span>input_bld</span><span>,</span> <span>params</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    B: batch size</span>
<span>    L: sequence length</span>
<span>    M: memory length </span>
<span>    D: model dimension</span>
<span>    H: number of attention heads in a layer</span>
<span>    K: size of each attention key or value</span>
<span>    &#34;&#34;&#34;</span>
    <span>normalized_bld</span> <span>=</span> <span>norm</span><span>(</span><span>input_bld</span><span>,</span> <span>params</span><span>.</span><span>attn_norm</span><span>)</span>
    <span>query_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_q_dhk</span><span>)</span>
    <span>key_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_k_dhk</span><span>)</span>
    <span>value_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_v_dhk</span><span>)</span>
    <span>logits_bhlm</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,bmhk-&gt;bhlm&#39;</span><span>,</span> <span>query_blhk</span><span>,</span> <span>key_blhk</span><span>)</span>
    <span>_</span><span>,</span> <span>l</span><span>,</span> <span>h</span><span>,</span> <span>k</span> <span>=</span> <span>query_blhk</span><span>.</span><span>shape</span>
    <span>logits_bhlm</span> <span>=</span> <span>logits_bhlm</span> <span>/</span> <span>jnp</span><span>.</span><span>sqrt</span><span>(</span><span>k</span><span>)</span>
    <span>mask</span> <span>=</span> <span>jnp</span><span>.</span><span>triu</span><span>(</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>l</span><span>,</span> <span>l</span><span>)),</span> <span>k</span><span>=</span><span>1</span><span>)</span><span>.</span><span>astype</span><span>(</span><span>input_bld</span><span>.</span><span>dtype</span><span>)</span>
    <span>logits_bhlm</span> <span>=</span> <span>logits_bhlm</span> <span>-</span> <span>jnp</span><span>.</span><span>inf</span> <span>*</span> <span>mask</span><span>[</span><span>None</span><span>,</span> <span>None</span><span>,</span> <span>:,</span> <span>:]</span>
    <span>weights_bhlm</span> <span>=</span> <span>jax</span><span>.</span><span>nn</span><span>.</span><span>softmax</span><span>(</span><span>logits_bhlm</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
    <span>wtd_values_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,bhlm-&gt;blhk&#39;</span><span>,</span> <span>value_blhk</span><span>,</span> <span>weights_bhlm</span><span>)</span>
    <span>out_bld</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,hkd-&gt;bld&#39;</span><span>,</span> <span>wtd_values_blhk</span><span>,</span> <span>params</span><span>.</span><span>w_o_hkd</span><span>)</span>
    <span>return</span> <span>out_bld</span>

<span>def</span> <span>ffn</span><span>(</span><span>x</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w1</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w2</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w3</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>)</span> <span>-&gt;</span> <span>jax</span><span>.</span><span>Array</span><span>:</span>
  <span>return</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>jax</span><span>.</span><span>nn</span><span>.</span><span>silu</span><span>(</span><span>jnp</span><span>.</span><span>dot</span><span>(</span><span>x</span><span>,</span> <span>w1</span><span>))</span> <span>*</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>x</span><span>,</span> <span>w3</span><span>),</span> <span>w2</span><span>)</span>

<span>def</span> <span>transformer</span><span>(</span><span>tokens</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>params</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>)</span> <span>-&gt;</span> <span>jax</span><span>.</span><span>Array</span><span>:</span>
  <span>x</span> <span>=</span> <span>params</span><span>.</span><span>tok_embeddings</span><span>[</span><span>tokens</span><span>]</span>
  <span>def</span> <span>scan_fn</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>):</span>
    <span>h</span> <span>+=</span> <span>attention</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>)</span>
    <span>h</span> <span>+=</span> <span>ffn</span><span>(</span><span>norm</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>.</span><span>ffn_norm</span><span>),</span> <span>layer_weights</span><span>.</span><span>w1</span><span>,</span> <span>layer_weights</span><span>.</span><span>w2</span><span>,</span> <span>layer_weights</span><span>.</span><span>w3</span><span>)</span>
    <span>return</span> <span>h</span><span>,</span> <span>None</span>
  <span>h</span><span>,</span> <span>_</span> <span>=</span> <span>jax</span><span>.</span><span>lax</span><span>.</span><span>scan</span><span>(</span><span>scan_fn</span><span>,</span> <span>x</span><span>,</span> <span>params</span><span>.</span><span>layer_weights</span><span>)</span>
  <span>h</span> <span>=</span> <span>norm</span><span>(</span><span>h</span><span>,</span> <span>params</span><span>.</span><span>norm</span><span>)</span>
  <span>logits</span> <span>=</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>h</span><span>,</span> <span>params</span><span>.</span><span>output</span><span>.</span><span>T</span><span>)</span>
  <span>return</span> <span>logits</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>&#39;__main__&#39;</span><span>:</span>
  <span>vocab_size</span> <span>=</span> <span>32000</span>
  <span>dim</span> <span>=</span> <span>4096</span>
  <span>hidden_dim</span> <span>=</span> <span>14336</span>
  <span>n_layers</span> <span>=</span> <span>1</span>
  <span>n_heads</span> <span>=</span> <span>32</span>
  <span>head_dim</span> <span>=</span> <span>dim</span> <span>//</span> <span>n_heads</span>

  <span>layer_weights</span> <span>=</span> <span>LayerWeights</span><span>(</span>
      <span>attn_norm</span><span>=</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,)),</span>
      <span>ffn_norm</span><span>=</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,)),</span>
      <span>w_q_dhk</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,</span> <span>n_heads</span><span>,</span> <span>head_dim</span><span>)),</span>
      <span>w_k_dhk</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,</span> <span>n_heads</span><span>,</span> <span>head_dim</span><span>)),</span>
      <span>w_v_dhk</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,</span> <span>n_heads</span><span>,</span> <span>head_dim</span><span>)),</span>
      <span>w_o_hkd</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>n_heads</span><span>,</span> <span>head_dim</span><span>,</span> <span>dim</span><span>)),</span>
      <span>w1</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dim</span><span>)),</span>
      <span>w2</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>hidden_dim</span><span>,</span> <span>dim</span><span>)),</span>
      <span>w3</span><span>=</span><span>jnp</span><span>.</span><span>zeros</span><span>((</span><span>n_layers</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dim</span><span>))</span>
    <span>)</span>
  <span>params</span> <span>=</span> <span>XfmrWeights</span><span>(</span><span>tok_embeddings</span><span>=</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>vocab_size</span><span>,</span> <span>dim</span><span>)),</span> <span>layer_weights</span><span>=</span><span>layer_weights</span><span>,</span> <span>norm</span><span>=</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>dim</span><span>,)),</span> <span>output</span><span>=</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>vocab_size</span><span>,</span> <span>dim</span><span>)))</span>
  <span>tokens</span> <span>=</span> <span>jnp</span><span>.</span><span>array</span><span>([[</span><span>123</span><span>,</span><span>234</span><span>,</span><span>234</span><span>,</span><span>345</span><span>,</span><span>446</span><span>]])</span>
  <span>out</span> <span>=</span> <span>transformer</span><span>(</span><span>tokens</span><span>,</span> <span>params</span><span>)</span>
  <span>print</span><span>(</span><span>f</span><span>&#39;</span><span>{</span><span>out</span><span>.</span><span>shape</span><span>=}</span><span>&#39;</span><span>)</span>
</pre></div>
<p>Let’s first look at the simplest one —&gt; FFN
Then we proceed to top down with transformer block and finally into the multi-head attention block (lots of einsum but nothing to be afraid of)</p>
<hr/>
<h2 id="feed-forward-network-mlp">feed forward network / mlp</h2><div><pre><span></span><span>def</span> <span>ffn</span><span>(</span><span>x</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w1</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w2</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>w3</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>)</span> <span>-&gt;</span> <span>jax</span><span>.</span><span>Array</span><span>:</span>
  <span>return</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>jax</span><span>.</span><span>nn</span><span>.</span><span>silu</span><span>(</span><span>jnp</span><span>.</span><span>dot</span><span>(</span><span>x</span><span>,</span> <span>w1</span><span>))</span> <span>*</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>x</span><span>,</span> <span>w3</span><span>),</span> <span>w2</span><span>)</span>
</pre></div>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>FFN</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo>,</mo><msub><mi>W</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mtext>SiLU</mtext><mo stretchy="false">(</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>⊙</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mi>W</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo stretchy="true" fence="true" form="postfix">)</mo></mrow><msub><mi>W</mi><mn>2</mn></msub></mrow></math><p>Transformer layer have the feedforward network typically after the attention blocks to increase non-linearity and capture the information learnt by the attention heads. This MLP has two layers of linear transformation with a SiLU activation.</p>
<ol>
<li>Two parallel linear transformations: <code>dot(x, W1)</code> and <code>dot(x, W3)</code></li>
<li>SiLU activation applied to <code>dot(x, W1)</code></li>
<li>Element-wise multiplication of the result from step 2 with <code>dot(x, W3)</code></li>
<li>Final linear transformation: dot product of the result from step 3 with W2</li>
</ol>
<hr/>
<h3 id="transformer-block">Transformer block</h3><p>Before we go to the transformer block, I want to talk about <code>jax.lax.scan</code> function.</p>
<p>When you have a for loop where you update a value in each step and want to return the final result along with all the intermediate values from each step (np.stack), you use <code>jax.lax.scan</code>.
Under the hood, it can unroll loops (and do some jit stuff) for speedup. Another purpose is to express the scan_fn as a pure function (avoid mutable states).</p>
<div><pre><span></span><span>from</span> <span>jax</span> <span>import</span> <span>lax</span>

<span>def</span> <span>cumulative_sum</span><span>(</span><span>accumulated_sum</span><span>,</span> <span>current_element</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    - `accumulated_sum`: The accumulated sum from the previous loop iteration.</span>
<span>    - `current_element`: The current array element being processed.</span>
<span>    &#34;&#34;&#34;</span>
    <span>new_sum</span> <span>=</span> <span>accumulated_sum</span> <span>+</span> <span>current_element</span>
    <span>return</span> <span>new_sum</span><span>,</span> <span>new_sum</span>  <span># (&#34;carryover&#34;, &#34;accumulated&#34;)</span>

<span>initial_sum</span> <span>=</span> <span>0</span>
<span>final_sum</span><span>,</span> <span>cumulative_sums</span> <span>=</span> <span>lax</span><span>.</span><span>scan</span><span>(</span><span>cumulative_sum</span><span>,</span> <span>initial_sum</span><span>,</span> <span>array</span><span>)</span>
</pre></div>
<p>In a transformer, we need to apply the same operations (attention and feed-forward) multiple times, once for each layer. This is where <code>jax.lax.scan</code> comes in handy.</p>
<p>Instead of writing a loop to apply these operations, we can use <code>scan</code> to do it more efficiently. We use it to write the (Multi-head attention + FFN) block repeatedly.</p>
<p>The transformer function shown is a decoder-decoder only implementation (causal masking is the hint). There is no positional encoding.</p>
<p>
<img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719003057.png" width="40%" alt="Untitled 2"/>
</p>
<div><pre><span></span><span>def</span> <span>transformer</span><span>(</span><span>tokens</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>,</span> <span>params</span><span>:</span> <span>jax</span><span>.</span><span>Array</span><span>)</span> <span>-&gt;</span> <span>jax</span><span>.</span><span>Array</span><span>:</span>
  <span>x</span> <span>=</span> <span>params</span><span>.</span><span>tok_embeddings</span><span>[</span><span>tokens</span><span>]</span>
  <span>def</span> <span>scan_fn</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>):</span>
    <span>h</span> <span>+=</span> <span>attention</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>)</span>
    <span>h</span> <span>+=</span> <span>ffn</span><span>(</span><span>norm</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>.</span><span>ffn_norm</span><span>),</span> <span>layer_weights</span><span>.</span><span>w1</span><span>,</span> <span>layer_weights</span><span>.</span><span>w2</span><span>,</span> <span>layer_weights</span><span>.</span><span>w3</span><span>)</span>
    <span>return</span> <span>h</span><span>,</span> <span>None</span>
  <span>h</span><span>,</span> <span>_</span> <span>=</span> <span>jax</span><span>.</span><span>lax</span><span>.</span><span>scan</span><span>(</span><span>scan_fn</span><span>,</span> <span>x</span><span>,</span> <span>params</span><span>.</span><span>layer_weights</span><span>)</span>
  <span>h</span> <span>=</span> <span>norm</span><span>(</span><span>h</span><span>,</span> <span>params</span><span>.</span><span>norm</span><span>)</span>
  <span>logits</span> <span>=</span> <span>jnp</span><span>.</span><span>dot</span><span>(</span><span>h</span><span>,</span> <span>params</span><span>.</span><span>output</span><span>.</span><span>T</span><span>)</span>
  <span>return</span> <span>logits</span>
</pre></div>
<div><pre><span></span><span>h</span> <span>+=</span> <span>attention</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>)</span>
<span>h</span> <span>+=</span> <span>ffn</span><span>(</span><span>norm</span><span>(</span><span>h</span><span>,</span> <span>layer_weights</span><span>.</span><span>ffn_norm</span><span>),</span> <span>layer_weights</span><span>.</span><span>w1</span><span>,</span> <span>layer_weights</span><span>.</span><span>w2</span><span>,</span> <span>layer_weights</span><span>.</span><span>w3</span><span>)</span>
</pre></div>
<p>These are the residual connections as you can see in the diagram too. We are collecting output from each hidden layer</p>
<hr/>
<p>Next section, we look into the attention block after all &#34;Attention is all you need&#34;</p>
<blockquote><p lang="en" dir="ltr">attention is all you need <a href="https://t.co/pL5spdO1SY">pic.twitter.com/pL5spdO1SY</a></p>— sankalp (@dejavucoder) <a href="https://twitter.com/dejavucoder/status/1737451464069681529?ref_src=twsrc%5Etfw">December 20, 2023</a></blockquote> 
<h3 id="attention-block">Attention block</h3><p>
<img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719002779.png" width="40%" alt="Untitled 3"/>
</p>
<p>Attention is at the heart of transformers, allowing the model to discern which parts of the input to attend to. In this section, we look at the multi-head attention block implementation.</p>
<hr/>
<p><em>Single-Head Attention</em>:
In single-head attention <code>h = 1</code></p>
<p>Before jumping to full attention implementation, let&#39;s just take a minute to go through common einsums here.To understand the einsum involved for creating query matrix. We are projecting the input into a single attention space.</p>
<p><code>query_blk = jnp.einsum(&#39;bld, dk -&gt; blk&#39;, normalized_bld, params.w_q_dk)</code></p>
<p>Here, &#39;b&#39; is batch size, &#39;l&#39; is sequence length, &#39;d&#39; is the model dimension, and &#39;k&#39; is the query/key dimension (latent key space). <code>normalized_bld</code> is the input, <code>params.w_q_dk</code> are learnable weights.</p>
<p><strong>Note: elements in q refers to the token for which attention is calculated, elements in k (latent space) are about tokens that can be attended to</strong></p>
<p>The above einsum is basically a matrix multiplication / dot product between each token’s embedding and set of learnt weight vectors. <code>ld, dk -&gt; lk</code></p>
<p>More formally, this projection transforms each token&#39;s representation from &#39;d&#39; dimensions to &#39;k&#39; dimensions. Here, the <code>h</code> i.e number of heads i 1.</p>
<hr/>
<p><em>Multi-head attention</em>: However, we are using Multi-Head attention in our implementation. You can think of it as single head attention repeated h times.</p>
<p>Honestly I was not 100% clear on why we do summation upon <code>d</code> . Mr. xjdr says think of it as ”for each of these token embeddings, tell me everything you know about them, per attention head, in the latent space of size dim”</p>
<p>If single-head is about looking at a scene through a single lens, multi-head is looking at same scene with multiple lenses, each having different perspective.</p>
<p><code>query_blhk = jnp.einsum(&#39;bld, dhk -&gt; blhk&#39;, normalized_bld, params.w_q_dhk)</code></p>
<p>Notice that this is just matrix multiplication again with an extra dimension (h) where the summation is taking across the d axis. Now we are projecting the input into h attention subspaces.</p>
<hr/>
<p>Now let’s see the full multi-head attention block.</p>
<p>Below is the scaled dot product attention equation. This is calculated for h heads (hence mutli-head)</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/sankalp-1719002783.png" alt="Untitled 4"/></p>
<div><pre><span></span><span>def</span> <span>attention</span><span>(</span><span>input_bld</span><span>,</span> <span>params</span><span>):</span>
<span>    </span><span>&#34;&#34;&#34;</span>
<span>    Implements multi-head self-attention mechanism.</span>
<span>    </span>
<span>    B: batch size</span>
<span>    L: sequence length</span>
<span>    M: memory length (same as L for self-attention)</span>
<span>    D: model dimension</span>
<span>    H: number of attention heads in a layer</span>
<span>    K: size of each attention key or value</span>
<span>    &#34;&#34;&#34;</span>
    
    <span># Layer normalization</span>
    <span>normalized_bld</span> <span>=</span> <span>norm</span><span>(</span><span>input_bld</span><span>,</span> <span>params</span><span>.</span><span>attn_norm</span><span>)</span>
    
    <span># Linear projections to obtain query, key, and value</span>
    <span># Notice they are just matrix multiplications with an extra batch dim</span>
    <span># XWq operation</span>
    <span>query_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_q_dhk</span><span>)</span>
    <span># XWk operation</span>
    <span>key_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_k_dhk</span><span>)</span>
    <span># XWv operation</span>
    <span>value_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bld,dhk-&gt;blhk&#39;</span><span>,</span> <span>normalized_bld</span><span>,</span> <span>params</span><span>.</span><span>w_v_dhk</span><span>)</span>
    
    <span># Compute attention scores (dot product of queries and keys)</span>
    <span># Notice that keys don&#39;t have sequence length, they have memory length</span>
    <span># Memory is the length of context model can attend to</span>
    <span># i.e how many previous tokens it can refer to</span>
    <span>logits_bhlm</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,bmhk-&gt;bhlm&#39;</span><span>,</span> <span>query_blhk</span><span>,</span> <span>key_blhk</span><span>)</span>
    
    
    <span># Get shape for scaling</span>
    <span>_</span><span>,</span> <span>l</span><span>,</span> <span>h</span><span>,</span> <span>k</span> <span>=</span> <span>query_blhk</span><span>.</span><span>shape</span>
    
    <span># Scale dot products by sqrt(d_k)</span>
    <span>logits_bhlm</span> <span>=</span> <span>logits_bhlm</span> <span>/</span> <span>jnp</span><span>.</span><span>sqrt</span><span>(</span><span>k</span><span>)</span>
    
    
    <span># Create causal mask to prevent attending future tokens</span>
    <span># causal mask (lower triangular)</span>
    <span>mask</span> <span>=</span> <span>jnp</span><span>.</span><span>triu</span><span>(</span><span>jnp</span><span>.</span><span>ones</span><span>((</span><span>l</span><span>,</span> <span>l</span><span>)),</span> <span>k</span><span>=</span><span>1</span><span>)</span><span>.</span><span>astype</span><span>(</span><span>input_bld</span><span>.</span><span>dtype</span><span>)</span>
    
    <span># Apply mask (set upper triangular region to -inf)</span>
    <span>logits_bhlm</span> <span>=</span> <span>logits_bhlm</span> <span>-</span> <span>jnp</span><span>.</span><span>inf</span> <span>*</span> <span>mask</span><span>[</span><span>None</span><span>,</span> <span>None</span><span>,</span> <span>:,</span> <span>:]</span>
    
    <span># Apply softmax to get attention weights</span>
    <span>weights_bhlm</span> <span>=</span> <span>jax</span><span>.</span><span>nn</span><span>.</span><span>softmax</span><span>(</span><span>logits_bhlm</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
    
    <span># Compute weighted sum of values</span>
    <span>wtd_values_blhk</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,bhlm-&gt;blhk&#39;</span><span>,</span> <span>value_blhk</span><span>,</span> <span>weights_bhlm</span><span>)</span>
    
    <span># Final linear projection</span>
    <span>out_bld</span> <span>=</span> <span>jnp</span><span>.</span><span>einsum</span><span>(</span><span>&#39;blhk,hkd-&gt;bld&#39;</span><span>,</span> <span>wtd_values_blhk</span><span>,</span> <span>params</span><span>.</span><span>w_o_hkd</span><span>)</span>
    
    <span>return</span> <span>out_bld</span>
</pre></div>
<p>Why summation upon k in <code>logits_bhlm = jnp.einsum(&#39;blhk,bmhk-&gt;bhlm&#39;, query_blhk, key_blhk)</code> ? Since k contains information about which token embeddings to attend to in each head, that’s why we collect information across that axis.</p>
<p>I hope you have a better understanding of einsum and know a bit more about transformers and jax more than before. <strong>Please upvote and share if you liked.</strong></p>
<p>I am also <strong>looking for GenAI(LLMs) oriented roles,</strong> at big companies or funded startups (India (hybrid) and US/EU remote roles). Open for contract roles too. If you are looking out, please drop a DM On <a href="https://x.com/dejavucoder">twitter</a> or send me a &#34;Hi&#34; on hgirl3078@gmail.com.</p>
<p>My background is ~2 years of production experience at backend/generalist software engineering at a mid sized USA based fintech company. I have also dabbled into deep learning(college era) and applied LLMs(recently).</p>
<p>A recent project you may find interesting - codeQA. A chat-with-a-codebase project utilizing tree-sitter to generate AST trees and construct a codebase index for embeddings. Then I implement a simple top-K RAG pipeline with meta-characteristic search, HyDE, LanceDB, BM-25 etc. to get the chatting working.</p>
<ul>
<li><a href="https://sankalp1999.notion.site/Learnings-from-codeQA-Part-1-5eb12ceb948040789d0a0aca1ac23329">Part 1: Learnings from codeQA - Tree-sitter</a></li>
<li><a href="https://sankalp1999.notion.site/Learnings-from-codeQA-Part-2-ed70346f75364e0583a9173d7ea7dcf1">Part 2: Learnings from codeQA - Improving Retrieval</a></li>
</ul>


</div></div>
  </body>
</html>
