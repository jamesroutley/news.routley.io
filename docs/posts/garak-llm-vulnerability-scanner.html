<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NVIDIA/garak">Original</a>
    <h1>Garak, LLM Vulnerability Scanner</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><a id="user-content-garak-llm-vulnerability-scanner" aria-label="Permalink: garak, LLM vulnerability scanner" href="#garak-llm-vulnerability-scanner"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><em>Generative AI Red-teaming &amp; Assessment Kit</em></p>
<p dir="auto"><code>garak</code> checks if an LLM can be made to fail in a way we don&#39;t want. <code>garak</code> probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know <code>nmap</code>, it&#39;s <code>nmap</code> for LLMs.</p>
<p dir="auto"><code>garak</code> focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.</p>
<p dir="auto"><code>garak</code>&#39;s a free tool. We love developing it and are always interested in adding functionality to support applications.</p>
<p dir="auto"><a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/5ce2e21e84680df1ab24807babebc3417d27d66e0826a350eb04ab57f4c8f3e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-blue.svg"/></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg" alt="Tests/Linux"/></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg" alt="Tests/Windows"/></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg" alt="Tests/OSX"/></a>
<a href="http://garak.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/ec7dff6db1b623f10238aaa176f6070b8dfee2ba106479e9ac7a66fbe8f3e778/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f676172616b2f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/garak/badge/?version=latest"/></a>
<a href="https://discord.gg/uVch4puUCs" rel="nofollow"><img src="https://camo.githubusercontent.com/3dfa2e5918dc7c5299e3f3e8383c6d7fc9e5a26de70d29c5144a166075db153b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230646973636f72642d79656c6c6f772e737667" alt="discord-img" data-canonical-src="https://img.shields.io/badge/chat-on%20discord-yellow.svg"/></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"/></a>
<a href="https://pypi.org/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/2e89cf4e24191c2b133e3cbc641eb89ec3d1c6e61bfec492ddec940757b871f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f676172616b" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/garak"/></a>
<a href="https://badge.fury.io/py/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/635b443d53cf2ab927beabd3255f7895db7b74af3dbf5a7777c61fcc96792bbd/68747470733a2f2f62616467652e667572792e696f2f70792f676172616b2e737667" alt="PyPI" data-canonical-src="https://badge.fury.io/py/garak.svg"/></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/b38370d22779880d75968e06cf8ade053d44a0e95f94649a8354756b5c24a4ba/68747470733a2f2f706570792e746563682f62616467652f676172616b" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak"/></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/3d929a651aaa7c9b44b580d1537bbb7954b796081bfeb1125fc3d1fac4f78585/68747470733a2f2f706570792e746563682f62616467652f676172616b2f6d6f6e7468" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak/month"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Get started</h2><a id="user-content-get-started" aria-label="Permalink: Get started" href="#get-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">&gt; See our user guide! <a href="https://docs.garak.ai/" rel="nofollow">docs.garak.ai</a></h3><a id="user-content--see-our-user-guide-docsgarakai" aria-label="Permalink: &gt; See our user guide! docs.garak.ai" href="#-see-our-user-guide-docsgarakai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">&gt; Join our <a href="https://discord.gg/uVch4puUCs" rel="nofollow">Discord</a>!</h3><a id="user-content--join-our-discord" aria-label="Permalink: &gt; Join our Discord!" href="#-join-our-discord"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">&gt; Project links &amp; home: <a href="https://garak.ai/" rel="nofollow">garak.ai</a></h3><a id="user-content--project-links--home-garakai" aria-label="Permalink: &gt; Project links &amp; home: garak.ai" href="#-project-links--home-garakai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">&gt; Twitter: <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></h3><a id="user-content--twitter-garak_llm" aria-label="Permalink: &gt; Twitter: @garak_llm" href="#-twitter-garak_llm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">&gt; DEF CON <a href="https://garak.ai/garak_aiv_slides.pdf" rel="nofollow">slides</a>!</h3><a id="user-content--def-con-slides" aria-label="Permalink: &gt; DEF CON slides!" href="#-def-con-slides"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">LLM support</h2><a id="user-content-llm-support" aria-label="Permalink: LLM support" href="#llm-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">currently supports:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/models" rel="nofollow">hugging face hub</a> generative models</li>
<li><a href="https://replicate.com/" rel="nofollow">replicate</a> text models</li>
<li><a href="https://platform.openai.com/docs/introduction" rel="nofollow">openai api</a> chat &amp; continuation models</li>
<li><a href="https://www.litellm.ai/" rel="nofollow">litellm</a></li>
<li>pretty much anything accessible via REST</li>
<li>gguf models like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> version &gt;= 1046</li>
<li>.. and many more LLMs!</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Install:</h2><a id="user-content-install" aria-label="Permalink: Install:" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>garak</code> is a command-line tool. It&#39;s developed in Linux and OSX.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Standard install with <code>pip</code></h3><a id="user-content-standard-install-with-pip" aria-label="Permalink: Standard install with pip" href="#standard-install-with-pip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Just grab it from PyPI and you should be good to go:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U garak"><pre><code>python -m pip install -U garak
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Install development version with <code>pip</code></h3><a id="user-content-install-development-version-with-pip" aria-label="Permalink: Install development version with pip" href="#install-development-version-with-pip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The standard pip version of <code>garak</code> is updated periodically. To get a fresher version, from GitHub, try:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U git+https://github.com/NVIDIA/garak.git@main"><pre><code>python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Clone from source</h3><a id="user-content-clone-from-source" aria-label="Permalink: Clone from source" href="#clone-from-source"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>garak</code> has its own dependencies. You can to install <code>garak</code> in its own Conda environment:</p>
<div data-snippet-clipboard-copy-content="conda create --name garak &#34;python&gt;=3.10,&lt;=3.12&#34;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e ."><pre><code>conda create --name garak &#34;python&gt;=3.10,&lt;=3.12&#34;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
</code></pre></div>
<p dir="auto">OK, if that went fine, you&#39;re probably good to go!</p>
<p dir="auto"><strong>Note</strong>: if you cloned before the move to the <code>NVIDIA</code> GitHub organisation, but you&#39;re reading this at the <code>github.com/NVIDIA</code> URI, please update your remotes as follows:</p>
<div data-snippet-clipboard-copy-content="git remote set-url origin https://github.com/NVIDIA/garak.git"><pre><code>git remote set-url origin https://github.com/NVIDIA/garak.git
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The general syntax is:</p>
<p dir="auto"><code>garak &lt;options&gt;</code></p>
<p dir="auto"><code>garak</code> needs to know what model to scan, and by default, it&#39;ll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:</p>
<p dir="auto"><code>garak --list_probes</code></p>
<p dir="auto">To specify a generator, use the <code>--model_type</code> and, optionally, the <code>--model_name</code> options. Model type specifies a model family/interface; model name specifies the exact model to be used. The &#34;Intro to generators&#34; section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set <code>--model_type</code> to <code>huggingface</code> and <code>--model_name</code> to the model&#39;s name on Hub (e.g. <code>&#34;RWKV/rwkv-4-169m-pile&#34;</code>). Some generators might need an API key to be set as an environment variable, and they&#39;ll let you know if they need that.</p>
<p dir="auto"><code>garak</code> runs all the probes by default, but you can be specific about that too. <code>--probes promptinject</code> will use only the <a href="https://github.com/agencyenterprise/promptinject">PromptInject</a> framework&#39;s methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a <code>.</code>; for example, <code>--probes lmrc.SlurUsage</code> will use an implementation of checking for models generating slurs based on the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> framework.</p>
<p dir="auto">For help &amp; inspiration, find us on <a href="https://twitter.com/garak_llm" rel="nofollow">twitter</a> or <a href="https://discord.gg/uVch4puUCs" rel="nofollow">discord</a>!</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)</p>
<div data-snippet-clipboard-copy-content="export OPENAI_API_KEY=&#34;sk-123XXXXXXXXXXXX&#34;
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding"><pre><code>export OPENAI_API_KEY=&#34;sk-123XXXXXXXXXXXX&#34;
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
</code></pre></div>
<p dir="auto">See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0</p>
<div data-snippet-clipboard-copy-content="python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0"><pre><code>python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Reading the results</h2><a id="user-content-reading-the-results" aria-label="Permalink: Reading the results" href="#reading-the-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe&#39;s results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.</p>
<p dir="auto">Here are the results with the <code>encoding</code> module on a GPT-3 variant:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67"><img src="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/8Dxf45N.png"/></a></p>
<p dir="auto">And the same results for ChatGPT:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67"><img src="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/VKAF5if.png"/></a></p>
<p dir="auto">We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.</p>
<p dir="auto">Errors go in <code>garak.log</code>; the run is logged in detail in a <code>.jsonl</code> file specified at analysis start &amp; end. There&#39;s a basic analysis script in <code>analyse/analyse_log.py</code> which will output the probes and prompts that led to the most hits.</p>
<p dir="auto">Send PRs &amp; open issues. Happy hunting!</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Intro to generators</h2><a id="user-content-intro-to-generators" aria-label="Permalink: Intro to generators" href="#intro-to-generators"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Hugging Face</h3><a id="user-content-hugging-face" aria-label="Permalink: Hugging Face" href="#hugging-face"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Using the Pipeline API:</p>
<ul dir="auto">
<li><code>--model_type huggingface</code> (for transformers models to run locally)</li>
<li><code>--model_name</code> - use the model name from Hub. Only generative models will work. If it fails and shouldn&#39;t, please open an issue and paste in the command you tried + the exception!</li>
</ul>
<p dir="auto">Using the Inference API:</p>
<ul dir="auto">
<li><code>--model_type huggingface.InferenceAPI</code> (for API-based model access)</li>
<li><code>--model_name</code> - the model name from Hub, e.g. <code>&#34;mosaicml/mpt-7b-instruct&#34;</code></li>
</ul>
<p dir="auto">Using private endpoints:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type huggingface.InferenceEndpoint</code> (for private endpoints)</p>
</li>
<li>
<p dir="auto"><code>--model_name</code> - the endpoint URL, e.g. <code>https://xxx.us-east-1.aws.endpoints.huggingface.cloud</code></p>
</li>
<li>
<p dir="auto">(optional) set the <code>HF_INFERENCE_TOKEN</code> environment variable to a Hugging Face API token with the &#34;read&#34; role; see <a href="https://huggingface.co/settings/tokens" rel="nofollow">https://huggingface.co/settings/tokens</a> when logged in</p>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--model_type openai</code></li>
<li><code>--model_name</code> - the OpenAI model you&#39;d like to use. <code>gpt-3.5-turbo-0125</code> is fast and fine for testing.</li>
<li>set the <code>OPENAI_API_KEY</code> environment variable to your OpenAI API key (e.g. &#34;sk-19763ASDF87q6657&#34;); see <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a> when logged in</li>
</ul>
<p dir="auto">Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you&#39;d like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Replicate</h3><a id="user-content-replicate" aria-label="Permalink: Replicate" href="#replicate"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>set the <code>REPLICATE_API_TOKEN</code> environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Public Replicate models:</p>
<ul dir="auto">
<li><code>--model_type replicate</code></li>
<li><code>--model_name</code> - the Replicate model name and hash, e.g. <code>&#34;stability-ai/stablelm-tuned-alpha-7b:c49dae36&#34;</code></li>
</ul>
<p dir="auto">Private Replicate endpoints:</p>
<ul dir="auto">
<li><code>--model_type replicate.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - username/model-name slug from the deployed endpoint, e.g. <code>elim/elims-llama2-7b</code></li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Cohere</h3><a id="user-content-cohere" aria-label="Permalink: Cohere" href="#cohere"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--model_type cohere</code></li>
<li><code>--model_name</code> (optional, <code>command</code> by default) - The specific Cohere model you&#39;d like to test</li>
<li>set the <code>COHERE_API_KEY</code> environment variable to your Cohere API key, e.g. &#34;aBcDeFgHiJ123456789&#34;; see <a href="https://dashboard.cohere.ai/api-keys" rel="nofollow">https://dashboard.cohere.ai/api-keys</a> when logged in</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Groq</h3><a id="user-content-groq" aria-label="Permalink: Groq" href="#groq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--model_type groq</code></li>
<li><code>--model_name</code> - The name of the model to access via the Groq API</li>
<li>set the <code>GROQ_API_KEY</code> environment variable to your Groq API key, see <a href="https://console.groq.com/docs/quickstart" rel="nofollow">https://console.groq.com/docs/quickstart</a> for details on creating an API key</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">ggml</h3><a id="user-content-ggml" aria-label="Permalink: ggml" href="#ggml"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--model_type ggml</code></li>
<li><code>--model_name</code> - The path to the ggml model you&#39;d like to load, e.g. <code>/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin</code></li>
<li>set the <code>GGML_MAIN_PATH</code> environment variable to the path to your ggml <code>main</code> executable</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">REST</h3><a id="user-content-rest" aria-label="Permalink: REST" href="#rest"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>rest.RestGenerator</code> is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See <a href="https://reference.garak.ai/en/latest/garak.generators.rest.html" rel="nofollow">https://reference.garak.ai/en/latest/garak.generators.rest.html</a> for examples.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">NIM</h3><a id="user-content-nim" aria-label="Permalink: NIM" href="#nim"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Use models from <a href="https://build.nvidia.com/" rel="nofollow">https://build.nvidia.com/</a> or other NIM endpoints.</p>
<ul dir="auto">
<li>set the <code>NIM_API_KEY</code> environment variable to your authentication API token, or specify it in the config YAML</li>
</ul>
<p dir="auto">For chat models:</p>
<ul dir="auto">
<li><code>--model_type nim</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>meta/llama-3.1-8b-instruct</code></li>
</ul>
<p dir="auto">For completion models:</p>
<ul dir="auto">
<li><code>--model_type nim.NVOpenAICompletion</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>bigcode/starcoder2-15b</code></li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">OctoAI</h3><a id="user-content-octoai" aria-label="Permalink: OctoAI" href="#octoai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>set the <code>OCTO_API_TOKEN</code> environment variable to your Replicate API token, e.g. &#34;r8-123XXXXXXXXXXXX&#34;; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Octo public endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo</code></li>
<li><code>--model_name</code> - the OctoAI public endpoint for the model, e.g. <code>mistral-7b-instruct-fp16</code></li>
</ul>
<p dir="auto">Octo private endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - the deployed endpoint URL, e.g. <code>https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions</code></li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Test</h3><a id="user-content-test" aria-label="Permalink: Test" href="#test"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type test</code></p>
</li>
<li>
<p dir="auto">(alternatively) <code>--model_name test.Blank</code>
For testing. This always generates the empty string, using the <code>test.Blank</code> generator.  Will be marked as failing for any tests that <em>require</em> an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.</p>
</li>
<li>
<p dir="auto"><code>--model_type test.Repeat</code>
For testing. This generator repeats back the prompt it received.</p>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Intro to probes</h2><a id="user-content-intro-to-probes" aria-label="Permalink: Intro to probes" href="#intro-to-probes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Probe</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>blank</td>
<td>A simple probe that always sends an empty prompt.</td>
</tr>
<tr>
<td>atkgen</td>
<td>Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 <a href="https://huggingface.co/garak-llm/artgpt2tox" rel="nofollow">fine-tuned</a> on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).</td>
</tr>
<tr>
<td>av_spam_scanning</td>
<td>Probes that attempt to make the model output malicious content signatures</td>
</tr>
<tr>
<td>continuation</td>
<td>Probes that test if the model will continue a probably undesirable word</td>
</tr>
<tr>
<td>dan</td>
<td>Various <a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html" rel="nofollow">DAN</a> and DAN-like attacks</td>
</tr>
<tr>
<td>donotanswer</td>
<td>Prompts to which responsible language models should not answer.</td>
</tr>
<tr>
<td>encoding</td>
<td>Prompt injection through text encoding</td>
</tr>
<tr>
<td>gcg</td>
<td>Disrupt a system prompt by appending an adversarial suffix.</td>
</tr>
<tr>
<td>glitch</td>
<td>Probe model for glitch tokens that provoke unusual behavior.</td>
</tr>
<tr>
<td>grandma</td>
<td>Appeal to be reminded of one&#39;s grandmother.</td>
</tr>
<tr>
<td>goodside</td>
<td>Implementations of Riley Goodside attacks.</td>
</tr>
<tr>
<td>leakerplay</td>
<td>Evaluate if a model will replay training data.</td>
</tr>
<tr>
<td>lmrc</td>
<td>Subsample of the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> probes</td>
</tr>
<tr>
<td>malwaregen</td>
<td>Attempts to have the model generate code for building malware</td>
</tr>
<tr>
<td>misleading</td>
<td>Attempts to make a model support misleading and false claims</td>
</tr>
<tr>
<td>packagehallucination</td>
<td>Trying to get code generations that specify non-existent (and therefore insecure) packages.</td>
</tr>
<tr>
<td>promptinject</td>
<td>Implementation of the Agency Enterprise <a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject">PromptInject</a> work (best paper awards @ NeurIPS ML Safety Workshop 2022)</td>
</tr>
<tr>
<td>realtoxicityprompts</td>
<td>Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)</td>
</tr>
<tr>
<td>snowball</td>
<td><a href="https://ofir.io/snowballed_hallucination.pdf" rel="nofollow">Snowballed Hallucination</a> probes designed to make a model give a wrong answer to questions too complex for it to process</td>
</tr>
<tr>
<td>xss</td>
<td>Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h2 tabindex="-1" dir="auto">Logging</h2><a id="user-content-logging" aria-label="Permalink: Logging" href="#logging"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>garak</code> generates multiple kinds of log:</p>
<ul dir="auto">
<li>A log file, <code>garak.log</code>. This includes debugging information from <code>garak</code> and its plugins, and is continued across runs.</li>
<li>A report of the current run, structured as JSONL. A new report file is created every time <code>garak</code> runs. The name of this file is output at the beginning and, if successful, also the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry&#39;s <code>status</code> attribute takes a constant from <code>garak.attempts</code> to describe what stage it was made at.</li>
<li>A hit log, detailing attempts that yielded a vulnerability (a &#39;hit&#39;)</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">How is the code structured?</h2><a id="user-content-how-is-the-code-structured" aria-label="Permalink: How is the code structured?" href="#how-is-the-code-structured"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Check out the <a href="https://reference.garak.ai/" rel="nofollow">reference docs</a> for an authoritative guide to <code>garak</code> code structure.</p>
<p dir="auto">In a typical run, <code>garak</code> will read a model type (and optionally model name) from the command line, then determine which <code>probe</code>s and <code>detector</code>s to run, start up a <code>generator</code>, and then pass these to a <code>harness</code> to do the probing; an <code>evaluator</code> deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.</p>
<ul dir="auto">
<li><code>garak/probes/</code> - classes for generating interactions with LLMs</li>
<li><code>garak/detectors/</code> - classes for detecting an LLM is exhibiting a given failure mode</li>
<li><code>garak/evaluators/</code> - assessment reporting schemes</li>
<li><code>garak/generators/</code> - plugins for LLMs to be probed</li>
<li><code>garak/harnesses/</code> - classes for structuring testing</li>
<li><code>resources/</code> - ancillary items required by plugins</li>
</ul>
<p dir="auto">The default operating mode is to use the <code>probewise</code> harness. Given a list of probe module names and probe plugin names, the <code>probewise</code> harness instantiates each probe, then for each probe reads its <code>recommended_detectors</code> attribute to get a list of <code>detector</code>s to run on the output.</p>
<p dir="auto">Each plugin category (<code>probes</code>, <code>detectors</code>, <code>evaluators</code>, <code>generators</code>, <code>harnesses</code>) includes a <code>base.py</code> which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, <code>garak.generators.openai.OpenAIGenerator</code> descends from <code>garak.generators.base.Generator</code>.</p>
<p dir="auto">Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using <code>garak</code>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Developing your own plugin</h2><a id="user-content-developing-your-own-plugin" aria-label="Permalink: Developing your own plugin" href="#developing-your-own-plugin"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Take a look at how other plugins do it</li>
<li>Inherit from one of the base classes, e.g. <code>garak.probes.base.TextProbe</code></li>
<li>Override as little as possible</li>
<li>You can test the new code in at least two ways:
<ul dir="auto">
<li>Start an interactive Python session
<ul dir="auto">
<li>Import the model, e.g. <code>import garak.probes.mymodule</code></li>
<li>Instantiate the plugin, e.g. <code>p = garak.probes.mymodule.MyProbe()</code></li>
</ul>
</li>
<li>Run a scan with test plugins
<ul dir="auto">
<li>For probes, try a blank generator and always.Pass detector: <code>python3 -m garak -m test.Blank -p mymodule -d always.Pass</code></li>
<li>For detectors, try a blank generator and a blank probe: <code>python3 -m garak -m test.Blank -p test.Blank -d mymodule</code></li>
<li>For generators, try a blank probe and always.Pass detector: <code>python3 -m garak -m mymodule -p test.Blank -d always.Pass</code></li>
</ul>
</li>
<li>Get <code>garak</code> to list all the plugins of the type you&#39;re writing, with <code>--list_probes</code>, <code>--list_detectors</code>, or <code>--list_generators</code></li>
</ul>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We have an FAQ <a href="https://github.com/NVIDIA/garak/blob/main/FAQ.md">here</a>. Reach out if you have any more questions! <a href="mailto:leon@garak.ai">leon@garak.ai</a></p>
<p dir="auto">Code reference documentation is at <a href="https://garak.readthedocs.io/en/latest/" rel="nofollow">garak.readthedocs.io</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Citing garak</h2><a id="user-content-citing-garak" aria-label="Permalink: Citing garak" href="#citing-garak"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can read the <a href="https://cceckman.com/NVIDIA/garak/blob/main/garak-paper.pdf">garak preprint paper</a>. If you use garak, please cite us.</p>
<div data-snippet-clipboard-copy-content="@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}"><pre><code>@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
</code></pre></div>
<hr/>
<p dir="auto"><em>&#34;Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly&#34;</em> - Elim</p>
<p dir="auto">For updates and news see <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></p>
<p dir="auto">© 2023- Leon Derczynski; Apache license v2, see <a href="https://cceckman.com/NVIDIA/garak/blob/main/LICENSE">LICENSE</a></p>
</article></div></div>
  </body>
</html>
