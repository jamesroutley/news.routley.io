<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/poeticoding/yolo_elixir">Original</a>
    <h1>Show HN: Real-Time YOLO Object Detection in Elixir: Fast, Simple, Extensible</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">You Only Look Once (YOLO) is a cutting-edge, real-time object detection system. It works by predicting bounding boxes and class probabilities directly from an input image in a single evaluation, making it exceptionally fast compared to other object detection methods. YOLO models are widely used in applications like autonomous driving, surveillance, and robotics due to their balance of speed and accuracy.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/guides/images/traffic_detections.jpg"><img src="https://decomposition.al/poeticoding/yolo_elixir/raw/main/guides/images/traffic_detections.jpg" alt="traffic image"/></a></p>
<p dir="auto"><code>YOLO</code> is an Elixir library designed to simplify object detection by providing seamless integration of YOLO models. With this library, you can efficiently utilize the power of YOLO for real-time object detection.</p>
<p dir="auto">The library is built with the following objectives:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Speed</strong></p>
</li>
<li>
<p dir="auto"><strong>Ease of Use</strong></p>
</li>
<li>
<p dir="auto"><strong>Extensibility</strong></p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">IMPORTANT: ONNX model files!</h2><a id="user-content-important-onnx-model-files" aria-label="Permalink: IMPORTANT: ONNX model files!" href="#important-onnx-model-files"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To run a YOLOv8 model, we typically need the neural network structure and the pre-trained weights obtained from training the model. While we could build the network using libraries like Axon (or Keras in Python), I&#39;ve prefered to simplify the process by using a pre-existing PyTorch model from <a href="https://docs.ultralytics.com/models/yolov8/" rel="nofollow">Ultralytics</a>. Ultralytics developed and trained <a href="https://github.com/ultralytics/assets/releases/tag/v8.3.0">YOLOv8 models</a>, which are available under a GPL license. However, to avoid potential licensing conflicts with this library, I haven&#39;t included the ONNX model exports directly here. Instead, you need to convert the PyTorch .pt file into an ONNX format. No worries, I got you covered!</p>
<p dir="auto">Ultralytics&#39; YOLOv8 comes in several sizes: n (nano), s (small), m (medium), l (large), and x (extra-large). Larger models offer better performance in terms of classification accuracy and object detection but require more computational resources and memory.</p>
<p dir="auto">You can use the <code>python/yolov8_to_onnx.py</code> script found in the <a href="https://github.com/poeticoding/yolo_elixir">GitHub repo</a>.</p>
<p dir="auto">First, install the dependencies (<code>requests</code> and <code>ultralytics</code>)</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r python/requirements.txt"><pre>pip install -r python/requirements.txt</pre></div>
<p dir="auto">Then, run the script by specifying the model size, such as <code>n</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python python/yolov8_to_onnx.py n"><pre>python python/yolov8_to_onnx.py n</pre></div>
<p dir="auto">The script will download the <code>.pt</code> model and generate two files:</p>
<ul dir="auto">
<li><code>models/yolov8n.onnx</code>: the YOLOv8n model with weights</li>
<li><code>models/yolov8n_classes.json</code>: the list of object classes</li>
</ul>

<p dir="auto">First install the library and configure Nx.</p>
<div dir="auto" data-snippet-clipboard-copy-content="Mix.install([
  {:yolo, &#34;&gt;= 0.0.0&#34;},

  # I&#39;m using EXLA as Nx backend on my MacBook Air M3
  {:exla, &#34;~&gt; 0.9.2&#34;},
  # evision for image processing (you can use :image instead)
  {:evision, &#34;~&gt; 0.2.0&#34;}
]
], config: [
  nx: [default_backend: EXLA.Backend]
])"><pre><span>Mix</span><span>.</span><span>install</span><span>(</span><span>[</span>
  <span>{</span><span>:yolo</span><span>,</span> <span>&#34;&gt;= 0.0.0&#34;</span><span>}</span><span>,</span>

  <span># I&#39;m using EXLA as Nx backend on my MacBook Air M3</span>
  <span>{</span><span>:exla</span><span>,</span> <span>&#34;~&gt; 0.9.2&#34;</span><span>}</span><span>,</span>
  <span># evision for image processing (you can use :image instead)</span>
  <span>{</span><span>:evision</span><span>,</span> <span>&#34;~&gt; 0.2.0&#34;</span><span>}</span>
<span>]</span>
<span>]</span><span>,</span> <span>config: </span><span>[</span>
  <span>nx: </span><span>[</span><span>default_backend: </span><span>EXLA.Backend</span><span>]</span>
<span>]</span><span>)</span></pre></div>
<p dir="auto">Then you need just a few lines of code to get a list of objects detected in the image.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = YOLO.load([
  model_path: &#34;models/yolov8n.onnx&#34;, 
  classes_path: &#34;models/yolov8n_classes.json&#34;
])

mat = Evision.imread(image_path)

model
|&gt; YOLO.detect(mat)
|&gt; YOLO.to_detected_objects(model.classes)
"><pre><span>model</span> <span>=</span> <span>YOLO</span><span>.</span><span>load</span><span>(</span><span>[</span>
  <span>model_path: </span><span>&#34;models/yolov8n.onnx&#34;</span><span>,</span> 
  <span>classes_path: </span><span>&#34;models/yolov8n_classes.json&#34;</span>
<span>]</span><span>)</span>

<span>mat</span> <span>=</span> <span>Evision</span><span>.</span><span>imread</span><span>(</span><span>image_path</span><span>)</span>

<span>model</span>
<span>|&gt;</span> <span>YOLO</span><span>.</span><span>detect</span><span>(</span><span>mat</span><span>)</span>
<span>|&gt;</span> <span>YOLO</span><span>.</span><span>to_detected_objects</span><span>(</span><span>model</span><span>.</span><span>classes</span><span>)</span></pre></div>
<p dir="auto">This code returns a list of detected objects along with their bounding box coordinates.</p>
<div dir="auto" data-snippet-clipboard-copy-content="[
  %{
    class: &#34;person&#34;,
    prob: 0.5744523406028748,
    bbox: %{h: 126, w: 70, cx: 700, cy: 570},
    class_idx: 0
  },
  %{
    class: &#34;bicycle&#34;,
    prob: 0.6159384846687317,
    bbox: %{h: 102, w: 71, cx: 726, cy: 738},
    class_idx: 1
  },
  %{class: &#34;car&#34;, prob: 0.6243442893028259, bbox: %{h: 87, w: 102, cx: 1039, cy: 268}, class_idx: 2},
  ...
]"><pre><span>[</span>
  <span>%</span><span>{</span>
    <span>class: </span><span>&#34;person&#34;</span><span>,</span>
    <span>prob: </span><span>0.5744523406028748</span><span>,</span>
    <span>bbox: </span><span>%</span><span>{</span><span>h: </span><span>126</span><span>,</span> <span>w: </span><span>70</span><span>,</span> <span>cx: </span><span>700</span><span>,</span> <span>cy: </span><span>570</span><span>}</span><span>,</span>
    <span>class_idx: </span><span>0</span>
  <span>}</span><span>,</span>
  <span>%</span><span>{</span>
    <span>class: </span><span>&#34;bicycle&#34;</span><span>,</span>
    <span>prob: </span><span>0.6159384846687317</span><span>,</span>
    <span>bbox: </span><span>%</span><span>{</span><span>h: </span><span>102</span><span>,</span> <span>w: </span><span>71</span><span>,</span> <span>cx: </span><span>726</span><span>,</span> <span>cy: </span><span>738</span><span>}</span><span>,</span>
    <span>class_idx: </span><span>1</span>
  <span>}</span><span>,</span>
  <span>%</span><span>{</span><span>class: </span><span>&#34;car&#34;</span><span>,</span> <span>prob: </span><span>0.6243442893028259</span><span>,</span> <span>bbox: </span><span>%</span><span>{</span><span>h: </span><span>87</span><span>,</span> <span>w: </span><span>102</span><span>,</span> <span>cx: </span><span>1039</span><span>,</span> <span>cy: </span><span>268</span><span>}</span><span>,</span> <span>class_idx: </span><span>2</span><span>}</span><span>,</span>
  <span>...</span>
<span>]</span></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/guides/images/traffic_detections.jpg"><img src="https://decomposition.al/poeticoding/yolo_elixir/raw/main/guides/images/traffic_detections.jpg" alt="traffic image"/></a></p>

<p dir="auto">To run a larger YoloV8 model is simple.</p>
<p dir="auto">Use the python script to generate the <code>yolov8x.onnx</code> model and the <code>yolov8x_classes.json</code> file.</p>
<div data-snippet-clipboard-copy-content="python python/yolov8_to_onnx.py x"><pre><code>python python/yolov8_to_onnx.py x
</code></pre></div>
<p dir="auto">Then you can load the model and use it just like before.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model_x = YOLO.load(model_path: &#34;code/yolo/models/yolov8x.onnx&#34;, classes_path: classes_path)

model_x
|&gt; YOLO.detect(mat)
|&gt; YOLO.to_detected_objects(model_x.classes)    
"><pre><span>model_x</span> <span>=</span> <span>YOLO</span><span>.</span><span>load</span><span>(</span><span>model_path: </span><span>&#34;code/yolo/models/yolov8x.onnx&#34;</span><span>,</span> <span>classes_path: </span><span>classes_path</span><span>)</span>

<span>model_x</span>
<span>|&gt;</span> <span>YOLO</span><span>.</span><span>detect</span><span>(</span><span>mat</span><span>)</span>
<span>|&gt;</span> <span>YOLO</span><span>.</span><span>to_detected_objects</span><span>(</span><span>model_x</span><span>.</span><span>classes</span><span>)</span>    </pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/guides/images/traffic_yolov8x_detections.jpg"><img src="https://decomposition.al/poeticoding/yolo_elixir/raw/main/guides/images/traffic_yolov8x_detections.jpg" alt="traffic image"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Make it faster with FastNMS</h2><a id="user-content-make-it-faster-with-fastnms" aria-label="Permalink: Make it faster with FastNMS" href="#make-it-faster-with-fastnms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Non-Maximum Suppression (NMS) is what makes the postprocessing slow in the Elixir implementation. The native Nx implementation is not fast enough to achieve good real-time performance. With the <code>YoloFastNMS</code> library providing a Rust NIF implementation, it&#39;s possible to run the postprocessing much faster - simply include the dependency and pass the module to <code>detect/3</code>. This can <strong>speed up postprocessing by around 100x</strong> (from ~400ms to ~4ms on a MacBook Air M3).</p>
<div dir="auto" data-snippet-clipboard-copy-content="def deps do
  [
    {:yolo_fast_nms, &#34;~&gt; 0.1&#34;}
  ]
end"><pre><span>def</span> <span>deps</span> <span>do</span>
  <span>[</span>
    <span>{</span><span>:yolo_fast_nms</span><span>,</span> <span>&#34;~&gt; 0.1&#34;</span><span>}</span>
  <span>]</span>
<span>end</span></pre></div>
<p dir="auto">Then you can run the detection with FastNMS by passing the <code>YoloFastNMS</code> module to <code>detect/3</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="YOLO.detect(model, mat, nms_fun: &amp;YoloFastNMS.run/3)"><pre><span>YOLO</span><span>.</span><span>detect</span><span>(</span><span>model</span><span>,</span> <span>mat</span><span>,</span> <span>nms_fun: </span><span>&amp;</span><span>YoloFastNMS</span><span>.</span><span>run</span><span>/</span><span>3</span><span>)</span></pre></div>

<p dir="auto">Benchmarks below are run on a MacBook Air M3 with EXLA as Nx backend.</p>
<div data-snippet-clipboard-copy-content="Operating System: macOS
CPU Information: Apple M3
Number of Available Cores: 8
Available memory: 16 GB
Elixir 1.17.2
Erlang 27.1.2
JIT enabled: true"><pre lang="text"><code>Operating System: macOS
CPU Information: Apple M3
Number of Available Cores: 8
Available memory: 16 GB
Elixir 1.17.2
Erlang 27.1.2
JIT enabled: true
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto"><code>detect/3</code> with <code>YoloFastNMS</code> vs Ultralytics</h3><a id="user-content-detect3-with-yolofastnms-vs-ultralytics" aria-label="Permalink: detect/3 with YoloFastNMS vs Ultralytics" href="#detect3-with-yolofastnms-vs-ultralytics"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/benchmarks/yolov8n.exs">benchmarks/yolov8n.exs</a></p>
<div data-snippet-clipboard-copy-content="Name                            ips        average  deviation         median         99th %
detect/3 with FastNMS         26.09       38.32 ms     ±4.43%       38.09 ms       48.83 ms

Memory usage statistics:

Name                          average  deviation         median         99th %
detect/3 with FastNMS        48.42 KB     ±5.92%       47.25 KB       56.06 KB"><pre lang="text"><code>Name                            ips        average  deviation         median         99th %
detect/3 with FastNMS         26.09       38.32 ms     ±4.43%       38.09 ms       48.83 ms

Memory usage statistics:

Name                          average  deviation         median         99th %
detect/3 with FastNMS        48.42 KB     ±5.92%       47.25 KB       56.06 KB
</code></pre></div>
<p dir="auto"><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/benchmarks/yolov8n_cpu.py">benchmarks/yolov8n_cpu.py</a></p>
<div data-snippet-clipboard-copy-content="0: 384x640 17 persons, 3 bicycles, 6 cars, 1 truck, 2 traffic lights, 39.0ms
Speed: 1.2ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)"><pre lang="text"><code>0: 384x640 17 persons, 3 bicycles, 6 cars, 1 truck, 2 traffic lights, 39.0ms
Speed: 1.2ms preprocess, 39.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)
</code></pre></div>

<p dir="auto"><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/benchmarks/nms.exs">benchmarks/nms.exs</a></p>
<div data-snippet-clipboard-copy-content="Name                  ips        average  deviation         median         99th %
YoloFastNMS        528.63        1.89 ms     ±0.56%        1.89 ms        1.93 ms
YOLO.NMS             2.28      437.78 ms     ±0.43%      438.21 ms      441.01 ms

Comparison:
YoloFastNMS        528.63
YOLO.NMS             2.28 - 231.42x slower +435.89 ms

Memory usage statistics:

Name           Memory usage
YoloFastNMS      0.00651 MB
YOLO.NMS          810.22 MB - 124497.68x memory usage +810.21 MB"><pre lang="text"><code>Name                  ips        average  deviation         median         99th %
YoloFastNMS        528.63        1.89 ms     ±0.56%        1.89 ms        1.93 ms
YOLO.NMS             2.28      437.78 ms     ±0.43%      438.21 ms      441.01 ms

Comparison:
YoloFastNMS        528.63
YOLO.NMS             2.28 - 231.42x slower +435.89 ms

Memory usage statistics:

Name           Memory usage
YoloFastNMS      0.00651 MB
YOLO.NMS          810.22 MB - 124497.68x memory usage +810.21 MB
</code></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto"><code>preprocess/3</code>, <code>run/2</code>, <code>postprocess/4</code></h3><a id="user-content-preprocess3-run2-postprocess4" aria-label="Permalink: preprocess/3, run/2, postprocess/4" href="#preprocess3-run2-postprocess4"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/benchmarks/yolov8n_pipeline.exs">benchmarks/yolov8n_pipeline.exs</a></p>
<div data-snippet-clipboard-copy-content="Name                          ips        average  deviation         median         99th %
preprocess                 467.31        2.14 ms    ±11.15%        2.12 ms        2.76 ms
postprocess FastNMS        421.43        2.37 ms     ±6.04%        2.36 ms        2.68 ms
run                         27.04       36.98 ms     ±3.01%       36.74 ms       40.18 ms

Comparison:
preprocess                 467.31
postprocess FastNMS        421.43 - 1.11x slower +0.23 ms
run                         27.04 - 17.28x slower +34.84 ms

Memory usage statistics:

Name                        average  deviation         median         99th %
preprocess                 43.78 KB     ±0.00%       43.78 KB       43.78 KB
postprocess FastNMS         2.02 KB    ±92.71%        1.18 KB        6.62 KB
run                         1.18 KB     ±0.00%        1.18 KB        1.18 KB

Comparison:
preprocess                 43.78 KB
postprocess FastNMS         2.02 KB - 0.05x memory usage -41.76387 KB
run                         1.18 KB - 0.03x memory usage -42.60156 KB"><pre lang="text"><code>Name                          ips        average  deviation         median         99th %
preprocess                 467.31        2.14 ms    ±11.15%        2.12 ms        2.76 ms
postprocess FastNMS        421.43        2.37 ms     ±6.04%        2.36 ms        2.68 ms
run                         27.04       36.98 ms     ±3.01%       36.74 ms       40.18 ms

Comparison:
preprocess                 467.31
postprocess FastNMS        421.43 - 1.11x slower +0.23 ms
run                         27.04 - 17.28x slower +34.84 ms

Memory usage statistics:

Name                        average  deviation         median         99th %
preprocess                 43.78 KB     ±0.00%       43.78 KB       43.78 KB
postprocess FastNMS         2.02 KB    ±92.71%        1.18 KB        6.62 KB
run                         1.18 KB     ±0.00%        1.18 KB        1.18 KB

Comparison:
preprocess                 43.78 KB
postprocess FastNMS         2.02 KB - 0.05x memory usage -41.76387 KB
run                         1.18 KB - 0.03x memory usage -42.60156 KB
</code></pre></div>

<ul dir="auto">
<li><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/examples/yolov8_single_image.livemd">YoloV8 Single Image</a>: A beginner-friendly example that demonstrates object detection on a single image, comparing results between the lightweight YOLOv8n model and the more accurate YOLOv8x model.</li>
<li><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/examples/yolo_fast_nms.livemd">Make it faster with YoloFastNMS</a>: A livebook that shows how to use the <code>YoloFastNMS</code> library to speed up the object detection postprocessing.</li>
<li><a href="https://decomposition.al/poeticoding/yolo_elixir/blob/main/examples/yolov8_webcam.livemd">Real-time Object Detection</a>: A livebook that demonstrates real-time object detection using your computer&#39;s webcam, with live frame updates and visualization of detected objects.</li>
</ul>

<p dir="auto">Let&#39;s see how <code>YOLO.detect/3</code> works.</p>

<p dir="auto">Loads the <em>YoloV8n</em> model using the <code>model_path</code> and <code>classes_path</code>. Optionally, specify <code>model_impl</code>, which defaults to <code>YOLO.Models.YoloV8</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = YOLO.load([
  model_path: &#34;models/yolov8n.onnx&#34;, 
  classes_path: &#34;models/yolov8n_classes.json&#34;
])"><pre><span>model</span> <span>=</span> <span>YOLO</span><span>.</span><span>load</span><span>(</span><span>[</span>
  <span>model_path: </span><span>&#34;models/yolov8n.onnx&#34;</span><span>,</span> 
  <span>classes_path: </span><span>&#34;models/yolov8n_classes.json&#34;</span>
<span>]</span><span>)</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="mat = Evision.imread(image_path)

{input_tensor, scaling_config} = YOLO.Models.YoloV8.preprocess(model, mat, [frame_scaler: YOLO.FrameScalers.EvisionScaler])"><pre><span>mat</span> <span>=</span> <span>Evision</span><span>.</span><span>imread</span><span>(</span><span>image_path</span><span>)</span>

<span>{</span><span>input_tensor</span><span>,</span> <span>scaling_config</span><span>}</span> <span>=</span> <span>YOLO.Models.YoloV8</span><span>.</span><span>preprocess</span><span>(</span><span>model</span><span>,</span> <span>mat</span><span>,</span> <span>[</span><span>frame_scaler: </span><span>YOLO.FrameScalers.EvisionScaler</span><span>]</span><span>)</span></pre></div>
<p dir="auto">Before running object detection, the input image needs to be preprocessed to match the model&#39;s expected input format. The preprocessing steps are:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Resize and Pad Image to 640x640</strong></p>
<ul dir="auto">
<li>The image is resized while preserving aspect ratio to fit within 640x640 pixels</li>
<li>Any remaining space is padded with gray color (value 114) to reach exactly 640x640</li>
<li>This is handled by the <code>FrameScaler</code> behaviour and its implementations</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Convert to Normalized Tensor</strong></p>
<ul dir="auto">
<li>The image is converted to an Nx tensor with shape <code>{1, 3, 640, 640}</code></li>
<li>Pixel values are normalized from <code>0-255</code> to <code>0.0-1.0</code> range</li>
<li>The channels are reordered from <code>RGB</code> to the model&#39;s expected format (<code>BGR</code> in this case)</li>
</ul>
</li>
</ol>
<p dir="auto">The <code>FrameScaler</code> behaviour provides a consistent interface for handling different image formats:</p>
<ul dir="auto">
<li><code>EvisionScaler</code> - For OpenCV Mat images from Evision</li>
<li><code>ImageScaler</code> - For images using the Image library</li>
<li><code>NxIdentityScaler</code> - For ready to use Nx tensors</li>
</ul>

<p dir="auto">Then run the detection by passing the <code>model</code> and the image tensor <code>input_tensor</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# input_tensor {1, 3, 640, 640}
output_tensor = YOLO.Models.run(model, input_tensor)
# output_tensor {1, 84, 8400}"><pre><span># input_tensor {1, 3, 640, 640}</span>
<span>output_tensor</span> <span>=</span> <span>YOLO.Models</span><span>.</span><span>run</span><span>(</span><span>model</span><span>,</span> <span>input_tensor</span><span>)</span>
<span># output_tensor {1, 84, 8400}</span></pre></div>
<p dir="auto">You can also adjust detection thresholds (<code>iou_threshold</code> and <code>prob_threshold</code>, which both default to <code>0.45</code> and <code>0.25</code> respectively) using the third argument.</p>

<div dir="auto" data-snippet-clipboard-copy-content="result_rows = YOLO.Models.YoloV8.postprocess(model, output_tensor, scaling_config, opts)"><pre><span>result_rows</span> <span>=</span> <span>YOLO.Models.YoloV8</span><span>.</span><span>postprocess</span><span>(</span><span>model</span><span>,</span> <span>output_tensor</span><span>,</span> <span>scaling_config</span><span>,</span> <span>opts</span><span>)</span></pre></div>
<p dir="auto">where <code>result_rows</code> is a list of lists, where each inner list represents a detected object with 6 elements:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[
  [cx, cy, w, h, prob, class_idx],
  ...
]"><pre><span>[</span>
  <span>[</span><span>cx</span><span>,</span> <span>cy</span><span>,</span> <span>w</span><span>,</span> <span>h</span><span>,</span> <span>prob</span><span>,</span> <span>class_idx</span><span>]</span><span>,</span>
  <span>...</span>
<span>]</span></pre></div>
<p dir="auto">The model&#39;s raw output needs to be post-processed to extract meaningful detections. For YOLOv8n, the <code>output_tensor</code> has shape <code>{1, 84, 8400}</code> where:</p>
<ul dir="auto">
<li>84 represents 4 bbox coordinates + 80 class probabilities</li>
<li>8400 represents the number of candidate detections</li>
</ul>
<p dir="auto">The postprocessing steps are:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Filter Low Probability Detections</strong></p>
<ul dir="auto">
<li>Each of the 8400 detections has probabilities for 80 classes</li>
<li>Only keep detections where max class probability exceeds <code>prob_threshold</code> (default 0.25)</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Non-Maximum Suppression (NMS)</strong></p>
<ul dir="auto">
<li>Remove overlapping boxes for the same object</li>
<li>For each class, compare boxes using Intersection over Union (IoU)</li>
<li>If IoU &gt; <code>iou_threshold</code> (default 0.45), keep only highest probability box</li>
<li>This prevents multiple detections of the same object</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Scale Coordinates</strong></p>
<ul dir="auto">
<li>The detected coordinates are based on the model&#39;s 640x640 input</li>
<li>Use the <code>scaling_config</code> from preprocessing to map back to original image size</li>
<li>This accounts for any resizing/padding done during preprocessing</li>
</ul>
</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">Convert Detections to Structured Maps</h3><a id="user-content-convert-detections-to-structured-maps" aria-label="Permalink: Convert Detections to Structured Maps" href="#convert-detections-to-structured-maps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Finally, convert the raw detection results into structured maps containing bounding box coordinates, class labels, and probabilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="iex&gt; YOLO.to_detected_objects(result_rows, model.classes)
[
  %{
    class: &#34;person&#34;,
    prob: 0.57,
    bbox: %{h: 126, w: 70, cx: 700, cy: 570},
    class_idx: 0
  },
  ...
]"><pre><span>iex</span><span>&gt;</span> <span>YOLO</span><span>.</span><span>to_detected_objects</span><span>(</span><span>result_rows</span><span>,</span> <span>model</span><span>.</span><span>classes</span><span>)</span>
<span>[</span>
  <span>%</span><span>{</span>
    <span>class: </span><span>&#34;person&#34;</span><span>,</span>
    <span>prob: </span><span>0.57</span><span>,</span>
    <span>bbox: </span><span>%</span><span>{</span><span>h: </span><span>126</span><span>,</span> <span>w: </span><span>70</span><span>,</span> <span>cx: </span><span>700</span><span>,</span> <span>cy: </span><span>570</span><span>}</span><span>,</span>
    <span>class_idx: </span><span>0</span>
  <span>}</span><span>,</span>
  <span>...</span>
<span>]</span></pre></div>

<p dir="auto">The current implementation supports YOLOv8 models with a fixed <code>640x640</code> input size (even though YOLOv8x6 supports <code>1280x1280</code> images) and a fixed <code>84x8400</code> output size. This setup handles 80 classes from the COCO dataset and 8400 detections.</p>
<p dir="auto">The library is designed to be extensible through the <code>YOLO.Model</code> behaviour, allowing other YOLO versions or custom model implementations to be added in the future.</p>

<p dir="auto">One of the next goals (listed in the TODO section below) is to support models with different input and output sizes. This update would allow the library to work with YOLO models trained on other datasets or even custom datasets, making it more flexible and useful.</p>


<ul>
<li> Support dynamically different input and output shapes. (This is going to be foundamental to support different models with custom classes).</li>
<li> Kino library to easily visualize detections</li>
</ul>

<ul>
<li> CUDA benchmarks</li>
<li> Object tracking</li>
<li> Run it on Nerves Rpi5</li>
<li> Run it on Jetson</li>
<li> Sharding</li>
</ul>
</article></div></div>
  </body>
</html>
