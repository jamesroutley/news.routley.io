<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2210.07558">Original</a>
    <h1>DyLoRA: Parameter Efficient Tuning of Pre-Trained Models</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2210.07558">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  With the ever-growing size of pre-trained models (PMs), fine-tuning them has
become more expensive and resource-hungry. As a remedy, low-rank adapters
(LoRA) keep the main pre-trained weights of the model frozen and just introduce
some learnable truncated SVD modules (so-called LoRA blocks) to the model.
While LoRA blocks are parameter efficient, they suffer from two major problems:
first, the size of these blocks is fixed and cannot be modified after training
(for example, if we need to change the rank of LoRA blocks, then we need to
re-train them from scratch); second, optimizing their rank requires an
exhaustive search and effort. In this work, we introduce a dynamic low-rank
adaptation (DyLoRA) technique to address these two problems together. Our
DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank
by sorting out the representation learned by the adapter module at different
ranks during training. We evaluate our solution on different tasks of the GLUE
benchmark using the RoBERTa model. Our results show that we can train dynamic
search-free models with DyLoRA at least $7\times$ faster than LoRA without
significantly compromising performance. Moreover, our models can perform
consistently well on a much larger range of ranks compared to LoRA.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Mojtaba Valipour [<a href="https://arxiv.org/show-email/fbf66644/2210.07558">view email</a>]
      </p></div></div>
  </body>
</html>
