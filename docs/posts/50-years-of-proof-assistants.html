<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lawrencecpaulson.github.io//2025/12/05/History_of_Proof_Assistants.html">Original</a>
    <h1>50 years of proof assistants</h1>
    
    <div id="readability-page-1" class="page">
    


    <section>

      
<p>05 Dec 2025</p>

<span>[
  
    
    <a href="https://lawrencecpaulson.github.io/tag/memories"><code><nobr>memories</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/LCF"><code><nobr>LCF</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/HOL_system"><code><nobr>HOL system</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/Isabelle"><code><nobr>Isabelle</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/Coq"><code><nobr>Coq</nobr></code> </a>
  
    
    <a href="https://lawrencecpaulson.github.io/tag/MJC_Gordon"><code><nobr>MJC Gordon</nobr></code> </a>
  
]</span>

<p>Crackpots ranging from billionaire Peter Thiel to random YouTube influencers claim that science has been stagnating for the past 50 years. They admit that computing is an exception: they don’t pretend that my personal 32GB laptop is not an advance over the 16MB mainframe that served the whole Caltech community when I was there. Instead they claim that advances in computing were driven solely by industrial research, quite overlooking the role of academia 
and government funding
in pushing the VLSI revolution, RISC processor design, networking, hypertext, virtual memory and indeed computers themselves. As for the industrial research,
most of it came from just two “blue sky” institutes – <a href="https://sites.stat.columbia.edu/gelman/research/published/bell.pdf">Bell Labs</a> 
and <a href="https://spectrum.ieee.org/xerox-parc">Xerox PARC</a> – that closed a long time ago. 
LCF-style proof assistants are a world away from mainstream computing,
so let’s look at 50 years of progress there.</p>

<h3 id="19751985-edinburgh-lcf">1975–1985: Edinburgh LCF</h3>

<p>The first instance of LCF was Stanford LCF, developed by Robin Milner in 1972, but it was <strong>not</strong> an LCF-style proof assistant! LCF meant “Logic for Computable Functions”, a quirky formalism based on Scott domains and intended for reasoning about small functional programs. But “LCF-style proof assistant” means one that, like Edinburgh LCF, was coded in some form of 
the ML programming language and provided a proof kernel, 
encapsulated in an abstract type definition, to ensure that a theorem could only be generated 
by applying inference rules to axioms or other theorems:</p>

<blockquote>
  <p>… the ML type discipline is used… so that—whatever complex procedures are defined—all values of type <code>thm</code> must be theorems, as only inferences can compute such values…. This security releases us from the need to preserve whole proofs… — an important practical gain since large proofs tended to clog up the working space… [<em>Edinburgh LCF</em>, page IV]</p>
</blockquote>

<p>Edinburgh LCF was first announced in 1975, which conveniently is exactly 50 years ago, 
at the almost mythical conference on <em>Proving and Improving Programs</em> held at Arc-et-Senans. 
The <a href="https://link.springer.com/book/10.1007/3-540-09724-4">user manual</a>, published in the Springer lecture notes series, came out in 1979.
Edinburgh LCF introduced some other principles that people still adhere to today:</p>

<ul>
  <li>inference rules in the <em>natural deduction</em> style, with a dynamic set of assumptions</li>
  <li>a <em>goal-directed</em> proof style, where you start with the theorem statement and work backwards</li>
  <li>a structured system of <em>theories</em> to organise groups of definitions</li>
</ul>

<p>Edinburgh LCF had its own version of the ML language.
It supported a fragment of first-order logic containing
the logical symbols $\forall$, $\land$ and $\to$ along with
the relation symbols $\equiv$ and $\sqsubseteq$.
It introduced proof tactics and also <em>tacticals</em>:
operators for combining tactics.
Tactics supported goal-directed proof,
but Edinburgh LCF had no notion of the current goal or anything to help the user manage the tree of subgoals.
Its user interface was simply the ML top level and the various theorem-proving primitives were simply ML functions.
ML stood for <em>metalanguage</em>, since managing the process of proof was its exact job.</p>

<p>Avra Cohn and Robin Milner wrote a <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-20.html">report</a> 
on proving the correctness of a parsing algorithm 
using Edinburgh LCF. 
The proof consists of one single induction followed by 
a little simplification and other reasoning.
The report includes a succinct description of Edinburgh LCF and
is a nice snapshot of the state of the art in 1982
when Cambridge in 1982 to join a project run by Robin Milner and Mike Gordon.
Full of youthful enthusiasm, I told Mike that it would be great 
if one day we could formalise the Prime Number Theorem.
I hardly knew what the theorem was about or how to prove it, 
but my college roommate had told me it was really deep.</p>

<p>Disappointed to discover that we only had $\forall$, $\land$ and $\to$,
I set out to fix that, to support full first-order logic. 
I ended up changing so much 
(backwards compatibility is overrated) that people eventually shamed me into writing my own <a href="https://www.cambridge.org/gb/universitypress/subjects/computer-science/programming-languages-and-applied-logic/logic-and-computation-interactive-proof-cambridge-lcf">user manual</a>.
Cambridge LCF never caught on because, well, 
nobody liked the LCF formalism.
But I used it for a development that seemed big at the time: to <a href="https://doi.org/10.1016/0167-6423(85)90009-7">verify the unification algorithm</a>.
This development was later <a href="https://isabelle.in.tum.de/dist/library/HOL/HOL-ex/Unification.html">ported to Isabelle</a>.
It contains 36 inductions, so we were making progress.
And this takes us to 1985, exactly 40 years ago;
see also <a href="https://doi.org/10.48456/tr-54">this survey</a> of the state of play.
But there was almost no mathematics: no negative numbers and no decimal notation, so you could not even write 2+2=4.
As far as the broader computer science community was concerned, we were a joke.</p>

<h3 id="19851995-cambridge-lcf-and-hol">1985–1995: Cambridge LCF and HOL</h3>

<p>Cambridge LCF was in itself a dead end, but because it included a much faster ML compiler,
it ended up <a href="https://lawrencecpaulson.github.io/2022/09/28/Cambridge_LCF.html">being incorporated</a> into a lot of other proof assistants, notably Mike’s <a href="https://github.com/theoremprover-museum/HOL88">HOL88</a>. 
And just like that, <a href="https://lawrencecpaulson.github.io/2023/01/04/Hardware_Verification.html">hardware verification</a> became a reality. 
Although software verification seemed stuck in the doldrums,
a couple of production-ready chip designs were verified!
Mike’s explanation was that hardware verification was simply easier.</p>

<p>Also in 1985, we got a new <a href="https://doi.org/10.1145/3386336">standard for the ML language</a>
and, soon, two compilers for it.
So then I started working on experiments that would 
<a href="https://lawrencecpaulson.github.io/2022/07/13/Isabelle_influences.html">lead to Isabelle</a>.
It would be like LCF but would support constructive type theory, 
crucially allowing both unification and backtracking, like in Prolog.
But there was no working system yet, just a grant application. 
And that was the state of play 40 years ago.</p>

<p>Funding secured, Isabelle development started in earnest in 1986.
It was coded in <a href="https://www.lfcs.inf.ed.ac.uk/software/ML/">Standard ML</a> from the start, while HOL88 was ported from the Cambridge LCF version of ML 
to Standard ML, emerging as HOL90.
Mike acquired a bevy of energetic PhD students, 
who engaged in verification projects or built extensions for HOL.
Versions of HOL were being used in institutes around the world.</p>

<p>Stepping aside from HOL for a moment, other proof assistants had made great progress 
by the mid 1990s.
The addition of inductive definitions to the calculus of constructions
gave us the <a href="https://rdcu.be/eR7e8">calculus of inductive constructions</a>,
which in essence is the formalism used today by Rocq and Lean.
The very first release of Isabelle/HOL <a href="https://rdcu.be/eR7gl">happened in 1991</a>, 
primarily the work of Tobias Nipkow, though I was soon to
<a href="https://www.cl.cam.ac.uk/~lp15/Grants/holisa.html">join in</a>.
Isabelle/ZF, which was my pet project, formalised axiomatic set theory 
to some <a href="https://arxiv.org/abs/cs/9612104">quite deep results</a>.</p>

<p>But I am still not certain whether negative numbers were supported (can somebody help me?).
Our weak support for arithmetic may seem odd 
when our research community was aware that the real numbers 
had been <a href="https://lawrencecpaulson.github.io/2022/06/22/Why-formalise.html">formalised in AUTOMATH</a>, 
but we didn’t seem to want them. 
To many, we were still a joke. This was about to change.</p>

<h3 id="19952005-proof-assistants-come-of-age">1995–2005: Proof assistants come of age</h3>

<p>In 1994, came the Pentium with its <a href="https://www.techradar.com/news/computing-components/processors/pentium-fdiv-the-processor-bug-that-shook-the-world-1270773">FDIV bug</a>: 
a probably insignificant but detectable error in floating-point division.
The subsequent product recall cost Intel nearly half a billion dollars.
John Harrison, a student of Mike’s, decided to devote his PhD research
to the verification of floating-point arithmetic.
By June 1996 he had submitted an extraordinary <a href="https://doi.org/10.48456/tr-408">thesis</a>, 
<em>Theorem Proving with the Real Numbers</em>,
which described a formidable series of achievements:</p>

<ul>
  <li>a formalisation of the real member system in HOL</li>
  <li>formalised analysis including metric spaces, sequences and series, limits, continuity and differentiation, power series and transcendental functions, integration</li>
  <li>proper numerals represented internally by symbolic binary, and calculations on them</li>
  <li>computer algebra techniques including a decision procedure for real algebra</li>
  <li>tools and techniques for floating-point verification by reference to the IEEE standard</li>
</ul>

<p>This thesis, which I had the privilege to examine, won a Distinguished Dissertation Award
and was <a href="https://link.springer.com/book/10.1007/978-1-4471-1591-5">published as a book</a> by Springer.
So by the middle of the 1990s, which was 30 years ago, 
we had gone from almost no arithmetic to a decent chunk of formalised real analysis
that was good enough to verify actual floating-point algorithms.</p>

<p>This period also saw something of an arms race in automation.
My earlier, Prolog-inspired vision of backtracking search
had led to some <a href="https://doi.org/10.48456/tr-396">fairly general automation</a> that was effective not just in standard predicate logic 
but with any theorems were expressed in a form suitable for forward or backward chaining.
I had also done experiments with classical automatic techniques such as model elimination, which, although pathetic compared with automatic provers of that era, 
was good enough to troll users on the <code>hol-info</code> mailing list.
Soon I had provoked John Harrison to build a superior version of ME for HOL Light.
Later, Joe Hurd built his <code>metis</code> superposition prover, which found its way into HOL4.
Not to be outdone, Tobias made Isabelle’s simplifier the best in its class incorporating a number of sophisticated refinements, including some great ideas from Nqthm.</p>

<p>Twenty years from the start of this chronology we now had 
several reasonably mature and powerful systems, including Isabelle/ZF, Isabelle/HOL, 
multiple versions of the HOL system, and Coq (now Rocq).<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>
Many of them used <a href="https://proofgeneral.github.io">Proof General</a>, 
a common user interface for tactic-based proof assistants
based on the Emacs editor.
And we had 100MHz machines, some with 64MB of memory!
We were ready to do big things.</p>

<p>During this period, I did a lot of work on the 
<a href="https://doi.org/10.3233/JCS-1998-61-205">verification of cryptographic protocols</a>, 
also <a href="https://doi.org/10.48550/arXiv.2105.06319">here</a>.
These secure Internet connections and other network communications;
they are valuable when you need to know who is on the other end 
and need to keep messaging secure from eavesdropping and tampering.
Among the protocols investigated were the ubiquitous TLS
and the late, unlamented SET protocol.
These proofs were not at the level of code or bits;
buggy implementations could and did emerge.</p>

<p>In 2005, the big thing that caught everyone’s eye
was <a href="https://rdcu.be/eSgTy">George Gonthier’s formalisation</a> (in Coq) 
of the Four Colour Theorem.
Most educated people had heard of the theorem already, 
and its history is fascinating:
numerous proofs had been attempted and rejected since the mid 19th century.
The 1977 proof by Appel and Haken was questioned 
because it relied on a lot of ad-hoc computer code.
Suddenly, despite the still unwelcome involvement of computers, 
no one could doubt the theorem anymore.</p>

<p>At the opposite extreme was <a href="https://doi.org/10.1112/S1461157000000449">my own formalisation</a> of Gödel’s proof of the relative consistency of the axiom of choice in Isabelle/ZF.
This was the apex of my ZF work, technically difficult but incomprehensible to most people.
My early dream of having a formalisation of the Prime Number Theorem came true in 2005
when Jeremy Avigad <a href="https://arxiv.org/abs/cs/0509025">formalised</a> the theorem in Isabelle.
Somewhat later, John Harrison <a href="https://rdcu.be/eShga">formalised a different proof</a> in HOL Light.
And there was much more. Without any doubt, our systems were capable of serious mathematics.</p>

<p>Perhaps the most consequential achievement of this period was Mike Gordon’s collaboration 
with Graham Birtwistle and Anthony Fox to <a href="https://rdcu.be/eShzn">verify the ARM6 processor</a>.
Graham, at Leeds, formally specified the instruction set architecture of the processor 
(i.e. the assembly language level), while Mike and Anthony at Cambridge verified the implementation of that architecture in terms of lower level hardware components.
Eventually a <a href="https://doi.org/10.1145/3290384">number of other processors</a> were similarly specified, 
and some verified.
Without any doubt, our systems were capable of serious verification.</p>

<p>Despite of the focus on applications in this section, 
system development continued in the run-up to 2005.
I am only familiar with Isabelle development, but they were tremendous:</p>

<ul>
  <li>the <em>Isar language</em> for structured, legible proofs (a break with the LCF idea that the top level must be a programming language, i.e. ML)</li>
  <li><em>axiomatic type classes</em>, providing principled overloading</li>
  <li><em>counterexample finders</em>: <a href="https://doi.org/10.1109/SEFM.2004.1347524">Quickcheck</a> and Refute (now Nitpick)</li>
  <li><em>code generation</em> from the executable fragment of higher-order logic, and reflection</li>
  <li><em>sledgehammer</em> was under active development, but only ready a couple of years later.</li>
</ul>

<p>With so much going on, it’s not surprising that our community started doing big things, 
and other people were starting to notice.</p>

<h3 id="20052015-the-first-landmarks">2005–2015: The first landmarks</h3>

<p>I am not used to phone calls from journalists:
for most of my career, formal verification has been seen as (at best) niche.
But the journalist on the end of the line was asking for information about
<a href="https://doi.org/10.1145/1629575.1629596">seL4</a>, 
the first operating system kernel ever to be formally verified.
Tools for extended static checking were by then able to detect a lot of program faults, but the seL4 verification claimed to cover <em>full functional correctness</em>: 
the code did exactly what it was supposed to do.
There is now an <a href="https://sel4.systems">entire ecosystem</a> around seL4, 
backed by a million lines of Isabelle/HOL proofs.</p>

<p>People have wanted to verify compilers 
<a href="https://doi.org/10.1007/3-540-10886-6">since forever</a>.
The task of fully specifying a programming language, target machine 
and compiler already seemed impossible, let alone providing the actual proof.
With <a href="https://inria.hal.science/hal-01238879v1">CompCert</a>, that task was finally fulfilled, for a large subset of the C language:</p>

<blockquote>
  <p>What sets CompCert apart from any other production
compiler, is that it is formally verified, using machine-
assisted mathematical proofs, to be exempt from mis-
compilation issues. In other words, the executable code
it produces is proved to behave exactly as specified by
the semantics of the source C program.</p>
</blockquote>

<p>A seemingly intractable problem with compiler verification 
was how to translate your verified compiler into machine code.
For example, CompCert is mostly written in Rocq, 
which is then extracted to OCaml code. 
The OCaml compiler had never been verified, 
so how do we know that its compiled code is correct?</p>

<p><a href="https://cakeml.org">CakeML</a> squares this circle through <a href="https://doi.org/10.1145/3437992.3439915">bootstrapping</a>.
CakeML translates from its source language (a dialect of ML) 
to assembly language, accompanied by a proof that the two pieces of code are equivalent.
This work was an outgrowth of the ARM6 project mentioned earlier.
<a href="https://www.cse.chalmers.se/~myreen/">Magnus Myreen</a> 
had <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-765.html">developed techniques</a> for
automatically and verifiably translating between assembly language 
and recursive functions in higher-order logic, in both directions.
At the start of the bootstrapping process, 
a tiny compiler was written in pure logic and proved correct.
It was now safe to run this compiler 
and use its tiny language to implement a bigger language.
This process ultimately produced a verified compiler in both source form 
and assembly language form, with a proof of their equivalence, 
as well as <a href="https://doi.org/10.1145/2364527.2364545">verified extraction</a> from higher-order logic to ML.</p>

<p>The end of the decade also saw impressive results in the formalisation of mathematics:</p>

<ul>
  <li><a href="https://rdcu.be/eSZwv">Gödel second incompleteness theorem</a>, by yours truly, in Isabelle/HOL</li>
  <li>the <a href="https://arxiv.org/abs/1405.7012">Central Limit Theorem</a>, by Avigad et al., ditto</li>
  <li>the <a href="https://github.com/flyspeck/flyspeck">Flyspeck</a> project, by Hales et al., in Isabelle/HOL and HOL Light</li>
  <li>the <a href="https://doi.org/10.1145/2480359.2429071">odd order theorem</a>, in Rocq</li>
</ul>

<p>Without going into details here, each of these was an ambitious proof, combining in various ways deep mathematics, intricate technicalities and sheer bulk.
Our community was proud of our achievements.
We were no longer a joke, but what exactly we were good for?</p>

<h3 id="20152025-breaking-through">2015–2025: Breaking through</h3>

<p>This period brought something astonishing: 
acceptance of proof assistants by many mainstream mathematicians.
I mostly recall mathematicians regardeding computers 
with something close to contempt. 
Even some logicians regarded formalised mathematics as impossible, 
somehow fixating on Gödel’s incompleteness or that notorious proof of 1+1=2 on page 360.
Regarding my work formalising big chunks of ZF theory, 
someone commented “only for finite sets obviously”.</p>

<p>My EU-funded <a href="https://www.cl.cam.ac.uk/~lp15/Grants/Alexandria/">ALEXANDRIA</a> project started in 2017. 
My team formalised more advanced and deep mathematics 
than I ever imagined to be possible, using Isabelle/HOL.
(I have told this story in an <a href="https://lawrencecpaulson.github.io/2023/08/31/ALEXANDRIA_finished.html">earlier blogpost</a>.)
But ALEXANDRIA alone would not have had much of an impact on mathematical practice.
What made a difference was <a href="https://xenaproject.wordpress.com/what-is-the-xena-project/">Kevin Buzzard</a> and his enthusiastic, tireless promotion of the idea of formalising mathematics 
in <a href="https://lean-lang.org">Lean</a>.
He recruited a veritable army.
I got the idea of blogging from him, but my blog has not had the same impact. Where are you guys?</p>

<p>In 2022, for the first time ever, machine assistance 
was <a href="https://leanprover-community.github.io/blog/posts/lte-final/">used to confirm</a> 
brand-new mathematics that a Fields Medallist had concerns about.
Mathematicians will for the most part continue to work the way they always have done, 
but proof assistants are getting better and better, 
and they will encroach more and more on the everyday practice of mathematics.</p>

<p>Meanwhile, Isabelle continued to be useful for verification.
I was amazed to hear that that the systems group here in the Computer Lab 
had completed a <a href="https://doi.org/10.1145/3133933">major verification</a> using Isabelle/HOL.
The tradition is for systems people to despise verification tools 
for sweeping aside ugly things like overflow and floating point errors, even though they no longer do.
Besides, a research tool like Isabelle is only used by its own developer and his students.
Times were changing.</p>

<p>Isabelle is also one of the several proof assistants involved 
with <a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/">CHERI</a>, a large-scale project
reviving the old idea of <em>capabilities</em> to ensure security at the hardware level.
CHERI has produced numerous publications, some of which 
(for example <a href="https://doi.org/10.1007/978-3-030-99336-8_7">this one</a> 
and <a href="https://doi.org/10.1109/SP40000.2020.00055">that one</a>) describe very large proofs.
These concern the design and implementation of novel computer architectures 
with fine-grained memory protection, 
and a design process with formal verification at its heart.</p>

<p>Isabelle has also contributed to the design of <a href="https://webassembly.org">WebAssembly</a>, 
a relatively new platform for web applications.
By subjecting the WebAssembly specification to <a href="https://doi.org/10.1145/3167082">formal scrutiny</a>, 
Conrad Watt was able to identify a number of issues in time for them to be fixed.</p>

<p>Finally, I’d like to mention this announcement (4 December 2025) by Dominic Mulligan of Amazon Web Services (AWS):</p>

<blockquote>
  <p>Over three years, lots of hard work, and 260,000 lines of Isabelle/HOL code later, the Nitro Isolation Engine (NIE) <a href="https://www.aboutamazon.com/news/aws/aws-graviton-5-cpu-amazon-ec2">is finally announced</a> alongside Graviton5.</p>

  <p>Working with our colleagues in EC2, Annapurna, and AWS AppSec, we have been working to rearchitect the Nitro system for Graviton5+ instances around a small, trusted separation kernel. Written from scratch in Rust, we have additionally specified the behaviour of a core subset of the Nitro Isolation Engine kernel, verified that the implementation meets this specification, and additionally proved deep security properties—confidentiality and integrity—of the implementation.</p>
</blockquote>

<p>I am biased, since I’ve been working with AWS on this exact project, but this is a big deal.
AWS has been using formal verification tools for a considerable time.
A notable earlier accomplishment was verify tricky but efficient algorithms using HOL Light,
<a href="https://www.amazon.science/blog/formal-verification-makes-rsa-faster-and-faster-to-deploy">speeding up</a>
RSA encryption by a massive factor.</p>

<h3 id="20252035-becoming-ordinary">2025–2035 Becoming ordinary</h3>

<p>A couple of months ago, Apple announced new models in their iPhone range,
but no crowds formed around Apple Stores.
They once did: the iPhone was once regarded as revolutionary.
Now, smartphones are a commodity, which is the final stage of a new technology.
Formal verification is not ordinary yet. 
But it’s coming: more and more software will be seen as too important to develop any other way, 
as is already the case for hardware.</p>

<h3 id="postscript">Postscript</h3>

<p>I am well aware that there is much outstanding work adjacent to that
described here, e.g. using other interactive tools, such as Nqthm and ACL2,
PVS and Agda, and much else using Rocq. There have been amazing advances 
in the broader theorem proving world, also in model checking,
SAT/SMT solving and their applications to extended static checking of software.
I have related what I personally know.
And remember, the point of this post is not (simply) to boast 
but to demonstrate the progress of our research community, 
so the more achievements the better. Feel free to add some in the comments!</p>

<p>This post does not prove anything about other fields of science, 
such as solid-state physics, molecular biology or mathematics.
But it’s fair to assume that such fields have not been idle either. 
People have proved Fermat’s Last Theorem and the Poincaré conjecture, 
and settled more obscure questions such as the projective plane of order 10.
People have located the remains of King Richard III, who died in 1485, 
excavating and positively identifying the body by its DNA.
People have linked a piece of bloody cloth to Adolf Hitler and diagnosed  that he had a specific genetic condition.
The immensely complex James Webb Space Telescope
was successfully deployed;
it is now revealing secrets about the early Universe.</p>

<p>Sometimes I wonder about the motives of those who claim that science is moribund. 
Do they have political aims, or just unrealistic expectations?
Were they expecting time travel or some sort of warp drive?
People need to remember that movies are fiction.</p>







      


    </section>

  

</div>
  </body>
</html>
