<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://anyahope.me/blog/comments-doctorow-ai-will-fail/">Original</a>
    <h1>Comments on Cory Doctorow&#39;s &#34;AI companies will fail&#34;</h1>
    
    <div id="readability-page-1" class="page"><article>
      

      <span> Anya Hope <span></span> 14 min read <span></span> February 01, 2026 <span></span>updated: February 01, 2026 <span></span> #<a href="https://anyahope.me/tags/ai/">ai</a>  #<a href="https://anyahope.me/tags/society/">society</a>  #<a href="https://anyahope.me/tags/media/">media</a> </span>

    


<p>A friend shared this long-ish piece by <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Cory_Doctorow">Cory Doctorow,</a> which
was published on January 18, 2026 by The Guardian,
with the title
<a rel="noopener external" target="_blank" href="https://www.theguardian.com/us-news/ng-interactive/2026/jan/18/tech-ai-bubble-burst-reverse-centaur"><em>&#34;AI companies will fail. We can salvage something from the wreckage.&#34;</em></a></p>
<p>As I was reading it, I started to write down some comments I wanted to share with friends. I will borrow Bob Ross&#39;s attitude and decide that if you&#39;re here, you&#39;re a friend.</p>
<hr/>
<p>I recognise that Doctorow is saying things that I really want to be hearing, and that I want to be true. It feels validating to read this kind of piece.</p>
<p>But he&#39;s also making an argument, and argumentative pieces often leave out complexities and nuance. The older I get, the more I struggle with that aspect of opinion pieces.</p>
<p>I wanted to stay clear-eyed about where I like his arguments because I believe them, where I like his arguments because I really want
to believe them (but can&#39;t quite accept them in the way Doctorow makes them), where I see what he&#39;s trying to argue,
but don&#39;t find it effective, and where I disagree completely.</p>
<p>I don&#39;t promise to address every concern you might have while reading that piece, so I encourage you to read it yourself.</p>
<h2 id="intro"><a href="#intro" aria-label="Anchor link for: intro">§</a>Intro</h2>
<p>The beginning of Doctorow&#39;s piece felt very good to read. He opens with his background as a science fiction writer, which is fun, and then goes into this:</p>
<blockquote>
<p>Then there are science-fiction fans who believe that they are <em>reading</em> the future. A depressing number of those people appear to have become AI bros.
These guys can’t shut up about the day that their spicy autocomplete machine
will wake up and turn us all into <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer">paperclips</a></p>
</blockquote>
<p>Yes! Those people are annoying, and seem to be high on glue. I don&#39;t like them, either. I appreciate him not mincing words about this.</p>
<h2 id="an-army-of-reverse-centaurs"><a href="#an-army-of-reverse-centaurs" aria-label="Anchor link for: an-army-of-reverse-centaurs">§</a>&#34;An army of reverse centaurs&#34;</h2>
<p>This, too, felt good to read:</p>
<blockquote>
<p>Tech bosses want us to believe that there is only one way a technology can be used.
<a rel="noopener external" target="_blank" href="https://www.theguardian.com/technology/2024/sep/19/social-media-companies-surveillance-ftc">Mark Zuckerberg</a> wants you to think that it is technologically impossible
to have a conversation with a friend without him listening in.
Tim Cook wants you to think that it is impossible for you to have a reliable computing experience unless he gets a veto over which software
you install and without him taking <a rel="noopener external" target="_blank" href="https://www.theguardian.com/technology/2025/apr/30/apple-fortnite-court-order-violation">30 cents</a>
out of every dollar you spend. Sundar Pichai wants you to think that it is impossible for you to find a webpage unless he gets to spy on you from asshole to appetite.</p>
</blockquote>
<p>The mention of &#34;asshole&#34; was distracting, and it took me a second to figure out the imagery, but yes, I agree with this.
It&#39;s also easy to agree with this. Bashing the billionaire executives in charge of tech monopolies is a crowd-pleaser, and Doctorow does it effectively.
That makes sense - he has a lot of experience bashing tech monopolists.</p>
<p>This is where his argument starts taking shape as it relates to AI:</p>
<blockquote>
<p>This is all a kind of vulgar Thatcherism. Margaret Thatcher’s mantra was: “There is no alternative.” She repeated this so often they called her “Tina” Thatcher: There. Is. No. Alternative.</p>
<p>“There is no alternative” is a cheap rhetorical sleight. It’s a demand dressed up as an observation. “There is no alternative” means: “Stop trying to think of an alternative.”</p>
</blockquote>
<p>Yep. I only recently learned about <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/There_is_no_alternative"><em>&#34;There is no alternative&#34;</em></a>.
But even before that, I was feeling really frustrated with the extent to which the pro-AI businesspeople appeal to existential anxiety
(<em>start using AI, or you will lose your job to someone who is [because we will fire you]</em>) and a sense of fait accompli
(<em>like it or not, AI is here to stay [in exactly the way we are pushing it]</em>). The parts in square brackets are
the ones that they try not to say out loud, except sometimes they do.</p>
<p>For as long as I&#39;ve been a conscious person, tech people have always appealed to anxiety (<em>&#34;don&#39;t get left behind!&#34;</em> might as well be
the corporate slogan for the digital transformation of our society),
and projected a smug determinism about technological progress (<em>&#34;there&#39;s no going back&#34;</em> could be their slogan, too).
But I also don&#39;t remember tech companies having to engage in so much arm-twisting and hard sales tactics with earlier technologies
(like personal computers, or Internet, or smartphones, or social media, or streaming services).</p>
<p>There were commercial technologies that were pushed on people without being taken up
by the public, and there were some cringe attempts to keep forcing then onto the public despite general resistance
(guided, most likely, by some version of the fictitious <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Gartner_hype_cycle#Criticisms_of_the_model">Gartner hype cycle.</a>)
But I don&#39;t remember a time when the whole industry
decided to bleed itself dry to get you to watch 3D movies, or
buy a VR headset, like they are doing with AI. Maybe it&#39;s just my recency bias.</p>
<h2 id="ai-can-t-do-your-job"><a href="#ai-can-t-do-your-job" aria-label="Anchor link for: ai-can-t-do-your-job">§</a>&#34;AI can’t do your job&#34;</h2>
<p>I appreciated this example of the &#34;reverse centaur&#34; (which I found more compelling then the one about the Amazon drivers Doctorow gave at the beginning of the piece):</p>
<blockquote>
<p>Let’s say my hospital bought some AI radiology tools and told its radiologists: “Hey folks, here’s the deal. [...]
From now on, we’re going to get an instantaneous second opinion from the AI, and if the AI thinks you’ve missed a tumor, we want you to go back and have another look.
[...] we just care about finding all those tumors.”</p>
<p>If that’s what they said, I’d be delighted. [...] [But] the remaining radiologists’ job will be to oversee the diagnoses the AI makes at superhuman speed – and somehow remain vigilant as they do so.</p>
<p>“And if the AI misses a tumor, this will be the human <em>radiologist’s</em> fault, because they are the ‘human in the loop’. It’s their signature on the diagnosis.”</p>
<p>This is a reverse centaur, and it is a specific kind of reverse centaur: it is what <a rel="noopener external" target="_blank" href="https://profilebooks.com/work/the-unaccountability-machine/">Dan Davies</a> calls an “accountability sink”. The radiologist’s job is not really to oversee the AI’s work, it is to take the blame for the AI’s mistakes.</p>
</blockquote>
<p><em>(emphasis in the original)</em></p>
<p>Note: A regular &#34;centaur&#34;, mentioned earlier in the piece, is &#34;a person who is assisted by a machine. Driving a car makes you a centaur, and so does using autocomplete.&#34;</p>
<h2 id="how-to-pump-a-bubble"><a href="#how-to-pump-a-bubble" aria-label="Anchor link for: how-to-pump-a-bubble">§</a>&#34;How to pump a bubble&#34;</h2>
<p>I am skipping this section because I don&#39;t know enough about finance or economics to say anything useful.
I (honestly) don&#39;t know about Doctorow&#39;s expertise on this, but this section is probably worth a read for those who do.</p>
<h2 id="ai-can-t-do-your-job-1"><a href="#ai-can-t-do-your-job-1" aria-label="Anchor link for: ai-can-t-do-your-job-1">§</a>&#34;AI can’t do your job&#34;</h2>
<p>Here, I find it disappointing that Doctorow is making some simplifications at the expense of rigour. Systems based on LLMs are not really</p>
<blockquote>
<p>just a word-guessing program, because all it does is calculate the most probable word to go next [...] All it can do is predict what word will come next based on all the words that have been typed in the past.</p>
</blockquote>
<p>I find it unhelpful when opponents of AI state that LLMs are just a next-word guesser, because they risk weakening their overall argument.</p>
<p>For example, computers are &#34;just&#34; very fast arithmetic calculators, but we know they&#39;re also more than the sum of those basic parts. You would struggle to make an effective argument about the social implications of computers if you kept describing them only as big calculators.</p>
<p>Yes, predicting the most probable next token in a sequence is at the core of a language model.
But at this point there are <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">some</a>
<a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Top-p_sampling">real</a> <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation">innovations</a>
<a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Reasoning_model">on top of that</a> core algorithm. Commercially available LLMs (starting from ChatGPT) have never been just next-token predictors.</p>
<p>I don&#39;t know how much this weakens Doctorow&#39;s overall argument in practice,
because this topic has become extremely polarised, and pro-AI people in our community might not accept any argument at all.
Still, the strongest arguments against the current reckless deployment of AI tools would account for the real complexity of these systems,
and attack them with that complexity in mind.</p>
<p>Even with sophisticated sampling techniques, RLHF, integrated knowledge bases, reasoning architectures, agentic chains, and whatever else, LLM-based tools are still  drowning us in low-quality trash, and that low quality, integrated in critical tools, is leaving us open to catastrophic failures.</p>
<p>Though Doctorow&#39;s explanation here is unrigorous and flawed, I agree with where he takes it:</p>
<blockquote>
<p>That means that [an LLM] will “hallucinate” a library called lib.pdf.text.parsing, because that matches the pattern it’s already seen. And the thing is, malicious hackers <em>know</em> that the AI will make this error,
so they will go out and <em>create</em> a library with the predictable, hallucinated name, and that library will get automatically sucked into the AI’s program [...]</p>
<p>And you, the human in the loop – the reverse centaur – you have to spot this subtle, hard-to-find error, this bug that is indistinguishable from correct code.</p>
<p>For AI to be valuable, it has to replace <em>high</em>-wage workers, and those are precisely the workers who might spot some of those statistically camouflaged AI errors.</p>
</blockquote>
<p><em>(emphasis in the original)</em></p>
<p>Aside: Doctorow is not explicitly making the following point, but I&#39;ve become quite convinced that there is a conspiracy (perhaps not a formal one, but a de-facto one) to
purge our industry of experienced software engineers who are reluctant to go all-in on AI tools.
We see this with <a rel="noopener external" target="_blank" href="https://www.seattletimes.com/business/microsoft/behind-microsofts-layoffs-a-new-attitude-shaped-by-ai/">Microsoft</a>
<a rel="noopener external" target="_blank" href="https://sfstandard.com/2025/07/24/layoffs-firing-ai-engineers-tech-white-collar-jobs/">and other companies telling engineers they have to use AI,</a>
or risk being fired, without good prospects for finding new employment.</p>
<p>I don&#39;t know if flushing out all AI-resistant engineers from the field is tech executives&#39; clear intent, or if they understand themselves to be malicious,
but by <a rel="noopener external" target="_blank" href="https://en.wikipedia.org/wiki/Hanlon%27s_razor#Origin">Grey&#39;s law,</a> it doesn&#39;t matter. What matters are their actions, and their actions&#39; effects.
If all experienced software engineers who prefer not to use AI tools are driven from our industry,
we will collectively suffer significant professional damage. At some point, there may be no one
left in the room to question the continued deployment of AI tools, or AI-written code,
and I fear that&#39;s the point.</p>
<h2 id="expanding-copyright-is-not-the-answer"><a href="#expanding-copyright-is-not-the-answer" aria-label="Anchor link for: expanding-copyright-is-not-the-answer">§</a>&#34;Expanding copyright is not the answer&#34;</h2>
<p>I am mostly skipping the section on AI art because I don&#39;t have much useful to say, but it&#39;s worth a read.</p>
<hr/>
<p>I was, of course, nodding along to this:</p>
<blockquote>
<p>We should not simply shrug our shoulders and accept Thatcherism’s fatalism: “There is no alternative.”</p>
<p>So what is the alternative? A lot of artists and their allies think they have an answer: they say we should extend copyright to cover the activities associated with training a model.</p>
<p>And I am here to tell you they are wrong. [...]</p>
<p>Creative workers who cheer on lawsuits by the big studios and labels need to remember the first rule of class warfare: things that are good for your boss are rarely what’s good for you.</p>
</blockquote>
<p>If you know me, you know I&#39;ll agree with everything he says here.
More copyright is not the answer. Using the tools of pro-corporate copyright law will not, in the long term, help us get out of a situation that was created by corporations in the first place.</p>
<blockquote>
<p>But creative workers do not have to rely on the US government to rescue us from AI predators.
We can do it ourselves, the way the writers did in their historic writers’ strike. [...]</p>
<p>They did it because they are organized and solidaristic, but also are allowed to do something that virtually no other workers are allowed to do:
they can engage in “sectoral bargaining”, whereby all the workers in a sector can negotiate a contract with every employer in the sector.</p>
<p>If we are gonna campaign to get a new law passed in hopes of making more money and having more control over our labor,
we should campaign to restore sectoral bargaining.</p>
</blockquote>
<p>(The reference to the US government at the beginning of the quote is from Doctorow describing how the US Copyright Office has been legally enforcing its decision to classify AI works as un-copyrightable, and belonging in the public domain.)</p>
<p>If you know me, you know I&#39;ll also agree with not wanting to rely on the government
to do something it might never do. In this context, I do find it ironic that Doctorow
suggests argues not rely on the government, and then suggests we campaign for new laws (even if he does so half-heartedly).</p>
<p>I have become very nihilistic, and don&#39;t believe that under the current system,
the US legislature can pass any law that is so decidedly pro-human and pro-worker
as to legalise sectoral bargaining.
But on the spirit of what he&#39;s saying, hell yes.
To resist, we need to work together, and be in solidarity with each other.</p>
<h2 id="how-to-burst-the-bubble"><a href="#how-to-burst-the-bubble" aria-label="Anchor link for: how-to-burst-the-bubble">§</a>&#34;How to burst the bubble&#34;</h2>
<p>I like the dose of techno-optimism Doctorow gives towards the end, even if I struggle
to genuinely feel it:</p>
<blockquote>
<p>If there had never been an AI bubble, if all this stuff arose merely because computer scientists and product managers noodled around for a few years coming up with cool new apps, most people would have been pleasantly surprised with these interesting new things their computers could do. We would call them “plugins”.</p>
<p>It’s the bubble that sucks, not these applications.</p>
</blockquote>
<p>I 100% agree with this. If LLMs stayed a fun tech demo, and
AI tools were built upon gradually,
with real collaboration between industry, academia and the public,
and without immediately being forced onto everyone in the world,
I would have likely still been very into them (as I was in November 2022, when OpenAI released the first janky version of ChatGPT).
Had that been the reality, I think most hackers, and tech-curious people in general, would have continued
experimenting with LLMs without feeling like we&#39;re taking sides in a civil war.</p>
<p>But I don&#39;t really share his optimism about the good things
that might come if the AI bubble bursts.
Yes, there will be many engineers with AI backgrounds who might bring their expertise to other areas,
or a flood of cheap GPUs to use for other things.</p>
<p>But I fear that a non-bubble market just wouldn&#39;t need so many
ML engineers with our specialised skills,
and I think many of us will be left without a clear path.
And, while I would love a glut of cheap GPUs, I don&#39;t think used datacentre GPUs that will have been deployed 24/7 for AI work would be all that useful, or even usable, for post-AI bubble applications.
I hope I am wrong.</p>
<p>And, either way, keeping the AI mania going only risks making the eventual crash
more painful.</p>
<blockquote>
<p>The collapse of the AI bubble is going to be ugly. Seven AI companies currently account for more than a third of the stock market, and they endlessly pass around the same $100bn IOU.</p>
<p>AI is the asbestos in the walls of our technological society, stuffed there with wild abandon by a finance sector and tech monopolists run amok. We will be excavating it for a generation or more.</p>
</blockquote>
<p>This, I fully agree with.</p>
<p>The ending is kind of a pep talk, but that&#39;s how these kinds of pieces go.</p>
<p>Thanks to Cory Doctorow for writing this piece, and my friend for sharing it.</p>



    <p><small><i>If you have feedback, I would love to hear from you! Please use one of the links below to get in touch.</i></small>
    </p>
      <nav>
        
        
      </nav>
    </article></div>
  </body>
</html>
