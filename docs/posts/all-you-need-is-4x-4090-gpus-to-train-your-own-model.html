<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sabareesh.com/posts/llm-rig/">Original</a>
    <h1>All You Need Is 4x 4090 GPUs to Train Your Own Model</h1>
    
    <div id="readability-page-1" class="page"><div>

<article>
   
  <div>
<p>My journey into Large Language Models (LLMs) began with the excitement of seeing ChatGPT in action. I started by exploring diffusion models, drawn to their ability to create beautiful visuals. However, working on an M1 chip had its limitations, which motivated me to build a custom rig with an NVIDIA 4090 GPU. As I continued to explore LLMs and experimented with multi-agent systems, I came to realize the importance of mastering the fundamentals. This realization led me to focus on training LLMs from scratch—not just to use them but to deeply understand how they function and evolve.</p>
<h2 id="rig-evolution">Rig Evolution</h2>
<p><img alt="Rig with 2x NVIDIA 4090 GPUs" loading="lazy" src="https://sabareesh.com/blog-images/ws-1.jpeg"/></p>
<blockquote>
<p><strong>Note:</strong> This setup is capable of training models with up to 1 billion parameters; however, it performs better with ~500 million parameter models to achieve higher model utilization (MFU).</p>
</blockquote>
<ul>
<li><strong>Initial Build</strong>: Rig with 2x NVIDIA 4090 GPUs.</li>
<li><strong>Upgraded Build</strong>: Rig with 4x NVIDIA 4090 GPUs.</li>
</ul>
<p>Here’s a comprehensive guide to building a custom rig tailored for LLM training.</p>
<p><strong>Total Cost:</strong> This entire setup cost approximately <strong>$12,000 USD</strong>. This may not be cost effective but I wanted to get hands on and do more experimentation.</p>
<p>You can always go with cloud for a fraction of the cost . Please check out <a href="https://lambdalabs.com/">https://lambdalabs.com/</a> , <a href="https://hyperbolic.xyz/">https://hyperbolic.xyz/</a> and several others</p>
<p><a href="https://sabareesh.com/posts/llm-intro/">Next Step: My journey into LLMs</a></p>
<hr/>
<h2 id="1-planning-your-build">1. Planning Your Build</h2>
<ul>
<li>
<p><strong>Define Your Objectives</strong>: Determine the scale and type of models you want to train. Smaller models may work with limited resources, but larger architectures demand higher computational power.</p>
</li>
<li>
<p><strong>Budgeting</strong>: Set a realistic budget to balance performance with cost. Note that high-end components, especially GPUs, can be costly.</p>
</li>
</ul>
<hr/>
<h2 id="2-selecting-hardware-components">2. Selecting Hardware Components</h2>
<ul>
<li>
<p><strong>Motherboard</strong>: Choose a server or workstation board, primarily for the number of PCIe lanes and compatibility with multiple GPUs. I recommend the <strong>SuperMicro M12SWA-TF</strong>. While it’s an excellent board, its noisy chipset fan may need replacement with a beefier heatsink and a Noctua fan.</p>
</li>
<li>
<p><strong>CPU</strong>: Opt for a robust processor like the <strong>AMD Threadripper PRO 5955WX</strong>. The primary reason for choosing this CPU is its <strong>128 PCIe lanes</strong>, allowing you to connect multiple GPUs without bandwidth constraints.</p>
</li>
<li>
<p><strong>Memory (RAM)</strong>: Ensure compatibility between your RAM and motherboard. A setup with <strong>128 GB memory</strong> is recommended for large datasets and computational tasks.</p>
</li>
<li>
<p><strong>GPUs</strong>: NVIDIA 4090 GPUs are ideal for LLM training due to their advanced Ada architecture. Key benefits include:</p>
<ul>
<li><strong>24 GB VRAM</strong>: Sufficient for handling large models and datasets.</li>
<li><strong>BFloat16 Performance</strong>: Fourth-generation tensor cores deliver exceptional performance with up to 330 TFLOPS of bfloat16 precision, ensuring efficient computation for AI workloads.</li>
<li><strong>CUDA Cores</strong>: 16,384 CUDA cores ensure unparalleled parallel processing capabilities.</li>
<li><strong>Architecture Advantages</strong>: Enhanced ray tracing, Shader Execution Reordering, and DLSS 3 technology for improved efficiency. Well this may not directly provide benefit but because this is a consumer grade card these features enabled having support for more advanced features such as bfloat16 and event float8 training support also the sheer number of cuda cores.</li>
<li><strong>Flash Attention</strong>: Previous generations such as 3090 dont support latest Flash attentions</li>
</ul>
<p>A setup with <strong>4x NVIDIA 4090s</strong>, connected using riser cables like <a href="https://www.amazon.com/dp/B0CNNJHK93">this one</a>, offers top-notch performance for training LLMs. Several people discourage using just the cable due to potential PCIe errors, but in my experience, it worked flawlessly without any issues.</p>
</li>
<li>
<p><strong>Storage</strong>: Invest in high-capacity storage solutions. My setup includes <strong>6 TB of NVMe SSDs</strong> for blazing-fast access and <strong>8 TB of HDD storage</strong> for archiving.</p>
</li>
<li>
<p><strong>Power Supply</strong>: Dual PSU setups are often necessary for high-power builds. I used <strong>2x 1500 Watt Be Quiet PSUs</strong> (<a href="https://www.amazon.com/dp/B08F5DKK24">Amazon link</a>). Each PSU powers two GPUs, with one also powering the motherboard and CPU. Each GPU was consuming around 450W of power using DDP 500M model for ~10 days</p>
</li>
<li>
<p><strong>Case/Frame</strong>: For mounting, I recommend <a href="https://www.amazon.com/dp/B08XJGG2YX">this case</a>, which accommodates multiple GPUs and robust cooling.</p>
</li>
<li>
<p><strong>Cooling System</strong>: Replace noisy chipset fans with heatsinks like <a href="https://www.amazon.com/dp/B074DXFB66">this one</a> for quieter and more efficient cooling.</p>
</li>
<li>
<p><strong>Motherboard Baseboard</strong>: Use a baseboard like <a href="https://www.amazon.com/dp/B09WHVF3SN">this one</a> for proper fitting in the case.</p>
</li>
</ul>
<hr/>
<h2 id="3-assembling-the-rig">3. Assembling the Rig</h2>
<ul>
<li>
<p><strong>Dual PSU Setup</strong>: When using two power supplies, ensure one powers the motherboard and CPU, while each PSU powers two GPUs. Specialized adapters can help synchronize their power-on sequence. This needs 30 AMP circuit, you  might able to connect it to 2 different breakers but this is not recommended.</p>
</li>
<li>
<p><strong>Compatibility Check</strong>: Ensure all components are compatible to avoid assembly issues.</p>
</li>
<li>
<p><strong>Physical Assembly</strong>: Carefully install components, paying special attention to GPU placement and spacing for optimal airflow.</p>
</li>
<li>
<p><strong>Cable Management</strong>: Organize cables neatly to improve airflow and simplify maintenance.</p>
</li>
</ul>
<hr/>
<h2 id="4-software-configuration">4. Software Configuration</h2>
<ul>
<li>
<p><strong>Operating System</strong>: Use a Linux-based OS (e.g., Ubuntu), known for its stability and suitability for machine learning tasks.</p>
</li>
<li>
<p><strong>Drivers and Dependencies</strong>: Install the latest GPU drivers, CUDA, and cuDNN libraries to maximize GPU performance.</p>
</li>
<li>
<p><strong>Machine Learning Frameworks</strong>: Set up frameworks like PyTorch or TensorFlow, essential for model training.</p>
</li>
<li>
<p><strong>Custom Kernel</strong>: I used a <a href="https://github.com/tinygrad/open-gpu-kernel-modules">custom kernel from Tinygrad</a> to enable P2P communication between GPUs, further enhancing performance.</p>
</li>
</ul>
<hr/>
<h2 id="5-training-large-language-models">5. Training Large Language Models</h2>
<ul>
<li>
<p><strong>Data Preparation</strong>: Curate, clean, and preprocess datasets to ensure high-quality inputs for training.</p>
</li>
<li>
<p><strong>Model Selection</strong>: Choose architectures like Llama2 or GPT, tailored to your hardware and training goals.</p>
</li>
<li>
<p><strong>Training Process</strong>: Initiate training, monitor resource utilization, and adjust configurations as needed for optimal results.</p>
</li>
</ul>
<hr/>
<h2 id="6-optimization-and-scaling">6. Optimization and Scaling</h2>
<ul>
<li>
<p><strong>Multi-GPU Training</strong>: Use distributed training techniques such as Distributed Data Parallel (DDP) or ZeRO to fully utilize multiple GPUs.</p>
</li>
<li>
<p><strong>George’s Hack</strong>: Leverage the <a href="https://github.com/geohot/tinygrad">kernel patch by George Hotz</a> to enable peer-to-peer (P2P) communication for NVIDIA 4xxx GPUs, overcoming the lack of official support.</p>
</li>
<li>
<p><strong>Performance Tuning</strong>: Optimize hyperparameters, batch sizes, and learning rates to achieve better convergence and efficiency.</p>
</li>
</ul>
<hr/>
<h2 id="7-maintenance-and-monitoring">7. Maintenance and Monitoring</h2>
<ul>
<li>
<p><strong>Regular Updates</strong>: Keep your system and software updated to leverage the latest optimizations and security patches.</p>
</li>
<li>
<p><strong>System Monitoring</strong>: Use tools like NVIDIA’s nvidia-smi or Prometheus to track system health, utilization, and temperature.</p>
</li>
</ul>
<hr/>
<h2 id="key-insights-and-tips">Key Insights and Tips</h2>
<ul>
<li>
<p><strong>Hardware Alternatives</strong>: While GPUs like the A100 or H100 provide higher VRAM, consumer GPUs such as the 4090 offer excellent performance for cost-conscious setups.</p>
</li>
<li>
<p><strong>Cloud Considerations</strong>: On-premise rigs are ideal for long-term projects and experimentation, but cloud solutions offer flexibility for short-term tasks.</p>
</li>
<li>
<p><strong>Community Resources</strong>: Explore tutorials from experts like <a href="https://github.com/karpathy/nanoGPT">Andrej Karpathy</a> and guides from Hugging Face for additional insights.</p>
</li>
</ul>
<p>Building a rig for LLM training is a challenging but rewarding endeavor that opens up opportunities to push the boundaries of AI development. With careful planning and execution, your custom setup can become a powerful tool for exploring the vast landscape of machine learning.</p>
<p><img alt="Rig with 4x NVIDIA 4090 GPUs" loading="lazy" src="https://sabareesh.com/blog-images/ws-2.jpeg"/></p>


  </div>

  
</article>
    </div></div>
  </body>
</html>
