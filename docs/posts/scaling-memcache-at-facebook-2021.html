<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.micahlerner.com/2021/05/31/scaling-memcache-at-facebook.html">Original</a>
    <h1>Scaling Memcache at Facebook (2021)</h1>
    
    <div id="readability-page-1" class="page"><section id="post-content">
<p>Discussion on <a href="https://news.ycombinator.com/item?id=36397485"> Hacker News</a></p> <p><em>These paper reviews can be <a href="https://newsletter.micahlerner.com/">delivered weekly to your inbox</a>, or you can subscribe to the <a href="https://www.micahlerner.com/feed.xml">Atom feed</a>. As always, feel free to reach out on <a href="https://twitter.com/micahlerner">Twitter</a> with feedback or suggestions!</em></p>
<p><a href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf">Scaling Memcache at Facebook</a> Nishtala, et al. NSDI 2013</p>
<p>After reading about <a href="https://www.micahlerner.com/2021/03/28/noria-dynamic.html">Noria</a>, I decided to read Facebook’s implementation of a caching system at scale. This paper was enjoyable to read for a few reasons - it not only points out the tradeoffs made in designing such a system, but also the learnings associated with operating it.</p>
<h2 id="what-are-the-papers-contributions">What are the paper’s contributions?</h2>
<p>The paper discusses how Facebook built a distributed key value <label for="dkv"></label><span>A distributed key value store often allows for gettting, setting, and deleting values from a datastore with multiple copies (although specifics of how many and when data is copied are use-case specific, as the paper talks about!). </span> store on top of <a href="https://www.memcached.org/">memcached</a><label for="perl"></label><span>In the process of writing this, I learned that memcached was originally written in Perl! </span> in order to cache a wide variety of data, including database query results and data for backend services.</p>
<p>On its own, memcached is a basic key value store, yet Facebook viewed memcached’s simplicity as a positive rather than a negative. The system’s simplicity meant that Facebook was able to easily tailor the application to its use case of serving millions of user requests every second, as well as adding more advanced functionality as needed.</p>
<figure><img src="https://www.micahlerner.com/assets/fbmc/memcache.png"/><figcaption>Memcache usage</figcaption></figure>
<p>Of this paper’s contributions, the “how?” of scaling such a system is significant - their distributed key-value store needed to be scaled from a single cluster (in one data center), to many clusters in a single region, and finally to many regions with many clusters each. The paper also includes rationales for design decisions, along with acknowledgements of potential edge cases (and often times reasoning for why an unresolved edge case does not have an impact on the running system).</p>
<h2 id="so-how-did-facebook-approach-scaling-memcache">So, how did Facebook approach scaling memcache?</h2>
<p>In order to understand how Facebook scaled memcache, it is helpful to frame the scaling in three areas: within a cluster, within a region (a region may have many clusters), and between many regions (where each region has many clusters).</p>
<figure><img src="https://www.micahlerner.com/assets/fbmc/architecture.png"/><figcaption></figcaption></figure>
<h3 id="scaling-within-a-cluster">Scaling within a cluster</h3>
<p>The primary concern for scaling memcache within a cluster was reducing <em>latency</em> and <em>load</em><label for="landl"></label><span>Latency meaning response time to user request and load meaning the computational load placed on the backing datastore </span>. Additionally, there is some discussion of increasing the reliability of the system through automatic failure recovery.</p>
<p><strong>Reducing Latency</strong></p>
<p>To reduce <em>latency</em>, Facebook engineers implemented three main features: request parallelization, the <em>mcrouter</em>, and <em>congestion control</em> measures.</p>
<p>First, they noticed that memcache requests were being performed serially, so they modified their web server code to increase request parallelization. This improvement meant that unrelated data could be fetched in parallel.<label for="DAG"></label><span>The paper does not go into great depth into how the client determines which memcache requests can be parallelized, only adding that a DAG of request dependencies is used. </span>.</p>
<p>An additional measure to reduce latency was the addition of a proxy (<em>mcrouter</em>) in between the web servers and the actual backing memcache servers in order to distribute load and route requests. This <em>mcrouter</em> exposes the same interface as the memcache server and maintains TCP connections with threads on the web server. The web server sends memcache requests that mutate state (<em>set</em>, <em>delete</em>) to the mcrouter over TCP (given the built-in reliability of TCP), but sends all other memcache requests (like <em>get</em> requests) directly to the backing memcache servers over UDP. This decision to use TCP versus UDP is based on the fact that maintaining TCP connections from all web server threads to all memcached servers (of which there are many) would incur significant cost. <label for="networking"></label><span>For a quick refresher on this, Computer Networking: A Top-Down Approach is very good. </span></p>
<p>To limit congestion on the network (more congestion = more latency), memcache clients are prohibited from issuing unbounded requests. Instead, a sliding window was added to memcache clients that prohibits more than <em>n</em> requests to be in-flight at once (where <em>n</em> is a configurable setting). If the in-flight request limit is reached by a memcache client, they are put into a request queue. Based on the data in the paper, it turned out that this idea is great for reducing contention, and didn’t impact clients that are operating normally. This insight is a great instance of using behavior in production to guide implementation!</p>
<p><strong>Reducing Load</strong></p>
<p>To reduce <em>load</em> on the backing data store, three features were added: <em>leases</em>, <em>memcache pools</em>, and <em>replication within pools</em>.</p>
<p>Leases were implemented to address two main problems, <em>stale sets</em> and <em>thundering herds</em><label for="stalesets"></label><span>A stale set is when a client sets a value with an old value, and a thundering herd is when a specific key undergoes heavy read or write volume. </span>, and are values given out to clients for a specific key. To solve stale sets, the backend server checks what is the most recent lease given out for a specific key, and will block writes from an old copy of the key. To solve thundering herds (for example, many clients trying to fetch data for the same key, but the key is not in the cache), leases are given out at a constant rate. If a client requests data for a key, but a lease for the key has already been given out, the lease request will fail and the client will need to retry. Meanwhile, the owner of the lease will cause the key to be filled from cache, and the client will succeed on retry. Crisis avoided.</p>
<p>Another optimization occurred when Facebook realized that different datasets stored in memcached have different churn rates - for example, some keys in the cache change frequently, while others remain the same for the long time. If a long-lived key is in a cache with items that change frequently, based on an LRU caching policy long-lived the key is likely to be evicted. To fix this, keys with different churn rates can be separated (and the infrastructure for the different key sets can be sized appropriately).</p>
<p>For small datasets (the dataset can fit in one or two memcache servers) that have high request rates, the data is replicated. Replicating the dataset across multiple servers means that the load can be spread out, limiting the chance of a bottleneck at any given server.</p>
<h4 id="automatic-failure-recovery">Automatic failure recovery</h4>
<p>Facebook has large computing clusters and likely has many memcached servers failing every day because computers break in weird ways. To prevent these failures from cascading, Facebook built a system called <em>Gutter</em>. <em>Gutter</em> kicks in if a memcache client doesn’t get a response for a key. In this event, the data is fetched from the database and placed on the <em>Gutter</em> server, essentially diverting that key away from the main cluster. This approach is explicitly chosen over the alternative of redistributing keys from a failed machine across the remaining healthy machines (which the paper argues is a more dangerous alternative that could overload the healthy servers).</p>
<h3 id="scaling-among-clusters-within-a-region">Scaling among clusters within a region</h3>
<p>Within a region, the paper highlights that the biggest concern is data-replication between multiple copies of the cache. To solve this problem space, Facebook implemented three features: an invalidation daemon (a.k.a McSqueal) that replicates the cache invalidations across all cache copies in region, a <em>regional pool</em> of memcache servers that all clusters in a region share for certain types of data, and a mechanism for preparing clusters before they come online.</p>
<figure><img src="https://www.micahlerner.com/assets/fbmc/mcsqueal.png"/><figcaption></figcaption></figure>
<p>The invalidation daemon used to replicate cache-invalidations among clusters reads the MySQL commit log, transforming deletes into the impacted MySQL keys that need to be deleted from the cache, and eventually batching the deletes in a message to the <em>mcrouter</em> that sits in front of the memcache servers. <label for="mcsqueal"></label><span>Personal opinion: using the MySQL commit log as a stream that daemons operate on is a great design pattern (and was likely ahead of its time when the paper came out)! </span></p>
<p>The next section of the paper talks about <em>regional pools</em>, which are a strategy to maintain single copies of data in order to limit data usage and inter-cluster traffic from replication. Normally datasets with smaller values and lower traffic are placed here, although the paper waves the hands a little bit about a manual heuristic that figures out which keys would be good candidates for regional pools.</p>
<p>The last topic related to scaling among clusters within a region is the cluster warmup process. A cluster that just started up may have access to the database, but completely empty memcache servers. To limit the cache misses hitting the database, the cold cluster will forward requests to a cluster that already has a satisfactory memcache hit-rate.</p>
<h3 id="scaling-among-regions">Scaling among regions</h3>
<figure><img src="https://www.micahlerner.com/assets/fbmc/architecture.png"/><figcaption>The same architecture image as above, but repeated for reference.</figcaption></figure>
<p>Facebook uses many regions around the world to get computers closer to their customers (which in turn results in lower latency) and reduce the risk that abnormal events like a <a href="https://www.datacenterdynamics.com/en/news/fire-destroys-ovhclouds-sbg2-data-center-strasbourg/">fire</a> or power outage bring their whole site down. Making a cache among these many regions is certainly difficult, and the paper discusses how <em>consistency</em> is their primary concern at this level.</p>
<p>At the time of the paper’s publication, Facebook relied on MySQL’s replication to keep databases up to date between regions. One region would be the master, while the rest would be the slaves <label for="terms"></label><span>I use the terms master/slave from the literature, rather than choosing them myself. </span>. Given the huge amount of data that Facebook has, they were willing to settle for eventual consistency (the system will tolerate out of sync data if the slave regions fall behind the master region).</p>
<p>Tolerating replication lag means that there are a few situations that need to be thought through.</p>
<p><strong>What happens if a MySQL delete happens in a master region?</strong></p>
<p>The MySQL commit log is consumed in the master region and produces a cache invalidation <em>only in the master region</em>. Because cache invalidations are produced from the MySQL commit log (versus cache invalidations and the commit log being replicated separately) the cache invalidation won’t even appear in a non-master region until the replication log is replicated there. Imagine all of the weird situations that could happen if the cache invalidations were replicated separately and a cache invalidation would show up before the database even knew about it (you could try to invalidate something that wasn’t in cache yet).</p>
<p><strong>What happens if a stale read happens in a non-master region?</strong></p>
<p>Because the system is eventually consistent, data in the slave regions will be out-of-date at some point. To limit the impact of clients reading out-of-date data, Facebook added a <em>remote marker mechanism</em>. When a web server wants to update a dataset and ensure that stale data is not read (or at least that there is a lower chance of stale reads), the server sets a marker for the key (where the marker’s value is a region may or not be the master region). Then, the server deletes the value from the region’s cache. Future reads will then be redirected to the region value set in the marker.</p>
<h3 id="takeaways">Takeaways</h3>
<p>This paper contains an incredible amount of detail on how Facebook scaled their memcache infrastructure, although the paper was published in 2013 and 8 years is a long time. I would be willing to bet that their infrastructure has changed significantly since the paper was originally published.</p>
<p>Even with the knowledge that the underlying infrastructure has likely changed, this paper provides useful insights into how the engineering org made many tradeoffs in the design based on data from the production system (and with the ultimate goal of a maintaining as simple of a design as possible.</p>
<p>Since 2013, a number of other companies have built key value stores and published research on their architectures - in the future I hope to read those papers and contrast their approaches with Facebook’s!</p>

</section></div>
  </body>
</html>
