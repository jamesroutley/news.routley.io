<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nan.fyi/database">Original</a>
    <h1>Build your own database</h1>
    
    <div id="readability-page-1" class="page"><article><header><p>A step-by-step guide to building a key-value database from scratch.</p></header><p>If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we&#39;ll explore how to build a <strong>key-value database</strong> from the ground up.</p>
<p>A key-value database works more or less like objects in JavaScript—you can store values using a key and retrieve them later using that same key:</p>
<div><pre><p><span>$</span><span> </span><span>db</span><span> </span><span>set</span><span> </span><span>&#39;hello&#39;</span><span> </span><span>&#39;world&#39;</span></p><p><span>$</span><span> </span><span>db</span><span> </span><span>get</span><span> </span><span>&#39;hello&#39;</span></p><p><span>world</span></p></pre></div>
<p>Let&#39;s find out how they work!</p>
<hr/>
<section><div><div><h2>The Humble File</h2><p>Databases were made to solve one problem:</p><div><header><h4>Problem</h4></header><p>How do we store data <strong>persistently</strong> and then <strong>efficiently</strong> look it up later?</p></div><p>The typical way to store any kind of data persistently in a computer is to use a <span> <em>file</em> </span>. When we want to store data, we add the key-value pair to the file:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>When we want to look for a specific key, we iterate through the pairs to see if there&#39;s a matching key:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>For updates, we&#39;ll find the key and replace the value in-place:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>And for deletes, we&#39;ll delete the record from the file:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>Easy! We&#39;re done right?</p></div></div></section>
<hr/>
<h3>Mutable Updates</h3>
<p>This approach, simple as it is, doesn&#39;t actually work very well in practice. The problem lies with the way we&#39;re doing updates and deletes—they&#39;re wholly inefficient.</p>
<p>To a computer, our file looks something like this—nothing more than a long sequence of bytes:</p>
<div><p>001:Lorem␣ipsum\n018:dolor␣sit\n005:adipiscing␣elit.\n014:Vestibulum␣varius\n002:vel␣mauris\n007:consectetur␣adipiscing␣elit.\n010:Vestibulum␣varius\n016:vel␣mauris\n003:consectetur␣adipiscing␣elit.</p></div>
<p>When we go to update or delete a record, we&#39;re currently updating that record in-place, which means we potentially have to <em>move</em> all of the data that comes after that record:</p>
<div><p><span>001:Lorem␣ipsum\n018:dolor␣sit\n<span>005:adipiscing␣elit.<span><span>␣vel␣mauris</span></span></span><span>\n014:Vestibulum␣varius\n002:vel␣mauris\n007:consectetur␣adipiscing␣elit.\n010:Vestibulum␣varius\n016:vel␣mauris\n003:consectetur␣adipiscing␣elit.</span></span></p></div>
<p>In this case, updating the record <code>005</code> to &#34;<code>adipiscing␣elit.␣vel␣mauris</code>&#34; means moving all of the records that come after it by 11 bytes (the length of the added string &#34;<code>␣vel␣mauris</code>&#34;). This can quickly get really costly, especially as our database grows in size!</p>
<hr/>
<section><div><div><h3>Append-Only Files</h3><p>One way to work around the update problem is to <strong>make records immutable</strong>. In other words, we add the constraint that we can only <em>add</em> new records to the end of the file and never update or delete existing ones.</p><p>With this approach, updates are treated the same as inserts—just add a new record to the end of the file:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>But now we have another problem—there are duplicate keys in the file!</p><p>To work around this, we have to change our search algorithm to look for the <em>last</em> occurrence of the key instead of the first:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>To delete records, we create a special &#34;tombstone&#34; record that marks the key as deleted. There&#39;s no single way to do this, but one way is to use a special value like <code>null</code>:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li></ol></div></div><p>And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.</p></div></div></section>
<hr/>
<p>Now this implementation isn&#39;t perfect; right now, there are two major issues:</p>
<ol><li><p><strong>The file can get very large</strong>. Since we&#39;re only appending to the file, the file will grow infinitely over time. Not good!</p></li><li><p><strong>Searching is slow</strong>. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!</p></li></ol>
<p>How can we fix these problems?</p>
<hr/>
<section><div><div><h2>Keeping Files Small</h2><div><header><h4>Problem</h4></header><p><strong>How do we make sure the file doesn&#39;t grow indefinitely?</strong> Because we&#39;re using an append-only file, we need some mechanism to periodically &#34;shrink&#34; the file so it doesn&#39;t eventually take over our entire hard drive.</p></div><p>Take a look at our database here after a few updates and deletes:</p><div><div><ol><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 1 &#34;Lorem ipsum&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 18 &#34;dolor sit&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 7 &#34;adipiscing elit.&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>delete</span><span> 7</span></li><li><span>$ <!-- -->db<!-- --> </span><span>set</span><span> 10 &#34;consectetur adipiscing elit.&#34;</span></li><li><span>$ <!-- -->db<!-- --> </span><span>delete</span><span> 1</span></li></ol></div></div><p>Our database file has six entries, but only two represent actual records—the rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!</p></div></div></section>
<hr/>
<h3>Segments and Compaction</h3>
<div><p>Here&#39;s an idea: once a file exceeds a certain size, we&#39;ll <span> <em>close</em> </span> the file and create a new one. While the new file ingests new data (in the same way we&#39;ve been doing so far), we&#39;ll <em>compact</em> the old file by deleting all of its irrelevant data.</p><p>Meaning, we stop adding new data to the file.</p></div>
<p>Here, we&#39;ve set the maximum file size to seven records. Notice that the database is full—try clicking on &#34;Add&#34; to add a new record and notice what happens:</p>

<p>Now, our database consists of two different files which we&#39;ll call <strong>segments</strong>. Each segment will usually become a lot smaller after compaction, which means we can merge them together as part of the compaction process.</p>
<p>With that, we&#39;ve made a mechanism to stop our database from growing indefinitely!</p>
<hr/>
<h2>Your First Index</h2>
<p>Our next problem is on search performance:</p>
<div><header><h4>Problem</h4></header><p><strong>How do we make searching fast?</strong> Right now, we have to iterate through all of the records in the database to find a specific key. This is super slow!</p></div>
<p>What if we use <em>objects</em>? That&#39;s right, these little guys:</p>

<p>JavaScript objects, otherwise known as <em>hash tables</em> or <em>dictionaries</em>, are really efficient at storing and looking up key-value pairs:</p>
<div><pre><p><span>const</span><span> </span><span>hashTable</span><span> </span><span>=</span><span> {</span></p><p><span>  hello: </span><span>&#34;world&#34;</span><span>,</span></p><p><span>  foo: </span><span>&#34;bar&#34;</span><span>,</span></p><p><span>  baz: </span><span>&#34;qux&#34;</span><span>,</span></p><p><span>};</span></p><p><span>const</span><span> </span><span>value</span><span> </span><span>=</span><span> hashTable[</span><span>&#34;hello&#34;</span><span>]; </span><span>// &#34;world&#34;</span></p></pre></div>
<p>It doesn&#39;t matter how many records there are—the time it takes to look up and retrieve a value in a hash table is more or less constant. The catch is they must live <em>in memory</em>.</p>
<hr/>
<details><summary><p><svg aria-hidden="true" width="20" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" style="transition-duration:460.52ms;transition-timing-function:linear(
  0 0%,
  0.00935 1%,
  0.034933 2%,
  0.073343 3%,
  0.121559 4%,
  0.176933 5%,
  0.237164 6%,
  0.300282 7.000000000000001%,
  0.364621 8%,
  0.428795 9%,
  0.491674 10%,
  0.552354 11%,
  0.610135 12%,
  0.664498 13%,
  0.715078 14.000000000000002%,
  0.761645 15%,
  0.804085 16%,
  0.842377 17%,
  0.87658 18%,
  0.906817 19%,
  0.933259 20%,
  0.956116 21%,
  0.975624 22%,
  0.992037 23%,
  1.005618 24%,
  1.016636 25%,
  1.025356 26%,
  1.032036 27%,
  1.036926 28.000000000000004%,
  1.040261 28.999999999999996%,
  1.042263 30%,
  1.043137 31%,
  1.043072 32%,
  1.04224 33%,
  1.040793 34%,
  1.038871 35%,
  1.036594 36%,
  1.034067 37%,
  1.031382 38%,
  1.028616 39%,
  1.025833 40%,
  1.023087 41%,
  1.020422 42%,
  1.017869 43%,
  1.015456 44%,
  1.013201 45%,
  1.011116 46%,
  1.009207 47%,
  1.007478 48%,
  1.005927 49%,
  1.00455 50%,
  1.00334 51%,
  1.002289 52%,
  1.001387 53%,
  1.000624 54%,
  0.999987 55.00000000000001%
)"><circle cx="12" cy="12" r="10"></circle><path d="M16 11.9999L8 12.0001M12 16V8.00012" stroke="white" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg><h3>Aside: <!-- -->In-Memory vs. On-Disk</h3></p></summary><div><p>When you write a variable in your code, the computer will &#34;remember&#34; the value of that variable only for as long as the program is running.</p><p>This program will always print <code>2</code> and <code>3</code> because the value of <code>x</code> &#34;resets&#34; every time we run the program. This is because <code>x</code> is stored <span> <em>in-memory</em> </span>, and any value stored in memory is discarded when the program stops.</p><p>If we want our data to &#34;stick&#34; between runs, we&#39;ll need to store it <span><em>on-disk</em></span>—in other words, a file.</p><p>This time, <code>x</code> will print <code>2</code> and <code>3</code> the first run, and <code>4</code> and <code>5</code> the second run.</p><p>The tradeoff to persistence is performance—accessing data from memory is about 80x faster on average than accessing it from disk.</p></div></details>
<hr/>
<p>Here&#39;s how the index will work. For every record that we have in our database, we&#39;ll store that record&#39;s <strong>offset</strong>—the number of bytes from the beginning of the file to the start of the record—in the index:</p>
<div><div><div><p>file.txt</p><div><p>1:Lorem␣ipsum\n</p><div><p><span>Waiting<span>.</span><span>.</span><span>.</span></span></p><p data-name="next-record" data-file="true"><span>1<!-- -->:<!-- -->Lorem␣ipsum<!-- -->\n</span></p></div></div></div></div></div>
<p>The second record, <code>18: dolor sit</code>, for example, has an offset of 15 because:</p>
<ol><li><p>Each character is 1 byte large;</p></li><li><p>The first record is 13 characters long (<code>1:Lorem ipsum</code>);</p></li><li><p>The first record ends with a newline character, which is (at most) 2 bytes long;</p></li></ol>
<p>This gives us an offset of <code>13 + 2 = 15</code>.</p>
<p>One thing to note is that we need an index for each segment because the offset is relative to the start of the file—in other words, the start of each segment.</p>
<hr/>
<h3>Searching With Indices</h3>
<p>Using an index, our search algorithm can now run a lot more efficiently:</p>
<ol><li><p>Starting at the most recent segment, look up the key in the index;</p></li><li><p>If the key is found, read the record at the offset;</p></li><li><p>If the key is not found, move on to the next segment;</p></li><li><p>Repeat (2) and (3) until the key is found or all segments have been searched.</p></li></ol>

<hr/>
<h3>Updating Indices</h3>
<p>An index is only useful if it&#39;s in sync with our data. Whenever we update, delete, or insert a record, we have to change the index accordingly:</p>

<p>Notice what this implies—<strong>writing to the database is slower with an index!</strong> This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.</p>
<hr/>
<h3>Tradeoffs</h3>
<p>An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:</p>
<ol><li><p><strong>Keys have to fit in memory</strong>. Since we&#39;re using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there&#39;s a limit on the number of keys we can store!</p></li><li><p><strong>Range queries are inefficient</strong>. Our index wouldn&#39;t help for search queries; if we wanted to find all the records between the keys <code>12</code> and <code>18</code>, for example, we&#39;d have to iterate through the entire database!</p></li></ol>
<hr/>
<h2>Sorted String Tables</h2>
<p>Here&#39;s an idea: what if we <strong>ensure our database is always sorted by key?</strong> By sorting our data, we can immediately make range queries fast:</p>

<hr/>
<h3>Sparse Indices</h3>
<p>One benefit of sorting our data is that we no longer need to store the offset of <em>every</em> record in memory.</p>
<p>Take a look at this database with four records. Since there&#39;s no logical order to the records, there&#39;s no way to determine where a record is without storing its key or searching through the entire database.</p>

<p>Knowing that <code>10</code> has an offset of <code>50</code> doesn&#39;t help us find where <code>18</code> is.</p>
<hr/>
<p>Now if these records were sorted, we could determine the location of each record using <em>any</em> of the keys in the index, even if it&#39;s not the key we&#39;re looking for.</p>
<p>Let&#39;s say our database is sorted but we only had the offset for the key <code>10</code>:</p>

<p>Let&#39;s say we want to find the key <code>18</code>. We know that <code>18</code> is greater than <code>10</code>, which means it must be after <code>10</code> in the database. In other words, we can start searching for <code>18</code> from <code>10</code>&#39;s offset—<code>36</code> in this case.</p>

<p>While this is certainly slower than having the offset for <code>18</code> directly, it&#39;s still faster than looping through the database in its entirety.</p>
<p>The real unlock here lies in being able to control the trade-off between memory and performance: <strong>a denser index means faster lookups, but more memory usage</strong>.</p>
<hr/>
<h3>Sorting in Practice</h3>
<p>Ensuring our database is always sorted is much easier said than done; by definition, sorting data requires moving around records as new ones get added—something that cannot be done efficiently when we&#39;re storing data on-disk. This brings us to our problem:</p>
<div><header><h4>Problem</h4></header><p><strong>How do we keep our data sorted <em>and</em> append-only?</strong> It&#39;s too slow to sort the data on-disk every time we add a new record; is there another way?</p></div>
<hr/>
<p>The trick is to first <strong>sort the data in memory</strong>, and <em>then</em> write it to disk.</p>
<ol><li><p>When we add a new record, add it to a sorted in-memory list;</p></li><li><p>When our in-memory list gets too large, we&#39;ll write it to disk;</p></li><li><p>When we want to read a record, we&#39;ll read the in-memory list first, and then the disk if necessary.</p></li></ol>

<p>The data structure used to store the in-memory list is usually one optimized for sorted data like a <strong>balanced binary search tree</strong> or more commonly, a <strong>skip list</strong>.</p>
<hr/>
<p>Of course, the main downside of having some of your data in-memory is that it&#39;s not persistent—if the program crashes or the computer shuts down, all of the data in the in-memory list is lost.</p>
<p>The fix here is thankfully pretty straightforward—every time we add a record to the list, <strong>we also write it to an append-only file on disk</strong>. This way, we have a backup in case a crash does happen (which it most certainly will).</p>

<p>The append-only file doesn&#39;t need to be sorted nor does it need to have every record in the database; only the ones that are currently in memory.</p>
<hr/>
<section><div><div><p>With that, we have our very own key-value database! Let&#39;s recap how it works.</p><p>Our database starts out empty. When we go to add a new record, we&#39;ll add it to a <strong>sorted in-memory list</strong>, keeping a copy in an append-only file in case of crashes.</p><figure><div><div></div></div></figure></div><div><p>When the in-memory list gets too large, we&#39;ll <em>flush</em> the list by writing all of the records to a file in sorted order. In the process, we&#39;ll keep note of each record&#39;s offset in an <strong>index</strong> so we can efficiently look them up later.</p><figure><div><div></div><div></div></div></figure></div><div><p>When we want to look up a record, we&#39;ll first check the in-memory list. If the record isn&#39;t there, we&#39;ll check the index to see if it&#39;s in the on-disk file.</p><figure><div><div></div><div></div></div></figure></div><div><p>Once a file is saved to disk, it&#39;s considered <strong>immutable</strong> which means we can only ever read from the file and never update it. To work around this, we&#39;ll treat updates and deletes the same as inserting new records—add them to the in-memory list.</p><figure><div></div></figure></div><div><p>Treating updates and deletes as new records means our file will only ever grow larger. To prevent this, we&#39;ll occassionally <em>compact</em> the on-disk files by deleting all duplicate records.</p></div></div></section>
<hr/>
<h2>LSM Trees</h2>
<p>What we just built actually exists in the real world—it&#39;s called <strong>an LSM or Log-Structured Merge Tree</strong>.</p>
<p>An LSM tree works by combining an in-memory list (often called a <em>memtable</em>) with an on-disk file (typically called a <em>sorted string table</em> or SST) to create a really fast key-value database.</p>
<p>LSM trees are the underlying data structure used for large-scale key-value databases like <a href="https://github.com/google/leveldb">Google&#39;s LevelDB</a> and <a href="https://aws.amazon.com/dynamodb/">Amazon&#39;s DynamoDB</a>, and they have proven to perform really well at scale—on Prime Day 2020, DynamoDB peaked at 80 <em>million</em> requests per second!</p>
<p>Now, LSM trees aren&#39;t perfect, and they&#39;re certainly not the only way to structure a database. In fact, relational databases like PostgreSQL or MySQL use a completely different structure called a <strong>B-Tree</strong> to store their data—but that&#39;s a deep dive for another post.</p>
<p>For now, thanks for reading!</p></article></div>
  </body>
</html>
