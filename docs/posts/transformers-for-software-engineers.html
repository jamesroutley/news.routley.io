<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.nelhage.com/post/transformers-for-software-engineers/">Original</a>
    <h1>Transformers for software engineers</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>Ever since its introduction in the 2017 paper, <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>, the Transformer model architecture has taken the deep-learning world by storm. Initially introduced for machine translation, it has become the tool of choice for a wide range of domains, including text, audio, video, and others. Transformers have also driven most of the massive increases in model scale and capability in the last few years. OpenAI’s <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> and <a href="https://openai.com/blog/openai-codex/">Codex</a> models are Transformers, as are DeepMind’s <a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">Gopher</a> models and many others.</p>
<p>About a year ago, I started working on the Interpretability team at <a href="https://www.anthropic.com/">Anthropic</a>, trying to reverse-engineer models in this family. Our goals is to ultimately understand just how these models implement their impressive capabilities; ideally, we aspire to be able to articulate and describe in detail the algorithms that are being executed inside the massive parameters of these models.</p>
<h2 id="reverse-engineering-a-model">Reverse-engineering a model <a href="#reverse-engineering-a-model"><i>	🔗︎</i></a> </h2>
<p>In some ways, the project of <a href="https://distill.pub/2020/circuits/">circuits-style interpretability</a> is similar to reverse-engineering a large unknown binary for which we lack source code. We can trace “execution” of a model step-by-step and observe the weights and activations at every point, but what we want is to extract higher-level human-comprehensible algorithms and names which describe what the model is actually doing with those low-level details, in a way that is useful for helping humans predict the behavior of these models.</p>
<p>My background is as a software engineer, and I’ve also done some reverse-engineering work in the past, so I tend to – among other lenses – think of Transformer models through very “computational” or “software engineering” lenses.</p>
<p>This post is an attempt to present the Transformer architecture in a way that highlights some of the perspectives and intuitions that view affords. We’ll walk through a (mostly) complete implementation of a GPT-style Transformer, but the goal will not be running code; instead, I’m going to use the language of software engineering and programming to explain how these models work and articulate some of the perspectives we bring to them when doing interpretability work.</p>
<p>This post is probably best read alongside traditional presentations of these models; I’ve glossed over a lot of details to focus on the pieces I find interesting. <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> is one great resource, although note we’ll be looking at “Decoder-only” models with only the “decoder” stack, and no “encoder”.</p>
<p>I hope for this post to be a helpful introduction to the details of the Transformer model for anyone with a SWE background who is in the process of getting into ML, and for it to be particularly helpful if you’re looking to do interpretability work.</p>
<p>If you want to follow along, you can <a href="https://github.com/nelhage/transformer-rs/blob/master/src/lib.rs">find all the code in the post on github</a>.</p>
<h2 id="the-transformer-model">The Transformer Model <a href="#the-transformer-model"><i>	🔗︎</i></a> </h2>
<p>We’ll start with some hyperparameters. I’ve picked the values from GPT-3 175B for concreteness but these can vary across many orders of magnitude. We’ll explain these as needed so don’t worry if you don’t recognize all of them right now.</p>
<p>You can compare these to Table 2.1 <a href="https://arxiv.org/pdf/2005.14165.pdf">in the GPT-3 paper.</a></p>
<p>(All the code in this post will be in Rust. I’ve tried to avoid using
any overly esoteric features, so hopefully it will be somewhat
readable even if you aren’t comfortable with Rust in particular)</p>
<div><pre tabindex="0"><code data-lang="rust"><span>const</span> N_LAYERS: <span>usize</span> <span>=</span> <span>96</span>;
<span>const</span> D_MODEL: <span>usize</span> <span>=</span> <span>12288</span>;
<span>const</span> D_MLP: <span>usize</span> <span>=</span> <span>4</span> <span>*</span> D_MODEL;
<span>const</span> D_HEAD: <span>usize</span> <span>=</span> <span>128</span>;
<span>const</span> N_HEADS: <span>usize</span> <span>=</span> D_MODEL <span>/</span> D_HEAD;
<span>const</span> N_VOCAB: <span>usize</span> <span>=</span> <span>50000</span>;</code></pre></div>
<h2 id="autoregressive-language-modeling">Autoregressive language modeling <a href="#autoregressive-language-modeling"><i>	🔗︎</i></a> </h2>
<p>At the highest level, an autoregressive language model (including the decoder-only Transformer) will take in a sequence of text (which we’ll refer to as a “context”), and output a sequence of “logits” the same length as the context. These logits represent, at each position, the model’s prediction for the <strong>next</strong> token. At each position, there is one logit value per entry in our vocabulary; by taking a softmax over the logit vector, we can get a probability distribution over tokens.</p>
<p>The logits for the final input position thus correspond to predictions for the <strong>next</strong> token following the context; by sampling from this distribution we can generate a continuation of the context. This  method (with various variations) is how Transformers can be used to generate text.</p>
<p><img src="https://twitter.com/images/posts/transformers/autogressive-model.png" alt=""/></p>
<div><pre tabindex="0"><code data-lang="rust"><span>type</span> <span>Token</span> <span>=</span> <span>u64</span>;
<span>type</span> <span>Logits</span> <span>=</span> [<span>f32</span>; N_VOCAB];

<span>trait</span> ARModel {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, tokens: <span>&amp;</span>[Token]) -&gt; Vec<span>&lt;</span>Logits<span>&gt;</span>;
}</code></pre></div>
<h2 id="the-residual-stream-state-vector">The residual stream “state vector” <a href="#the-residual-stream-state-vector"><i>	🔗︎</i></a> </h2>
<p>The core internal architecture of the Transformer is a stack of identically-structured layers. Each layer takes in the previous “hidden state” of the model — one state per each token position — does some computation, and then updates that state to produce the following state.</p>
<p>In a Transformer, this “state” is just a vector of floating-point values, one vector for each position, each vector of length <code>D_MODEL</code>. We’ll define a type to hold that state:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>#[derive(Clone)]</span>
<span>struct</span> <span>State</span>([<span>f32</span>; D_MODEL]);

<span>// At every point inside the model, we have one State per token
</span><span>// position
</span><span></span><span>type</span> <span>ResidualStream</span> <span>=</span> [State];</code></pre></div>
<p>I often like to think of the state (which we call the “residual stream”) as an “opaque data structure.” It contains within it many different features, but those features are opaque and cannot be directly accessed. Instead, the state exposes two main operations:</p>
<ul>
<li>We can <strong>query</strong> it. To do this, we provide a “query” vector, and take a dot product with that vector to produce a single floating-point value.</li>
<li>We can <strong>update</strong> it, by providing an “update” vector which will be added to the state vector.</li>
</ul>
<p>We’ll define type aliases for those types to help keep them straight in our presentation. They’re both the same physical type as the state vector, but we’ll often think of them pretty differently. We’ll also define the basic operations on them.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>type</span> <span>Query</span> <span>=</span> State;
<span>type</span> <span>Update</span> <span>=</span> State;

<span>impl</span> State {
    <span>fn</span> <span>zero</span>() -&gt; <span>Self</span> {
        State([<span>0.0</span>; D_MODEL])
    }

    <span>fn</span> <span>update</span>(<span>&amp;</span>self, right: <span>&amp;</span><span>Update</span>) -&gt; <span>State</span> {
        <span>let</span> <span>mut</span> out <span>=</span> self.clone();
        <span>for</span> (i, r) <span>in</span> right.<span>0.</span>iter().enumerate() {
            out.<span>0</span>[i] <span>+=</span> r;
        }
        out
    }

    <span>fn</span> <span>query</span>(<span>&amp;</span>self, right: <span>&amp;</span><span>Query</span>) -&gt; <span>f32</span> {
        dot(<span>&amp;</span>self.<span>0</span>, <span>&amp;</span>right.<span>0</span>)
    }
}

<span>fn</span> <span>dot</span><span>&lt;</span><span>const</span> N: <span>usize</span><span>&gt;</span>(l: <span>&amp;</span>[<span>f32</span>; N], r: <span>&amp;</span>[<span>f32</span>; N]) -&gt; <span>f32</span> {
    <span>let</span> <span>mut</span> out <span>=</span> <span>0.0</span>;
    <span>for</span> (i, r) <span>in</span> r.iter().enumerate() {
        out <span>+=</span> l[i] <span>*</span> r;
    }
    out
}</code></pre></div>
<p>It’s worth saying a few words about these definitions and how I’m trying to use them to convey intuitions and mental models I have.</p>
<ul>
<li>Because of the way layers can only update the state by adding an <code>Update</code> into it, and not by replacing the entire thing, I often think of the Transformer as a “functional program” implemented on a very unusual machine. We pass the state stream through and repeatedly update it to produce to a new state, but we never mutate it in-place or completely throw it away. In this model, we might think of the “model architecture” and the layers as the “instructions” for our weird machines, and the weights as our “operands” for those instructions.</li>
<li>We will shortly end up with vectors of States, one per token position. Traditionally we represent these directly as 2d tensors (3d with a batch dimension); by defining a <code>State</code> alias we can make it clearer when to think of a 2d tensor as a vector of 1d states, vs as a 2d matrix encoding some linear transformation.</li>
<li>The “opaque” state and the “query” and “update” operations get at the fact that the residual stream state vectors have <a href="https://transformer-circuits.pub/2021/framework/index.html#def-privileged-basis">no privileged basis space</a>. We should not expect the individual dimensions of the state vector to have meaning, and so we abstract them out.</li>
<li>Along that line, it’s a quirk of high-dimensional spaces that in N-dimensional space you can construct exponentially-many “almost-orthogonal” vectors (which all have very small pairwise dot products). We suspect Transformers use this property to (noisily) encode far more than <code>D_MODEL</code> features into the residual stream, and I think the “opaque structure you can query for the presence of a particular direction” fits well with this model.</li>
</ul>
<h2 id="the-layers">The layers <a href="#the-layers"><i>	🔗︎</i></a> </h2>
<p>Now we move to the actual layers of the model. A Transformer work starts with an embedding layer, followed by some number of blocks (which we refer to as “residual blocks” or “resblocks”), and finally an unembedding layer.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>Transformer</span> {
    embedding: <span>Embedding</span>,
    layers: [ResBlock; N_LAYERS],
    unembedding: <span>Unembedding</span>,
}</code></pre></div>
<h2 id="the-unembedding">The (un)embedding <a href="#the-unembedding"><i>	🔗︎</i></a> </h2>
<p>The purpose of embedding and unembedding is to convert between tokens to and from the internal state. We’ll define them for the standard construction of Transformers that act on tokenized text streams; however, we could also construct models that work on images, audio, or just about anything else pretty much solely by swapping out the (un)embeddings.</p>
<p>The embedding is simply a table containing the initial state vector for each possible token. Applying the embedding is simply an array index:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>Embedding</span>([State; N_VOCAB]);

<span>impl</span> Embedding {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, tok: <span>Token</span>) -&gt; <span>State</span> {
        self.<span>0</span>[tok <span>as</span> <span>usize</span>].clone()
    }
}</code></pre></div>
<p>Note that, mathematically, indexing into a 2d array like this is equivalent to a matrix multiply with a “one-hot” vector, so you’ll sometimes see this operation described in papers as a matrix multiplication.</p>
<p>We also see here one of the reasons I’ve opted for a named “State” type, in practice. The embedding is physically a 2d <code>D_MODEL x N_VOCAB</code> matrix, and is usually written that way, but I find it much more natural most of the time to think of it as a 1d lookup table of these opaque state objects. This avoids, for instance, any confusion about which axis to index your tensor over.</p>
<p>Of course, it’s also <a href="https://transformer-circuits.pub/2021/framework/index.html#zero-layer-transformers">often useful</a> to view it as a matrix so that we can multiply it by other matrices for various analyses: true fluency requires being able to hold both perspectives and move between them.</p>
<p>The unembedding, at the end of the model, is responsible for turning the state back into logits. Typically this is also represented as a matrix of shape <code>D_MODEL x N_VOCAB</code>, and it is applied to a state via a matrix multiplication. This is computationally efficient and mathematically nice, but I’ve opted for a slightly different view:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>LogitFn</span>(Query);
<span>impl</span> LogitFn {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, st: <span>&amp;</span><span>State</span>) -&gt; <span>f32</span> {
        self.<span>0.</span>query(st)
    }
}

<span>struct</span> <span>Unembedding</span>([LogitFn; N_VOCAB]);

<span>impl</span> Unembedding {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, state: <span>&amp;</span><span>State</span>) -&gt; <span>Logits</span> {
        <span>let</span> <span>mut</span> out: <span>Logits</span> <span>=</span> [<span>0.0</span>; N_VOCAB];
        <span>for</span> (i, f) <span>in</span> self.<span>0.</span>iter().enumerate() {
            out[i] <span>=</span> f.apply(state);
        }
        out
    }
}</code></pre></div>
<p>Each vocabulary element has a <code>LogitFn</code> which converts the state into a single floating-point value, by querying the state according to some particular query. To compute the overall logit vector, we simply apply each vocab entry’s query.</p>
<p>This formulation is mathematically identical to the standard matrix multiply formulation. I choose this presentation to emphasize a few points:</p>
<ul>
<li>The computation of each individual logit is independent of the others; there are no cross-interactions at this stage of the computation. We’ll get cross-interactions once we apply the softmax, but not in the logit computation.</li>
<li>The logit computation uses our standard “query” operation; that really is the main way the model looks at the residual stream.</li>
<li>As with the embedding, I find it useful to be able to move between a view of an NxK matrix as any of:
<ul>
<li>A 2d array of floating-point numbers</li>
<li>A linear transformation from K-dimensional space into N-dimensional space.</li>
<li>A 1d array of K-dimensional vectors</li>
<li>A 1d array of linear functions from K-dimensional space to scalars</li>
</ul>
</li>
</ul>
<p>(and of course, we can transpose or multiply on the other side, and thus flip the roles of N and K in any or all of the above statements!)</p>
<p>This presentation emphasizes the last two views, which are less frequently spelled out.</p>
<h2 id="the-residual-blocks">The residual blocks <a href="#the-residual-blocks"><i>	🔗︎</i></a> </h2>
<p>The meat of the transformer is the <code>N_LAYERS</code> “residual blocks,” which happen one after another in between the embedding and the unembedding. Each block consists of an attention layer, followed by an MLP layer.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>ResBlock</span> {
    attn: <span>AttnLayer</span>,
    mlps: <span>MLPLayer</span>,
}</code></pre></div>
<p>Let’s take each in turn.</p>
<h2 id="attention">Attention <a href="#attention"><i>	🔗︎</i></a> </h2>
<p>Transformers use “multi-headed” attention; each attention layer consists of a number of different attention heads operating a smaller subspace of size <code>D_HEAD</code>. Each head is made up of four different (linear) functions:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>AttnLayer</span> {
    heads: [AttnHead; N_HEADS],
}

<span>type</span> <span>AttnVector</span> <span>=</span> [<span>f32</span>; D_HEAD];

<span>struct</span> <span>AttnHead</span> {
    W_Q: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>State) -&gt; <span>AttnVector</span><span>&gt;</span>,
    W_K: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>State) -&gt; <span>AttnVector</span><span>&gt;</span>,
    W_V: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>State) -&gt; <span>AttnVector</span><span>&gt;</span>,
    W_O: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>AttnVector) -&gt; <span>Update</span><span>&gt;</span>,
}</code></pre></div>
<p>I’ve chosen to completely abstract the Q, K, V, and O weight matrices as generic functions from the relevant domain into the relevant range. We’ll remember, though, that we don’t actually mean “arbitrary function” here, but rather “arbitrary linear function,” aka “matrix.”</p>
<p>I like this representation because it helps me keep straight the separation between the “entire self-attention operation” – which includes the Q, K, V, and O projections – and the “core attention” operation which uses the resulting vectors. It also emphasizes that we want to think of these weights not just as arbitrary bags of floating-point numbers, but as operations that transform between different vector spaces. The type signatures also make it very clear that W_O is of a different type than W_Q, W_K, and W_V.</p>
<p>Attention is the only operation that mixes information <strong>between</strong> token positions. Thus, methods on AttnHead and AttnLayer will operate on a <code>&amp;[State]</code> instead of a single State object. I find this convention very helpful for keeping straight which parts of the Transformer operate across multiple positions, and which ones are local to a position. I love it when we can use a type system to make properties like this more obvious to a reader of our code.</p>
<p>Attention heads are probably the most complex component in a Transformer. I’ll present the code with heavy commenting inline to try to explain what’s going on.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>impl</span> AttnHead {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, states: <span>&amp;</span>[State]) -&gt; Vec<span>&lt;</span>Update<span>&gt;</span> {
        <span>// Apply the Q, K, and V projections to produce Q, K, and V
</span><span></span>        <span>// vectors for each token position.
</span><span></span>        <span>let</span> qs: Vec<span>&lt;</span>AttnVector<span>&gt;</span> <span>=</span> states.iter().map(<span>&amp;</span>self.W_Q).collect();
        <span>let</span> ks: Vec<span>&lt;</span>AttnVector<span>&gt;</span> <span>=</span> states.iter().map(<span>&amp;</span>self.W_K).collect();
        <span>let</span> vs: Vec<span>&lt;</span>AttnVector<span>&gt;</span> <span>=</span> states.iter().map(<span>&amp;</span>self.W_V).collect();

        <span>let</span> <span>mut</span> values: Vec<span>&lt;</span>_<span>&gt;</span> <span>=</span> states.iter().map(<span>|</span>_<span>|</span> [<span>0.0</span>; D_HEAD]).collect();

        <span>// Iterate over each token position to compute the output at
</span><span></span>        <span>// that position
</span><span></span>        <span>for</span> (src, my_q) <span>in</span> qs.iter().enumerate() {
            <span>// Each position may attend to any earlier position. We
</span><span></span>            <span>// compute an attention &#34;score&#34; between the current
</span><span></span>            <span>// position and each earlier position by dot-producting
</span><span></span>            <span>// our Q vector with their K vector.
</span><span></span>            <span>let</span> <span>mut</span> scores <span>=</span> Vec::with_capacity(src);

            <span>let</span> visible_indices <span>=</span> <span>0</span>..<span>=</span>src;

            <span>for</span> i <span>in</span> visible_indices.clone() {
                scores.push(dot(my_q, <span>&amp;</span>ks[i]));
            }

            <span>// We use a softmax to turn that vector of scores into a
</span><span></span>            <span>// probability distribution
</span><span></span>            softmax(<span>&amp;</span><span>mut</span> scores);

            <span>// Now we loop over each visible position again, weight
</span><span></span>            <span>// their V vector by their attention weight, and sum them
</span><span></span>            <span>// all together.
</span><span></span>            <span>for</span> i <span>in</span> visible_indices {
                <span>let</span> score <span>=</span> scores[i];
                <span>let</span> v <span>=</span> vs[i];
                <span>for</span> (j, vj) <span>in</span> v.iter().enumerate() {
                    values[src][j] <span>+=</span> vj <span>*</span> score;
                }
            }
        }

        <span>// Now we have a value vector for each position. Use the O
</span><span></span>        <span>// projection to project it up to a full State vector
</span><span></span>        values.iter().map(<span>&amp;</span>self.W_O).collect()
    }
}</code></pre></div>
<p>I’ve chosen to write the actual “attention” operation as an explicit loop over “source” positions. In most presentations this is written in a more vectorized form, with a mask to encode the “autoregressive” or “causal” property that tokens can only “see” to earlier tokens.</p>
<p>Here, again, I go back and forth between different perspectives, but I do often find it easiest to reason about attention from the perspective of a <strong>single</strong> source position at a time, and only then efficiently vectorize it in my ML framework; and I think this perspective aligns well with that view.</p>
<p>An attention layer just applies each attention head, and sums their outputs:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>impl</span> AttnLayer {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, states: <span>&amp;</span>[State]) -&gt; Vec<span>&lt;</span>Update<span>&gt;</span> {
        <span>let</span> <span>mut</span> updates: Vec<span>&lt;</span>Update<span>&gt;</span> <span>=</span> states.iter().map(<span>|</span>_<span>|</span> State::zero()).collect();

        <span>for</span> h <span>in</span> self.heads.iter() {
            <span>let</span> head_out <span>=</span> h.apply(states);

            updates <span>=</span> updates
                .iter()
                .zip(head_out.iter())
                .map(<span>|</span>(l, r)<span>|</span> l.update(r))
                .collect();
        }

        updates
    }
}</code></pre></div>
<p>I’ll make a few observations about this formulation:</p>
<ul>
<li>I use a lot more explicit loops and indexes than in the usual mathematical notation, which lets us elide a lot of details down into nice Σ expressions, matrix multiplies, and such. The math is very convenient and expressive once you get fluent with it, but I also like having a more explicit version available in my head. I also think I would have found this presentation helpful when I was first understanding Transformers, just because it’s written in a language I’m already fluent in.</li>
<li>Notice that all of the “computation” in an attention head happens in the lower-dimensional <code>AttnVector</code> space. I mentioned earlier that I like to think of the State vector as an “opaque type”; in that lens, the Q, K, and V projections act as “accessors,” retrieving some (head-specific) attributes or features stored in that larger opaque abstraction; we can then do computation on those features, and use the W_O “setter” to tell us how to store the result back into the state vector.
<ul>
<li>If we wanted, we could write (e.g.) <code>W_Q</code> as <code>[Query; D_HEAD]</code> — it projects out <code>D_HEAD</code> queries of the state vector. I chose not to do so because the individual dimensions of queries and keys are also not necessarily meaningful, so I don’t want to overly highlight them.</li>
</ul>
</li>
<li>This formulation, for me, hints at the fact (discussed in more length in our first paper) that W_Q and W_K interact, and that W_V and W_O interact, but that there’s no direct interactions between the other pairs of matrices (e.g W_Q and W_V). I’ll present an alternate formulation of attention later, which is impractical to implement, but makes this observation explicit.</li>
</ul>
<h2 id="mlp-layers">MLP Layers <a href="#mlp-layers"><i>	🔗︎</i></a> </h2>
<p>By comparison, an MLP layer is almost trivial. MLP layers are usually written as parameterized by two matrices, sometimes called W_in and W_out, or W_up and W_down. I’ve chosen instead to emphasize the computational structure of those matrices by phrasing a layer as a list of individual “neurons”; each neuron in turn has an “input” and “output,” using our <code>Query</code>  and <code>Update</code> types.</p>
<p>Each neuron then has a very simple operation:</p>
<ul>
<li>It queries the state using its input vector, to produce a single scalar.</li>
<li>It applies a nonlinear function to that scalar (this is typically <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> or <a href="https://arxiv.org/abs/1606.08415">GeLU</a>, although other options exist)</li>
<li>It then scales its output vector by that scalar.</li>
</ul>
<p>The MLP layer as a whole applies each neuron in parallel, and sums the result.</p>
<p>Typically we’d think of the activation function as a property of the entire Transformer, not each layer, and use one consistent activation function. I’ve compromised here and put it on the layer because it makes the code nicely self-contained.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>Neuron</span> {
    read: <span>Query</span>,
    write: <span>Update</span>,
}

<span>struct</span> <span>MLPLayer</span> {
    mlps: [Neuron; D_MLP],
    nonlinear: <span>fn</span>(<span>f32</span>) -&gt; <span>f32</span>,
}

<span>impl</span> MLPLayer {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, state: <span>&amp;</span><span>State</span>) -&gt; <span>Update</span> {
        <span>let</span> <span>mut</span> out: <span>Update</span> <span>=</span> State::zero();
        <span>for</span> mlp <span>in</span> self.mlps.iter() {
            <span>// &#34;act&#34; is for &#34;activation&#34;, a pretty standard naming
</span><span></span>            <span>// convention
</span><span></span>            <span>let</span> pre_act <span>=</span> mlp.read.query(state);
            <span>let</span> post_act <span>=</span> (self.nonlinear)(pre_act);
            <span>let</span> unit_out: <span>Update</span> <span>=</span> State(mlp.write.<span>0.</span>map(<span>|</span>f<span>|</span> f <span>*</span> post_act));
            out <span>=</span> out.update(<span>&amp;</span>unit_out)
        }
        out
    }
}</code></pre></div>
<p>We note, once again, that the types immediately clarify that MLPs only happen to one position at a time.</p>
<p>I find this presentation somewhat helpful for clarifying and demystifying MLP layers. It makes it clear that each neuron acts completely independently, and also that the operation of each individual neuron is almost trivial in its simplicity: a dot product, a nonlinearity on a single point, and then a multiply-accumulate of an output vector. The complexity of the MLP layer comes entirely from the aggregation of all of these individual effects.</p>
<p>As with everything else in this document, I find the real value comes from having multiple perspectives that I can pivot between as needed. Treating the input and output projections as matrices or linear transformations exposes a lot of rich structure that we can’t readily talk about in this “computational” view.</p>
<h2 id="putting-it-all-together">Putting it all together <a href="#putting-it-all-together"><i>	🔗︎</i></a> </h2>
<p>We’re now ready to implement the full Transformer. At this point it’s mostly ceremony, but we’ll include it for completeness:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>impl</span> ARModel <span>for</span> Transformer {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, tokens: <span>&amp;</span>[Token]) -&gt; Vec<span>&lt;</span>Logits<span>&gt;</span> {
        <span>// Embeddings: convert tokens into initial states
</span><span></span>        <span>let</span> <span>mut</span> states <span>=</span> tokens
            .iter()
            .map(<span>|</span>t<span>|</span> self.embedding.apply(<span>*</span>t))
            .collect::<span>&lt;</span>Vec<span>&lt;</span>_<span>&gt;&gt;</span>();

        <span>// Pass the hidden state through each layer in turn
</span><span></span>        <span>for</span> layer <span>in</span> self.layers.iter() {
            <span>let</span> attn_out <span>=</span> layer.attn.apply(<span>&amp;</span>states);
            states <span>=</span> states
                .iter()
                .zip(attn_out.iter())
                .map(<span>|</span>(l, r)<span>|</span> l.update(r))
                .collect();

            <span>for</span> i <span>in</span> <span>0</span>..states.len() {
                <span>let</span> mlp_out <span>=</span> layer.mlps.apply(<span>&amp;</span>states[i]);
                states[i] <span>=</span> states[i].update(<span>&amp;</span>mlp_out);
            }
        }

        <span>// Then apply the unembedding to get out logits
</span><span></span>        states.iter().map(<span>|</span>s<span>|</span> self.unembedding.apply(s)).collect()
    }
}</code></pre></div>

<p>I’ve left a few important details out of the model, either because there’s no single standard implementation, or because I felt they would distract from the core takeaways. Consult an actual implementation (such as <a href="https://github.com/huggingface/transformers">HuggingFace</a>’s or <a href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a>) if you want to actually build and run a Transformer. But I’ll mention some of the pieces I left out here.</p>
<h2 id="positional-encodings">Positional Encodings <a href="#positional-encodings"><i>	🔗︎</i></a> </h2>
<p>Astute readers might have noticed that our implementation contains no positional information; the attention operation sees all earlier positions and has no mechanism to know which positions are closer to the current token, or any other information about their position.</p>
<p>Real Transformers use any one of a number of mechanisms to give models positional information. I’ll mention two:</p>
<ul>
<li>The classic “Attention is All You Need” Transformer <strong>added</strong> a bunch of sine and cosine waves of varying frequencies directly into the activations just after the embedding. We assume the model will use to distinct subspaces for “content” and “positional” information, and using basic trig functions lets the model learn operations like “subtract 1 from my position” as linear transformations of the “positional” subspace. Consult <a href="https://transformer-circuits.pub/2021/exercises/index.html">the exercises from our first paper</a> if you want to work through the details.</li>
<li><a href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Attention</a> is an increasingly common choice; it uses a clever transformation on the keys and values in order to effectively implement a relative attention mechanism (where the model sees only <strong>relative</strong> positions, like “three tokens ago,” instead of absolute positions, like “token index 21”) efficiently and cheaply.</li>
</ul>
<h2 id="layer-normalization">Layer Normalization <a href="#layer-normalization"><i>	🔗︎</i></a> </h2>
<p>Real Transformer implementations use <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> (or sometimes <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a>) either before or in between each layer, in order to aid with stability during training. Proper normalization is critical to stability during training, but adds only a very small amount of computation or additional parameters to the model, so for many purposes we believe it can be glossed over from a conceptual perspective, and I’ve chosen to do so here.</p>
<p>Our team’s first paper <a href="https://transformer-circuits.pub/2021/framework/index.html#model-simplifications">includes a few more notes</a> about this choice.</p>
<h2 id="biases">Biases <a href="#biases"><i>	🔗︎</i></a> </h2>
<p>Transformers often include bias terms following some of the matrix multiplications, which take the term of a single vector of parameters added in following the multiply. This to say, we replace the linear transformations with affine transformations.</p>
<p>In practice, we believe biases not to be terribly important (they, too, represent a small fraction of parameters and computation), and they can always be simulated if desired by keeping one dimension in the residual stream fixed to <code>1.0</code> and using that term of our matrices as the constant offset. Thus, I also omit them here for simplicity.</p>

<p>In our first paper, we <a href="https://transformer-circuits.pub/2021/framework/index.html#splitting-attention-head-terms-into-circuits">talk about splitting attention heads into two circuits</a>, the “QK” and “OV” circuits. As a bonus — and caveating that this implementation would be even more tragically inefficient than the previous one — I want to show how we would represent that perspective in the model here.</p>
<p>Our alternate attention heads only two fields, one for each “circuit”, with type signatures describing their new functions:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>struct</span> <span>AlternateAttnHead</span> {
    W_QK: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>State, <span>&amp;</span>State) -&gt; <span>f32</span><span>&gt;</span>,
    W_OV: Box<span>&lt;</span>dyn Fn(<span>&amp;</span>State) -&gt; <span>Update</span><span>&gt;</span>,
}</code></pre></div>
<p>The implementation is fairly similar, but now we don’t ever materialize queries, keys, or vectors, but operate always in the <code>State</code> space:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>impl</span> AlternateAttnHead {
    <span>fn</span> <span>apply</span>(<span>&amp;</span>self, states: <span>&amp;</span>[State]) -&gt; Vec<span>&lt;</span>Update<span>&gt;</span> {
        <span>let</span> <span>mut</span> output <span>=</span> states
            .iter()
            .map(<span>|</span>_<span>|</span> State::zero())
            .collect::<span>&lt;</span>Vec<span>&lt;</span>Update<span>&gt;&gt;</span>();

        <span>// Iterate over each token position to compute the output at
</span><span></span>        <span>// that position
</span><span></span>        <span>for</span> (src, src_state) <span>in</span> states.iter().enumerate() {
            <span>// Each position may attend to any earlier position. We
</span><span></span>            <span>// compute an attention &#34;score&#34; between the current
</span><span></span>            <span>// position and each earlier position by applying the QK circuit to both
</span><span></span>            <span>// positions
</span><span></span>            <span>let</span> <span>mut</span> scores <span>=</span> Vec::with_capacity(src);

            <span>let</span> visible_indices <span>=</span> <span>0</span>..<span>=</span>src;

            <span>for</span> i <span>in</span> visible_indices.clone() {
                scores.push((self.W_QK)(<span>&amp;</span>src_state, <span>&amp;</span>states[i]));
            }

            softmax(<span>&amp;</span><span>mut</span> scores);

            <span>// Now we loop over each visible position again, compute
</span><span></span>            <span>// the output update using the OV weight, scale it by the
</span><span></span>            <span>// attention weight, and accumulate.
</span><span></span>            <span>for</span> i <span>in</span> visible_indices {
                <span>let</span> score <span>=</span> scores[i];
                <span>let</span> o <span>=</span> (self.W_OV)(<span>&amp;</span>states[i]);
                <span>let</span> scaled <span>=</span> State(o.<span>0.</span>map(<span>|</span>f<span>|</span> f <span>*</span> score));
                output[src] <span>=</span> output[src].update(<span>&amp;</span>scaled);
            }
        }

        output
    }
}</code></pre></div>
<p>This main downside to this approach is that it doesn’t expose the key fact that attention takes places in a smaller vector space of size <code>D_HEAD</code>. That fact is both mathematically important (attention can only move a small subspace worth of information, not the entire residual stream), and important to the computational efficiency of attention.</p>
<p>However, even though we would never <strong>use</strong> an implementation like this, I find it informative to write out as a tool for thought, and I enjoy the exercise of using code as a medium for communication and reasoning to express what would usually be expressed in mathematical notation.</p>
<h2 id="why-is-this-so-inefficient">Why is this so inefficient? <a href="#why-is-this-so-inefficient"><i>	🔗︎</i></a> </h2>
<p>Let’s just spell out why this implementation is so much less efficient.</p>
<p>We’ll start by looking at the normal implementation of attention, for reference. We’ll consider a single attention head; for the total work we can just multiply everything by \(n_{heads}\), which will mostly have the effect of changing \(d_{head}\) to \(d_{model}\) everywhere.</p>
<p>We’re also just going to count multiplication operations, for simplicity; in practice this covers essentially all of the relevant work.</p>
<ul>
<li>Each of the K, Q, V, and O multiplications is  \( d_{head}\times{}d_{model} \). Applying those at every position nets out to \(4d_{model}d_{head} \) operations across all heads.</li>
<li>Then we have the nested loops over positions. If we have \(n_{ctx}\) positions in our context, there are \(n_{ctx}^2\) pairs of them<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>; when people refer to a vanilla Transformer as “quadratic”, this loop is what they’re talking about.</li>
<li>At each pair of positions, we do the QK dot product (\(d_{head}\) multiplications), and the V*score multiplication (another \(d_{head}\) multiplications). Thus we arrive at \(2n_{ctx}^2d_{head}\) multiplications.</li>
<li>Summing these terms and factoring out a bit we arrive at</li>
</ul>
<p>$$2d_{head}n_{ctx}(2d_{model} + n_{ctx})$$</p>
<p>total multiplications for a single attention head. We note that for large models, we generally have \(d_{model} \gg n_{ctx}\) (GPT-3 175B: \(d_{model}=12288\) vs \(n_{ctx}=2048\)), and so the quadratic term actually contributes very little to the computational cost!</p>
<p>However, what if we did the full <code>State</code> operations at every position? For maximum efficiency we would represent <code>W_QK</code> and <code>W_OV</code> in factored form, as a pair of \(d_{model}\times{}d_{head}\) matrices which we multiply in the appropriate order. We would do this operation at each point inside the nested loop, for a total of</p>
<p>$$n_{ctx}^2(4d_{head}d_{model})~~=~~2d_{head}n_{ctx}(2d_{model}n_{ctx})$$</p>
<p>multiplications! Now, instead of \(d_{model}\) and \((n_{ctx}\) *adding* on the right hand side, they *multiply*, and our computational cost goes absolutely through the roof. Essentially, what has happened was that the traditional version “lifts” the expensive matrix multiplies out of the inner loop, so that we only have relatively-cheap 1d operations inside the loop. We’ve undone that optimization, and are now doing the much-more-expensive 2d matrix operations inside the inner loop, which is never desirable.</p>

<p>I really enjoyed this writeup. I feel pretty satisfied about this alternate perspective for understanding and thinking about Transformers, and about the use of notation and types to express the some of the concepts I use to think about these models. I’m hopeful this post will be helpful to other software engineers who are interested in getting into or learning modern ML models, especially anyone interested in Transformer interpretability!</p>
<p>If you want to read more, I recommend our <a href="https://transformer-circuits.pub/">transformer-circuits</a> site, where we’ve published our first two papers and some additional resources outlining what we’ve learned and how we think about these models!</p>
<p>Alternately, if this post has left you feeling like you better-understand the Transformer but wanting to check your understanding, I recommend trying to write your own in PyTorch or JAX! You can get a working single-GPU model in very few lines of code, and it’s very cheap to get small amounts of GPU time in <a href="https://colab.research.google.com/">Google Colab</a> or TPU time from the <a href="https://sites.research.google/trc/about/">Google TPU Research Cloud</a>, even if you don’t have a GPU of your own. Writing a full model end-to-end and watching your loss start to come down is really quite fun.</p>
<p>And, of course, if you’re interested in working with me on these models full time, <a href="https://www.anthropic.com/#careers">Anthropic is hiring</a>! We don’t require ML experience for strong software and systems engineering candidates; we tend to be <a href="https://www.lesswrong.com/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers">bottlenecked on engineering</a> as much as or more so than on ML expertise, and we’re very happy to help strong engineers learn the ML that they need to in order to be productive here!</p>



</div></div>
  </body>
</html>
