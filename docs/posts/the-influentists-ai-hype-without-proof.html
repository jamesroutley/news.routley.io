<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://carette.xyz/posts/influentists/">Original</a>
    <h1>The Influentists: AI hype without proof</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>Last week, the developer community was busy discussing about a single tweet:</p>
<blockquote><p lang="en" dir="ltr">I&#39;m not joking and this isn&#39;t funny. We have been trying to build distributed agent orchestrators at Google since last year. There are various options, not everyone is aligned... I gave Claude Code a description of the problem, it generated what we built last year in an hour.</p>— Jaana Dogan ヤナ ドガン (@rakyll) <a href="https://twitter.com/rakyll/status/2007239758158975130?ref_src=twsrc%5Etfw">January 2, 2026</a></blockquote>



<p>The author is Jaana Dogan (known as <a href="https://github.com/rakyll">Rakyll</a>), a highly respected figure in the Google ecosystem, in the open-source world, and in my heart (thank you Rakyll for your great <a href="https://rakyll.org/">Go blog posts</a>).</p>
<p>At first glance, the tweet suggests an enormous shift in the software industry: the ability to build <strong>in just one hour</strong> what previously required weeks or months for a team of sofware engineers, using <strong>just</strong> the description of the problem. The tweet was too-much dramatic in my own opinion, but actually impressive!</p>
<p>The post triggered an immediate wave of “doom-posting,” with many fearing for the future of software engineering (as each week since a year now).
However, as the conversation reached a high number of replies and citations on social networks, Rakyll released a follow-up thread to provide context:</p>
<blockquote><p lang="en" dir="ltr">To cut through the noise on this topic, it’s helpful to provide more more context:</p>— Jaana Dogan ヤナ ドガン (@rakyll) <a href="https://twitter.com/rakyll/status/2007659740126761033?ref_src=twsrc%5Etfw">January 4, 2026</a></blockquote>



<p>This response thread revealed a story <strong>far less miraculous</strong> than the original tweet suggested.
Let’s analyze it.</p>
<p>Crucially, <strong>the foundational “thinking” had already been performed by Rakyll herself</strong>, who guided the AI using architectural concepts (honed over several weeks or months of prior effort) rather than the AI thinking and inventing the “product” from scratch.</p>
<p>Hmm. Now, this is far less exciting…</p>
<h2 id="under-influence">
  Under influence
  <a href="#under-influence">#</a>
</h2>
<p>This pattern of “<em>hype first and context later</em>” is actually part of a growing trend.</p>
<p>I call the individuals participating to that trend “<strong>The Influentists</strong>”.
Those people are members of a scientific or technical community, and leverage their large audiences to propagate claims that are, at best, unproven and, at worst, <em>intentionally</em> misleading.</p>
<p>But how can we spot them?</p>
<p>I personally identify these “Influentists” by four personality traits that characterize their public discourse.</p>
<blockquote><p lang="en" dir="ltr">I&#39;ve never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become…</p>— Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/2004607146781278521?ref_src=twsrc%5Etfw">December 26, 2025</a></blockquote>



<h2 id="a-growing-pattern">
  A Growing Pattern
  <a href="#a-growing-pattern">#</a>
</h2>
<p>Rakyll is far from alone. We see this “hype-first” approach across major AI firms like Anthropic, OpenAI, or Microsoft.</p>
<p>Consider Galen Hunt, a Distinguished Engineer at Microsoft. He recently made waves by claiming a goal to rewrite Microsoft’s massive C/C++ codebases into Rust by 2030 using AI.</p>
<figure>

    <p><img loading="lazy" alt="Galen Hunt original linkedin post" src="https://writing.natwelch.com/images/galen_hunt_original_post.png"/>
    </p>

    
</figure>

<p>When the industry pointed out the near-impossible complexity of this task, but also asking clarity for popular and critical products like Microsoft Windows, he was forced to clarify that it was only a “research project”.</p>
<figure>

    <p><img loading="lazy" alt="Galen Hunt updated linkedin post" src="https://writing.natwelch.com/images/galen_hunt_updated_post.png"/>
    </p>

    
</figure>

<p>Similarly, engineers from Anthropic and OpenAI oftenly post teasers about “AGI being achieved internally” to release months later models that disappoint the crowd.</p>
<blockquote><p lang="en" dir="ltr">Wait, can we consider seriously the hypothesis that 1) the recent hyped tweets from OA&#39;s staff </p>— Siméon (@Simeon_Cps) <a href="https://twitter.com/Simeon_Cps/status/1706078819617063304?ref_src=twsrc%5Etfw">September 24, 2023</a></blockquote>



<blockquote><p lang="en" dir="ltr">Liam, I have been a professional programmer for 36 years. I spent 11 years at Google, where I ended up as a Staff Software Engineer, and now work at Anthropic. I&#39;ve worked with some incredible people - you might have heard of Jaegeuk Kim or Ted Ts&#39;o - and some ridiculously… <a href="https://t.co/Ku8agTrps3">https://t.co/Ku8agTrps3</a></p>— Paul Crowley (@ciphergoth) <a href="https://twitter.com/ciphergoth/status/2006446942453387675?ref_src=twsrc%5Etfw">December 31, 2025</a></blockquote>



<p>Similarly, many other companies lie over what they are solving or willing to solve:</p>
<p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/tNmgmwEtoWE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

<h2 id="the-cost-of-unchecked-influence">
  The Cost of Unchecked Influence
  <a href="#the-cost-of-unchecked-influence">#</a>
</h2>
<p>When leaders at major labs propagate these hyped-based results, it can create a “technical debt of expectations” for the rest of us.
Junior developers see these viral threads and feel they are failing because they can’t reproduce a year of work in an hour, not realizing the “magic” was actually a highly-curated prototype guided by a decade of hidden expertise.</p>
<p>We must stop granting automatic authority to those who rely on hype, or vibes, rather than evidence.</p>
<p>The tech community must shift its admiration back toward <strong>reproducible results</strong> and away from this “<em>trust-me-bro</em>” culture.</p>

    </div></div>
  </body>
</html>
