<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/">Original</a>
    <h1>Iggy.rs â€“ building message streaming in Rust</h1>
    
    <div id="readability-page-1" class="page"><div><article><ul><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#origins">Origins</a></li><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#project">Project</a></li><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#team">Team</a></li><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#features">Features</a></li><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#roadmap">Roadmap</a></li><li><a href="https://blog.iggy.rs/posts/building-message-streaming-in-rust/#future">Future</a></li></ul><section><h2 id="origins">Origins</h2><p>Over half a year ago (in April, to be exact), I eventually decided to learn Rust for good. My previous attempt during the <a href="https://github.com/spetz/advent-of-code-2022">2022 AoC</a> had failed rather quickly, after a few days of completing the exercises - I finally realized that <strong>I needed a real project to work on</strong>. For the last few years, I&#39;ve been dealing with the different kinds of distributed systems (mostly using C#), including the typical microservices architecture or Web3. Regardless of their nature, some sort of the messaging between the independent components was always required. I had a chance to use the tools such as <a href="https://rabbitmq.com">RabbitMQ</a>, <a href="https://zeromq.org">ZeroMQ</a>, <a href="https://kafka.apache.org">Kafka</a> or <a href="https://aeron.io">Aeron</a> (just to name a few), as well as implementing the low-level peer-to-peer communication <a href="https://libp2p.io">protocols</a>.</p><p>After a few months of trying to figure out (or just staying in the limbo I guess), what would be the best project to work on, I decided to build the <strong>message streaming platform</strong> (keep in mind that streaming is <a href="https://blog.iron.io/message-queue-vs-streaming/">not the same</a> as regular message broker). The other reason (besides getting to know Rust) was to <strong>truly understand the internals of the messaging systems</strong> and the trade-offs that were made by their developers - some of them being the sole implication of the theory of distributed systems (ordering, consistency, partitioning etc.), while others the result of the implementation details (programming language, OS, hardware and so on).</p><p>And this is how the <a href="https://iggy.rs">Iggy.rs</a> was born. The name is an abbreviation of the Italian Greyhound (yes, I own <a href="https://www.instagram.com/fabio.and.cookie/">two of them</a>), small yet extremely fast dogs, the best in their class.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggys.jpeg"/></p><p>Therefore, what I want, or actually <strong>what we want</strong> (since there&#39;s a few of us working on it already) for Iggy.rs to be - the best message streaming platform in its class. <strong>Lightweight</strong> in terms of the resource consumption, <strong>fast</strong> (and predictable) when it comes to the throughput and latency, and <strong>easy to use</strong> when speaking of its API, SDK and configuration of the project.</p><h2 id="project">Project</h2><p>At the very beginning, Iggy had rather limited functionality, and everything was handled using the <a href="https://www.chromium.org/quic/">QUIC</a> protocol based on <a href="https://github.com/quinn-rs/quinn">Quinn</a> library. You could connect multiple applications into the server, and start exchanging the messages between them, simply by appending the data to the stream (from the producer perspective), and fetching the records on the consumer side, by providing an offset (numeric value specifying from which element in the stream, you&#39;d like to query the data) - that&#39;s pretty much the very basics of how the message streaming platform works in terms of the underlying infrastructure.</p><p>After spending a few weeks on building the initial version, and then another few weeks on rewriting its core part (yes, prototyping + validation repeated in a continuous loop worked quite well), I managed to implement the persistent streaming server being capable of parallel writes/reads to/from independent streams supporting many distinct apps connected into it. Simply put, one could easily have many applications, and even <strong>thousands of the streams</strong> (depending on how do you decide to split your data between them e.g. one stream for user related events, another one for the payments events etc.) and start producing &amp; consuming the messages without interfering to each other.</p><p>On top of this, the support for <strong>TCP</strong> and <strong>HTTP</strong> protocols have been added. Under the hood, the typical architecture of streams, consisting of the topics being split into the partitions, which eventually operate on a raw file data using so-called segments has been implemented as well.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_sample.jpeg"/></p><p><strong>It was one of the &#34;aha&#34; moments</strong>, when reimplementing the parallel access to the data with the usage of underlying synchronization mechanism (<em>RwLock</em> etc.), optimized data structures e.g. for dealing with <a href="https://crates.io/crates/bytes">bytes</a>, along with the <a href="https://tokio.rs">Tokio</a> <strong>work stealing</strong> approach, yielded the great improvements for the overall throughput.</p><p>I do believe, that somewhere at this point I had realized, that <strong>Iggy might actually become something useful</strong> - not just a toy project, to be abandoned after reaching its initial goal (which was sort of already achieved).</p><pre data-lang="rust"><code data-lang="rust"><span>let</span><span> polled_messages </span><span>=</span><span> client</span><span>.</span><span>poll_messages</span><span>(</span><span>&amp;</span><span>PollMessages {
</span><span>  stream_id</span><span>: </span><span>Identifier</span><span>::</span><span>numeric(</span><span>1</span><span>)</span><span>?</span><span>,
</span><span>  topic_id</span><span>: </span><span>Identifier</span><span>::</span><span>named(</span><span>&#34;orders&#34;</span><span>)</span><span>?</span><span>,
</span><span>  consumer</span><span>: </span><span>Consumer</span><span>::</span><span>group(Identifier</span><span>::</span><span>named(</span><span>&#34;payments&#34;</span><span>)</span><span>?</span><span>)</span><span>,
</span><span>  partition_id</span><span>: </span><span>None</span><span>,
</span><span>  strategy</span><span>: </span><span>PollingStrategy</span><span>::</span><span>offset(</span><span>0</span><span>)</span><span>,
</span><span>  count</span><span>: </span><span>10</span><span>,
</span><span>  auto_commit</span><span>: </span><span>true</span><span>,
</span><span>})</span><span>.</span><span>await</span><span>?</span><span>;
</span></code></pre><p>After running some benchmarks (yes, we have a dedicated app for the <strong><a href="https://github.com/iggy-rs/iggy/tree/master/bench">benchmarking purposes</a></strong>) and seeing the promising numbers (<strong>range of 2-6 GB/s for both, writes &amp; reads when processing millions of messages</strong>), I eventually decided to give it a long-term shot. Being fully aware that there&#39;s still lots to be done (speaking of many months, or even years), I couldn&#39;t be more happy to find out that there&#39;s also someone else out there, who would like to contribute to the project and become a part of the team.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_bench.jpeg"/></p><h2 id="team">Team</h2><p>At the time of writing this post, <strong>Iggy consists of around 10 members</strong> contributing to its different parts. Some of us do work on the core streaming server, while the other ones are focused on SDKs for the different programming languages or tooling such as Web UI or CLI - all these projects are equally important, as they add up to the overall ecosystem. But how do you actually gather a team of open source contributors, who are willing to spend their free time working on it?</p><p>Well, I wish I had an answer to that question - honestly, in case of Iggy I wasn&#39;t actually looking for anyone, as I didn&#39;t think this could be an interesting project to work on (except for myself). Then how did that happen anyway? There were only 2 things in common - all the people that joined the project were part of the same Discord communities, yet more importantly <strong>they all shared the passion for programming</strong>, and I&#39;m not talking about Rust language specifically. From junior to senior, from embedded to front-end developers - regardless of the years of experience and current occupation, everyone has found a way to implement something meaningful.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_server.jpeg"/></p><p>For example, when I asked one guy what was the reason behind building an SDK in Go, the reply was the need of playing with and learning a new language. Why C# SDK? Well, the other guy wanted to dive more into the low-level concepts and decided to squeeze out great performance from the managed runtime. Why build Web UI in Svelte? At work, I mostly use React, and I wanted to learn a new framework - another member said.</p><p>My point is - as long as you believe in what you&#39;re building, and you&#39;re consistent about it (it was one of the main reasons why I&#39;ve been contributing to Iggy every day since its inception, and still doing so), there&#39;s a chance that someone out there will notice it and happily join you in your efforts. <strong>Lead by example, or whatever you call it.</strong></p><p>At the same time, we&#39;ve started receiving the first, <strong>external contributions from all around the world</strong> - whether talking about simpler tasks, or the more sophisticated ones, requiring significant amount of time being spent on both, the implementation and the discussions to eventually deliver the code.</p><p>It gave us even more confidence, that there are other people (outside our internal bubble), who find this project to be interesting and worth spending their time. And without all these amazing contributors, it&#39;d be much harder (or even impossible) to deliver so many features.</p><h2 id="features">Features</h2><p>At first, let me just point out some of the properties and features that are part of the core streaming server:</p><ul><li><strong>Highly performant</strong>, persistent append-only log for the message streaming</li><li><strong>Very high throughput</strong> for both writes and reads</li><li><strong>Low latency and predictable resource usage</strong> thanks to the Rust compiled language (no GC)</li><li><strong>Users authentication and authorization</strong> with granular permissions and PAT (Personal Access Tokens)</li><li>Support for multiple streams, topics and partitions</li><li>Support for <strong>multiple transport protocols</strong> (QUIC, TCP, HTTP)</li><li>Fully operational RESTful API which can be optionally enabled</li><li>Available client SDK in multiple languages</li><li><strong>Works directly with the binary data</strong> (lack of enforced schema and serialization/deserialization)</li><li>Configurable server features (e.g. caching, segment size, data flush interval, transport protocols etc.)</li><li>Possibility of storing the <strong>consumer offsets</strong> on the server</li><li>Multiple ways of polling the messages: <ul><li>By offset (using the indexes)</li><li>By timestamp (using the time indexes)</li><li>First/Last N messages</li><li>Next N messages for the specific consumer</li></ul></li><li>Possibility of <strong>auto committing the offset</strong> (e.g. to achieve <em>at-most-once</em> delivery)</li><li><strong>Consumer groups</strong> providing the message ordering and horizontal scaling across the connected clients</li><li><strong>Message expiry</strong> with auto deletion based on the configurable <strong>retention policy</strong></li><li>Additional features such as <strong>server side message deduplication</strong></li><li><strong>TLS</strong> support for all transport protocols (TCP, QUIC, HTTPS)</li><li>Optional server-side as well as client-side <strong>data encryption</strong> using AES-256-GCM</li><li>Optional metadata support in the form of <strong>message headers</strong></li><li>Built-in <strong>CLI</strong> to manage the streaming server</li><li>Built-in <strong>benchmarking app</strong> to test the performance</li><li><strong>Single binary deployment</strong> (no external dependencies)</li><li>Running as a single node (no cluster support yet)</li></ul><p>And as already mentioned, we&#39;ve been working on SDKs for the multiple programming languages:</p><ul><li><a href="https://crates.io/crates/iggy">Rust</a></li><li><a href="https://github.com/iggy-rs/iggy-dotnet-client">C#</a></li><li><a href="https://github.com/iggy-rs/iggy-go-client">Go</a></li><li><a href="https://github.com/iggy-rs/iggy-node-client">Node</a></li><li><a href="https://github.com/iggy-rs/iggy-python-client">Python</a></li><li><a href="https://github.com/iggy-rs/iggy-java-client">Java</a></li></ul><p>Please keep in mind, though, that some of them e.g. for Rust or C# are more up to date with the recent server changes, while the other ones might still need to do some catching up with the latest features. However, given the amount of available methods on the server&#39;s API and the underlying TCP/UDP stack with custom serialization to be implemented from the scratch (except for HTTP transport, that&#39;s the easier one), I&#39;d say we&#39;re doing quite ok, and I can&#39;t stress enough <strong>how grateful I am to all the contributors for their huge amount of work</strong>!</p><p>But wait, there&#39;s even more - what would be a message streaming platform without some additional tooling for managing it? We&#39;ve also been developing the <strong><a href="https://github.com/iggy-rs/iggy/tree/master/cmd">CLI</a></strong>.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_cli.jpeg"/></p><p>As well as modern <strong><a href="https://github.com/iggy-rs/iggy-web-ui">Web UI</a></strong> to make it happen :)</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_web_ui.jpeg"/></p><p>Last but not least, we&#39;ve got a fully-featured <a href="https://github.com/iggy-rs/iggy/actions">CI/CD pipeline</a> responsible for running all the checks and tests on multiple platforms, and finally producing the release artifacts and <a href="https://hub.docker.com/u/iggyrs">Docker images</a>.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/iggy_ci_cd.jpeg"/></p><p>At first glance, it might look like there&#39;s plenty of features already in place, but for anyone who has ever worked with the message streaming infrastructure before, that might be just a tip of an iceberg, thus let&#39;s discuss the roadmap.</p><h2 id="roadmap">Roadmap</h2><p>After gaining some traction a few months ago (mostly due to landing on the GitHub trending page in July), we&#39;ve talked to some users potentially interested in making Iggy part of their infrastructure (there&#39;s even one <a href="https://neferdata.com">company</a> using it already), and discussed what features would be a good addition to the current stack.</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/gh_trending.jpeg"/></p><p>Considering what&#39;s already there, being worked on or planned for the future releases, such as interactive CLI, modern Web UI, optional data compression and archivization, plugin support or multiple SDKs, there are at least three additonal challenges to overcome:</p><p><strong>Clustering</strong> - the possibility of having a <strong>highly available and fault tolerant</strong> distributed message streaming platform in a production environment, is typically one of the most important aspects when considering the particular tool. While it wouldn&#39;t be too difficult to implement the extension (think of a simple proxy/load balancer), allowing to monitor and deliver the data either to the primary or secondary replica (treated as a fallback server) and switch between them when one of the nodes goes down, such a solution would still result in SPOF and wouldn&#39;t really scale. Instead, we&#39;ve started experimenting with <a href="https://raft.github.io/">Raft</a> consensus mechanism (de facto the industry standard) in a dedicated <a href="https://github.com/iggy-rs/iggy-cluster-sandbox">repository</a>, which should allow us in delivering the truly fault tolerant, distributed infrastructure with an additional data replication at the partition level (so-called unit of parallelization).</p><p><img alt="image" src="https://blog.iggy.rs/building-message-streaming/leader_elections.jpeg"/></p><p><strong>Low-level I/O</strong> - although the current results (based on the benchmarking tool measuring the throughput etc.) are satisfying, we strongly believe that there&#39;s still (potentially a huge) room for improvement. We&#39;re planning to use <strong><a href="https://unixism.net/loti/what_is_io_uring.html">io_uring</a></strong> for all I/O operations (disk or network related). The brand new, completion based API (available in the recent Linux kernels) shows a significant boost when compared to the existing solutions such as <strong>epoll</strong> or <strong>kqueue</strong> - at the end of the day, the streaming server at its core is all about writing &amp; reading data to/from disk and sending it via the network buffer. We&#39;ve decided to give a try <strong><a href="https://github.com/bytedance/monoio">monoio</a></strong> runtime, as it seems to be the most performant one. Going further, we&#39;d like to incorporate techniques such as <strong>zero-copy</strong>, <strong>kernel bypass</strong> and all the other goodies e.g. from <strong><a href="https://www.dpdk.org/">DPDK</a></strong> or other relevant frameworks.</p><p><strong>Thread-per-core</strong> - in order to avoid the rather costly context switches due to the usage of synchronization mechanism when accessing the data from the different threads (e.g. via Tokio&#39;s work stealing mechanism), we&#39;re planning to explore (or actually, already doing it, in the previously mentioned repository for clustering sandbox) thread-per-core architecture, once again, delivered as part of <strong><a href="https://github.com/bytedance/monoio">monoio</a></strong> runtime. The overall idea can be described in two words - <strong>share nothing</strong> (or as little as possible). For example, the streams could be tied to the particular CPU cores, resulting in <strong>no additional overhead</strong> (via <em>Mutexes</em>, <em>RwLocks</em> etc.) when writing or reading the data. As good as it might sound, there are always some tradeoffs - what if some specific streams are more frequently accessed than the others? Would the remaining cores remain idle instead of doing something useful? On the other hand, tools such as <a href="https://www.scylladb.com/product/technology/shard-per-core-architecture/">ScyllaDB</a> or <a href="https://redpanda.com/blog/tpc-buffers">Redpanda</a> seem to be leveraging this model quite effectively (both are using the same <a href="https://seastar.io/">Seastar</a> framework). We will be looking for the answers, before deciding which approach (thread-per-core or work stealing) suits Iggy better in the future.</p><h2 id="future">Future</h2><p><strong>Why building another message streaming then?</strong> A few months ago, I would probably answer - strictly for fun. Yet, after exploring more in-depth the status quo, what we would like to achieve is sort of twofold - on one hand, it&#39;d be great to deliver the general-purpose tool, such as Kafka. On the other hand, why not to try and really push hard the OS and hardware to its limits when speaking of the performance, reliability, throughput and latency, something what e.g. Aeron does? And what if we could <strong>put this all together</strong> into the easy-to-use, unified platform, supporting the most popular programming languages, with the addition of modern CLI and Web UI for managing it?</p><p>Only the time will tell, but <strong>we&#39;re already excited enough to challenge ourselves</strong>. We&#39;d love to hear your thoughts, ideas and <strong>feedback</strong> - anything that will help us in building the best message streaming platform in Rust that you will enjoy using! Feel free to join our <a href="https://iggy.rs/discord">Discord</a> community and let us know what do you think :)</p><p>P.S.</p><p>This blog uses <a href="https://www.getzola.org/">Rust</a>.</p></section></article></div></div>
  </body>
</html>
