<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://film-net.github.io/">Original</a>
    <h1>Film: Frame Interpolation for Large Motion</h1>
    
    <div id="readability-page-1" class="page">

<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span><sup>1</sup>Google Research    </span>
            <span><sup>2</sup>University of Washington    </span>
          </p>
          
          
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <p>
      <video id="teaser" autoplay="" muted="" loop="" height="100%">
        <source src="static/images/moment.mp4" type="video/mp4"/>
      </video>
      <h2>
        <span>FILM</span> turns near-duplicate photos into a slow motion footage that look like shot with a video camera.
      </h2>
    </p>
  </div>
</section>


<section>
  
</section>


<section>
  <div>

    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            We present a frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion.
            Recent methods use multiple networks to estimate optical flow or depth and a separate network dedicated to frame synthesis. 
            This is often complex and requires scarce optical flow or depth ground-truth. 
            In this work, we present a single unified network, distinguished by a multi-scale feature extractor that shares weights at all scales, and is trainable from frames alone.
            To synthesize crisp and pleasing frames, we propose to optimize our network with the Gram matrix loss that measures the correlation difference between feature maps. 
            Our approach outperforms state-of-the-art methods on the Xiph large motion benchmark. 
            We also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing to methods that use perceptual losses. 
            We study the effect of weight sharing and of training with datasets of increasing motion range. 
            Finally, we demonstrate our model&#39;s effectiveness in synthesizing high quality and temporally coherent videos on a challenging near-duplicate photos dataset.
          </p>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->

<section>
  <div>
    <div style="margin-bottom:120px;">
      <h2>Loss Functions Ablation</h2>
      <p><img src="https://film-net.github.io/static/images/loss_ablation.png" alt="overview_image"/>
    </p></div> 
  </div>
</section>

<section>
  <div>
    <div style="margin-bottom:120px;">
      <h2><span>FILM</span> Architecture Overview</h2>
      <p><img src="https://film-net.github.io/static/images/FILM_architecture_slim.png" alt="overview_image"/>
    </p></div> 
  </div>
</section>

<section>
  <div>
  <!-- Paper video. -->
    <div>
      <div>
        <h2>Video</h2>
        <p>
          <iframe src="https://www.youtube.com/embed/OAD-BieIjH4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        </p>
      </div>
    </div>
  </div>
</section>


<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>
@misc{reda2022film,
  title={FILM: Frame Interpolation for Large Motion},
  author={Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},
  booktitle = {The European Conference on Computer Vision (ECCV)},
  year={2022}
}
</code></pre>
  </div>
</section>
    





</section></div>
  </body>
</html>
