<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/google-research/frame-interpolation">Original</a>
    <h1>FILM: Frame Interpolation for Large Motion</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h3 dir="auto"><a id="user-content-website--paper--google-ai-blog--tensorflow-hub-colab--youtube-" aria-hidden="true" href="#website--paper--google-ai-blog--tensorflow-hub-colab--youtube-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://film-net.github.io/" rel="nofollow">Website</a> | <a href="https://arxiv.org/pdf/2202.04901.pdf" rel="nofollow">Paper</a> | <a href="https://ai.googleblog.com/2022/10/large-motion-frame-interpolation.html" rel="nofollow">Google AI Blog</a> | <a href="https://www.tensorflow.org/hub/tutorials/tf_hub_film_example" rel="nofollow">Tensorflow Hub Colab</a> | <a href="https://www.youtube.com/watch?v=OAD-BieIjH4" rel="nofollow">YouTube</a> <br/></h3>
<p dir="auto">The official Tensorflow 2 implementation of our high quality frame interpolation neural network. We present a unified single-network approach that doesn&#39;t use additional pre-trained networks, like optical flow or depth, and yet achieve state-of-the-art results. We use a multi-scale feature extractor that shares the same convolution weights across the scales. Our model is trainable from frame triplets alone. <br/></p>
<p dir="auto"><a href="https://arxiv.org/abs/2202.04901" rel="nofollow">FILM: Frame Interpolation for Large Motion</a> </p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/googlestaging/frame-interpolation/blob/main/moment.gif"><img src="https://github.com/googlestaging/frame-interpolation/raw/main/moment.gif" alt="A sample 2 seconds moment." data-animated-image=""/></a>
FILM transforms near-duplicate photos into a slow motion footage that look like it is shot with a video camera.</p>
<h2 dir="auto"><a id="user-content-web-demo" aria-hidden="true" href="#web-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Web Demo</h2>
<p dir="auto">Integrated into <a href="https://huggingface.co/spaces" rel="nofollow">Huggingface Spaces <g-emoji alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ðŸ¤—</g-emoji></a> using <a href="https://github.com/gradio-app/gradio">Gradio</a>. Try out the Web Demo: <a href="https://huggingface.co/spaces/johngoad/frame-interpolation" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<p dir="auto">Try the interpolation model with the replicate web demo at
<a href="https://replicate.com/google-research/frame-interpolation" rel="nofollow"><img src="https://camo.githubusercontent.com/6f7cbd6b796dcaf2e032815c2e0072739e71cf05aa31cf6ec021dc64ad4ee222/68747470733a2f2f7265706c69636174652e636f6d2f676f6f676c652d72657365617263682f6672616d652d696e746572706f6c6174696f6e2f6261646765" alt="Replicate" data-canonical-src="https://replicate.com/google-research/frame-interpolation/badge"/></a></p>
<p dir="auto">Try FILM to interpolate between two or more images with the PyTTI-Tools at <a href="https://colab.sandbox.google.com/github/pytti-tools/frame-interpolation/blob/main/PyTTI_Tools_FiLM-colab.ipynb#scrollTo=-7TD7YZJbsy_" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="PyTTI-Tools:FILM" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p dir="auto">An alternative Colab for running FILM on arbitrarily more images, rather than two, <a href="https://colab.research.google.com/drive/1NuaPPSvUhYafymUf2mEkvhnEtpD5oihs" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="FILM-Gdrive" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<h2 dir="auto"><a id="user-content-change-log" aria-hidden="true" href="#change-log"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Change Log</h2>
<ul dir="auto">
<li><strong>Nov 28, 2022</strong>: Upgrade <code>eval.interpolator_cli</code> for <strong>high resolution frame interpolation</strong>. <code>--block_height</code> and <code>--block_width</code> determine the total number of patches (<code>block_height*block_width</code>) to subdivide the input images. By default, both arguments are set to 1, and so no subdivision will be done.</li>
<li><strong>Mar 12, 2022</strong>: Support for Windows, see <a href="https://github.com/google-research/frame-interpolation/blob/main/WINDOWS_INSTALLATION.md">WINDOWS_INSTALLATION.md</a>.</li>
<li><strong>Mar 09, 2022</strong>: Support for <strong>high resolution frame interpolation</strong>. Set <code>--block_height</code> and <code>--block_width</code> in <code>eval.interpolator_test</code> to extract patches from the inputs, and reconstruct the interpolated frame from the iteratively interpolated patches.</li>
</ul>
<h2 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h2>
<ul dir="auto">
<li>Get Frame Interpolation source codes</li>
</ul>
<div data-snippet-clipboard-copy-content="git clone https://github.com/google-research/frame-interpolation
cd frame-interpolation"><pre><code>git clone https://github.com/google-research/frame-interpolation
cd frame-interpolation
</code></pre></div>
<ul dir="auto">
<li>Optionally, pull the recommended Docker base image</li>
</ul>
<div data-snippet-clipboard-copy-content="docker pull gcr.io/deeplearning-platform-release/tf2-gpu.2-6:latest"><pre><code>docker pull gcr.io/deeplearning-platform-release/tf2-gpu.2-6:latest
</code></pre></div>
<ul dir="auto">
<li>
<p dir="auto">If you do not use Docker, set up your NVIDIA GPU environment with:</p>
<ul dir="auto">
<li><a href="https://www.anaconda.com/products/individual" rel="nofollow">Anaconda Python 3.9</a></li>
<li><a href="https://developer.nvidia.com/cuda-11.2.1-download-archive" rel="nofollow">CUDA Toolkit 11.2.1</a></li>
<li><a href="https://developer.nvidia.com/rdp/cudnn-download" rel="nofollow">cuDNN 8.1.0</a></li>
</ul>
</li>
<li>
<p dir="auto">Install frame interpolation dependencies</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="pip3 install -r requirements.txt
sudo apt-get install -y ffmpeg"><pre><code>pip3 install -r requirements.txt
sudo apt-get install -y ffmpeg
</code></pre></div>
<h3 dir="auto"><a id="user-content-see-windows_installation-for-windows-support" aria-hidden="true" href="#see-windows_installation-for-windows-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>See <a href="https://github.com/google-research/frame-interpolation/blob/main/WINDOWS_INSTALLATION.md">WINDOWS_INSTALLATION</a> for Windows Support</h3>
<h2 dir="auto"><a id="user-content-pre-trained-models" aria-hidden="true" href="#pre-trained-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Pre-trained Models</h2>
<ul dir="auto">
<li>Create a directory where you can keep large files. Ideally, not in this
directory.</li>
</ul>
<div data-snippet-clipboard-copy-content="mkdir -p &lt;pretrained_models&gt;"><pre><code>mkdir -p &lt;pretrained_models&gt;
</code></pre></div>
<ul dir="auto">
<li>Download pre-trained TF2 Saved Models from
<a href="https://drive.google.com/drive/folders/1q8110-qp225asX3DQvZnfLfJPkCHmDpy?usp=sharing" rel="nofollow">google drive</a>
and put into <code>&lt;pretrained_models&gt;</code>.</li>
</ul>
<p dir="auto">The downloaded folder should have the following structure:</p>
<div data-snippet-clipboard-copy-content="&lt;pretrained_models&gt;/
â”œâ”€â”€ film_net/
â”‚   â”œâ”€â”€ L1/
â”‚   â”œâ”€â”€ Style/
â”‚   â”œâ”€â”€ VGG/
â”œâ”€â”€ vgg/
â”‚   â”œâ”€â”€ imagenet-vgg-verydeep-19.mat"><pre><code>&lt;pretrained_models&gt;/
â”œâ”€â”€ film_net/
â”‚   â”œâ”€â”€ L1/
â”‚   â”œâ”€â”€ Style/
â”‚   â”œâ”€â”€ VGG/
â”œâ”€â”€ vgg/
â”‚   â”œâ”€â”€ imagenet-vgg-verydeep-19.mat
</code></pre></div>
<h2 dir="auto"><a id="user-content-running-the-codes" aria-hidden="true" href="#running-the-codes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Running the Codes</h2>
<p dir="auto">The following instructions run the interpolator on the photos provided in
&#39;frame-interpolation/photos&#39;.</p>
<h3 dir="auto"><a id="user-content-one-mid-frame-interpolation" aria-hidden="true" href="#one-mid-frame-interpolation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>One mid-frame interpolation</h3>
<p dir="auto">To generate an intermediate photo from the input near-duplicate photos, simply run:</p>
<div data-snippet-clipboard-copy-content="python3 -m eval.interpolator_test \
   --frame1 photos/one.png \
   --frame2 photos/two.png \
   --model_path &lt;pretrained_models&gt;/film_net/Style/saved_model \
   --output_frame photos/output_middle.png"><pre><code>python3 -m eval.interpolator_test \
   --frame1 photos/one.png \
   --frame2 photos/two.png \
   --model_path &lt;pretrained_models&gt;/film_net/Style/saved_model \
   --output_frame photos/output_middle.png
</code></pre></div>
<p dir="auto">This will produce the sub-frame at <code>t=0.5</code> and save as &#39;photos/output_middle.png&#39;.</p>
<h3 dir="auto"><a id="user-content-many-in-between-frames-interpolation" aria-hidden="true" href="#many-in-between-frames-interpolation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Many in-between frames interpolation</h3>
<p dir="auto">It takes in a set of directories identified by a glob (--pattern). Each directory
is expected to contain at least two input frames, with each contiguous frame
pair treated as an input to generate in-between frames. Frames should be named such that when sorted (naturally) with <code>natsort</code>, their desired order is unchanged.</p>
<div data-snippet-clipboard-copy-content="python3 -m eval.interpolator_cli \
   --pattern &#34;photos&#34; \
   --model_path &lt;pretrained_models&gt;/film_net/Style/saved_model \
   --times_to_interpolate 6 \
   --output_video"><pre><code>python3 -m eval.interpolator_cli \
   --pattern &#34;photos&#34; \
   --model_path &lt;pretrained_models&gt;/film_net/Style/saved_model \
   --times_to_interpolate 6 \
   --output_video
</code></pre></div>
<p dir="auto">You will find the interpolated frames (including the input frames) in
&#39;photos/interpolated_frames/&#39;, and the interpolated video at
&#39;photos/interpolated.mp4&#39;.</p>
<p dir="auto">The number of frames is determined by <code>--times_to_interpolate</code>, which controls
the number of times the frame interpolator is invoked. When the number of frames
in a directory is <code>num_frames</code>, the number of output frames will be
<code>(2^times_to_interpolate+1)*(num_frames-1)</code>.</p>
<h2 dir="auto"><a id="user-content-datasets" aria-hidden="true" href="#datasets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Datasets</h2>
<p dir="auto">We use <a href="http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip" rel="nofollow">Vimeo-90K</a> as
our main training dataset. For quantitative evaluations, we rely on commonly
used benchmark datasets, specifically:</p>
<ul dir="auto">
<li><a href="http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip" rel="nofollow">Vimeo-90K</a></li>
<li><a href="https://vision.middlebury.edu/flow/data" rel="nofollow">Middlebury-Other</a></li>
<li><a href="https://people.cs.umass.edu/~hzjiang/projects/superslomo/UCF101_results.zip" rel="nofollow">UCF101</a></li>
<li><a href="https://github.com/sniklaus/softmax-splatting/blob/master/benchmark.py">Xiph</a></li>
</ul>
<h3 dir="auto"><a id="user-content-creating-a-tfrecord" aria-hidden="true" href="#creating-a-tfrecord"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Creating a TFRecord</h3>
<p dir="auto">The training and benchmark evaluation scripts expect the frame triplets in the
<a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="nofollow">TFRecord</a> storage format. <br/></p>
<p dir="auto">We have included scripts that encode the relevant frame triplets into a
<a href="https://www.tensorflow.org/api_docs/python/tf/train/Example" rel="nofollow">tf.train.Example</a>
data format, and export to a TFRecord file. <br/></p>
<p dir="auto">You can use the commands <code>python3 -m datasets.create_&lt;dataset_name&gt;_tfrecord --help</code> for more information.</p>
<p dir="auto">For example, run the command below to create a TFRecord for the Middlebury-other
dataset. Download the <a href="https://vision.middlebury.edu/flow/data" rel="nofollow">images</a> and point <code>--input_dir</code> to the unzipped folder path.</p>
<div data-snippet-clipboard-copy-content="python3 -m datasets.create_middlebury_tfrecord \
  --input_dir=&lt;root folder of middlebury-other&gt; \
  --output_tfrecord_filepath=&lt;output tfrecord filepath&gt; \
  --num_shards=3"><pre><code>python3 -m datasets.create_middlebury_tfrecord \
  --input_dir=&lt;root folder of middlebury-other&gt; \
  --output_tfrecord_filepath=&lt;output tfrecord filepath&gt; \
  --num_shards=3
</code></pre></div>
<p dir="auto">The above command will output a TFRecord file with 3 shards as <code>&lt;output tfrecord filepath&gt;@3</code>.</p>
<h2 dir="auto"><a id="user-content-training" aria-hidden="true" href="#training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Training</h2>
<p dir="auto">Below are our training gin configuration files for the different loss function:</p>
<div data-snippet-clipboard-copy-content="training/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ film_net-L1.gin
â”‚   â”œâ”€â”€ film_net-VGG.gin
â”‚   â”œâ”€â”€ film_net-Style.gin"><pre><code>training/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ film_net-L1.gin
â”‚   â”œâ”€â”€ film_net-VGG.gin
â”‚   â”œâ”€â”€ film_net-Style.gin
</code></pre></div>
<p dir="auto">To launch a training, simply pass the configuration filepath to the desired
experiment. </p>
<div data-snippet-clipboard-copy-content="python3 -m training.train \
   --gin_config training/config/&lt;config filename&gt;.gin \
   --base_folder &lt;base folder for all training runs&gt; \
   --label &lt;descriptive label for the run&gt;"><pre><code>python3 -m training.train \
   --gin_config training/config/&lt;config filename&gt;.gin \
   --base_folder &lt;base folder for all training runs&gt; \
   --label &lt;descriptive label for the run&gt;
</code></pre></div>
<ul dir="auto">
<li>When training finishes, the folder structure will look like this:</li>
</ul>
<div data-snippet-clipboard-copy-content="&lt;base_folder&gt;/
â”œâ”€â”€ &lt;label&gt;/
â”‚   â”œâ”€â”€ config.gin
â”‚   â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ saved_model/"><pre><code>&lt;base_folder&gt;/
â”œâ”€â”€ &lt;label&gt;/
â”‚   â”œâ”€â”€ config.gin
â”‚   â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ saved_model/
</code></pre></div>
<h3 dir="auto"><a id="user-content-build-a-savedmodel" aria-hidden="true" href="#build-a-savedmodel"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Build a SavedModel</h3>
<p dir="auto">Optionally, to build a
<a href="https://www.tensorflow.org/guide/saved_model" rel="nofollow">SavedModel</a> format from a trained
checkpoints folder, you can use this command:</p>
<div data-snippet-clipboard-copy-content="python3 -m training.build_saved_model_cli \
   --base_folder &lt;base folder of training sessions&gt; \
   --label &lt;the name of the run&gt;"><pre><code>python3 -m training.build_saved_model_cli \
   --base_folder &lt;base folder of training sessions&gt; \
   --label &lt;the name of the run&gt;
</code></pre></div>
<ul dir="auto">
<li>By default, a SavedModel is created when the training loop ends, and it will be saved at
<code>&lt;base_folder&gt;/&lt;label&gt;/&lt;saved_model&gt;</code>.</li>
</ul>
<h2 dir="auto"><a id="user-content-evaluation-on-benchmarks" aria-hidden="true" href="#evaluation-on-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluation on Benchmarks</h2>
<p dir="auto">Below, we provided the evaluation gin configuration files for the benchmarks we
have considered:</p>
<div data-snippet-clipboard-copy-content="eval/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ middlebury.gin
â”‚   â”œâ”€â”€ ucf101.gin
â”‚   â”œâ”€â”€ vimeo_90K.gin
â”‚   â”œâ”€â”€ xiph_2K.gin
â”‚   â”œâ”€â”€ xiph_4K.gin"><pre><code>eval/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ middlebury.gin
â”‚   â”œâ”€â”€ ucf101.gin
â”‚   â”œâ”€â”€ vimeo_90K.gin
â”‚   â”œâ”€â”€ xiph_2K.gin
â”‚   â”œâ”€â”€ xiph_4K.gin
</code></pre></div>
<p dir="auto">To run an evaluation, simply pass the configuration file of the desired evaluation dataset. </p>
<div data-snippet-clipboard-copy-content="python3 -m eval.eval_cli \
   --gin_config eval/config/&lt;eval_dataset&gt;.gin \
   --model_path &lt;pretrained_models&gt;/film_net/L1/saved_model"><pre><code>python3 -m eval.eval_cli \
   --gin_config eval/config/&lt;eval_dataset&gt;.gin \
   --model_path &lt;pretrained_models&gt;/film_net/L1/saved_model
</code></pre></div>
<p dir="auto">The above command will produce the PSNR and SSIM scores presented in the paper.</p>
<h2 dir="auto"><a id="user-content-citation" aria-hidden="true" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Citation</h2>
<p dir="auto">If you find this implementation useful in your works, please acknowledge it
appropriately by citing:</p>
<div data-snippet-clipboard-copy-content="@inproceedings{reda2022film,
 title = {FILM: Frame Interpolation for Large Motion},
 author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},
 booktitle = {European Conference on Computer Vision (ECCV)},
 year = {2022}
}"><pre><code>@inproceedings{reda2022film,
 title = {FILM: Frame Interpolation for Large Motion},
 author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},
 booktitle = {European Conference on Computer Vision (ECCV)},
 year = {2022}
}
</code></pre></div>
<div data-snippet-clipboard-copy-content="@misc{film-tf,
  title = {Tensorflow 2 Implementation of &#34;FILM: Frame Interpolation for Large Motion&#34;},
  author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/google-research/frame-interpolation}}
}"><pre><code>@misc{film-tf,
  title = {Tensorflow 2 Implementation of &#34;FILM: Frame Interpolation for Large Motion&#34;},
  author = {Fitsum Reda and Janne Kontkanen and Eric Tabellion and Deqing Sun and Caroline Pantofaru and Brian Curless},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/google-research/frame-interpolation}}
}
</code></pre></div>
<p dir="auto">Contact: Fitsum Reda (<a href="mailto:fitsum@google.com">fitsum@google.com</a>)</p>
<h2 dir="auto"><a id="user-content-acknowledgments" aria-hidden="true" href="#acknowledgments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgments</h2>
<p dir="auto">We would like to thank Richard Tucker, Jason Lai and David Minnen. We would also
like to thank Jamie Aspinall for the imagery included in this repository.</p>
<h2 dir="auto"><a id="user-content-coding-style" aria-hidden="true" href="#coding-style"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Coding style</h2>
<ul dir="auto">
<li>2 spaces for indentation</li>
<li>80 character line length</li>
<li>PEP8 formatting</li>
</ul>
<h2 dir="auto"><a id="user-content-disclaimer" aria-hidden="true" href="#disclaimer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Disclaimer</h2>
<p dir="auto">This is not an officially supported Google product.</p>
</article>
          </div></div>
  </body>
</html>
