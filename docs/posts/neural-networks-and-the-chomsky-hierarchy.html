<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2207.02098">Original</a>
    <h1>Neural Networks and the Chomsky Hierarchy</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Del%C3%A9tang%2C+G">Grégoire Delétang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ruoss%2C+A">Anian Ruoss</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grau-Moya%2C+J">Jordi Grau-Moya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Genewein%2C+T">Tim Genewein</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wenliang%2C+L+K">Li Kevin Wenliang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Catt%2C+E">Elliot Catt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cundy%2C+C">Chris Cundy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter%2C+M">Marcus Hutter</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Legg%2C+S">Shane Legg</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Veness%2C+J">Joel Veness</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ortega%2C+P+A">Pedro A. Ortega</a></p></div>
      
    
  
    <p><a href="https://arxiv.org/pdf/2207.02098">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Reliable generalization lies at the heart of safe ML and AI. However,
understanding when and how neural networks generalize remains one of the most
important unsolved problems in the field. In this work, we conduct an extensive
empirical study (10250 models, 15 tasks) to investigate whether insights from
the theory of computation can predict the limits of neural network
generalization in practice. We demonstrate that grouping tasks according to the
Chomsky hierarchy allows us to forecast whether certain architectures will be
able to generalize to out-of-distribution inputs. This includes negative
results where even extensive amounts of data and training time never lead to
any non-trivial generalization, despite models having sufficient capacity to
fit the training data perfectly. Our results show that, for our subset of
tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can
solve regular and counter-language tasks, and only networks augmented with
structured memory (such as a stack or memory tape) can successfully generalize
on context-free and context-sensitive tasks.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Anian Ruoss [<a href="https://arxiv.org/show-email/2bb7664e/2207.02098">view email</a>]
      </p></div></div>
  </body>
</html>
