<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/anordin95/run-llama-locally">Original</a>
    <h1>Run Llama locally with only PyTorch on CPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Running Llama locally with minimal dependencies</h2><a id="user-content-running-llama-locally-with-minimal-dependencies" aria-label="Permalink: Running Llama locally with minimal dependencies" href="#running-llama-locally-with-minimal-dependencies"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">I want to peel back the layers of the onion and other gluey-mess to gain insight into these models.</p>
<p dir="auto">There are other popular (and likely more performant) ways to invoke these models, such as <a href="https://ollama.com/" rel="nofollow">Ollama</a>, <a href="https://github.com/pytorch/torchchat">torchchat</a>, <a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a> and Hugging-Face&#39;s general API package: <a href="https://pypi.org/project/transformers/" rel="nofollow">transformers</a>. But those hide the interesting details -- things like the activations in each layer, the weights and their shapes, the output probability distribution, etc. -- behind an API.
I was a bit surprised Meta didn&#39;t publish an example way to simply invoke one of these LLM&#39;s with only <code>torch</code> (or some minimal set of dependencies), though I am obviously grateful and so pleased with their contribution of the public weights!</p>

<ol dir="auto">
<li>
<p dir="auto">Download the relevant model weight(s) via <a href="https://www.llama.com/llama-downloads/" rel="nofollow">https://www.llama.com/llama-downloads/</a></p>
</li>
<li>
<p dir="auto"><code>$ pip install -r requirements.txt</code></p>
</li>
<li>
<p dir="auto"><code>$ cd llama-models; pip install -e .; cd ..</code></p>
</li>
<li>
<p dir="auto"><code>$ python minimal_run_inference.py</code> or <code>$ python run_inference.py</code></p>
</li>
</ol>
<div dir="auto"><h3 tabindex="-1" dir="auto">Exploring the model &amp; outputs</h3><a id="user-content-exploring-the-model--outputs" aria-label="Permalink: Exploring the model &amp; outputs" href="#exploring-the-model--outputs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><code>run_inference.py</code> is more bloated than <code>minimal_run_inference.py</code>. It implements beam-search &amp; features far more explanatory comments.</p>
<p dir="auto"><code>minimal_run_inference.py</code> is a simple, few lines of code way to run the Llama models. It&#39;s a great place to start hacking around or exploring on your own. If one of the steps in it doesn&#39;t make sense, peek over at <code>run_inference.py</code> where there are likely detailed comments.</p>

<p dir="auto">The global variables in the <code>run_inference.py</code> scripts: <code>MODEL_NAME</code>, <code>LLAMA_MODELS_DIR</code>, <code>INPUT_STRING</code> and <code>DEVICE</code> take the values you&#39;d expect (there are adjacent comments with examples and more details too). They should be modified as you see fit.</p>


<p dir="auto">The minimal set of dependencies I found includes <code>torch</code> (perhaps, obviously), a lesser known library also published by Meta: <code>fairscale</code>, which implements a variety of highly scalable/parallelizable analogues of <code>torch</code> operators and <code>blobfile</code>, which implements a general file I/O mechanism that Meta&#39;s Tokenizer implementation uses.</p>
<p dir="auto">Meta provides the language-model weights in a simple way, but a model-architecture to drop them into is still needed. This is provided, in a less obvious way, in the <a href="https://github.com/meta-llama/llama-models">llama_models</a> repo. The model-architecture class therein relies on both <code>torch</code> and <code>fairscale</code> and expects each, specifically <code>torch.distributed</code> and <code>fairscale</code>, to be initialized appropriately. The use of CUDA is hard-coded in a few places in the official repo. I changed that and bundled that version here (as a git submodule).</p>
<p dir="auto">With those initializations squared away, the model-architecture class can be instantiated. Though, that model is largely a blank slate until we then drop the weights in.</p>
<p dir="auto">The tokenizer is similarly available in <a href="https://github.com/meta-llama/llama-models">llama_models</a> and relies on a dictionary-like file distributed along with the model-weights. I&#39;m not sure why, but that file&#39;s strings (which map to unique integers or indices) are base64 encoded. Technically, you don&#39;t need to know that to use the Tokenizer, but if you&#39;re curious to see the actual tokens the system uses, make sure to decode appropriately!</p>

<p dir="auto">I believe most systems use beam-search rather than greedily taking the most-likely token at each time-step, so I implemented the same. Beam search takes the k (say 5) most likely
tokens at the first time-step and uses them as a seed for k distinct sequences. For all future time-steps, only the most likely token is appended to the sequence. At the end, the overall most likely sequence is selected.</p>

<p dir="auto">Using CPU, I can pretty comfortably run the 1B model on my Mac M1 Air that has 16GB of RAM averaging about 1 token per second. The 3B model struggles and gets about 1 token every 60 seconds. And the 8B model typically gets killed by the OS for using too much memory.</p>
<p dir="auto">Initially, using <code>mps</code> (Metal Performance Shaders), i.e. Apple&#39;s GPU, would produce all <code>nan</code>&#39;s as model output. The issue is due to a known-bug in <code>torch.triu</code> which I implemented a workaround for in the <code>llama-models</code> git submoudle.</p>
<p dir="auto">With <code>mps</code>, the inference time of the first few tokens on the 1B model is notably faster, but the memory usage is much higher. It&#39;s not entirely clear to me why the memory usage differs so notably, particularly given Apple&#39;s unified memory layout (i.e. cpu &amp; gpu share memory). Once the sequence is about 100 or 200 tokens, the throughput slows down notably -- about half of the cpu&#39;s throughput. I suspect that the relatively higher memory-load of the GPU (caused for unknown reasons) in conjunction with a growing sequence length starts to swamp my system&#39;s available memory to a degree which effects the computation speed.</p>
<p dir="auto">Aside on GPU memory: I&#39;m using a batch-size of 1, so there&#39;s no batch parallelism (i.e. presumably multiple full models in memory). And, the memory of each transformer layer should be relatively constant, unless perhaps each attention-heads&#39; parameters are loaded into memory then discarded, whereas in the parallel (i.e. gpu) case all heads are simultaneously loaded AND that difference is enough to cause a notable change in memory-load. If you know why, drop a note!</p>
</article></div></div>
  </body>
</html>
