<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/libriscv/multi_tenant_drogon">Original</a>
    <h1>Show HN: Ephemeral VMs in 1 Microsecond</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">Multi-tenancy allows one server to be safely shared among many users, each of which cannot access each others or negatively affect the HTTP service.</p>

<p dir="auto">Specialized sandboxes are instantiated for each request and immediately destroyed after the request, all within a single microsecond.</p>
<ul>
<li> Ephemeral sandboxes gives hard guarantee that no request leaves traces in another</li>
<li> Hot-reloading of tenant programs avoiding service restart</li>
<li> Enforced memory- and CPU- limits</li>
</ul>
<p dir="auto">Note: This project is written like a production system, but contains only the necessary parts for realistic benchmarking. A real production system would have implemented a lot of observability, logging, metering etc.</p>

<p dir="auto">Sandboxed &#39;Hello World&#39; responses with 8, 32 and 64 threads.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./wrk -c8 -t8 http://127.0.0.1:8080/z --latency
Running 10s test @ http://127.0.0.1:8080/z
  8 threads and 8 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.51us    3.08us   0.95ms   94.16%
    Req/Sec   100.91k    11.85k  119.43k    80.20%
  Latency Distribution
     50%    9.00us
     75%   10.00us
     90%   11.00us
     99%   17.00us
  8112849 requests in 10.10s, 1.62GB read
Requests/sec: 803251.42
Transfer/sec:    163.93MB

$ ./wrk -c16 -t16 http://127.0.0.1:8080/z --latency
Running 10s test @ http://127.0.0.1:8080/z
  16 threads and 16 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    13.66us    4.12us 530.00us   70.01%
    Req/Sec    70.82k    13.70k   96.97k    52.54%
  Latency Distribution
     50%   12.00us
     75%   16.00us
     90%   19.00us
     99%   23.00us
  11388772 requests in 10.10s, 2.27GB read
Requests/sec: 1127617.15
Transfer/sec:    230.13MB

$ ./wrk -c32 -t32 http://127.0.0.1:8080/z --latency
Running 10s test @ http://127.0.0.1:8080/z
  32 threads and 32 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    22.49us   20.93us   3.92ms   95.44%
    Req/Sec    45.34k    12.58k   87.98k    59.65%
  Latency Distribution
     50%   19.00us
     75%   26.00us
     90%   34.00us
     99%   59.00us
  14581049 requests in 10.10s, 2.91GB read
Requests/sec: 1443679.94
Transfer/sec:    294.64MB

$ ./wrk -c64 -t64 http://127.0.0.1:8080/z --latency
Running 10s test @ http://127.0.0.1:8080/z
  64 threads and 64 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    39.44us   36.99us   4.12ms   93.77%
    Req/Sec    27.27k     8.29k   67.72k    68.41%
  Latency Distribution
     50%   32.00us
     75%   47.00us
     90%   66.00us
     99%  156.00us
  17540692 requests in 10.10s, 3.50GB read
Requests/sec: 1736727.97
Transfer/sec:    354.44MB"><pre>$ ./wrk -c8 -t8 http://127.0.0.1:8080/z --latency
Running 10s <span>test</span> @ http://127.0.0.1:8080/z
  8 threads and 8 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.51us    3.08us   0.95ms   94.16%
    Req/Sec   100.91k    11.85k  119.43k    80.20%
  Latency Distribution
     50%    9.00us
     75%   10.00us
     90%   11.00us
     99%   17.00us
  8112849 requests <span>in</span> 10.10s, 1.62GB <span>read</span>
Requests/sec: 803251.42
Transfer/sec:    163.93MB

$ ./wrk -c16 -t16 http://127.0.0.1:8080/z --latency
Running 10s <span>test</span> @ http://127.0.0.1:8080/z
  16 threads and 16 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    13.66us    4.12us 530.00us   70.01%
    Req/Sec    70.82k    13.70k   96.97k    52.54%
  Latency Distribution
     50%   12.00us
     75%   16.00us
     90%   19.00us
     99%   23.00us
  11388772 requests <span>in</span> 10.10s, 2.27GB <span>read</span>
Requests/sec: 1127617.15
Transfer/sec:    230.13MB

$ ./wrk -c32 -t32 http://127.0.0.1:8080/z --latency
Running 10s <span>test</span> @ http://127.0.0.1:8080/z
  32 threads and 32 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    22.49us   20.93us   3.92ms   95.44%
    Req/Sec    45.34k    12.58k   87.98k    59.65%
  Latency Distribution
     50%   19.00us
     75%   26.00us
     90%   34.00us
     99%   59.00us
  14581049 requests <span>in</span> 10.10s, 2.91GB <span>read</span>
Requests/sec: 1443679.94
Transfer/sec:    294.64MB

$ ./wrk -c64 -t64 http://127.0.0.1:8080/z --latency
Running 10s <span>test</span> @ http://127.0.0.1:8080/z
  64 threads and 64 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    39.44us   36.99us   4.12ms   93.77%
    Req/Sec    27.27k     8.29k   67.72k    68.41%
  Latency Distribution
     50%   32.00us
     75%   47.00us
     90%   66.00us
     99%  156.00us
  17540692 requests <span>in</span> 10.10s, 3.50GB <span>read</span>
Requests/sec: 1736727.97
Transfer/sec:    354.44MB</pre></div>
<p dir="auto">With 64 threads, the sandboxes handle 1.7M req/s at an average of 39 micros/req.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Drogon vanilla benchmarks</h2><a id="user-content-drogon-vanilla-benchmarks" aria-label="Permalink: Drogon vanilla benchmarks" href="#drogon-vanilla-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">A simple Drogon hello world HTTP response, with no sandboxes involved:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./wrk -c8 -t8 http://127.0.0.1:8080/ --latency
Running 10s test @ http://127.0.0.1:8080/
  8 threads and 8 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     8.54us    3.37us 534.00us   83.19%
    Req/Sec   111.76k    27.06k  146.04k    65.97%
  Latency Distribution
     50%    7.00us
     75%    9.00us
     90%   14.00us
     99%   17.00us
  8985045 requests in 10.10s, 1.79GB read
Requests/sec: 889613.44
Transfer/sec:    181.56MB"><pre>$ ./wrk -c8 -t8 http://127.0.0.1:8080/ --latency
Running 10s <span>test</span> @ http://127.0.0.1:8080/
  8 threads and 8 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     8.54us    3.37us 534.00us   83.19%
    Req/Sec   111.76k    27.06k  146.04k    65.97%
  Latency Distribution
     50%    7.00us
     75%    9.00us
     90%   14.00us
     99%   17.00us
  8985045 requests <span>in</span> 10.10s, 1.79GB <span>read</span>
Requests/sec: 889613.44
Transfer/sec:    181.56MB</pre></div>
<p dir="auto">A vanilla Drogon response took 8.5 micros, while the sandboxed request required 9.5 micros. We can say that the total overhead of a fully integrated multi-tenancy solution is ~1 microsecond at 800k req/s. At 99% latency percentile, they both required 17 microseconds, however Drogon executed 10% more requests in the same amount of time.</p>

<p dir="auto">The test program is a simple Pythran to C++ transpilation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="def on_init():
	print(&#34;Hello from Python&#34;)

page = &#34;&#34;&#34;	this is a very
	long string if I had the
	energy to type more and more ...
&#34;&#34;&#34;

def on_client_request():
	def generator():
		return &#34;text/plain&#34;, page
	return generator"><pre><span>def</span> <span>on_init</span>():
	<span>print</span>(<span>&#34;Hello from Python&#34;</span>)

<span>page</span> <span>=</span> <span>&#34;&#34;&#34;	this is a very</span>
<span>	long string if I had the</span>
<span>	energy to type more and more ...</span>
<span>&#34;&#34;&#34;</span>

<span>def</span> <span>on_client_request</span>():
	<span>def</span> <span>generator</span>():
		<span>return</span> <span>&#34;text/plain&#34;</span>, <span>page</span>
	<span>return</span> <span>generator</span></pre></div>
</article></div></div>
  </body>
</html>
