<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dangeng.github.io/visual_anagrams/">Original</a>
    <h1>Visual Anagrams: Generating optical illusions with diffusion models</h1>
    
    <div id="readability-page-1" class="page">


<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span>University of Michigan</span>
          </p>

          <p><span>Correspondence to: dgeng@umich.edu</span>
          </p>

          
        </div>
      </div>
    </div>
  </div>
</section>


<!-- TEASER + INTRO -->
<section>
  <h2>
    <b>tl;dr:</b> We use pretrained diffusion models</h2>
  
</section>


<!-- OVERVIEW -->
<section>
  
</section>


<!-- JIGSAW GRID -->
<section>
  
</section>

<!-- FLIPS GRID -->
<section>
  <div>
    <p>
      <h2 id="flip_examples">Flips and 180° Rotations</h2>
    </p>
    
    
    
    
  </div>
</section>

<!-- 90 DEGREE GRID -->
<section>
  
</section>

<!-- INVERSIONS GRID -->
<section>
  
</section>

<!-- MISC GRID -->
<section>
  <div>
    <p>
      <h2 id="misc_examples">Miscellaneous Permutations</h2>
    </p>
    
    
  </div>
</section>

<!-- PATCH GRID -->
<section>
  <div>
    <p>
      <h2 id="perm_examples">Random Patch Permutations</h2>
    </p>
      
      
  </div>
</section>

<!-- METHOD -->
<section>
  <div>
    <div>
      <div>
        <h2 id="method">Method</h2>
        <div>
          <p><img src="https://dangeng.github.io/visual_anagrams/static/images/method.jpg"/></p><p>
            Our method is conceptually simple. We take an off-the-shelf diffusion model and use it
            to estimate the noise in different views or transformations, \(v_i\), of an image. 
            The noise estimates are then aligned by applying the inverse view, \(v_i^{-1}\),
            and averaged together. This averaged noise estimate is then used to take a diffusion step.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- CONDITIONS -->
<section>
  <div>
    <div>
      <div>
        <h2>Conditions on Views</h2>
        <p>
            We find that not every view function works with the above method. Of course, \(v_i\) must
            be invertible, but we discuss two additional constraints.
          </p>
        <h2>Linearity</h2>
        <p>
            A diffusion model is trained to estimate the noise in noisy data \(\mathbf{x}_t\) conditioned 
            on time step \(t\). The noisy data \(\mathbf{x}_t\) is expected to have the form 
            \[\mathbf{x}_t = w_t^{\text{signal}}\underbrace{\mathbf{x}_0}_{\text{signal}} + w_t^{\text{noise}}\underbrace{\epsilon\vphantom{\mathbf{x}_0}}_{\text{noise}}.\]
            That is, \(\mathbf{x}_t\) is a weighted average of pure signal \(\mathbf{x_0}\) 
            and pure noise \(\epsilon\), specifically with weights \(w_t^{\text{signal}}\) and \(w_t^{\text{noise}}\). 
            Therefore, our view, \(v\) must maintain this weighting between signal and noise. This can be achieved
            by making \(v\) linear, which we represent by the square matrix \(\mathbf{A}\). By linearity
            \[\begin{aligned} v(\mathbf{x}_t) &amp;= \mathbf{A}(w_t^{\text{signal}} \mathbf{x}_0+w_t^{\text{noise}} \epsilon)\\[7pt] &amp;= w_t^{\text{signal}} \underbrace{\mathbf{A}\mathbf{x}_0}_{\text{new signal}} + w_t^{\text{noise}} \underbrace{\mathbf{A}\epsilon}_{\text{new noise}}. \end{aligned}\]
            Effectively, \(v\) acts on the signal and the noise independently, and combines the result with the correct weighting.
          </p>
        <h2>Statistical Consistency</h2>
        <div>
          <p>
            Diffusion models are trained with the assumption that the noise is drawn iid from a standard normal.
            Therefore we must ensure that the transformed noise also follows these statistics. That is, we need
            \[\mathbf{A}\epsilon \sim \mathcal{N}(0, I).\]
            For linear transformations, this is equivalent to the condition that \(\mathbf{A}\) is orthogonal.
            Intuitively, orthogonal matrices respect the spherical symmetry of the standard multivariate Gaussian distribution.
          </p>
          <p>
            Therefore, for a transformation to work with our method, it is <b>sufficient for it to be orthogonal.</b>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ORTHOGONAL -->
<section>
  <div>
    <div>
      <div>
        <h2>Orthogonal Transformations</h2>
        <p>
            Most orthogonal transformations on images are meaningless, visually. For example, we transform
            the image below with a randomly sampled orthogonal matrix.
          </p>
        <p><img src="https://dangeng.github.io/visual_anagrams/static/images/orthogonal.jpeg"/>
        </p>
        <p>
            However, <b>permutations matrices are a subset of orthogonal matrices,</b> and are quite interpretable. 
            They are just rearrangements of pixels in an image. This is where the idea of a <b>visual anagram</b>
            comes from. The majority of illusions here can be interpreted this way—as specific rearrangements of pixels—such as
            <a href="#rot90_examples">rotations</a>, <a href="#flip_examples">flips</a>, 
            <a href="#misc_examples">skews</a>, <a href="#misc_examples">&#34;inner rotations,&#34;</a> 
            <a href="#jigsaw_examples">jigsaw rearrangements</a>, and 
            <a href="#perm_examples">patch permutations</a>. Finally, <a href="#inversion_examples">color inversions</a>
            are not permutations, but are orthogonal as they are a negation of pixel values.
            
          </p>
      </div>
    </div>
  </div>
</section>




<section>
  <div>
    <!-- Paper video. -->
    <div>
      <div>
        <h2>Video (Coming Soon!)</h2>
        <p>
          <!--<iframe src="https://www.youtube.com/embed/jNQXAC9IVRw?si=8Dnrd2U1f2My6KUC"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
          <iframe src="https://www.youtube.com/embed/1234567890" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
        </p>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section>
  <div>
    <!-- Concurrent Work. -->
    <div>
      <div>
        <h2>Related Links</h2>

        <div>
          <p>
            This project is inspired by previous work in this area, including:
          </p>
          <p>
            <a href="https://diffusionillusions.com/" target="_blank">Diffusion Illusions</a>, 
            by <a href="https://ryanndagreat.github.io/" target="_blank">Ryan Burgert</a> <i>et al.</i>,
            which produces multi-view illusions, along with other visual effects, through score distillation sampling.
          </p>
          <p>
            This <a href="https://github.com/tancik/Illusion-Diffusion" target="_blank">colab notebook</a> by 
            <a href="https://www.matthewtancik.com/about-me" target="_blank">Matthew Tancik</a>, 
            which introduces a similar idea to ours. We improve upon it significantly in 
            terms of quality of illusions, range of transformations, and theoretical analysis.
          </p>
          <p>
            <a href="https://www.reddit.com/r/StableDiffusion/comments/16ew9fz/spiral_town_different_approach_to_qr_monster/" target="_blank">Recent work by a pseudonymous artist</a>, Ugleh,
            uses a Stable Diffusion model finetuned for generating QR codes to produce images whose global structure subtly matches a given template image.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{geng2023visualanagrams,
  title     = {Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models},
  author    = {Geng, Daniel and Park, Inbum and Owens, Andrew},
  journal   = {arXiv:2311.17919},
  year      = {2023},
  month     = {Novemeber},
  abbr      = {Preprint},
  url       = {https://arxiv.org/abs/2311.17919},
}</code></pre>
  </div>
</section>






</div>
  </body>
</html>
