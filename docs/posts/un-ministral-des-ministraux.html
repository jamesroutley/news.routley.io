<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/ministraux/">Original</a>
    <h1>Un Ministral, Des Ministraux</h1>
    
    <div id="readability-page-1" class="page"><div><h2 id="introducing-the-worlds-best-edge-models">Introducing the world’s best edge models</h2><p>On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.</p><p>These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.</p><h3 id="use-cases">Use cases</h3><p>Our most innovative customers and partners have increasingly been asking for local, privacy-first inference for critical applications such as on-device translation, internet-less smart assistants, local analytics, and autonomous robotics. Les Ministraux were built to provide a compute-efficient and low-latency solution for these scenarios. From independent hobbyists to global manufacturing teams, les Ministraux deliver for a wide variety of use cases.</p><p>Used in conjunction with larger language models such as Mistral Large, les Ministraux are also efficient intermediaries for function-calling in multi-step agentic workflows. They can be tuned to handle input parsing, task routing, and calling APIs based on user intent across multiple contexts at extremely low latency and cost.</p><h3 id="benchmarks">Benchmarks</h3><p>We demonstrate the performance of les Ministraux across multiple tasks where they consistently outperform their peers. We re-evaluated all models with our internal framework for fair comparison.</p><h4 id="pretrained-models">Pretrained Models</h4><p><img src="https://mistral.ai/images/news/ministraux/pretrain_table.png" alt="Pretrained model comparison table" width="80%"/></p><p><b>Table 1:</b> Ministral 3B and 8B models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B on multiple categories</p><p><img src="https://mistral.ai/images/news/ministraux/pretrain_with_gemma.png" alt="Pretrained model comparison graph" width="80%"/></p><p><b>Figure 1:</b> Ministral 3B and 8B base models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B and Mistral 7B</p><h4 id="instruct-models">Instruct Models</h4><p><img src="https://mistral.ai/images/news/ministraux/instruct_table_with_gemma.png" alt="Instruct model comparison table" width="80%"/></p><p><b>Table 2:</b> Ministral 3B and 8B Instruct models compared to Gemma 2 2B, Llama 3.2 3B, Llama 3.1 8B, Gemma 2 9B and Mistral 7B on different evaluation categories.</p><p><img src="https://mistral.ai/images/news/ministraux/instruct_plot_3b_no_qwen_with_mistral_logo.png" alt="3B Instruct model comparison graph" width="80%"/></p><p><b>Figure 2:</b> A comparison of the 3B family of Instruct models - Gemma 2 2B, Llama 3.2 3B and Ministral 3B. The figure showcases the improvements of Ministral 3B over the much larger Mistral 7B.</p><p><img src="https://mistral.ai/images/news/ministraux/instruct_plot_8b_with_mistral_logo.png" alt="8B Instruct model comparison graph" width="80%"/></p><p><b>Figure 3:</b> A comparison of the 8B family of Instruct models - Gemma 2 9B, Llama 3.1 8B, Mistral 7B and Ministral 8B.</p><h3 id="availability-and-pricing">Availability and pricing</h3><p>Both models are available starting today.</p><table><thead><tr><th>Model</th><th>API</th><th>Pricing on la Plateforme</th><th>License</th></tr></thead><tbody><tr><td>Ministral 8B</td><td>ministral-8b-latest</td><td>$0.1 / M tokens (input and output)</td><td>Mistral Commercial License</td></tr><tr><td>Ministral 3B</td><td>ministral-3b-latest</td><td>$0.04 / M tokens (input and output)</td><td>Mistral Commercial License</td></tr></tbody></table><p>For self-deployed use, <a href="https://mistral.ai/contact/">please reach out to us</a> for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.</p><p>The model weights for <a href="https://huggingface.co/mistralai/Ministral-8B-Instruct-2410">Ministral 8B Instruct</a> are available for research use. Both models will be available from our <a href="https://docs.mistral.ai/deployment/cloud/overview/">cloud partners</a> shortly.</p><h3 id="more-to-come">More to come</h3><p>At Mistral AI, we continue pushing the state-of-the-art for frontier models. It’s been only a year since the release of Mistral 7B, and yet our smallest model today (Ministral 3B) already outperforms it on most benchmarks. We can’t wait for you to try out les Ministraux and give us feedback.</p><p><img src="https://mistral.ai/images/news/ministraux/meme_cropped.png" alt="More to come" width="50%"/></p></div></div>
  </body>
</html>
