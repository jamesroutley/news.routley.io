<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2410.16144">Original</a>
    <h1>1-Bit AI Infrastructure</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.16144">View PDF</a>
    <a href="https://arxiv.org/html/2410.16144v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption. These developments also enable local LLM deployment across a broad range of devices. In this work, we introduce <a href="http://bitnet.cpp" rel="external noopener nofollow">this http URL</a>, a tailored software stack designed to unlock the full potential of 1-bit LLMs. Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments demonstrate that <a href="http://bitnet.cpp" rel="external noopener nofollow">this http URL</a> achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes. The code is available at <a href="https://github.com/microsoft/BitNet" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Shaoguang Mao [<a href="https://arxiv.org/show-email/cbd43e5e/2410.16144" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
