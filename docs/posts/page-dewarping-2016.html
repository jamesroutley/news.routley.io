<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mzucker.github.io/2016/08/15/page-dewarping.html">Original</a>
    <h1>Page Dewarping (2016)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Flattening images of curled pages, as an optimization problem.</p>



<p>A while back, I wrote a script to create PDFs from
photos of hand-written text. It was nothing special – just
<a href="http://docs.opencv.org/3.0-last-rst/modules/imgproc/doc/miscellaneous_transformations.html#cv2.adaptiveThreshold">adaptive thresholding</a>
and combining multiple images into a PDF – but it came in handy
whenever a student emailed me their homework as a pile of JPEGs. After I
demoed the program to my fiancée, she ended up asking me to
run it from time to time on photos of archival documents for her
linguistics research.  This summer, she came back from the library
with a number of images where the text was significantly warped due to
curled pages.</p>

<p>So I decided to write a program that <em>automatically</em> turns pictures like the one on
the left below to the one on the right:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_before_after.png" alt="before and after dewarp"/></p>

<p>As with every project on this blog, the code is
<a href="https://github.com/mzucker/page_dewarp">up on github</a>. Also feel free
to <a href="#results">skip to the results section</a> if you want a sneak peek of some more before-and-after shots.</p>



<p>I am by no means the first person to come up with a method for
document image dewarping – it’s even implemented in Dan Bloomberg’s open-source
image processing library
<a href="http://www.leptonica.com/dewarping.html">Leptonica</a> – but when it
comes to understanding a problem, there’s nothing quite like
implementing it yourself. Aside from browsing through the Leptonica
code, I also skimmed a few papers on the topic, including a
<a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.99.7439">summary</a>
of the results of a dewarping contest, as well as an
<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.552.8971">article</a>
about the contest-winning Coordinate Transform Model (CTM) method.</p>

<p>Both the Leptonica dewarping method and the CTM method share a similar
hierarchical problem decomposition:</p>

<ol>
  <li>
    <p>Split the text into lines.</p>
  </li>
  <li>
    <p>Find a warp or coordinate transformation that makes the lines
parallel and horizontal.</p>
  </li>
</ol>

<p>To me, Leptonica’s approach to the second subproblem seems a bit
ad-hoc compared to CTM’s 3D “cylinder” model. To be honest, I had a
bit of trouble deciphering the CTM paper, but I liked the idea of a
model-based approach. I decided to create my own parametric model
where the appearance of the page is determined by a number of
parameters:</p>

<ul>
  <li>
    <p>a rotation vector \(\mathbf{r}\) and a translation vector
\(\mathbf{t}\), both in \(\mathbb{R}^3\), that parameterize the 3D
orientation and position of the page</p>
  </li>
  <li>
    <p>two slopes \(\alpha\) and \(\beta\) that specify the curvature of
the page surface (see spline plots below)</p>
  </li>
  <li>
    <p>the vertical offsets \(y_1, \ldots, y_n\) of \(n\) horizontal
spans on the page</p>
  </li>
  <li>
    <p>for each span \(i \in \{ 1, \ldots, n \}\), the horizontal
offsets \(x_i^{(1)}, \ldots, x_i^{(m_i)}\) of \(m_i\) points in
the horizontal span (all at vertical offset \(y_i\))</p>
  </li>
</ul>

<p>The page’s 3D shape comes from sweeping a curve along the local
\(y\)-axis (top-to-bottom direction). Each \(x\) (left-to-right)
coordinate on the page maps to a displacement \(z\) of the page
surface. I model the horizontal cross-section of the page surface as a
cubic spline whose endpoints are fixed at zero. The shape of the
spline can be specified completely by its slopes \(\alpha\) and
\(\beta\) at the endpoints:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/cubic_splines.png" alt="cubic splines with varying slope"/></p>

<p>As the plot shows, changing the slope parameters gives a variety of
“page-like” curves. Below, I’ve generated an animation that fixes the
page dimensions and all \((x, y)\) coordinates, while varying the
pose/shape parameters \(\mathbf{r}\), \(\mathbf{t}\), \(\alpha\), and
\(\beta\) – you can begin to appreciate that the parameter space
spans a useful variety of page appearances:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/page_warping.gif" alt="oooh dancing page"/></p>

<p>Importantly, once the pose/shape parameters are fixed, each \((x,
y)\) coordinate on the page is projected to a determined location on
the image plane. Given this rich model, we can now frame the entire
dewarping puzzle as an optimization problem:</p>

<ul>
  <li>
    <p>identify a number of <em>keypoints</em> along horizontal text spans in the
original photograph</p>
  </li>
  <li>
    <p>starting from a naïve initial guess, find the parameters \(\mathbf{r}\),
 \(\mathbf{t}\), \(\alpha\), \(\beta\), \(y_1\), \(\ldots\),
 \(y_n\), \(x_1^{(1)}\), \(\ldots\), \(x_n^{(m_n)}\) 
 which minimize the <a href="https://en.wikipedia.org/wiki/Reprojection_error">reprojection error</a>
 of the keypoints</p>
  </li>
</ul>

<p>Here is an illustration of reprojection before and after optimization:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_keypoints.png" alt="reprojection before and after optimization"/></p>

<p>The red points in both image are detected keypoints on text spans, and
the blue ones are reprojections through the model.  Note that the left
image (initial guess) assumes no curvature at all, so all blue points
are collinear; whereas the right image (final optimization output) has
established the page pose/shape well enough to place almost all of the
blue points on top of each corresponding red point.</p>

<p>Once we have a good model, we can isolate the pose/shape parameters,
and invert the resulting page-to-image mapping to dewarp the entire
image. Of course, the devil is in the details.</p>



<p>Here is a rough description of the steps I took.</p>

<ol>
  <li>
    <p><strong>Obtain page boundaries.</strong> It’s a good idea not to consider the
entire image, as borders beyond the page can contain lots of
garbage.  Instead of
<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.1467">intelligently identifying page borders</a>,
I opted for a simpler approach, just carving out the middle hunk
of the image with fixed margins on the edges.</p>
  </li>
  <li>
    <p><strong>Detect text contours.</strong> Next, I look for regions that look
“text-like”. This is a multi-step process that involves an
initial adaptive threshold:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_0.1_thresholded.png" alt="detect contours step 1"/></p>

    <p>…<a href="https://en.wikipedia.org/wiki/Dilation_(morphology)">morphological dilation</a>
by a horizontal box to connect up horizontally adjacent mask pixels:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_0.2_dilated.png" alt="detect contours step 2"/></p>

    <p>…<a href="https://en.wikipedia.org/wiki/Erosion_(morphology)">erosion</a> by
a vertical box to eliminate single-pixel-high “blips”:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_0.3_eroded.png" alt="detect contours step 3"/></p>

    <p>and finally,
<a href="https://en.wikipedia.org/wiki/Connected-component_labeling">connected component analysis</a>
with a filtering step to eliminate any blobs which are too tall
(compared to their width) or too thick to be text. Each remaining
text contour is then approximated by its best-fitting line
segment using
<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>,
as shown here:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_1_contours.png" alt="detect contours step 4"/></p>

    <p>Since some of the images that my fiancée supplied were of tables
full of vertical text, I also specialized my program to attempt
to detect horizontal lines or rules if not enough horizontal text
is found. Here’s an example image and detected contours:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_b_line_contours.png" alt="detect contours alt"/></p>
  </li>
  <li>
    <p><strong>Assemble text into spans.</strong> Once the text contours have been
identified, we need to combine all of the contours corresponding
to a single horizontal span on the page. There is probably a
linear-time method for accomplishing this, but I settled on a
greedy quadratic method here (runtime doesn’t matter much here
since nearly 100% of program time is spent in optimization
anyways).</p>

    <p>Here is pseudocode illustrating the overall approach:</p>

    <div><div><pre><code>edges = []
     
for each contour a:
  for each other contour b:
     cost = get_edge_cost(a, b)
     if cost &lt; INFINITY:
        edges.append( (cost, a, b) )
             
sort edges by cost
            
for each edge (cost, a, b) in edges:
  if a and b are both unconnected:
    connect a and b with edge e
</code></pre></div>    </div>

    <p>Basically, we generate candidate edges for every pair of text
contours, and score them. The resulting cost is infinite if the
two contours overlap significantly along their lengths, if they
are too far apart, or if they diverge too much in
angle. Otherwise, the score is a linear combination of distance
and change in angle.</p>

    <p>Once the connections are made, the contours can be easily grouped
into spans; I also filter these to eliminate any that are too
small to be useful in determining the page model.</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_2_spans.png" alt="assemble spans"/></p>

    <p>Above, you can see the span grouping has done a good job
amalgamating the text contours because each line of text has its
own color.</p>
  </li>
  <li>
    <p><strong>Sample spans.</strong> Because the parametric model needs discrete
keypoints, we need to generate a small number of representative
points on each span. I do this by choosing one keypoint per 20 or
so pixels of text contour:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_debug_3_span_points.png" alt="sample spans"/></p>
  </li>
  <li>
    <p><strong>Create naïve parameter estimate.</strong> I use PCA to estimate the
mean orientation of all spans; the resulting principal components
are used to analytically establish the initial guess of the \(x\)
and \(y\) coordinates, along with the pose of a flat,
curvature-free page using
<a href="http://docs.opencv.org/3.0-last-rst/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.solvePnP"><code>cv2.solvePnP</code></a>.
The reprojection of the keypoints will be accomplished by
sampling the cubic spline to obtain the \(z\)-offsets of the
object points and calling
<a href="http://docs.opencv.org/3.0-last-rst/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.projectPoints"><code>cv2.projectPoints</code></a>.
to project into the image plane.</p>
  </li>
  <li>
    <p><strong>Optimize!</strong> To minimize the reprojection error, I use
<a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"><code>scipy.optimize.minimize</code></a>
with the <code>&#39;Powell&#39;</code> solver as a black-box, derivative-free
optimizer. Here’s reprojection again, before and after optimization:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_keypoints.png" alt="reprojection before and after optimization"/></p>

    <p>Nearly 100% of the program runtime is spent doing this
optimization. I haven’t really experimented much with other
solvers, or with using a specialized solver for
<a href="https://en.wikipedia.org/wiki/Non-linear_least_squares">nonlinear least squares</a>
problems (which is exactly what this is, by the way). It might be
possible to speed up the optimization a lot!</p>
  </li>
  <li>
    <p><strong>Remap image and threshold.</strong> Once the optimization completes, I
isolate the pose/shape parameters \(\mathbf{r}\), \(\mathbf{t}\),
\(\alpha\), and \(\beta\) to establish a coordinate
transformation. The actual dewarp is obtained by projecting a
dense mesh of 3D page points via
<a href="http://docs.opencv.org/3.0-last-rst/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#cv2.projectPoints"><code>cv2.projectPoints</code></a>
and supplying the resulting image coordinates to
<a href="http://docs.opencv.org/3.0-last-rst/modules/imgproc/doc/geometric_transformations.html#cv2.remap"><code>cv2.remap</code></a>.
I get the final output with
<a href="http://docs.opencv.org/3.0-last-rst/modules/imgproc/doc/miscellaneous_transformations.html#cv2.adaptiveThreshold"><code>cv2.adaptiveThreshold</code></a>
and save it as a bi-level PNG using
<a href="http://python-pillow.org/">Pillow</a>. Again, before and after
shots:</p>

    <p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_before_after.png" alt="before and after dewarp"/></p>
  </li>
</ol>



<p>I’ve included several
<a href="https://github.com/mzucker/page_dewarp/tree/master/example_input">example images</a>
in the github repository to illustrate how the program works on a
variety of inputs. Here are the images, along with the program output:</p>

<p><strong>boston_cooking_a.jpg</strong>:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/boston_cooking_a_before_after.png" alt="before and after dewarp"/></p>

<p><strong>boston_cooking_b.jpg</strong>:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/boston_cooking_b_before_after.png" alt="before and after dewarp"/></p>

<p><strong>linguistics_thesis_a.jpg</strong>:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_a_before_after.png" alt="before and after dewarp"/></p>

<p><strong>linguistics_thesis_b.jpg</strong>:</p>

<p><img src="https://mzucker.github.io/images/page_dewarp/linguistics_thesis_b_before_after.png" alt="before and after dewarp"/></p>

<p>I also compiled some statistics about each program run (take the
runtimes with a grain of salt, this is for a single run on my 2012
MacBook Pro):</p>

<table>
  <thead>
    <tr>
      <th>Input</th>
      <th>Spans</th>
      <th>Keypoints</th>
      <th>Parameters</th>
      <th>Opt. time (s)</th>
      <th>Total time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>boston_cooking_a.jpg</td>
      <td>38</td>
      <td>554</td>
      <td>600</td>
      <td>23.3</td>
      <td>24.8</td>
    </tr>
    <tr>
      <td>boston_cooking_b.jpg</td>
      <td>38</td>
      <td>475</td>
      <td>521</td>
      <td>18.0</td>
      <td>18.8</td>
    </tr>
    <tr>
      <td>linguistics_thesis_a.jpg</td>
      <td>20</td>
      <td>161</td>
      <td>189</td>
      <td>5.1</td>
      <td>6.1</td>
    </tr>
    <tr>
      <td>linguistics_thesis_b.jpg</td>
      <td>7</td>
      <td>89</td>
      <td>104</td>
      <td>4.2</td>
      <td>5.3</td>
    </tr>
  </tbody>
</table>

<p>You can see these are not exactly <em>small</em> optimization problems. The
smallest one has 89 parameters in the model, and the largest
has 600. Still, I’m sure the optimization speed could be improved by
trying out different methods and/or using a compiled language.</p>



<p>The way this project unfolded represents a fairly typical workflow for
me these days: do a bit of reading to collect background knowledge,
and then figure out how to formulate the entire problem as the output
of some optimization process. I find it’s a pretty effective way of
tackling a large number of technical problems. Although I didn’t think
of it at the time, the overall approach I took here is reminiscent of
both
<a href="https://people.eecs.berkeley.edu/~rbg/latent/">deformable part models</a>
and
<a href="https://www.cs.cmu.edu/~efros/courses/AP06/Papers/matthews_ijcv_2004.pdf">active appearance models</a>,
though not as sophisticated as either.</p>

<p>Both Leptonica and the CTM method go one step further than I did, and
try to model/repair horizontal distortion as well as vertical. That
would be useful for my code, too – because the cubic spline is not an
<a href="https://en.wikipedia.org/wiki/Arc_length">arc-length</a>
parameterization, the text is slightly compressed in areas where the
cubic spline has a large slope. Since this project was mostly intended
as a proof-of-concept, I decided not to pursue the issue further.</p>

<p>Before putting up the final code on github, I tried out using the
automated Python style checker <a href="https://www.pylint.org/">Pylint</a> for
the first time. For some reason, on its first run it informed me that
all of the <code>cv2</code> module members were undefined, leading to an initial
rating of -6.88/10 (yes, negative). Putting the line</p>



<p>near the top of the file made it shut up about that. After tweaking
the program for a while to make Pylint happier, I got the score up to
9.09/10, which seems good enough for now. I’m not sure I agree 100%
with all of its default settings, but it was interesting to try it out
and learn a new tool.</p>

<p>I do all of my coding these days in
<a href="https://www.gnu.org/software/emacs/">GNU Emacs</a>, which usually suits
my needs; however, messing around with Pylint led me to discover a
feature I had never used. Pylint is not fond of short variable names
like <code>h</code> (but has no problem with <code>i</code>, go figure). If I use the normal
Emacs <code>query-replace</code> function bound to <code>M-%</code> and try to replace <code>h</code>
with <code>height</code> everywhere, I have to pay close attention to make sure
that it doesn’t also try to replace the h other identifiers (like
<code>shape</code>) as well. A while back, I discovered I could sidestep this by
using <code>query-replace-regexp</code> instead, and entering
the regular expression <code>\bh\b</code> as the replacement text (the <code>\b</code>
stands for a word <em>b</em>oundary, so it will only match the entire “word”
<code>h</code>). On the other hand, it’s a bit more work, and I thought there
must be a better place to do “whole-word” replacement. A bunch of
Googling led me to
<a href="http://emacs.stackexchange.com/a/12691/12975">this Stack Exchange answer</a>,
which says that using the <code>universal-argument</code> command <code>C-u</code> in Emacs
<em>before</em> a <code>query-replace</code> will do exactly what I want. I never knew
about <code>universal-argument</code> before – always good to learn new tricks!</p>

<p>At this point, I don’t anticipate doing much more with the dewarping
code. It could definitely use a thorough round of commenting, but the
basics are pretty much spelled out in this document, so I’ll just slap
a link here on the
<a href="https://github.com/mzucker/page_dewarp">github repo</a> and call it a
day. Who knows – maybe I’ll refer back to this project again the next
time I teach
<a href="http://www.swarthmore.edu/NatSci/mzucker1/e27_s2016/">computer vision</a>…</p>
















  </div>
  <!-- load mathjax -->
  
</article>


      </div>
    </div></div>
  </body>
</html>
