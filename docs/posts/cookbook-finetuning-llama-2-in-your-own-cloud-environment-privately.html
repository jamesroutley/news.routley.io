<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.skypilot.co/finetuning-llama2-operational-guide/">Original</a>
    <h1>Cookbook: Finetuning Llama 2 in your own cloud environment, privately</h1>
    
    <div id="readability-page-1" class="page"><div><figure><img loading="lazy" src="https://blog.skypilot.co/finetuning-llama2-operational-guide/images/demo.gif" alt="Result model in action, trained using this guide. From the SkyPilot and Vicuna teams."/><figcaption>Result model in action, trained using this guide. From the SkyPilot and Vicuna teams.</figcaption></figure><p>Meta released <a href="https://ai.meta.com/llama/">Llama 2</a> two weeks ago and has made a big wave in the AI community. In our opinion, its biggest impact is that the model is now released under a <a href="https://github.com/facebookresearch/llama/blob/main/LICENSE">permissive license</a> that <strong>allows the model weights to be used commercially</strong><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. This differs from Llama 1 which cannot be used commercially.</p><p>Simply put: Organizations can now take this base model and finetune it on their own data (be it internal documentation, customer conversations, or code), in a completely private environment, and use it in commercial settings.</p><p>In this post, we provide a step-by-step recipe to do exactly that: Finetuning Llama 2 on your own data, in your existing cloud environment, while using 100% open-source tools.</p><h2 id="why">Why?</h2><p>We provide <strong>an operational guide for LLM finetuning</strong> with the following characteristics:</p><ul><li><strong>Fully open-source</strong>: While many hosted finetuning services popped up, this guide only uses open-source, Apache 2.0 software, including SkyPilot. Thus, this recipe can be used in any setting, be it research or commercial.</li><li><strong>Everything in your own cloud</strong>: All compute, data, and trained models stay in your own cloud environment (VPC, VMs, buckets). You retain full control and there’s no need to trust third-party hosted solutions.</li><li><strong>Automatic multicloud</strong>: The same recipe runs on all hyperscalers (AWS, GCP, Azure, OCI, ..) or GPU clouds (Lambda). See the <a href="https://skypilot.readthedocs.io/en/latest/">7+ cloud providers</a> supported in SkyPilot.</li><li><strong>High GPU availability</strong>: By using all regions/clouds you have access to, SkyPilot automatically finds the <a href="https://blog.skypilot.co/announcing-skypilot-0.3/#what-does-more-clouds-mean-for-ai-workloads">highest GPU availability</a> for user jobs. No console wrangling.</li><li><strong>Lowest costs</strong>: SkyPilot auto-shops for the cheapest zone/region/cloud. This recipe supports finetuning on spot instances with <em>automatic recovery</em>, lowering costs by 3x.</li></ul><p>With this recipe, users not only can get started with minimal effort, but also ensure their data and model checkpoints are not seen by any third-party hosted solution.</p><h2 id="recipe-train-your-own-vicuna-on-llama-2">Recipe: Train your own Vicuna on Llama 2</h2><p><a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> is one of the first high-quality LLMs finetuned on Llama 1.
We (Wei-Lin and Zhanghao), Vicuna’s co-creators, updated the exact recipe that we used to train Vicuna to be based on Llama 2 instead, producing this finetuning guide.</p><p>In this recipe, we will show how to train your own Vicuna on Llama 2, using SkyPilot to easily find available GPUs on the cloud, while reducing costs to only ~$300.</p><p>This recipe (<a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2">download from GitHub</a>) is written in a way for you to copy-paste and run. For detailed explanations, see the <a href="#unpacking-the-recipe">next section</a>.</p><h3 id="prerequisites">Prerequisites</h3><ol><li>Apply for access to the Llama-2 model</li></ol><p>Go to the <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">application page</a> and apply for access to the model weights.</p><ol start="2"><li>Get an access token from HuggingFace</li></ol><p>Generate a read-only access token on HuggingFace <a href="https://huggingface.co/settings/token">here</a>. Go to the HuggingFace page for Llama-2 models <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat/tree/main">here</a> and apply for access. Ensure your HuggingFace email is the same as the email on the Meta request. It may take 1-2 days for approval.</p><ol start="3"><li>Download <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2">the recipe</a> and install SkyPilot</li></ol><div><pre><code data-lang="fallback">git clone https://github.com/skypilot-org/skypilot.git
cd skypilot
pip install -e &#34;.[all]&#34;
cd ./llm/vicuna-llama-2
</code></pre></div><p>Paste the access token into <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/train.yaml">train.yaml</a>:</p><div><pre><code data-lang="yaml"><span>envs</span><span>:</span><span>
</span><span>  </span><span>HF_TOKEN</span><span>:</span><span> </span><span>&lt;your-huggingface-token&gt; </span><span> </span><span># Change to your own huggingface token</span><span>
</span></code></pre></div><h3 id="training-data-and-model-identity">Training data and model identity</h3><p>By default, we use the <a href="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json">ShareGPT data</a> and the identity questions in <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/scripts/hardcoded_questions.py">hardcoded_questions.py</a>.</p><p><strong>Optional</strong>: To use custom data, you can change the following line in <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/train.yaml">train.yaml</a>:</p><div><pre><code data-lang="yaml"><span>setup</span><span>:</span><span> </span><span>|</span><span>
</span><span>  ...
</span><span>  wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json  -O $HOME/data/sharegpt.json
</span><span>  ...</span><span>  
</span></code></pre></div><p>The above json file is an array, each element of which having the following format (the conversation can have multiple turns, between <code>human</code> and <code>gpt</code>):</p><div><pre><code data-lang="json"><span>{</span>
  <span>&#34;id&#34;</span><span>:</span> <span>&#34;i6IyJda_0&#34;</span><span>,</span>
  <span>&#34;conversations&#34;</span><span>:</span> <span>[</span>
    <span>{</span>
      <span>&#34;from&#34;</span><span>:</span> <span>&#34;human&#34;</span><span>,</span>
      <span>&#34;value&#34;</span><span>:</span> <span>&#34;How to tell if a customer segment is well segmented? In 3 bullet points.&#34;</span>
    <span>},</span>
    <span>{</span>
      <span>&#34;from&#34;</span><span>:</span> <span>&#34;gpt&#34;</span><span>,</span>
      <span>&#34;value&#34;</span><span>:</span> <span>&#34;1. Homogeneity: The segment should consist of customers who share similar characteristics and behaviors.\n2. Distinctiveness: The segment should be different from other segments in terms of their characteristics and behaviors.\n3. Stability: The segment should remain relatively stable over time and not change drastically. The characteristics and behaviors of customers within the segment should not change significantly.&#34;</span>
    <span>}</span>
  <span>]</span>
<span>}</span><span>,</span>
</code></pre></div><p><strong>Optional</strong>: To make the model know about its identity, you can change the hardcoded questions <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/scripts/hardcoded_questions.py">hardcoded_questions.py</a></p><blockquote><p><strong>Note</strong>: Models trained on ShareGPT data may have restrictions on commercial usage. Swap it out with your own data for commercial use.</p></blockquote><h3 id="kick-start-training-on-any-cloud">Kick start training on any cloud</h3><p>Start training with a single command</p><div><pre><code data-lang="bash">sky launch --down -c vicuna train.yaml <span>\
</span><span></span>  --env <span>ARTIFACT_BUCKET_NAME</span><span>=</span>&lt;your-bucket-name&gt; <span>\
</span><span></span>  --env <span>WANDB_API_KEY</span><span>=</span>&lt;your-wandb-api-key&gt;
</code></pre></div><p>This will launch the training job on the cheapest cloud that has 8x A100-80GB spot GPUs available.</p><blockquote><p><strong>Tip</strong>: You can get <code>WANDB_API_KEY</code> at <a href="https://wandb.ai/settings">https://wandb.ai/settings</a>. To disable Weights &amp; Biases, simply leave out that <code>--env</code> flag.</p></blockquote><blockquote><p><strong>Tip</strong>: You can set <code>ARTIFACT_BUCKET_NAME</code> to a new bucket name, such as <code>&lt;whoami&gt;-tmp-bucket</code>, and SkyPilot will create the bucket for you.</p></blockquote><p><strong>Use on-demand instead to unlock more clouds</strong>: Inside <code>train.yaml</code> we requested using spot instances:</p><div><pre><code data-lang="yaml"><span>resources</span><span>:</span><span>
</span><span>  </span><span>accelerators</span><span>:</span><span> </span><span>A100-80GB:8</span><span>
</span><span>  </span><span>disk_size</span><span>:</span><span> </span><span>1000</span><span>
</span><span>  </span><span>use_spot</span><span>:</span><span> </span><span>true</span><span>
</span></code></pre></div><p>However, spot A100-80GB:8 is currently only supported on GCP. On-demand versions are supported on AWS, Azure, GCP, Lambda, and more. (Hint: check out the handy outputs of <code>sky show-gpus A100-80GB:8</code>!)</p><p>To use those clouds, add the <code>--no-use-spot</code> flag to request on-demand instances:</p><div><pre><code data-lang="fallback">sky launch --no-use-spot ...
</code></pre></div><p><img loading="lazy" src="https://blog.skypilot.co/finetuning-llama2-operational-guide/images/optimizer.gif" alt="Optimizer"/></p><p><strong>Optional</strong>: Try out the training for the 13B model:</p><div><pre><code data-lang="bash">sky launch -c vicuna train.yaml <span>\
</span><span></span>  --env <span>ARTIFACT_BUCKET_NAME</span><span>=</span>&lt;your-bucket-name&gt; <span>\
</span><span></span>  --env <span>WANDB_API_KEY</span><span>=</span>&lt;your-wandb-api-key&gt; <span>\
</span><span></span>  --env <span>MODEL_SIZE</span><span>=</span><span>13</span>
</code></pre></div><h3 id="reducing-costs-by-3x-with-spot-instances">Reducing costs by 3x with spot instances</h3><p><a href="https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html">SkyPilot Managed Spot</a> is a library built on top of SkyPilot that helps users run jobs on spot instances without worrying about interruptions. That is the tool used by the LMSYS organization to train the first version of Vicuna (more details can be found in their <a href="https://lmsys.org/blog/2023-03-30-vicuna/">launch blog post</a> and <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna">example</a>). With this, the training cost can be reduced from $1000 to <strong>$300</strong>.</p><p>To use SkyPilot Managed Spot, you can simply replace <code>sky launch</code> with <code>sky spot launch</code> in the above command:</p><div><pre><code data-lang="bash">sky spot launch -n vicuna train.yaml <span>\
</span><span></span>  --env <span>ARTIFACT_BUCKET_NAME</span><span>=</span>&lt;your-bucket-name&gt; <span>\
</span><span></span>  --env <span>WANDB_API_KEY</span><span>=</span>&lt;your-wandb-api-key&gt;
</code></pre></div><h3 id="serve-your-model">Serve your model</h3><p>After the training is done, you can serve your model in your own cloud environment with a single command:</p><div><pre><code data-lang="bash">sky launch -c serve serve.yaml --env <span>MODEL_CKPT</span><span>=</span>&lt;your-model-checkpoint&gt;/chatbot/7b
</code></pre></div><p>In <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/serve.yaml">serve.yaml</a>, we specified launching a Gradio server that serves the model checkpoint at <code>&lt;your-model-checkpoint&gt;/chatbot/7b</code>.</p><figure><img loading="lazy" src="https://blog.skypilot.co/finetuning-llama2-operational-guide/images/demo.gif" alt="demo"/><figcaption>Serving the resulting model with Gradio.</figcaption></figure><blockquote><p><strong>Tip</strong>: You can also switch to a cheaper accelerator, such as L4, to save costs, by adding <code>--gpus L4</code> to the above command.</p></blockquote><h2 id="unpacking-the-recipe">Unpacking the recipe</h2><p>Let’s unpack <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/train.yaml"><code>train.yaml</code></a>.</p><h3 id="provisioning-resources">Provisioning resources</h3><p>The <a href="https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html"><code>resources</code></a> dict specifies the resource requirements for the finetuning job:</p><div><pre><code data-lang="yaml"><span>resources</span><span>:</span><span>
</span><span>  </span><span>accelerators</span><span>:</span><span> </span><span>A100-80GB:8</span><span>
</span><span>  </span><span>disk_size</span><span>:</span><span> </span><span>1000</span><span>
</span><span>  </span><span>use_spot</span><span>:</span><span> </span><span>true</span><span>
</span></code></pre></div><p>Taking this cloud-agnostic spec, SkyPilot automates finding the best (cheapest &amp; available) cloud location and instance type that can satisfy them. It then goes ahead to provision the resources, with retries to handle out-of-capacity errors, and launch the job.</p><p>For the user, there’s no need to wrangle different cloud consoles anymore just to find what regions have GPU VMs available.</p><blockquote><p><strong>Tip</strong>: If you wish, you can still hard-code a specific cloud/region/zone or instance type to use. Check out the <a href="https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html">YAML spec</a> for all the knobs you can set.</p></blockquote><h3 id="maximizing-cloud-gpu-availability">Maximizing cloud GPU availability</h3><p>Every provisioning request (<code>sky launch</code>, <code>sky spot launch</code>) automates looking for resources in the search space given. This <a href="https://skypilot.readthedocs.io/en/latest/examples/auto-failover.html">automatic failover</a> mechanism is key to maximizing GPU availability:</p><figure><img loading="lazy" src="https://i.imgur.com/zDl2zob.png" alt="SkyPilot&amp;rsquo;s auto-failover across regions and clouds to improve GPU availability"/><figcaption>SkyPilot&#39;s auto-failover across regions and clouds to improve GPU availability.</figcaption></figure><p>There’s a tradeoff between the search space you give to SkyPilot vs. the GPU availability it can help find. For example, assume your laptop has access to AWS and GCP (see the helpful outputs of <code>sky check</code>). Then, if you specified in the <code>resources</code> dict:</p><ul><li><code>cloud: aws</code>, <code>region: us-east-1</code>: SkyPilot will only search in that region. If there are no requested GPUs available there, the provision request will fail. (You can pass <code>-r/--retry-until-up</code> to keep retrying.)</li><li><code>cloud: aws</code>: SkyPilot will search in all zones/regions in AWS.</li><li>No location constraints: SkyPilot will search in the “Sky”: all zones/regions/clouds you have access to (AWS and GCP in this example).</li></ul><p>As you can see, the last case is truly using multicloud transparently and should give the highest availability. That said, if users have special quotas or discounts in a specific location, there are knobs to narrow down the location.</p><h3 id="what-gpus-to-use">What GPUs to use</h3><p>The key requirement is that the GPU VRAM has to be big enough. In the YAML we used <code>A100-80GB:8</code>, which is essential for finetuning LLMs <em>without any performance loss</em>.</p><p>However, if you choose to use QLoRA or other PEFT (parameter-efficient fine-tuning) methods, the GPU requirement can be significantly relaxed. See one example of using <code>A10:1</code> (24GB) to finetune the 7B base model <a href="https://lambdalabs.com/blog/fine-tuning-metas-llama-2-on-lambda-gpu-cloud">here</a>, or Tobi Lütke’s <a href="https://github.com/artidoro/qlora/pull/132">QLoRA recipe for Llama1</a>.</p><p>To use a different GPU type/count, simply change the <code>resources.accelerators</code> field in YAML, or override the CLI command with a <code>--gpus &lt;name&gt;:&lt;count&gt;</code> flag.</p><p>Use this handy tool to look up GPUs in different clouds and their real-time prices:</p><h3 id="checkpointing-to-cloud-storage">Checkpointing to cloud storage</h3><p>In the YAML’s file_mounts section, we specified that a bucket named <code>$ARTIFACT_BUCKET_NAME</code> (passed in via an env var) should be mounted at <code>/artifacts</code> inside the VM:</p><div><pre><code data-lang="yaml"><span>file_mounts</span><span>:</span><span>
</span><span>  </span><span>/artifacts</span><span>:</span><span>
</span><span>    </span><span>name</span><span>:</span><span> </span><span>$ARTIFACT_BUCKET_NAME</span><span>
</span><span>    </span><span>mode</span><span>:</span><span> </span><span>MOUNT</span><span>
</span></code></pre></div><p>When launching the job, we then simply pass <code>/artifacts</code> to its <code>--output_dir</code> flag, to which it will write all checkpoints and other artifacts:</p><div><pre><code data-lang="fallback">torchrun ... --output_dir /artifacts/chatbot/${MODEL_SIZE}b ...
</code></pre></div><p>In other words, your training program uses this mounted path as if it’s local to the VM! Files/dirs written to the mounted directory are automatically synced to the cloud bucket.</p><blockquote><p><strong>Tip</strong>: You can either pass in a new name (e.g., <code>&lt;whoami&gt;-tmp-bucket</code>) for SkyPilot to automatically create a new bucket for you, or use an existing bucket’s name.</p></blockquote><blockquote><p><strong>Tip</strong>: All created buckets are private buckets that live in your own cloud account.</p></blockquote><p>You can inspect or download the outputs from the corresponding object store. Example bucket on Google Cloud Storage (GCS):</p><p><img src="https://blog.skypilot.co/finetuning-llama2-operational-guide/images/google-cloud-storage-bucket.png" width="485" alt="Example bucket"/></p><p>To learn more about interacting with cloud storage, see the <a href="https://skypilot.readthedocs.io/en/latest/reference/storage.html">SkyPilot Storage docs</a>.</p><h3 id="how-to-handle-spot-instance-preemptions">How to handle spot instance preemptions</h3><p>Spot GPUs are highly cost-effective, which are about 2.5x–3x cheaper than on-demand instances on the major clouds:</p><div><pre><code data-lang="fallback">» sky show-gpus A100-80GB:8

GPU        QTY  CLOUD   INSTANCE_TYPE              DEVICE_MEM  vCPUs  HOST_MEM  HOURLY_PRICE  HOURLY_SPOT_PRICE  REGION
A100-80GB  8    Azure   Standard_ND96amsr_A100_v4  -           96     1924GB    $ 32.770      $ 12.977           eastus
A100-80GB  8    GCP     a2-ultragpu-8g             -           96     1360GB    $ 40.222      $ 12.866           asia-southeast1
...
</code></pre></div><p>Interestingly, we have observed that <strong>spot GPUs sometimes can be more available than on-demand GPUs</strong>. This was recently observed to be the case on GCP’s A100s.</p><p>The question is: How can we easily handle spot preemptions?</p><h4 id="managed-vs-unmanaged">Managed vs. unmanaged</h4><p>SkyPilot supports two modes of using spot instances:</p><ul><li><a href="https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html"><strong>Managed Spot</strong></a>: Jobs launched with <code>sky spot launch</code>. Preemptions are automatically recovered by SkyPilot relaunching a new spot cluster in the next cheapest and available cloud location (this is why the bigger search space you give to SkyPilot, the higher the GPU availability!). The managed job is auto-restarted on the recovered cluster.</li><li>Unmanaged spot: Jobs launched with <code>sky launch</code> that request spot instances (either the <code>resources.use_spot</code> field in YAML or the CLI <code>--use-spot</code> flag). Under this mode, spot preemptions are <em>not</em> auto-recovered.</li></ul><p>Our recommendation is to use unmanaged spot for development since it allows easy login and debug (<code>ssh &lt;my cluster&gt;</code>), but beware of sudden preemptions. Once you’ve verified training proceeds normally, switch to managed spot for full runs to enable auto-recovery.</p><h4 id="handling-partial-checkpoints">Handling partial checkpoints</h4><p>Recall that we save checkpoints to a cloud storage bucket. This comes in handy because whenever a spot job is restarted on a newly recovered instance, it can reload the latest checkpoint from the bucket and resume training from there.</p><p>Our recipe’s <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vicuna-llama-2/scripts/train.py#L319C55-L319C55"><code>train.py</code></a> uses HuggingFace’s <code>Trainer</code>, which natively supports resuming from a checkpoint:</p><div><pre><code data-lang="python">    <span>trainer</span><span>.</span><span>train</span><span>(</span><span>resume_from_checkpoint</span><span>=</span><span>resume_from_checkpoint</span><span>)</span>
</code></pre></div><p>There’s one edge case to handle, however: During a checkpoint write, the instance may get preempted suddenly and only partial state is written to the cloud bucket. When this happens, resuming from a corrupted partial checkpoint will crash the program.</p><p>To solve this, we added a simple <code>transformers.TrainerCallback</code> that</p><ol><li>Writes out a <code>complete</code> indicator file after each checkpoint has been saved</li><li>On program restart, removes any incomplete checkpoints in the bucket</li></ol><p>Relevant code snippet:</p><div><pre><code data-lang="python"><span>class</span> <span>CheckpointCallback</span><span>(</span><span>transformers</span><span>.</span><span>TrainerCallback</span><span>):</span>

    <span>def</span> <span>on_save</span><span>(</span><span>self</span><span>,</span> <span>args</span><span>,</span> <span>state</span><span>,</span> <span>control</span><span>,</span> <span>**</span><span>kwargs</span><span>):</span>
        <span>&#34;&#34;&#34;Add complete indicator to avoid incomplete checkpoints.&#34;&#34;&#34;</span>
        <span>if</span> <span>state</span><span>.</span><span>is_world_process_zero</span><span>:</span>
            <span>ckpt_path</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>args</span><span>.</span><span>output_dir</span><span>,</span>
                                     <span>f</span><span>&#39;checkpoint-</span><span>{</span><span>state</span><span>.</span><span>global_step</span><span>}</span><span>&#39;</span><span>)</span>
            <span>with</span> <span>open</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>ckpt_path</span><span>,</span> <span>&#39;complete&#39;</span><span>),</span> <span>&#39;w&#39;</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
                <span>f</span><span>.</span><span>write</span><span>(</span><span>&#39;&#39;</span><span>)</span>
            <span>print</span><span>(</span><span>f</span><span>&#39;Checkpoint </span><span>{</span><span>state</span><span>.</span><span>global_step</span><span>}</span><span> saved.&#39;</span><span>)</span>
        <span>torch</span><span>.</span><span>distributed</span><span>.</span><span>barrier</span><span>()</span>


<span>def</span> <span>cleanup_incomplete_checkpoints</span><span>(</span><span>output_dir</span><span>):</span>
    <span>&#34;&#34;&#34;Remove incomplete checkpoints.&#34;&#34;&#34;</span>
    <span>checkpoints</span> <span>=</span> <span>list</span><span>(</span><span>pathlib</span><span>.</span><span>Path</span><span>(</span><span>output_dir</span><span>)</span><span>.</span><span>glob</span><span>(</span><span>&#39;checkpoint-*&#39;</span><span>))</span>
    <span>...</span>  <span># Sort by step</span>
    <span>for</span> <span>checkpoint</span> <span>in</span> <span>checkpoints</span><span>:</span>
        <span>if</span> <span>not</span> <span>(</span><span>checkpoint</span> <span>/</span> <span>&#39;complete&#39;</span><span>)</span><span>.</span><span>exists</span><span>():</span>
            <span>print</span><span>(</span><span>f</span><span>&#39;Removing incomplete checkpoint </span><span>{</span><span>checkpoint</span><span>}</span><span>&#39;</span><span>)</span>
            <span>shutil</span><span>.</span><span>rmtree</span><span>(</span><span>checkpoint</span><span>)</span>
        <span>else</span><span>:</span>
            <span>...</span>
            <span>break</span>
</code></pre></div><p>Hook these up on program start:</p><div><pre><code data-lang="python"><span>def</span> <span>train</span><span>():</span>
    <span>...</span>
    <span>if</span> <span>local_rank</span> <span>==</span> <span>0</span><span>:</span>
        <span>cleanup_incomplete_checkpoints</span><span>(</span><span>training_args</span><span>.</span><span>output_dir</span><span>)</span>
    <span>torch</span><span>.</span><span>distributed</span><span>.</span><span>barrier</span><span>()</span>
    <span>...</span>
    <span>trainer</span><span>.</span><span>add_callback</span><span>(</span><span>CheckpointCallback</span><span>)</span>
    <span>trainer</span><span>.</span><span>train</span><span>(</span><span>resume_from_checkpoint</span><span>=</span><span>resume_from_checkpoint</span><span>)</span>
    <span>...</span>
</code></pre></div><p>And that’s it! With about 30 lines of code, we’ve now made our trainer program fully robust to spot preemptions. You can now use <code>sky spot launch</code> for all finetuning runs, for both high cost savings and GPU availability.</p><h3 id="monitoring">Monitoring</h3><p>The recipe has set up Weights &amp; Biases integration for monitoring metrics.</p><p>How can we easily view different recoveries of the same job in the same charts? In the YAML, we passed <code>--run_name $SKYPILOT_TASK_ID</code> to our main program, and this <a href="https://skypilot.readthedocs.io/en/latest/running-jobs/environment-variables.html#skypilot-environment-variables">env var</a> is guaranteed to be the same across recoveries of the same spot job. (You can also assign your own run name.)</p><p>With this setup, you can use W&amp;B’s filter feature on <code>run_name</code> to view different recoveries of the same run:</p><p><img src="https://blog.skypilot.co/finetuning-llama2-operational-guide/images/wandb.png" width="525" alt="Example bucket"/></p><p>Here, the job was preempted once, resulting in 2 segments. Notice the overlap? That is due to some inherent progress loss from resumption. For example, the first segment saved a checkpoint, made some progress (but not to the next checkpoint), then got preempted. The second segment had to reload that last checkpoint and redo some work.</p><h2 id="conclusion">Conclusion</h2><p>With Llama 2 and SkyPilot, you can now finetune your own LLMs on your private data in your own cloud account, and do so cost-effectively. We hope this recipe helps practitioners unleash the power of LLMs in private settings. Happy finetuning!</p><p><strong>Next steps</strong></p><ul><li>After finetuning, host a finetuned Llama 2 LLM in your own cloud: <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/llama-2">example</a>.</li><li>Speed up your LLM serving by up to 24x with the vLLM project: <a href="https://github.com/skypilot-org/skypilot/tree/master/llm/vllm">example</a>, <a href="https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/">blog</a>.</li></ul><p><em>Got questions or feedback? Please reach out to us on <a href="https://github.com/skypilot-org/skypilot">SkyPilot GitHub</a> or <a href="https://slack.skypilot.co/">Slack</a>!</em></p></div></div>
  </body>
</html>
