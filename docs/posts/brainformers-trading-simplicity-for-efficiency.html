<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2306.00008">Original</a>
    <h1>Brainformers: Trading Simplicity for Efficiency</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yanqi Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du%2C+N">Nan Du</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yanping Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peng%2C+D">Daiyi Peng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lan%2C+C">Chang Lan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+D">Da Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shakeri%2C+S">Siamak Shakeri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=So%2C+D">David So</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai%2C+A">Andrew Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yifeng Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhifeng Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Le%2C+Q">Quoc Le</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cui%2C+C">Claire Cui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Laundon%2C+J">James Laundon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dean%2C+J">Jeff Dean</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2306.00008">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Transformers are central to recent successes in natural language processing
and computer vision. Transformers have a mostly uniform backbone where layers
alternate between feed-forward and self-attention in order to build a deep
network. Here we investigate this design choice and find that more complex
blocks that have different permutations of layer primitives can be more
efficient. Using this insight, we develop a complex block, named Brainformer,
that consists of a diverse sets of layers such as sparsely gated feed-forward
layers, dense feed-forward layers, attention layers, and various forms of layer
normalization and activation functions. Brainformer consistently outperforms
the state-of-the-art dense and sparse Transformers, in terms of both quality
and efficiency. A Brainformer model with 8 billion activated parameters per
token demonstrates 2x faster training convergence and 5x faster step time
compared to its GLaM counterpart. In downstream task evaluation, Brainformer
also demonstrates a 3% higher SuperGLUE score with fine-tuning compared to GLaM
with a similar number of activated parameters. Finally, Brainformer largely
outperforms a Primer dense model derived with NAS with similar computation per
token on fewshot evaluations.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Yanqi Zhou [<a href="https://arxiv.org/show-email/ac3cf2ff/2306.00008">view email</a>]
      </p></div></div>
  </body>
</html>
