<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hypernerf.github.io/">Original</a>
    <h1>HyperNeRF</h1>
    
    <div id="readability-page-1" class="page">

<nav role="navigation" aria-label="main navigation">
  
  
</nav>


<section>
  <div>
    <div>
      <div>
        <p><img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/images/logo.svg" alt="HyperNeRF"/>
        </p>
      </div>
      <div>
        
        

        <p><span><sup>1</sup>University of Washington,</span>
          <span><sup>2</sup>Google Research</span>
        </p>

        
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <p>
      <video id="teaser" autoplay="" controls="" muted="" loop="" height="100%">
        <source src="./static/images/teaser.mp4" type="video/mp4"/>
      </video>
      <h2>
        <i>HyperNeRF</i> handles topological variations by modeling a
        family of shapes in a higher-dimensional space, thereby producing more realistic renderings
        and more accurate geometric reconstructions.
      </h2>
    </p>
  </div>
</section>


<section>
  <div>
    <div>
      

      <div>
        <p>
            Here we show results generated with <i>HyperNeRF</i>. These videos show the input video
            being
            played
            back with a stabilized novel camera path. The right side video shows the depth of the
            scene.
            Click on the arrows or drag to see more results.
          </p>
      </div>
    </div>
  </div>
</section>


<section>
  <div>

    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <div>
          <p>
            Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented
            fidelity,
            and various recent works have extended NeRF to handle dynamic scenes. A common approach
            to reconstruct such non-rigid scenes is through the use of a learned deformation field
            mapping from coordinates in each input image into a canonical template coordinate space.
            However, these deformation-based approaches struggle to model changes in topology, as
            topological changes require a discontinuity in the deformation field, but these
            deformation fields are necessarily continuous.
          </p>
          <p>
            We address this limitation by lifting
            NeRFs into a higher dimensional space, and by representing the 5D radiance field
            corresponding to each individual input image as a slice through this &#34;hyper-space&#34;. Our
            method is inspired by level set methods, which model the evolution of surfaces as slices
            through a higher dimensional surface. We evaluate our method on two tasks: (i)
            interpolating smoothly between &#34;moments&#34;, i.e., configurations of the scene, seen in the
            input images while maintaining visual plausibility, and (ii) novel-view synthesis at
            fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing
            methods on both tasks. Compared to Nerfies, <i>HyperNeRF</i> reduces average error rates by
            4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div>
      <div>
        <h2>Video</h2>
        <p>
          <iframe width="640" height="480" src="https://www.youtube.com/embed/qzgdE_ghkaI" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        </p>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<hr/>

<section>
  <div>
    <h2>Motivation</h2>
    <div>
      <!-- Level Sets. -->
      <div>
        <div>
          <h3>Level-Set Methods</h3>
          <div>
            <p>
              HyperNeRF represents changes in scene topology by providing a
              NeRF with a higher-dimensional input. This is inspired by level-set methods.
              Level-set methods provide a means to model a family of topologically-varying shapes as
              slices of a higher dimensional auxiliary function. For example, these shapes
            </p>
            <p><img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/0.svg"/>
              <img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/1.svg"/>
              <img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/2.svg"/>
            </p>
            <p>
              can be represented as slices through this auxiliary shape
            </p>
            <model-viewer src="./static/figures/level_set/level_set_3d.glb" alt="Slices through a 3D ambient surface." environment-image="neutral" auto-rotate="" camera-controls=""></model-viewer>
            <p>
              We can naturally model topologically varying shapes by just moving the
              slicing plane along the higher dimensions. For example, this animation was generated
              by
              moving the slicing plane from top to bottom:
            </p>
            
          </div>
        </div>
      </div>
      <!--/ Level Sets. -->

      <div>
        <div>
          <h3>Slicing Surfaces</h3>
          <p>
            Consider the follow shapes, which have different permutations of O xand X.
          </p>
          <p><img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/ox/0.svg"/>
            <img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/ox/1.svg"/>
            <img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/ox/2.svg"/>
            <img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/level_set/ox/3.svg"/>
          </p>
          <p>
            Traditionally, level-set methods use straight planes to slice the higher-dimensional
            surface:
          </p>
          <model-viewer src="./static/figures/level_set/ox/ox_ap.glb" alt="Slices through a 3D ambient surface." environment-image="neutral" auto-rotate="" camera-controls=""></model-viewer>
          <p>
            This means the higher-dimensional shape must contain copies of the same shape since
            each permutation has to lie along a single straight slice through the z-axis. If we let
            the slicing plane bend, it results in a much cleaner template:
          </p>
          <model-viewer src="./static/figures/level_set/ox/ox_ds.glb" alt="Slices through a 3D ambient surface." environment-image="neutral" auto-rotate="" camera-controls=""></model-viewer>
          <p>Please see the paper for details.</p>
        </div>
      </div>

    </div>
  </div>


  <div>
    <div>
      <h2>HyperNeRF</h2>
      <p>
        The HyperNeRF architecture is a straightforward extension to Nerfies. The key difference is
        that the template NeRF is conditioned on additional higher-dimensional coordinates, where
        the higher dimensional coordinates are given by an &#34;ambient slicing surface&#34; which can be
        thought of as a higher dimensional analog to the deformation field.
      </p>
      <p><img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/architecture.svg" alt="HyperNeRF architecture."/>
      </p>
      <h3>Hyper-Space Template</h3>
      <p>
        HyperNeRF leverages main idea of level set methods by using a template NeRF which lives in
        higher dimensions. In addition to the spatial coordinates (X, Y, Z), the NeRF MLP takes
        additional higher dimensional coordinates W<sub>1</sub> and W<sub>2</sub>. We call these
        the &#34;ambient dimensions&#34;.
      </p>
      <p>
        Here is an interactive viewer for the hyper-space of capture shown in the teaser. Drag the
        <span>blue cursor</span> around to change the ambient dimension rendered
        on the right.
      </p>
    </div>
    <div>
      <div>
        <div>
          <p>
          Ambient Dimension Coordinates
          </p></div>
      </div>
      <div>
        <div>
          <p><img src="https://www.wildlondon.org.uk/blog/chris-farthing/static/figures/hyper_grid.jpg"/>
          </p>
        </div><p>
        The hyper-space template rendered from a fixed viewpoint.
      </p></div>
    </div>
  </div>
</section>

<hr/>

<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{park2021hypernerf,
  author = {Park, Keunhong and Sinha, Utkarsh and Hedman, Peter and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Martin-Brualla, Ricardo and Seitz, Steven M.},
  title = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},
  journal = {ACM Trans. Graph.},
  issue_date = {December 2021},
  publisher = {ACM},
  volume = {40},
  number = {6},
  month = {dec},
  year = {2021},
  articleno = {238},
}</code></pre>
  </div>
</section>


<section id="acknowledgements">
  <div>
    <h2>Acknowledgements</h2>
    <p>Special thanks to <a href="https://homes.cs.washington.edu/~holynski/">Aleksander Hołyński</a>,
      <a href="https://roxanneluo.github.io/">Xuan Luo</a>, and Haley Cho for their support and
      help with collecting data. Thanks to <a href="https://zhengqili.github.io/">Zhengqi Li</a> and
      <a href="http://www.oliverwang.info/">Oliver Wang</a> for their help with the NSFF experiments.</p>
  </div>
</section>







</div>
  </body>
</html>
