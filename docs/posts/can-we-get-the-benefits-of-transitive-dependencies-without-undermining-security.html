<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tratt.net/laurie/blog/2024/can_we_retain_the_benefits_of_transitive_dependencies_without_undermining_security.html">Original</a>
    <h1>Can we get the benefits of transitive dependencies without undermining security?</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body">





<p>One of life’s great pleasures is trust: having confidence in another person
to do the right thing warms the hearts of both parties. Despite the cynicism
that we sometimes mistake for profundity, modern society would be impossible
without a large degree of trust, including trust between people who don’t know each other.</p>
<p>However, we all know that trust has limits. I have a legal and moral right to
leave my home for a week’s holiday, jam the doors wide open, pin a sign
to the gate saying “back in a week”, and to expect the contents of my house
to be untouched. I would be unwise, though, to trust that everyone will
respect my rights. Some people, alas, spend their lives
looking for opportunities to abuse other’s trust; some may only act when
an “opportunity” confronts them and their willpower buckles. Either way, it is
sensible for me to acknowledge this reality and to lock my door.</p>
<p>Just as with people, we place a great degree of trust in software, and
different pieces of software place a great degree of trust in each other. I
trust that, after I log into my bank account, my web browser won’t transfer
my money behind my back; my web browser itself trusts an image processing library to
decode arbitrary data from the internet without allowing an attacker to take
over the computer; and so on.</p>
<p>In this post I’m going to argue that the growth in transitive dependencies in
software is the equivalent of jamming our door open and hoping for the best — we
are putting too much trust in things we don’t and can’t know in detail.
However, I don’t think that the best long-term solution is to <a href="https://lucumr.pocoo.org/2025/1/24/build-it-yourself/">avoid transitive
dependencies all
together</a> — we’re
increasing our use of direct and indirect dependencies because it makes us more
productive and our software better. Is it possible to get the advantages without
the disadvantages?</p>
<h2><a name="trust_in_modern_software">Trust in modern software</a></h2>
<p>Writing modern software increasingly involves the use of <em>dependencies</em>, that
is other pieces of software . For example, I “wrote” the software that produces the
website you’re looking at right now by gluing together a number of Rust libraries. If those
libraries hadn’t existed – and if <code>cargo</code>  didn’t make it so easy to use them
– I doubt I’d have attempted to do so: it would have taken me too long , and
the results too fragile, to be practical. Only by placing trust in those libraries –
both in their overt “quality” <em>and</em> the motives of their authors – could
I tackle and complete my task.</p>
<p>However, there is a problem: my trust in those direct dependencies ends up being
transitively extended to their dependencies. My website software uses 20
libraries directly , but transitive dependencies mean that it ends
up building 181 libraries! It’s easy for me to forget that the trust I
place in those 20 direct dependencies is extended unchanged to the 161 indirect
dependencies.</p>
<p>This is very unlike
the real world, where trust decays exponentially the further it extends.
If a good friend introduces me to one of their friends and says that they trust
them 100%, I will immediately upgrade my level of trust in that person — but
not to 100%. If that person immediately introduces me to another person, and
repeats the 100% trust claim, I may well not upgrade my default trust at all.</p>
<p>In software, in contrast, a flaw
– deliberate or otherwise – in a dependency of a dependency of a dependency of a dependency
of a dependency is implicitly a flaw in my software, because I have to
extend identical levels of trust to each part of the dependency chain equally.</p>
<h2><a name="processes_are_our_main_line_of_defence">Processes are our main line of defence</a></h2>
<p>Our current software security model was designed for a more innocent age:
a greater degree of trust and was assumed; and software was
vastly smaller and more easily reasoned about.</p>
<p>For most of us, the fundamental aspect of our security model is the
<a href="https://en.wikipedia.org/wiki/Process_(computing)"><em>process</em></a> ,
the runtime “cage” that an operating system like Unix will run a program in.
Processes have been such a successful security abstraction that we often forget
to think of them as an explicit concept: they enable us to place reliable,
easily reasoned about, and impermeable boundaries <em>between</em> different pieces of
software. It is really difficult for one process to subvert the security
of another process .</p>
<p>However, <em>within</em> a process there is virtually no security. There are
various aspects to this, but let’s consider just a process’s memory (i.e.
the RAM it is using). Simplifying only slightly, every machine code instruction
executed has the ability to read from, and write to, anywhere within a processes’
memory.</p>
<p>If the software building my website does something clever with passwords
, any one of those 181 dependencies could decide that it will scan my
processes’ memory for passwords, and send any it finds over the internet
to a bad person.</p>
<h2><a name="possible_mitigations">Possible mitigations</a></h2>
<p>Is there anything I can do to stop one “part” of a process doing something with
another “part” that I would prefer it didn’t?</p>
<p>For example, let’s assume that
my “threat model” for Rust code is that I consider anything which uses <code>unsafe</code>
to be a security threat . I could then try banning any
direct or indirect dependency which uses <code>unsafe</code>, such that my dependencies
really are guaranteed not to undermine security. However,
since large parts of the Rust standard library use <code>unsafe</code>, would I ban libraries which
call such functions? Depending on how strict my ban was, I may well find that
there are virtually no dependencies I could actually use.</p>
<p>I could manually examine, and then whitelist, trustworthy Rust libraries but
that’s incredibly difficult to do reliably,
and what happens when a new version of a dependency is released — do I have
to examine it again from scratch?</p>
<p>We very quickly see that a seemingly easy rule – ban use of <code>unsafe</code> – is
impractical. Interestingly – and this is a problem with most such schemes –
it seems I can never find the right
balance between too little and too much trust.</p>
<p>There are, of course, many different approaches. For example, modern CPUs have
started to provide additional intra-process defences.
Most of these try to aid “control-flow integrity”, which aims to restrict
the program to executing “machine code” only at the places the process
put known, good machine code at.
This makes it harder for an attacker to upload valid machine
code to a process and then force the process to execute that code. It also makes it
harder to exploit existing “gadgets” in a process’s binary.
Control-flow integrity does improve security — but it wouldn’t
mitigate the simple password stealing attack I outlined earlier.</p>
<h2><a name="capability_architectures">Capability architectures</a></h2>
<p>Capability architectures more actively try to “compartmentalise” processes
in a wider sense. Probably the best known such architecture right now is
<a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/">CHERI</a>, though
there are other interesting designs, such as <a href="http://erights.org/elang/">E</a> and
<a href="https://github.com/pizlonator/llvm-project-deluge/">Fil-C</a> .</p>
<p>Most of us tend to focus on “memory safety” when thinking about capability
systems, that is preventing things like buffer overflows, use-after-frees, and
so on. This is undoubtedly the major user case for such systems, and the sole
focus of some. This is not a minor advantage: even today, too many security
flaws (though many fewer than in the past ) result from mistakes made when
programming in C/C++. By adding memory safety to memory unsafe languages, we
make the use of those languages – and dependencies written in those languages!
– harder to exploit.</p>
<p>However, capability systems – particularly CHERI – give us a tempting glimpse
of a more advanced future. Using CHERI, one can create various flavours of
‘compartments’, that is different portions of a process which are at least
somewhat prevented from attacking other portions of that process. There are
different ways of doing this, which I’ve <a href="https://tratt.net/laurie/blog/2023/two_stories_for_what_is_cheri.html">explored in more detail
elsewhere</a>, but the one
most people will think of is to use <em>pure capabilities</em>.</p>
<p>In essence, capabilities in CHERI are double width pointers with fine-grained
permissions: code can only access capabilities to which it is given permission.
One can use this, for example, to lock code into a subset of a process, only able to escape
via a single well-defined exit point. Exploring the possibilities is great fun,
but the more sophisticated one’s compartment mechanism becomes, the more likely
it is to be incomplete.</p>
<p>I have come to think that this style of compartmentalisation most useful for
ensuring that cooperative (i.e. trusted) software doesn’t go wrong
accidentally. Ensuring that actively malicious code doesn’t undermine the
hoped-for security guarantees is much harder.</p>
<p>In particular, most compartments
require very careful temporal  reasoning about a program, and this
is something that us humans find difficult to do. A single mistake – and my
compiler tells me that I can make low hundreds of obvious mistakes a day, so
who knows how many non-obvious mistakes I make? – can accidentally <a href="https://tratt.net/laurie/blog/2023/how_hard_is_it_to_adapt_a_memory_allocator_to_cheri.html">gift a
capability with unexpectedly high
privileges</a>.
One can use systems like CHERI to raise the bar fairly high, but they might
not be the right tool to raise the bar all the way.</p>
<h2><a name="the_problem_will_only_increase">The problem will only increase</a></h2>
<p>One could argue that the password-scanning attack I outlined earlier will
be so obvious to anyone looking at the code that it will soon be spotted. I
agree — but attacks don’t have to be obvious to be successful. I know a lot
of people who laughed at JavaScript for the <a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident">leftpad
incident</a>, but I didn’t see
anyone laugh at the more recent <a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">xz
attack</a> — it shocked many
people, who had assumed that open-source software could not be exploited
in such a manner.</p>
<p>Arguably all that such things are doing is forcing us to acknowledge
the possibilities for bad things to occur. For example, <a href="https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html#:~:text=merely%20asking%20pip%20to%20download%20a%20package%20can%20execute%20arbitrary%20code">one brave
soul</a>
tried installing every Python library available, and found all sorts of
unpleasant things happening to, and ending up on, their computer . Or
there was the time when some foolhardy researchers thought it would be a good
idea to <a href="https://www.theverge.com/2021/4/30/22410164/linux-kernel-university-of-minnesota-banned-open-source">see how easily one could slip vulnerabilities into the Linux kernel
without anyone
noticing</a>.
Or, just yesterday, it turned out that an “AI” <a href="https://x.com/davidtolnay/status/1883906113428676938">had cloned and then at least
in part made worse a well-known Rust
library</a> — which had
then become a direct dependency of 134 other libraries!</p>
<p>It’s not as if this is a recent idea. I would loved to have
been present at Ken Thompson’s <a href="https://dl.acm.org/doi/10.1145/358198.358210">Reflections on Trusting
Trust</a> talk in 1984 , where an
unsuspecting audience were led, step-by-step, through a series of innocuous
observations until they slowly realised that Thompson had undermined the integrity of the Unix <code>login</code>
program. That attack undermined the operating system, so it’s marginally different
than the point I’m making in this post — but only marginally.</p>
<p>All this has led me, slowly and reluctantly, to the conclusion that our
dependency-heavy approach to building software is fundamentally incompatible
with security. I say this with great reluctance — I find it <em>much</em> easier to
write large, reliable software than I did 10 years ago, and the quality and
quantity of dependencies that is now available is a big part of that. However,
I am painfully aware that this approach means that I’m taking on more risk than
I should be comfortable with.</p>
<p>It is possible to dismiss this as a theoretical worry. Yes, someone <em>could</em>
inject a security vulnerability into a library and use it for nefarious reasons
later, but there’s no evidence for it, right?</p>
<p>I have little time for head-in-the-sand thinking like this. History is replete with examples
of people who thought that they could avoid bad behaviour by others by
asserting that it couldn’t happen — and to whom the bad behaviour later happened.
Money spent on defence might seem wasted, but war is much more expensive.</p>
<p>I find it hard to believe that bad actors have seen things like
the xz attack and not then thought “I could do something like that”. Indeed,
I would be astonished if multiple such ‘bugs’ have not been inserted into
various pieces of software over many years. Quantifying the existence of
“secret attacks” is, of course, rather difficult: in general, all we can do is
observe explicit attacks that were spotted in the wild.</p>
<p>Several factors are working against us. First, modern package systems – Rust’s
cargo, Python’s pip, and so on – make publishing and using large quantities of
dependencies ever easier. Second, our software continues to increases in size
, and the larger it becomes, the easier it will become for problems
to seep through the cracks. Third, we have surprisingly few security mechanisms
beyond the Unix process.</p>
<h2><a name="what_might_we_be_able_to_do">What might we be able to do?</a></h2>
<p>The most fundamental part of our current security model – the process – would
be entirely recognisable to programmers from the late 1960s. Perhaps we stumbled
across the perfect security abstraction so early in computing’s history, but
such a belief requires more optimism than I can muster. The more dependencies we
use within a single process, the less suitable the process is as a security mechanism.</p>
<p>However, stating that there is a problem that needs solving doesn’t mean that
there is an obvious solution, or that any such solution is practical, or that
there is even a “solution” at all. Sometimes we have to accept that all the
trade-offs available to us are unpalatable.</p>
<p>With that qualification in mind, let me outline where I would one day like our
software to go. I would like to run software, built from multiple components
(i.e. dependencies of some kind), in such a way that:</p>
<ol>
<li>Components are isolated from each other as much as possible.</li>
<li>Each component only has the minimum permissions it needs.</li>
</ol>
<p>For example, I don’t want my image decoding component to have network access,
or the ability to access RAM with passwords in; but I do want my network
downloading component to have network access, and I do want to be able to
create a component that can manage and use passwords.</p>
<p>What do I mean by “component”? I’ve deliberately introduced a new term to
abstract away from some of the mental hang-ups that come with terms such as
“dependency”, “library”, “process”, and the like.</p>
<p>Roughly speaking, I imagine a “component” to encompass both a static and
dynamic thing: statically, a component corresponds to our notion of a
“program”; dynamically, a component corresponds to our notion of a “process”.</p>
<p>In other words, I want to split software up into mutually distrusting dynamic
“cells”, like processes, but with the ability to communicate more easily,
frequently, and cheaply. The communications between dynamic components would
need to be tightly specified, and if a component fails to communicate in exactly the required way,
other components should ignore all interactions. Another way of
looking at this is that it is a more rigorous enforcement of the age-old
<a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least privilege</a>:
our current approach to software hands out far too many privileges
to dependencies, and we need to rethink how we build software for
this to become a thing of the past.</p>
<p>The general outline of what I’m suggesting has some obvious antecedents:</p>
<ul>
<li>It is reminiscent of <a href="https://web.archive.org/web/20230226020624/http://www.citi.umich.edu/u/provos/ssh/privsep.html">privilege
separation</a>
as found in OpenSSH which splits single “static programs” into multiple dynamic processes.
Compromising one process does not compromise another.</li>
<li>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a>, which defines
how interacting “things”  can communicate with each other. This is
a fairly large umbrella term, ranging from languages such as <a href="https://www.erlang.org/">Erlang</a>
to various libraries and frameworks; few have security as an
explicit aim.</li>
</ul>
<p>My guess is that the ideal solution is a combination of these two ideas. That
doesn’t mean this is going to be easy: both solutions are well known, but
– depending on how liberally you interpret “actor” – neither has
quite taken over the world. There are a variety of reasons for that,
some shallow, some deeper. Combining these two ideas will, I think
pose two fundamental challenges:</p>
<ol>
<li>Performance. Components will have to be able to communicate efficiently and
in a suitably restricted manner. IPC (Inter-Process Communication) on a
typical Unix is something like 5-7 orders of magnitude slower than
intra-process communication, which is far too slow for the component model.
Unix-esque shared memory, though much faster, is far too difficult to use
reliably for untrusted components.</li>
<li>Expressivity. Different components will need to maintain as much of the ease
of use of current-era libraries as possible, without descending into the
horrors that RPC (Remote Procedure Call) tends to descend to.</li>
</ol>
<p>Addressing the first of these points requires at least somewhat rethinking of
hardware and operating systems; the second point requires rethinking
programming languages. Neither will be easy — but both, I believe, are doable.</p>
<p>Indeed, there are plenty of clues that what I’m suggesting is doable: from
reimagined operating systems (e.g. <a href="https://sel4.systems/">seL4</a>) to reimagined CPUs (e.g. Arm’s
<a href="https://www.arm.com/architecture/cpu/morello">Morello</a>
to the evolution of programming languages (from Erlang to Rust). Indeed, WebAssembly
seems to be aiming for a <a href="https://component-model.bytecodealliance.org/introduction.html">version of
componentisation</a>,
though I have the impression this is mostly aimed at lightweight compartmentalisation
of small processes . Put another way, I think this is a partial
solution to point (1) above, but on its own it can’t address point (2).</p>
<p>There will definitely be challenges along the way. In particular, what do we
do with all our existing software? Some sort of migration process is
inevitable, but I think it unlikely that we can magically “upgrade” our
existing software to the model I’m suggesting. Fundamentally, most software does not contain the
information we would need to retrospectively make it really secure. Indeed,
much software does the opposite, deliberately doing things that are insecure
. Realistically, we’d need a transition period where we would
have to accept that “old style” software was not as secure as we would like.</p>
<p>I also think we will find opportunities in rethinking how we structure
software: perhaps the componentisation I’m suggesting will allow us – finally!
– to be able to meaningfully and consistently distribute computations across
cores and machines.</p>
<h2><a name="too_much_not_enough_or_just_right">Too much, not enough, or just right?</a></h2>
<p>It is tempting to recoil from the change I’m proposing; to consider the problems
I’ve highlighted not all <em>that</em> bad; or to say that it would have been good to
do this 20 years ago, but it’s too late now. For longer than I should have, <em>I</em>
have recoiled in the same way! But, ultimately, two questions
have helped focus my mind:</p>
<ol>
<li>Will we come to view the proliferation of transitive dependencies as the
point when we lost the ability to secure software?</li>
</ol>
<p>Looking to the future, I find it very difficult to answer this in any other way
than “yes”. Despite this depressing answer, I don’t think we want to turn the
clock back to the time before the explosion in transitive dependencies: they
have allowed us to give our software more features while making it cheaper
to write and more reliable — quite the trio! Which
brings us to the next question:</p>
<ol start="2">
<li>Have we already written most of the software we will ever need, or does most
of it remain to be written?</li>
</ol>
<p>Imagine if, in 100 years from now, people were to look back to 2025: would
they say “they already
had nearly all the software tools that we’ve ever needed”? If the answer
to that question turns out to be “yes” then it means that we’ve
done a good enough job already, and we don’t need to rethink anything we’re
currently doing. I do not find this plausible: my strong intuition is that most
of the software we will ever need remains to be written.</p>
<p>Now, <em>how</em> this will happen is an open question. The software industry is more
productive than ever, but arguably less imaginative. We’re mostly using operating
systems that are almost exclusively recognisably 1960s/1970s in style, and programming languages
and CPUs that are mostly recognisably 1970s/1980s in style. I hope that this reflects
a current period of consolidation, and that it is not indicative of permanent
stasis.</p>
<p>Indeed, I am hopeful that other imaginative, optimistic souls might take the
plunge on ideas along the lines I’m suggesting. Perhaps just as likely they
will imagine another different, but equally fundamentally rethought, model to
how we can create software. Either way, anyone who tackles this problem will
have my heartfelt gratitude: we need a better future for our software, one
that can exploit the advantages of dependencies — without the downsides!</p>


<p>

2025-01-28 11:15

<a href="https://tratt.net/laurie/blog/2024/structured_editing_and_incremental_parsing.html">Older</a>

</p>

<div id="article-updates"><p>
If you’d like updates on new blog posts: follow me on
<a href="https://mastodon.social/@ltratt">Mastodon</a>
or <a href="https://twitter.com/laurencetratt">Twitter</a>;
or <a href="https://tratt.net/laurie/blog/blog.rss">subscribe to the RSS feed</a>;
or <a href="https://tratt.net/laurie/newsletter/">subscribe to email updates</a>:

</p>

</div>


<h3>Footnotes</h3>




































<h3>Comments</h3>







</div></div>
  </body>
</html>
