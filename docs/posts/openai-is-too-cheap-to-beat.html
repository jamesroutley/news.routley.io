<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://generatingconversation.substack.com/p/openai-is-too-cheap-to-beat">Original</a>
    <h1>OpenAI is too cheap to beat</h1>
    
    <div id="readability-page-1" class="page"><div class=""><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png" width="1024" height="1024" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1024,&#34;width&#34;:1024,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0866ffe3-ff87-4f83-ba5c-b327f403b8fc_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a><figcaption>Source: Ideogram. “OpenAI on a throne surrounded by piles of money.”</figcaption></figure></div><p>Since the beginning of the internet, data flywheels have created giant companies — first Google, quickly followed by social media companies, and now, OpenAI and other LLM providers.</p><p>OpenAI alone likely has more usage than all the other model providers combined, with Google and Anthropic making up most of the rest. These companies are collecting enormous amounts of data — not only can they see user prompts, they also get explicit feedback (thumbs up/thumbs down) as well as implicit feedback (e.g., asking more questions if you didn’t get the answer you wanted). Better yet, they are also at the forefront of customer conversations, understanding where LLM users are pushing the boundaries and where the models fail.</p><p><span>All of this is grist for the mill of future model training, and investment is only accelerating: Anthropic CEO Dario Amodei </span><a href="https://www.youtube.com/watch?v=Nlkk3glap_U" rel="">recently predicted that we’ll have models that cost $10B in 2 years</a><span>.</span></p><p>Model quality is definitely a big advantage, but it’s only a part of the story. The more impressive moat these companies have is the scalability of their infrastructure and the quality of their service. Let’s look at fine-tuning APIs as an illustrative example.</p><p>Our team at RunLLM has been running experiments recently with the GPT fine-tuning API. A single fine-tuning run on GPT-3.5 costs us anywhere from $4-12 dollars and take about 1-1.5 hours to fine-tune over about 1 million tokens.</p><p><span>Meanwhile, a single </span><code>p4d.24xlarge</code><span> on AWS costs $32.77 per-hour on-demand or $19.22 per-hour if you reserve for 1 year. Each machine comes with 8 Nvidia A100 GPUs. Assuming that OpenAI only uses 8 GPUs to fine-tune GPT-3.5, it’s 3-8x cheaper to use OpenAI than it is to rent a </span><code>p4d.24xlarge</code><span> from Amazon — without even accounting for the technical expertise required to deploy and run the jobs.</span></p><p><span>AWS is obviously charging a markup on its EC2 instances, but OpenAI’s costs include training and storing the model weights (likely reasonably cheap </span><a href="https://generatingconversation.substack.com/p/lora-explained" rel="">with LoRA</a><span>), building &amp; maintaining the fine-tuning infrastructure, and the expertise needed to rack &amp; stack thousands of GPUs internally</span></p><p><span>.</span></p><p><span>If we had a dense enough workload, perhaps we could justify renting a </span><code>p4d.24xlarge</code><span> at the yearly reserved cost. At $19.22 per-hour, we’ll be paying about $166K per-year.</span></p><p>Let’s assume again we’re using LoRA to fine-tune a model on 8 A100s, perhaps at 2 hours per run. We can do 12 fine-tuning runs per-day. on these GPUs, or 4,380 fine-tuning runs per year. We’ll allocate one engineer to deploy, check, and validate fine-tuning runs full-time (we don’t envy them!), which will cost us perhaps $200K per-year. (Let’s also assume that we have plenty of data readily available to us to keep fine-tuning jobs going constantly.)</p><p>At $366K ($166K AWS + $200K talent), we’re paying around $80 per-fine-tuning run, about 8-20x higher than what we’re paying OpenAI!</p><p>This just to fine-tune a model. While per-token inference costs for fine-tuned GPT-3.5 is 10x more expensive than GPT-3.5 it is still 10x cheaper than GPT-4! Serving a model on your own hardware is significantly more expensive unless you can reach a large enough scale to fully utilize serving hardware or elastically scale (impossible when GPU availability is limited).</p><p>We’ll give the back of the envelope math a rest, but it proves a critical point: The major LLM providers’ advantage doesn’t just lie in the quality of the model but in their ability to serve models at extreme economies of scale. It simply doesn’t make sense for most organizations to run after their own open-source LLM deployments. They’ll be sinking needless time, talent, and money into an unsolvable optimization problem, while competitors will move faster and likely achieve better quality by layering on top of OpenAI.</p><p><span>Of course, that doesn’t mean that open-source models have no future. We </span><a href="https://generatingconversation.substack.com/p/open-source-llms-shouldnt-try-to" rel="">touched on this last week</a><span>, and our friend Nathan Lambert at Interconnects </span><a href="https://www.interconnects.ai/p/are-open-llms-viable" rel="">recently wrote about the future of open-source models as well</a><span>. Open-source models must get smaller over time to reduce the cost, complexity, and time required to customize and run them.</span></p><p>For everything else, the major LLM providers will dominate.</p></div></div></div></div>
  </body>
</html>
