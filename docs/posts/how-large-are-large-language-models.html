<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gist.github.com/rain-1/cf0419958250d15893d8873682492c3e">Original</a>
    <h1>How large are large language models?</h1>
    
    <div id="readability-page-1" class="page"><div>
  <div id="file-base-model-trends-md">
      
      <div id="file-base-model-trends-md-readme" tabindex="0" role="region" aria-label="base model trends.md content, created by rain-1 on 10:38AM today.">
    <article itemprop="text">
<p dir="auto">This aims to be factual information about the size of large language models. None of this document was written by AI. I do not include any information from leaks or rumors. The focus of this document is on base models (the raw text continuation engines, not &#39;helpful chatbot/assistants&#39;). This is a view from a few years ago to today of one very tiny fraction of the larger LLM story that&#39;s happening.</p>

<ul dir="auto">
<li><strong>GPT-2,-medium,-large,-xl</strong> (2019): 137M, 380M, 812M, 1.61B. Source: <a href="https://huggingface.co/openai-community/gpt2" rel="nofollow">openai-community/gpt2</a>. Trained on the unreleased <a href="https://archive.ph/4aJSn" rel="nofollow">WebText</a> dataset said to 40GB of Internet text - I estimate that to be roughly 10B tokens. You can see a list of the websites that went into that data set here <a href="https://github.com/openai/gpt-2/blob/master/domains.txt">domains.txt</a>.</li>
<li><strong>GPT-3 aka davinci, davinci-002</strong> (2020): 175B parameters. There is a good breakdown of how those parameters are &#39;spent&#39; here <a href="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters" rel="nofollow">How does GPT-3 spend its 175B parameters?</a>. Trained on around 400B tokens composed of CommonCrawl, WebText2, Books1, Books2 and Wikipedia. Source <a href="https://arxiv.org/abs/2005.14165" rel="nofollow">Language Models are Few-Shot Learners</a>. These training runs required months of a data center full of tens of thousands of A100 GPUs <a href="https://www.bloomberg.com/news/articles/2023-03-13/microsoft-built-an-expensive-supercomputer-to-power-openai-s-chatgpt" rel="nofollow">source</a>.</li>
<li><strong>GPT-3.5, GPT-4</strong> (2022, 2023): No official factual information on architecture or training data available.</li>
</ul>

<ul dir="auto">
<li>
<p dir="auto">*<em>Llama 7B, 13B, 33B, 65B</em>: The 65B model was pretrained on a 1.4T (trillion tokens) dataset. LLaMA was officially stated to use Books3 <a href="https://arxiv.org/abs/2302.13971" rel="nofollow">source</a> as a data set - this is a very important dataset which has been pivotal in lawmaking regarding the training of AIs on large amounts of copyrighted and potentially pirated material.</p>
</li>
<li>
<p dir="auto"><strong>Llama-3.1 405B</strong> (2024): The 405B llama model was released. This is a dense transformer model, meaning all parameters are used in inference passes. Initial pretraining: 2.87T tokens, long context: 800B, annealing: 40M - so 3.67T total. source: <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/" rel="nofollow">The Llama 3 Herd of Models</a>. By this point meta has learned to say less about what data goes into the models <em>&#34;We create our dataset for language model pre-training from a variety of data sources containing knowledge&#34;</em> - so I can&#39;t say as much about what goes into the training data here.</p>
</li>
</ul>
<blockquote>
<p dir="auto">Empirically, we find that annealing (see Section 3.4.3) on small amounts of high-quality code and mathematical data can boost the performance of pre-trained models on key benchmarks</p>
</blockquote>
<p dir="auto">The emerging trend of <em>annealing</em> pretrained models to &#39;benchmax&#39; is unfortunate in that it biases the base language models somewhat away from being pure text continuation engines. This should really be part of the post-training which aims to make the models role play as some kind of AI-chatbot helpful assistant character. But these companies care very much about metrics and scores.</p>
<ul dir="auto">
<li><strong>Llama-4</strong> (2025): The largest in the llama4 family is a 2T total parameter MoE model - A288B 16E (active 288B parameters, 16 Experts). <em>It is unreleased</em>. As for the smaller llama4 models (which are distilled from the large one): There was a scandal as facebook decided to mislead people by gaming the lmarena benchmark site - they served one version of llama4 maverick there and released a different version of llama4 maverick, for some reason. This academic misconduct led to reduced trust in the llama team which seems to have imploded shortly after. It&#39;s unclear if the 2T Behemoth model will ever be released after what happened. The smaller LLama4 models (maverick and scout), are distilled off this large one, and are generally considered to be of low-intelligence.</li>
</ul>

<p dir="auto">For a long time, there weren&#39;t really any <em>large</em> language models available to download. There was certainly nothing comparable with GPT-3 for a few years. There were projects to try to match it, but generally they operated by fine tuning things like small (70B) llama models on a bunch of GPT-3 generated texts (synthetic data - which can result in degeneration when AI outputs are fed back into AI training intputs).</p>
<p dir="auto">The release of 405B was a turning point here. Just before that (Dec 2023) <a href="https://mistral.ai/news/mixtral-of-experts" rel="nofollow">Mistral</a> released Mixtral 8x7B - a MoE model, and then in April 2024 <a href="https://mistral.ai/news/mixtral-8x22b" rel="nofollow">Mixtral-8x22B</a> was released - a 141B total, A39B sparse MoE model. Even though this was not a dense model, like GPT-3 (175B) it is comparable in total parameter size. The MoE arch. enabled larger models to be trained and used by more people - people without access to thousands of interconnected GPUs.</p>


<p dir="auto">This was released the day after Christmas 2024. In the words of the <a href="https://api-docs.deepseek.com/news/news1226" rel="nofollow">deepseek webpage</a>:</p>
<pre><code>ðŸŽ‰ Whatâ€™s new in V3

    ðŸ§  671B MoE parameters
    ðŸš€ 37B activated parameters
    ðŸ“š Trained on 14.8T high-quality tokens
</code></pre>
<p dir="auto"><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" rel="nofollow">deepseek-ai/DeepSeek-V3-Base</a> <a href="https://arxiv.org/pdf/2412.19437" rel="nofollow">paper</a>.</p>
<p dir="auto">This was a gigantic leap forward in model size, and when R1 (the reasoning model built on top of this base model) was released it impressed a lot of people, I think this may have been the first time a truly GPT-4 level model was available to download and use. For reasons unclear this temporarily tanked the NVDA stock price.</p>
<p dir="auto">This really opened the door to new large MoE language models being trained, especially in China, and released freely for people to use. Note the following models are also starting to be multi-modal, as well as multi-linguial, so they have been provided large amounts of new types of data during training.</p>

<ul dir="auto">
<li><a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm" rel="nofollow">https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm</a></li>
<li><a href="https://huggingface.co/databricks/dbrx-base" rel="nofollow">https://huggingface.co/databricks/dbrx-base</a></li>
<li>Arch: <strong>132B A36B 12T</strong></li>
</ul>
<blockquote>
<p dir="auto">Compared to other open MoE models like Mixtral-8x7B and Grok-1, DBRX is fine-grained, meaning it uses a larger number of smaller experts. DBRX has 16 experts and chooses 4, while Mixtral-8x7B and Grok-1 have 8 experts and choose 2.</p>
</blockquote>

<p dir="auto"><a href="https://huggingface.co/MiniMaxAI/models#repos" rel="nofollow">https://huggingface.co/MiniMaxAI/models#repos</a>
<a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01" rel="nofollow">https://huggingface.co/MiniMaxAI/MiniMax-Text-01</a>
<a href="https://arxiv.org/pdf/2501.08313" rel="nofollow">https://arxiv.org/pdf/2501.08313</a>
456B A45.9B
Attention, Softmax Attention and Mixture-of-Experts (MoE).</p>
<blockquote>
<p dir="auto">Building upon the architecture design and computation optimizations, we train our foundational
language model, MiniMax-Text-01
To assess document quality at a granular level, we utilize our previous-generation
model as the reward labeler (a MoE model with 5B activations and 60B total parameters).</p>
</blockquote>

<ul dir="auto">
<li><a href="https://huggingface.co/rednote-hilab/dots.llm1.base" rel="nofollow">https://huggingface.co/rednote-hilab/dots.llm1.base</a></li>
<li><a href="https://www.arxiv.org/pdf/2506.05767" rel="nofollow">https://www.arxiv.org/pdf/2506.05767</a></li>
<li><strong>143B A14B 11.2T tokens training data, 32,768 token context length</strong></li>
</ul>
<blockquote>
<p dir="auto"><code>dots.llm1</code> achieves performance comparable to Qwen2.5-72B after pretrained on high-quality corpus without synthetic data
Architecture: Multi-head Attention with QK-Norm in attention Layer, fine-grained MoE utilizing top-6 out of 128 routed experts, plus 2 shared experts.</p>
</blockquote>

<ul dir="auto">
<li><a href="https://huggingface.co/tencent/Hunyuan-A13B-Pretrain" rel="nofollow">https://huggingface.co/tencent/Hunyuan-A13B-Pretrain</a></li>
<li><a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf">https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf</a></li>
<li><strong>MoE GQA - 80B A13B 20T 256K-ctx</strong></li>
</ul>
<blockquote>
<p dir="auto">During the training stage, the shared expert remains perpetually active, while only 8 non-shared experts are activated simultaneously.</p>
</blockquote>

<ul dir="auto">
<li><a href="https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT" rel="nofollow">https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT</a></li>
<li><a href="https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf" rel="nofollow">https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf</a></li>
<li><strong>424B A47B</strong></li>
</ul>
<p dir="auto">It is not clear to me how many tokens the base model here was trained on but the hf page says &#34;trillions&#34;.</p>

<p dir="auto">For a long time there were very very few LLMs on the same scale as GPT-3 that are available. Attempts to match GPT-3 level performance with downloadable weights were hindered by this, and genuinely, I do not think people understood that the raw size of the model being comparable to 175B was required. All that was available were the &lt;=70B llama models and people tried to work with them.</p>
<p dir="auto">405B is the latest large dense base model available that I&#39;m aware of, but it&#39;s annealed and contains recent data in its pretraining (meaning that there will be people discussing LLMs and sharing logs and transcripts of LLMs), so it&#39;s a little bit more like an &#39;assistant&#39; than previous base models. The same flaws apply to the recent wave of MoE models. They also have some aspets of Chinese culture baked into them.</p>
<p dir="auto">It&#39;s not completely clear how to compare MoE models with dense models. Perhaps there are aspects of LLM-intelligence that can only be achieved with sufficient depth/density. I don&#39;t think the current automated benchmarks are able to capture this, so everyone is just going all in on MoEs now.</p>
<p dir="auto">Newer models might be trained with new architectures (RWKV, byte-latent, bitnet), or new techniques of synthetic data generation (to avoid lawsuits, and to get good scores on benchmarks), but it&#39;s unclear how important these things really are for making a good raw text continuation engine - which I believe is the foundation for the capabilities that whatever type of fine-tuning elicits from these neural networks. Currently the trend is to make chatbots that roleplay as &#39;ai assistants&#39; - and I really hope that more people investigate alternatives.</p>
</article>
  </div>

  </div>
</div></div>
  </body>
</html>
