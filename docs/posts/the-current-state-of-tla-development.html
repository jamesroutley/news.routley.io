<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ahelwer.ca/post/2025-05-15-tla-dev-status/">Original</a>
    <h1>The current state of TLA⁺ development</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        <p>The <a href="https://conf.tlapl.us/2025-etaps/">2025 TLA⁺ Community Event</a> was held last week on May 4th at McMaster University in Hamilton, Ontario, Canada.
It was a satellite event to <a href="https://etaps.org/2025/">ETAPS 2025</a>, which I also attended, and plan to write about in the near future.
I gave a talk somewhat-hucksterishly titled <em>It’s never been easier to write TLA⁺ tooling!</em> which I will spin into a general account of the state of TLA⁺ development here.
The conference talks were all recorded, so if you’d like this blog post in video form you can watch it below:</p>

    <p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/KrhZebeRn90?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>


<p>The thesis of this post is that almost all the dreams &amp; desires we have for TLA⁺ are downstream of making it easy to develop in or on TLA⁺ language tooling.
We break this down into three parts:</p>
<ol>
<li>Overview of existing TLA⁺ language tooling</li>
<li>Overcoming the legacy code challenge</li>
<li>Some TLA⁺ development ideas for the near and middle future</li>
</ol>
<p>I’m very optimistic about the future of TLA⁺, despite the challenges.
We now have the <a href="https://foundation.tlapl.us/">TLA⁺ Foundation</a>, which has very generously been paying me a comfortable living wage to work on the tools for the past six months.
I will talk about some of the things I’ve accomplished.
Being paid to work on FOSS is a blessed existence; if that sounds appealing to you as well, the TLA⁺ Foundation is <a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">looking to fund more contributors</a>!</p>

<p>This is a summary of what TLA⁺ language tooling exists, to stoke the imagination for what tools you could contribute to or build upon.</p>
<h3 id="parsers">Parsers</h3>
<p>Parsers are of course the foundation of any language tooling, because your tools need to know what some source code actually says.
We have a decent stable of them:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools">SANY</a> (Syntactic ANalYzer) - our flagship parser, a Java-based recursive descent parser generated by <a href="https://github.com/javacc/javacc">JavaCC</a>.
This is the only parser that really handles all the syntax and does a lot of semantic checks (including level-checking, which is the closest thing TLA⁺ has to type checking) that other parsers don’t.</li>
<li><a href="https://github.com/tlaplus/tlapm">The TLAPM parser</a> - a parser-combinator-based OCaml parser, this is used by the TLA⁺ Proof System.
It handles almost all TLA⁺ syntax and does some rudimentary semantic analysis.</li>
<li><a href="https://github.com/tlaplus-community/tree-sitter-tlaplus/">tree-sitter-tlaplus</a> - I wrote &amp; <a href="https://rgoswami.me/post/2023-01-11-tree-sitter-tlaplus/">presented this</a> at the 2021 TLA⁺ Community Event.
Like all <a href="https://tree-sitter.github.io/tree-sitter/">tree-sitter</a> grammars, it uses a JavaScript-based DSL to generate a LR(1) parser in C along with some handwritten C to handle context-sensitive syntax.
This makes it very portable, and a strong tree-sitter ecosystem has developed with nice bindings for Python, Rust, JavaScript, and other languages.
This parser only handles syntax, no semantic checks, but that’s good enough for a lot of tools.
Its LR(1) nature also makes it a great static analyzer for ambiguity introduced by <a href="https://github.com/tlaplus/rfcs/issues/9">proposed TLA⁺ language extensions</a>.</li>
</ul>
<p>Those are the big three.
There are a couple others floating around, in varying stages of completion: <a href="https://pypi.org/project/tla/">one in Python</a>, <a href="https://github.com/ret/specifica">Haskell</a>, <a href="https://github.com/bugarela/tla-transmutation">Haskell (again)</a>, and <a href="https://codeberg.org/tlaplus/rstla">Rust</a> (I got halfway through writing that last one myself before getting lost in the sauce of the Rust type system &amp; burning out).</p>
<h3 id="interpreters">Interpreters</h3>
<p>Moving up the stack.
For the longest time we had only one interpreter, and now we have two!</p>
<ul>
<li>The one used by the <a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> (Temporal Logic Checker) finite model checker - it’s written in Java and uses SANY for parsing.
It also now has a REPL.</li>
<li><a href="https://github.com/will62794/spectacle">Spectacle</a> - presented by <a href="https://will62794.github.io/">William Schultz</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA⁺ conference</a>,  this is a web-based interpreter written in JavaScript using tree-sitter-tlaplus as a parser.
I think this tool is really neat!
It has a large focus on sharing specs &amp; traces with colleagues.</li>
</ul>
<h3 id="model-checkers">Model Checkers</h3>
<p>Again moving up the stack.
We now have <em>three</em> model checkers!</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> - mentioned previously, is the only fully-featured model checker.
It’s a finite state model checker, with all the benefits (predictable &amp; understandable performance) &amp; drawbacks (combinatoric state explosion) that implies.
It is the only checker that fully handles liveness, in particular refinement - a banner TLA⁺ language feature, and arguably its <em>raison d’être</em>.
It is written in Java and uses SANY for parsing.</li>
<li><a href="https://github.com/apalache-mc/apalache">Apalache</a> - a symbolic model checker that uses <a href="https://github.com/Z3Prover/z3">Z3</a> to model-check TLA⁺ specifications.
It handles some liveness properties but not all.
It is written in Scala, and also uses SANY for parsing.</li>
<li><a href="https://informal.systems/">Spectacle</a> - this was recently updated to do basic safety checking by running a breadth-first search of the model’s state space, in the browser!
Once you do the difficult work of developing a TLA⁺ interpreter it is not too much extra effort to add safety checking (liveness checking is where it really starts to get hairy).
As covered above, this is written in JavaScript and uses tree-sitter-tlaplus for parsing.</li>
</ul>
<h3 id="others">Others</h3>
<p>Here’s a grab bag of other notable TLA⁺ language tools:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlapm">TLAPM</a> - a system for validating proofs written in TLA⁺’s proof language.
It’s written in OCaml and uses its own parser, described above.
TLA⁺ proofs are translated to obligations for discharge by an array of backend provers including <a href="https://www.cl.cam.ac.uk/research/hvg/Isabelle/">Isabelle</a>, <a href="https://github.com/Z3Prover/z3">Z3</a>, and <a href="https://github.com/zenon-prover/zenon">Zenon</a>.</li>
<li><a href="https://apalache-mc.org/docs/apalache/typechecker-snowcat.html">The Snowcat Type Checker</a> - this is bundled with Apalache and verifies TLA⁺ type annotations encoded in comments.
I think it’s a very neat project that is underutilized.
Modulo some weirdness around conversions between functions and sequences, many industry-produced TLA⁺ specs have variables spanning a small set of values and would benefit from catching type errors at parse time.</li>
<li><a href="https://github.com/FedericoPonzi/tlaplus-formatter">Tlaplus-formatter</a> &amp; <a href="https://github.com/domodwyer/tlafmt">tlafmt</a> - we now have not one but <em>two</em> TLA⁺ formatters!
The former is written in Java and uses SANY for parsing, while the latter is written in Rust and uses tree-sitter-tlaplus.</li>
<li><a href="https://github.com/tlaplus/tlapm/tree/e9b8bb51818f0b454384e8d94fe614899a0aaa78/lsp">LSP Server</a> - written by <a href="https://github.com/kape1395">Karolis Petrauskas</a>, this is aimed at improving support for the TLA⁺ proof language in the TLA⁺ VS Code extension.
It is written in OCaml on top of the TLAPM parser.</li>
<li><a href="https://github.com/tlaplus-community/tlauc">TLAUC</a> - the TLA⁺ Unicode Converter, written by myself in Rust using tree-sitter-tlaplus.
This converts your TLA⁺ specs between their LaTeX-like ASCII and Unicode representations.
I introduced <a href="https://rgoswami.me/post/2024-05-28-tla-unicode/">Unicode support to TLA⁺</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA⁺ conference</a>.
Write your TLA⁺ specs in Unicode!
It’s great!</li>
<li><a href="https://github.com/tlaplus/vscode-tlaplus/">VS Code Extension</a> - slowly replacing the Eclipse-based TLA⁺ Toolbox as the IDE of choice, this is written in TypeScript and uses regexes to cobble together some concept of TLA⁺ parsing.
Don’t get me wrong, it’s really a nice project!
It recently <a href="https://github.com/tlaplus/vscode-tlaplus/pull/367">bundled SANY</a> so is offloading more and more functionality onto a real parser.</li>
</ul>
<h3 id="summing-it-up">Summing it up</h3>
<p>I think the current state of the TLA⁺ tooling ecosystem is <em>strong</em>.
This section was quite long once I wrote it all out.
People don’t just want to use TLA⁺, they want to make it better!
That’s a really good energy to have in the community.
A lot of people don’t like being tied to the JVM but alternatives are slowly taking shape.</p>

<p>Many working software engineers have read (or at least heard of) this book:</p>
<p><img src="https://rgoswami.me/posts/remote-access-cvpn/legacy-code.jpg" alt=""/></p>
<p>It has a concise &amp; objective definition of legacy code that I like: legacy code is code without tests.
However, as I’ve grown in my career I’ve arrived at a fuzzier definition: legacy code is code not in living knowledge.
The codebase or even a particular module does not exist in totality inside the heads of the people who work on it.
We’ve probably all written software from scratch and know how easy it is to make massive changes because you know how every part works.
If you hand the same task to a codebase newcomer you can reasonably expect it to take 10-100x as long to effect the same changes, and the codebase end state will be much less coherent.</p>
<p>We have a large shortfall of living knowledge in the TLA⁺ project, which I will further elaborate in the next section.
The thesis of this section is simple: we must overcome this challenge if TLA⁺ is to <em><strong>thrive</strong></em>.
TLA⁺ will trundle along on inertia for quite a while, but eventually find its way into irrelevance if we do not conquer the challenge before us.</p>
<p>One last note on living knowledge: it is interlinked with tests.
Once living knowledge is lost it must be rebuilt from scratch, and that is only possible if you can write tests for the codebase.
Poke &amp; prod it, see how it flows and is interlinked.
Most of <em>Working Effectively with Legacy Code</em> deals with how to bootstrap your way to safely making changes in a codebase that lacks tests, when writing tests <em>requires</em> changing the codebase to make it testable.</p>
<h3 id="the-challenge-for-tla">The Challenge for TLA⁺</h3>
<p>First, a quick history:
Leslie Lamport published the paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#old-tla-src"><em>A Temporal Logic of Actions</em></a> in April of 1990.
Jean-Charles Gregoir wrote the initial SANY TLA⁺ parser in the late 1990s.
The TLC model checker was written over 25 years ago, announced in the 1999 paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#yuanyu-model-checking"><em>Model Checking TLA⁺ Specifications</em></a> by Yuan Yu and Panagiotis Manolios which described writing an interpreter &amp; safety checker for TLA⁺.
By 2002 the liveness checker <a href="https://lamport.azurewebsites.net/pubs/pubs.html#spec-and-verifying">had been developed</a>.
The tools were subsequently heavily modified around 2010 for the release of <a href="https://lamport.azurewebsites.net/tla/tla2.html">TLA⁺ 2</a>, which added the proof language.
Around this time Inria &amp; Microsoft Research partnered to write TLAPM so the proof language could be formally checked.
Inria funding lapsed in the late 2010s and TLAPM development slowed dramatically.
Development on the Apalache symbolic model checker began in the late 2010s, and accelerated with <a href="https://rgoswami.me/posts/remote-access-cvpn/informal.systems">Informal Systems</a> bringing it in-house in the early 2020s, but development has similarly slowed dramatically since Informal Systems spun out Apalache near the end of 2024.
The <a href="https://foundation.tlapl.us/">TLA⁺ Foundation</a> was founded to support TLA⁺ as it was spun out from Microsoft Research in the leadup to Leslie Lamport’s retirement at the end of 2024.</p>
<p>We will focus on the challenge of maintaining SANY and TLC.
By comparison to SANY &amp; TLC, TLAPM and Apalache are much less approachable by non-academic software engineers and I would be interested in watching talks on how their development can be ramped up &amp; made sustainable.</p>
<p>So, we have a large quarter-century-old Java codebase that is difficult to unit test due to prolific global static state, where all the original authors have long since left the project, and where the three regular core contributors lack knowledge of its many parts.
Each of us focuses on a different area: I tend to work on the parser, <a href="https://github.com/lemmy">Markus Kuppe</a> tends to work on the model checker, and <a href="https://calvin.loncaric.us/">Calvin Loncaric</a> tends to work on the interpreter &amp; is also our resident true Java expert.
There are many things we want to do but are restrained by the unknowns.
This all sounds somewhat dire, so why am I optimistic?</p>
<h3 id="the-dream-and-our-blessings">The Dream and Our Blessings</h3>
<p>Let’s start with a dream: <strong>a world where we are bold and unafraid to make large changes to the core TLA⁺ tools</strong>.
How do we get there?</p>
<p>Thankfully, we start from a position of strength.
Language tooling is a blessed area of software engineering.
It is one of the few places where program requirements are relatively complete &amp; unambiguous.
It is actually possible to <em>complete</em> a parser!
Usually starting a software project is kind of like casting a curse upon yourself, where you must continually provide it your labor or it will disappear from relevance and all your work will be for nought.
Not so here.
Of course, conventional language tooling demands do become stronger over time.
Parsers have to be useful in a language server context now, which requires better error recovery than was previously standard.
But these are from relatively slow changes in development culture and only crop up every decade or so.</p>
<h3 id="concrete-strategies">Concrete Strategies</h3>
<p>Enough faffing around.
Here are the three strategies I think will bring us from dream to reality:</p>
<ol>
<li><strong>Tests</strong> - Of course!
The TLA⁺ Foundation has been funding me to put a <em>ton</em> of effort into developing <a href="https://github.com/tlaplus/rfcs/tree/2a772d9dd11acec5d7dedf30abfab91a49de48b8/language_standard/tests">standardized implementation-independent test suites</a> for TLA⁺ parsers, then applying them to the existing parsers.
We now have a nice source-code-input/expected-AST-output syntax suite applied to the three prominent parsers (good for conformance!) along with a semantic-level test suite with lots of in-language assertions about levels and references (thanks to <a href="https://lobste.rs/s/58jah7/what_implementation_independent_test">lobste.rs users</a> for some great suggestions on its design) and - my favorite - a set of 150ish TLA⁺ parse inputs that should each trigger a specific standardized error code in any conformant parser.
Combined, I think these make every TLA⁺ parser open for development!
We are no longer constrained to analyzing bugs by git-bisecting &amp; comparing inscrutable code; just write a test and trace it in the debugger!</li>
<li><strong>Developer Onboarding</strong> - We must go beyond documentation.
Treat the codebase as a fixed artifact to be studied &amp; learned by any new contributor, then develop a syllabus for it.
The TLA⁺ Foundation has been funding me for this in various ways, such as writing <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/DEVELOPING.md">developing</a> &amp; <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/CONTRIBUTING.md">contributing</a> guides, highlighting good first issues in <a href="https://foundation.tlapl.us/blog/index.html">monthly development updates</a>, and - most substantially - writing a long tutorial on <a href="https://docs.tlapl.us/creating:start">how to write your <em>own</em> minimal TLA⁺ model checker</a>!
This last one is very fun and is based on the excellent free online textbook <a href="https://craftinginterpreters.com/"><em>Crafting Interpreters</em></a> by Robert Nystrom.
Give it a look!</li>
<li><strong>Grants &amp; Stipends</strong> - The fundamental contradiction of FOSS development is that every substantial FOSS project requires <em>full-time</em> commitment for long periods of time, but people need to eat and it’s hard to make money giving something away for free.
When I first got into this game I thought Linux was something people developed on their evenings and weekends.
That’s true for some people, but the overwhelming majority of code is contributed by people working on Linux full-time!
A lot of digital ink has been spilled about how to manage the fundamental FOSS contradiction.
Foundations with corporate sponsorship are one method, and that’s what we have.
The TLA⁺ Foundation can fund people to work on the tools, and they are.
Interested?
<a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">Join us</a>!</li>
</ol>
<h3 id="summing-it-up-1">Summing it up</h3>
<p>So, we are starting from a position of strength and have concrete strategies to overcome the remaining challenges.
We should also recognize that our struggle is shared.
People in the TLA⁺ community occasionally say things like “TLA⁺ is not a programming language”, but we face the same challenges as the developers of every other programming language out there!
Even the exotic parts of TLA⁺ are “just” algorithms.
By which I mean learning algorithms is relatively easy, fun, &amp; contained compared to the challenge of developing an unknown codebase.
As complicated as the liveness checking algorithm is, it’s “just” an algorithm on graphs that an ordinary developer can learn in a few days of study, given a suitable tutorial.
And it’s our job to write that tutorial and make it fun.</p>

<p>I ended the talk with a few ideas I had for TLA⁺ tool development in the near future.
One, inspired by attendance at <a href="https://bugbash.antithesis.com">Antithesis BugBash</a> last month, is generative testing for the TLA⁺ tools!
It’s all well &amp; good to lock me in an office to crank out hundreds of test cases, but what if that process could be automated?
It would certainly be more fun to develop than a large static test corpus.
Actually it’s good to have both: static test cases to nail down the core language functionality, and generative tests to explore the nooks &amp; crannies.</p>
<p>I also tentatively proposed simplifications to the TLA⁺ syntax.
I’m a bit of a weird guy who <em>likes</em> the TLA⁺ syntax (especially in its Unicode form) but other people do struggle with it, as <a href="https://www.hillelwayne.com/">Hillel Wayne</a> can attest from his work teaching TLA⁺ both online and offline.
I’m not a purist who is against evolving the syntax.
I have what I think is <a href="https://github.com/tlaplus/rfcs/issues/10">quite a nice RFC</a> to combine &amp; disambiguate the set filter &amp; set map syntax.
Larger changes will have to be debated.
This is design &amp; consensus work; the actual code changes will likely not be substantial in comparison.</p>
<p>I also want to improve the SANY API &amp; consumption experience so people can easily build their own tools instead of requesting features be upstreamed.
A new API has <a href="https://github.com/tlaplus/tlaplus/pull/1125">already been prototyped</a>, and all SANY unit tests are currently exercising it.
Markus has also put effort into automatically <a href="https://oss.sonatype.org/content/repositories/snapshots/org/lamport/tla2tools/">publishing a Java package</a> from the rolling release.
Finally, we need to write a collection of how-to guides for consuming &amp; developing against the package.
There are actually quite a few projects &amp; features in TLA⁺ land that exist but have not had documentation written &amp; disseminated for them.
I think I could spend an entire year writing only tutorials &amp; documentation.
I am coming closer to being persuaded that would actually be a good use of time, maybe even the best use of my time for TLA⁺.</p>
<h3 id="the-one-billion-states-per-minute-initiative">The One Billion States Per Minute Initiative</h3>
<p>I wanted to end the talk with a bang so here it is: I think we can increase the TLC model checker throughput to 1 billion states per minute (1000x speedup) by writing a <em>bytecode interpreter</em>.
Currently, TLC clocks about 1 million states per minute.
Calvin has done some <em>very</em> preliminary prototyping with transpiling TLA⁺ specs to C++ then compiling &amp; running them as little breadth-first search programs, and saw some head-turning numbers.
The TLC interpreter is a tree-walk interpreter written in Java.
As optimized as it is, there’s a limit to what can be done with the format.
This would be a bold, substantial project but I think it is worth exploring.</p>

<p>Thanks for reading!
I hope this has invigorated &amp; inspired you to consider developing your own TLA⁺ tooling or contribute to the existing tools, while giving a nice overview of the current state of things.</p>

<p><a href="https://cse.buffalo.edu/~demirbas/">Murat Demirbas</a> wrote <a href="https://muratbuffalo.blogspot.com/2025/05/modular-verification-of-mongodb.html">a blog post</a> version of his talk at the TLA⁺ community event, on using TLA⁺ to specify cross-shard transactions in MongoDB.
He also wrote a post <a href="https://muratbuffalo.blogspot.com/2025/05/notes-from-tla-community-event.html">summarizing other talks</a> from the event.</p>
<p><a href="https://emptysqua.re/blog/about/">A. Jesse Jiryu Davis</a> wrote <a href="https://emptysqua.re/blog/are-we-serious-about-statistical-properties-tlaplus/">a blog post</a> version of his TLA⁺ community event talk, on the possibility of adding statistical &amp; performance modeling capabilities to TLA⁺.
He also posted <a href="https://emptysqua.re/blog/2025-tlaplus-community-event/">his notes</a> about other talks at the event.</p>
<p>Discuss this post here:</p>
<ul>
<li><a href="https://lobste.rs/s/auqf51/current_state_tla_development">lobste.rs</a></li>
<li><a href="https://discuss.systems/@ahelwer/114513355067720408">Mastodon</a></li>
<li><a href="https://news.ycombinator.com/item?id=43998115">Hacker News</a></li>
<li><a href="https://www.linkedin.com/posts/ahelwer_the-current-state-of-tla-development-activity-7328855373866504192-uv45">LinkedIn</a></li>
</ul>


        
          
        

        

        
      </article>

      
        
      


      

    </div>
  </div>
</div></div>
  </body>
</html>
