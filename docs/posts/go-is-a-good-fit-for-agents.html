<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://docs.hatchet.run/blog/go-agents">Original</a>
    <h1>Go is a good fit for agents</h1>
    
    <div id="readability-page-1" class="page"><article><main><div><p><span title="Blog">Blog</span></p><svg viewBox="0 0 24 24" stroke="currentColor" fill="none" stroke-width="2" height="14"><path d="M9 5l7 7-7 7" stroke-linecap="round" stroke-linejoin="round"></path></svg><p><span title="Why Go is a good fit for agents">Why Go is a good fit for agents</span></p></div>
<p><em>Since you’re here, you might be interested in checking out <a href="https://hatchet.run" target="_blank" rel="noreferrer">Hatchet</a> — the platform for running background tasks, data pipelines and AI agents at scale.</em></p>
<div><h5><p>Alexander Belanger</p></h5><p>Published on June 3, 2025</p></div>
<p>Like seemingly everyone else on the planet, we’ve been spending the last few months fussing over agents.</p>
<p>In particular, we’ve been seeing the adoption of agents drive the growth of our orchestration platform, so we have some insight into what sorts of stacks and frameworks — <a href="https://github.com/humanlayer/12-factor-agents" target="_blank" rel="noreferrer">or lack thereof</a> — work well here.</p>
<p>One of the more interesting things we’ve seen is a proliferation of hybrid stacks: a typical Next.js or FastAPI backend, coupled with an <em>agent written in Go,</em> even at a very early stage.</p>
<p>As a long-time Go developer, this is rather exciting; here’s why I think this will be a more common approach moving forward.</p>
<h2 id="whats-an-agent">What’s an agent?<a href="#whats-an-agent" aria-label="Permalink for this section"></a></h2>
<p>Terminology here is muddled, but in general I’m referring to a process which is executing in a loop, where the process has some agency over the next step in its execution path. Contrast this with a predefined execution path, like a set of steps defined as a directed acyclic graph, which we’d call a <a href="https://www.anthropic.com/engineering/building-effective-agents" target="_blank" rel="noreferrer">workflow</a>. Agents often contain an exit condition based on maximum depth or a condition (like “tests pass”) being met.</p>
<img src="https://docs.hatchet.run/anthropic_agent.png" alt="Anthropic agent architecture"/>
<p><em>Source: <a href="https://www.anthropic.com/engineering/building-effective-agents" target="_blank" rel="noreferrer">https://www.anthropic.com/engineering/building-effective-agents</a></em></p>
<p>Agents typically have a number of shared characteristics when they start to scale (read: <em>have actual users</em>):</p>
<ol>
<li>They are long-running — anywhere from seconds to minutes to hours.</li>
<li>Each execution is expensive — not just the LLM calls, but the nature of the agent is to replace something that would typically require a human operator. Development environments, browser infrastructure, large document processing — these all cost $$$.</li>
<li>They often involve input from a user (or another agent!) at some point in their execution cycle.</li>
<li>They spend a lot of time awaiting i/o or a human.</li>
</ol>
<p>Let’s translate this set of characteristics into runtime. To constrain the problem, let’s assume that we’re dealing with an agent that is executed <em>remotely</em> and not on a user’s machine (though Go would be a great choice for distributing a local agent as well). In the case of remote execution, it would be incredibly expensive to run a separate container for each agent execution, so in most cases (particularly when our agent is simple i/o and LLM calls), we are going to end up with a bunch of lightweight processes that are running concurrently. Each process can be in a given state (for example, “Searching files,” “Generating code,” “Test”). Note that the ordering of states is not the same in for different agent executions.</p>
<img src="https://docs.hatchet.run/agent_architecture.png" alt="Pseudo agent architecture"/>
<p>This system of many concurrent, long-running processes is quite different from a traditional web architecture of ~10 years ago, where requests to a server were much faster, and thousands of daily users could be served efficiently using <a href="https://nickcraver.com/blog/2013/11/22/what-it-takes-to-run-stack-overflow/" target="_blank" rel="noreferrer">some caches, efficient handlers and OLTP databases</a>.</p>
<p>It turns out, this shift in architecture is really well-suited to Go’s concurrency model, reliance on channels for communication, centralized cancellation mechanism, and tooling built around i/o.</p>
<h2 id="high-concurrency">High concurrency<a href="#high-concurrency" aria-label="Permalink for this section"></a></h2>
<p>Let’s start with the obvious — Go has an incredibly simple and powerful concurrency model. Spawning a new goroutine costs very little memory and time, as there’s only 2kb of pre-allocated memory per goroutine.</p>
<img src="https://docs.hatchet.run/go_threading.png" alt="Go threading model"/>
<p><em>Source: <a href="https://www.youtube.com/watch?v=KBZlN0izeiY&amp;t=586s" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=KBZlN0izeiY&amp;t=586s</a></em></p>
<p>Effectively this means that you can run many goroutines at once with little overhead, and they run on <em>multiple OS threads</em> under the hood which take advantage of all CPU cores in your server. This is rather important, because if you <em>happen</em> to do something very CPU-intensive in a goroutine (like deserializing a large JSON payload), you will see less of an impact than if you were using a single-threaded runtime like Node.js (where you’d need to spawn out to a worker thread or child process for something that blocks the thread) or Python’s async/await.</p>
<p>What does this mean for agents? Because agents are longer-running than a typical web request, concurrency becomes a much greater point of concern. In Go, you’re much less likely to be constrained by spawning a goroutine per agent than if you ran a thread per agent in Python or an async function per agent in Node.js. Couple this with a lower baseline memory footprint and compilation into a single binary, and it becomes incredibly easy to run thousands of concurrent agent executions at the same time on lightweight infrastructure.</p>

<p>For those unaware, there’s a common Go idiom that says: <em>Do not communicate by sharing memory; instead, share memory by communicating.</em></p>
<p>In practice, this means that instead of attempting to synchronize the contents of memory across many concurrent processes (a common problem when using something like Python’s <code dir="ltr">multithreading</code> libraries), each process can acquire ownership over an object by acquiring and releasing it over a channel. The effect is that each process is only concerned about the local state of an object while it has ownership over that object, but otherwise doesn’t need to coordinate ownership — no mutexes necessary!</p>
<p>To be quite honest — in most Go programs I’ve written, I’ve often used wait groups and mutexes more often than channels, because it’s often simpler (which is <a href="https://go.dev/wiki/MutexOrChannel" target="_blank" rel="noreferrer">in line</a> with the Go community’s recommendations) and there’s only one location where data gets accessed concurrently.</p>
<p>But this paradigm is useful when modeling agents, because an agent often needs to asynchronously respond to messages from a user or another agent, and it’s helpful to think about an instance of your application as a pool of agents.</p>
<p>To make this more concrete, let’s write some example code to represent the core of an agentic loop:</p>
<div><pre tabindex="0"><code dir="ltr"><span><span>// NOTE: in a real-world example, we&#39;d want a mechanism to gracefully</span></span>
<span><span>// shut down the loop and protect against channel</span></span>
<span><span>// closure; this is a simplified example.</span></span>
<span><span>func</span><span> Agent</span><span>(</span><span>in</span><span> &lt;-chan</span><span> Message</span><span>, </span><span>out</span><span> chan&lt;-</span><span> Output</span><span>, </span><span>status</span><span> chan&lt;-</span><span> State</span><span>) {</span></span>
<span><span>	internal </span><span>:=</span><span> make</span><span>(</span><span>chan</span><span> Message</span><span>, </span><span>10</span><span>)</span></span>
<span> </span>
<span><span>	for</span><span> {</span></span>
<span><span>		select</span><span> {</span></span>
<span><span>		case</span><span> msg </span><span>:=</span><span> &lt;-</span><span>internal:</span></span>
<span><span>			processMessage</span><span>(msg, internal, out, status)</span></span>
<span><span>		case</span><span> msg </span><span>:=</span><span> &lt;-</span><span>in:</span></span>
<span><span>			processMessage</span><span>(msg, internal, out, status)</span></span>
<span><span>		}</span></span>
<span><span>	}</span></span>
<span><span>}</span></span>
<span> </span>
<span><span>func</span><span> processMessage</span><span>(</span><span>msg</span><span> Message</span><span>, </span><span>internal</span><span> chan&lt;-</span><span> Message</span><span>, </span><span>out</span><span> chan&lt;-</span><span> Output</span><span>, </span><span>status</span><span> chan&lt;-</span><span> State</span><span>) {</span></span>
<span><span>	result </span><span>:=</span><span> execute</span><span>(msg)</span></span>
<span><span>	status </span><span>&lt;-</span><span> State</span><span>{msg.sessionId, result.status}</span></span>
<span> </span>
<span><span>	if</span><span> next </span><span>:=</span><span> result.</span><span>next</span><span>(); next </span><span>!=</span><span> nil</span><span> {</span></span>
<span><span>		internal </span><span>&lt;-</span><span> next</span></span>
<span><span>	}</span></span>
<span> </span>
<span><span>	out </span><span>&lt;-</span><span> result</span></span>
<span><span>}</span></span></code></pre></div>
<p>(Note that the <code dir="ltr">&lt;-chan</code> means that the receiver can <em>only read</em> from the channel, while the <code dir="ltr">chan&lt;-</code> means that the receiver can <em>only write</em> to the channel.)</p>
<p>This agent is a long-running process which waits for messages to arrive on the <code dir="ltr">in</code> channel, processes the message, and then asynchronously sends the result to the <code dir="ltr">out</code> channel. The <code dir="ltr">status</code> channel is used to send updates about the agent’s state, which can be useful for monitoring or sending incremental results to a user, while the <code dir="ltr">internal</code> channel is used to handle internal agent loops. For example, the internal loop could implement the “Until tests pass” loop in the diagram below:</p>
<img src="https://docs.hatchet.run/anthropic_agent_call_graph.png" alt="Call graph example"/>
<p><em>Source: <a href="https://www.anthropic.com/engineering/building-effective-agents" target="_blank" rel="noreferrer">https://www.anthropic.com/engineering/building-effective-agents</a></em></p>
<p>Even though we’re running the agent as a <code dir="ltr">for</code> loop, the instance of the agent doesn’t have any internal state that it needs to maintain between messages. It’s effectively a <a href="https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-12-stateless-reducer.md" target="_blank" rel="noreferrer">stateless reducer</a>, which doesn’t depend on some internal state to make a decision about the next step in its execution path. Importantly, this means that <em>any instance of the agent is able to process the next message</em>. This also allows for the agent to use a durable boundary in between messages, for example writing messages to a database or message queue.</p>
<p>This is a toy example — for a more concrete walkthrough of this Go idiom, much of this is inspired by <a href="https://go.dev/doc/codewalk/sharemem/" target="_blank" rel="noreferrer">this codewalk</a>.</p>
<h2 id="centralized-cancellation-mechanism-with-contextcontext">Centralized cancellation mechanism with <code dir="ltr">context.Context</code><a href="#centralized-cancellation-mechanism-with-contextcontext" aria-label="Permalink for this section"></a></h2>
<p>Remember how agents are expensive? Let’s say a user triggers a $10 execution, and suddenly changes their mind and hits “stop generating” — to save yourself some money, you’d like to cancel the execution.</p>
<p>It turns out, cancelling long-running work in Node.js and Python is incredibly difficult for multiple reasons:</p>
<ol>
<li>Libraries can’t agree on a shared mechanism to actually cancel work — while both languages have support for abort signals and controllers, it doesn’t guarantee that your third-party library calls will respect those signals.</li>
<li>If that fails, forcefully terminating threads <a href="https://stackoverflow.com/questions/323972/is-there-any-way-to-kill-a-thread" target="_blank" rel="noreferrer">is a miserable process</a> and can cause thread leakage or corrupted resources.</li>
</ol>
<p>Luckily, Go’s adoption of <code dir="ltr">context.Context</code> makes it trivial to cancel work, because the vast majority of libraries expect and respect this pattern. And even if they don’t: because there’s only one concurrency model in Go, there’s a bunch of tooling like <code dir="ltr">goleak</code> which makes it much easier to catch leaking goroutines and problematic libraries.</p>
<h2 id="expansive-standard-library">Expansive standard library<a href="#expansive-standard-library" aria-label="Permalink for this section"></a></h2>
<p>When you start working in Go, you’ll immediately notice that the Go standard library is expansive and very high quality. Many parts of it are also built for web i/o — like <code dir="ltr">net/http</code>, <code dir="ltr">encoding/json</code> , and <code dir="ltr">crypto/tls</code> — which are useful for the core logic of your agent.</p>
<p>There’s also an implicit assumption that all i/o is blocking within a goroutine — again, because there’s only one way to run work concurrently — which encourages the core of your business logic to be written as <a href="https://en.wikipedia.org/wiki/Straight-line_program" target="_blank" rel="noreferrer">straight-line programs</a>. You don’t need to worry about deferring execution to the scheduler with an <code dir="ltr">await</code> wrapping each function call.</p>
<p>Contrast this with Python: library developers need to think about asyncio, multithreading, multiprocessing, eventlet, gevent, and some other patterns, and it’s nearly impossible to support all concurrency models equally. As a result, if you’re writing your agent in Python, you’ll need to research each library’s support for your concurrency model, and potentially adopt multiple patterns if your third-party libraries don’t support exactly what you’d like.</p>
<p>(The story is much better in Node.js, though the addition of other runtimes like Bun and Deno has added some layers of incompatibility.)</p>
<h2 id="profiling">Profiling<a href="#profiling" aria-label="Permalink for this section"></a></h2>
<p>Agents seem to be quite susceptible to memory leaks because of their statefulness and thread leaks because of the number of long-running processes. Go has great tooling in <code dir="ltr">runtime/pprof</code> for figuring out the source of a memory leak using the heap and alloc profiles, or the source of a goroutine leak using the goroutine profiles.</p>
<img src="https://docs.hatchet.run/pprof.png" alt="Example pprof memory profile"/>
<p><em>Source: One of my more embarrasing memory leaks</em></p>
<h2 id="bonus-llms-are-good-at-writing-go-code">Bonus: LLMs are good at writing Go code<a href="#bonus-llms-are-good-at-writing-go-code" aria-label="Permalink for this section"></a></h2>
<p>Because Go has a very simple syntax (a common criticism being that Go is “wordy”) and has an expansive standard library, LLMs are quite good at writing idiomatic Go code. I’ve found that they’re particularly good at writing table tests, which are a common pattern in Go codebases.</p>
<p>Go engineers also tend to be anti-framework, which means that LLMs don’t need to track which framework (or version of a framework) you’re using.</p>
<h2 id="the-bad-parts">The bad parts<a href="#the-bad-parts" aria-label="Permalink for this section"></a></h2>
<p>Given all of these benefits, there are still a lot of reasons not to use Go for your agent:</p>
<ol>
<li>Third-party support still lags behind Python and Typescript</li>
<li>Using Go for anything that involves real machine learning is nearly impossible</li>
<li>If you’re looking for the <em>best performance possible</em>, there are better languages like Rust and C++</li>
<li>You’re a maverick and don’t like to handle errors (<a href="https://go.dev/blog/error-syntax" target="_blank" rel="noreferrer">you’re not alone</a>)</li>
</ol>
<hr/>
<div><div><h3>Subscribe for more technical deep dives</h3><p>Stay updated with our latest work. We share insights about distributed systems, workflow engines, and developer tools.</p></div></div></main></article></div>
  </body>
</html>
