<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://metaphysic.ai/nerf-successor-deepfakes/">Original</a>
    <h1>NeRF: An Eventual Successor for Deepfakes?</h1>
    
    <div id="readability-page-1" class="page"><div data-id="230321ec" data-element_type="widget" data-widget_type="theme-post-content.default"><div><div data-elementor-type="wp-post" data-elementor-id="2025"><section data-id="06efe1a" data-element_type="section"><div><div data-id="3608a2c" data-element_type="column"><div><div data-id="f2d8953" data-element_type="widget" data-widget_type="text-editor.default"><div><p>In February of 2022, a <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.2120481119">study</a> from Lancaster University in the UK found not only that most people cannot distinguish deepfake faces from real faces, but also that we tend to trust synthesized faces more.</p><p>While this is excellent media fodder, and while there’s little doubt that AI-generated humans have increased in quality since the advent of deepfakes in 2017, it’s worth considering the extent to which our discriminatory powers and standards for authenticity are likely to rise in line with improvements in deepfake generation; that there’s still a long way to go before machine learning can reproduce humans with complete fidelity; and that the technologies that are currently dazzling us are not necessarily the ones that will achieve the next evolutionary leap in this sphere.</p><p>Most of the attention-garnering headlines regarding deepfakes are referring to the use of two open source packages that became popular after deepfakes entered the public arena in 2017: <a href="https://github.com/iperov/DeepFaceLab">DeepFaceLab</a> (DFL) and <a href="https://github.com/deepfakes/faceswap">FaceSwap</a>.</p></div></div><div data-id="19d37fe" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="352" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/deepfacelab-and-faceswap-1024x451.jpg" alt="deepfacelab-and-faceswap" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/deepfacelab-and-faceswap-1024x451.jpg 1024w, https://metaphysic.ai/wp-content/uploads/2022/06/deepfacelab-and-faceswap-300x132.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/deepfacelab-and-faceswap-768x339.jpg 768w, https://metaphysic.ai/wp-content/uploads/2022/06/deepfacelab-and-faceswap.jpg 1100w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption> DeepFaceLab (lower right) and FaceSwap (left), both derived from the original 2017 deepfakes source code, have come to dominate deepfake production workflows.</figcaption></figure></div></div><div data-id="87bb7b5" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Though each of these packages boasts a wide user-base and an active community of developers, neither project has significantly departed from the now five year-old <a href="https://github.com/joshua-wu/deepfakes_faceswap">GitHub code</a>, which was almost immediately abandoned by its enigmatic creator when controversy began to build over the startling new technology.</p><p>The DFL and FaceSwap developers have not been idle, for sure: it’s now possible to use larger input images for training deepfake models (see image below), though this requires more expensive video cards; masking out occlusions (such as hands in front of faces) in deepfakes has been semi-automated by innovations such as <a href="https://www.youtube.com/watch?v=ljMXS8vovx4">XSEG training</a>; and the often ungainly command line environment of DFL has been integrated into the more user-friendly ministrations of the <a href="https://github.com/MachineEditor/MachineVideoEditor">Machine Editor</a> graphical host environment.</p></div></div><div data-id="2662816" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="1000" height="436" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/maximum-training-sizes.jpg" alt="Maximum Training" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/maximum-training-sizes.jpg 1000w, https://metaphysic.ai/wp-content/uploads/2022/06/maximum-training-sizes-300x131.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/maximum-training-sizes-768x335.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"/><figcaption>Between 2018 and 2021, the maximum size of input training data has gone up for DeepFaceLab, though it’s not a &#39;magic bullet&#39; in terms of obtaining more realistic results. Source: https://github.com/iperov/DeepFaceLab</figcaption></figure></div></div><div data-id="1b917a2" data-element_type="widget" data-widget_type="text-editor.default"><p>But in truth, the improvements in deepfake quality that so many media outlets <a href="https://www.cbsnews.com/news/deepfake-artificial-intelligence-60-minutes-2021-10-10/">have noted</a> over the past three years have been primarily due to end-users gaining time-consuming and hard-won experience in data gathering; in discovering the best ways to train models (where it can take several weeks to run even a single experiment); and generally learning to fully exploit and extend the outermost limits of the original 2017 code.</p></div></div></div></div></section><section data-id="0125a49" data-element_type="section"><div><div data-id="b9a3b66" data-element_type="column"><div><div data-id="f76c318" data-element_type="widget" data-widget_type="heading.default"><p><h2>The Challenge of &#39;Scaling Up&#39;</h2></p></div></div></div></div></section><section data-id="cfa8089" data-element_type="section"><div><div data-id="20e0288" data-element_type="column"><div><div data-id="5910817" data-element_type="widget" data-widget_type="text-editor.default"><p>Among a myriad of new innovations in the VFX and ML research communities, some are attempting to break through the ‘hard limits’ of the popular deepfake packages by extending the architecture so that a machine learning model can train on images as large as 1024×1024 pixels – double the current practical confines of DeepFaceLab or FaceSwap, and nearer to the kind of resolution that would be useful in film and television production:</p></div></div></div></div></section><section data-id="e52edf5" data-element_type="section"></section><section data-id="b853e18" data-element_type="section"><div><div data-id="abc8db9" data-element_type="column"><div><div data-id="c60d596" data-element_type="widget" data-widget_type="text-editor.default"><div><p>We’ll take a deeper look at this proprietary technique when we chat with its creator, in a later article on autoencoder-based deepfakes.</p><p>However, results as impressive as these are difficult to obtain with standard open source deepfakes software; require expensive and powerful hardware; and usually entail very long training times to obtain very limited sequences.</p><p>Machine learning models are trained and developed within the capacity of the VRAM and <a href="https://cvw.cac.cornell.edu/GPUarch/tensor_cores">tensor cores</a> on a single video card — a prospect that becomes more and more challenging in the age of <a href="https://spectrum.ieee.org/andrew-ng-data-centric-ai">hyperscale datasets</a>, and which presents some specific obstacles to improving deepfake quality.</p><p>Approaches that shunt training cycles <a href="https://arxiv.org/pdf/2101.06840.pdf">to the CPU</a>, or divide the workload up among multiple GPUs via <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/data-parallelism-task-parallel-library">Data Parallelism</a> or <a href="https://huggingface.co/docs/transformers/parallelism">Model Parallelism</a> techniques (we’ll examine these more closely in a later article) are still in the early stages. For the near future, a single-GPU training setup remains the most common scenario.</p><p>Consequently, improvements in deepfake techniques must work around this architectural bottleneck. For instance, in order to train a deepfake model on 5-10,000 source images, those images must be passed through the ‘live’ area of the training architecture in limited <a href="https://galaxyinferno.com/epochs-iterations-and-batch-size-deep-learning-basics-explained/">batches</a>. The larger the input image size (512×512 pixels is currently considered quite large), the smaller the batch must be, even on the most expensive and best-specced cards available.</p></div></div></div></div></div></section><section data-id="8968cf8" data-element_type="section"><div><div data-id="328b0ef" data-element_type="column"><div><div data-id="4a72326" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="307" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/batch-sizes_NerF_article-1024x393.jpg" alt="Batch Size" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/batch-sizes_NerF_article-1024x393.jpg 1024w, https://metaphysic.ai/wp-content/uploads/2022/06/batch-sizes_NerF_article-300x115.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/batch-sizes_NerF_article-768x295.jpg 768w, https://metaphysic.ai/wp-content/uploads/2022/06/batch-sizes_NerF_article.jpg 1176w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption>Larger training images cut down batch sizes during model training. Some space on the GPU must also be sacrificed to accommodate the software architecture.</figcaption></figure></div></div><div data-id="2983365" data-element_type="widget" data-widget_type="text-editor.default"><p>Small batches can hinder the model from ‘generalizing’ at an optimal level, risking <a href="https://www.ibm.com/cloud/learn/overfitting">overfitting</a> or <a href="https://www.ibm.com/cloud/learn/underfitting">under-fitting</a>. In these cases, the final model only operates well on the original data, or else fails to distil the essential features of the data — and in either eventuality, does not obtain a useful and flexible solution.</p></div><div data-id="fa6ac41" data-element_type="widget" data-widget_type="heading.default"><p><h2>Neural Radiance Fields (NeRF)</h2></p></div></div></div></div></section><section data-id="c272d04" data-element_type="section"><div><div data-id="32ff3c4" data-element_type="column"><div><div data-id="752a332" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Though popular branches of the 2017 deepfakes code (such as DFL and FaceSwap) may eventually benefit from better multi-GPU deployments, innovations in GPU architecture, and the restoration of chip production after <a href="https://www.forbes.com/sites/forbestechcouncil/2021/11/17/why-the-global-chip-shortage-is-lasting-so-long-and-why-it-will-have-a-positive-end/">years of GPU famine</a>, their architecture is quite brittle, and not necessarily amenable to some of the most interesting new developments in human image synthesis.</p><p>It’s therefore possible that new and less constrained methods will eventually provide the next evolutionary leaps in deepfake technologies.</p><p>One such approach is <a href="https://www.matthewtancik.com/nerf">Neural Radiance Fields</a> (NeRF), which emerged in 2020 as a method of recreating objects and environments by stitching together multiple viewpoint photos inside a neural network.</p></div></div></div></div></div></section><section data-id="e1f6561" data-element_type="section"><div><div data-id="62c3cb4" data-element_type="column"><div><div data-id="9fd37fb" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="296" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/Neural-radiance-fields-nerf-photogrammetry.jpg" alt="Neural Radiance" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/Neural-radiance-fields-nerf-photogrammetry.jpg 912w, https://metaphysic.ai/wp-content/uploads/2022/06/Neural-radiance-fields-nerf-photogrammetry-300x111.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/Neural-radiance-fields-nerf-photogrammetry-768x284.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption>NeRF&#39;s photogrammetry: virtual light rays are calculated along the estimated geometry and RGB values of a scene. Source: https://www.matthewtancik.com/nerf</figcaption></figure></div></div></div></div></div></section><section data-id="d4ce81d" data-element_type="section"><div><div data-id="5ffadef" data-element_type="column"><div><div data-id="989d586" data-element_type="widget" data-widget_type="text-editor.default"><p>Given a limited number of viewpoints, NeRF calculates the ‘missing views’ by recognizing shapes, textures, transparency, and lighting values, and estimating and synthesizing the views that aren’t present in the source data.</p></div></div></div></div></section><section data-id="fde2741" data-element_type="section"><div><div data-id="b82facf" data-element_type="column"><div><div data-id="55954fa" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="659" height="457" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/nerf-coverage.jpg" alt="Nerf Coverage" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/nerf-coverage.jpg 659w, https://metaphysic.ai/wp-content/uploads/2022/06/nerf-coverage-300x208.jpg 300w" sizes="(max-width: 659px) 100vw, 659px"/><figcaption>Here we see the origin of the term &#39;radiance&#39; in Neural Radiance Fields, as the training data&#39;s various viewpoints &#39;radiate out&#39; from the object being assimilated into the neural network. Source: https://www.youtube.com/watch?v=EpH175PY1A0</figcaption></figure></div></div></div></div></div></section><section data-id="491a6a2" data-element_type="section"><div><div data-id="b0171d0" data-element_type="column"><div><div data-id="3a13cbe" data-element_type="widget" data-widget_type="text-editor.default"><p><span>The NeRF process maps the entire volume of the target scene, which is now similar to a solid Lego construction made from thousands of bricks. </span></p></div></div></div></div></section><section data-id="afe435e" data-element_type="section"><div><div data-id="877b567" data-element_type="column"><div><div data-id="a50b006" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="190" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/neural-sparse-voxel-fields.jpg" alt="Neural Sparse" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/neural-sparse-voxel-fields.jpg 1000w, https://metaphysic.ai/wp-content/uploads/2022/06/neural-sparse-voxel-fields-300x71.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/neural-sparse-voxel-fields-768x182.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption>Neural Sparse</figcaption></figure></div></div></div></div></div></section><section data-id="d59725b" data-element_type="section"><div><div data-id="95f37a1" data-element_type="column"><div><div data-id="7997554" data-element_type="widget" data-widget_type="text-editor.default"><p>In terms of ‘classic’ CGI techniques, these blocks are equivalent to traditional CGI <a href="https://news.mit.edu/2020/versatile-building-blocks-1118">voxels</a>, in that they are mapped to three-dimensional coordinates in a restricted volume of space, and can then be explored in a 2D volume rendering.</p></div></div></div></div></section><section data-id="0cda86f" data-element_type="section"><div><div data-id="b52389c" data-element_type="column"><div><div data-id="670c5d1" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="600" height="350" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/NeRF-in-real-time.gif" alt="NeRF-in-real-time"/><figcaption>By &#39;baking&#39; information in a NeRF, this 2021 project from Google Research enabled real-time rendering of a NeRF environment on a laptop, at frame rates above 30fps. Note the controlling cursor. Source: https://phog.github.io/snerg/</figcaption></figure></div></div></div></div></div></section><section data-id="30fa88b" data-element_type="section"></section><section data-id="37177a2" data-element_type="section"><div><div data-id="b9e1fe2" data-element_type="column"><div><div data-id="ce59c1d" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Together with storage requirements, the most significant obstacle to the deployment and development of NeRF-based VFX pipelines has been the extensive training times required for earlier implementations.</p><p>For instance, in the original 2020 paper, the training of a single frame took 30 seconds, while an entire scene took 24-48 hours to train to 300,000 iterations over the 32gb of VRAM in a NVIDIA V100 GPU.</p><p>Though later innovations, such as <a href="https://www.youtube.com/watch?v=EpH175PY1A0">Mip-NeRF</a>, <a href="https://alexyu.net/plenoctrees/">Plenoctrees</a>, <a href="http://www.cvlibs.net/publications/Reiser2021ICCV.pdf">KiloNeRF</a>, <a href="https://arxiv.org/pdf/2104.00677.pdf">DietNeRF</a> and <a href="https://alexyu.net/plenoxels/">Plenoxels</a> would eventually offer notable reductions in training times, the announcement of NVIDIA’s own <a href="https://nvlabs.github.io/instant-ngp/">Instant Neural Graphics Primitives</a> (NGP) framework in January of 2022 represents a potential evolutionary leap for NeRF-based image synthesis, offering usable training times as short as <em><i>five seconds</i></em>:</p></div></div></div></div></div></section><section data-id="66864fd" data-element_type="section"><div><div data-id="b976638" data-element_type="column"><div><div data-id="fabe02b" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="286" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/instant_nerf_training_NGP-1.gif" alt="Instant Nerf Training"/><figcaption>NeRF training in seconds, rather than days, under NVIDIA’s new NeRF architecture. Source: https://nvlabs.github.io/instant-ngp/</figcaption></figure></div></div></div></div></div></section><section data-id="877c06a" data-element_type="section"><div><div data-id="1193ff9" data-element_type="column"><div><div data-id="6d18fbe" data-element_type="widget" data-widget_type="text-editor.default"><p>Instant NeRF is able to infer an extraordinary breadth of simulated frames from only a handful of ‘real’ photos:</p></div><div data-id="193f44d" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="450" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/NVIDIA-Instant_NeRF-2022-NeRF_article.gif" alt="NVidia Instant"/><figcaption>NVIDIA&#39;s Instant NeRF derives a complex and explorable neural scene from just four &#39;real&#39; photos, complete with realistic depth of field. Source: https://www.youtube.com/watch?v=DJ2hcC1orc4</figcaption></figure></div></div><div data-id="6f4f2a4" data-element_type="widget" data-widget_type="text-editor.default"><p><span>This extraordinary new economy is achieved in part by </span><i><span>multiresolution hash encoding</span></i><span>, which stores </span><i><span>representative </span></i><span>markers for the high volume of data in the NeRF’s neural network, helping the system to discard any information that does not directly result in image content.</span></p></div></div></div></div></section><section data-id="585a4f8" data-element_type="section"><div><div data-id="a3e86c4" data-element_type="column"><div><div data-id="d895434" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="331" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/discarding-redundant-information-1024x424.png" alt="Discarding Redundant Information" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/discarding-redundant-information-1024x424.png 1024w, https://metaphysic.ai/wp-content/uploads/2022/06/discarding-redundant-information-300x124.png 300w, https://metaphysic.ai/wp-content/uploads/2022/06/discarding-redundant-information-768x318.png 768w, https://metaphysic.ai/wp-content/uploads/2022/06/discarding-redundant-information.png 1100w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption>NVIDIA’s Neural Graphics Primitive workflow uses hashes as representative placeholders for voxel vertices, so that it’s no longer necessary to traverse or consider the entirety of the voxel coordinates (including non-contributing coordinates) in a local neural network. Source: https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="5188afe" data-element_type="section"><div><div data-id="62302db" data-element_type="column"><div><div data-id="e49da8d" data-element_type="widget" data-widget_type="text-editor.default"><p><span>Thus the many thousands of non-visible ‘Lego bricks’ which intervene between the viewer and the depicted objects no longer need to be considered, or initialized, at training time.</span></p></div></div></div></div></section><section data-id="862e6d6" data-element_type="section"><div><div data-id="11f1267" data-element_type="column"><div><div data-id="06f7bf7" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="359" height="270" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/NGP-factory.gif" alt="NGP Factory"/><figcaption>A neural representation of a factory scene under NVIDIA’s NGP. Source: https://github.com/NVlabs/instant-ngp</figcaption></figure></div></div></div></div></div></section><section data-id="390d7e5" data-element_type="section"><div><div data-id="2556ea2" data-element_type="column"><div><div data-id="6505969" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The best of the prior optimization methods, such as Plenoctrees, required the development of a slower, feature-rich NeRF (including redundant information that would later be automatically excised during training) in order to produce surfaces to optimize.</p><p>By contrast, NVIDIA’s method pre-calculates the network’s trainable feature vectors in lightweight hash tables. These hash tables are generated at multiple resolutions (hence ‘parametric’), which can be explored according to the requirements of the viewpoint at any given time.</p><p>An additional benefit of this sparse approach to hashing is that it makes caching far more flexible and capable, leading to a more responsive interface experience.</p></div></div></div></div></div></section><section data-id="2270bc8" data-element_type="section"><div><div data-id="469a6d7" data-element_type="column"><div><div data-id="b9a2f44" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="535" height="311" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/ngp-cache.gif" alt="NGP Cache"/><figcaption>he optimized NGP approach improves caching, explorability and latency of NeRF environments.</figcaption></figure></div></div></div></div></div></section><section data-id="8657619" data-element_type="section"><div><div data-id="e16c024" data-element_type="column"><div><div data-id="5f3219f" data-element_type="widget" data-widget_type="heading.default"><p><h2>Human Representations in NeRF</h2></p></div></div></div></div></section><section data-id="6fd9563" data-element_type="section"><div><div data-id="a2c380c" data-element_type="column"><div><div data-id="207d225" data-element_type="widget" data-widget_type="text-editor.default"><p><span>NeRF was quickly adapted to facilitate the depiction of motion in a neural environment. Since it proved possible to capture the movement of any object, a large portion of the research community has shifted emphasis to the re-creation of human appearance and movement. </span></p></div></div></div></div></section><section data-id="d134998" data-element_type="section"><div><div data-id="b3f3ff1" data-element_type="column"><div><div data-id="9996438" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="600" height="338" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/st-nerf-animated-1.gif" alt="ST Nerf"/><figcaption>Above is a clip of test footage from ST-NeRF, a 2021 ShanghaiTech University implementation of NeRF that allows individual temporal captures of actors and performers to be arbitrarily resized and edited in neural environments (for high quality footage from the project, see the accompanying video).</figcaption></figure></div></div></div></div></div></section><section data-id="e11f064" data-element_type="section"></section><section data-id="94e6d95" data-element_type="section"><div><div data-id="cb36147" data-element_type="column"><div><div data-id="bd4ece2" data-element_type="widget" data-widget_type="text-editor.default"><div><p><i><span>In the above clip from the ST-NeRF project, we see that two separate performances have been merged into a single rendering, but at different playback rates.</span></i></p><p>In February of 2022, the same team behind ST-NeRF released a <a href="https://arxiv.org/pdf/2202.08614.pdf">new project</a> which claims to enable real-time rendering of NeRF environments at a processing rate 3000 times faster than the original NeRF implementation, and an order of magnitude faster than the best competing state of the art approaches.</p></div></div></div></div></div></section><section data-id="3919edc" data-element_type="section"><div><div data-id="b97d3be" data-element_type="column"><div><div data-id="105d99b" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="600" height="368" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/Fourier-plenOctrees-1.gif" alt="Fourier-plenOctrees"/><figcaption>A novel implementation of the Plenoctrees algorithm has allowed researchers in China to create virtual humans in a dynamic interactively-rendered neural network. Project: Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time, https://arxiv.org/pdf/2202.08614.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="e3fd0dd" data-element_type="section"><div><div data-id="20e9778" data-element_type="column"><div><div data-id="f4bb31a" data-element_type="widget" data-widget_type="text-editor.default"><p><span>The authors contribute to the ShanghaiTech Digital Human Project, which is actively engaged in advancing the cause of NeRF-based digital humans. Among its projects is the recent </span><a href="https://arxiv.org/pdf/2112.02789.pdf"><span>HumanNeRF</span></a><span>, an initiative that offers free-view exploration and manipulation of neural humans captured from studio settings.</span></p></div></div></div></div></section><section data-id="2d4e6a0" data-element_type="section"><div><div data-id="3eab08c" data-element_type="column"><div><div data-id="cdc1f18" data-element_type="widget" data-widget_type="text-editor.default"><p><span>Meanwhile 2021’s </span><a href="https://arxiv.org/pdf/2108.04913.pdf"><span>FLAME-in-NeRF</span></a><span>, a collaboration between Adobe and Stony Brook University, uses Neural Radiance Fields to control and puppet expressions, allowing arbitrary expressions and novel views.</span></p></div><div data-id="181398a" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="512" height="256" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/flame-in-nerf-1.gif" alt="Flame in Nerf"/><figcaption>Flame-in-NeRF produces free viewpoint synthesis of face subjects that can be &#39;driven&#39; by a source video (on the right). Source: https://arxiv.org/pdf/2108.04913.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="26d9af7" data-element_type="section"><div><div data-id="9c54786" data-element_type="column"><div><div data-id="c93597b" data-element_type="widget" data-widget_type="text-editor.default"><p><span>This kind of deepfake puppetry is a popular strand in NeRF research, with some of the strongest new initiatives emerging from Asian research communities. </span><a href="https://yudongguo.github.io/ADNeRF/"><span>AD-NeRF</span></a><span>, a collaboration between four Chinese universities, addresses the popular challenge of generating video from audio speech, using NeRF techniques to create synthetic face reconstructions that are driven by interpreted audio recordings:</span></p></div></div></div></div></section><section data-id="e9fe3f9" data-element_type="section"></section><section data-id="006aa82" data-element_type="section"><div><div data-id="899d324" data-element_type="column"><div><div data-id="9610196" data-element_type="widget" data-widget_type="text-editor.default"><p><span>A 2020 collaboration between Facebook Reality Labs and the Technical University of Munich offers </span><a href="https://arxiv.org/pdf/2012.03065.pdf"><span>Dynamic Neural Radiance Fields</span></a><span>, an AR/VR-centric application of NeRF that uses a low-resolution 3D model to provide the ‘driving’ movement model, removing the need for expensive and elaborate facial capture setups.</span></p></div></div></div></div></section><section data-id="e8d60c4" data-element_type="section"></section><section data-id="83ec7a2" data-element_type="section"><div><div data-id="e19235b" data-element_type="column"><div><div data-id="6779c67" data-element_type="widget" data-widget_type="text-editor.default"><p><a href="https://youngjoongunc.github.io/nhp/"><span>Neural Human Performe</span></a><span>r, a joint research project from Adobe Research and two universities in the US and Korea, is a seminal initiative towards the fabrication of controllable parametric humans derived from NeRF workflows. NHP is capable of generating ‘unseen’ poses (i.e. abstract and arbitrary poses that were not originally captured in source footage):</span></p></div></div></div></div></section><section data-id="cb64ee0" data-element_type="section"></section><section data-id="b4133f5" data-element_type="section"></section><section data-id="48ed062" data-element_type="section"></section><section data-id="ab44680" data-element_type="section"><div><div data-id="071c804" data-element_type="column"><div><div data-id="83b1b81" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>NeRF offers a path forward to pure neural rendering of humans, authentically capturing not only facial characteristics, but correct proportions and behavior for the </span><i><span>entire body</span></i><span> of the target character or personage. </span><span>This is almost certainly unattainable functionality for the current crop of popular deepfake distributions, since their source architecture is not very extensible, and they remain rooted in the strictures and limitations of the original 2017 release.</span></p><p><span>On the other hand, NeRF faces many of the same technological bottlenecks as DeepFaceLab and its stablemates, most notably in the form of practical limitations for input size of training images (see above). Additionally, most of the current crop of NeRF accelerator initiatives sacrifice other useful features (such as flexibility and/or explorability) in exchange for low latency, more interactive environments, and savings on training time and storage. </span></p><p><span>Furthermore, current trends in neural network training make parallel computation or any kind of real scalability as challenging for NeRF as it is for ‘traditional’ deepfakes.</span></p><p>Finally, editing methodologies must be devised to make NeRF a truly controllable and flexible environment. This shortcoming is currently being addressed by early projects such as <a href="https://arxiv.org/pdf/2204.10850.pdf">Control-NeRF</a>, which can remove objects from NeRF scenes, and <a href="https://neverstopzyy.github.io/mofanerf/">MoFaNeRF</a>, which can perform free view synthesis, face editing and even face rigging:</p></div></div></div></div></div></section><section data-id="bb4008f" data-element_type="section"><div><div data-id="fe65825" data-element_type="column"><div><div data-id="1004907" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="267" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/MoFaNeRF.gif" alt=""/><figcaption>Examples of facial transformations with MoFaNeRF. Source: https://neverstopzyy.github.io/mofanerf/</figcaption></figure></div></div></div></div></div></section><section data-id="bda0068" data-element_type="section"><div><div data-id="8560662" data-element_type="column"><div><div data-id="5bb29be" data-element_type="widget" data-widget_type="text-editor.default"><p><span>What NeRF </span><i><span>can</span></i><span> offer, however, is a different </span><i><span>kind</span></i><span> of scalability: the possibility of assembling complex and high resolution environments and objects from multiple NeRFs of lower resolution and complexity. This is the concept behind </span><a href="https://waymo.com/research/block-nerf/"><span>Block-NeRF</span></a><span>, a new paper from UC Berkeley, Google Research, and autonomous driving research company Waymo.</span></p></div></div></div></div></section><section data-id="e0862ec" data-element_type="section"></section><section data-id="5d08d11" data-element_type="section"><div><div data-id="91ef692" data-element_type="column"><div><div data-id="85a5629" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>Block-NeRF essentially orchestrates an array of NeRFs into a cohesive environment, collectively creating an ‘uber-NeRF’ that has greater breadth, resolution, scalability, and disentanglement than any single NeRF output could offer. </span></p><p><span>A similar approach was taken by the Chinese-led </span><a href="https://city-super.github.io/citynerf/"><span>CityNeRF</span></a><span> project from late 2021. CityNeRF provides a kind of ‘NeRF on demand’ facility, similar to the way that videogame assets are loaded when the player gets so near to them that they are likely to be needed, or that online maps will pre-load resources that are adjacent to the data that’s currently being used:</span></p></div></div></div></div></div></section><section data-id="e62463c" data-element_type="section"><div><div data-id="af870ed" data-element_type="column"><div><div data-id="c40ff69" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="600" height="338" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/citynerf.gif" alt="City Nerf"/><figcaption>CityNeRF assets are loaded dynamically as needed. Source: https://city-super.github.io/citynerf/</figcaption></figure></div></div></div></div></div></section><section data-id="c7b781c" data-element_type="section"><div><div data-id="fdf9cec" data-element_type="column"><div><div data-id="5171cbe" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>This is a core advantage for NeRF: deepfake architectures based on the 2017 code produce a </span><i><span>process</span></i><span> that cannot easily be abstracted into a multi-instance framework, whereas NeRF training produces a discrete </span><i><span>object</span></i><span> that can be used as a component in more complex objects. </span></p><p><span>In terms of replicating human appearance, composable NeRFs are a clear possibility, with separate instances at least for the head and body, and granular sub-objects for hair, eyes, and any other asset of facial or personal appearance that could benefit from dedicated training and curation inside an orchestrated ‘master’ host.</span></p></div></div></div></div></div></section><section data-id="db41781" data-element_type="section"></section><section data-id="593ff5a" data-element_type="section"><div><div data-id="4e46eef" data-element_type="column"><div><div data-id="6eac469" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>NeRF is not the sole challenger to 2017-era deepfakes repositories. In the course of time we’ll take a look at the intense research activity around the new crop of deepfake generation techniques that leverage Generative Adversarial Networks (GANs*), and other emerging and innovative routes to deepfake creation. </span></p><p><span>Besides the struggle to non-destructively lower its resource usage and training time, what stands against NeRF as a deepfake successor is that it’s a new technology that’s far behind GAN-based facial simulation research in terms of resolution, detail, and instrumentality. </span></p><p><span>On the plus side, besides being composable, NeRF draws </span><i><span>consistent</span></i><span> data unambiguously from the real world, and quantifies it in geometrical terms without getting lost in the </span><a href="https://www.gwern.net/docs/ai/2019-abdal.pdf"><span>mysteries</span></a><span> of the GAN latent space — and without surrendering ground to hybrid, CGI-based approaches, such as </span><a href="https://studios.disneyresearch.com/2021/11/30/rendering-with-style-combining-traditional-and-neural-approaches-for-high-quality-face-rendering/"><span>Disney Research’s offering</span></a><span> from late 2021 (which superimposes ‘traditional’ texture maps into the latent space of a StyleGAN2 network):</span></p></div></div></div></div></div></section><section data-id="c7eb453" data-element_type="section"></section><section data-id="a504d41" data-element_type="section"><div><div data-id="2c233b3" data-element_type="column"><div><div data-id="5b992ac" data-element_type="widget" data-widget_type="text-editor.default"><p><span>Perhaps inevitably, the two technologies may end up compensating each other’s shortfalls: a paper </span><a href="https://arxiv.org/pdf/2202.13162.pdf"><span>released in February</span></a><span> proposes Pix2NeRF, an architecture that feeds GAN-generated imagery into a NeRF pipeline:</span></p></div></div></div></div></section><section data-id="c102d65" data-element_type="section"><div><div data-id="13b26cb" data-element_type="column"><div><div data-id="dc021e8" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="282" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/06/pix2nerf-1024x361.jpg" alt="Pix 2 Nerf" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/06/pix2nerf-1024x361.jpg 1024w, https://metaphysic.ai/wp-content/uploads/2022/06/pix2nerf-300x106.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/06/pix2nerf-768x271.jpg 768w, https://metaphysic.ai/wp-content/uploads/2022/06/pix2nerf.jpg 1289w" sizes="(max-width: 800px) 100vw, 800px"/><figcaption>Single-shot novel view synthesis with Pix2NeRF, which imposes the output of Generative Adversarial Networks into a NeRF workflow . Source: https://arxiv.org/pdf/2202.13162.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="1f68826" data-element_type="section"><div><div data-id="0be339e" data-element_type="column"><div><div data-id="6d686b0" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>Another crossover GAN/NeRF project is the US/China collaboration </span><a href="https://arxiv.org/pdf/2111.15490"><span>FENeRF</span></a><span>, which leverages ‘Semantic Radiance Fields’ to generate consistent novel views of GAN-generated faces. </span></p><p><span>Since NeRF is fueled by real world imagery, such crossover projects could offer a way to create stable and explorable fictitious neural environments, including GAN-generated people, without the need to map a GAN’s latent space through relatively clumsy tools such as </span><a href="https://ramprs.github.io/static/docs/IJCV_Grad-CAM.pdf"><span>GradCAM heatmaps</span></a><span>. </span></p><p><span>Naturally, there is nothing that prevents the use of traditionally rendered CGI content as source material for NeRF. CGI practitioners concerned about the advance of such neural rendering techniques are likely to have some years yet to catch up to the AI VFX scene, since physics simulation (water, fire, kinetics, inverse kinematics) is a relatively laggard area of neural rendering research at the moment, while textural stability and temporal coherency remains a problem for GAN approaches, and the aforementioned hardware limitations currently affect both technologies.</span></p></div></div></div></div></div></section><section data-id="bb505c1" data-element_type="section"></section><section data-id="ed5ded6" data-element_type="section"><div><div data-id="8f4c65e" data-element_type="column"><div><div data-id="cee2d7a" data-element_type="widget" data-widget_type="text-editor.default"><div><p><span>It may be that the 2017 deepfakes release will be remembered as the last time that an ‘all-in-one’ solution proves capable of delivering state-of-the-art facial and identity simulation without adjunct technologies, proprietary code, or the need for computing resources that are inaccessible to hobbyists and consumers. </span></p><p><span>Nonetheless, NeRF’s late entry into the race for Deepfakes 2.0 represents the tantalizing possibility of an end-to-end neural workflow for deepfake creation that could yet challenge the state of the art while remaining in the open source arena.</span></p><p><b>* </b><i><span>GAN is used only as a refinement tool in DeepFaceLab, and not at all in FaceSwap. The functionality of these projects is based on an encoder-decoder architecture, not a GAN architecture.</span></i></p></div></div></div></div></div></section></div></div></div></div>
  </body>
</html>
