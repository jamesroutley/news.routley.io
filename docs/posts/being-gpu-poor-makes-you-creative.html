<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://dilawar.ai/2025/07/04/Multi-Cluster%20Distributed%20Training%20on%20Heterogeneous%20Hardware/">Original</a>
    <h1>Being GPU Poor makes you creative</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><img src="http://dilawar.ai/images/gpu_poor.jpg"/>
</p>

<p>So there I was, fresh on sabbatical, feeling pretty GPU poor. Going from having access to whatever compute I needed to my lonely MacBook was… humbling. But then I walked into the Recurse Center hub in NYC for the first time, and someone casually mentioned they had “a couple of GPUs.”</p>
<p>My brain immediately went: <em>interesting.</em></p>
<p>But here’s the thing. These weren’t just any GPUs. RC had some NVIDIA GeForce GTX TITAN X cards, but I’m rocking Apple Silicon in my M4 Max MacBook. Different architectures, different vendors, different everything. The classic “how do I get these to work together?” problem.</p>
<p>That’s when I remembered <a target="_blank" rel="noopener" href="https://ray.io/">Ray</a> exists.</p>
<h2 id="The-Heterogeneous-Hardware-Challenge"><a href="#The-Heterogeneous-Hardware-Challenge" title="The Heterogeneous Hardware Challenge"></a>The Heterogeneous Hardware Challenge</h2><p>Training models across different hardware is becoming more common. Whether you’re at a company that acquired another team with different infrastructure, or you’re at RC trying to use whatever’s available, you’ll eventually run into this.</p>
<p>Here’s what I was dealing with:</p>
<ul>
<li><strong>MacBook M4 Max</strong>: Apple Silicon with MPS (Metal Performance Shaders)</li>
<li><strong>RC machines</strong>: NVIDIA GeForce GTX TITAN X with CUDA</li>
<li><strong>Different machines</strong>: Mercer and Crosby (yes, RC names their machines)</li>
</ul>
<p>Traditional distributed training frameworks have trouble with this. NCCL (NVIDIA’s communication library) and Apple’s MPS don’t exactly play nice together. <a target="_blank" rel="noopener" href="https://ray.io/">Ray</a> handles this by creating an abstraction layer that coordinates between different hardware types.</p>
<p><img src="http://dilawar.ai/images/distributed_training.jpg"/>
</p>

<h2 id="How-Ray-Makes-This-Work"><a href="#How-Ray-Makes-This-Work" title="How Ray Makes This Work"></a>How Ray Makes This Work</h2><p>Ray’s approach is abstraction. Instead of trying to make your Apple Silicon GPU talk directly to an NVIDIA GPU, Ray creates a unified layer that handles all the communication.</p>
<p><img src="http://dilawar.ai/images/param_actor.png"/>
</p>

<p>Here’s the basic approach:</p>
<ol>
<li><strong>Parameter Server Pattern</strong>: One central coordinator manages the model weights</li>
<li><strong>Heterogeneous Workers</strong>: Each machine uses whatever hardware it has</li>
<li><strong>Gradient Aggregation</strong>: Ray handles collecting and averaging gradients across all devices</li>
<li><strong>Automatic Scheduling</strong>: Ray figures out what work goes where</li>
</ol>
<p>You write your training code once, and <a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/ray-overview/getting-started.html">Ray distributes it across whatever hardware</a> you have available.</p>
<h2 id="The-Code-Making-It-Actually-Work"><a href="#The-Code-Making-It-Actually-Work" title="The Code: Making It Actually Work"></a>The Code: Making It Actually Work</h2><p>Let me walk you through the key pieces of my setup. First, the magic <code>@ray.remote</code> decorator:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>@ray.remote(<span>num_gpus=<span>1</span></span>)</span></span></pre></td></tr></tbody></table></figure>

<p>This worker will automatically land on machines with GPUs. But here’s the clever bit:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>@ray.remote(<span>num_cpus=<span>2</span></span>)</span></span></pre></td></tr></tbody></table></figure>

<p>By requesting different resources (<code>num_gpus=1</code> vs <code>num_cpus=2</code>), Ray automatically distributes workers across different types of hardware. The NVIDIA workers land on the CUDA machines, while the CPU/MPS worker ends up on my MacBook.</p>
<h2 id="The-Parameter-Server-Keeping-Everyone-in-Sync"><a href="#The-Parameter-Server-Keeping-Everyone-in-Sync" title="The Parameter Server: Keeping Everyone in Sync"></a>The Parameter Server: Keeping Everyone in Sync</h2><p>The parameter server is where the magic happens:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>@ray.remote(<span>num_cpus=<span>1</span></span>)</span></span></pre></td></tr></tbody></table></figure>

<p>Each worker computes gradients on its own hardware (CUDA, MPS, or CPU), then sends them back to the parameter server. The parameter server averages everything and updates the global model.</p>
<h2 id="The-Training-Loop-Where-It-All-Comes-Together"><a href="#The-Training-Loop-Where-It-All-Comes-Together" title="The Training Loop: Where It All Comes Together"></a>The Training Loop: Where It All Comes Together</h2><p>Here’s what a training iteration looks like:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>for</span> iteration <span>in</span> <span>range</span>(training_iterations):</span></pre></td></tr></tbody></table></figure>

<p>The beauty here is the <code>ray.get(futures)</code> call. Ray handles all the complexity of coordinating between different devices and architectures. You just get your results back.</p>
<h2 id="What-I-Learned-The-Real-Benefits"><a href="#What-I-Learned-The-Real-Benefits" title="What I Learned: The Real Benefits"></a>What I Learned: The Real Benefits</h2><p>After running this for a while, here’s what became clear:</p>
<h3 id="1-It-Actually-Works"><a href="#1-It-Actually-Works" title="1. It Actually Works"></a>1. <strong>It Actually Works</strong></h3><p>I had one model training simultaneously on:</p>
<ul>
<li>My MacBook M4 Max (using MPS)</li>
<li>Mercer’s NVIDIA GeForce GTX TITAN X (using CUDA)</li>
<li>Crosby’s NVIDIA GeForce GTX TITAN X (using CUDA)</li>
</ul>
<p>All three machines contributing to training the same model. In real time. No hacky workarounds.</p>
<h3 id="2-Resource-Utilization-is-Better"><a href="#2-Resource-Utilization-is-Better" title="2. Resource Utilization is Better"></a>2. <strong>Resource Utilization is Better</strong></h3><p>Instead of leaving hardware idle because it doesn’t match your “preferred” setup, you can use everything. </p>
<h3 id="3-The-Parameter-Server-Pattern-Scales"><a href="#3-The-Parameter-Server-Pattern-Scales" title="3. The Parameter Server Pattern Scales"></a>3. <strong>The Parameter Server Pattern Scales</strong></h3><p>This isn’t just a toy example. The parameter server pattern scales to much larger models and clusters. Companies like <a target="_blank" rel="noopener" href="https://www.anyscale.com/blog/heterogeneous-training-cluster-with-ray-at-netflix">Netflix are using Ray for heterogeneous training clusters</a> with mixed hardware across their infrastructure.</p>
<p><img src="http://dilawar.ai/images/hetero_servers.jpg"/>
</p>

<h3 id="4-Fault-Tolerance-Comes-Free"><a href="#4-Fault-Tolerance-Comes-Free" title="4. Fault Tolerance Comes Free"></a>4. <strong>Fault Tolerance Comes Free</strong></h3><p>Ray handles worker failures gracefully. If one machine crashes, the training continues with the remaining workers. No manual intervention needed.</p>
<h2 id="The-Bigger-Picture-Why-This-Matters"><a href="#The-Bigger-Picture-Why-This-Matters" title="The Bigger Picture: Why This Matters"></a>The Bigger Picture: Why This Matters</h2><p>This isn’t just about using random hardware lying around. As AI models get larger, the economics of training matter more. Being locked into a single hardware vendor gets expensive.</p>
<p>Recent research shows that <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2505.17548">heterogeneous training can achieve up to 16% better performance</a> compared to traditional homogeneous setups, especially for models with over 1 billion parameters. Companies are realizing that vendor diversity isn’t just about avoiding lock-in—it’s about optimizing cost and performance.</p>
<h2 id="Scaling-to-Real-Models"><a href="#Scaling-to-Real-Models" title="Scaling to Real Models"></a>Scaling to Real Models</h2><p>Now, my demo used a simple neural network, but the same principles apply to large language models. The key differences for scaling up:</p>
<ol>
<li><strong>Memory Management</strong>: Use <a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/train/user-guides.html">gradient checkpointing and model sharding</a></li>
<li><strong>Communication Optimization</strong>: Ray supports various communication backends</li>
<li><strong>Data Pipeline</strong>: <a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/data/data.html">Ray Data</a> can handle distributed data loading efficiently</li>
</ol>
<p>For 1B+ parameter models, you’d want to add:</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/train/deepspeed.html">DeepSpeed integration</a></strong> for memory efficiency</li>
<li><strong>Mixed precision training</strong> for speed</li>
<li><strong>Dynamic batching</strong> based on hardware capabilities</li>
</ul>
<h2 id="The-Setup-Scripts-Making-It-Reproducible"><a href="#The-Setup-Scripts-Making-It-Reproducible" title="The Setup Scripts: Making It Reproducible"></a>The Setup Scripts: Making It Reproducible</h2><p>I wrote some simple scripts to make this whole thing reproducible:</p>
<p><strong>setup_cluster.sh</strong>:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>#!/bin/bash</span></span></pre></td></tr></tbody></table></figure>

<p><strong>teardown_cluster.sh</strong>:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>#!/bin/bash</span></span></pre></td></tr></tbody></table></figure>

<p>Then your MacBook just connects as another worker in the cluster. No complex Kubernetes setup, no Docker orchestration, no SLURM. Just Ray being Ray.</p>
<h2 id="What-I-Built-A-Framework-for-Everyone"><a href="#What-I-Built-A-Framework-for-Everyone" title="What I Built: A Framework for Everyone"></a>What I Built: A Framework for Everyone</h2><p>After getting this working, I realized other people probably run into the same problem. So I built a simple framework that wraps all this complexity: <a target="_blank" rel="noopener" href="https://github.com/dilawarm/distributed-hetero-ml">distributed-hetero-ml</a>.</p>
<p>The idea was to make heterogeneous distributed training as simple as possible. Instead of writing all the parameter server and worker coordination code yourself, you just define your model and data, and the framework handles the rest:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>from</span> distributed_hetero_ml <span>import</span> DistributedTrainer, TrainingConfig</span></pre></td></tr></tbody></table></figure>

<p>The framework automatically detects your hardware and configures itself. Got NVIDIA GPUs? It uses CUDA. Apple Silicon? MPS backend. Mixed setup? No problem.</p>
<p>It also handles checkpointing, cluster connections, and resource management. The goal was to abstract away all the Ray boilerplate while keeping the flexibility for when you need to dig deeper.</p>
<h2 id="Looking-Forward-The-1B-Parameter-Goal"><a href="#Looking-Forward-The-1B-Parameter-Goal" title="Looking Forward: The 1B Parameter Goal"></a>Looking Forward: The 1B Parameter Goal</h2><p>My next step is scaling this to actual language models. <a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/train/train.html">Ray Train</a> has built-in support for <a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/train/getting-started-transformers.html">Hugging Face Transformers</a>, so I can take a 1B parameter model and train it across all the available hardware at RC, plus whatever cloud resources I want to add.</p>
<p>The workflow would be:</p>
<ol>
<li><strong><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/data/data.html">Use Ray Data</a></strong> for distributed data loading</li>
<li><strong><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/train/train.html">Ray Train</a></strong> for orchestrating the training</li>
<li><strong><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune</a></strong> for hyperparameter optimization</li>
<li><strong><a target="_blank" rel="noopener" href="https://docs.ray.io/en/latest/serve/index.html">Ray Serve</a></strong> for deployment</li>
</ol>
<p>All while using whatever hardware is available, from Apple Silicon to NVIDIA to potentially even AMD GPUs if I can get my hands on some.</p>
<h2 id="The-Bottom-Line"><a href="#The-Bottom-Line" title="The Bottom Line"></a>The Bottom Line</h2><p>Ray made it possible to use all the available hardware, regardless of vendor. No more “sorry, this only works on NVIDIA” or “you need identical hardware across all nodes.” Point Ray at your mismatched collection of machines and it figures out how to use them.</p>
<p>For anyone working on distributed training - whether you’re at a startup with mixed hardware, a company dealing with merger integration, or just someone at RC trying to train bigger models - Ray makes heterogeneous computing practical.</p>
<p>It’s pretty cool watching your MacBook contribute gradients alongside a couple of NVIDIA rigs, all working together on the same model. Turns out you don’t need to pick a side in the hardware wars.</p>
<p>What started as a “hey, can I use these random GPUs?” problem at RC turned into <a target="_blank" rel="noopener" href="https://github.com/dilawarm/distributed-hetero-ml">a framework</a> that hopefully makes this easier for other people. Sometimes being GPU poor forces you to get creative.</p>

  </div></div>
  </body>
</html>
