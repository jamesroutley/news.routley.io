<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://machinelearning.apple.com/research/seedlm-compressing">Original</a>
    <h1>SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><section></section><section><div><div><div><p>Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we
find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with
Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and  3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.</p>
<p>† Meta</p></div></div></div></section><section><div><div><div data-testid="card-compress-compare"><div><p>*Equal Contributors
To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support…</p><p><a href="https://machinelearning.apple.com/research/compress-compare" aria-label="See full paper details regarding Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments">See paper details</a></p></div></div><div data-testid="card-lossless-compression"><div><p>This paper was accepted at the ICML 2021 conference as well as the Theory and Practice of Differential Privacy workshop at the ICML 2021 conference.
Locally Differentially Private (LDP) Reports are commonly used for collection of statistics and machine learning in the federated setting. In many cases the best known LDP algorithms require sending prohibitively large messages from the client device to the server (such as when constructing…</p><p><a href="https://machinelearning.apple.com/research/lossless-compression" aria-label="See full paper details regarding Lossless Compression of Efficient Private Local Randomizers">See paper details</a></p></div></div></div></div></section><section></section></div></div>
  </body>
</html>
