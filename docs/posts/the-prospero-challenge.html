<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mattkeeter.com/projects/prospero/">Original</a>
    <h1>The Prospero Challenge</h1>
    
    <div id="readability-page-1" class="page">
    
    

    <hr/>
    <p><a href="https://github.com/mkeeter/fidget/blob/main/models/prospero.vm"><code>prospero.vm</code></a>
    is a plain-text file containing 7866 math expressions.

    </p><p>It begins as follows:
    </p><pre><code># Text of a monologue from The Tempest
_0 const 2.95
_1 var-x
_2 const 8.13008
_3 mul _1 _2
_4 add _0 _3</code></pre>
    ...and continues in that vein for many, <em>many</em> lines.
    <p>If you evaluate the expression with <code>(x,y)</code> values in the ±1
    square, then color pixels black or white based on their sign, you get the
    following image:
    </p><p><img src="https://www.mattkeeter.com/projects/prospero/image.png" alt="text of a monologue from The Tempest, in black-on-white text"/>
    </p><p>The &#34;challenge&#34; is simple: render the image as quickly as possible.
    </p><p>A basic renderer is 28 lines of Python, using Numpy for pixel parallelism:
</p><pre><code>import numpy as np

with open(&#39;prospero.vm&#39;) as f:
    text = f.read().strip()

image_size = 1024
space = np.linspace(-1, 1, image_size)
(x, y) = np.meshgrid(space, -space)
v = {}

for line in text.split(&#39;\n&#39;):
    if line.startswith(&#39;#&#39;):
        continue
    [out, op, *args] = line.split()
    match op:
        case &#34;var-x&#34;: v[out] = x
        case &#34;var-y&#34;: v[out] = y
        case &#34;const&#34;: v[out] = float(args[0])
        case &#34;add&#34;: v[out] = v[args[0]] + v[args[1]]
        case &#34;sub&#34;: v[out] = v[args[0]] - v[args[1]]
        case &#34;mul&#34;: v[out] = v[args[0]] * v[args[1]]
        case &#34;max&#34;: v[out] = np.maximum(v[args[0]], v[args[1]])
        case &#34;min&#34;: v[out] = np.minimum(v[args[0]], v[args[1]])
        case &#34;neg&#34;: v[out] = -v[args[0]]
        case &#34;square&#34;: v[out] = v[args[0]] * v[args[0]]
        case &#34;sqrt&#34;: v[out] = np.sqrt(v[args[0]])
        case _: raise Exception(f&#34;unknown opcode &#39;{op}&#39;&#34;)
out = v[out]

with open(&#39;out.ppm&#39;, &#39;wb&#39;) as f: # write the image out
    f.write(f&#39;P5\n{image_size} {image_size}\n255\n&#39;.encode())
    f.write(((out &lt; 0) * 255).astype(np.uint8).tobytes())</code></pre>

    <p>On my machine, this takes about <b>15 seconds</b> to produce a 1024×1024 image.

    </p><p>(<em>Spoilers:</em> it also allocates 60+ GB of RAM for intermediate
    results, so consider reducing the image size before running it on your own
    machine)

    </p><p>&#34;But wait!&#34;, you say.  &#34;There&#39;s so much obvious room for improvement!  You
    could pre-parse the expression, or use <code>numba</code>, or run it on
    the GPU, or use LLVM ...&#34;

    </p><p><b>Yes.</b>  I would like you to do those things, and then tell me about
    them!

    </p><hr/>

    <p>This page has been seeded with a few of my projects, but I&#39;d like for
    it to grow to contain different experiments by other people.

    </p><p>A few ideas to think about:
    </p><ul>
        <li>The Python implementation above is a simple interpreter.  What would
            an optimized interpreter look like?  How about a compiler?
        </li><li>How can we take advantage of the expression&#39;s mathematical
            structure?
        </li><li>Start-up time versus steady-state performance.  What kinds of
            precomputation help, and how long does it take?  (Obviously, you
            could precompute the entire image, but that&#39;s against the spirit of
            the challenge)
        </li><li>Along those lines, what happens if the user is making changes to the
            model?  Strategies which rely on expensive precomputation may not be
            suitable for interactive use.
        </li><li>How does rendering performance change with image size?  A 1024×1024
            image has 4× as many pixels as 512×512; does it necessarily
            take 4× longer to render?
        </li><li>Panning and zooming is a simple transform of the incoming
            <code>(x,y)</code> coordinates.  How does your system handle
            interactive viewing of the model?
    </li></ul>

    <h2>Submitting your experiments</h2>
    <p>Feel free to submit both successful and unsuccessful experiments via
    email to
    </p><p>The ideal submission would be a blog post or code repository, along
    with a few-sentence description that you&#39;d like included on this page.  I&#39;d
    also be happy to include hero images or video embeds.

    </p><p>The description should call out any particularly promising results,
    clever ideas, or interesting tools!  Don&#39;t get <em>too fixated</em> on
    absolute speed; the goal is to explore ideas, and that can absolutely be
    done without breaking any performance records!

    </p><h2>Is there a prize?</h2>
    <p>Besides eternal fame and glory?
    </p><p>If you submit an interesting write-up, I will buy you a coffee or drink
    if we happen to be in the same city.  It will be particularly easy for
    Cambridge / Boston / Somerville residents to receive their prize; I make no
    guarantees for submissions from farther afield.

    </p><h2>Background material</h2>
    <p><a href="https://fab.cba.mit.edu/classes/S62.12/docs/Duff_interval_CSG.pdf">
    Interval Arithmetic and Recursive Subdivision for Implicit Functions and
    Constructive Solid Geometry</a> (Duff &#39;92) is a great introduction to common
    techniques.  In particular, the combination of interval arithmetic, spatial
    subdivision, and expression simplification remains the basis for modern
    renderers.

    </p><p>These ideas are also presented in §3.7 of my thesis,
    <a href="https://www.mattkeeter.com/research/thesis.pdf">Hierarchical Volumetric Object Representations
             for Digital Fabrication Workflows</a> (2013),
    although my personal implementations have evolved since then (see below for
    examples).

    </p><p>Finally, I gave a talk titled
    <a href="https://www.youtube.com/watch?v=UxGxsGnbyJ4">Implicit Surfaces &amp; Independent Research</a>
    (2025), which covers all of this material and hews closely to my
    current implementations.  The audience was CS undergraduates in their senior
    year, so it&#39;s intended to be approachable!

    </p><h2>Gallery
    </h2><h3>Massively Parallel Rendering of Complex Closed-Form Implicit Surfaces</h3>
    <p>Matt Keeter, 2020</p>

    <p><a href="https://www.mattkeeter.com/research/mpr">Paper homepage</a>

    </p><p>The MPR implementation compiles the expression down to a register-based
    bytecode, then executes it with a small interpreter running in a CUDA
    kernel.  The interpreter evaluates the expression using
    <a href="https://en.wikipedia.org/wiki/Interval_arithmetic">interval arithmetic</a>
    on many spatial regions in parallel, then simplifies the expression,
    subdivides the active regions, and recurses.  Below a certain size, we swap
    over to per-pixel evaluation, also in parallel on the GPU.

    </p><p>Expression simplification is particularly important for performance: by the
    time we evaluate individual pixels, the expression is 200× smaller!

    </p><p>Most of the hard work went into making a deeply recursive algorithm (with
    heterogeneous workloads in each branch) into something more GPU-friendly.

    </p><p>Rendering a 1024×1024 image takes <b>3.9 milliseconds</b> using a Tesla
    V100, which was a high-end GPU back in 2020 (and is still absurdly
    powerful).  Performance is basically flat through 4096×4096, indicating that
    we&#39;re far from saturating the GPU for this kind of 2D rendering (the paper
    also describes a similar strategy for 3D rendering).

    </p><p>Unfortunately, I don&#39;t have timing numbers for setup time, but the
    kernels are generic interpreters, i.e. <em>not</em> specialized to a
    particular expression.

    </p><h3>Fidget</h3>
    <p>Matt Keeter, 2025</p>
    <p><a href="https://www.mattkeeter.com/projects/fidget">Project homepage</a>
    </p><p>Fidget uses the same broad strategy of interval evaluation and expression
    simplification, but runs on the CPU and compiles expressions down to
    <b>machine code</b> with a JIT compiler.

    </p><p>Running on an M1 Max CPU (2021), it renders a 1024×1024 image in
    <b>6.3 milliseconds</b>, with about 11 ms of setup time:

</p><pre>$ cargo run -pfidget-cli --release -- render2d -i models/prospero.vm \
    -s1024 --eval=jit -N500 --mode mono
[2025-03-23T21:26:44Z INFO  fidget_cli] Loaded file in 4.994333ms
[2025-03-23T21:26:44Z INFO  fidget_cli] Built shape in 6.04525ms
[2025-03-23T21:26:47Z INFO  fidget_cli] Rendered 500x at 6.331644 ms/frame</pre>

    <p>Scaling up to 4096×4096, it takes <b>57 milliseconds</b>, which is about
    9× slower; this is sublinear scaling, because it&#39;s 16× more pixels.

    </p><p>There are a few interesting aspects to this project:
    </p><ul>
        <li>The JIT compiler&#39;s architecture is specialized for very fast
            compilation, because it&#39;s running many times to evaluate a single
            frame!
        </li><li>There&#39;s also a VM interpreter, which is <em>almost</em> as fast: 7.5
            ms for a 1024×1024 image
        </li><li>Everything except the JIT compiler can be run in WebAssembly, albeit
            with a performance penalty
    </li></ul>

    <h3>Prospero challenge, now with more garbage collection</h3>
    <p>Max Bernstein, 2025</p>
    <p><a href="https://bernsteinbear.com/blog/prospero/">Blog post</a>
    </p><p>Max addresses the &#34;60 GB of intermediate results&#34; issue by adding garbage
    collection to the Python interpreter, seeing a quick 4× speedup (and a
    dramatic reduction in RAM usage, down to about 1 GB).

    </p><p>The post also shows off <a href="https://cupy.dev/">CuPy</a>, a drop-in
    replacement for Numpy which offloads computation to the GPU.  Simply
    replacing the import statement yields a further 6.6× speedup!

    </p><h3>Generating a native CUDA kernel</h3>
    <p>Kevin Wang, 2025</p>
    <p><a href="https://github.com/kevmo314/prospero.vm">Github repo</a>
    </p><p>This demo shows the raw power of a modern GPU, rendering a <b>4096×4096
    frame in about 0.5 milliseconds</b>.  The expression is translated directly into
    a CUDA kernel then compiled and run, evaluating all of the pixels in
    parallel without the overhead of an interpreter loop.

    </p><p>The downside to this approach is significant startup time – about 2
    seconds to compile the kernel – which makes it less suitable for interactive
    design work.  Still, for workflows that focus on viewing static expressions,
    it’s hard to beat.

    </p><p>As a side note, Kevin and I are both puzzled by the specific performance
    numbers.  His GPU has a theoretical max performance of
    <a href="https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889">83
    TFLOPS</a>; however, 0.5 milliseconds for 4096×4096 pixels (with 6461
    non-<code>const</code> expressions) implies about 200 TFLOPS!  If you
    have theories about the discrepancy, feel free to reach out.

    </p><hr/>
    <p>That&#39;s all for now; but you can help by submitting your own experiments!

    </p>


</div>
  </body>
</html>
