<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://her.esy.fun/posts/0031-how-i-protect-my-forgejo-instance-from-ai-web-crawlers/index.html">Original</a>
    <h1>How I protect my Forgejo instance from AI web crawlers</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><article><p>Put that in your nginx config:</p><pre><code>location / {
  # needed to still allow git clone from http/https URLs
  if ($http_user_agent ~* &#34;git/|git-lfs/&#34;) {
        set $bypass_cookie 1;
  }
  # If we see the expected cookie; we could also bypass the blocker page
  if ($cookie_Yogsototh_opens_the_door = &#34;1&#34;) {
        set $bypass_cookie 1;
  }
  # Redirect to 418 if neither condition is met
  if ($bypass_cookie != 1) {
     add_header Content-Type text/html always;
     return 418 &#39;&lt;script&gt;document.cookie = &#34;Yogsototh_opens_the_door=1; Path=/;&#34;; window.location.reload();&lt;/script&gt;&#39;;
  }
  # rest of your nginx config
</code></pre><p>Preferably run a string replace from
<code>Yogsototh_opens_the_door</code> to your own personal Cookie
name.</p><p>Main advantage, is that it is almost invisible to the users of my
website compartively to other solutions like Anubis.</p><p>Not so long ago I started to host my code to <a href="https://forgejo.org">forgejo</a>. There is a promise that in the
future it will support federation and forgejo is the same project that
is used for <a href="https://codeberg.org">codeberg</a>.</p><p>The only problem I had was one day, I discovered that my entire node
was down. At first I didn&#39;t investigate and just restarted the node. But
soon after a few hours, it was down again. Looking at the reason,
clearly thousands of requests that looked at every commit which put too
much pressure on the system. Who could be so interested in using the web
API to look at every commit instead, ofâ€¦ you know, clone the repository
locally and explore it. Quickly, yep, like so many of you, I discovered
that tons of crawlers that did not respect the <code>robots.txt</code>
are crawling my forgejo instance until death ensues.</p><p>So I had no choice, I first used a radical approach and blocked my
website entirely except from me. But hey, why having a public forge if
not for people to be able to look into it time to time?</p><p>I then installed Anubis, but it wasn&#39;t really for me. It is way too
heavy for my needs, not as easy as I would have hoped to configure and
install.</p><p>Then I saw this article <a href="https://fxgn.dev/blog/anubis/">You
don&#39;t need anubis</a> on <a href="https://lobste.rs">lobste.rs</a> using
a simple configuration in caddy that should block these pesky crawlers.
I made some adjustments to adapt it to nginx. For now, this is working
perfectly well, my users are just redirected once, without really
noticing it. And they could use forgejo as they could before. And this
puts the crawlers away.</p><p>The strategy is pretty basic; in fact, a lot less advanced than the
strategy adopted by Anubis. For every access of my website, I just check
if the user has a specific cookie set. If not, I redirect the user to a
418 HTML page containing some js code to execute that set this specific
cookie and reload the page.</p><p>That&#39;s it.</p><p>I also tried to return a 302 and add a cookie from the response
without javascript, but the crawlers are immune to that second strategy.
Unfortunately this means, my website could only be seen if you enable
javascript in your browser. I feel this is acceptable. I guess, someday
this very basic protection will not be enough and my forgejo instance
will break again, and I will be forced to use more advanced system like
<a href="https://anubis.techaro.lol">Anubis</a> or perhaps even <a href="https://iocaine.madhouse-project.org">iocaine</a>.</p><p>I hope this could be helpful, because, I recently saw many
discussions on that subject where people were not totally happy to use
Anubis, while at least for me, this quick dirty fix does the trick. And
I am fully aware that this would be very easy to bypass. But for now, I
think the volume is more important than the quality for these crawlers
and it may take a while for them to need to adapt. Also, by publishing
this, I know if too many people use the same trick, quickly, these
crawlers will adapt.</p></article></div></div>
  </body>
</html>
