<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cornell-relaxml.github.io/quip-sharp/">Original</a>
    <h1>QuIP#: 2-bit Quantization for LLMs</h1>
    
    <div id="readability-page-1" class="page">

<h2 id="quip-quip-with-lattice-codebooks">QuIP#: <a href="https://github.com/jerry-chee/QuIP">QuIP</a> with Lattice
Codebooks</h2>
<p><a href="https://tsengalb99.github.io">Albert Tseng*</a>, <a href="https://jerry-chee.github.io/">Jerry Chee*</a>, <a href="https://nalzok.github.io/">Qingyao Sun</a>, <a href="https://www.cs.cornell.edu/~kuleshov/">Volodymyr Kuleshov</a>, and
<a href="https://www.cs.cornell.edu/~cdesa/">Chris De Sa</a></p>
<hr/>
<p><img src="https://cornell-relaxml.github.io/quip-sharp/img/overview.svg"/></p>
<p>Large language models (LLMs) exhibit amazing performance on a wide
variety of tasks such as text modeling and code generation. However,
they are also very large. For example Llama 2 70B has 70 billion
parameters that require 140GB of memory to store in half precision. This
presents many challenges, such as needing multiple GPUs just to serve a
single LLM. To address these issues, researchers have developed
compression methods that reduce the size of models without destroying
performance.</p>
<p>One class of methods, post-training quantization, compresses trained
model weights into lower precision formats to reduce memory
requirements. For example, quantizing a model from 16 bit to 2 bit
precision would reduce the size of the model by 8x, meaning that even
Llama 2 70B would fit on a single 24GB GPU. In this work, we introduce
<strong>QuIP#</strong>, which combines lattice codebooks with
incoherence processing to create state-of-the-art 2 bit quantized
models. These two methods allow QuIP# to significantly close the gap
between 2 bit quantized LLMs and unquantized 16 bit models.</p>
<div>
<table>
<caption>Quantization results on Llama 2 70B. QuIP# achieves near-native
performance at 2 bits, outperforming all other presented
baselines.</caption>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Method</th>
<th>Precision</th>
<th>Wiki <span>\(\downarrow\)</span></th>
<th>C4 <span>\(\downarrow\)</span></th>
<th>ArcE <span>\(\uparrow\)</span></th>
<th>PiQA <span>\(\uparrow\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Native</td>
<td>16 bit</td>
<td>3.120</td>
<td>5.533</td>
<td>0.597</td>
<td>0.809</td>
</tr>
<tr>
<td>OPTQ</td>
<td>3 bit</td>
<td>4.577</td>
<td>6.838</td>
<td>0.544</td>
<td><strong>0.786</strong></td>
</tr>
<tr>
<td>OPTQ</td>
<td>2 bit</td>
<td>109.820</td>
<td>62.692</td>
<td>0.253</td>
<td>0.505</td>
</tr>
<tr>
<td>QuIP</td>
<td>2 bit</td>
<td>5.574</td>
<td>8.268</td>
<td>0.544</td>
<td>0.751</td>
</tr>
<tr>
<td><strong>QuIP#</strong></td>
<td><strong>2 bit</strong></td>
<td><strong>4.156</strong></td>
<td><strong>6.545</strong></td>
<td><strong>0.595</strong></td>
<td>0.785</td>
</tr>
</tbody>
</table>
</div>
<div>
<table>
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><span>☞</span></td>
<td><strong>Our method, QuIP#, creates 2 bit LLMs that achieve
near-native performance, a previously unseen result. We provide a <a href="https://huggingface.co/relaxml">full suite of 2 bit Llama 1 and 2
models quantized using QuIP#</a>, as well as a full codebase that allows
users to quantize and deploy their own models. We also provide CUDA
kernels that accelerate inference for QuIP# models. Our code is
available <a href="https://github.com/Cornell-RelaxML/quip-sharp">here</a>.</strong></td>
</tr>
</tbody>
</table>
</div>
<h3 id="method-overview">Method Overview</h3>
<p>QuIP# relies on two main components: <em>incoherence processing</em>
and <em>lattice codebooks</em>. Incoherence processing in the context of
model quantization was introduced in QuIP. While QuIP used a Kronecker
product to perform incoherence processing, we introduce a Hadamard
transform-based incoherence approach that is more amenable to fast GPU
acceleration.</p>
<p>Incoherence-processed weights are approximately Gaussian-distributed,
which means that they are suitable for quantizing with symmetric and
“round” codebooks. We introduce a new lattice codebook based on the
<span>\(E_8\)</span> lattice, which achieves the
optimal 8 dimension unit ball packing density. Our codebooks are
specifically designed to be hardware-friendly by exploiting symmetries
in these lattices.</p>
<h3 id="quantization-background">Quantization Background</h3>
<p>In QuIP#, we follow existing state-of-the-art post training
quantization methods and round weights to minimize the per-layer
“adaptive rounding” proxy objective</p>
<p><span>\[
\ell(\hat W)
  = E_x \left[ \| (\hat W - W)x \|^2 \right]
  = \operatorname{tr}\left(
    (\hat W - W) H (\hat W - W)^T
   \right).
\]</span></p>
<p>Here, <span>\(W \in \mathbb{R}^{m \times
n}\)</span> is the original weight matrix in a given layer, <span>\(\hat W = \mathbb{R}^{m \times n}\)</span> are the
quantized weights, <span>\(x \in
\mathbb{R}^n\)</span> is an input vector drawn uniformly at random from
a calibration set, and <span>\(H\)</span> is the
second moment matrix of these vectors, interpreted as a proxy Hessian.
This intra-layer formulation makes quantization tracatable for large
language models. The original QuIP paper forumlated a class of adaptive
rounding methods that used linear feedback to minimize <span>\(\ell\)</span>. Within this class, the LDLQ
rounding algorithm was shown to be optimal; we use LDLQ in QuIP# as
well.</p>
<h3 id="incoherence-processing">Incoherence Processing</h3>
<p>The main insight of QuIP is that incoherent weight and hessian
matrices result in improved quantization performance. Informally, this
means that weights that are even in magnitude with important rounding
directions (the Hessians) that are not too large in any one coordinate
are significantly easier to quantize without catastrophic error. In some
sense, incoherence processing can be viewed as a form of outlier
suppression across weight and activation spaces.</p>
<p><strong>Definition.</strong> <em>We say a symmetric Hessian matrix
<span>\(H \in \mathbb{R}^{n \times n}\)</span> is
<span>\(\mu\)</span>-incoherent if it has an
eigendecomposition <span>\(H = Q \Lambda
Q^T\)</span> such that for all <span>\(i\)</span>
and <span>\(j\)</span>, <span>\(|Q_{ij}| = |e_i^T Q e_j| \leq \mu /
\sqrt{n}\)</span>. By extension, we say a weight matrix <span>\(W \in \mathbb{R}^{m \times n}\)</span> is <span>\(\mu\)</span>-incoherent if for all <span>\(i\)</span> and <span>\(j\)</span>, <span>\(|W_{ij}| =
|e_i^T W e_j| \leq \mu \|W\|_F / \sqrt{mn}\)</span>.</em></p>
<p>Incoherence is an important property for quantizing models. In QuIP,
the incoherence condition on <span>\(H\)</span> is
required to show that LDLQ achieves a superior proxy loss to nearest and
stochastic rounding through a spectral bound on <span>\(H\)</span>. Therefore, it is important to be able
to incoherence-process weight and hessian matrices efficiently so that
incoherence-processed models can be tractably deployed.</p>
<p>One way to do this is by conjugating <span>\(W\)</span> and <span>\(H\)</span> by random orthogonal matrices. Let
<span>\(U \in \mathbb{R}^{m \times m}\)</span>, and
<span>\(V \in \mathbb{R}^{n \times n}\)</span> be
two random orthogonal matrices. If we assign <span>\(\tilde H \gets V H V^T\)</span> and <span>\(\tilde W \gets U W V^T\)</span>, <span>\(\tilde H\)</span> and <span>\(\tilde W\)</span> become incoherence processed
with high probability (see QuIP for proof). One can verify that this
transformation preserves the proxy objective as <span>\[\operatorname{tr}(\tilde W \tilde H \tilde W^T) =
\operatorname{tr}((U W V^T) (V H V^T) (V W^T U^T)) =
\operatorname{tr}(WHW^T).\]</span></p>
<h4 id="randomized-hadamard-transformation-rht">Randomized Hadamard
Transformation (RHT)</h4>
<p>To construct <span>\(U\)</span> and <span>\(V\)</span> from above, we use the RHT, which is
amenable to fast GPU implementation. In fact, one of the CUDA sample
kernels is the RHT. The RHT performs the multiplication <span>\(x \in \mathbb{R}^n \to \mathbb{H}Sx\)</span>,
where <span>\(\mathbb{H}\)</span> is a <span>\(n \times n\)</span> Hadamard matrix (scaled by a
normalization factor) and <span>\(S\)</span> is a
<span>\(n\)</span> dimensional random sign vector.
The RHT concentrates the entries of <span>\(x\)</span> and thus results in incoherent matrices
through an <a href="http://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/teaching/15859-fall20/lecture_2.1.pdf">application
of the Azuma-Hoeffding inequality</a>. Note that the Hadamard transform
can be computed more efficiently than a matrix multiplication via the
fast Walsh-Hadamard transform, which we employ directly for powers of 2.
To handle non power-of-two values of <span>\(n\)</span>, we perform the following
algorithm:</p>
<ol type="1">
<li>Let <span>\(p\)</span> be the remaining
dimension and reshape <span>\(Sx\)</span> into a
<span>\(n/p \times p\)</span> matrix.</li>
<li>Perform the fast Walsh-Hadamard transform on <span>\(Sx\)</span> associated with dimension <span>\(n/p\)</span>.</li>
<li>Let <span>\(\mathbb{H}&#39;\)</span> be a <span>\(p \times p\)</span> scaled Hadamard matrix. Apply
this Hadamard transform to <span>\(Sx\)</span> on
the right, and reshape back.</li>
</ol>
<p>The only consequence of performing RHT is needing to store two sign
vectors per layer: <span>\(S_U\)</span> and <span>\(S_V\)</span>. Since large language models have
large weight and Hessian matrices, this only increases the number of
bits per weight in practice by less than 0.01, or a negligible
amount.</p>
<h3 id="lattice-codebooks">Lattice Codebooks</h3>
<p>Incoherence processed weights are approximately Gaussian-distributed,
meaning that they are symmetric and “round.” To take advantage of this
“roundness,” we can use <span>\(n\)</span>
dimensional codebooks that quantize <span>\(n\)</span> weights at once. Specifically, to
quantize <span>\(x \in \mathbb{R}^n\)</span> to a
<span>\(n\)</span> dimensional codebook <span>\(C \in \mathbb{R}^{m \times n}\)</span>, we round
<span>\(x\)</span> to its nearest distance-wise
entry in <span>\(C\)</span>. This requires <span>\(\log_2m\)</span> bits to represent which index in
<span>\(C\)</span> to store, and results in <span>\(k = \frac{\log_2m}{n}\)</span> bits per
weight.</p>
<p>Increasing <span>\(n\)</span> results in a
“rounder” codebook that reduces quantization error. However, note that
the number of bits per weight is determined by <em>both</em> the number
of entries in <span>\(C\)</span> (m) as well as the
dimension of <span>\(C\)</span> (n). To maintain a
set number of bits per weight, a linear increase in <span>\(n\)</span> requires an exponential increase in
<span>\(m\)</span>. For example, a naively designed
16-dimensional codebook requires <span>\(2^{32}\)</span> entries to achieve 2 bits per
weight, but performing lookups into a size <span>\(2^{32}\)</span> codebook is intractable. Thus, it
is important to design codebooks that both have relatively large <span>\(n\)</span> while being compressible so the actual
lookup happens with less than <span>\(2^{nk}\)</span> entries.</p>
<p>Geometric lattices are suitable bases for such codebooks as most
lattices have inherent symmetries and certain lattices achieve optimal
bin packing densities. For example, our E8P codebook based on the <span>\(E_8\)</span> lattice has <span>\(2^{16}\)</span> entries but only requires looking
up into a size <span>\(2^8\)</span> codebook due to
symmetries inherent to the <span>\(E_8\)</span>
lattice itself – more on this later. In QuIP#, we present the E8P
codebook based on the 8-dimensional <span>\(E_8\)</span> lattice. This lattice achieves the 8
dimensional kissing number, or the maximum number of unit balls touching
a central unit ball in 8 dimensions. Interestingly, Maryna Viazovska
recently won the Fields Medal in 2022 “for the proof that the <span>\(E_8\)</span> lattice provides the densest packing
of identical spheres in 8 dimensions.”</p>
<figure>
<img src="https://cornell-relaxml.github.io/quip-sharp/img/kissing2d.png" alt="The 2D kissing number is 6, which is achieved by this packing configuration. Image from Wikipedia."/>
<figcaption aria-hidden="true">The 2D kissing number is 6, which is
achieved by this packing configuration. Image from
Wikipedia.</figcaption>
</figure>
<h4 id="e8p-codebook">E8P Codebook</h4>
<p>Our E8P codebook is a version of the <span>\(E_8\)</span> lattice intersected with a ball,
padded (hence the P in E8P) to reach <span>\(2^{16}\)</span> entries. This results in <span>\(k = 16/8 = 2\)</span> bits per weight. The <span>\(E_8\)</span> lattice is composed of 8 dimensional
all-integer or all-half integer vectors whose sum is an even number. In
set-builder notation, <span>\[E_8 = \left\{x \mid x
\in \left(\mathbb{Z}^8 \cup \left(\mathbb{Z}+\frac{1}{2}\right)^8\right)
\land \sum_i x_i \equiv 0 \pmod 2\right\}.\]</span> Note that <span>\(E_8 + \frac{1}{4}\)</span> has the same packing
density of <span>\(E_8\)</span> and is equivalent to
<span>\(D_8 + \frac{1}{2} \pm \frac{1}{4}\)</span>,
where <span>\(D_8\)</span> is the set of 8
dimensional all-integer vectors with even sum. Denote <span>\(D_8 + \frac{1}{2}\)</span> as <span>\(\hat{D_8}\)</span>; all elements in <span>\(\hat{D_8}\)</span> also have even sum parity.</p>
<p>Now, note that if we flip an even number of signs of an element in
<span>\(\hat{D_8}\)</span>, we get another element
in <span>\(\hat{D_8}\)</span>, whereas flipping an
odd number of signs results in something not in <span>\(\hat{D_8}\)</span>. This is due to <span>\(\hat{D_8}\)</span> being a half integer grid;
flipping a single half integer results in changing the sum parity. Since
<span>\(\hat{D_8}\)</span> has 8 dimensions, there
are <span>\(2^8/2 = 128\)</span> possible “even sign
flip” patterns to stay within <span>\(\hat{D_8}\)</span>. Conversely, there are also 128
“odd sign flip” patterns.</p>
<p>If we start from some “source codebook” <span>\(S\)</span> that is a subset of <span>\(|\hat{D_8}|\)</span>, where <span>\(|\cdot|\)</span> denotes the elementwise absolute
value, we can use 128 odd or even sign flips to generate a subset of
<span>\(\hat{D_8}\)</span>. Each entry in <span>\(S\)</span> is either an odd or even number of
flips away from an entry in <span>\(\hat{D_8}\)</span>, but not both. Thus, given an
entry <span>\(s \in S\)</span> and 7 out of the 8
sign flips, we can infer the last one from the parity of the 7 sign
flips and <span>\(s\)</span>. This lets us use the
following bit pattern to store a 16-bit codeword in <span>\(E_8 + \frac{1}{4}\)</span>: 8 bits for the entry
index in <span>\(S\)</span>, 7 bits for the sign
flips of the right 7 dimensions of the entry in <span>\(S\)</span>, and 1 bit to add or subtract <span>\(\frac{1}{4}\)</span>.</p>
<p>For example, if we had the codeword <code>0001010110010111</code>,
the first 8 bits <code>00010101</code> = 21 would indicate that we start
with the 21st entry in <span>\(S\)</span>. In this
example, let this be the vector</p>
<p><span>\[\left\{\frac{1}{2}, \frac{1}{2},
\frac{1}{2}, \frac{3}{2}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2},
\frac{1}{2}\right\},\]</span></p>
<p>which is not in <span>\(\hat{D_8}\)</span>. Thus,
<span>\(s\)</span> requires an odd number of sign
flips to get into <span>\(\hat{D_8}\)</span>. Then,
the next 7 bits <code>1001011</code> would indicate that we need to
negate the 1st, 2nd, 4th, and 7th from right bits. Since we need an odd
number of sign flips, the 8th from right bit is also a sign flip. The
sign-decoded vector is then</p>
<p><span>\[\left\{-\frac{1}{2}, -\frac{1}{2},
\frac{1}{2}, \frac{3}{2}, -\frac{1}{2}, \frac{1}{2}, -\frac{1}{2},
-\frac{1}{2}\right\},\]</span></p>
<p>which we can verify is in <span>\(E_8\)</span>.
Finally, the last bit <code>1</code> indicates that we need to add <span>\(\frac{1}{4}\)</span>, so the final decoded vector
is</p>
<p><span>\[\left\{-\frac{1}{4}, -\frac{3}{4},
\frac{3}{4}, \frac{7}{4}, -\frac{1}{4}, \frac{3}{4}, -\frac{1}{4},
-\frac{1}{4}\right\},\]</span></p>
<p>which is in <span>\(E_8 + \frac{1}{4}\)</span> as
desired.</p>
<p>Putting this all together, this lets us decode a size <span>\(2^{16}\)</span> codebook by looking up into only a
size <span>\(2^8\)</span> codebook (namely <span>\(S\)</span>) and performing some operations. On
hardware, this means that we can store the smaller <span>\(2^8\)</span> codebook in local caches, avoiding
performance killing memory accesses that the larger <span>\(2^{16}\)</span> codebook would require. The
question remains then of how to choose <span>\(S\)</span>. In our implementation, we set <span>\(S\)</span> to be the 227 elements of <span>\(|\hat{D_8}|\)</span> with norm <span>\(\le \sqrt{10}\)</span> plus 29 elements from <span>\(|\hat{D_8}|\)</span> that have norm <span>\(\sqrt{12}\)</span>. The exact elements chosen can
be found in our code.</p>
<h4 id="codebook-errors">Codebook Errors</h4>
<p>To show the optimality of our lattice codebooks, we plotted the
minimum achievable elementwise MSE of quantizing a <span>\(n\)</span>-dimensional multivariate Gaussian to
various <span>\(k\)</span> bit codebooks. To create
each codebook, we intersected a ball with the base lattice and increased
the radius of the ball to get more bits. The half integer codebooks are
formed from the <span>\(n\)</span>-dimensional half
integer grids.</p>
<p>Specifically, each point in the graph below plots <span>\((k, y)\)</span> where</p>
<p><span>\[y = \min_{s \in \mathbb{R}^+}
\frac{1}{n}\left\|\mbox{quantize}\left(\frac{\mathcal{N}(0_n, I_n)}{s},
\mbox{codebook}\right)*s - \mathcal{N}(0_n, I_n)\right\|^2\]</span></p>
<figure>
<img src="https://cornell-relaxml.github.io/quip-sharp/img/lattice_err.png" title="Lattice Errors" alt="Lowest element-wise mean squared error (MSE) achievable for quantizing a multivariate Gaussian to various codebooks. The E_8 lattice achieves the densest unit-sphere packing in 8 dimensions and our derivative codebooks have the lowest MSE."/>
<figcaption aria-hidden="true">Lowest element-wise mean squared error
(MSE) achievable for quantizing a multivariate Gaussian to various
codebooks. The <span>\(E_8\)</span> lattice achieves
the <a href="https://en.wikipedia.org/wiki/Kissing_number">densest
unit-sphere packing in 8 dimensions</a> and our derivative codebooks
have the lowest MSE.</figcaption>
</figure>
<p>The <span>\(E_8\)</span>-based codebooks achieves
lower MSEs than all other codebooks, including those based on the <span>\(D_4\)</span> lattice that achieves the 4
dimensional kissing number. This figure also shows the importance of
having a large number of columns <span>\(n\)</span>.
Increasing the number of columns decreases the error for the half
integer grid, as the resulting codebook is more “round.” Since the E8P
codebook is actually the union of two shifted codebooks, each of which
is a ball intersected with <span>\(\hat{D_8}\)</span>, it is not perfectly round.
This is reflected in the MSE plot, where it sits slightly above the
<span>\(E_8\)</span> line. However, there does not
exist a <span>\(E_8\)</span> codebook with exactly 2
bits, so E8P is still practically superior.</p>
<h3 id="results">Results</h3>
<p>Here, we present quantization results using QuIP# on Llama 1 and 2
models. All models were quantized using the Hadamard transform for
incoherence processing and a weight scale factor of roughly 0.9 times
the optimal scale for a multivariate Gaussian to compensate for
inter-layer interactions. Furthermore, all Llama 2 models were evaluated
using a context lenth of 4096 and all Llama 1 models were evaluated with
context length 2048; these numbers match the context length the models
were trained with. These and other models can be found in our <a href="https://huggingface.co/relaxml">Hugging Face repository</a>.</p>
<p>The table below contains results for all Llama 1 and 2 models when
quantized to 2 bits using the E8P codebook. QuIP# achieves excellent
performance across all model sizes on both language modeling and zero
shot tasks. Furthermore, on the zero-shot tasks (ArcC, ArcE, BoolQ,
PiQA, WinoGrande), QuIP# models achieve near-native performance with
minimal degradation. Additional results are available <a href="https://docs.google.com/spreadsheets/d/18woLrIBdVGUr9CuFDbK9pl_6QzEBl09hfnoe4Qkg7Hg/edit?usp=sharing">here</a>.</p>
<div>
<table>
<caption>QuIP# results across all Llama 1 and 2 models. QuIP# achieves
near-native performance at 2 bits on language modeling (C4, Wiki) and
zero shot (ArcC, ArcE, BoolQ, PiQA, WinoGrande) tasks.</caption>
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<thead>
<tr>
<th>Model</th>
<th>Method</th>
<th>C4 <span>\(\downarrow\)</span></th>
<th>Wiki <span>\(\downarrow\)</span></th>
<th>ArcC <span>\(\uparrow\)</span></th>
<th>ArcE <span>\(\uparrow\)</span></th>
<th>BoolQ <span>\(\uparrow\)</span></th>
<th>PiQA <span>\(\uparrow\)</span></th>
<th>WinoGrande <span>\(\uparrow\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>2-70B</td>
<td>fp16</td>
<td>5.533</td>
<td>3.120</td>
<td>0.480</td>
<td>0.597</td>
<td>0.766</td>
<td>0.809</td>
<td>0.768</td>
</tr>
<tr>
<td>2-70B</td>
<td>QuIP#</td>
<td>6.535</td>
<td>4.156</td>
<td>0.469</td>
<td>0.595</td>
<td>0.795</td>
<td>0.785</td>
<td>0.740</td>
</tr>
<tr>
<td>2-13B</td>
<td>fp16</td>
<td>6.520</td>
<td>4.574</td>
<td>0.443</td>
<td>0.580</td>
<td>0.690</td>
<td>0.790</td>
<td>0.699</td>
</tr>
<tr>
<td>2-13B</td>
<td>QuIP#</td>
<td>8.769</td>
<td>6.003</td>
<td>0.381</td>
<td>0.502</td>
<td>0.643</td>
<td>0.751</td>
<td>0.637</td>
</tr>
<tr>
<td>2-7B</td>
<td>fp16</td>
<td>7.036</td>
<td>5.116</td>
<td>0.406</td>
<td>0.535</td>
<td>0.710</td>
<td>0.769</td>
<td>0.670</td>
</tr>
<tr>
<td>2-7B</td>
<td>QuIP#</td>
<td>12.208</td>
<td>8.201</td>
<td>0.346</td>
<td>0.454</td>
<td>0.647</td>
<td>0.726</td>
<td>0.618</td>
</tr>
<tr>
<td>1-65b</td>
<td>fp16</td>
<td>5.811</td>
<td>3.532</td>
<td>0.463</td>
<td>0.588</td>
<td>0.823</td>
<td>0.809</td>
<td>0.771</td>
</tr>
<tr>
<td>1-65b</td>
<td>QuIP#</td>
<td>6.749</td>
<td>4.573</td>
<td>0.435</td>
<td>0.566</td>
<td>0.831</td>
<td>0.792</td>
<td>0.756</td>
</tr>
<tr>
<td>1-30B</td>
<td>fp16</td>
<td>6.130</td>
<td>4.101</td>
<td>0.453</td>
<td>0.590</td>
<td>0.684</td>
<td>0.801</td>
<td>0.728</td>
</tr>
<tr>
<td>1-30B</td>
<td>QuIP#</td>
<td>7.465</td>
<td>5.311</td>
<td>0.422</td>
<td>0.537</td>
<td>0.659</td>
<td>0.776</td>
<td>0.714</td>
</tr>
<tr>
<td>1-13B</td>
<td>fp16</td>
<td>6.798</td>
<td>5.091</td>
<td>0.444</td>
<td>0.599</td>
<td>0.684</td>
<td>0.792</td>
<td>0.701</td>
</tr>
<tr>
<td>1-13B</td>
<td>QuIP#</td>
<td>8.426</td>
<td>6.353</td>
<td>0.382</td>
<td>0.537</td>
<td>0.665</td>
<td>0.757</td>
<td>0.687</td>
</tr>
<tr>
<td>1-7B</td>
<td>fp16</td>
<td>7.343</td>
<td>5.677</td>
<td>0.415</td>
<td>0.525</td>
<td>0.731</td>
<td>0.774</td>
<td>0.670</td>
</tr>
<tr>
<td>1-7B</td>
<td>QuIP#</td>
<td>10.927</td>
<td>8.146</td>
<td>0.347</td>
<td>0.471</td>
<td>0.673</td>
<td>0.724</td>
<td>0.621</td>
</tr>
</tbody>
</table>
</div>


</div>
  </body>
</html>
