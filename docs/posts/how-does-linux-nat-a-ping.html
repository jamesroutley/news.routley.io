<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://devnonsense.com/posts/how-does-linux-nat-a-ping/">Original</a>
    <h1>How does Linux NAT a ping?</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A few months ago, I found myself wondering how a command like <code>ping 1.1.1.1</code> works from within a private network.</p><p>In most private networks, multiple hosts connect to the Internet through a router. For IPv4, the router performs network address translation (NAT) by rewriting the original host’s source address to the router’s public IP address. The router can lookup the correct host for a reply packet based on the packet’s <em>port</em> field, at least for protocols like TCP and UDP.</p><p>But a command like <code>ping</code> doesn’t use TCP or UDP; it uses ICMP, and those packets do <em>not</em> have a port field. So how does NAT work for ICMP packets?</p><p>This led me down a deep rabbit hole: running experiments in network namespaces, capturing packets, reading RFCs, and tracing through the Linux source code. This post summarizes what I did and learned along the way.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p><em>Before these experiments, I hadn’t spent much time in the Linux networking code – this is something new I’m learning. If I’ve made any mistakes please <a href="https://devnonsense.com/contact">let me know</a> so I can correct them.</em></p><details open=""><summary><b>Table of contents</b></summary><nav id="TableOfContents"><ul><li><a href="#experiment-setup">Experiment setup</a><ul><li><a href="#step-1-connect-two-clients-to-a-bridge">Step 1: Connect two clients to a bridge</a></li><li><a href="#step-2-connect-natbox-and-server">Step 2: Connect natbox and server</a></li><li><a href="#step-3-configure-routing-and-nat">Step 3: Configure routing and NAT</a></li></ul></li><li><a href="#packet-capture">Packet capture</a></li><li><a href="#rfc-792">RFC 792</a></li><li><a href="#ping-source-code">Ping source code</a></li><li><a href="#id-conflict">ID conflict</a></li><li><a href="#netfilter-conntrack-and-nat">Netfilter, conntrack, and NAT</a></li><li><a href="#bpftrace">bpftrace</a></li><li><a href="#conclusion">Conclusion</a></li></ul></nav></details><h2 id="experiment-setup">Experiment setup</h2><p>One of the best ways to understand Linux networking is through experimentation. These days, it’s easy to run experiments using <a href="https://www.man7.org/linux/man-pages/man7/network_namespaces.7.html">network namespaces</a> to simulate multiple devices on a single Linux machine.</p><p>This is the setup I wanted to test:</p><p><img src="https://devnonsense.com/img/icmp-nat-setup.svg" alt="Diagram showing the setup of the experiment"/></p><p>There are two clients (client1 and client2) connected to a router (natbox) performing NAT from private network 192.168.99.0/24 to public network 10.0.100.0/24. The clients, natbox, and server are each separate network namespaces. Once everything is ready, a <code>ping</code> from either client to the server at <code>10.0.100.2</code> should get a reply!</p><p>For these experiments, I used a Fedora 38 Server VM running version 6.2.9 of the Linux kernel. Most of the below commands (<code>ip</code>, <code>iptables</code>, <code>tcpdump</code>, etc.) were run as the root user.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><h3 id="step-1-connect-two-clients-to-a-bridge">Step 1: Connect two clients to a bridge</h3><p>The first step is to create two clients connected to a bridge, like this:
<img src="https://devnonsense.com/img/icmp-nat-clients-and-bridge.svg" alt="Diagram showing two clients connected to a bridge"/></p><p>To set it up:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># Create a network namespace for each client.</span>
</span></span><span><span>ip netns add <span>&#34;client1&#34;</span>
</span></span><span><span>ip netns add <span>&#34;client2&#34;</span>
</span></span><span><span>
</span></span><span><span><span># Create a virtual bridge.</span>
</span></span><span><span>ip link add name <span>&#34;br0&#34;</span> <span>type</span> bridge
</span></span><span><span>ip link <span>set</span> dev <span>&#34;br0&#34;</span> up
</span></span><span><span>
</span></span><span><span><span># Disable iptables processing for bridges so rules don&#39;t block traffic over br0.</span>
</span></span><span><span>sysctl -w net.bridge.bridge-nf-call-iptables<span>=</span><span>0</span>
</span></span><span><span>
</span></span><span><span><span># Connect client1 to the bridge with a veth pair and assign IP address 192.168.99.1</span>
</span></span><span><span>ip link add dev <span>&#34;vethclient1&#34;</span> <span>type</span> veth peer name <span>&#34;eth0&#34;</span> netns <span>&#34;client1&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethclient1&#34;</span> master <span>&#34;br0&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethclient1&#34;</span> up
</span></span><span><span>ip -n <span>&#34;client1&#34;</span> addr add dev <span>&#34;eth0&#34;</span> <span>&#34;192.168.99.1/24&#34;</span>
</span></span><span><span>ip -n <span>&#34;client1&#34;</span> link <span>set</span> dev <span>&#34;eth0&#34;</span> up
</span></span><span><span>
</span></span><span><span><span># Same for client2, with IP address 192.168.99.2</span>
</span></span><span><span>ip link add dev <span>&#34;vethclient2&#34;</span> <span>type</span> veth peer name <span>&#34;eth0&#34;</span> netns <span>&#34;client2&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethclient2&#34;</span> master <span>&#34;br0&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethclient2&#34;</span> up
</span></span><span><span>ip -n <span>&#34;client2&#34;</span> addr add dev <span>&#34;eth0&#34;</span> <span>&#34;192.168.99.2/24&#34;</span>
</span></span><span><span>ip -n <span>&#34;client2&#34;</span> link <span>set</span> dev <span>&#34;eth0&#34;</span> up
</span></span></code></pre></div><p>If this worked, then:</p><ul><li><code>ip netns</code> should show <code>client1</code> and <code>client2</code>.</li><li><code>ip -n client1 addr</code> and <code>ip -n client2 addr</code> should show <code>192.168.99.1</code> and <code>192.168.99.2</code> respectively, and the <code>eth0</code> interface should show “state UP”.</li></ul><p>Now the two clients can ping each other over the bridge:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># ping client1 -&gt; client2</span>
</span></span><span><span>ip netns <span>exec</span> client1 ping 192.168.99.2
</span></span><span><span>
</span></span><span><span><span># ping client2 -&gt; client1</span>
</span></span><span><span>ip netns <span>exec</span> client2 ping 192.168.99.1
</span></span></code></pre></div><h3 id="step-2-connect-natbox-and-server">Step 2: Connect natbox and server</h3><p>Next, create network namespaces for the natbox and server:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip netns add <span>&#34;natbox&#34;</span>
</span></span><span><span>ip netns add <span>&#34;server&#34;</span>
</span></span></code></pre></div><p>Then connect the natbox to the bridge:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip link add dev <span>&#34;vethnatbox&#34;</span> <span>type</span> veth peer name <span>&#34;eth0&#34;</span> netns <span>&#34;natbox&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethnatbox&#34;</span> master <span>&#34;br0&#34;</span>
</span></span><span><span>ip link <span>set</span> <span>&#34;vethnatbox&#34;</span> up
</span></span><span><span>ip -n <span>&#34;natbox&#34;</span> addr add dev <span>&#34;eth0&#34;</span> <span>&#34;192.168.99.3/24&#34;</span>
</span></span><span><span>ip -n <span>&#34;natbox&#34;</span> link <span>set</span> dev <span>&#34;eth0&#34;</span> up
</span></span></code></pre></div><p>The natbox needs a second interface in the 10.0.100.0/24 network, so add that and call it “eth1”. Since there’s only one server, there’s no need for a bridge – just connect the natbox and server directly with a veth pair:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip -n <span>&#34;natbox&#34;</span> link add <span>&#34;eth1&#34;</span> <span>type</span> veth peer name <span>&#34;eth1&#34;</span> netns <span>&#34;server&#34;</span>
</span></span><span><span>ip -n <span>&#34;natbox&#34;</span> addr add dev <span>&#34;eth1&#34;</span> <span>&#34;10.0.100.1/24&#34;</span>
</span></span><span><span>ip -n <span>&#34;natbox&#34;</span> link <span>set</span> dev <span>&#34;eth1&#34;</span> up
</span></span><span><span>ip -n <span>&#34;server&#34;</span> addr add dev <span>&#34;eth1&#34;</span> <span>&#34;10.0.100.2/24&#34;</span>
</span></span><span><span>ip -n <span>&#34;server&#34;</span> link <span>set</span> dev <span>&#34;eth1&#34;</span> up
</span></span></code></pre></div><p>Now the natbox can reach both clients and the server. Test it with ping:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># ping natbox -&gt; client1</span>
</span></span><span><span>ip netns <span>exec</span> natbox ping 192.168.99.1
</span></span><span><span>
</span></span><span><span><span># ping natbox -&gt; client2</span>
</span></span><span><span>ip netns <span>exec</span> natbox ping 192.168.99.2
</span></span><span><span>
</span></span><span><span><span># ping natbox -&gt; server</span>
</span></span><span><span>ip netns <span>exec</span> natbox ping 10.0.100.2
</span></span></code></pre></div><p>At this point, every network namespace, interface, and veth pair has been created:
<img src="https://devnonsense.com/img/icmp-nat-setup.svg" alt="Diagram showing the setup of the experiment"/></p><p>However, the client cannot yet ping the server because the natbox isn’t forwarding traffic between its interfaces or performing NAT.</p><h3 id="step-3-configure-routing-and-nat">Step 3: Configure routing and NAT</h3><p>Add a default route in each client to send traffic to the natbox:</p><pre tabindex="0"><code>ip -n client1 route add 0.0.0.0/0 via 192.168.99.3
ip -n client2 route add 0.0.0.0/0 via 192.168.99.3
</code></pre><p>For security reasons, Linux does not forward packets between interfaces unless specifically enabled. So configure the natbox to forward traffic by setting <code>net.ipv4.ip_forward</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip netns <span>exec</span> natbox sysctl <span>&#34;net.ipv4.ip_forward=1&#34;</span>
</span></span></code></pre></div><p>At this point, packets from a client will reach the server. However, these packets will retain the original source IP in the 192.168.99.0/24 network, so replies from the server back to this IP will go… nowhere. Fix it by configuring the natbox to NAT the traffic from a client IP (in network 192.168.99.0/24) to the natbox’s public IP (10.0.100.1/24). The easiest way to do this is to add a MASQUERADE rule to the iptables “nat” chain:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip netns <span>exec</span> natbox iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
</span></span></code></pre></div><p>At last, clients can reach the server through the natbox! Test it with ping:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># ping client1 -&gt; server via natbox</span>
</span></span><span><span>ip netns <span>exec</span> natbox ping 10.0.100.2
</span></span><span><span>
</span></span><span><span><span># ping client2 -&gt; server via natbox</span>
</span></span><span><span>ip netns <span>exec</span> natbox ping 10.0.100.2
</span></span></code></pre></div><h2 id="packet-capture">Packet capture</h2><p>Now capture ICMP packets from both client and server network namespaces.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>ip netns <span>exec</span> client1 tcpdump -n icmp
</span></span><span><span>ip netns <span>exec</span> server tcpdump -n icmp
</span></span></code></pre></div><p>This is the tcpdump for client1:</p><pre tabindex="0"><code>08:01:33.549598 IP 192.168.99.1 &gt; 10.0.100.2: ICMP echo request, id 31428, seq 1, length 64
08:01:33.549661 IP 10.0.100.2 &gt; 192.168.99.1: ICMP echo reply, id 31428, seq 1, length 64
08:01:34.610605 IP 192.168.99.1 &gt; 10.0.100.2: ICMP echo request, id 31428, seq 2, length 64
08:01:34.610654 IP 10.0.100.2 &gt; 192.168.99.1: ICMP echo reply, id 31428, seq 2, length 64
</code></pre><p>… and the corresponding tcpdump for the server:</p><pre tabindex="0"><code>08:01:33.549643 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 31428, seq 1, length 64
08:01:33.549654 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 31428, seq 1, length 64
08:01:34.446611 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 33391, seq 1, length 64
08:01:34.446619 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 33391, seq 1, length 64
08:01:34.610635 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 31428, seq 2, length 64
08:01:34.610646 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 31428, seq 2, length 64
08:01:35.506411 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 33391, seq 2, length 64
08:01:35.506423 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 33391, seq 2, length 64
</code></pre><p>These captures show that:</p><ul><li>Traffic is being NAT’d. By the time an ICMP echo request reaches the server (10.0.100.2), its source IP has been rewritten to the IP of the natbox (10.0.100.1).</li><li>Each client has a different “id” field (in the capture above, client1 has ID 31428 and client2 has ID 33391).</li></ul><p>The “id” field seemed like it might allow the natbox to distinguish reply packets destined for each client. But what does the “id” field mean, and how is it chosen?</p><h2 id="rfc-792">RFC 792</h2><p>ICMP is a very, very old protocol. It is defined in <a href="https://datatracker.ietf.org/doc/html/rfc792">RFC 792</a>, which was published in 1981. The RFC specifies the exact structure of an ICMP echo and echo reply message:</p><pre tabindex="0"><code>    0                   1                   2                   3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |     Type      |     Code      |          Checksum             |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |           Identifier          |        Sequence Number        |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |     Data ...
   +-+-+-+-+-
</code></pre><p>The “type” field distinguishes an echo request (8) from an echo reply (1). Code is always 0 (I guess it isn’t used for anything?). What about “sequence number” and “identifier”?</p><blockquote><p>If code = 0, an identifier to aid in matching echos and replies,
may be zero…</p><p>If code = 0, a sequence number to aid in matching echos and
replies, may be zero…</p><p>The identifier and sequence number may be used by the echo sender
to aid in matching the replies with the echo requests. For
example, the identifier might be used like a port in TCP or UDP to
identify a session, and the sequence number might be incremented
on each echo request sent. The echoer returns these same values
in the echo reply.</p></blockquote><p>The RFC doesn’t say anything about how the IDs are actually chosen. That’s not part of the protocol specification, so the next step is to look at an implementation – in this case, the source code for the <code>ping</code> command.</p><h2 id="ping-source-code">Ping source code</h2><p>The <code>ping</code> command is part of the “iputils” package, with source code available at <a href="https://github.com/iputils/iputils">github.com/iputils/iputils</a>. There is a <a href="https://github.com/iputils/iputils/blob/b50711313236696e322b38fb34c0b11cc13cc526/ping/ping.c#L1511-L1519">comment</a> just before <code>ping4_send_probe</code>:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>/*
</span></span></span><span><span><span> * pinger --
</span></span></span><span><span><span> * 	Compose and transmit an ICMP ECHO REQUEST packet.  The IP packet
</span></span></span><span><span><span> * will be added on by the kernel.  The ID field is a random number,
</span></span></span><span><span><span> * and the sequence number is an ascending integer.  The first several bytes
</span></span></span><span><span><span> * of the data portion are used to hold a UNIX &#34;timeval&#34; struct in VAX
</span></span></span><span><span><span> * byte-order, to compute the round-trip time.
</span></span></span><span><span><span> */</span>
</span></span></code></pre></div><p>So <code>ping</code> chooses the identifier randomly. It’s a bit difficult to see where this actually happens in the code, but from what I understand:</p><ol><li>There is a <code>struct ping_rts</code> that has a field <code>ident</code>.</li><li>The <code>ident</code> field <a href="https://github.com/iputils/iputils/blob/b50711313236696e322b38fb34c0b11cc13cc526/ping/ping.c#L327C9-L327C9">defaults to <code>-1</code></a>, but can be <a href="https://github.com/iputils/iputils/blob/b50711313236696e322b38fb34c0b11cc13cc526/ping/ping.c#L375-L378">overridden by the CLI flag “-e”</a> to any value between zero and <code>IDENTIFIER_MAX</code> (0xFFFF).</li><li>When <code>rts-&gt;ident == -1</code>, <code>ping</code> binds to a socket with type <code>SOCK_DGRAM</code> and protocol <code>IPPROTO_ICMP</code>. <a href="https://github.com/iputils/iputils/blob/b50711313236696e322b38fb34c0b11cc13cc526/ping/ping.c#L893-L895">In this configuration, it does not modify <code>source.sin_port</code></a>, so the source port is zero.</li></ol><p>I didn’t find much documentation for how Linux implements <code>SOCK_DGRAM</code> sockets with <code>IPPROTO_ICMP</code>, except for this description from the <a href="https://lore.kernel.org/lkml/20110413113204.GB6948@albatros/T/">mailing list “net: ipv4: add IPPROTO_ICMP socket kind”</a>:</p><blockquote><p>ICMP headers given to send() are checked and sanitized. The type must be
ICMP_ECHO and the code must be zero (future extensions might relax this,
see below). <strong>The id is set to the number (local port) of the socket</strong>, the
checksum is always recomputed.</p></blockquote><p>I suspect that when <code>ping</code> doesn’t specify a source port (<code>source.sin_port == 0</code>), then the Linux kernel chooses a free port at random. This port then gets used as the ID for ICMP packets.</p><h2 id="id-conflict">ID conflict</h2><p>What happens if two <code>ping</code> processes on different hosts both choose the exact same ID? Test it using <code>ping -e</code> to explicitly set the ICMP ID to the same value for both clients:</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span># ping from client1 -&gt; server with ICMP ID 999</span>
</span></span><span><span>ip netns <span>exec</span> client1 ping 10.0.100.2 -e <span>999</span>
</span></span><span><span>
</span></span><span><span><span># ping from client2 -&gt; server with ICMP ID 999</span>
</span></span><span><span>ip netns <span>exec</span> client2 ping 10.0.100.2 -e <span>999</span>
</span></span></code></pre></div><p>This time, the packet capture from the server shows something different:</p><pre tabindex="0"><code>10:22:18.807289 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 999, seq 1, length 64
10:22:18.807300 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 999, seq 1, length 64
10:22:19.838650 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 999, seq 2, length 64
10:22:19.838661 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 999, seq 2, length 64
10:22:20.011677 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 30218, seq 1, length 64
10:22:20.011687 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 30218, seq 1, length 64
10:22:20.862591 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 999, seq 3, length 64
10:22:20.862603 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 999, seq 3, length 64
10:22:21.054598 IP 10.0.100.1 &gt; 10.0.100.2: ICMP echo request, id 30218, seq 2, length 64
10:22:21.054614 IP 10.0.100.2 &gt; 10.0.100.1: ICMP echo reply, id 30218, seq 2, length 64
</code></pre><p>One of the clients is using ID 999, but the other one is using ID 30218. Where did that second ID come from? Time to go to the Linux source code.</p><h2 id="netfilter-conntrack-and-nat">Netfilter, conntrack, and NAT</h2><p>The kernel subsystem responsible for implementing iptables rules is called “netfilter.” The iptables MASQUERADE rule is responsible for NAT’ing packets, so the NAT implementation for ICMP must be in netfilter. Grep’ing through the <code>net/netfilter</code> directory in the Linux repository, I found a few places where the ICMP “id” field is used:</p><ul><li>In “nf_nat_core.c” the function <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L580"><code>nf_nat_setup_info</code></a> calls <code>get_unique_tuple</code>, which calls <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L380"><code>nf_nat_l4proto_unique_tuple</code></a>. There is a switch statement with a case for <code>IPPROTO_ICMP</code>, and a reference to <code>&amp;tuple-&gt;src.u.icmp.id</code>.</li><li>In “nf_nat_proto.c” the function <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_proto.c#L419"><code>nf_nat_manip_pkt</code></a> calls <code>nf_nat_ipv4_manip_pkt</code>, which calls <code>l4proto_manip_pkt</code>. When the protocol is <code>IPPROTO_ICMP</code> this calls <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_proto.c#L223"><code>icmp_manip_pkt</code>, which has a line <code>hdr-&gt;un.echo.id = tuple-&gt;src.u.icmp.id</code></a>.</li></ul><p>In order to NAT packets, netfilter needs to store something called a <em>connection</em>. For TCP, not surprisingly, this represents the TCP connection, uniquely identified by the 5-tuple (src IP, src port, dst IP, dst port, L4 protocol). However, in netfilter the term “connection” has a broader meaning: it can correlate outgoing and incoming packets <em>even for connectionless protocols</em> like UDP and ICMP.</p><p>Examining the <a href="https://elixir.bootlin.com/linux/v6.2.9/source/include/net/netfilter/nf_conntrack.h#L75"><code>nf_conn</code></a> data structure:</p><ul><li><code>nf_conn</code> has a field <code>struct nf_conntrack_tuple_hash tuplehash[IP_CT_DIR_MAX]</code>. There are two tuple hashes, one for outgoing packets and one for incoming packets (<code>IP_CT_DIR_ORIGINAL</code> and <code>IP_CT_DIR_REPLY</code> respectively).</li><li>Each <code>nf_conntrack_tuple_hash</code> has a field <code>nf_conntrack_tuple tuple</code> with the tuple uniquely identifying the connection.</li><li>Each tuple is split into a part that can be manipulated, called <code>src</code>, and a part that is immutable called <code>dst</code>.<ul><li><code>src</code> has type <code>struct nf_conntrack_man</code>, which has an IP address (<code>union nf_inet_addr u3</code>) and protocol-specific fields (<code>union nf_conntrack_man_proto u</code>). For ICMP, the protocol-specific field is <code>__be16 id</code>.</li><li><code>dst</code> has the unmodified IP address as well as the ICMP <code>type</code> and <code>code</code> fields.</li></ul></li></ul><p>Connection tracking and NAT are closely related. To NAT a packet, netfilter needs to “remember” how it modified the outgoing packet so it can reverse those modifications on the reply packet. It does so by representing the modifications in a connection.</p><p>For ICMP, I believe netfilter works like this:</p><ol><li>When natbox receives an ICMP echo, <code>nf_nat_setup_info</code> creates a new connection. This is where it chooses whether it needs to rewrite the source IP address and/or the ICMP id field on the outgoing packet.</li><li>For each incoming and outgoing ICMP packet, the function <code>nf_nat_manip_pkt</code> sets the source IP and ICMP id field to whatever is set in the connection. The argument <code>ip_conntrack_dir dir</code> determines whether the packet is treated as an outgoing echo (rewrite the source IP) or incoming reply (rewrite the destination IP).</li></ol><p><a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L580"><code>nf_nat_setup_info</code></a> is responsible for choosing the ICMP ID for the NAT’d packets. The NAT rewrites happen in <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L504"><code>get_unique_tuple</code></a>.</p><p>Here are the key steps:</p><ol><li>On <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L541">line 541</a>, <code>find_best_ips_proto(zone, tuple, range, ct, maniptype)</code> rewrites the source IP address.</li><li>On <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L548">lines 548-560</a>, <code>nf_nat_used_tuple(tuple, ct)</code> checks whether the tuple is already being used; if not, the current tuple is returned. This explains why when two clients use <em>different</em> ICMP IDs, those IDs are preserved in the NAT’d packets.</li><li>On <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L563">line 563</a>, <code>nf_nat_l4proto_unique_tuple</code> is called to perform protocol-specific NAT (in this case manipulating the ICMP ID field).</li><li>In <code>nf_nat_l4proto_unique_tuple</code> <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L393">lines 393-403</a> set <code>keyptr = &amp;tuple-&gt;src.u.icmp.id</code> to choose the ICMP ID field as the “key” to NAT, then jumps to <code>find_free_id</code> at the end of the function.</li><li><code>find_free_id</code> on <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L471">line 471</a> calls <code>get_random_u16()</code> to generate a random ID, adjusts the value into the range<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> of valid ICMP IDs (on <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L485">line 485</a>), then checks if it’s used (another call to <code>nf_nat_used_tuple</code> on <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L486">line 486</a>).</li><li>If a tuple with the random ID not yet used, then it gets returned. Otherwise, netfilter searches for an unused ID from progressively smaller ranges starting at random offsets (<a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L483">lines 483-494</a>).</li><li>If an unused tuple cannot be found within a maximum number of attempts, then <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L491"><code>nf_nat_l4_proto_unique_tuple</code> returns</a>, leaving the duplicate ID in the connection. Later, <a href="https://elixir.bootlin.com/linux/v6.2.9/source/net/netfilter/nf_nat_core.c#L502">__nf_conntrack_confirm will detect the duplicate and drop the packet</a>.</li></ol><h2 id="bpftrace">bpftrace</h2><p>To verify my understanding of the netfilter code, I used a tool called <a href="https://github.com/iovisor/bpftrace/"><code>bpftrace</code></a>.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> After much tinkering, I ended up with this program to trace the kernel functions <code>nf_nat_setup_info</code> and <code>nf_nat_manip_pkt</code>:</p><pre tabindex="0"><code>// from linux/socket.h
#define AF_INET		2	/* Internet IP Protocol 	*/

// from net/netfilter/nf_nat.h
enum nf_nat_manip_type {
	NF_NAT_MANIP_SRC,
	NF_NAT_MANIP_DST
};

// from include/uapi/linux/netfilter/nf_conntrack_tuple_common.h
// Use #define instead of enum so we can use these in bpftrace array indices.
#define IP_CT_DIR_ORIGINAL 0
#define IP_CT_DIR_REPLY 1

kprobe:nf_nat_setup_info {
	// nf_nat_setup_info gets called twice, once in the prerouting chain
	// to modify the destination (actually a no-op), and once in the output
	// chain to modify the source (which is what we care about).
	$mtype = arg2;
	if ($mtype != NF_NAT_MANIP_SRC) {
		return;
	}

	$conn = (struct nf_conn *)arg0;
	if ($conn-&gt;tuplehash[IP_CT_DIR_ORIGINAL].tuple.dst.protonum == IPPROTO_ICMP) {
		@setupConn[tid] = $conn;
	}
}

kretprobe:nf_nat_setup_info {
	if (@setupConn[tid] == none) {
		return;
	}
	$conn = (struct nf_conn *)@setupConn[tid];
	$origTuple = $conn-&gt;tuplehash[IP_CT_DIR_ORIGINAL].tuple;
	$replyTuple = $conn-&gt;tuplehash[IP_CT_DIR_REPLY].tuple;
	printf(&#34;nf_nat_setup_info: origTuple.addr=%s, origTuple.id=%d, replyTuple.addr=%s, replyTuple.id=%d\n&#34;,
		ntop(AF_INET, $origTuple.src.u3.ip),
		bswap($origTuple.src.u.icmp.id),
		ntop(AF_INET, $replyTuple.src.u3.ip),
		bswap($replyTuple.src.u.icmp.id));
	delete(@setupConn[tid]);
}

kprobe:nf_nat_manip_pkt {
	$mtype = arg2;
	$skb = (struct sk_buff *)arg0;
	$iphdr = (struct iphdr *)$skb-&gt;data;
	$icmphdr = (struct icmphdr *)($skb-&gt;data + $iphdr-&gt;ihl * 4);
	printf(&#34;nf_nat_manip_pkt before: mtype=%d, saddr=%s, daddr=%s, icmp.type=%d, icmp.id=%d\n&#34;,
		$mtype,
		ntop(AF_INET, $iphdr-&gt;saddr),
		ntop(AF_INET, $iphdr-&gt;daddr),
		bswap($icmphdr-&gt;type),
		bswap($icmphdr-&gt;un.echo.id));
	@manipType[tid] = $mtype;
	@manipSkb[tid] = $skb
}

kretprobe:nf_nat_manip_pkt {
	$mtype = @manipType[tid];
	$skb = @manipSkb[tid];
	$iphdr = (struct iphdr *)$skb-&gt;data;
	$icmphdr = (struct icmphdr *)($skb-&gt;data + $iphdr-&gt;ihl * 4);
	printf(&#34;nf_nat_manip_pkt after: mtype=%d, saddr=%s, daddr=%s, icmp.type=%d, icmp.id=%d\n&#34;,
		$mtype,
		ntop(AF_INET, $iphdr-&gt;saddr),
		ntop(AF_INET, $iphdr-&gt;daddr),
		bswap($icmphdr-&gt;type),
		bswap($icmphdr-&gt;un.echo.id));
	delete(@manipType[tid]);
	delete(@manipSkb[tid]);
}
</code></pre><p>The important parts are:</p><ul><li><p><code>kprobe</code> traces when a kernel function is called, and <code>kretprobe</code> traces when the function returns.</p></li><li><p><code>kretprobe</code> cannot access function arguments directly, so store the arguments in a BPF map on entry and retrieve them on exit. For example, <code>kprobe:nf_nat_setup_info</code> writes the netfilter connection argument to <code>@setupConn[tid]</code> (a BPF map keyed by thread ID). Then <code>kretprobe:nf_nat_setup_info</code> reads the connection from the map and deletes the entry.</p></li><li><p><code>struct sk_buff</code> is how the Linux kernel <a href="https://docs.kernel.org/networking/skbuff.html">represents a packet</a>.</p></li><li><p><code>bswap</code> reverses byte order, which is used to convert from big endian (network byte order) to little endian.</p></li><li><p><code>ntop</code> returns the string representation of an IP address.</p></li><li><p>The BPF program can reference kernel data structures like <code>sk_buff</code> and <code>nf_conn</code> without including any headers. This is the magic of BPF Type Format (BTF) available in recent versions of the Linux kernel.</p></li><li><p>I tested this program on Linux kernel version 6.2.9. It may or may not work on other kernel versions.</p></li></ul><p>To execute the program, I saved the above code to a file called <code>trace.bt</code> then ran <code>bpftrace trace.bt</code> as root. This is what the output looks like with two clients pinging the server using the same ICMP ID (999):</p><pre tabindex="0"><code>$ bpftrace trace.bt
Attaching 4 probes...
nf_nat_setup_info: origTuple.addr=192.168.99.1, origTuple.id=999, replyTuple.addr=10.0.100.2, replyTuple.id=999
nf_nat_manip_pkt before: mtype=0, saddr=192.168.99.1, daddr=10.0.100.2, icmp.type=8, icmp.id=999
nf_nat_manip_pkt after: mtype=0, saddr=10.0.100.1, daddr=10.0.100.2, icmp.type=8, icmp.id=999
nf_nat_manip_pkt before: mtype=1, saddr=10.0.100.2, daddr=10.0.100.1, icmp.type=0, icmp.id=999
nf_nat_manip_pkt after: mtype=1, saddr=10.0.100.2, daddr=192.168.99.1, icmp.type=0, icmp.id=999
nf_nat_setup_info: origTuple.addr=192.168.99.2, origTuple.id=999, replyTuple.addr=10.0.100.2, replyTuple.id=32809
nf_nat_manip_pkt before: mtype=0, saddr=192.168.99.2, daddr=10.0.100.2, icmp.type=8, icmp.id=999
nf_nat_manip_pkt after: mtype=0, saddr=10.0.100.1, daddr=10.0.100.2, icmp.type=8, icmp.id=32809
nf_nat_manip_pkt before: mtype=1, saddr=10.0.100.2, daddr=10.0.100.1, icmp.type=0, icmp.id=32809
nf_nat_manip_pkt after: mtype=1, saddr=10.0.100.2, daddr=192.168.99.2, icmp.type=0, icmp.id=999
</code></pre><p>The output shows that <code>nf_nat_setup_info</code> gets called twice, once for each client.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> For the first client (IP 192.168.99.1), both the original and reply tuple have the ICMP ID sent by the client (999). For the second client (IP 192.168.99.2), however, the reply tuple has been <em>rewritten</em> to ID 32809. For both clients, the source IP address has been rewritten to the IP of the natbox (10.0.100.2).</p><p>Once <code>nf_nat_setup_info</code> has created the connection, <code>nf_nat_manip_pkt</code> modifies the echo and echo reply ICMP packets. For the echo packet, <code>mtype=0</code> (<code>NF_NAT_MANIP_SRC</code>) because the source IP is rewritten. Likewise, the reply packet has <code>mtype=1</code> (<code>NF_NAT_MANIP_DST</code>) to rewrite the destination IP of the incoming reply back to the original client IP.</p><h2 id="conclusion">Conclusion</h2><p>So that is how Linux NATs a ping! In the end, maybe the answer isn’t very surprising – and, in fact, I discovered much later that most of this behavior is documented in the <a href="https://netfilter.org/documentation/HOWTO/netfilter-hacking-HOWTO-4.html#ss4.4">Netfilter Hacking HOWTO</a>. But it was a fun journey, and it’s nice to know exactly where this magic happens in the code.</p></div></div>
  </body>
</html>
