<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ml-explore/mlx">Original</a>
    <h1>MLX: An array framework for Apple Silicon</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="#quickstart"><strong>Quickstart</strong></a> | <a href="#installation"><strong>Installation</strong></a> |
<a href="https://ml-explore.github.io/mlx/build/html/index.html" rel="nofollow"><strong>Documentation</strong></a> |
<a href="#examples"><strong>Examples</strong></a></p>
<p dir="auto">MLX is an array framework for machine learning on Apple silicon, brought to you
by Apple machine learning research.</p>
<p dir="auto">Some key features of MLX include:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Familiar APIs</strong>: MLX has a Python API that closely follows NumPy.
MLX also has a fully featured C++ API, which closely mirrors the Python API.
MLX has higher-level packages like <code>mlx.nn</code> and <code>mlx.optimizers</code> with APIs
that closely follow PyTorch to simplify building more complex models.</p>
</li>
<li>
<p dir="auto"><strong>Composable function transformations</strong>: MLX has composable function
transformations for automatic differentiation, automatic vectorization,
and computation graph optimization.</p>
</li>
<li>
<p dir="auto"><strong>Lazy computation</strong>: Computations in MLX are lazy. Arrays are only
materialized when needed.</p>
</li>
<li>
<p dir="auto"><strong>Dynamic graph construction</strong>: Computation graphs in MLX are built
dynamically. Changing the shapes of function arguments does not trigger
slow compilations, and debugging is simple and intuitive.</p>
</li>
<li>
<p dir="auto"><strong>Multi-device</strong>: Operations can run on any of the supported devices
(currently, the CPU and GPU).</p>
</li>
<li>
<p dir="auto"><strong>Unified memory</strong>: A notable difference from MLX and other frameworks
is the <em>unified memory model</em>. Arrays in MLX live in shared memory.
Operations on MLX arrays can be performed on any of the supported
device types without moving data.</p>
</li>
</ul>
<p dir="auto">MLX is designed by machine learning researchers for machine learning
researchers. The framework is intended to be user-friendly, but still efficient
to train and deploy models. The design of the framework itself is also
conceptually simple. We intend to make it easy for researchers to extend and
improve MLX with the goal of quickly exploring new ideas.</p>
<p dir="auto">The design of MLX is inspired by frameworks like
<a href="https://numpy.org/doc/stable/index.html" rel="nofollow">NumPy</a>,
<a href="https://pytorch.org/" rel="nofollow">PyTorch</a>, <a href="https://github.com/google/jax">Jax</a>, and
<a href="https://arrayfire.org/" rel="nofollow">ArrayFire</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-examples" aria-hidden="true" tabindex="-1" href="#examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Examples</h2>
<p dir="auto">The <a href="https://github.com/ml-explore/mlx-examples">MLX examples repo</a> has a
variety of examples, including:</p>
<ul dir="auto">
<li><a href="https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm">Transformer language model</a> training.</li>
<li>Large-scale text generation with
<a href="https://github.com/ml-explore/mlx-examples/tree/main/llama">LLaMA</a> and
finetuning with <a href="https://github.com/ml-explore/mlx-examples/tree/main/lora">LoRA</a>.</li>
<li>Generating images with <a href="https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion">Stable Diffusion</a>.</li>
<li>Speech recognition with <a href="https://github.com/ml-explore/mlx-examples/tree/main/whisper">OpenAI&#39;s Whisper</a>.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-quickstart" aria-hidden="true" tabindex="-1" href="#quickstart"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quickstart</h2>
<p dir="auto">See the <a href="https://ml-explore.github.io/mlx/build/html/quick_start.html" rel="nofollow">quick start
guide</a>
in the documentation.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" tabindex="-1" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">MLX is available on <a href="https://pypi.org/project/mlx/" rel="nofollow">PyPi</a>. To install the Python API, run:</p>

<p dir="auto">Checkout the
<a href="https://ml-explore.github.io/mlx/build/html/install.html#" rel="nofollow">documentation</a>
for more information on building the C++ and Python APIs from source.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributing" aria-hidden="true" tabindex="-1" href="#contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributing</h2>
<p dir="auto">Check out the <a href="https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md">contribution guidelines</a> for more information
on contributing to MLX.</p>
</article>
          </div></div>
  </body>
</html>
