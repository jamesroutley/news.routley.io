<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://galileo-unbound.blog/2021/06/28/a-random-walk-in-10-dimensions/">Original</a>
    <h1>A Random Walk in 10 Dimensions (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
		
<p>Physics in high dimensions is becoming the norm in modern dynamics.  It is not only that string theory operates in ten dimensions (plus one for time), but virtually every complex dynamical system is described and analyzed within state spaces of high dimensionality.  Population dynamics, for instance, may describe hundreds or thousands of different species, each of whose time-varying populations define a separate axis in a high-dimensional space.  Coupled mechanical systems likewise may have hundreds or thousands (or more) of degrees of freedom that are described in high-dimensional <a href="https://physicstoday.scitation.org/doi/10.1063/1.3397041">phase space</a>. </p>



<blockquote><p>In high-dimensional landscapes, mountain ridges are much more common than mountain peaks.  This has profound consequences for the evolution of life, the dynamics of complex systems, and the power of machine learning.</p></blockquote>



<p>For these reasons, as physics students today are being increasingly exposed to the challenges and problems of high-dimensional dynamics, it is important to build tools they can use to give them an intuitive feeling for the highly unintuitive behavior of systems in high-D.</p>



<p>Within the rapidly-developing field of <a href="https://towardsdatascience.com/machine-learning/home">machine learning</a>, which often deals with landscapes (loss functions or objective functions) in high dimensions that need to be minimized, high dimensions are usually referred to in the negative as “The Curse of Dimensionality”.</p>



<p>Dimensionality might be viewed as a curse for several reasons.  First, it is almost impossible to visualize data in dimensions higher than d = 4 (the fourth dimension can sometimes be visualized using colors or time series).  Second, too many degrees of freedom create too many variables to fit or model, leading to the classic problem of overfitting.  Put simply, there is an absurdly large amount of room in high dimensions.  Third, our intuition about relationships among areas and volumes are highly biased by our low-dimensional 3D experiences, causing us to have serious misconceptions about geometric objects in high-dimensional spaces.  Physical processes occurring in 3D can be over-generalized to give preconceived notions that just don’t hold true in higher dimensions.</p>



<p>Take, for example, the random walk.  It is usually taught starting from a 1-dimensional random walk (flipping a coin) that is then extended to 2D and then to 3D…most textbooks stopping there.  But random walks in high dimensions are the rule rather than the exception in complex systems.  One example that is especially important in this context is the problem of molecular evolution.  Each site on a genome represents an independent degree of freedom, and molecular evolution can be described as a random walk through that space, but the space of all possible genetic mutations is enormous.  Faced with such an astronomically large set of permutations, it is difficult to conceive of how random mutations could possibly create something as complex as, say, <a href="https://pdb101.rcsb.org/motm/72">ATP synthase</a> which is the basis of all higher bioenergetics.  Fortunately, the answer to this puzzle lies in the physics of random walks in high dimensions. </p>



<h2><strong>Why Ten Dimensions?</strong></h2>



<p>This blog presents the physics of random walks in 10 dimensions.  Actually, there is nothing special about 10 dimensions versus 9 or 11 or 20, but it gives a convenient demonstration of high-dimensional physics for several reasons.  First, it is high enough above our 3 dimensions that there is no hope to visualize it effectively, even by using projections, so it forces us to contend with the intrinsic “unvisualizability” of high dimensions.  Second, ten dimensions is just big enough that it behaves roughly like any higher dimension, at least when it comes to random walks.  Third, it is about as big as can be handled with typical memory sizes of computers.  For instance, a ten-dimensional hypercubic lattice with 10 discrete sites along each dimension has 10^10 lattice points (10 Billion or 10 Gigs) which is about the limit of what a typical computer can handle with internal memory.</p>



<p>As a starting point for visualization, let’s begin with the well-known 4D hypercube but extend it to a 4D hyperlattice with three values along each dimension instead of two.  The resulting 4D lattice can be displayed in 2D as a network with 3^4 = 81 nodes and 216 links or edges.  The result is shown in Fig. 1, represented in two dimensions as a network graph with nodes and edges.  Each node has four links with neighbors.  Despite the apparent 3D look that this graph has about it, if you look closely you will see the frustration that occurs when trying to link to 4 neighbors, causing many long-distance links.</p>



<p><a href="https://www.youtube.com/watch?v=gXNpyyVoh80">[See YouTube video for movies showing evolving hyperlattices and random walks in 10D.]</a></p>


<div>
<figure><img data-attachment-id="3636" data-permalink="https://galileo-unbound.blog/image-20-10/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png" data-orig-size="1666,1722" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-20" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=290" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=730" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=991" alt="" width="448" height="462" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=448 448w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=894 894w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=145 145w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=290 290w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-20.png?w=768 768w" sizes="(max-width: 448px) 100vw, 448px"/><figcaption><mark>Fig. 1   A 4D hyperlattice with three sites along each of the 4 dimensions.  This high dimensional discrete lattice is represented as a network graph in 2D with nodes and edges.</mark></figcaption></figure></div>


<p>We can also look at a 10D hypercube that has 2^10 = 1024 nodes and 5120 edges, shown in Fig. 2.  It is a bit difficult to see the hypercubic symmetry when presented in 2D, but each node has exactly 10 links.</p>


<div>
<figure><img data-attachment-id="3648" data-permalink="https://galileo-unbound.blog/image-26-7/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png" data-orig-size="1612,1600" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-26" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=1024" alt="" width="-180" height="-178" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=1024 1024w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=150 150w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=300 300w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png?w=768 768w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-26.png 1612w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><mark>Fig. 2   A 10D hypercube of 1024 nodes and 5120 edges.  Each node has exactly 10 links to neighbors</mark></figcaption></figure></div>


<p>Extending this 10D lattice to 10 positions instead of 2 and trying to visualize it is prohibitive, since the resulting graph in 2D just looks like a mass of overlapping circles. However, our interest extends not just to ten locations per dimension, but to an unlimited number of locations. This is the 10D infinite lattice on which we want to explore the physics of the random walk.</p>



<h2>Diffusion in Ten Dimensions</h2>



<p>An unconstrained random walk in 10D is just a minimal extension beyond a simple random walk in 1D.  Because each dimension is independent, a single random walker takes a random step along any of the 10 dimensions at each iteration so that motion in any one of the 10 dimensions is just a 1D random walk.  Therefore, a simple way to visualize this random walk in 10D is simply to plot the walk against each dimension, as in Fig. 3.  There is one chance in ten that the walker will take a positive or negative step along any given dimension at each time point.  </p>


<div>
<figure><img data-attachment-id="3654" data-permalink="https://galileo-unbound.blog/image-28-7/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png" data-orig-size="1398,1230" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-28" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=1024" alt="" width="434" height="382" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=434 434w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=868 868w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=150 150w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=300 300w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-28.png?w=768 768w" sizes="(max-width: 434px) 100vw, 434px"/><figcaption><mark>Fig. 3   A single walker taking random unit steps in 10 dimensions.  The position of the walker as a function of time is shown for all ten dimensions.</mark></figcaption></figure></div>


<p>An alternate visualization of the 10D random walker is shown in Fig. 4 for the same data as Fig. 3.  In this case the displacement is color coded, and each column is a different dimension.  Time is on the vertical axis (starting at the top and increasing downward).  This type of color map can easily be extended to hundreds of dimensions.  Each row is a position vector of the single walker in the 10D space</p>


<div>
<figure><img data-attachment-id="3673" data-permalink="https://galileo-unbound.blog/image-30-5/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png" data-orig-size="1642,1588" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-30" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=1024" alt="" width="364" height="352" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=364 364w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=728 728w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=150 150w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-30.png?w=300 300w" sizes="(max-width: 364px) 100vw, 364px"/><figcaption><mark>Fig. 4  Same data as in Fig. 3 for a single 10D random walker on a hyperlattice.  Distance is color coded.  Time is on the vertical axis (increasing downward). Each row is a 10D position vector, and this representation is of a single 10D trajectory.</mark></figcaption></figure></div>


<p>In the 10D hyperlattice in this section, all lattice sites are accessible at each time point, so there is no constraint preventing the walk from visiting a previously-visited node.  There is a possible adjustment that can be made to the walk that prevents it from ever crossing its own path.  This is known as a <a href="https://www.wired.com/story/what-random-walks-in-multiple-dimensions-teach-you-about-life/">self-avoiding-walk</a> (SAW).  In two dimensions, there is a major difference in the geometric and dynamical properties of an ordinary walk and an SAW.  However, in dimensions larger than 4, it turns out that there are so many possibilities of where to go (high-dimensional spaces have so much free room) that it is highly unlikely that a random walk will ever cross itself.  Therefore, in our 10D hyperlattice we do not need to make the distinction between an ordinary walk and a self-avoiding-walk.  However, there are other constraints that can be imposed that mimic how complex systems evolve in time, and these constraints can have important consequences, as we see next.</p>



<h2>Random Walk in a Maximally Rough Landscape</h2>



<p>In the infinite hyperlattice of the previous section, all lattice sites are the same and are all equally accessible.  However, in the study of complex systems, it is common to assign a value to each node in a high-dimensional lattice.  This value can be assigned by a potential function, producing a high-dimensional potential landscape over the lattice geometry.  Or the value might be the survival fitness of a species, producing a high-dimensional fitness landscape that governs how species compete and evolve.  Or the value might be a loss function (an objective function) in a minimization problem from multivariate analysis or machine learning.   In all of these cases, the scalar value on the nodes defines a landscape over which a state point executes a walk.  The question then becomes, what are the properties of a landscape in high dimensions, and how does it affect a random walker?</p>



<p>As an example, let’s consider a landscape that is completely random point-to-point.  There are no correlations in this landscape, making it maximally rough.  Then we require that a random walker takes a walk along iso-potentials in this landscape, never increasing and never decreasing its potential.  Beginning with our spatial intuition living in 3D space, we might be concerned that such a walker would quickly get confined in some area of the lanscape.  Think of a 2D topo map with countour lines drawn on it — If we start at a certain elevation on a mountain side, then if we must walk along directions that maintain our elevation, we stay on a given contour and eventually come back to our starting point after circling the mountain peak — we are trapped!  But this intuition informed by our 3D lives is misleading.  What happens in our 10D hyperlattice?</p>



<p>To make the example easy to analyze, let’s assume that our potential function is restricted to N discrete values.  This means that of the 10 neighbors to a given walker site, on average only 10/N are likely to have the same potential value as the given walker site.  This constrains the available sites for the walker, and it converts the uniform hyperlattice into a hyperlattice site percolation problem.</p>



<p>Percolation theory is a fascinating topic in statistical physics.  There are many deep concepts that come from asking simple questions about how nodes are connected across a network.  The most important aspect of percolation theory is the concept of a percolation threshold.  Starting with a complete network that is connected end-to-end, start removing nodes at random.  For some critical fraction of nodes removed (on average) there will no longer be a single connected cluster that spans the network.  This critical fraction is known as the percolation threshold.  Above the percolation threshold, a random walker can get from one part of the network to another.  Below the percolation threshold, the random walker is confined to a local cluster.  </p>



<p>If a hyperlattice has N discrete values for the landscape potential (or height, or contour) and if a random walker can only move to site that has the same value as the walker’s current value (remains on the <a href="https://mathinsight.org/level_sets">level set</a>), then only a fraction of the hyperlattice sites are available to the walker, and the question of whether the walker can find a path the spans the hyperlattice becomes simply a question of how the fraction of available sites relates to the percolation threshold.</p>



<p>The percolation threshold for hyperlattices is well known.  For reasonably high dimensions, it is given to good accuracy by</p>


<div>
<figure><img data-attachment-id="3666" data-permalink="https://galileo-unbound.blog/image-29-6/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png" data-orig-size="1000,308" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-29" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=1000" alt="" width="301" height="92" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=299 299w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=597 597w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-29.png?w=150 150w" sizes="(max-width: 301px) 100vw, 301px"/></figure></div>


<p>where d is the dimension of the hyperlattice.  For a 10D hyperlattice the percolation threshold is p<sub>c</sub>(10) = 0.0568, or about 6%.  Therefore, if more than 6% of the sites of the hyperlattice have the same value as the walker’s current site, then the walker is free to roam about the hyperlattice.  </p>



<p>If there are N = 5 discrete values for the potential, then 20% of the sites are available, which is above the percolation threshold, and walkers can go as far as they want.  This statement holds true no matter what the starting value is.  It might be 5, which means the walker is as high on the landscape as they can get.  Or it might be 1, which means the walker is as low on the landscape as they can get.  Yet even if they are at the top, if the available site fraction is above the percolation threshold, then the walker can stay on the high mountain ridge, spanning the landscape.  The same is true if they start at the bottom of a valley.  Therefore, mountain ridges are very common, as are deep valleys, yet they allow full mobility about the geography.  On the other hand, a so-called mountain peak would be a 5 surrounded by 4’s or lower.  The odds for having this happen in 10D are 0.2*(1-0.8^10) = 0.18.  Then the total density of mountain peaks, in a 10D hyperlattice with 5 potential values, is only 18%.  Therefore, mountain peaks are rare in 10D, while mountain ridges are common.  In even higher dimensions, the percolation threshold decreases roughly inversely with the dimensionality, and mountain peaks become extremely rare and play virtually no part in walks about the landscape.</p>



<p>To illustrate this point, Fig. 5 is the same 10D network that is in Fig. 2, but only the nodes sharing the same value are shown for N = 5, which means that only 20% of the nodes are accessible to a walker who stays only on nodes with the same values.  There is a “giant cluster” that remains connected, spanning the original network.  If the original network is infinite, then the giant cluster is also infinite but contains a finite fraction of the nodes.</p>


<div>
<figure><img data-attachment-id="3677" data-permalink="https://galileo-unbound.blog/image-31-5/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png" data-orig-size="904,838" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-31" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=904" alt="" width="693" height="642" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=693 693w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=150 150w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=300 300w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png?w=768 768w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-31.png 904w" sizes="(max-width: 693px) 100vw, 693px"/><figcaption><mark>Fig. 5  A 10D cluster that spans the network in Fig. 2 for 1/5 of the nodes sharing the same landscape value.  This cluster represents a mountain ridge that spans the space.  There are four additional co-existing clusters, each of which separately spans the same 10D space.</mark></figcaption></figure></div>


<p>The quantitative details of the random walk can change depending on the proximity of the sub-networks (the clusters, the ridges or the level sets) to the percolation threshold.  For instance, a random walker in D =10 with N = 5 is shown in Fig. 6.  The diffusion is a bit slower than in the unconstrained walk of Figs. 3 and 4.  But the ability to wander about the 10D space is retained.</p>


<div>
<figure><img data-attachment-id="3688" data-permalink="https://galileo-unbound.blog/image-33-4/" data-orig-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png" data-orig-size="2434,1084" data-comments-opened="0" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-33" data-image-description="" data-image-caption="" data-medium-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=300" data-large-file="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=730" loading="lazy" src="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=1024" alt="" width="729" height="325" srcset="https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=1024 1024w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=729 729w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=1458 1458w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=150 150w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=300 300w, https://galileo-unbound.blog/wp-content/uploads/2021/06/image-33.png?w=768 768w" sizes="(max-width: 729px) 100vw, 729px"/><figcaption><mark>Fig. 6  A random walker on the level-set cluster of Fig. 5</mark></figcaption></figure></div>


<p>This is then the general important result: <em><strong>In high-dimensional landscapes, mountain ridges are much more common than mountain peaks. This has profound consequences for the evolution of life, the dynamics of complex systems, and the power of machine learning.</strong></em></p>



<h2>Consequences for Evolution and Machine Learning</h2>



<p>When the high-dimensional space is the space of possible mutations on a genome, and when the landscape is a fitness landscape that assigns a survival advantage for one mutation relative to others, then the random walk describes the evolution of a species across generations.  The prevalence of ridges, or more generally level sets, in high dimensions has a major consequence for the evolutionary process, because a species can walk along a level set acquiring many possible mutations that have only neutral effects on the survivability of the species.  At the same time, the genetic make-up is constantly drifting around in this “neutral network”, allowing the species’ genome to access distant parts of the space.  Then, at some point, natural selection may tip the species up a nearby (but rare) peak, and a new equilibrium is attained for the species.  </p>



<p>One of the early criticisms of fitness landscapes was the (erroneous) criticism that for a species to move from one fitness peak to another, it would have to go down and cross wide valleys of low fitness to get to another peak.  But this was a left-over from thinking in 3D.  In high-D, neutral networks are ubiquitous, and a mutation can take a step away from one fitness peak onto one of the neutral networks, which can be sampled by a random walk until the state is near some distant peak.  It is no longer necessary to think in terms of high peaks and low valleys of fitness — just random walks.  The evolution of extremely complex structures, like ATP synthase, can then be understood as a random walk along networks of nearly-neutral fitness — once our 3D biases are eliminated.</p>



<p>The same arguments hold for many situations in machine learning and especially deep learning.  When training a deep neural network, there can be thousands of neural weights that need to be trained through the minimization of a loss function, also known as an objective function.  The loss function is the equivalent to a potential, and minimizing the loss function over the thousands of dimensions is the same problem as maximizing the fitness of an evolving species.  </p>



<p>At first look, one might think that deep learning is doomed to failure.  We have all learned, from the earliest days in calculus, that enough adjustable parameter can fit anything, but the fit is meaningless because it predicts nothing.  Deep learning seems to be the worst example of this.  How can fitting thousands of adjustable parameters be useful when the dimensionality of the optimization space is orders of magnitude larger than the degrees of freedom of the system being modeled?</p>



<p>The answer comes from the geometry of high dimensions.  The prevalence of neutral networks in high dimensions gives lots of chances to escape local minima.  In fact, local minima are actually rare in high dimensions, and when they do occur, there is a neutral network nearby onto which they can escape (if the effective temperature of the learning process is set sufficiently high).  Therefore, despite the insanely large number of adjustable parameters, general solutions, that are meaningful and predictive, can be found by adding random walks around the objective landscape as a partial strategy in combination with gradient descent.</p>



<p>Given the superficial analogy of deep learning to the human mind, the geometry of random walks in ultra-high dimensions may partially explain our own intelligence and consciousness.</p>



<h2>Biblography</h2>



<p>S. Gravilet, <em>Fitness Landscapes and the Origins of Species</em>. Princeton University Press, 2004.</p>



<p>M. Kimura, <em>The Neutral Theory of Molecular Evolution</em>. Cambridge University Press, 1968.</p>



<p><a href="https://www.youtube.com/watch?v=gXNpyyVoh80">YouTube Vlog on A Random Walk in 10 Dimensions</a></p>
			</div></div>
  </body>
</html>
