<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gianluca.ai/2024-38/">Original</a>
    <h1>Weeknotes: Continuing at RC, a new game, and epic links</h1>
    
    <div id="readability-page-1" class="page"><div><p>I started writing weeknotes as part of <a href="https://www.recurse.com/self-directives#learn-generously">learning generously</a> when I <a href="https://gianluca.ai/recurse-init">started at Recurse</a>. But the distinction between Recurse and non-Recurse has increasingly blurred and I’d like to continue making weeknotes after I <a href="https://www.recurse.com/about#never-graduate">never graduate</a>. To that end, I’m trying out a new format for my weeknotes. Tell me what you think!</p><p>See previous weeknotes via the <a href="https://gianluca.ai/tags/weeknotes">weeknotes tag</a>.</p><hr/><h2 id="updates">Updates</h2><ul><li>Decided to extend to a 12-week batch at the <a href="https://gianluca.ai/recurse-init">Recurse Center</a>. You can continue following my progress by filtering for posts with the <a href="https://gianluca.ai/tags/recurse">Recurse tag</a>.</li><li>“QS Rubicon”: After 8 years of doing extensive self-tracking and <a href="https://gianluca.ai/quantified-sleep">ML-powered Quantified Self projects</a>, I’ve decided to just stop collecting most data. I have plenty to say about the whole experience, so it should probably be its own blog post or talk. But the immediate feeling of being “out of control” is still something I’m trying to reflect on.</li></ul><h2 id="inputs--outputs">Inputs / outputs</h2><ul><li>Wrote and published 4 posts<ul><li><a href="https://gianluca.ai/make-games-people-play">Make games people play</a>: What Paul Graham doesn’t tell you about delighting 7-year-olds.</li><li><a href="https://gianluca.ai/aimong-us-game">AImong Us: a reverse Turing Test game</a>: Can you outwit a gang of LLMs?</li><li><a href="https://gianluca.ai/what-o1-means">What OpenAI’s o1 really means</a>: The AGI is dead. Long live the AGI.</li><li><a href="https://gianluca.ai/rc-weeknotes-05">RC Weeknotes 05</a>: Move fast and make games.</li></ul></li><li><a href="https://github.com/gianlucatruda/aaahhh">AAAHHH!</a> The Creative Coding prompt at RC this week was “Aaaahhhh!” I whipped up something between a click trainer game and a psychological torture device. It also works on mobile if you like torturing yourself on the go.</li><li>Participated in an intense Rock Paper Scissors tournament at RC. It involved multiple rounds of submitting a Python bot (with no dependencies) that played 200 rounds against some simple bots as well as the other participants’ bots. I managed to finish 3rd using a third-order Markov model with a stop-loss heuristic to switch back to the Game Theory optimal random play style.</li><li>Made massive improvements to <a href="https://github.com/gianlucatruda/dotfiles">my dotfiles</a> for better macOS package management with advanced Homebrew wizardry, and better LSP configuration for Python and Markdown in NeoVim.</li><li>Improved <a href="https://github.com/simonw/llm/pull/571">PR</a> to <code>llm</code> with tests.</li></ul><p>Try AAAHHH full screen at <a href="https://aaahhh.vercel.app/">aaahhh.vercel.app</a></p><h2 id="ideas">Ideas</h2><ul><li><a href="https://open.spotify.com/playlist/58OzzuOaqlkjseZxl7Qmp2?si=2fa1032bd5724833">Industrial techno</a> is more powerful than nootropics for high-octane coding.</li><li>My hunch: The selective pressure of gradient descent optimises the explainability out of the model.<ul><li>Based on <a href="https://youtu.be/9-Jl0dxWQs8?t=1025">implications of representational superposition</a>.</li><li>LLMs favour almost-orthogonal representations (89º-91º) which makes interpretability harder, but adds a ton more “capacity” to the model. My hunch is gradient descent is such a strong pressure on the network that it’s akin to evolutionary pressure on gene networks. The chaos and difficulty of interpretation comes from aggressively optimising for performance.</li></ul></li><li>“There are no AI-shaped holes” <a href="https://x.com/matthewclifford/status/1834271090295644477">via Matt Clifford</a><blockquote><p>“There are no AI-shaped holes lying around” -&gt; this is how I reconcile the facts that (a) AI is already powerful and (b) it’s having relatively little impact so far. Making AI work today requires ripping up workflows and rebuilding <em>for</em> AI. This is hard and painful to do…</p></blockquote></li><li>Leave something for tomorrow: “One of my favorite things to do: Stop working right in the middle of something and leave the unfinished work for the next day. On the next day, you know exactly where to pick up and can start right away. Hemingway, apparently, had the same habit. Here’s the crucial bit: ‘stop when you are going good and when you know what will happen next.’” <a href="https://registerspill.thorstenball.com/p/leave-something-for-tomorrow">via Thorsten Ball</a></li></ul><h2 id="links">Links</h2><ul><li>Google’s <a href="https://notebooklm.google.com/">NotebookLM</a> turns massive quantities of text into almost-perfect podcast discussions. I tried it on my <a href="https://gianluca.ai/table-diffusion">MSc Thesis</a>. It happily gobbled up the entire PDF and produced an accessible and almost entirely correct discussion about it in around 5 minutes. I’m shocked to be saying this, but Google absolutely cooked!</li><li><a href="https://github.com/vladkens/macmon">macmon</a>, a sudoless CPU/GPU/power monitor for Apple Silicon. Vlad pipped me to implementing this before I could learn enough Rust. So good, it killed my leading RC project idea.</li><li><a href="https://www.youtube.com/watch?v=mTa2d3OLXhg">This interview</a> with DHH (creator of ruby on rails). Deeply cracked and charismatic. This got me pumped!</li><li><a href="https://vale.sh/">Vale</a> - A linter for prose (which I’m using while writing this post in neovim)</li><li>An <a href="https://youtu.be/2ccPTpDq05A">S3 doc</a> about 1X Technologies’ robot for the home</li><li><a href="https://github.com/charmbracelet/glow">Glow</a> is a handy command line tool for rendering markdown. It doesn’t support streaming, but is nevertheless handy for viewing README etc.</li><li>Pieter Levels (@levelsio) <a href="https://overcast.fm/+AAeZyCWiX34">on the Lex Fridman podcast</a>. The personification of a cracked and scrappy developer doing things simply and rapidly.</li><li><a href="https://youtu.be/9-Jl0dxWQs8">3Blue1Brown video on LLM interpretation</a>. As usual for Grant, it’s an excellent video full of intuition pumps. But the <a href="https://youtu.be/9-Jl0dxWQs8?t=1025">section on superposition</a> was both mind-shattering and brilliantly visualised.</li><li><a href="https://youtu.be/2LiTewGxvr0">Dylan Beattie’s video on UTF-16 edge cases</a>, told in the most lovely and brilliant storyteller style.</li></ul><h2 id="this-week-i-learned">This week I learned</h2><p>(Copy-pasted from my <code>#TIL</code>-tagged notes in Obsidian from the past week.)</p><ul><li>From <a href="https://youtu.be/kCc8FmEb1nY">one of Karpathy’s NN:ZtoH videos</a><ul><li><code>#TIL</code> in transformers, a <em>decoder</em> block masks out future inputs with a lower triangular matrix, but an <em>encoder</em> block does not. Thus an encoder can “see the future.” Via <a href="https://youtu.be/kCc8FmEb1nY?t=4794">Let’s build GPT: from scratch, in code, spelled out. - YouTube</a></li><li><code>#TIL</code> Karpathy has a nice set of intuitions on why Residual / skip connections work well: a “gradient superhighway.” Because addition operation means that backprop distributes the gradients equally across branches, so you maintain a direct input-output pathway throughout training, meaning the complex branches don’t choke the training initially. <a href="https://youtu.be/kCc8FmEb1nY?t=5294">Let’s build GPT: from scratch, in code, spelled out. - YouTube</a></li><li>BatchNorm is normalising (u=0, s=1) activations across batch dimension.</li><li>LayerNorm is normalising activations across the layer dimension.</li></ul></li><li><code>#TIL</code> about this trick with <code>/dev/tty</code>. My current workaround for streaming LLM response but still rendering rich (with glow) at the end: <code>llm &#34;write some markdown&#34; | tee /dev/tty | glow</code></li><li><code>#TIL</code> <code>cmd+option+H</code> hides others on macOS. I use <code>cmd+H</code> to hide the focussed app all the time, but being able to hide everything but the current app is super helpful for quickly cleaning up. Via <a href="https://github.com/dharmapoudel/hammerspoon-config">GitHub - dharmapoudel/hammerspoon-config: My configuration files for Hammerspoon.</a></li><li><code>#TIL</code> you can use <code>mas</code> and <code>cu</code> alongside homebrew to manage and automatically update non-homebrew apps that are installed via the Mac App Store. Also, instead of <code>brew cask uninstall &lt;caskname&gt;</code> you can do <code>brew cask zap &lt;caskname&gt;</code> which may also do additional removal of preferences, caches, updaters, etc. stored in <code>~/Library</code>. — via <a href="https://gist.github.com/ChristopherA/a579274536aab36ea9966f301ff14f3f">this gist</a>, notes in [[Brew-Bundle-Brewfile-Tips-·-GitHub]]</li><li><code>#TIL</code> (<a href="https://simonwillison.net/2024/Aug/21/usrbinenv-uv-run/">via Simon Willison</a>) that with a <code>uv</code> shebang and some header material, you can make a great deployable Python script / app:
<code>#!/usr/bin/env -S uv run</code><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>#!/usr/bin/env -S uv run</span>
</span></span><span><span><span># /// script</span>
</span></span><span><span><span># requires-python = &#34;&gt;=3.12&#34;</span>
</span></span><span><span><span># dependencies = [</span>
</span></span><span><span><span>#     &#34;flask==3.*&#34;,</span>
</span></span><span><span><span># ]</span>
</span></span><span><span><span># ///</span>
</span></span><span><span>print(<span>&#34;Hello world&#34;</span>)
</span></span></code></pre></td></tr></tbody></table></div></div></li><li><code>#TIL</code> if you see line endings that are <code>^M</code> in a CL tool, it’s how unix-like systems represent the Windows line endings <code>\r\n</code>. So if you do find-replace on the <code>\r</code> it gets rid of that. Then use <code>:set fileformat=unix</code> in vim and save.</li><li><code>#TIL</code> with <a href="https://manned.org/strings">GNU strings</a> you can quickly print all the strings found in a binary. It didn’t seem to work on Rust binaries, so I presume it’s just C. Handy for reverse engineering stuff.</li><li><code>#TIL</code> I can use the following workflow to find misspellings in my text files:<ul><li><code>cat index.md | aspell --lang=en_GB list | sort | uniq</code></li><li><code>cat content/posts/**/*index.md | aspell --lang=en_GB list | sort | uniq</code> gave me an alphabetical wordlist to consider for my <code>ignore.txt</code> list.</li></ul></li><li><code>#TIL</code> with nvim-lspconfig, <code>[d</code> and <code>]d</code> map to <code>vim.diagnostic.goto_prev()</code> and <code>vim.diagnostic.goto_next()</code> (respectively). A good way to linearly clear warnings and error in neovim.</li></ul><hr/></div></div>
  </body>
</html>
