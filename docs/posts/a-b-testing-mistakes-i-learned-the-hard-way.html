<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://newsletter.posthog.com/p/ab-testing-mistakes-i-learned-the">Original</a>
    <h1>A/B testing mistakes I learned the hard way</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em><span>Welcome to Product for Engineers, a newsletter created by </span><a href="https://posthog.com/?utm_source=posthog-newsletter&amp;utm_medium=email" rel="">PostHog</a><span> for engineers and founders who want to build successful startups.</span></em></p><p>Running experiments is equal parts powerful and terrifying. </p><p>Powerful because you can validate changes that will transform your product for the better; terrifying because there are so many ways to mess them up.</p><p><span>I’ve run hundreds of A/B tests, both in my previous life as a growth engineer at Meta, and </span><a href="https://liorn.substack.com/p/6-pricing-ab-tests-ive-run-and-which" rel="">on my personal side project</a><span>. </span></p><p>These are some classic mistakes I’ve learned the hard way and how to avoid them.</p><p><span>A good hypothesis explains both </span><em>what </em><span>you’re testing and </span><em>why </em><span>you’re testing it. A bad one will just lead to a lot of wasted time, or worse still changes that unknowingly damage your product.</span></p><p>To understand this better, let&#39;s look at a classic example of a bad hypothesis:</p><blockquote><p><strong>❌ Bad hypothesis:</strong><span> Changing the color of the &#34;Proceed to checkout&#34; button will increase purchases.</span></p></blockquote><p><span>It’s bad because it&#39;s unclear both why we&#39;re testing this change </span><em>and</em><span> why we expect it to increase purchases.</span></p><p>As a result, we’re unsure what we need to measure. Do we only need to count button clicks, or are there other metrics we need to look at?</p><p>Here&#39;s a better version:</p><blockquote><p><span>✅ </span><strong>Good hypothesis:</strong><span> User research showed that people are unsure of how to proceed to the checkout page. Changing the button&#39;s color will lead to more people noticing it and proceeding to the checkout page. This will lead to more purchases.</span></p></blockquote><p>It&#39;s clear now what we need to measure both:</p><ol><li><p><strong>Button clicks</strong><span>, to see if changing the color leads to more people noticing the checkout button.</span></p></li><li><p><strong>Number of purchases</strong><span>, since more people arriving at the checkout page should mean more purchases.</span></p></li></ol><p>This also makes it easier to investigate the results when they’re not what we expect:  </p><ul><li><p>If button clicks stay the same, the page may have a different problem. </p></li><li><p>If button clicks increase but the number of purchases do not, there may be an issue with the checkout page.</p></li></ul><p>Ensuring your hypothesis answers these three questions:</p><ol><li><p>Why am I running the test?</p></li><li><p>What change am I testing?</p></li><li><p>What do I expect to happen?</p></li></ol><p>Imagine you’re testing a change to your sign-up flow and you run the test on 1,000 users. </p><p>The change affects both desktop and mobile users and you get the following results:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png" width="626" height="147" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:147,&#34;width&#34;:626,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:17121,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F231487f3-a132-4ec4-b4cf-e5918d6439a0_626x147.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>This looks like a clear win for the new version, but breaking down the results by device-type shows something completely different:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png" width="622" height="219" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:219,&#34;width&#34;:622,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:28618,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1fa109ed-19b0-4b89-9ca7-d3f7c625564d_622x219.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>In fact, while the new flow worked great on mobile, conversion was lower on desktop – an insight we missed when we combined these metrics. </p><p><span>This phenomenon is known as </span><strong><a href="https://medium.com/homeaway-tech-blog/simpsons-paradox-in-a-b-testing-93af7a2f3307" rel="">Simpson&#39;s paradox</a></strong><span> – i.e. when experiments show one outcome when analyzed at an aggregated level, but a different one when analyzed by subgroups. </span></p><p>Breaking down your experiments by relevant user properties, like:</p><ul><li><p>User location or language</p></li><li><p>Subscription or pricing tier</p></li><li><p>New vs existing users</p></li><li><p>Device type</p></li><li><p>Business size</p></li><li><p>Job title or category</p></li><li><p>Acquisition channel</p></li></ul><p>This means including users who don&#39;t have access to the feature you&#39;re testing, or have already completed the goal of your test – e.g. running a pricing experiment, but including users who have already subscribed to your product.</p><p>Including these unaffected users will skew results, either leading to incorrect conclusions, inconclusive results, and/or longer experiment durations because there are too many “idle” users in an experiment’s test group. </p><p><span>Filtering out ineligible users in your code </span><em>before</em><span> you evaluate their experiment variant</span><strong>. </strong></p><p><span>These code blocks assume you’re using </span><a href="https://posthog.com/ab-testing" rel="">PostHog for A/B testing</a><span>, but the principles apply to any tool you’re using.</span></p><pre><code><span>❌ Wrong

function showNewChanges(user) {
  </span><strong>if (posthog.getFeatureFlag(&#39;experiment-key&#39;)</strong><span> === &#39;control&#39;) {
    return false;
  }

  </span><strong>if (user.hasCompletedAction) {</strong><span>
    return false
  }

  // other checks

  return true
}
</span></code></pre><p><span>The above is wrong because </span><code>posthog.getFeatureFlag</code><span> is called before the </span><code>if (user.hasCompletedAction)</code><span> check.</span></p><pre><code><span>✅ Correct

function showNewChanges(user) {

  </span><strong>if (user.hasCompletedAction) {</strong><span>
    return false
  }

  // other checks

  </span><strong>if (posthog.getFeatureFlag(&#39;experiment-key&#39;)</strong><span> === &#39;control&#39;) {
    return false;
  }

  return true
}</span></code></pre><p>This code will filter out those users before including them in the experiment.</p><p>… aka The Peeking Problem.</p><p>​​Sure, you might want to see how your test is performing before it&#39;s finished, but don’t make decisions using incomplete data.</p><p><span>Be especially wary if the early results are statistically significant, since this may actually change in the final results. As </span><a href="https://posthog.com/product-engineers/ab-testing-examples#improving-accuracy-with-dynamic-p-values" rel="">Airbnb found</a><span>, it’s possible for tests to hit “significance” early in an experiment’s running, only to converge back to neutral.</span></p><p>Don&#39;t fall victim to temptation.</p><p>Calculating how long you need to run your A/B test before you start and sticking to it. Without it, you can’t differentiate between intermediate or final results.</p><p><span>We </span><a href="https://posthog.com/docs/experiments/sample-size-running-time" rel="">calculate the recommended test duration</a><span> automatically in PostHog, but you can do it manually using any number of online calculators provided you have the following:</span></p><ul><li><p>The current count or conversion rate for your metric.</p></li><li><p>Your &#34;minimum detectable effect&#34; – the smallest change you want to detect.</p></li><li><p>Your desired level of confidence. The industry standard is 95%.</p></li></ul><p><span>Sometimes engineers are so eager to get results from their experiments that they jump straight to running them with </span><em>all </em><span>their users. </span></p><p>This might be okay if everything is set up correctly, but if you&#39;ve made a mistake, you may be unable to rerun your experiment. </p><p>Imagine you start an A/B test with all your users, but a few days later you notice that your change is causing the app to crash for many users. You immediately stop the experiment and fix the root cause of the crash. </p><p>Restarting the experiment now will produce biased results since many users have already seen your change.</p><p><span>Heeding the sage advice </span><a href="https://youtu.be/bueFTrwHFEs?si=QvTVjqeO3twWMDa9&amp;t=84" rel="">Ice Cube</a><span>: </span><em>“Check yourself before you wreck yourself.”</em></p><p><span>Run a </span><a href="https://posthog.com/tutorials/phased-rollout" rel="">phased rollout</a><span> with a small group of users for a few days. Once you&#39;re confident everything works correctly, you can start the experiment with the remaining users.</span></p><p>Here’s a list of what to check during your rollout:</p><ul><li><p>Logging is working correctly.</p></li><li><p>No increase in crashes or other errors.</p></li><li><p><a href="https://posthog.com/session-replay" rel="">Session replays</a><span> to ensure your app is behaving as expected.</span></p></li><li><p>Users are assigned to the control and test groups in the ratio you are expecting (e.g. 50/50).</p></li></ul><p>Counter metrics are vital metrics that could be indirectly impacted by your experiment. They can reveal unintended negative side-effects that harm your product or business.</p><p>For example, say you&#39;re testing a change to your signup page. Conversion increases, indicating the change is working, but you also see a drop in activation, and little discernible increase in daily active users.</p><p>These are counter metrics and they’re suggesting your change is leading to more new users who don’t onboard or use the product. This could be because the new page is misleading users about what your app does, resulting in more signups but also more churn.</p><p><span>Creating a list of </span><a href="https://posthog.com/product-engineers/product-health-metrics" rel="">product health metrics</a><span> you want to monitor, such as retention, session duration, or daily active users, and check how they change during and after your experiment.</span></p><p><span>Another option is to </span><a href="https://posthog.com/tutorials/holdout-testing" rel="">run a holdout test</a><span>, where a small group of users is not shown your changes for a long period of time – e.g. weeks or months after your experiment ends. This helps you to verify that the experiment doesn’t have negative long-term effects.</span></p><ul><li><p><strong><a href="https://posthog.com/blog/session-replay-pricing" rel="">We’ve decided to make less money [Part 1]</a><span> and </span><a href="https://posthog.com/blog/analytics-pricing" rel="">[Part 2]</a></strong><span> – We recently decided to massively cut pricing for both our session replay and analytics products. These posts cover how and why.</span></p></li><li><p><strong><a href="https://read.highgrowthengineer.com/p/why-perfectionism-destroys-your-productivity" rel="">Why perfectionism destroys your productivity, and what to do instead</a></strong><span> – </span></p><span> and </span><span> share how to fight the urge to over polish your work, and discover the magic of “good enough”.</span></li><li><p><strong><a href="https://alexeymk.com/2023/09/11/statistical-significance-on-a-shoestring-budget" rel="">Statistical Significance on a Shoestring Budget</a><span> </span></strong><span>–</span><strong> </strong><span>Alexey shares techniques on how to increase experimental power when you’re lacking user volume.</span></p></li><li><p><strong><a href="https://posthog.com/product-engineers/ab-testing-guide-for-engineers" rel="">A software engineer&#39;s guide to A/B testing</a><span> </span></strong><span>– This guide I wrote for the PostHog blog covers common misconceptions about A/B testing, and how to approach running your first ever A/B test.</span></p></li></ul><p><em><span>Words by </span><a href="https://twitter.com/LiorNn" rel="">Lior Neu-ner</a><span>, who has wasted months of his life running pointless A/B tests.</span></em></p></div></div></div>
  </body>
</html>
