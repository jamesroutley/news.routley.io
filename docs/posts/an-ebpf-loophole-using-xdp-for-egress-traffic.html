<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://loopholelabs.io/blog/xdp-for-egress-traffic">Original</a>
    <h1>An eBPF Loophole: Using XDP for Egress Traffic</h1>
    
    <div id="readability-page-1" class="page"><div><h4 id="tldr"><a data-card="" href="#tldr">TL;DR:</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h4>
<p><em>XDP (eXpress Data Path) is the fastest packet processing framework in linux - but it only works for incoming (ingress)
traffic. We discovered how to use it for outgoing (egress) traffic by exploiting a loophole in how the linux kernel
determines packet direction. Our technique delivers 10x better performance than current solutions, works with existing
Docker/Kubernetes containers, and requires zero kernel modifications.</em></p>
<p><em>This post not only expands on the overall implementation but also outlines how existing container and VM workloads can
immediately take advantage with minimal effort and zero infrastructure changes.</em></p>

<p>At Loophole Labs, we live migrate everything - <a href="https://loophole.sh/rejekts2024" rel="noreferrer noopener" target="_blank">containers</a>,
<a href="https://loophole.sh/kc2024" rel="noreferrer noopener" target="_blank">VMs</a>, and even <a href="https://youtu.be/HrtX0JrjekE?si=h1xDagSnR4vaOB2z&amp;t=1683" rel="noreferrer noopener" target="_blank">network connections</a>.</p>
<p>During a migration every single packet for a workload needs to be intercepted, modified, encapsulated, encrypted, and
rerouted to its new destination - all without the application noticing. Our scale requires us to be able to move
workloads across clouds at <em>hundreds of gigabits per second</em> - and with that sort of performance requirement, every
single CPU cycle matters.</p>
<p>All of this is to say, we need to be able to process packets at <strong>line-rate</strong> (however much the underlying network can
support, whether that&#39;s 20Gbps or 200Gbps), and there&#39;s really only one approach that lets us do that:</p>
<div><p><img id="xdp-process-dark" alt="Linux Packet Processing Performance Comparison" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-dark.png&amp;w=3840&amp;q=75"/><img id="xdp-process-light" alt="Linux Packet Processing Performance Comparison" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-process-light.png&amp;w=3840&amp;q=75"/></p><p aria-label="description for xdp-process" id="xdp-process-label">Linux Packet Processing Performance Comparison</p></div>
<p>In Linux, the gold standard for high-performance packet processing is
<a href="https://docs.ebpf.io/linux/program-type/BPF_PROG_TYPE_XDP/" rel="noreferrer noopener" target="_blank">XDP (eXpress Data Path)</a>. By intercepting packets as soon as
they arrive at the network driver (before reaching the kernel) XDP is able to achieve line-rate speeds in most environments.</p>
<p>Our own benchmarks above show how easily we were able to reach line-rate with XDP, not to mention the fact that major companies
like <a href="https://engineering.fb.com/2018/05/22/open-source/open-sourcing-katran-a-scalable-network-load-balancer/" rel="noreferrer noopener" target="_blank">Meta</a>,
<a href="https://blog.cloudflare.com/l4drop-xdp-ebpf-based-ddos-mitigations/" rel="noreferrer noopener" target="_blank">Cloudflare</a>, and
<a href="https://gcore.com/blog/geoip-filtering" rel="noreferrer noopener" target="_blank">GCore</a> have already been using it for more than 5 years now to handle 10s of
millions of packets per second.</p>
<h2 id="xdps-main-limitation"><a data-card="" href="#xdps-main-limitation">XDP&#39;s Main Limitation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Unfortunately XDP has one fundamental flaw that everyone accepts as fact: <strong>it only works for ingress (incoming)
traffic</strong>. This isn&#39;t a bug or an oversight - it&#39;s the entire identity of XDP, one of the main characteristics that
define it. XDP <em>only</em> processes packets on ingress. Period.</p>
<p>For routers and load balancers, this limitation is perfectly fine: every packet they handle arrives from an external
interface, making it all ingress from the kernel&#39;s perspective.</p>
<p>Our network plane, on the other hand, has to run on the same compute nodes as the workloads that we&#39;re live migrating.
And when these workloads generate packets - initiating connections, sending responses, etc. - that&#39;s considered egress
traffic by the host kernel. XDP simply does not work in this scenario.</p>

<p>A popular method for handling egress packets is <a href="https://docs.ebpf.io/linux/program-type/BPF_PROG_TYPE_SCHED_CLS/" rel="noreferrer noopener" target="_blank">Traffic Control (TC)</a>,
another eBPF-based mechanism that allows for packet processing at both ingress <em>and egress</em>. TC is already commonly used
for traffic shaping, queuing, filtering, and policing outbound traffic. In fact, it&#39;s the de facto standard in the
Kubernetes ecosystem - CNIs like <a href="https://docs.cilium.io/en/stable/network/ebpf/intro/" rel="noreferrer noopener" target="_blank">Cilium</a> and
<a href="https://docs.tigera.io/calico/latest/about/kubernetes-training/about-ebpf#architecture-overview" rel="noreferrer noopener" target="_blank">Calico</a> all rely on
TC for egress control because, until now, XDP for egress simply wasn&#39;t possible.</p>
<p>Given all of this, TC might seem like an obvious choice for our use case as well, but it has a fundamental flaw of its
own:</p>
<p><em>Performance.</em></p>
<p>We haven&#39;t been able to process more than <a href="#benchmarks">21Gbps with TC on egress</a> (or more than
<a href="#xdp-process">23Gbps on ingress</a>),which makes it a non-starter for our needs. The reason why TC suffers from
this performance bottleneck is due to how (and more importantly <em>when</em>) the linux kernel runs the TC program:</p>
<div><p><img id="tc-dark" alt="TC Program Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-dark.png&amp;w=3840&amp;q=75"/><img id="tc-light" alt="TC Program Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Ftc-light.png&amp;w=3840&amp;q=75"/></p><p aria-label="description for tc" id="tc-label">TC Program Flow Diagram</p></div>
<p>As shown in the diagram above, TC programs operate quite late in the networking stack, after packets have already spent
some time travelling through the linux kernel. By the time a packet reaches the TC hook, the kernel has already processed
it through various subsystems for routing, firewalling, and even connection tracking. This means that we&#39;ve wasted quite
a few CPU cycles before our TC program even runs.</p>
<p>Another major limitation of TC is that it works on socket buffers (called <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;M16.5921 9.1962s-.354-3.298-3.627-3.39c-3.2741-.09-4.9552 2.474-4.9552 6.14 0 3.6651 1.858 6.5972 5.0451 6.5972 3.184 0 3.5381-3.665 3.5381-3.665l6.1041.365s.36 3.31-2.196 5.836c-2.552 2.5241-5.6901 2.9371-7.8762 2.9201-2.19-.017-5.2261.034-8.1602-2.97-2.938-3.0101-3.436-5.9302-3.436-8.8002 0-2.8701.556-6.6702 4.047-9.5502C7.444.72 9.849 0 12.254 0c10.0422 0 10.7172 9.2602 10.7172 9.2602z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>struct</span><span> sk_buff</span></span></code></span> <a href="https://docs.kernel.org/networking/skbuff.html" rel="noreferrer noopener" target="_blank">in the
linux kernel</a>) which are allocated per-packet. This structure -
while necessary for linux&#39;s own packet handling - comes with a significant performance hit due to the allocations
themselves as well as the additional memory copies required to populate it. This all becomes doubly problematic when
you&#39;re trying to process millions of packets every second.</p>
<p>XDP, on the other hand, not only operates directly on the raw packet memory (because it runs directly in the network
drivers before the packet even reaches the linux kernel) but does so at the earliest point in the packet&#39;s lifecycle,
meaning almost no CPU cycles have been spent by the time our XDP program starts running. All this results in zero-copy
packet processing, meaning packets can be inspected, modified, and redirected with the absolute minimum overhead possible.</p>
<p>For us XDP is a hard requirement, and while the industry seems to have accepted that this is impossible, we haven&#39;t.</p>

<p>One of our core beliefs at Loophole Labs is that every so-called &#34;limitation&#34; imposed by modern infrastructure is really
just a problem we haven&#39;t solved yet. In the spirit of this, we decided to go digging through the linux kernel source in
an attempt to understand exactly why and how the kernel decides to classify a packet as &#34;ingress&#34; in the first place.</p>
<p>As it turns out, linux doesn&#39;t actually classify the packet at all. When a packet arrives at a physical network
interface, the network card writes the contents into an RX ring buffer - a memory region allocated by the
device driver that the network card can write to directly via
<a href="https://docs.kernel.org/core-api/dma-api-howto.html" rel="noreferrer noopener" target="_blank">DMA (Direct Memory Access)</a>.</p>
<p>Next, the network card uses an interrupt to signal the device driver that there&#39;s a packet available for processing.
The device driver then copies the packet from the ring buffer into its RX queue. And this is exactly when the XDP
program runs: directly on the packet in the RX queue. This process is illustrated in the diagram below:</p>
<div><p><img id="xdp-dark" alt="XDP Program Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-dark.png&amp;w=3840&amp;q=75"/><img id="xdp-light" alt="XDP Program Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fxdp-light.png&amp;w=3840&amp;q=75"/></p><p aria-label="description for xdp" id="xdp-label">XDP Program Flow Diagram</p></div>
<p>If this entire process makes one thing clear, it&#39;s that there is very little work being done in between the packet
arriving at the physical interface and it being ready for the XDP program to run. The RX queue is the trigger that tells
the linux kernel how to &#34;classify&#34; the packet as ingress and whether it should run the XDP hook.</p>
<p>As we saw in <a href="#tc">this</a> diagram, the RX queue is not used at all for egress packets, and this simple limitation is
the cause of all our headaches.</p>
<p>Now that we know all this, how can we get around it? As it turns out, <em>we don&#39;t have to</em>.</p>

<p>We were reading through the various linux interface docs, hoping to find some little insight into our predicament, when
an interesting virtual interface caught our eye: <strong>Virtual Ethernet</strong>.</p>
<p>A Virtual Ethernet (<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span>) interface is a pair of network interfaces that act as a direct tunnel between each
other. When a packet is transmitted from the TX queue of one side of the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pair, a pointer to the packet&#39;s memory is
simply moved <em>to the RX queue of the other interface</em>. This makes the packet appear as if it were received by a physical
network interface with very low overhead.</p>
<p>Yep, you read that right - <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces <em>have an RX queue that&#39;s used when receiving a packet from the other side</em>.</p>
<p>To illustrate this better, let&#39;s take an example setup like the one below. We have two applications running in their own
network namespaces, with two <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs (<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth0-A</span></span></code></span> and <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth0-B</span></span></code></span>) being used to route traffic out of the namespaces.</p>
<div><p><img id="diagram-dark" alt="XDP for Egress Traffic Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-dark.png&amp;w=3840&amp;q=75"/><img id="diagram-light" alt="XDP for Egress Traffic Flow Diagram" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fdiagram-light.png&amp;w=3840&amp;q=75"/></p><p aria-label="description for diagram" id="diagram-label">XDP for Egress Traffic Flow Diagram</p></div>
<p>The key insight here is that if we send outgoing traffic through one end of the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs (<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth0-A</span></span></code></span> in the diagram
above), then from the perspective of the second interface (<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth1-A</span></span></code></span>), the packet arrives at the
RX queue of an interface, and is now considered ingress traffic. And, since XDP programs can be attached to any
interfaceâ€™s RX queue, our XDP hook will automatically run on that <em>egress</em> packet.</p>
<p>Furthermore, if we run our XDP programs in native mode like in the diagram above, packets can be processed with
zero-copy and will bypass the linux kernel entirely when we use <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>XDP_REDIRECT</span></span></code></span> to route directly to the TX queue of the
<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>eth0</span></span></code></span> interface.</p>
<p>What makes this discovery even more powerful is that modern container runtimes - Docker, Kubernetes, containerd -
already use <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs and network namespaces for container networking. Every container you&#39;re running right now is
already connected through <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces, and it looks exactly like the diagram above.</p>
<p>That&#39;s right - not only can we use XDP for egress traffic in any of these environments, but we can do it without having to
change them in any way.</p>

<p>Unfortunately, while implementing this technique seemed straightforward at first, we quickly hit a snag while
benchmarking. Our packets kept getting dropped after our XDP program ran, and at first we couldn&#39;t figure out why.</p>
<p>We decided to run <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>tcpdump</span></span></code></span> on the receiving host and realized the packets weren&#39;t even making it over the network.
Next, we decided to run <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>tcpdump</span></span></code></span> on the <em>switch</em> handling the packets, and that&#39;s when we realized what we&#39;d
missed.</p>
<p>As it turns out, when you bypass the kernel&#39;s networking stack, you inherit its responsibilities.</p>
<p>Normally, when a packet is sent out via the linux kernel, it handles the routing, checksumming, and ARP resolution for
us. But we of course have bypassed the kernel&#39;s networking stack entirely, meaning now <em>we</em> have to take full
responsibility for ensuring packets are properly formed and can actually reach their next hop.</p>
<p>Our network plane already handles proper routing for us, but we&#39;d missed both checksumming and ARP resolution.</p>
<h3 id="checksum-calculations-in-xdp"><a data-card="" href="#checksum-calculations-in-xdp">Checksum Calculations in XDP</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>XDP programs unfortunately are not provided with the same
<a href="https://docs.ebpf.io/linux/helper-function/bpf_csum_update/" rel="noreferrer noopener" target="_blank">checksum helpers</a> that TC programs get. For NAT (Network
Address Translation) or any other packet header modifications, you need to recalculate checksums manually - and when
performance matters, the trick is to use incremental checksum updates rather than full recalculations:</p>
<figure dir="ltr" tabindex="0"><div><pre><code><span><span>static</span><span> __always_inline __u16 </span><span>csum16_add</span><span>(</span><span>__u16 </span><span>csum</span><span>,</span><span> __u16 </span><span>addend</span><span>)</span><span> {</span></span>
<span><span>    csum </span><span>+=</span><span> addend</span><span>;</span></span>
<span><span>    return</span><span> csum </span><span>+</span><span> (csum </span><span>&lt;</span><span> addend)</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// Remove old IP from checksum</span></span>
<span><span>csum </span><span>=</span><span> csum16_add</span><span>(</span><span>~</span><span>tcp</span><span>-</span><span>&gt;</span><span>check</span><span>,</span><span> ~</span><span>old_ip_high</span><span>)</span><span>;</span></span>
<span><span>csum </span><span>=</span><span> csum16_add</span><span>(</span><span>csum</span><span>,</span><span> ~</span><span>old_ip_low</span><span>)</span><span>;</span></span>
<span></span>
<span><span>// Add new IP to checksum</span></span>
<span><span>csum </span><span>=</span><span> csum16_add</span><span>(</span><span>csum</span><span>,</span><span> new_ip_high</span><span>)</span><span>;</span></span>
<span><span>csum </span><span>=</span><span> csum16_add</span><span>(</span><span>csum</span><span>,</span><span> new_ip_low</span><span>)</span><span>;</span></span>
<span><span>tcp</span><span>-</span><span>&gt;</span><span>check </span><span>=</span><span> ~</span><span>csum</span><span>;</span></span></code></pre></div></figure>
<h3 id="arp-resolution"><a data-card="" href="#arp-resolution">ARP Resolution</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The linux kernel normally handles ARP to resolve IP addresses to MAC addresses and automatically sets the destination
MAC address in the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>ethernet</span></span></code></span> layer of the outgoing packet. With XDP however, we need to maintain our own ARP table
and pass in the destination MAC ourselves:</p>
<figure dir="ltr" tabindex="0"><div><pre><code><span><span>struct</span><span> arp_entry </span><span>{</span></span>
<span><span>    __u8 </span><span>mac</span><span>[ETH_ALEN]</span><span>;</span></span>
<span><span>}</span><span>;</span></span>
<span></span>
<span><span>struct</span><span> {</span></span>
<span><span>    __uint</span><span>(</span><span>type</span><span>,</span><span> BPF_MAP_TYPE_HASH</span><span>)</span><span>;</span></span>
<span><span>    __type</span><span>(</span><span>key</span><span>,</span><span> __be32</span><span>)</span><span>;</span><span>  // IP address</span></span>
<span><span>    __type</span><span>(</span><span>value</span><span>,</span><span> struct</span><span> arp_entry</span><span>)</span><span>;</span></span>
<span><span>    __uint</span><span>(</span><span>max_entries</span><span>,</span><span> 65535</span><span>)</span><span>;</span></span>
<span><span>}</span><span> arp_table </span><span>SEC</span><span>(</span><span>&#34;</span><span>.maps</span><span>&#34;</span><span>)</span><span>;</span></span>
<span></span>
<span><span>// In your XDP program, lookup the destination MAC</span></span>
<span><span>struct</span><span> arp_entry </span><span>*</span><span>entry </span><span>=</span><span> bpf_map_lookup_elem</span><span>(</span><span>&amp;</span><span>arp_table</span><span>,</span><span> &amp;</span><span>dest_ip</span><span>)</span><span>;</span></span>
<span><span>if</span><span> (</span><span>entry</span><span>)</span><span> {</span></span>
<span><span>    memcpy</span><span>(</span><span>eth</span><span>-&gt;</span><span>h_dest</span><span>,</span><span> entry</span><span>-&gt;</span><span>mac</span><span>,</span><span> ETH_ALEN</span><span>)</span><span>;</span></span>
<span><span>}</span></span></code></pre></div></figure>

<p>To validate our overall approach, we set up <a href="https://github.com/esnet/iperf" rel="noreferrer noopener" target="_blank">iPerf3</a> containers between two
200Gbps-capable EC2 instances in the same AWS VPC. We purposely reduced the MTU to 1500 since traffic to the public
internet generally <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html#jumbo_frame_instances" rel="noreferrer noopener" target="_blank">can&#39;t use jumbo frames in the first place</a>.</p>
<p><strong>We used the exact same container networking setup for all three tests</strong> - the same standard network namespaces with
<span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs that Docker and every other container runtime uses by default. The only thing we changed was how packets
were routed from the container&#39;s <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interface to the host&#39;s physical interface:</p>
<ul>
<li><strong>iptables</strong>: The default that everyone uses today - <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>PREROUTING</span></span></code></span> chains to move traffic out of the namespace</li>
<li><strong>Traffic Control</strong>: Using a TC egress program on the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces</li>
<li><strong>XDP</strong>: Our technique - Using an XDP program attached to the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces</li>
</ul>
<p>We also decided to benchmark both the generic and native XDP drivers implemented for <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces.</p>
<p>We&#39;ll let the results speak for themselves:</p>
<div><p><img id="benchmarks-dark" alt="iPerf3 Benchmark With Various Routing Strategies" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-dark.png&amp;w=3840&amp;q=75"/><img id="benchmarks-light" alt="iPerf3 Benchmark With Various Routing Strategies" loading="eager" width="0" height="0" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=3840&amp;q=75 3840w" src="https://loopholelabs.io/_next/image?url=%2Fblog%2Fposts%2Fxdp-for-egress-traffic%2Fbenchmarks-light.png&amp;w=3840&amp;q=75"/></p><p aria-label="description for benchmarks" id="benchmarks-label">iPerf3 Benchmark With Various Routing Strategies</p></div>
<p>The first two results are exactly what we expect - iptables introduces the most overhead because it routes through the
linux kernel, and traffic control performs better but still operates post-socket-buffer and can&#39;t come close to
achieving line-rate.</p>
<p>With the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>generic</span></span></code></span> XDP driver, however, we see something surprising: worse performance than our TC program. After a
little digging we realized this actually makes sense. The <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>generic</span></span></code></span> XDP driver does <em>not</em> run on the RX queue and
instead, like TC, runs after the socket buffer has been allocated. The worse performance is the result of running the
XDP program in the same place as TC but without any of the optimizations that TC benefits from.</p>
<p>With the <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>native</span></span></code></span> XDP driver (which is available in
<a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=638264dc90227cca00d20c26680171addce18e51" rel="noreferrer noopener" target="_blank">linux 4.19+</a>)
we finally see the results we&#39;ve been looking for - we&#39;re routing just shy of line-rate at about 194Gbps, 12.4 times the
throughput of iptables and about 9.2x the throughput of TC.</p>
<p>One final thing to note here are the error bars, which were significantly smaller with <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>native</span></span></code></span> XDP. This makes
sense since the bulk of our performance improvements come from bypassing the linux kernel <em>and doing less work</em>. iPerf3,
iptables, and the linux kernel are all constantly fighting for the CPU which results in inconsistent throughput.</p>

<p>One of the most exciting aspects of this discovery is how immediately applicable it is. We setup our benchmarks to
replicate how containers already use network namespaces and <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs. This means we can dramatically accelerate
container networking without changing how containers work or how they&#39;re orchestrated.</p>
<p>Consider what happens every time a containerized application sends a packet today: it traverses through iptables rules,
gets NAT&#39;d, maybe goes through connection tracking, and finally makes it out to the network. All of this happens in the
kernel, consuming precious CPU cycles that could be used by actual applications.</p>
<p>With XDP on veth interfaces, we can bypass all of that overhead. The packet goes straight from the container&#39;s namespace
through our XDP program to the physical interface. No iptables. No conntrack. Just pure, line-rate packet routing.</p>

<p>While our primary use case at Loophole Labs is live migration - where this technique enables us to transparently
reroute connections at line rates during migrations - we recognize the broader impact this can have on container
networking as a whole.</p>
<p>That&#39;s why we&#39;re working on a Docker network plugin that implements this technique. It&#39;ll be a drop-in replacement for
Docker&#39;s default bridge network driver, except it uses XDP instead of iptables for packet routing.</p>
<p>For simpler container deployments that don&#39;t need the full complexity of Kubernetes networking (microservices,
development environments, or edge computing nodes) this could mean:</p>
<ul>
<li>Doubling network throughput without any hardware upgrades</li>
<li>Dramatically reducing CPU usage for network-heavy workloads</li>
<li>Eliminating iptables as a bottleneck in container-to-container communication</li>
</ul>
<p>We plan to open source this plugin soon, but the beauty of this technique is that you don&#39;t need to wait for us.
Everything you need to implement this yourself is described in this post. The <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> pairs are already there, all that&#39;s
left is writing the XDP programs to route your packets.</p>

<p>Loophole Labs was built on a very simple premise: <a href="https://loopholelabs.io/about">Better Building Blocks = Better Applications</a>.</p>
<p>This discovery - that XDP can process egress traffic by taking advantage of <span tabindex="0" icon="&lt;svg viewBox=&#34;0 0 24 24&#34;&gt;&lt;path d=&#34;m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z&#34; fill=&#34;currentColor&#34; /&gt;&lt;/svg&gt;"><code><span><span>veth</span></span></code></span> interfaces - is the best
representation of just that, a better building block that results in significantly better applications.</p>
<p>While we&#39;ll be open-sourcing the Docker network plugin for those who want to take advantage of XDP&#39;s egress performance
for themselves, this discovery also powers something much bigger: <strong>Architect</strong>, our live migration platform.</p>
<p>Architect uses this XDP technique (as well as other breakthrough implementations for disk &amp; memory
checkpointing) to seamlessly live migrate your containers, VMs, and even active network connections between any clouds
or regions - all without your users noticing.</p>
<p>If you&#39;re interested in diving deeper into the technical details or implementing XDP egress in your own infrastructure,
join our <a href="https://loopholelabs.io/discord" rel="noreferrer noopener" target="_blank">Discord</a> where our engineering team hangs out and answers questions from
the community. Trust me, we love talking about this stuff.</p>
<h3 id="ready-to-use-live-migration"><a data-card="" href="#ready-to-use-live-migration">Ready to Use Live Migration?</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Join our waitlist to be among the first to dramatically reduce your infrastructure costs while improving
reliability:</p>
<hr role="separator" aria-label="Content divider" aria-orientation="horizontal"/><div><div><h4><span><span>â”€â”€</span><span>/</span><span>~</span><span>\</span> <span>Architect</span></span></h4><h4><span><span>â”€â”€</span><span>Optimize cluster costs and maximize node utilization, all without modifying your applications or your infrastructure.</span></span></h4><div><p><span>â”€â”€</span></p><form><div><div><p><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span><span>â–¡</span></p></div></div></form></div></div></div><hr role="separator" aria-label="Content divider" aria-orientation="horizontal"/>
<h3 id="going-to-kubecon-na-2025"><a data-card="" href="#going-to-kubecon-na-2025">Going to KubeCon NA 2025?</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>If you are in Atlanta for KubeCon NA 2025 (November 10-13), stop by <strong>Booth #1752</strong> to see live demos of workloads
migrating between clouds. We&#39;ll show you exactly how this XDP technique combines with our other innovations to make the
impossible, possible.</p></div></div>
  </body>
</html>
