<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boringsql.com/posts/regresql-testing-queries/">Original</a>
    <h1>RegreSQL: Regression Testing for PostgreSQL Queries</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p><strong>TL;DR</strong> - <em>RegreSQL brings PostgreSQL&#39;s regression testing methodology to your application queries, catching both correctness bugs and performance regressions before production.</em></p>
<p>As puzzling as it might seem, the common problem with production changes is the ever-present &#34;AHA&#34; moment when things start slowing down or crashing straight away. Testing isn&#39;t easy as it is, but there&#39;s a widespread practice gap when it comes to testing SQL queries. Some might pretend to &#34;fix it&#34; by using ORMs to abstract away the problem. Others treat SQL as &#34;just glue code&#34; that doesn&#39;t deserve systematic testing. Most settle for integration tests that verify the application layer works, never actually testing whether their queries will survive the next schema change or index modification.</p>
<p>For PostgreSQL development itself, the project has a robust regression test suite that has been preventing disasters in core development for decades. The database itself knows how to test SQL systematically - we just don&#39;t use those same techniques for our own queries. Enter <a href="https://github.com/boringSQL/regresql">RegreSQL</a>, a tool originally created by Dimitri Fontaine for <a href="https://theartofpostgresql.com"><em>The Art of PostgreSQL</em></a> book (which is excellent for understanding and mastering PostgreSQL as a database system), designed to bring the same regression testing framework to our application queries.</p>
<p>I&#39;ve been trying to use it for some time, but due to missing features and limitations gave up several times. Until now. I decided to fork the project and spend the time needed to take it to the next level.</p>
<h2 id="introduction">Introduction
</h2>
<p>The <strong>RegreSQL</strong> promise starts with the biggest strength and perceived weakness of SQL queries. They are just strings. And unless you use something like <a href="https://sqlc.dev">sqlc</a> (for Go), <a href="https://github.com/darioteixeira/pgocaml">PG&#39;OCaml</a> or Rust&#39;s <a href="https://github.com/launchbadge/sqlx">SQLx</a> toolkit giving you compile-time checking, your queries are validated only when they are executed. Which in better case mean either usually slow-ish test suite or integration tests, in worst scenario only when deployed. ORMs are another possibility - completely abstracting away SQL (but more on that later).</p>
<p>But even with compile-time checking, you are only checking for one class of problems: schema mismatches. What about behavior changes after schema migration or performance regressions? What about understanding whether your optimization actually made things faster or just moved the problem elsewhere?</p>
<p>This is where RegreSQL comes in. Rather than trying to turn SQL into something else, RegreSQL embraces &#34;SQL as strings&#34; reality and applies the same testing methodology PostgreSQL itself uses: regression testing. You write (or generate - continue reading) your SQL queries, provide input data, and RegreSQL verifies that future changes don&#39;t break those expectations.</p>
<p>The features don&#39;t stop there though - it tracks performance baselines, detects common query plan regressions (like sequential scans), and gives you framework for systematic experimentation with the schema changes and query change management.</p>
<h2 id="basic-regression-testing">Basic regression testing
</h2>
<p>Enough with theory. Let&#39;s jump in straight into the action and see what a sample run of RegreSQL looks like</p>
<pre><code><span>$ regresql text
</span><span>Connecting to &#39;postgres://radim:password123@192.168.139.28/cdstore_test&#39;… ✓
</span><span>
</span><span>Running regression tests...
</span><span>
</span><span>✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
</span><span>✓ album-by-artist_list-albums-by-artist.2.json (0.00s)
</span><span>
</span><span>✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
</span><span>✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)
</span><span>
</span><span>✓ artist_top-artists-by-album.1.json (0.00s)
</span><span>
</span><span>✓ genre-topn_genre-top-n.top-1.json (0.00s)
</span><span>✓ genre-topn_genre-top-n.top-3.json (0.00s)
</span><span>
</span><span>✓ genre-tracks_tracks-by-genre.json (0.00s)
</span><span>
</span><span>Results: 8 passed, 0 failed, 8 skipped (0.00s)
</span></code></pre>
<p>In this example based on <a href="https://github.com/lerocha/chinook-database">Chinook database</a> (as used originally in The Art of PostgreSQL book), RegreSQL scans the current directory (or one provided by <code>-C /path/to/project</code>) for <code>*.sql</code> files and attempts to run all queries against the configured PostgreSQL connection.</p>
<p>The individual files can contain either single or multiple sql queries. Like following example</p>
<pre data-lang="sql"><code data-lang="sql"><span>-- name: top-artists-by-album
</span><span>-- Get the list of the N artists with the most albums
</span><span>SELECT
</span><span>    </span><span>artist</span><span>.</span><span>name</span><span>,
</span><span>    </span><span>count</span><span>(</span><span>*</span><span>) AS albums
</span><span>FROM
</span><span>    artist
</span><span>    </span><span>LEFT JOIN</span><span> album </span><span>USING</span><span> (artist_id)
</span><span>GROUP BY
</span><span>    </span><span>artist</span><span>.</span><span>name
</span><span>ORDER BY
</span><span>    albums </span><span>DESC
</span><span>LIMIT</span><span> :n;
</span></code></pre>
<p>The syntax for the queries supports both positional arguments (like <code>$1</code> known from libpq library) or (preferred) <code>psql</code> style variable (<code>:varname</code>). The each identified query (not file) is then executed for 0..N times, based on number of predefined plans and verified to the expected results - validating the expected data matches the one returned. The support for SQL files handling is available separately with https://github.com/boringSQL/queries (Go version only for now).</p>
<p>This gives you what original RegreSQL tool has introduced - change your schema, refactor a query, run <code>regresql test</code> and see immediately what broke. The test suite now has ability to catch regressions before they are committed / shipped. The current version built on top of it, giving you better console formatter instead of TAP style output, as well as jUnit, JSON and GitHub actions formatters for better integration into your CI/CD pipelines.</p>
<h2 id="performance-regression-testing">Performance regression testing
</h2>
<p>Basic regression testing catches correctness issues - wrong results, broken queries, schema mismatches. But there&#39;s another class of production issues it misses. Performance regressions. No matter how unbelievable it might sound but queries get deployed without appropriate indexes, or they change over time. Simple fix - both for handwritten SQL or ORM code - can switch from milliseconds to seconds. You add index that helps one query, but tanks another. You modify conditionals and accidently force sequential scan of millions of rows. This is where it hurts.</p>
<p>RegreSQL addresses this by tracking performance baselines alongside correctness. Once baselines are generated</p>
<pre data-lang="text"><code data-lang="text"><span>$ regresql baseline
</span><span>Connecting to &#39;postgres://appuser:password123@192.168.139.28/cdstore_test&#39;… ✓
</span><span>Creating baselines directory: regresql/baselines
</span><span>Creating directory &#39;regresql/baselines&#39;
</span><span>
</span><span>Creating baselines for queries:
</span><span>
</span><span>  ./
</span><span>  Created baseline: album-by-artist_list-albums-by-artist.1.json
</span><span>  Created baseline: album-by-artist_list-albums-by-artist.2.json
</span><span>  Created baseline: album-tracks_list-tracks-by-albumid.1.json
</span><span>  Created baseline: album-tracks_list-tracks-by-albumid.2.json
</span><span>  Created baseline: artist_top-artists-by-album.1.json
</span><span>  Created baseline: genre-topn_genre-top-n.top-1.json
</span><span>  Created baseline: genre-topn_genre-top-n.top-3.json
</span><span>  Created baseline: genre-tracks_tracks-by-genre.json
</span><span>
</span><span>Baselines have been created successfully!
</span><span>Baseline files are stored in: regresql/baselines
</span></code></pre>
<p>the test command not only tests the regressions to the captured times, but also detects the common bad patterns in query execution plans. For now it provides warnings for detection of sequential scans - both on their and/or with nested loops and multiple sort operations. I believe this alone might provide a valuable insights and reduce the mishaps in production. It&#39;s also a place where further development of RegreSQL will take place.</p>
<p>To demonstrate this, let&#39;s review the test output with the baselines.</p>
<pre data-lang="text"><code data-lang="text"><span>Connecting to &#39;postgres://appuser:password123@192.168.139.28/cdstore_test&#39;… ✓
</span><span>
</span><span>Running regression tests...
</span><span>
</span><span>✓ album-by-artist_list-albums-by-artist.1.json (0.00s)
</span><span>✓ album-by-artist_list-albums-by-artist.2.json (0.00s)
</span><span>✓ album-by-artist_list-albums-by-artist.1.cost (22.09 &lt;= 22.09 * 110%) (0.00s)
</span><span>  ⚠️  Sequential scan detected on table &#39;artist&#39;
</span><span>    Suggestion: Consider adding an index if this table is large or this query is frequently executed
</span><span>  ⚠️  Nested loop join with sequential scan detected
</span><span>    Suggestion: Add index on join column to avoid repeated sequential scans
</span><span>✓ album-by-artist_list-albums-by-artist.2.cost (22.09 &lt;= 22.09 * 110%) (0.00s)
</span><span>  ⚠️  Sequential scan detected on table &#39;artist&#39;
</span><span>    Suggestion: Consider adding an index if this table is large or this query is frequently executed
</span><span>  ⚠️  Nested loop join with sequential scan detected
</span><span>    Suggestion: Add index on join column to avoid repeated sequential scans
</span><span>
</span><span>✓ album-tracks_list-tracks-by-albumid.1.json (0.00s)
</span><span>✓ album-tracks_list-tracks-by-albumid.2.json (0.00s)
</span><span>✓ album-tracks_list-tracks-by-albumid.1.cost (8.23 &lt;= 8.23 * 110%) (0.00s)
</span><span>✓ album-tracks_list-tracks-by-albumid.2.cost (8.23 &lt;= 8.23 * 110%) (0.00s)
</span><span>
</span><span>✓ artist_top-artists-by-album.1.json (0.00s)
</span><span>✓ artist_top-artists-by-album.1.cost (35.70 &lt;= 35.70 * 110%) (0.00s)
</span><span>  ⚠️  Multiple sequential scans detected on tables: album, artist
</span><span>    Suggestion: Review query and consider adding indexes on filtered/joined columns
</span><span>
</span><span>✓ genre-topn_genre-top-n.top-1.json (0.00s)
</span><span>✓ genre-topn_genre-top-n.top-3.json (0.00s)
</span><span>✓ genre-topn_genre-top-n.top-1.cost (6610.59 &lt;= 6610.59 * 110%) (0.00s)
</span><span>  ⚠️  Multiple sequential scans detected on tables: genre, artist
</span><span>    Suggestion: Review query and consider adding indexes on filtered/joined columns
</span><span>  ⚠️  Multiple sort operations detected (2 sorts)
</span><span>    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
</span><span>  ⚠️  Nested loop join with sequential scan detected
</span><span>    Suggestion: Add index on join column to avoid repeated sequential scans
</span><span>✓ genre-topn_genre-top-n.top-3.cost (6610.59 &lt;= 6610.59 * 110%) (0.00s)
</span><span>  ⚠️  Multiple sequential scans detected on tables: artist, genre
</span><span>    Suggestion: Review query and consider adding indexes on filtered/joined columns
</span><span>  ⚠️  Multiple sort operations detected (2 sorts)
</span><span>    Suggestion: Consider composite indexes for ORDER BY clauses to avoid sorting
</span><span>  ⚠️  Nested loop join with sequential scan detected
</span><span>    Suggestion: Add index on join column to avoid repeated sequential scans
</span><span>
</span><span>✓ genre-tracks_tracks-by-genre.json (0.00s)
</span><span>✓ genre-tracks_tracks-by-genre.cost (37.99 &lt;= 37.99 * 110%) (0.00s)
</span><span>  ⚠️  Multiple sequential scans detected on tables: genre, track
</span><span>    Suggestion: Review query and consider adding indexes on filtered/joined columns
</span><span>
</span><span>Results: 16 passed (0.00s)
</span></code></pre>
<p>As you can see, despite from not having baseline, RegreSQL is able to detect the basic bad patterns that should be addressed before queries can be considered &#34;production ready&#34;.</p>
<p>In some cases, having the detection of sequential scans, or just tracking query costs baselines might be considered undesirable, which would lead to false positives. RegreSQL enables this to be addressed by query metadata as demonstrated below.</p>
<pre data-lang="sql"><code data-lang="sql"><span>-- name: query_name
</span><span>-- metadata: key1=value1, key2=value2
</span><span>SELECT</span><span> ...;
</span></code></pre>
<p>At this point RegreSQL recognizes</p>
<ul>
<li><code>notest</code> to skip the query testing altogether (not just cost tracking)</li>
<li><code>nobaseline</code> to skip cost tracking</li>
<li><code>noseqscanwarn</code> to keep cost tracking but disable sequential scan warnings</li>
<li>and <code>difffloattolerance</code> to cost failure threshold (default 10% at the moment).</li>
</ul>
<pre data-lang="sql"><code data-lang="sql"><span>-- name: query_name
</span><span>-- regresql: notest, nobaseline
</span><span>-- regresql: noseqscanwarn
</span><span>-- regresql: difffloattolerance:0.25
</span><span>-- query that can vary in cost by 20% without being considered a failure
</span><span>SELECT</span><span> ...;
</span></code></pre>
<h2 id="orm-enters-the-room">ORM enters the room
</h2>
<p>ORMs abstract away SQL, but they still generate it - and that generated SQL can have performance problems you won&#39;t catch until production. Consider this common scenario: you start with a simple SQLAlchemy query that works fine, then months later add eager loading for related data:</p>
<pre data-lang="python"><code data-lang="python"><span>orders = (
</span><span>    session.</span><span>query</span><span>(Order)
</span><span>    .</span><span>filter</span><span>(Order.user_id == user_id)
</span><span>    .</span><span>options</span><span>(
</span><span>        </span><span>joinedload</span><span>(Order.user),
</span><span>        </span><span>joinedload</span><span>(Order.shipping_address),
</span><span>        </span><span>selectinload</span><span>(Order.items)  </span><span># NEW: Load order items
</span><span>    )
</span><span>    .</span><span>all</span><span>()
</span><span>)
</span></code></pre>
<p>That innocent <code>selectinload(Order.items)</code> generates a separate query - and without an index on <code>order_items.order_id</code>, it performs a sequential scan.</p>
<p>RegreSQL can catch this by intercepting ORM-generated SQL using SQLAlchemy&#39;s event system:</p>
<pre data-lang="python"><code data-lang="python"><span>@event.</span><span>listens_for</span><span>(engine, &#34;</span><span>before_cursor_execute</span><span>&#34;)
</span><span>def </span><span>capture_sql</span><span>(</span><span>conn</span><span>, </span><span>cursor</span><span>, </span><span>statement</span><span>, *</span><span>args</span><span>):
</span><span>    captured_queries.</span><span>append</span><span>(statement)
</span></code></pre>
<p>Run your ORM code, capture the SQL, save it as a .sql file, and test it with RegreSQL. The performance baseline testing will flag the missing index before it hits production. This is currently experimental, but ORM integration is a key area for RegreSQL&#39;s future development.</p>
<h2 id="test-data-management">Test Data Management
</h2>
<p>Up until now we have covered how RegreSQL verifies query correctness and tracks performance regressions. But there&#39;s a critical prerequisite we&#39;ve only skimmed through.  Every regression test needs consistent, reproducible data. Change the data, change their cardinality, and your expected results become meaningless. Your performance  baselines drift. Your tests become flaky.</p>
<p>Traditional approach to create test data might involve</p>
<ul>
<li><strong>Database dumps</strong> become unmanageable - 500MB files you can&#39;t review, can&#39;t understand, that break with every schema migration, and whose data becomes stale as production evolves. Which version of the dump are your tests even using?</li>
<li><strong>SQL scripts</strong> might be better than dumps, but still imperative and hard to maintain. You end up with INSERT statements scattered across multiple files, managing foreign keys manually, and debugging constraint violations.</li>
<li><strong>Factories in application code</strong> might work great for integration tests, but we&#39;re testing SQL directly. Do you really want to maintain parallel data generation in your application language just for SQL tests?</li>
<li><strong>Shared test database</strong> is the synonym for classic &#34;works on my machine&#34; problem. State leaks between tests. Parallel execution becomes impossible. Debugging is a nightmare.</li>
</ul>
<p>What we need is something that&#39;s declarative (what data, not how to insert it), reproducible (similar data every time), composable (build complex scenarios from simple pieces), and scalable (from 10 rows to 100,000).</p>
<p>This is where next improvement in RegreSQL&#39;s fixture system comes in. Think of it as infrastructure-as-code for your test data. You describe the data you need in YAML files, and RegreSQL handles the rest - dependencies, cleanup, foreign keys, and even realistic data generation at scale.</p>
<p>RegreSQL&#39;s fixture system lets you define test data in YAML files stored in <code>regresql/fixtures/</code>. Here&#39;s a simple example</p>
<pre data-lang="yaml"><code data-lang="yaml"><span>  </span><span>fixture</span><span>: </span><span>basic_users
</span><span>  </span><span>description</span><span>: </span><span>a handful of test users
</span><span>  </span><span>cleanup</span><span>: </span><span>rollback
</span><span>
</span><span>  </span><span>data</span><span>:
</span><span>    - </span><span>table</span><span>: </span><span>users
</span><span>      </span><span>rows</span><span>:
</span><span>        - </span><span>id</span><span>: </span><span>1
</span><span>          </span><span>email</span><span>: </span><span>alice@example.com
</span><span>          </span><span>name</span><span>: </span><span>Alice Anderson
</span><span>          </span><span>created_at</span><span>: </span><span>2024-01-15
</span><span>        - </span><span>id</span><span>: </span><span>2
</span><span>          </span><span>email</span><span>: </span><span>bob@example.com
</span><span>          </span><span>name</span><span>: </span><span>Bob Builder
</span><span>          </span><span>created_at</span><span>: </span><span>2024-02-20
</span></code></pre>
<p>To use this fixture in your tests, reference it in the query&#39;s plan file (<code>regresql/plans/get-user.yaml</code>) you can just reference the fixture</p>
<pre data-lang="yaml"><code data-lang="yaml"><span>  </span><span>fixtures</span><span>:
</span><span>    - </span><span>basic_users
</span><span>
</span><span>  &#34;</span><span>1</span><span>&#34;:
</span><span>    </span><span>email</span><span>: </span><span>alice@example.com
</span><span>
</span><span>  &#34;</span><span>2</span><span>&#34;:
</span><span>    </span><span>email</span><span>: </span><span>bob@example.com
</span></code></pre>
<p>And when you run <code>regresql test</code>, the fixture is automatically loaded before the query executes, and cleaned up afterward. No manual setup scripts, no state leakage between tests. But it does not stop with static fixtures. When you want to test queries against realistic volumes you can use range of <strong>data generators</strong> including</p>
<ul>
<li>sequences, random integer, decimal, string, uuid, email and name generators</li>
<li>date_between for generating random timestamps within a range</li>
<li>foreign key references to be able to reuse data from other table&#39;s fixtures</li>
<li>range to select value from predefined sources</li>
<li>Go template support</li>
</ul>
<pre data-lang="yaml"><code data-lang="yaml"><span> </span><span>fixture</span><span>: </span><span>realistic_orders
</span><span>  </span><span>generate</span><span>:
</span><span>    - </span><span>table</span><span>: </span><span>customers
</span><span>      </span><span>count</span><span>: </span><span>1000
</span><span>      </span><span>columns</span><span>:
</span><span>        </span><span>id</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>sequence
</span><span>          </span><span>start</span><span>: </span><span>1
</span><span>        </span><span>email</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>email
</span><span>          </span><span>domain</span><span>: </span><span>shop.example.com
</span><span>        </span><span>name</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>name
</span><span>          </span><span>type</span><span>: </span><span>full
</span><span>        </span><span>created_at</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>date_between
</span><span>          </span><span>start</span><span>: &#34;</span><span>2023-01-01</span><span>&#34;
</span><span>          </span><span>end</span><span>: &#34;</span><span>2024-12-31</span><span>&#34;
</span><span>
</span><span>    - </span><span>table</span><span>: </span><span>orders
</span><span>      </span><span>count</span><span>: </span><span>5000
</span><span>      </span><span>columns</span><span>:
</span><span>        </span><span>id</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>sequence
</span><span>          </span><span>start</span><span>: </span><span>1
</span><span>        </span><span>customer_id</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>int
</span><span>          </span><span>min</span><span>: </span><span>1
</span><span>          </span><span>max</span><span>: </span><span>1000
</span><span>        </span><span>amount</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>decimal
</span><span>          </span><span>min</span><span>: </span><span>10.00
</span><span>          </span><span>max</span><span>: </span><span>999.99
</span><span>          </span><span>precision</span><span>: </span><span>2
</span><span>        </span><span>order_date</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>date_between
</span><span>          </span><span>start</span><span>: &#34;</span><span>2023-01-01</span><span>&#34;
</span><span>          </span><span>end</span><span>: &#34;</span><span>2024-12-31</span><span>&#34;
</span></code></pre>
<p>This generates 1,000 customers and 5,000 orders with realistic-looking data - names, emails, dates, and amounts that feel production-like.</p>
<p>The fixtures are also <strong>stackable</strong> and can be build on top of each other. For example if you need to make sure users fixtures are created before orders fixtures, just declare the dependency (the already planned improvement is to include the support automatic foreign-key detection to avoid ID hard-coding). RegreSQL loads fixtures in dependency order and handles cleanup in reverse.</p>
<pre data-lang="yaml"><code data-lang="yaml"><span>  </span><span>fixture</span><span>: </span><span>orders_with_shipping
</span><span>  </span><span>depends_on</span><span>:
</span><span>    - </span><span>basic_users
</span><span>
</span><span>  </span><span>data</span><span>:
</span><span>    - </span><span>table</span><span>: </span><span>orders
</span><span>      </span><span>rows</span><span>:
</span><span>        - </span><span>id</span><span>: </span><span>101
</span><span>          </span><span>user_id</span><span>: </span><span>1  </span><span># References Alice from basic_users
</span><span>          </span><span>total</span><span>: </span><span>99.99
</span><span>          </span><span>status</span><span>: </span><span>shipped
</span></code></pre>
<p>Should the available options for fixtures (manual data or data generators) not be enough, you always have options to use good old SQL based data generation.</p>
<pre data-lang="yaml"><code data-lang="yaml"><span>  </span><span>fixture</span><span>: </span><span>mixed_setup
</span><span>  </span><span>description</span><span>: </span><span>Combine SQL with YAML and generated data
</span><span>  </span><span>cleanup</span><span>: </span><span>rollback
</span><span>
</span><span>  </span><span># SQL executes first (either as file or inline)
</span><span>  </span><span>sql</span><span>:
</span><span>    - </span><span>file</span><span>: </span><span>sql/setup_schema.sql
</span><span>    - </span><span>inline</span><span>: &#34;</span><span>INSERT INTO config (key, value) VALUES (&#39;version&#39;, &#39;1.0&#39;);</span><span>&#34;
</span><span>
</span><span>  </span><span># followed YAML data
</span><span>  </span><span>data</span><span>:
</span><span>    - </span><span>table</span><span>: </span><span>users
</span><span>      </span><span>rows</span><span>:
</span><span>        - </span><span>id</span><span>: </span><span>1
</span><span>          </span><span>email</span><span>: </span><span>admin@example.com
</span><span>
</span><span>  </span><span># and finally generated data
</span><span>  </span><span>generate</span><span>:
</span><span>    - </span><span>table</span><span>: </span><span>orders
</span><span>      </span><span>count</span><span>: </span><span>100
</span><span>      </span><span>columns</span><span>:
</span><span>        </span><span>id</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>sequence
</span><span>          </span><span>start</span><span>: </span><span>1
</span><span>        </span><span>user_id</span><span>:
</span><span>          </span><span>generator</span><span>: </span><span>int
</span><span>          </span><span>min</span><span>: </span><span>1
</span><span>          </span><span>max</span><span>: </span><span>1
</span></code></pre>
<p>RegreSQL provides commands to inspect and validate your fixtures</p>
<pre data-lang="bash"><code data-lang="bash"><span>  </span><span># List all available fixtures
</span><span>  </span><span>regresql</span><span> fixtures list
</span><span>
</span><span>  </span><span># Show fixture details and dependencies
</span><span>  </span><span>regresql</span><span> fixtures show realistic_orders
</span><span>
</span><span>  </span><span># Validate fixture definitions
</span><span>  </span><span>regresql</span><span> fixtures validate
</span><span>
</span><span>  </span><span># Show dependency graph
</span><span>  </span><span>regresql</span><span> fixtures deps
</span><span>
</span><span>  </span><span># Apply fixture manually (for debugging)
</span><span>  </span><span>regresql</span><span> fixtures apply basic_users
</span></code></pre>
<p>The fixture system has been design to transforms test data from a maintenance burden into a documented, version-controlled process. Your YAML files become the single source of truth for what data your tests need, making it easy to understand test scenarios and maintain test data as the application evolves.</p>
<h2 id="regresql-future">RegreSQL future
</h2>
<p>Introducing a new open source project is an ambitious goal, and RegreSQL is just starting up. Despite the fork being in works for almost 2 years. In coming weeks and months I plan further improvements, as well as better documentation and more tutorials. The project is maintained as part of my <strong>boringSQL</strong> brand, where it&#39;s vital component (together with pgTap) for building <a href="https://labs.boringsql.com">SQL Labs</a> which (as I sincerely hope) will provide a foundation for its further development.</p>
<p>At the same time <strong>RegreSQL</strong> is an attempt to give back to welcoming PostgreSQL community, make developer user experience slightly better if possible and (just maybe) provide one more argument against the case that SQL queries are not testable.</p>
<p>RegreSQL is available at <a href="https://github.com/boringSQL/regresql">GitHub</a> - feel free to open issue, or drop me email about the project at <a href="mailto:radim@boringsql.com">radim@boringsql.com</a> or connect on <a href="https://www.linkedin.com/in/1radim/">LinkedIn</a>.</p>

  </div></div>
  </body>
</html>
