<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mixtral-8x22b/">Original</a>
    <h1>Mixtral 8x22B</h1>
    
    <div id="readability-page-1" class="page"><div><p>Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.</p><p>Mixtral 8x22B comes with the following strengths:</p><ul><li>It is <strong>fluent in English, French, Italian, German, and Spanish</strong></li><li>It has strong maths and coding capabilities</li><li><strong>It is natively capable of function calling</strong>; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale</li><li>Its <strong>64K tokens context window</strong> allows precise information recall from large documents</li></ul><h3 id="truly-open">Truly open</h3><p>We believe in the power of openness and broad distribution to promote innovation and collaboration in AI.</p><p>We are, therefore, releasing Mixtral 8x22B under Apache 2.0, the most permissive open-source licence, allowing anyone to use the model anywhere without restrictions.</p><h3 id="efficiency-at-its-finest">Efficiency at its finest</h3><p>We build models that offer <strong>unmatched cost efficiency for their respective sizes</strong>, delivering the best performance-to-cost ratio within models provided by the community.</p><p>Mixtral 8x22B is a natural continuation of our open model family. Its sparse activation patterns make it faster than any dense 70B model, while being more capable than any other open-weight model (distributed under permissive or restrictive licenses). The base modelâ€™s availability makes it an excellent basis for fine-tuning use cases.</p><div><p><img src="https://mistral.ai/images/news/mixtral-8x22b/activ-param-perf.png" alt="MMLU of Mixtral 8x22b" width="100%"/></p><p>Figure 1: Measure of the performance (MMLU) versus inference budget tradeoff (number of active parameters). Mistral 7B, Mixtral 8x7B and Mixtral 8x22B all belong to a family of highly efficient models compared to the other open models.</p></div><h3 id="unmatched-open-performance">Unmatched open performance</h3><p>The following is a comparison of open models on standard industry benchmarks.</p><h4 id="reasoning-and-knowledge">Reasoning and knowledge</h4><p>Mixtral 8x22B is optimized for reasoning.</p><div><p><img src="https://mistral.ai/images/news/mixtral-8x22b/os-models-reasoning-bench.png" alt="MMLU of Mixtral 8x22b" width="100%"/></p><p>Figure 2: Performance on widespread common sense, reasoning and knowledge benchmarks of the top-leading LLM open models: MMLU (Measuring massive multitask language in understanding), HellaSwag (10-shot), Wino Grande (5-shot), Arc Challenge (5-shot), Arc Challenge (25-shot), TriviaQA (5-shot) and NaturalQS (5-shot).</p></div><h4 id="multilingual-capabilities">Multilingual capabilities</h4><p>Mixtral 8x22B has native multilingual capabilities. It strongly outperforms LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU benchmarks in French, German, Spanish and Italian.</p><div><p><img src="https://mistral.ai/images/news/mixtral-8x22b/os-models-multilingual-bench.png" alt="MMLU of Mixtral 8x22b" width="100%"/></p><p>Figure 3: Comparison of Mistral open source models and LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU in French, German, Spanish and Italian.</p></div><h4 id="maths--coding">Maths &amp; Coding</h4><p>Mixtral 8x22B performs best in coding and maths tasks compared to the other open models.</p><div><p><img src="https://mistral.ai/images/news/mixtral-8x22b/os-models-code-bench.png" alt="MMLU of Mixtral 8x22b" width="100%"/></p><p>Figure 4: Performance on popular coding and maths benchmarks of the leading open models: HumanEval pass@1, MBPP pass@1, GSM8K maj@1 (5 shot), GSM8K maj@8 (8-shot) and Math maj@4.</p></div><p>The instructed version of the Mixtral 8x22B released today shows even better math performance, with a score of 90.8% on GSM8K maj@8 and a Math maj@4 score of 44.6%.</p><p>Explore Mixtral 8x22B now on <a href="https://console.mistral.ai/">La Plateforme</a> and join the Mistral community of developers as we define the AI frontier together.</p></div></div>
  </body>
</html>
