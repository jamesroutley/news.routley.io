<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kzakka.com/robopianist/#demo">Original</a>
    <h1>RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning (2023)</h1>
    
    <div id="readability-page-1" class="page">
  

  <p>@ Conference on Robot Learning (CoRL) 2023</p>
  

  <!-- Authors -->
  
  

  <!-- Affiliations -->
  <div>
    <p><sup>1</sup>UC Berkeley</p>
    <p><sup>2</sup>Google DeepMind</p>
    <p><sup>3</sup>Stanford University</p>
    <p><sup>4</sup>Simon Fraser University</p>
  </div>
  

  <!-- Links -->
  

  
  

  <!-- tldr -->
  <p>
    <strong>TLDR</strong>
    <em>
      We train anthropomorphic robot hands to play the piano using deep RL</em>
  </p>
  

  <!-- wasm demo -->
  <section>
    <h2>Interactive Demo</h2>
    <p>
      This is a demo of our simulated piano playing agent trained with reinforcement learning. It runs
      MuJoCo natively in your browser thanks to <a href="https://github.com/zalo/mujoco_wasm/">WebAssembly</a>.
      You can use your mouse to interact with it, for example by dragging down the piano keys to generate sound
      or pushing the hands to perturb them. The controls section in the top right corner can be used to
      change songs and the simulation section to pause or reset the agent. Make sure you click the
      demo at least once to enable sound.
    </p>
    
  </section>

  <section>
    <h2>Overview</h2>
    <h3>Simulation</h3>
    <p>
      We build our simulated piano-playing environment using the open-source <a href="https://mujoco.org/">MuJoCo</a>
      physics engine.
      It consists in a full-size 88-key digital keyboard and two <a href="https://www.shadowrobot.com/">Shadow Dexterous
        Hands</a>,
      each with 24 degrees of freedom.
    </p>
    
    <img src="http://jimkang.com/weblog/articles/junior-calisthenics/assets/environment.png" id="robopianist-env"/>
    
    <h3>Musical representation</h3>
    <p>
      We use the Musical Instrument Digital Interface (<a href="https://en.wikipedia.org/wiki/MIDI">MIDI</a>) standard
      to represent
      a musical piece as a sequence of time-stamped messages corresponding to &#34;note-on&#34; or &#34;note-off&#34; events. A message
      carries
      additional pieces of information such as the pitch of a note and its velocity.
    </p>
    <p>
      We convert the MIDI file into a time-indexed note trajectory (also known as a <a href="https://en.wikipedia.org/wiki/Piano_roll">piano roll</a>),
      where each note is represented as a one-hot vector of length 88 (the number of keys on a piano). This trajectory
      is used as the goal
      representation for our agent, informing it which keys to press at each time step.
    </p>
    <p>
      The interactive plot below shows the song Twinkle Twinkle Little Star encoded as a piano roll. The x-axis
      represents
      time in seconds, and the y-axis represents musical pitch as a number between 1 and 88. You can hover over each
      note to
      see what additional information it carries.
    </p>
    
    <p>A <a href="https://en.wikipedia.org/wiki/Synthesizer">synthesizer</a> can be used to convert MIDI files to raw
      audio:</p>
    <audio controls="" src="assets/twinkle_twinkle.wav"></audio>
    <h3>Musical evaluation</h3>
    <p>
      We use precision, recall and F1 scores to evaluate the proficiency of our agent. If at a given instance of time
      there are keys that should be &#34;on&#34;
      and keys that should be &#34;off&#34;, precision measures how good the agent is at not hitting any of the keys that should
      be &#34;off&#34;, while recall measures
      how good the agent is at hitting all the keys that should be &#34;on&#34;. The F1 score combines the precision and recall
      into a single metric, and ranges
      from 0 (if either precision or recall is 0) to 1 (perfect precision and recall).
    </p>
    <h3>Piano fingering and dataset</h3>
    <p>
      Piano fingering refers to the assignment of fingers to notes in a piano piece (see figure below).
      Sheet music will typically provide sparse fingering labels for the tricky sections of a piece to
      help guide pianists, and pianists will often develop their own fingering preferences for a given
      piece.
    </p>
    <p>
      In RoboPianist, we found that the agent struggled to learn to play the piano with a sparse reward signal
      due to the exploration challenge associated with the high-dimensional action space. To overcome this issue,
      we added human priors in the form of the fingering labels to the reward function to guide its exploration.
    </p>
    <p>
      Since fingering labels aren&#39;t available in MIDI files by default, we used annotations from the Piano Fingering
      Dataset (<a href="https://beam.kisarazu.ac.jp/~saito/research/PianoFingeringDataset/">PIG</a>) to create 150
      labeled MIDI files, which we call Repertoire-150 and release as part of our environment.
    </p>
    <div>
      <p><img src="http://jimkang.com/weblog/articles/junior-calisthenics/assets/fingering.png"/></p><figcaption>Finger numbers (1 to 9) annotated above each note. Source: <a href="https://github.com/marcomusy/pianoplayer">PianoPlayer</a></figcaption>
    </div>
    <h3>MDP Formulation</h3>
    <p>
      We model piano-playing as a finite-horizon Markov Decision Process (MDP) defined by a tuple ,
      where  is the state space,  is the action space,  is
      the initial state distribution, 
      governs the dynamics,  is the reward function,  is the discount factor, and  is
      the horizon. The goal of the agent is to
      maximize its total expected discounted reward over the horizon .
    </p>
    <p>
      At every time step, the agent receives proprioceptive (i.e, hand joint angles), exteroceptive (i.e., piano
      key states) and goal observations (i.e., piano roll) and outputs 22 target joint angles for each hand. These
      are fed to proportional-position actuators which convert them to torques at each joint. The agent then
      receives a weighted sum of reward terms, including a reward for hitting the correct keys, a reward for
      minimizing energy consumption, and a shaping reward for adhering to the fingering labels.
    </p>
    <p>
      For our policy optimizer, we use a state-of-the-art model-free RL algorithm <a href="https://arxiv.org/abs/2110.02034">DroQ</a>
      and train our agent for 5 million steps with a control frequency of 20 Hz.
    </p>
  </section>

  <!-- quantitative results -->
  <section>
    <h2>Quantitative Results</h2>
    <p>
      With careful system design, we improve our agent&#39;s performance significantly. Specifically,
      adding an energy cost to the reward formulation, providing a few seconds worth of future goals
      rather than just the current goal, and constraining the action space helped the agent learn
      faster and achieve a higher F1 score. The plot below shows the additive effect of each of
      these design choices on three different songs of increasing difficulty.
    </p>
    <p><img src="http://jimkang.com/weblog/articles/junior-calisthenics/assets/design_considerations.png"/>
    </p>
    <p>
      When compared to a strong derivative-free model predictive control (MPC) baseline, <a href="https://arxiv.org/abs/2212.00541">Predictive Sampling</a>,
      our agent achieves a much higher F1 score, averaging 0.79 across Etude-12 versus 0.43 for Predictive Sampling.
    </p>
    <p><img src="http://jimkang.com/weblog/articles/junior-calisthenics/assets/etude_12.jpg"/>
    </p>
  </section>

  <!-- qualitative results -->
  <section>
    <h2>Qualitative Results</h2>
    <p>
      Each video below is playing real-time and shows our agent playing every song in the Etude-12 subset.
      In each video frame, we display the fingering labels by coloring the keys according to the
      corresponding finger color. When a key is pressed, it is colored green.
    </p>
    <h3>Debug dataset</h3>
    <p>
      This dataset contains &#34;entry-level&#34; songs (e.g., scales) and is useful for sanity checking an agent&#39;s performance.
      Fingering labels in this dataset were manually annotated by the authors of this paper. It is not part of the
      Repertoire-150 dataset.
    </p>
    <div>
      <p>
        <video controls="" src="assets/videos/simple/c_major.mp4"></video>
        <figcaption>C Major Scale</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/simple/d_major.mp4"></video>
        <figcaption>D Major Scale</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/simple/twinkle_twinkle.mp4"></video>
        <figcaption>Twinkle Twinkle Little Star</figcaption>
      </p>
    </div>
    <h3>Etude-12 subset</h3>
    <p>
      Etude-12 is a subset of the full 150-large dataset and consists of 12 songs of varying difficulty.
      It is a subset of the full benchmark reserved for more moderate compute budgets.
    </p>
    <div>
      <p>
        <video controls="" src="assets/videos/etude/Piano Sonata D845 1st Mov.mp4"></video>
        <figcaption>Piano Sonata D845 1st Mov (F1=0.72)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Partita No. 2 6th Mov.mp4"></video>
        <figcaption>Partita No. 2 6th Mov (F1=0.73)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Bagatelle Op. 3 No. 4.mp4"></video>
        <figcaption>Bagatelle Op. 3 No. 4 (F1=0.75)</figcaption>
      </p>
    </div>
    <div>
      <p>
        <video controls="" src="assets/videos/etude/French Suite No. 5 Sarabande.mp4"></video>
        <figcaption>French Suite No. 5 Sarabande (F1=0.89)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Waltz Op. 64 No. 1.mp4"></video>
        <figcaption>Waltz Op. 64 No. 1 (F1=0.78)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/French Suite No. 1 Allemande.mp4"></video>
        <figcaption>French Suite No. 1 Allemande (F1=0.78)</figcaption>
      </p>
    </div>
    <div>
      <p>
        <video controls="" src="assets/videos/etude/Piano Sonata No. 2 1st Mov.mp4"></video>
        <figcaption>Piano Sonata No. 2 1st Mov (F1=0.79)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Kreisleriana Op. 16 No. 8.mp4"></video>
        <figcaption>Kreisleriana Op. 16 No. 8 (F1=0.84)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Golliwoggs Cakewalk.mp4"></video>
        <figcaption>Golliwoggs Cakewalk (F1=0.85)</figcaption>
      </p>
    </div>
    <div>
      <p>
        <video controls="" src="assets/videos/etude/Piano Sonata No. 23 2nd Mov.mp4"></video>
        <figcaption>Piano Sonata No. 23 2nd Mov (F1=0.87)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/French Suite No. 5 Gavotte.mp4"></video>
        <figcaption>French Suite No. 5 Gavotte (F1=0.77)</figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/etude/Piano Sonata K279 1st Mov.mp4"></video>
        <figcaption>Piano Sonata K279 1st Mov (F1=0.78)</figcaption>
      </p>
    </div>
  </section>

  <!-- failures -->
  <section>
    <h2>Common failure modes</h2>
    <div>
      <p>
        <video controls="" src="assets/videos/failures/close_together.mp4"></video>
        <figcaption>
          Since the Shadow Hand forearms are thicker than a human&#39;s, the agent sometimes struggles to nail down notes
          that are really close together. Adding full rotational and translational degrees of freedom to the hands could
          give them the ability to overcome this limitation, but would pose additional challenges for learning.
        </figcaption>
      </p>
      <p>
        <video controls="" src="assets/videos/failures/large_stretches.mp4"></video>
        <figcaption>
          The agent struggles with songs that require stretching the fingers over many notes, sometimes more than 1
          octave.
        </figcaption>
      </p>
    </div>
  </section>

  <section>
    <h2>Acknowledgments</h2>
    <p>
      This work is supported in part by ONR #N00014-22-1-2121 under the Science of Autonomy program.
    </p>
    <p>
      This website was heavily inspired by <a href="https://brentyi.github.io/tilted/"> Brent Yi&#39;s</a>.
    </p>
  </section>




</div>
  </body>
</html>
