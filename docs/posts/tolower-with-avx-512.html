<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dotat.at/@/2024-07-28-tolower-avx512.html">Original</a>
    <h1>tolower() with AVX-512</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>A couple of years ago I wrote about <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">tolower() in bulk at speed using
SWAR tricks</a>. A couple of days ago I was interested by Olivier
Giniaux’s article about <a href="https://ogxd.github.io/articles/unsafe-read-beyond-of-death/">unsafe read beyond of death</a>, an
optimization for handling small strings with SIMD instructions, for a
fast hash function written in Rust.</p>
<p>I’ve long been annoyed that SIMD instructions can easily eat short
strings whole, but it’s irritatingly difficult to transfer short
strings between memory and vector registers. Oliver’s post caught my
eye because it seemed like a fun way to avoid the problem, at least
for loads. (Stores remain awkward!)</p>
<p>Actually, to be frank, Olivier nerdsniped me.</p>
<ul>
<li><a href="#signs-of-hope">signs of hope</a></li>
<li><a href="#tolower64-">tolower64()</a></li>
<li><a href="#bulk-load-and-store">bulk load and store</a></li>
<li><a href="#masked-load-and-store">masked load and store</a></li>
<li><a href="#benchmarking">benchmarking</a></li>
<li><a href="#conclusion">conclusion</a></li>
</ul>
<h2><a name="signs-of-hope" href="#signs-of-hope">signs of hope</a></h2>
<p>Reading more around the topic, I learned that <em>some</em> SIMD instruction
sets do, in fact, have useful masked loads and stores that are
suitable for string processing, that is, they have byte granularity.
They are:</p>
<ul>
<li>
<p>ARM SVE, which is available on recent big-ARM Neoverse cores, such
as Amazon Graviton, but not Apple Silicon.</p>
</li>
<li>
<p>AVX-512-BW, the bytes and words extension, which is available on
recent AMD Zen processors. AVX-512 is a complicated mess of
extensions that might or might not be available; support on Intel
is particularly random.</p>
</li>
</ul>
<p>I have an AMD Zen 4 box, so I thought I would try a little AVX-512-BW.</p>
<h2><a name="tolower64-" href="#tolower64-">tolower64()</a></h2>
<p>Using the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel intrinsics guide</a> I wrote a basic <code>tolower()</code>
function that can munch 64 bytes at once.</p>
<p>Top tip: You can use <code>*</code> as a wildcard in the search box, so I made
heavy use of <code>mm512*epi8</code> to find byte-wise AVX-512 functions (<code>epi8</code>
is an obscure alias for byte).</p>
<p>First, we fill a few registers with 64 copies of some handy bytes.</p>
<p>We need the letters A and Z:</p>
<pre><code>    __m512i A = _mm512_set1_epi8(&#39;A&#39;);
    __m512i Z = _mm512_set1_epi8(&#39;Z&#39;);
</code></pre>
<p>We need a number to add to uppercase letters to make them lowercase:</p>
<pre><code>    __m512i to_lower = _mm512_set1_epi8(&#39;a&#39; - &#39;A&#39;);
</code></pre>
<p>We compare our input characters <code>c</code> with A and Z. The result of each
comparison is a 64 bit mask which has bits set for the bytes where
the comparison is true:</p>
<pre><code>    __mmask64 ge_A = _mm512_cmpge_epi8_mask(c, A);
    __mmask64 le_Z = _mm512_cmple_epi8_mask(c, Z);
</code></pre>
<p>If it’s greater than or equal to A, <em>and</em> less than or equal to Z,
then it is upper case. (AVX mask registers have names beginning with
<code>k</code>.)</p>
<pre><code>    __mmask64 is_upper = _kand_mask64(ge_A, le_Z);
</code></pre>
<p>Finally, we do a masked add. We pass <code>c</code> twice: bytes from the first
<code>c</code> are copied to the result when <code>is_upper</code> is false, and when
<code>is_upper</code> is true the result is <code>c + to_lower</code>.</p>
<pre><code>    return  _mm512_mask_add_epi8(c, is_upper, c, to_lower);
</code></pre>
<h2><a name="bulk-load-and-store" href="#bulk-load-and-store">bulk load and store</a></h2>
<p>The <code>tolower64()</code> kernel in the previous section needs to be wrapped
up in more convenient functions such as copying a string while
converting it to lower case.</p>
<p>For long strings, the bulk of the work uses unaligned vector load and
store instructions:</p>
<pre><code>	__m512i src_vec = _mm512_loadu_epi8(src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_storeu_epi8(dst_ptr, dst_vec);
</code></pre>
<h2><a name="masked-load-and-store" href="#masked-load-and-store">masked load and store</a></h2>
<p>Small strings and the stub end of long strings use masked unaligned
loads and stores.</p>
<p><strong>This is the magic!</strong> Here is the reason I wrote this blog post!</p>
<p>The mask has its lowest <code>len</code> bits set (its first <code>len</code> bits in
little-endian order). I wrote these two lines with perhaps more
ceremony than required, but I thought it was helpful to indicate that
the mask is not any old 64 bit integer: it has to be loaded into one
of the SIMD unit’s mask registers.</p>
<pre><code>	uint64_t len_bits = (~0ULL) &gt;&gt; (64 - len);
	__mmask64 len_mask =  _cvtu64_mask64(len_bits);
</code></pre>
<p>The load and store look fairly similar to the full-width versions, but
with the mask stuff added. The <code>z</code> in <code>maskz</code> means zero the
destination register when the mask is clear, as opposed to copying
from another register (like in <code>mask_add</code> above).</p>
<pre><code>	__m512i src_vec = _mm512_maskz_loadu_epi8(len_mask, src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_mask_storeu_epi8(dst_ptr, len_mask, dst_vec);
</code></pre>
<p>That’s the essence of it: you can <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c">see the complete version of
<code>copytolower64()</code></a> in my git repository.</p>
<h2><a name="benchmarking" href="#benchmarking">benchmarking</a></h2>
<p>To see how well it works, I benchmarked several similar functions.
Here’s a chart of the results, compiled with Clang 16 on Debian 11,
and run on an AMD Ryzen 9 7950X.</p>
<p>The benchmark measures the time to copy about 1 MiByte, in chunks of
various lengths from 1 byte to 1 kilobyte. I wanted to take into
account differences in alignment in the source and destination
strings, so there are a few bytes between each source and destination
string, which are not counted as part of the megabyte.</p>
<p>On this CPU the L2 cache is 1 MiB per core, so I expect each run of
the test spills into the L3 cache.</p>
<p>To be sure I was measuring what I thought I was, I compiled each
function separately to avoid interference from inlining and code
motion. In real code it’s more likely that you would want to encourage
inlining, not prevent it!</p>
<p><img src="https://dotat.at/@/2024-07-vectolower.svg" alt="benchmark results"/></p>
<ul>
<li>
<p>The pink <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c"><code>tolower64</code></a> line is the code described in this blog post.
It is consistently near the fastest of all the functions under
test. (It drops a little at 65 bytes long, where it spills into a
second vector.)</p>
<p>The interesting feature of the line for my purposes is that it
rises fast and lacks deep troughs, showing that the masked loads
and stores were effective at handling small string fragments
quickly.</p>
</li>
<li>
<p>The green <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes64.c"><code>copybytes64</code></a> line is a version of <code>memcpy</code> using
AVX-512 in a similar manner to <code>tolower64</code>. It is (maybe
surprisingly) not much faster. I had to compile <code>copybytes64</code> with
Clang 11 because more recent versions are able to recognise what
the function does and rewrite it completely.</p>
</li>
<li>
<p>The orange <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes1.c"><code>copybytes1</code></a> line is a byte-by-byte version of <code>memcpy</code>
again compiled using Clang 11. It illustrates that Clang 11 had
relatively poor autovectorizer heuristics and was pretty bad for
string fragments less than 256 bytes long.</p>
</li>
<li>
<p>The very slow red <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower.c"><code>tolower</code></a> line calls the standard
<code>tolower()</code> from <code>&lt;ctype.h&gt;</code> to provide a baseline.</p>
</li>
<li>
<p>The purple <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower1.c"><code>tolower1</code></a> line is a simple byte-by-byte version of
<code>tolower()</code> compiled with Clang 16. It shows that Clang 16 has a
much better autovectorizer than Clang 11, but it is slower and
<a href="https://godbolt.org/z/9dPMo8h8a">much more complicated</a> than my
hand-written version. It is very spiky because the autovectorizer
did not handle short string fragments as well as <code>tolower64</code> does.</p>
</li>
<li>
<p>The brown <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower8.c"><code>tolower8</code></a> line is <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">the SWAR <code>tolower()</code> from my
previous blog post</a>. Clang valiantly tries to autovectorize
it, but the result is not great because the function is too
complicated. (It has the Clang-11-style 256-byte performance
cliffs despite being compiled with Clang 16.)</p>
</li>
<li>
<p>The blue <code>memcpy</code> line calls glibc’s <code>memcpy</code>. There’s something
curious going on here: it starts off fast but drops off to about
half the speed of <code>copybytes64</code>. Dunno why!</p>
</li>
</ul>
<h2><a name="conclusion" href="#conclusion">conclusion</a></h2>
<p>So, AVX-512-BW is <em>very nice indeed</em> for working with strings,
especially short strings. On Zen 4 it’s very fast, and the intrinsic
functions are reasonably easy to use.</p>
<p>The most notable thing is AVX-512-BW’s smooth performance: there’s
very little sign of the performance troughs that the autovectorizer
suffers from as it shifts to scalar code for small string fragments.</p>
<p>I don’t have convenient access to an ARM box with SVE support, so I
have not investigated it in detail. It’ll be interesting to see how
well SVE works for short strings.</p>
<p>I would like both of these instruction set extensions to be much more
widely available. They should improve the performance of string
handling tremendously.</p>
<p><a href="https://dotat.at/cgi/git/vectolower.git">The code for this blog post is available from my web site.</a></p>
<hr/>
<p>Thanks to <a href="https://news.ycombinator.com/item?id=41096934">LelouBil on Hacker News</a>
for pointing out a variable was named backwards. Ooops!</p>

</article></div>
  </body>
</html>
