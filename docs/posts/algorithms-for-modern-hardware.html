<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://en.algorithmica.org/hpc/">Original</a>
    <h1>Algorithms for Modern Hardware</h1>
    
    <div id="readability-page-1" class="page"><article>
<p>This is an upcoming high performance computing book titled “Algorithms for Modern Hardware” by <a href="http://sereja.me/">Sergey Slotin</a>.</p>
<p>Its intended audience is everyone from performance engineers and practical algorithm researchers to undergraduate computer science students who have just finished an advanced algorithms course and want to learn more practical ways to speed up a program than by going from $O(n \log n)$ to $O(n \log \log n)$.</p>
<p>All book materials are <a href="https://github.com/algorithmica-org/algorithmica">hosted on GitHub</a>, with code in a <a href="https://github.com/sslotin/scmm-code">separate repository</a>. This isn’t a collaborative project, but any contributions and feedback are very much welcome.</p>
<h3>FAQ</h3><p><strong>Bug/typo fixes.</strong> If you spot an error on any page, please do one of these — in the order of preference:</p>
<ul>
<li>fix it right away by either clicking on the pencil icon on the top right on any page (opens the <a href="https://prose.io/">Prose</a> editor) or, more traditionally, by modifying the page directly on GitHub (the link to the source is also on the top right);</li>
<li>create an issue on <a href="https://github.com/algorithmica-org/algorithmica">GitHub</a>;</li>
<li><a href="http://sereja.me/">tell me</a> about it directly;</li>
</ul>
<p>or leave a comment on some other website where it is being discussed — I read most of <a href="https://news.ycombinator.com/from?site=algorithmica.org">HackerNews</a>, <a href="https://codeforces.com/profile/sslotin">CodeForces</a>, and <a href="https://twitter.com/sergey_slotin">Twitter</a> threads where I’m tagged.</p>
<p><strong>Release date.</strong> The book is split into several parts that I plan to finish sequentially with long breaks in-between. Part I, Performance Engineering, is ~75% complete as of March 2022 and will hopefully be &gt;95% complete by summer.</p>
<p>“Release” for an open-source book like this means mostly freezing the table of contents, filling all the TODOs, doing one final round of heavy copyediting<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>, drawing illustrations, and then making a print-optimized pdf and figuring out the best way to distribute it. After that, I will mostly be fixing errors and only doing some minor edits reflecting the changes in technology or new algorithm advancements.</p>
<p>The e-book/printed editions will most likely be sold on a “pay what you want” basis, and in either case, the web version will always be available online in full.</p>
<p><strong>Pre-ordering / financially supporting the book.</strong> Due to my unfortunate citizenship and place of birth, you can’t — that is, until I find a way that at the same time complies with international sanctions, doesn’t sponsor <a href="https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine">the war</a>, and won’t put me in prison for tax evasion.</p>
<p>So, don’t bother. If you want to support this book, just share the articles you like on link aggregators and social media and help fix typos — that would be enough.</p>
<p><strong>Translations.</strong> The website has a separate functionality for creating and managing translations — and there are already volunteers willing to translate the book into Italian and Chinese (and I will personally translate at least some of it into my native Russian).</p>
<p>However, as the book is still evolving, it is probably not the best idea to start translation at least before the first part is complete — to not potentially waste the effort. That said, you are very much encouraged to make translations of any article and publish them in your blogs — just send me the link so that we can merge it back when a centralized translation process starts.</p>
<p><strong>“Translating” the Russian version.</strong> The articles hosted at <a href="https://ru.algorithmica.org/cs/">ru.algorithmica.org/cs/</a> are not about advanced performance engineering but mostly about classical computer science algorithms — without discussing how to speed them up beyond asymptotic complexity. Most of the information there is not unique and already exists in English on some other places on the internet: for example, the similar-spirited <a href="https://cp-algorithms.com/">cp-algorithms.com</a>.</p>
<p><strong>Teaching performance engineering in colleges.</strong> One of my goals for writing this book is to change the way computer science — algorithm design, to be more precise — is taught in colleges. Let me elaborate on that.</p>
<p>There are two highly impactful textbooks on which most computer science courses are built. Both are undoubtedly outstanding, but <a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">one of them</a> is 50 years old, and <a href="https://en.wikipedia.org/wiki/Introduction_to_Algorithms">the other</a> is 30 years old, and <a href="https://en.algorithmica.org/hpc/complexity/hardware">computers have changed a lot</a> since then. Asymptotic complexity is not the sole deciding factor anymore. In modern practical algorithm design, you choose the approach that makes better use of different types of parallelism available in the hardware over the one that theoretically does fewer raw operations on galaxy-scale inputs.</p>
<p>And yet, the computer science curricula in most colleges completely ignore this shift. Although there are some great courses that aim to correct that — such as “<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/">Performance Engineering of Software Systems</a>” from MIT, “<a href="https://ppc.cs.aalto.fi/">Programming Parallel Computers</a>” from Aalto University, and some non-academic ones like Denis Bakhvalov’s “<a href="https://github.com/dendibakh/perf-ninja">Performance Ninja</a>” — most computer science graduates still treat the hardware like something from the 90s.</p>
<p>What I ideally want to achieve is that performance engineering becomes taught right after introduction to algorithms. Writing the first comprehensive textbook on the subject is a large part of this — which is why I rush to finish it by summer so that the colleges can pick it up in the next academic year. But creating a new course requires more than that: you need a balanced curriculum, course infrastructure, lecture slides, lab assignments… So for some period after finishing the book, I will be working on materials and tools for teaching performance engineering — and I’m looking forward to working with other people who want to make it into reality as well.</p>
<h3>Part I: Performance Engineering</h3><p>The first part covers the basics of computer architecture and optimization of single-threaded algorithms.</p>
<p>It walks through the main CPU optimization topics such as caching, SIMD and pipelining, and provides brief examples in C++, followed by large case studies where we usually achieve a significant speedup over some STL algorithm or data structure.</p>
<p>Planned table of contents:</p>
<pre><code>0. Preface
1. Complexity Models
 1.1. Modern Hardware
 1.2. Programming Languages
 1.3. Models of Computation
 1.4. When to Optimize
2. Computer Architecture
 1.1. Instruction Set Architectures
 1.2. Assembly Language
 1.2. Loops and Conditionals
 1.3. Functions and Recursion
 1.4. Indirect Branching
 1.5. Interrupts and System Calls
 1.6. Machine Code Layout
3. Instruction-Level Parallelism
 3.1. Pipeline Hazards
 3.2. The Cost of Branching
 3.3. Branchless Programming
 3.4. Instruction Tables
 3.5. Instruction Scheduling
 3.6. Throughput Computing
 3.7. Theoretical Performance Limits
4. Compilation
 4.1. Stages of Compilation
 4.2. Flags and Targets
 4.3. Situational Optimizations
 4.4. Contract Programming
 4.5. Non-Zero-Cost Abstractions
 4.6. Compile-Time Computation
 4.7. Arithmetic Optimizations
 4.8. What Compilers Can and Can&#39;t Do
5. Profiling
 5.1. Instrumentation
 5.2. Statistical Profiling
 5.3. Program Simulation
 5.4. Machine Code Analyzers
 5.5. Benchmarking
 5.6. Getting Accurate Results
6. Arithmetic
 6.1. Floating-Point Numbers
 6.2. Interval Arithmetic
 6.3. Newton&#39;s Method
 6.4. Fast Inverse Square Root
 6.5. Integers
 6.6. Integer Division
 6.7. Bit Manipulation
(6.8. Data Compression)
7. Number Theory
 7.1. Modular Inverse
 7.2. Montgomery Multiplication
(7.3. Finite Fields)
(7.4. Error Correction)
 7.5. Cryptography
 7.6. Hashing
 7.7. Random Number Generation
8. External Memory
 8.1. Memory Hierarchy
 8.2. Virtual Memory
 8.3. External Memory Model
 8.4. External Sorting
 8.5. List Ranking
 8.6. Eviction Policies
 8.7. Cache-Oblivious Algorithms
 8.8. Spacial and Temporal Locality
(8.9. B-Trees)
(8.10. Sublinear Algorithms)
(9.13. Memory Management)
9. RAM &amp; CPU Caches
 9.1. Memory Bandwidth
 9.2. Memory Latency
 9.3. Cache Lines
 9.4. Memory Sharing
 9.5. Memory-Level Parallelism
 9.6. Prefetching
 9.7. Alignment and Packing
 9.8. Pointer Alternatives
 9.9. Cache Associativity
 9.10. Memory Paging
 9.11. AoS and SoA
10. SIMD Parallelism
 10.1. Intrinsics and Vector Types
 10.2. Loading and Writing Data
 10.3. Sums and Other Reductions
 10.4. Masking and Blending
 10.5. In-Register Shuffles
 10.6. Auto-Vectorization
11. Algorithm Case Studies
 11.1. Binary GCD
(11.2. Prime Number Sieves)
 11.3. Integer Factorization
 11.4. Logistic Regression
 11.5. Big Integers &amp; Karatsuba Algorithm
 11.6. Fast Fourier Transform
 11.7. Number-Theoretic Transform
 11.8. Argmin with SIMD
 11.9. Prefix Sum with SIMD
 11.10. Reading and Writing Integers
(11.11. Reading and Writing Floats)
(11.12. String Searching)
 11.13. Sorting
 11.14. Matrix Multiplication
12. Data Structure Case Studies
 12.1. Binary Search
 12.2. Static B-Trees
 12.3. Segment Trees
(12.4. Search Trees)
(12.5. Range Minimum Query)
 12.6. Hash Tables
(12.7. Bitmaps)
(12.8. Probabilistic Filters)
</code></pre><p>Among cool things that we will speed up:</p>
<ul>
<li>2x faster GCD (compared to <code>std::gcd</code>)</li>
<li>8-15x faster binary search (compared to <code>std::lower_bound</code>)</li>
<li>5-10x faster segment trees (compared to Fenwick trees)</li>
<li>5x faster hash tables (compared to <code>std::unordered_map</code>)</li>
<li>2x faster popcount (compared to repeatedly calling <code>popcnt</code>)</li>
<li>2x faster parsing series of integers (compared to <code>scanf</code>)</li>
<li>?x faster sorting (compared to <code>std::sort</code>)</li>
<li>2x faster sum (compared to <code>std::accumulate</code>)</li>
<li>2-3x faster prefix sum (compared to naive implementation)</li>
<li>10x faster argmin (compared to naive implementation)</li>
<li>10x faster array searching (compared to <code>std::find</code>)</li>
<li>100x faster matrix multiplication (compared to “for-for-for”)</li>
<li>optimal word-size integer factorization (~0.4ms per 60-bit integer)</li>
<li>optimal Karatsuba Algorithm</li>
<li>optimal FFT</li>
</ul>
<p>This work is largely based on blog posts, research papers, conference talks and other work authored by a lot of people:</p>
<ul>
<li><a href="https://agner.org/optimize/">Agner Fog</a></li>
<li><a href="https://lemire.me/en/#publications">Daniel Lemire</a></li>
<li><a href="https://erdani.com/index.php/about/">Andrei Alexandrescu</a></li>
<li><a href="https://twitter.com/chandlerc1024">Chandler Carruth</a></li>
<li><a href="http://0x80.pl/articles/index.html">Wojciech Muła</a></li>
<li><a href="https://probablydance.com/">Malte Skarupke</a></li>
<li><a href="https://travisdowns.github.io/">Travis Downs</a></li>
<li><a href="https://www.brendangregg.com/blog/index.html">Brendan Gregg</a></li>
<li><a href="http://embedded.cs.uni-saarland.de/abel.php">Andreas Abel</a></li>
<li><a href="https://cp-algorithms.com/">Jakob Kogler</a></li>
<li><a href="http://igoro.com/">Igor Ostrovsky</a></li>
<li><a href="https://hbfs.wordpress.com/">Steven Pigeon</a></li>
<li><a href="https://easyperf.net/notes/">Denis Bakhvalov</a></li>
<li><a href="https://pvk.ca/">Paul Khuong</a></li>
<li><a href="https://cglab.ca/~morin/">Pat Morin</a></li>
<li><a href="https://www.tacc.utexas.edu/about/directory/victor-eijkhout">Victor Eijkhout</a></li>
<li><a href="https://www.cs.utexas.edu/~rvdg/">Robert van de Geijn</a></li>
<li><a href="https://www.cc.gatech.edu/~echow/">Edmond Chow</a></li>
<li><a href="https://stackoverflow.com/users/224132/peter-cordes">Peter Cordes</a></li>
<li><a href="https://branchfree.org/">Geoff Langdale</a></li>
<li><a href="https://twitter.com/JuvHarlequinKFM">Matt Kulukundis</a></li>
<li><a href="https://gms.tf/">Georg Sauthoff</a></li>
<li><a href="https://mlochbaum.github.io/publications.html">Marshall Lochbaum</a></li>
<li><a href="https://ridiculousfish.com/blog/">ridiculous_fish</a></li>
<li><a href="https://www.youtube.com/c/WhatsACreel">Creel</a></li>
</ul>
<p>Volume: 450-600 pages</p>
<h3>Part II: Parallel Algorithms</h3><p>Concurrency, models of parallelism, green threads and concurrent runtimes, cache coherence, synchronization primitives, OpenMP, reductions, scans, list ranking and graph algorithms, lock-free data structures, heterogeneous computing, CUDA, kernels, warps, blocks, matrix multiplication and sorting.</p>
<p>Volume: 150-200 pages</p>
<h3>Part III: Distributed Computing</h3><p>Communication-constrained algorithms, message passing, actor model, partitioning, MapReduce, consistency and reliability at scale, storage, compression, scheduling and cloud computing, distributed deep learning.</p>
<p>Release date: ??? (more likely to be completed than not)</p>
<h3>Part IV: Compilers and Domain-Specific Architectures</h3><p>LLVM IR, compiler optimizations, JIT-compilation, Cython, JAX, Numba, Julia, OpenCL, DPC++ and oneAPI, XLA, Verilog, FPGAs, ASICs, TPUs and other AI accelerators.</p>
<p>Release date: ??? (less likely to be completed than not)</p>
<h3>Disclaimer: Technology Choices</h3><p>The examples in this book use C++, GCC, x86-64, CUDA and Spark, although the underlying principles we aim to convey are not specific to them.</p>
<p>To clear my conscience, I’m not happy with any of these choices: these technologies just happen to be the most widespread and stable at the moment, and thus more helpful for the reader. I would have respectively picked C / Rust, LLVM, arm, OpenCL and Dask; maybe there will be a 2nd edition in which some of the tech stack is changed.</p>

</article></div>
  </body>
</html>
