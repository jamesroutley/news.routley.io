<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">Original</a>
    <h1>Circuit Tracing: Revealing Computational Graphs in Language Models (Anthropic)</h1>
    
    <div id="readability-page-1" class="page"><div id="appendix">
<h3><a id="acknowledgments" href="#acknowledgments">Acknowledgments</a></h3>
<p>The case study on a model with hidden goals builds on a model organism developed by Sam Marks and Johannes Treutlein, with whom the authors also had helpful conversations. We would also like to acknowledge enabling work by Siddharth Mishra-Sharma training SAEs on the model used in the hidden goals case study.</p>
<p>We would like to thank the following people who reviewed an early version of the manuscript and provided helpful feedback that we used to improve the final version: Larry Abbott, Andy Arditi, Yonatan Belinkov, Yoshua Bengio, Devi Borg, Sam Bowman, Joe Carlsmith, Bilal Chughtai, Arthur Conmy, Jacob Coxon, Shaul Druckmann, Leo Gao, Liv Gorton, Helai Hesham, Sasha Hydrie, Nicholas Joseph, Harish Kamath, Tom McGrath, János Kramár, Aaron Levin, Ashok Litwin-Kumar, Rodrigo Luger, Alex Makolov, Sam Marks, Dan Mossing, Neel Nanda, Yaniv Nikankin, Senthooran Rajamanoharan, Fabien Roger, Rohin Shah, Lee Sharkey, Lewis Smith, Nick Sofroniew, Martin Wattenberg, and Jeff Wu.</p>
<p>We would also like to acknowledge Senthooran Rajamanoharan for helpful discussion on implementation of JumpReLU SAEs.</p>
<p>This paper was only possible due to the support of teams across Anthropic, to whom we&#39;re deeply indebted. The Pretraining and Finetuning teams trained Claude 3.5 Haiku and the 18-layer research model, which were the targets of our research. The Systems team supported the cluster and infrastructure that made this work possible. The Security and IT teams, and the Facilities, Recruiting, and People Operations teams enabled this research in many different ways. The Comms team (and especially Stuart Ritchie) supported public scientific communication of this work. </p>
<h3><a id="author-contributions" href="#author-contributions">Author Contributions</a></h3>
<p><span>Development of methodology:</span></p>
<ul><li>Chris Olah, Adly Templeton, and Jonathan Marcus developed ideas leading to general crosscoders, and the latter two implemented them in the Dictionary Learning codebase.</li><li>Jack Lindsey developed and first analyzed the performance of cross-layer transcoders.  </li><li>Tom Conerly, Jack Lindsey, Adly Templeton, Hoagy Cunningham, Basil Hosmer, and Adam Jermyn optimized the sparsity penalty and nonlinearity for CLTs. </li><li>Jack Lindsey and Michael Sklar ran scaling law experiments. </li><li>Jack Lindsey, Emmanuel Ameisen, Joshua Batson, and Chris Olah developed and refined the replacement model and attribution graph computation.</li><li>Jack Lindsey, Wes Gurnee, and Joshua Batson developed the graph pruning methodology, and Wes Gurnee systematically evaluated the approaches.</li><li>Emmanuel Ameisen, Joshua Batson, Brian Chen, Craig Citro, Wes Gurnee, Jack Lindsey, and Adam Pearce did initial exploration of example attribution graphs to validate and improve methodology. Wes Gurnee identified specific attention heads involved in certain prompts, and Adam Pearce analyzed feature splitting. Emmanuel Ameisen, Wes Gurnee, Jack Lindsey, and Adam Pearce identified specific examples to study.</li><li>Jack Lindsey, Emmanuel Ameisen, Wes Gurnee, Joshua Batson, and Chris Olah developed the methodology for the intervention analyses.</li><li>Wes Gurnee, Emmanuel Ameisen, Jack Lindsey, and Joshua Batson developed evaluation metrics for attribution graphs, and Wes Gurnee led their systematic implementation and analysis.</li><li>Michael Sklar and Jack Lindsey developed the approach for and executed perturbation experiments used to evaluate mechanistic faithfulness.</li><li>Nicholas L. Turner, Joshua Batson, Jack Lindsey, and Chris Olah developed the virtual weight and global weight approaches and analyses.</li><li>Brian Chen, Craig Citro, and Michael Sklar extended the method to handle neurons in addition to features.</li></ul>
<p><span>Infrastructure and Tooling:</span></p>
<ul><li>Tom Conerly, Adly Templeton, T. Ben Thompson, Basil Hosmer, David Abrahams, and Andrew Persic significantly improved the efficiency of dictionary learning and maintained the orchestration framework used for managing dictionary learning. </li><li>Adly Templeton organized efficiency work that enabled the largest runs on Claude 3.5 Haiku.</li><li>Adly Templeton significantly refactored the code to collect activations and train dictionaries, improving performance and usability.</li><li>Brian Chen designed and implemented scalability improvements for feature visualization with support from Tom Conerly.</li><li>Craig Citro, Emmanuel Ameisen, and Andy Jones improved and maintained the infrastructure for interacting with model internals.</li><li>Emmanuel Ameisen and Jack Lindsey developed the infrastructure for running the replacement model. Brian Chen implemented the layer norm and attention pattern freezing required for backpropagation in the local replacement model.</li><li>Emmanuel Ameisen developed a stable implementation of our graph generation pipeline for cross-layer transcoders.</li><li>Nicholas L. Turner led implementations of graph generation pipelines for alternative experimental crosscoder architectures with input from Craig Citro and Emmanuel Ameisen.</li><li>Nicholas L. Turner and Emmanuel Ameisen added the ability to visualize attributions to selected inactive features.</li><li>Wes Gurnee and Emmanuel Ameisen implemented efficiency improvements to graph generation.</li><li>Emmanuel Ameisen and Wes Gurnee added error nodes and embedding nodes to graph generation.</li><li>Wes Gurnee implemented adaptive, partial graph generation for large graphs.</li><li>Adam Pearce developed a method and interface for visualizing differences between pairs of graphs.</li><li>Tom Conerly and Jonathan Marcus improved the efficiency of loading feature weights which also sped up attribution graph generation. </li><li>Tom Conerly and Basil Hosmer made improvements to the integration of cross-layer transcoders with circuit attribution graph generation.</li><li>Brian Chen created the slack-based system for logging attribution graph runs.</li><li>Emmanuel Ameisen developed the infrastructure for patching experiments.</li><li>Adam Pearce, Jonathan Marcus, Zhenyi Qi, Thomas Henighan, and Emmanuel Ameisen identified open source datasets for visualization and generated feature visualization data for those datasets.</li><li>Shan Carter, Thomas Henighan, and Jonathan Marcus built an interactive tool for exploring feature activations.</li><li>Trenton Bricken, Thomas Henighan, and Jonathan Marcus provided infrastructure support and feedback for the hidden goals case study.</li><li>Trenton Bricken, Callum McDougall, and Brian Chen developed the autointerpretability framework used for initial exploration of attribution graphs.</li><li>Nicholas L. Turner designed and implemented the virtual weight pipeline to process the largest CLTs and handle processing requests from other members of the team. Joshua Batson, Tom Conerly, T. Ben Thompson, and Adly Templeton made suggestions on design decisions. Brian Chen and Tom Conerly made improvements to infrastructure that ended up supporting this effort.</li></ul>
<p><span>Interactive Graph Interface:</span></p>
<ul><li>The interactive attribution graph interface was built, and maintained by Adam Pearce, with assistance from Brian Chen and Shan Carter. Adam Pearce led the work to implement feature visualizations, subgraph display and editing, node pinning and most other elements of the interface.</li></ul>
<p><span>Case Studies:</span></p>
<ul><li>Wes Gurnee developed a systematic analysis of acronym completion, used for validating the original method and the NDAG example in the paper.</li><li>Emmanuel Ameisen investigated the Michael Jordan example.</li><li>Nicholas L. Turner, Adam Pearce, Joshua Batson, and Craig Citro investigated the arithmetic case study.</li></ul>
<p><span>Paper writing, infrastructure, and review:</span></p>
<ul><li>Figures</li></ul>
<ul><li>Chris Olah set the design language for the major figures.</li><li>Adam Pearce created the feature hovers which appear on paper figures.</li><li>Shan Carter created the explanatory figures, with assistance from Brian Chen.</li><li>Figure refinement and design consulting was provided by Shan Carter and Chris Olah.</li><li>The interactive interface for exploring addition feature global weights was made by Adam Pearce, Nicholas L. Turner, and Joshua Batson.</li></ul>
<ul><li>Writing and figures</li></ul>
<ul><li>Abstract - Emmanuel Ameisen</li><li>Summary of technical approach - Joshua Batson</li><li>Replacement model - Emmanuel Ameisen, Jack Lindsey</li><li>Attribution graphs - Emmanuel Ameisen, Brian Chen</li><li>Global weights - Nicholas L. Turner, Joshua Batson</li><li>Evaluations - Wes Gurnee, Jack Lindsey, Emmanuel Ameisen</li><li>Limitations - Jack Lindsey, Chris Olah. Adam Pearce made the feature splitting figure.</li><li>Discussion - Joshua Batson</li><li>Related work was drafted by Wes Gurnee, and Craig Citro significantly improved the completeness of the bibliography.</li></ul>
<ul><li>Detailed feedback on the paper and figures:</li></ul>
<ul><li>David Abrahams, Emmanuel Ameisen, Joshua Batson, Trenton Bricken, Brian Chen, Craig Citro, Tom Conerly, Wes Gurnee, Thomas Henighan, Adam Jermyn, Jack Lindsey, Jonathan Marcus, Chris Olah, Adam Pearce, Kelley Rivoire, Nicholas L. Turner, Sam Zimmerman.</li><li>Tom Conerly and Thomas Henighan led a detailed technical review. </li></ul>
<ul><li>Feedback from internal and external reviewers was managed by Nicholas L. Turner and Joshua Batson.</li><li>Paper publishing infrastructure was built and maintained by Adam Pearce and Craig Citro.</li></ul>
<p><span>Support and Leadership</span></p>
<ul><li>Sam Zimmerman managed the dictionary learning team and helped coordinate the team’s efforts scaling dictionary learning to enable cross-layer transcoders on Claude 3.5 Haiku.</li><li>Kelley Rivoire managed the interpretability team at large, provided support with project management for writing the papers, and helped with technical coordination across dictionary learning and attribution graph generation.</li><li>Tom Conerly provided research and engineering leadership for dictionary learning.</li><li>Chris Olah provided high-level research guidance.</li><li>Joshua Batson led the overall circuits project, supported technical coordination between teams, and provided research guidance throughout.</li></ul>
<h3><a id="citation-info" href="#citation-info">Citation Information</a></h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre>Ameisen, et al., &#34;Circuit Tracing: Revealing Computational Graphs in Language Models&#34;, Transformer Circuits, 2025.</pre>
<p>BibTeX citation</p>
<pre>@article{ameisen2025circuit,</pre>
<h3><a id="appendix-ml-details" href="#appendix-ml-details">CLT Implementation Details</a></h3>
<h4><a id="appendix-ml-details-plausible" href="#appendix-ml-details-plausible">Estimated Compute Requirements for CLT Training</a></h4>
<p>To give a rough sense of compute requirements to train CLTs, we share some estimated costs for CLTs on the Gemma 2 series of models <d-cite key="team2024gemma,lieberum2024gemmascope"></d-cite>. On the 2B parameter model, a run with 2M features and 1B train tokens would require 3.8e20 training flops (roughly 210 H100 hours at 50% flops efficiency). On the 9B parameter model a run with 5M features and 3B train tokens would require 6.9e21 training flops (roughly 3,844 H100 hours at 50% flops efficiency).</p>
<h4><a id="appendix-ml-details-ml" href="#appendix-ml-details-ml">ML details</a></h4>
<p>We train our cross-layer transcoder using a combination of mean-squared error reconstruction loss on MLP outputs and a Tanh sparsity penalty. Our features use the JumpReLU nonlinearity <d-cite key="rajamanoharan2024jumping"></d-cite>. We found the combination of JumpReLU + Tanh to modestly outperform alternative methods on MSE/L0, but we suspect that alternative choices of nonlinearity and sparsity penalty could perform comparably well. For more details on our training setup see <d-cite key="conerly2025optimization"></d-cite>. Here we highlight methodological details that differ from our prior work on SAEs:</p>
<ul><li>Following <d-cite key="lindsey2024crosscoders"></d-cite>, we separately normalize the activation of each residual stream layer and each MLP output prior to using them as inputs / targets for the cross-layer transcoder. </li><li>For our nonlinearity, we use the JumpReLU activation function with a straight-through gradient estimator <d-cite key="rajamanoharan2024jumping"></d-cite>. However, we deviate from Rajamanoharan <span>et al.</span> in several respects. First, we use a much higher bandwidth parameter (1.0) for the straight through estimator and a much higher initial JumpReLU threshold (0.03). Second, we allow the gradient to flow through the JumpReLU nonlinearity to all model parameters, rather than only the threshold parameter. We found these decisions improved the MSE/L0 frontier in our setup.</li><li>We schedule our sparsity penalty to linearly ramp up from 0 to its final value throughout the entire duration of training. In previous work, the ramp-up phase took place over a small initial fraction of training steps.</li><li>We introduce a new “pre-activation loss” of <d-math>\sum_f \text{ReLU}(-h_f)</d-math> where h_f is the pre-nonlinearity activation of feature f. We found this loss helps prevent dead features on large CLT runs. We only used this loss for our experiments on Haiku 3.5, with a coefficient of 3×10⁻⁶.</li></ul>
<p>We chose our number of training steps to scale slightly sublinearly with the number of features, and our learning rate to scale approximately as one over the square root of the number of FLOPs. These are rough best-guess estimates based on prior experiments, and not precise scaling laws. In our largest 18L run, we used ~3B training tokens. In our largest Haiku run, we used ~16B training tokens.</p>
<p>In 18L, we used a constant sparsity penalty across CLT sizes. In Haiku, we increased the penalty with the CLT size (we found that otherwise, the L0 of the runs increased with size). In general we targeted L0 values in the low hundreds, based on preliminary investigation of what produced interpretable results in our graph interface.</p>
<p>Our 18L model is a pretraining-only model, and thus we used only pretraining data when training CLTs for it. For Haiku, by contrast, we trained the CLT using a mix of pretraining and finetuning data.</p>
<p>Encoder parameters are initialized by sampling from <d-math>U(\frac{-1}{\sqrt{\text{n\_features}}}, \frac{1}{\sqrt{\text{n\_features}}})</d-math> and decoder parameters are initialized by sampling from <d-math>U(\frac{-1}{\sqrt{\text{n\_layers}\,\cdot\,\text{d\_model}}}, \frac{1}{\sqrt{\text{n\_layers}\,\cdot\,\text{d\_model}}})</d-math>.</p>
<p>We shuffle our training data at the token level. We’ve found that not shuffling leads to significantly worse performance. We suspect a variety of partial shuffles, such as the one used in <d-cite key="lieberum2024gemmascope"></d-cite>, could perform just as well as a full shuffle.</p>
<h4><a id="appendix-ml-details-eng" href="#appendix-ml-details-eng">Engineering &amp; Infrastructure Considerations</a></h4>
<p>Here we share some engineering considerations relevant to training CLTs at scale – in particular, how it differs from training standard transcoders (“per-layer transcoders”, PLTs).</p>
<p>In our training implementation, the key difference between PLTs and CLTs is that, <span>per </span><span>accelerator</span><span> (GPU, TPU, etc)</span>, they use the same number of FLOPS, but the CLT uses n_layers times more network bandwidth. Thus when training CLTs more time should be spent profiling and optimizing network operations. This isn’t obvious, so we work through the details below.</p>
<p>In our training setup, features are sharded across accelerators. For each batch:</p>
<ol><li>Each accelerator receives input and target activations.</li><li>Each accelerator computes partial predictions based on the features it stores.</li><li>An all-reduce operation is performed across all accelerators to combine these partial predictions.</li><li>Each accelerator then runs a backward pass and performs a gradient step using the combined predictions. A constant fraction of each accelerator&#39;s high bandwidth memory (HBM) is dedicated to parameters, with the remainder used for intermediate computations.</li></ol>
<p>PLTs and CLTs have the same number of parameters per accelerator because the same fraction of HBM is dedicated to parameters. The number of FLOPS scales with batch size multiplied by the number of parameters so PLTs and CLTs use the same FLOPS <span>per accelerator</span>. If a PLT and a CLT have the same number of features then the CLT would have more total parameters, use more accelerators, and use more total FLOPS, but FLOPS per accelerator would be the same.</p>

<p>Some optimizations to consider:</p>
<ul><li>Have each accelerator load a different training batch. When it’s time to train on a batch, the accelerator that loaded it broadcasts it to all other accelerators using fast accelerator-to-accelerator comms.</li><li>Broadcast the batch for the next training step during the backwards pass of the previous training step.</li><li>Fetch future training batches in the background while training.</li><li>Use lower precision (e.g. bfloat16) to reduce the amount of data sent.</li></ul>
<p>We store activations to a distributed file system, then load them during training. Both CLTs and PLTs on every layer require activations from every layer of the underlying model. Thus we’ve optimized our code that collects activations for that use case. Collecting activations from all layers requires the same FLOPs as collecting from a single layer, but n_layers times more network bandwidth.</p>
<p>A possible optimization we didn’t implement is sparse kernels from <d-cite key="gao2024scaling"></d-cite>. The decoder forward, decoder backward, and encoder backward passes only operate on active features, so with the proper sparse kernels, FLOPS would be significantly reduced. CLTs have the same number of encoder parameters and FLOPS as PLTs, but n_layers/2 times more decoder parameters and FLOPS. Thus this optimization is more important for CLTs. This optimization makes it more likely that network operations will be the training bottleneck. Note that JumpReLU’s straight-through estimator has nonzero gradients on inactive features. We think it’s plausible that that isn’t important or we could switch to a different nonlinearity, but we haven’t tried it.</p>
<p>Another possible optimization we didn’t implement is to change how the decoder is sharded to remove the all-reduce. We could shard the encoder over features and shard the decoder over the output dimension. The all-reduce is removed because each decoder shard computes the MSE over a slice of the output dimension. Each decoder shard needs access to all active features, so an all-to-all is needed to share all active features with all shards. Another all-to-all is required on the backward pass. Given feature sparsity, these network operations are much smaller than the all-reduce.</p>
<h4><a id="appendix-ml-details-efficiency" href="#appendix-ml-details-efficiency">Efficiency relative to alternatives</a></h4>
<p>Compared to alternatives like per-layer transcoders (PLTs), CLTs use more parameters – the same number of encoder parameters, but approximately n_layers/2 times as many decoder parameters.  Thus, even if CLTs provide value beyond PLTs with the same number of <span>features</span>, it is reasonable to ask whether they perform better at a fixed training <span>cost</span>. Empirically, we find that CLTs perform better (according to some metrics) or comparably (according to others) than PLTs at a fixed cost, even without the use of sparse kernels (which, as noted above, advantage CLTs more than PLTs). Broadly, this is because the number of <span>total features</span> (across all layers) required to achieve a given level of performance is much less for CLTs than SLTs, which compensates for their additional per-feature cost. In more detail:</p>
<ul><li>Note that in our experiments on the 18L model, we ran CLTs and sets-of-PLTs with 300K, 1M, 3M, and 10M total features. Note that we also scaled training steps along with n_features. Coincidentally, this meant that the number of FLOPs used by our PLT runs were comparable to the number of FLOPS used by the “next size smaller” CLT run (i.e. 10M PLT ≈ 3M CLT, etc.).  </li><li>Comparing approximately FLOPs-matched PLT and CLT runs on our various <a href="#evaluating">evaluations</a>, we notice that:</li></ul>
<ul><li>In terms of (MSE, L0) and (replacement model KL divergence, L0), the 1M feature CLT strictly outperforms the 10M feature set-of-PLTs, using 5–10× less compute.</li><li>What about feature interpretability scores?</li></ul>
<ul><li>If we compare the 3M feature CLT to the 10M set-of-PLTs (roughly FLOPs equivalent), the CLT attains somewhat lower MSE and KL divergence and roughly equal Sort Eval scores.</li><li>The picture for the Contrastive Eval is less clear.  The 3M feature CLT attains lower MSE and KL divergence than the 10M feature PLTs, but higher worse Contrastive Eval scores. Thus we cannot declare a Pareto-winner here (we would need to run a sweep over the sparsity penalty to make the judgment).</li></ul>
<ul><li>We have found that cross-layer dictionaries require a comparable order of magnitude of training tokens to single-layer dictionaries (a rough estimate based on our 18L experiments is that they benefit from ~2 times more tokens).</li></ul>
<p>Thus, it seems that overall, CLTs are at least as cost-effective as PLTs (and presumably per-layer SAEs, which have the same parameter-count as PLTs) to reach a given level of circuit understanding, and likely moreso. However, if another group finds PLTs to be easier to implement or more cost-effective, we expect it’s possible to find interesting results using PLTs as well. We expect that even making attribution graphs with raw neurons will yield plenty of interesting insights.</p>
<p>We also note that optimizations could make CLTs even more cost-effective – as discussed <a href="#appendix-ml-details-eng">above</a>, implementing sparse kernels and communications could in principle reduce the number of FLOPs performed by CLTs by a factor of up to n_layers/2.</p>
<p>We suspect there are potential algorithmic optimizations to be made as well. For instance, it is possible that most of the benefits of CLTs can be captured by learning layer-to-layer linear transformations that are common to <span>all</span> features, eliminating the need for each feature to have its own independent decoder parameters for every downstream model layer.</p>
<h3><a id="appendix-attribution-graph-computation" href="#appendix-attribution-graph-computation">Attribution Graph Computation</a></h3>
<p>We give complete definitions of nodes and edges in the attribution graph, continuing the discussion in the <a href="#graphs-constructing">main text</a>. Associated to each node type are two (sets of) vectors: input vectors, which affect the edges for which the node is a target, and output vectors, which affect the edges for which the node is a source.</p>
<ul><li>The output node corresponding to a vocabulary token <d-math>\text{tok}</d-math> has candidate output tokens has input vector <d-math>v_{in} = \nabla (\mathbf{logit}_\text{tok} - \overline{\mathbf{logit}})</d-math>, the gradient of the pre-softmax logit for the target token minus the mean logit.</li><li>A feature node corresponding to feature <d-math>s</d-math> at context position <d-math>c_s</d-math> has a set of output vectors  <d-math>v_{out}^\ell = W_{\text{dec}, \;s}^{\ell_s \to \ell}</d-math>, for <d-math>\ell_s \leq \ell</d-math>. It has input vector <d-math>v_{in}^\ell = W_{\text{enc}, \;s}^{\ell_s}</d-math>.</li><li>An embedding node corresponding to input token <d-math>\text{tok}</d-math> has output vector <d-math>v_{out} = \text{Emb}_{\text{tok}}</d-math>.</li><li>An error node at context position <d-math>c</d-math> and layer <d-math>\ell</d-math> has output vector <d-math>v_{out} = \text{MLP}_\ell (x_{c,\ell}) - \text{CLT}_\ell(x_c)</d-math> where <d-math>x</d-math> represents the full residual stream vector at all context positions and layers from a forward pass of the underlying model, <d-math>\text{MLP}_{c, \ell}</d-math> is the output of the layer <d-math>\ell</d-math> MLP, and <d-math>\text{CLT}_\ell</d-math> is the output of the CLT to layer <d-math>\ell</d-math>.</li></ul>
<p>For edges from embedding or error nodes to feature or output nodes, the edge weight is:</p>
<p><d-math block="">A_{s \rightarrow t} = v_{out, s}^T J_{c_s, \ell_s \rightarrow c_t, \ell_t}^{\blacktriangledown} v_{in, t}.</d-math></p>
<p>For edges from feature nodes to feature or output nodes, the edge weight is </p>
<p><d-math block="" sub="">A_{s \rightarrow t} = a_s \sum_{\ell_s \leq \ell &lt; \ell_t} (v_{out}^\ell)^T J_{c_s, \ell \rightarrow c_t, \ell_t}^{\blacktriangledown} v_{in, t}.</d-math></p>
<p>The Jacobian <d-math>J_{c_s,  \ell_s \rightarrow c_t, \ell_t}^{\blacktriangledown}</d-math> can be expanded into a sum over all paths of length <d-math>\ell_t - \ell_s</d-math> in the underlying model through attention heads and residual connections. Consider a path <d-math>p</d-math> starting at position <d-math>c_s</d-math> and layer <d-math>\ell_s</d-math> and ending at position <d-math>c_t</d-math> and layer <d-math>\ell_t</d-math> consisting of <d-math>i = 1, 2, \ldots, (\ell_t - \ell_s)</d-math> steps, each of which is either a residual stream step going from layer <d-math>\ell + i - 1</d-math> to <d-math>\ell + i</d-math> or an attention head step going from position <d-math>c_i</d-math> to <d-math>c_{i+1}</d-math> via attention head <d-math>h_i</d-math> with attention weight <d-math>a^{h_i}_{c_i \rightarrow c_{i+1}}</d-math> and applying transformation <d-math>OV_{h_i}</d-math>. Each step is associated to a linear transformation <d-math>\pi_i</d-math> which is the identity for a residual stream step and <d-math>a^{h_i}_{c_i \rightarrow c_{i+1}} OV_{h_i}</d-math> for an attention step. The whole path <d-math>p</d-math> is thus associated to a linear transformation <d-math>\pi_p = \prod_i \pi_i</d-math>. Then we can write</p>
<p><d-math block="" sub=""> J_{c_s, \ell_s \rightarrow c_t, \ell_t}^{\blacktriangledown} = \sum_{p \in \mathcal{P}(c_s, c_t, \ell_s, \ell_t)} \pi_p</d-math></p>
<p>where <d-math>\mathcal{P}(c_s, c_t, \ell_s, \ell_t)</d-math> is the set of all paths starting at position <d-math>c_s</d-math> and layer <d-math>\ell_s</d-math> and ending at position <d-math>c_t</d-math> and layer <d-math>\ell_t</d-math>.</p>
<p>To compute the graph edges in practice, we iterate over target nodes in the graph (output or feature nodes). For each node we:</p>
<ul><li>Inject the input vector for the node into the residual stream. For CLT features, we inject the encoder of the target CLT feature in the residual stream at the layer and token position where it reads. For logit nodes, we instead inject the gradient of the logit (minus the mean logit) at the final layer residual stream.</li><li>Do a backwards pass in the underlying model with</li></ul>
<ul><li>stop-gradient operators on the MLP outputs, and</li><li>normalization denominators and attention patterns set to values recorded from the underlying model&#39;s forward pass on the prompt.</li></ul>
<ul><li>For a CLT source node (feature, token position), we take the sum of the dot products of its decoder vector in each layer with the gradient in that layer, times the activation of that feature.</li><li>For an embedding or error source node, we simply take the product of its vector with the gradient.</li></ul>
<p>The cost to compute the graph is linear in the number of active features in the prompt, and is dominated by the cost of the underlying model backwards pass. To economize, we sometimes compute the graph adaptively: starting from the output nodes for the logits, then maintaining a queue of the feature nodes with the greatest influence on the logit, and computing the input edges for nodes based on their order in the queue. This allows us to compute the most important parts of the graph first.</p>
<h3><a id="appendix-graph-pruning" href="#appendix-graph-pruning">Graph Pruning</a></h3>
<p>To increase the signal to noise ratio of our manual interpretation, we rely heavily on a graph pruning step to reduce the number of nodes and edges in the graph. We employ a two-step algorithm which first prunes the nodes and then prunes the edges of the remaining nodes. The details are as follows:</p>
<ul><li>We take the adjacency matrix of the graph and replace all the edge weights with their absolute values to obtain an unsigned adjacency matrix. Then we normalize the input edges to each node so that they sum to 1. Let A refer to this normalized, unsigned adjacency matrix.</li><li>We compute the <span>indirect influence matrix </span><d-math>B = A + A^2 + A^3 + \cdots</d-math> which can be efficiently computed as <d-math>B = (I - A)^{-1} - I</d-math>. The entries of <d-math>B</d-math> indicate the sum of the strengths of all paths between a given pair of nodes, where the strength of any given path is given by the product of the values of its constituent edges in A.</li><li>We take the rows of <d-math>B</d-math> corresponding to logit nodes, and take their weighted average, weighting according to the model’s output probabilities for each token. This results in a vector of <span>logit influence scores</span> for each node in the graph.</li><li>We sort the logit influence scores of non-logit nodes in descending order and choose the minimum cutoff index such that the sum of the influence scores prior to the cutoff divided by the total sum exceeds a given threshold. We use a threshold of 0.8 unless otherwise noted. We prune away all non-logit nodes after the cutoff.</li><li>With the pruned graph, we re-compute a normalized unsigned adjacency matrix and logit influence scores for each node using the same method as above. Then we assign a logit influence score to each <span>edge</span> by multiplying the logit influence score of the edge’s output node by the normalized edge weight. We prune edges according to the same strategy as for nodes, but using a higher cutoff 0.98 unless otherwise noted.</li><li>Logit nodes are pruned separately. We keep the logit nodes corresponding to the top <d-math>K</d-math> most likely token outputs, where <d-math>K</d-math> is chosen so that the total probability of the logit nodes is greater than 0.95. If this would result in <d-math>K &gt; 10</d-math>, we clamp <d-math>K</d-math> to <d-math>10</d-math>.</li><li>Embedding nodes and error nodes are not pruned.</li></ul>
<p>We include pseudocode below. Our pruning thresholds can, very roughly speaking, be interpreted as determining the percent of “total logit influence” we lose from pruning. That is, we choose the subset of nodes that are responsible for ~80% of the influence on the logits, and the subset of their input edges responsible for ~98% of the remaining influence. Our choice of thresholds is arbitrary and was chosen manually to balance preservation of important paths with the need to prune graphs to a manageable, interpretable size.</p>
<p>For longer prompts, we can also employ an adaptive algorithm, to greedily construct a graph out of the most influential nodes, rather than generating the full graph only to prune most of it away. To do so, we maintain a set of <span>explored</span> nodes (i.e., nodes from which we have computed backward attributions) and an estimate of the most influential nodes, where <span>unexplored</span> nodes count as errors. At each step, we compute the top <d-math>k</d-math> most influential nodes using our influence scores, and attribute back from these. We terminate when a target number of nodes have been explored.</p>
<p>With our default parameters, pruned graphs are substantially smaller than the original raw graphs; the number of nodes typically decreases by ~10× and the number of edges typically decreases by ~500× (the exact numbers are sensitive to prompt length).</p>

<p>Pseudocode for pruning algorithm:</p>
<code>function compute_normalized_adjacency_matrix(graph):
<p>    # Convert graph to adjacency matrix A</p>
<p>    # A[j, i] = weight from i to j (note the transposition)</p>
<p>    A = convert_graph_to_adjacency_matrix(graph)</p>
<p>    A = absolute_value(A)<br/></p>
<p>    # Normalize each row to sum to 1</p>
<p>    row_sums = sum(A, axis=1)</p>
<p>    row_sums = maximum(row_sums, 1e-8)  # Avoid division by zero</p>
<p>    A = diagonal_matrix(1/row_sums) @ A<br/></p>
<p>    return A<br/></p>
<p>function prune_nodes_by_indirect_influence(graph, threshold):</p>
<p>    A = compute_normalized_adjacency_matrix(graph)<br/></p>
<p>    # Calculate the indirect influence matrix: B = (I - A)^-1 - I</p>
<p>    # This is a more efficient way to compute A + A^2 + A^3 …</p>
<p>    B = inverse(identity_matrix(size=A.shape[0]) - A) - identity_matrix(size=A.shape[0])<br/></p>
<p>    # Get weights for logit nodes.</p>
<p>    # This is 0 if a node is a non-logit node and equal to the probability for logit nodes</p>
<p>    logit_weights = get_logit_weights(graph)<br/></p>
<p>    # Calculate influence on logit nodes for each node</p>
<p>    influence_on_logits = matrix_multiply(B, logit_weights)<br/></p>
<p>    # Sort nodes by influence</p>
<p>    sorted_node_indices = argsort(influence_on_logits, descending=True)<br/></p>
<p>    # Calculate cumulative influence</p>
<p>    cumulative_influence = cumulative_sum(</p>
<p>        influence_on_logits[sorted_node_indices]) / sum(influence_on_logits)<br/></p>
<p>    # Keep nodes with cumulative influence up to threshold</p>
<p>    nodes_to_keep = cumulative_influence &lt;= threshold    </p>
<p>    # Create new graph with only kept nodes and their edges</p>
<p>    return create_subgraph(graph, nodes_to_keep)<br/></p>
<p># Edge pruning by thresholded influence</p>
<p>function prune_edges_by_thresholded_influence(graph, threshold):</p>
<p>    # Get normalized adjacency matrix</p>
<p>    A = compute_normalized_adjacency_matrix(graph)<br/></p>
<p>    # Calculate influence matrix (as before)</p>
<p>    B = estimate_indirect_influence(A)<br/></p>
<p>    # Get logit node weights (as before)</p>
<p>    logit_weights = get_logit_weights(graph)<br/></p>
<p>    # Calculate node scores (influence on logits)</p>
<p>    node_score = matrix_multiply(B, logit_weights)<br/></p>
<p>    # Edge score is weighted by the logit influence of the target node</p>
<p>    edge_score = A * node_score[:, None]<br/></p>
<p>    # Calculate edges to keep based on thresholded cumulative score</p>
<p>    sorted_edges = sort(edge_score.flatten(), descending=True)</p>
<p>    cumulative_score = cumulative_sum(sorted_edges) / sum(sorted_edges)</p>
<p>    threshold_index = index_where(cumulative_score &gt;= threshold)</p>
<p>    edge_mask = edge_score &gt;= sorted_edges[threshold_index]<br/></p>
<p>    # Create new graph with pruned adjacency matrix</p>
<p>    pruned_adjacency = A * edge_mask</p>
<p>    return create_subgraph_from_adjacency(graph, pruned_adjacency)</p>
</code>
<h3><a id="appendix-lrm-validation" href="#appendix-lrm-validation">Validating the Replacement Model</a></h3>
<h4><a id="appendix-lrm-validation-node" href="#appendix-lrm-validation-node">Validating Node-to-Logit Influence</a></h4>
<p>We rely on indirect influence statistics for pruning, adaptive generation, and our graph statistics. In this section, we validate that indirect influence is a better proxy for “importance” than other basic baselines like activation or logit attribution (with stop grads).</p>
<p>Specifically, on a sample of 20 prompts, we compute the effect of ablating every feature and measure the KL divergence between the model with the ablation and the clean forward pass. We perform this intervention using “constrained” patching, so we also sweep over different ranges of layers to apply the intervention. In the plot below, we report the average log-log Pearson correlation between ablation KL and initial activation, direct logit edge weight, and indirect influence on the logit. We see that node influence is most predictive of ablation effect, followed by direct edge weights. Both outperform a simple activation baseline.</p>

<h4><a id="appendix-lrm-validation-edges" href="#appendix-lrm-validation-edges">Validating Feature-to-feature Influence</a></h4>
<p>We can use edges in the attribution graphs to estimate the influence features should have on each other. However, if the replacement model is unfaithful or incomplete, a perturbation experiment might not yield results which are consistent with an estimate based on graph influence. For example, initially inactive components could engage in self-correction and dampen the effect of the perturbation. To estimate the extent to which this happens in our graphs, we measure how often perturbations have the expected effect by ablating every feature in the graph, and recording the activation of features downstream of it. Our ablations are done using constrained patching in the range <d-math>[i, i+2]</d-math> where <d-math>i</d-math> is the encoder layer of the feature being perturbed (note this choice of layer is arbitrary, see further discussion <a href="#appendix-cross-layer-steering">here</a>). When averaged over a dataset of 20 prompts, we find a Spearman correlation of 0.72 between the influence of a source feature on a target feature as described by the graph, and the effect of ablating said feature on the target feature’s activation (where we normalize by the original activation and take the absolute value since influence is unsigned). We include scatter plots from three examples from the paper:</p>

<p>These figures show that there are relatively few points that (1) have large influence and no ablation effect (lower right) and (2) have low influence but high ablation effect (upper left). This suggests that indirect influence through the graph is a fairly good proxy for real effects in the model.</p>
<h4><a id="appendix-lrm-validation-faithfulness" href="#appendix-lrm-validation-faithfulness">Evaluating Faithfulness of the Local Replacement Model</a></h4>
<p>In this work, we use (local) replacement models as a window into the mechanisms of the original model. This approach is problematic if the (local) replacement model uses very different mechanisms than the original model. In the main text, we performed perturbations of features to confirm specific mechanistic theories and measured how well the replacement model&#39;s outputs matched the underlying model&#39;s outputs. Here, we focus on how well their inner states match in response to a broader set of perturbations, including off-distribution perturbations, as an (imperfect) gauge of the replacement model’s quality.</p>
<p>We use the local replacement model (i.e. with reconstruction errors added back in as constant factors, and attention patterns frozen<d-footnote>We also freeze attention patterns in the underlying model.</d-footnote>), ensuring agreement of the replacement and original models under zero-size perturbations. Like <a href="#appendix-patching">“iterative patching,”</a> the replacement model may respond to perturbations by activating features which were not previously active at baseline.</p>
<p>We perturb the models at single token positions using 3 types of perturbations: adding a feature’s encoder-vector to the residual stream, adding random directions to the residual stream, and perturbing an upstream feature (to be defined precisely below).</p>
<p>Broadly, averaging across choices of intervention layers, we find that:</p>
<ul><li>Perturbation results are reasonably similar between the replacement model and the underlying model when measured one layer after the intervention.</li><li>Perturbation discrepancies compound significantly over layers.</li><li>Compounding errors have a gradually detrimental effect on the faithfulness of the <span>direction</span> of perturbation effects, which are largely consistent across CLT sizes, with signs of faithfulness worsening slightly as dictionary size increases.</li><li>Compounding errors can have a catastrophically detrimental effect on the <span>magnitude </span>of perturbations. This effect is worse for larger dictionaries.</li></ul>
<p><span>To measure faithfulness for perturbations in encoder directions</span>: </p>
<ul><li>We first choose a residual-stream layer and select a random active feature with an encoder-vector in that layer.</li><li>We take the encoder-vector of the chosen feature, and add a multiple of it (see next bullet) to the residual stream of both the replacement model and frozen underlying model. </li><li>This perturbation to the residual is scaled to increase the chosen feature’s activation to 0.1 above its original value.</li><li>After modifying the residual stream, we perform the forward pass of both perturbed models.</li></ul>
<p><span>To measure faithfulness for perturbations in random directions</span>: </p>
<ul><li>We follow the above process to determine the perturbation’s magnitude, but prior to its addition to both residual streams, the perturbation-vector is rotated to a random direction on the unit sphere.</li></ul>
<p><span>To measure faithfulness for perturbations to upstream features</span>: </p>
<ul><li>We select an active feature to perturb and an <span>intervention layer</span> after the feature’s encoder layer.</li><li>We increase the feature’s activation by 0.1 and run the local replacement model with this perturbation applied, up until the intervention layer.</li><li>At the intervention layer, we compare two forward passes:</li></ul>
<ul><li>In one, the residual-stream state of this forward pass is copied-over to the underlying model at the intervention layer, and the underlying model is run forward from there.</li><li>In the other, we continue the forward pass of the local replacement model.</li></ul>
<p>At each layer downstream of the perturbation, we can determine a net perturbation for both perturbed models by taking the activations from the (perturbed) forward pass and subtracting clean (un-perturbed) baseline activations. The net perturbations for each model are then compared to yield measures of faithfulness, by computing cosine-similarity and mean-squared-error between them.</p>
<p>We believe that perturbing upstream features produces the most “on-distribution” perturbations of these approaches, since random directions may fall into relatively inactive dimensions and encoder-directions are the most sensitive dimensions of the replacement model. Below are results for upstream-feature perturbations to features in layer 5 of the model, averaged over choice of the intervention layer.</p>

<p>Below are cosine-similarity faithfulness metrics for the 10m dictionary for other choices of upstream layers and other perturbation strategies.</p>

<p>As can be seen from the bottom rows of the graphic above, cosine-faithfulness of the 10m dictionary for 18L in the layer following the intervention or perturbation layer is around 60–80%. Computing the average of this bottom-row for different dictionary sizes, in the figure below, the largest dictionaries show signs of mildly diminished faithfulness for perturbations from upstream features.</p>

<p>The decay of faithfulness-metrics over multiple layers, at first glance, raises questions about the reliability of our interpretability graphs, analyses, and steering techniques. Indeed, these results seem at odds with the qualitatively successful results of many of the perturbation experiments we use to validate attribution graphs (both in this paper and its companion). We suspect this is due to a combination of factors:</p>
<ul><li>The compounding unfaithfulness problem affects perturbation <span>magnitudes </span>more severely than <span>directions.</span> We compensate for this issue in practice by trying a range of perturbation strengths in ad-hoc, empirical fashion.</li><li>Aggregating features into supernodes may help “denoise” faithfulness errors.</li><li>The perturbation experiments we perform in the context of graph validation are highly nonrandom – we are typically perturbing and measuring “important” or “cruxy” features, and doing so in the layers where their output is strong. A proportion of the effect we observe can also be attributed to “guaranteed effects”, as we describe in <a href="#appendix-cross-layer-steering">§ Nuances of Steering with Cross-Layer Features</a>.</li></ul>
<p>How do these results compare for per-layer transcoder (PLT) dictionaries? Cosine-faithfulness metrics for PLTs were broadly similar to those for CLTs. The PLT replacement model achieves lower normalized MSE at one layer after the intervention, but compounding errors in the largest PLT replacement models accumulate slightly more rapidly after several layers, as seen in the figure below. This may reflect an advantage of the CLT skip connections, which allow for shorter paths through the replacement model. </p>

<p>Normalized MSE results are sensitive to the scale of the perturbation. To make scales comparable across replacement model types for the figure above, the perturbation to the initial upstream feature in Layer 5 is scaled differently for each sample so that Layer 6 is perturbed by a constant proportion of its magnitude.</p>
<h3><a id="appendix-cross-layer-steering" href="#appendix-cross-layer-steering">Nuances of Steering with Cross-Layer Features</a></h3>
<p>Our perturbation methodology (constrained patching and iterative patching) is somewhat non-obvious at first glance. Why don’t we simply add to the residual stream along a feature’s decoder vector at each layer as we run a forward pass of the model? The reason for this is that it risks double-counting the effect of the feature. For instance, consider a feature in layer 1, with decoders writing to layers 2, 3, and onward. It may be that the activations being reconstructed in layer 2 were, in the original model, causally upstream of the activations being reconstructed in layer 3. Thus, injecting a perturbation into layers 2 and 3 would “double up” on the feature’s effect. Across more layers, this effect could compound to be quite significant.</p>
<p>Our constrained patching approach avoids this problem by computing perturbed MLP outputs once at the outset, and then clamping MLP outputs to these precomputed values (rather than adding to the MLP outputs). While this avoids the double-counting problem, it requires us to choose an intervention layer – we apply the perturbations in all layers up to the intervention layer, but not after (otherwise we would be clamping all of the MLP outputs of the model, and thus we wouldn’t be testing any hypotheses about how the model responds to perturbations). This makes it challenging to measure “the entire effect” of a feature (see <a href="#appendix-unexplained-var">§ Unexplained Variance &amp; Choice of Steering Factors</a>). </p>
<p>It also makes the interpretation of perturbation experiment effects somewhat awkward. Suppose a source feature is in layer 1, and our perturbation intervention layer is 5, i.e. we apply the perturbation in layers 1 through 5. This perturbation will have an impact on, say, features in layer 3. However, the “knock-on” effects of this impact will be at least partially (and perhaps completely) overwritten, since the MLP outputs at layers 4 and 5 are clamped to values computed at the outset. Note that our alternative approach to patching, <a href="#appendix-patching">“iterative patching,”</a> does capture such knock-on effects. However, because of this, it is not as suitable as a validation of mechanisms in an attribution graph, since the perturbation is less precise.</p>
<p>Another subtle aspect of our perturbation experiments is that <span>direct</span> feature-feature interactions described by an attribution graph are nearly forced to be confirmed in steering experiments (when we freeze attention patterns), since they measure linear effects of source feature decoders on target feature encoders (via fixed linear weights, conditioned on frozen attention patterns) – and guaranteed when choosing the intervention layer to be the same as the target feature’s encoder layer. Thus, the nontrivial thing we are testing with perturbation experiments is the validity of the attribution graph’s claims about “knock-on effects.” That is, suppose our attribution graph tells us that feature A excites feature B, which excites feature C, which upweights token X. When we perform a perturbation experiment by inhibiting feature A, we will get an inhibitory effect on feature B “for free,” and we are interested in validating whether the subsequent effects on features C and the output token are as expected.</p>
<h3><a id="appendix-unexplained-var" href="#appendix-unexplained-var">Unexplained Variance and Choice of Steering Factors</a></h3>
<p>Our choice of steering factor scales for intervention experiments is somewhat ad-hoc and empirically driven. For instance, in inhibition experiments, to meaningfully change the output predictions we often must clamp features to <span>negative </span>multiples of their original value, rather than simply to 0. In patching experiments where we add in a feature that was not originally active on a prompt, we often do so using activations significantly greater than that feature’s typical activations.</p>
<p>Why do we need to “overcompensate” in this fashion? We suspect that this is because even when our features play the mechanistic roles that we expect based on attribution graphs, our perturbation experiments capture these mechanisms incompletely:</p>
<ul><li>Unexplained variance – our feature dictionaries are not infinitely large, and our CLTs do not reconstruct activations fully. Some mechanisms may simply be missing, or the features we extract may be projections of “fuller” ground-truth features that partially reside in this unexplained variance.</li><li>Inexhaustive feature selection – on any given prompt, there are typically groups of active features with related meanings that have similar edges in the attribution graph. Thus, perturbing any single feature is insufficient – we need to perturb the full group to see its full mechanistic effect. However, identifying “the full group” is not precisely defined, and also cumbersome (it would require inspecting every active feature on every token position for each prompt). Thus, we end up perturbing groups of features that are most likely incomplete, requiring us to choose extra strong perturbation factors to compensate.</li><li>Incomplete capturing of cross-layer effects – due to the way we perform steering with cross-layer features, our steering results measure the effect of a feature <span>in a particular layer, </span>rather than the effect of the feature globally (see <a href="#appendix-cross-layer-steering">§ Nuances of Steering with Cross-Layer Features</a>). Thus, our perturbations systematically undervalue features’ whole effects.</li></ul>
<h3><a id="appendix-dupe-features" href="#appendix-dupe-features">Similar Features and Supernodes</a></h3>
<p>We often find that there are many features in a given graph which seem to have similar roles. There are a few candidate theories for explaining this phenomenon (which we discuss below). Whatever the underlying reason, this suggests that individual features are often best understood as <span>partial</span> contributors to a component of the mechanism used by the model. To capture these components more completely, we group related features into “supernodes”. While this process is inherently somewhat subjective, we find it is important to clarify key mechanisms.</p>
<p>Grouping features produces a simplified “supernode graph,” where we compute the edges between supernodes according to the formula below:</p>
<p><d-math block="" sub=""> \text{supernode\_adjacency}(t, s) = \frac{\sum_{n_t \in N_t} \text{frac\_external}(n_t) \cdot \sum_{n_s \in N_s} A&#39;_{n_t, n_s}}{\sum_{n_t \in N_t} \text{frac\_external}(n_t)} </d-math></p>
<p>Where:</p>
<ul><li><d-math>N_t</d-math> and <d-math>N_s</d-math> are the set of nodes in supernodes <d-math>t</d-math> and <d-math>s</d-math>, respectively.</li><li><d-math>n_t</d-math> and <d-math>n_s</d-math> refer to a node in the corresponding supernode.</li><li><d-math>\text{frac\_external}(n_t)</d-math> is the fraction of the absolute valued input weights to <d-math>n_t</d-math> that originate from nodes outside of the supernode.</li><li><d-math>A&#39;</d-math> refers to an input normalized graph adjacency matrix, where the weights of edges targeting each node are divided by a constant to sum to 1. <d-math>A&#39;_{n_t, n_s}</d-math> is the normalized edge weight from node <d-math>n_t</d-math> to node <d-math>n_s</d-math>.</li></ul>
<p>Hypotheses for why there so many related features include:</p>
<ul><li>These features represent subtly different concepts that are hard to discern from our visualization tools. Indeed, in some cases the differences are noticeable – for instance, we may choose to group together a “Texas cities” feature with a feature representing “the state of Texas” if both play a similar role in the circuit. See our <a href="#limitations-abstraction-level">§ Limitations</a> section on feature splitting for more on this issue. A related issue that could be involved is that of <span>feature manifolds</span> <d-cite key="olah2023manifold,engels2024not,olah2024multidimensional,gorton2024manifold"></d-cite> – concepts which are represented in truly multidimensional fashion by the model. Our CLTs may represent these as collections of hard-to-distinguish features.</li><li>Features in different layers may represent the same concept, but based on different inputs. For instance, a layer 5 “Michael Jordan” feature may be capable of detecting more indirect references to Michael Jordan, while a layer 1 “Michael Jordan” feature can only detect when the name is written out explicitly.</li><li>The output response of an MLP after increasing a given input feature is generically nonlinear, even when monotonic, and may require a few CLT features to approximate. (Note that a single-layer MLP with one input dimension and one output dimension can be highly nonlinear as you vary the input, and require multiple features to approximate; composing MLP layers could exacerbate this.) One experiment that could help evaluate this hypothesis would be to perturb an upstream feature using a sweep of strengths, and measure its effect on the activity of a downstream feature; if the Dallas→Austin interaction were capable of being mediated by a single Texas feature, then that dependency would have to be threshold-linear. If it&#39;s not, we&#39;d need a group of features to capture the mechanism.</li></ul>
<h3><a id="appendix-patching" href="#appendix-patching">Iterative Patching</a></h3>
<p>Instead of constrained patching, we may wish to measure the effect of a feature intervention within the patching range in addition to its effect outside of the range. To do so, we can iteratively recompute the values of features which read from layers in the patching range, letting them take on new values due to our intervention. We call this iterative patching. While iterative patching may seem advantageous since it accounts for all of the effects of a feature intervention (at least according to the replacement model), it can often lead to cascading errors, as we need to repeatedly encode and decode features. In addition, the main goal of interventions is to validate edges between nodes and supernodes in attribution graphs. Constrained patching provides a simpler way to do this by limiting indirect effects.</p>
<p>The illustration below shows cascading effects. We patch a feature at layer 1, and impact two features computed at layer 2, and so are affected by our patch. At the end of our patch, we inject the computed state of the residual stream back into the model.</p>

<h3><a id="appendix-interventions" href="#appendix-interventions">Details of Interventions</a></h3>
<p>In this section, we show full intervention results for some of the case studies we discussed above.</p>
<p>Below, we show the effect of inhibiting every supernode in the acronym case study with an inhibition factor of 1.</p>

<p>Supernodes require different steering strengths to show an effect because the strength of their outgoing edges varies. This strength depends both on the number of features in the supernode and their individual edge strengths. The steering plot above shows that inhibiting <span data-args="18l__b15808787_b27769861_b67082">“say _A”</span> or <span data-args="18l__b5477227_b29852699">“say DA”</span> has only a minor effect on the logit at a strength of −1. This is partially due to <span data-args="18l__b15808787_b27769861_b67082">“say _A”</span> containing only one feature, while supernodes like <span data-args="18l__b27339305_b25733589_b14129802_b8881455_b25876490_b18707149">“say D_”</span> contain six. <span data-args="18l__b15808787_b27769861_b67082">“say _A”</span>’s influence on the logit is also indirect and mediated by the <span data-args="18l__b5477227_b29852699">“say DA”</span> supernode. If we increase steering strength to −5, we do observe an effect from inhibiting <span data-args="18l__b15808787_b27769861_b67082">“say _A”</span>.</p>

<p>For reference, we also show the effect of inhibiting more of the supernodes in the 36+59 prompt.</p>

<h3><a id="appendix-interface" href="#appendix-interface">Notes on the Interface</a></h3>
<p>We share an MIT licensed version of the interactive attribution graph interface used in this paper on <a href="https://github.com/anthropics/attribution-graphs-frontend">github</a>.</p>
<p>This interface has been simplified from the full version used internally for rapid exploration in favor of a publicly sharable version useful for inspecting labeled graphs. Analysis tools which were trimmed, and may be useful for practitioners to reimplement, include:</p>
<ul><li>Feature scatterplots comparing arbitrary scalars attached to features during the graph generation pipeline. </li><li>In situ feature and supernode labeling.</li><li>Feature examples and additional detail shown on hover to skim through features faster.</li><li>A diff view comparing two attribution graphs, highlighting the difference and intersections in feature activations and edge weights.</li><li>A list of all the active features, their labels and edge weight to the active node.</li></ul>
<p>A rearrangeable grid containing these diverse “widgets” made it easier to experiment with new ideas and rescale the interface to show dozens or thousands of features.</p>
<h3><a id="appendix-res-norms" href="#appendix-res-norms">Residual Stream Norms Growth</a></h3>
<p>In many models, the norm of the residual stream grows exponentially over the forward pass. Heimersheim and Turner <d-cite key="stefanhex2023resstream"></d-cite> observed this norm growth in the GPT-2, OPT, and Pythia families, and conjectured it serves the functional purpose of making space for new information. Rather than deleting old information by explicitly writing its negative, a layer may simply write new information at a larger magnitude, drowning out the influence of the older information by increasing the denominator of the normalization denominators. Thus to keep information around and legible, the model must actively amplify in many layers. A cross-layer transcoder can absorb all of that into a single feature, while per-layer transcoders or neurons will have features (neurons) in many layers performing the same function, and all will appear in an attribution graph. This does occur <a href="#evaluating-graphs-paths">in practice</a>.</p>
<p>We observe the same phenomenon in our research model, 18L, which also has normalization denominators.</p>

<h3><a id="appendix-interference-weights" href="#appendix-interference-weights">Interference Weights over More Features</a></h3>
<p>In the <a href="#global-weights">main text</a>, we showed a single feature’s virtual weights, and found that many large connections had weak coactivation statistics. Here, we run a similar analysis on a larger sample of features to better understand if this pattern generalizes.</p>
<p>We begin with the set of features that activate on our addition prompts and appear in our pruned attribution graphs. From these, we sample 1000 features with equal representation across layers. We then compute the virtual weights and coactivation statistics for all input and output connections of these features using a dataset of ~150M tokens of the CLT’s training data.</p>
<p>Below, we show the virtual weight distribution across all of these connections, and compare it to the distribution of weights between pairs that ever coactivate on the dataset. The percentage of coactive weights within each bin appears as text labels in the figure.</p>

<p>We consider weights between features with no coactivations to be “unused” in our dataset; a source feature that never coactivates with a target feature likely never causally affects the target&#39;s activation value.<d-footnote>This interpretation is not strictly true with negative weights, as a strong negative weight might actually prevent coactivation.</d-footnote></p>
<p>In general, a large mass of small weights dominates the distribution (as expected). The smallest weights are very often unused with many bins having single-digit “use percentages.” Large negative weights also stand out as a difference between the raw and coactive distributions, yet this result isn’t surprising (even without interference); if an upstream feature is active, such a strong negative weight might guarantee that its downstream target never activates.</p>
<p>More significantly, we also observe that even among extremely large positive weights (beyond the 99.9999th percentile), a significant proportion are unused. This is not conclusive proof of interference, because we may still miss crucial tokens in our sampling where these edges are used. Still, we take this analysis as suggestive of interference in 18L.</p>
<h3><a id="appendix-full-number-weights" href="#appendix-full-number-weights">Number Output Weights for Select Features</a></h3>
<p>We show the full output weights (over tokens 0, 1, ..., 999) for three features whose output patterns are neither periodic nor approximate magnitudes. The first promotes “simple numbers”, promoting both smaller numbers and rounder numbers. The second promotes numbers starting with 9, and the latter promotes numbers starting (or, to a lesser degree, ending) with 95. (In the color scheme, upweighted tokens are red and downweighted tokens are blue.)</p>

<h3><a id="appendix-arithmetic-comparison" href="#appendix-arithmetic-comparison">Comparison of Addition Features Between Small and Large Models</a></h3>
<p>We find similar families of features in Haiku and 18L for performing addition, but note that the ones for 18L are less precise. 18L performs slightly worse on these prompts (98.6% top-1 accuracy vs. 100% for Haiku 3.5), which possibly reflects this imprecision.</p>

<h3><a id="appendix-eval-details" href="#appendix-eval-details">Additional Evaluation Details</a></h3>
<p>To evaluate our replacement model and attribution graphs, we curated a dataset of random but nontrivial pretraining tokens. Specifically, we considered short sequences from the Pile (minus books) dataset <d-cite key="gao2020pile"></d-cite> and target tokens where</p>
<ul><li>The underlying model made the correct top-1 prediction;</li><li>The token has a dataset density less than <code>1e-4</code> to filter out stop tokens and common tokens like articles, punctuations, and newlines;</li><li>The token did not appear elsewhere in the context (in these cases we often have high graph scores but that the graph is largely mediated by attention which we do not explain);</li><li>The token has loss &gt;0.2 to filter out other trivial predictions coming from completing multi-token words, fuzzy induction, memorized text, etc.</li></ul>
<p>Due to the expense of graph generation, we used a dataset size of n=260 for graph evaluation.</p>
<h3><a id="appendix-all-graphs" href="#appendix-all-graphs">Prompt and Graph Lists</a></h3>
<p>The below table contains a random sample of prompts and graphs for our Pile minus books evaluation set. It includes graphs for both our 10m crosslayer transcoder (CLT) and 10m per-layer transcoder (PLT) dictionaries on our 18L model.</p>
<table><tbody><tr><td colspan="1" rowspan="1"><p><span>Links</span></p></td><td colspan="1" rowspan="1"><p><span>Prompt</span></p></td><td colspan="1" rowspan="1"><p><span>Target</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-29-clt">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-29-plt">plt</a></p></td><td colspan="1" rowspan="1"><p><code>arrangement, the optical fiber 104 is optically</code>  </p></td><td colspan="1" rowspan="1"><p><code>coupled</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-37-clt">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-37-plt">plt</a></p></td><td colspan="1" rowspan="1"><p><code>&#34; &#34;I already called the police.&#34; &#34;They\&#39;re almost here.&#34; &#34;You got to go.&#34; &#34;Goodbye.&#34; &#34;Good</code> </p></td><td colspan="1" rowspan="1"><p><code>luck</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-50-clt">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-50-plt">plt</a></p></td><td colspan="1" rowspan="1"><p><code>types, such as a front projection type and a rear projection type, depending on how images are</code></p></td><td colspan="1" rowspan="1"><p><code>projected</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-99-clt">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-99-plt">plt</a></p></td><td colspan="1" rowspan="1"><p><code>for grants at the moment to pay a few students a paltry sum to stay and work in her lab over the</code> </p></td><td colspan="1" rowspan="1"><p><code>summer</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-179-clt">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pmb-179-plt">plt</a></p></td><td colspan="1" rowspan="1"><p><code>invention. In a separate aspect, the invention provides a method of potentiating the</code></p><p><code> actions of other CNS active compounds. This method comprises administering an</code></p></td><td colspan="1" rowspan="1"><p><code>effective</code></p></td></tr></tbody></table>
<p>We also include a sample of the basic curated prompts we used in the development of our techniques and their corresponding CLT and PLT graphs. These were designed to exercise basic capabilities such as factual recall, analogical reasoning, memorization, arithmetic, multilinguality, and in-context learning.</p>
<table><tbody><tr><td colspan="1" rowspan="1"><p><span>Links</span></p></td><td colspan="1" rowspan="1"><p><span>Prompt</span></p></td><td colspan="1" rowspan="1"><p><span>Target</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=michael-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=michael-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>Fact: Michael Jordan plays the sport of</code></p></td><td colspan="1" rowspan="1"><p><code>basketball</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=michael-fr-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=michael-fr-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>Fait: Michael Jordan joue au </code></p></td><td colspan="1" rowspan="1"><p><code>basket</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=sally-school-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=sally-school-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>At first, Sally hated school. But over time she changed her mind. Now she is </code></p></td><td colspan="1" rowspan="1"><p><code>happy</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=iasg-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=iasg-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>The International Advanced Security Group (IA </code></p></td><td colspan="1" rowspan="1"><p><code>SG</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=ndag-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=ndag-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>The National Digital Analytics Group (N </code></p></td><td colspan="1" rowspan="1"><p><code>DAG</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=count-by-sevens-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=count-by-sevens-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>7 14 21 28 35 </code></p></td><td colspan="1" rowspan="1"><p><code>42</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=common-colors-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=common-colors-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>grass: green sky: blue corn: yellow carrot: orange strawberry: </code></p></td><td colspan="1" rowspan="1"><p><code>red</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=opposite-hot-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=opposite-hot-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>The opposite of &#34;hot&#34; is &#34; </code></p></td><td colspan="1" rowspan="1"><p><code>cold</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=opposite-of-small-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=opposite-of-small-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>The opposite of &#34;small&#34; is &#34; </code></p></td><td colspan="1" rowspan="1"><p><code>big</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=five-plus-three-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=five-plus-three-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>5 + 3 = </code></p></td><td colspan="1" rowspan="1"><p><code>8</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=uspto-telephone-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=uspto-telephone-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>Examiner interviews are available via </code></p></td><td colspan="1" rowspan="1"><p><code>telephone</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=currency-analogy-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=currency-analogy-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>Mexico:peso :: Europe: </code></p></td><td colspan="1" rowspan="1"><p><code>euro</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=capital-analogy-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=capital-analogy-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>Zagreb:Croatia :: Copenhagen: </code></p></td><td colspan="1" rowspan="1"><p><code>Denmark</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pandas-group-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=pandas-group-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>def customer_spending(transaction_df): for customer_id, customer_df in transaction_df.</code> </p></td><td colspan="1" rowspan="1"><p><code>group</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=season-after-spring-fr-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=season-after-spring-fr-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>La saison après le printemps s&#39;appelle l&#39;</code> </p></td><td colspan="1" rowspan="1"><p><code>été</code></p></td></tr><tr><td colspan="1" rowspan="1"><p><a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=str-indexing-pos-0-clt-clean">clt</a> <a href="https://missfunmi.com/quick-tip-add-vscode-snippets/static_js/attribution_graphs/index.html?slug=str-indexing-pos-0-plt-clean">plt</a></p></td><td colspan="1" rowspan="1"><p><code>a = &#34;Craig&#34; assert a[0] == &#34; </code></p></td><td colspan="1" rowspan="1"><p><code>C</code></p></td></tr></tbody></table>
 
</div></div>
  </body>
</html>
