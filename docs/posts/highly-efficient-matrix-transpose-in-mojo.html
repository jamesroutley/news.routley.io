<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://veitner.bearblog.dev/highly-efficient-matrix-transpose-in-mojo/">Original</a>
    <h1>Highly efficient matrix transpose in Mojo</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-06T17:46Z">
                    06 Jun, 2025
                </time>
            </i>
        </p>
    

    <p>In this blogpost I will step by step show you how to implement a highly efficient transpose kernel for the <code>Hopper</code> architecture using Mojo.
The best kernel archives a bandwidth of <code>2775.49 GB/s</code>, i.e. <code>84.1056%</code>.  The optimisations are the same that I applied to archive a bandwidth of <code>2771.35 GB/s</code> using pure <code>CUDA</code> on the same <code>H100</code> that I use here. That shows that Mojo can archive <code>CUDA</code> like performance on exactly the same task. You may compare the kernels with the previous <a href="https://github.com/simveit/effective_transpose/tree/main">kernels</a> I wrote and read my other <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost</a> as well where I explain the concepts in detail. Here I will only briefly review them and instead focus on the implementation details. For readers without knowledge how to use <code>TMA</code> in Mojo I refer you to my previous <a href="https://veitner.bearblog.dev/use-tma-without-cuda/">blogpost</a> on this topic.</p>
<h2 id="naive-approach">Naive approach</h2><p>Before calling the kernel we need to initialise two <code>TMA descriptors</code>, this concept is similar to <code>cuTensorMapEncodeTiled</code> we can use in <code>CUDA</code>.</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span><span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We have two descriptors. Both in row major format, the one the transpose of the other. The corresponding <code>smems</code> in relation of transpose as well.
As a quick reminder he is the algorithm we are going to implement.
We take a tile, perform transpose inside the tile and put it at the opposite position in the matrix, i.e. at the transposed position</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/veitner/36.webp" alt="Screenshot 2025-06-06 at 19"/></p>
<p>Below is the code that archives that.</p>
<div><pre><span></span><span>@__llvm_arg_metadata</span><span>(</span><span>descriptor</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>@__llvm_arg_metadata</span><span>(</span><span>descriptor_tr</span><span>,</span> <span>`</span><span>nvvm</span><span>.</span><span>grid_constant</span><span>`</span><span>)</span>
<span>fn</span> <span>transpose_kernel_naive</span><span>[</span>
    <span>block_size</span><span>:</span> <span>Int</span>
<span>](</span><span>descriptor</span><span>:</span> <span>TMADescriptor</span><span>,</span> <span>descriptor_tr</span><span>:</span> <span>TMADescriptor</span><span>):</span>
    <span>var</span> <span>shmem</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>shmem_tr</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>block_size</span> <span>*</span> <span>block_size</span><span>,</span>
        <span>DType</span><span>.</span><span>float32</span><span>,</span>
        <span>alignment</span><span>=</span><span>1024</span><span>,</span>
        <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span><span>,</span>
    <span>]()</span>
    <span>var</span> <span>mbar</span> <span>=</span> <span>stack_allocation</span><span>[</span>
        <span>1</span><span>,</span> <span>Int64</span><span>,</span> <span>address_space</span> <span>=</span> <span>_GPUAddressSpace</span><span>.</span><span>SHARED</span>
    <span>]()</span>
    <span>var</span> <span>descriptor_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>
    <span>var</span> <span>descriptor_tr_ptr</span> <span>=</span> <span>UnsafePointer</span><span>(</span><span>to</span><span>=</span><span>descriptor_tr</span><span>)</span><span>.</span><span>bitcast</span><span>[</span><span>NoneType</span><span>]()</span>

    <span>x</span> <span>=</span> <span>block_idx</span><span>.</span><span>x</span> <span>*</span> <span>block_size</span>
    <span>y</span> <span>=</span> <span>block_idx</span><span>.</span><span>y</span> <span>*</span> <span>block_size</span>

    <span>col</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>%</span> <span>block_size</span>
    <span>row</span> <span>=</span> <span>thread_idx</span><span>.</span><span>x</span> <span>//</span> <span>block_size</span>

    <span># LOAD</span>
    <span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
        <span>mbarrier_init</span><span>(</span><span>mbar</span><span>,</span> <span>1</span><span>)</span>
        <span>mbarrier_arrive_expect_tx_shared</span><span>(</span><span>mbar</span><span>,</span> <span>block_size</span> <span>*</span> <span>block_size</span> <span>*</span> <span>4</span><span>)</span>
        <span>cp_async_bulk_tensor_shared_cluster_global</span><span>(</span>
            <span>shmem</span><span>,</span> <span>descriptor_ptr</span><span>,</span> <span>mbar</span><span>,</span> <span>Index</span><span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
        <span>)</span>
    <span>barrier</span><span>()</span>
    <span>mbarrier_try_wait_parity_shared</span><span>(</span><span>mbar</span><span>,</span> <span>0</span><span>,</span> <span>10000000</span><span>)</span>
</pre></div>
<p>We annotate the descriptors with <code>nvvm.grid_constant</code> similar to what we would do in <code>CUDA</code>.
After allocating the shared memories we define the upper left coordinate of the tile using <code>x</code> and <code>y</code> and get <code>row</code> and <code>column</code> the current thread is responsible fore.
We&#39;ll than copy over the tile to the shared memory array.
This kernel archives a bandwidth of <code>1056.08 GB/s</code> which is faster than the <code>875.46 GB/s</code> we archived using <code>CUDA</code>. I believe that to be the reason because we use the <code>PTX</code> api for <code>TMA</code> transfers in Mojo. You can read about the difference between these in the <code>CUDA</code> api in <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">this excellent blogpost</a>.</p>
<div><pre><span></span><span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span><span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col</span><span>]</span>

<span># FENCE</span>
<span>barrier</span><span>()</span>
<span>tma_store_fence</span><span>()</span>
</pre></div>
<p>We compute the transpose using our two arrays. We&#39;ll than create a <code>fence</code> to let the <code>TMA</code> know we are finished with computation.</p>
<h3 id="store-to-gmem">Store to gmem</h3><div><pre><span></span><span># STORE</span>
<span>if</span> <span>thread_idx</span><span>.</span><span>x</span> <span>==</span> <span>0</span><span>:</span>
	<span>cp_async_bulk_tensor_global_shared_cta</span><span>(</span>
		<span>shmem_tr</span><span>,</span> <span>descriptor_tr_ptr</span><span>,</span> <span>Index</span><span>(</span><span>y</span><span>,</span> <span>x</span><span>)</span>
	<span>)</span>
	<span>cp_async_bulk_commit_group</span><span>()</span>

<span>cp_async_bulk_wait_group</span><span>[</span><span>0</span><span>]()</span>
</pre></div>
<p>We store the transposed result to the GMEM using the transposed TMA descriptor.</p>
<h2 id="swizzling">Swizzling</h2><p>For a more detailed explanation of what swizzling is and how it works please in my previous <a href="https://veitner.bearblog.dev/making-matrix-transpose-really-fast-on-hopper-gpus/">blogpost on matrix transpose</a> the concept is the same for Mojo. In the repo I link to at the end there is also one program which you can use to understand swizzling yourself.
Only two things need to be adjusted to make swizzling work:</p>
<ul>
<li>The descriptors need to be provided with the appropriate swizzling mode</li>
<li>Inside the kernel we need to use swizzled indices</li>
</ul>
<p>This can be implemented as follows</p>
<div><pre><span></span><span>var</span> <span>descriptor</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_dev</span><span>,</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>GMEM_WIDTH</span><span>),</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_HEIGHT</span><span>,</span> <span>SMEM_WIDTH</span><span>),</span>
<span>)</span>
<span>var</span> <span>descriptor_tr</span> <span>=</span> <span>create_tma_descriptor</span><span>[</span>
	<span>DType</span><span>.</span><span>float32</span><span>,</span> <span>2</span><span>,</span> <span>TensorMapSwizzle</span><span>.</span><span>SWIZZLE_128B</span>
<span>](</span>
	<span>gmem_tr_dev</span><span>,</span>
	<span>(</span><span>GMEM_WIDTH</span><span>,</span> <span>GMEM_HEIGHT</span><span>),</span>
	<span>(</span><span>GMEM_HEIGHT</span><span>,</span> <span>1</span><span>),</span>
	<span>(</span><span>SMEM_WIDTH</span><span>,</span> <span>SMEM_HEIGHT</span><span>),</span>
<span>)</span>
</pre></div>
<p>We can compute swizzled indices like this:</p>
<div><pre><span></span><span>fn</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16_tr</span> <span>=</span> <span>(</span><span>col</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>row</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16_tr</span> <span>=</span> <span>i16_tr</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16_tr</span> <span>=</span> <span>i16_tr</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz_tr</span> <span>=</span> <span>y16_tr</span> <span>^</span> <span>x16_tr</span>
    <span>return</span> <span>((</span><span>x16_swz_tr</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>BLOCK_SIZE</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>row</span> <span>&amp;</span> <span>3</span><span>)</span>


<span>fn</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>:</span> <span>Int</span><span>](</span><span>col</span><span>:</span> <span>Int</span><span>,</span> <span>row</span><span>:</span> <span>Int</span><span>)</span> <span>-&gt;</span> <span>Int</span><span>:</span>
    <span>i16</span> <span>=</span> <span>(</span><span>row</span> <span>*</span> <span>BLOCK_SIZE</span> <span>+</span> <span>col</span><span>)</span> <span>*</span> <span>4</span> <span>&gt;&gt;</span> <span>4</span>
    <span>y16</span> <span>=</span> <span>i16</span> <span>&gt;&gt;</span> <span>3</span>
    <span>x16</span> <span>=</span> <span>i16</span> <span>&amp;</span> <span>7</span>
    <span>x16_swz</span> <span>=</span> <span>y16</span> <span>^</span> <span>x16</span>
    <span>return</span> <span>((</span><span>x16_swz</span> <span>*</span> <span>4</span><span>)</span> <span>&amp;</span> <span>(</span><span>block_size</span> <span>-</span> <span>1</span><span>))</span> <span>+</span> <span>(</span><span>col</span> <span>&amp;</span> <span>3</span><span>)</span>
</pre></div>
<p>and than use the swizzled indices inside our kernel like so:</p>
<div><pre><span></span><span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col</span><span>,</span> <span>row</span><span>)</span>
<span>...</span>
<span># COMPUTE</span>
<span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
	<span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
<span>]</span>
</pre></div>
<p>Everything else is exactly the same.</p>
<p>This kernel archives <code>1437.55 GB/s</code> compared to the <code>1251.76 GB/s</code> we get in <code>CUDA</code>.</p>
<h2 id="processes-a-batch-of-columns-per-thread">Processes a batch of columns per thread</h2><p>An important and common optimisation one can apply in memory bound kernels is thread coarsening which is essentially putting more work on each thread.
We can modify the previous kernel as follows to do that:</p>
<div><pre><span></span>    <span># COMPUTE</span>
    <span>@parameter</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>batch_size</span><span>):</span>
        <span>col_</span> <span>=</span> <span>col</span> <span>+</span> <span>i</span>
        <span>row_</span> <span>=</span> <span>row</span>
        <span>col_swizzle</span> <span>=</span> <span>calculate_col_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>row_swizzle</span> <span>=</span> <span>calculate_row_swizzle</span><span>[</span><span>block_size</span><span>](</span><span>col_</span><span>,</span> <span>row_</span><span>)</span>
        <span>shmem_tr</span><span>[</span><span>col</span> <span>*</span> <span>block_size</span> <span>+</span> <span>row_swizzle</span><span>]</span> <span>=</span> <span>shmem</span><span>[</span>
            <span>row</span> <span>*</span> <span>block_size</span> <span>+</span> <span>col_swizzle</span>
        <span>]</span>
</pre></div>
<p>Note that we launch less threads with this approach (we divide by a factor of <code>batch_size</code>) to account for the fact we are processing multiple columns per thread now.</p>
<p>This kernel archives a bandwidth of <code>2775.49 GB/s</code> compared to the <code>2771.35 GB/s</code> we archived in the equivalent <code>CUDA</code> kernel.</p>
<h2 id="conclusion">Conclusion</h2><p>I hope this blogpost showed you how to archive high performance on a common task in GPU computing using Mojo.
Feel free to contact me on <a href="https://www.linkedin.com/in/simon-veitner-174a681b6/">Linkedin</a> to chat about GPU programming or other topics related to MLSys.</p>
<p>The full code for the blogpost can be find on my <a href="https://github.com/simveit/efficient_transpose_mojo">Github</a>.</p>


    

    
        

        
            


        
    


  </div></div>
  </body>
</html>
