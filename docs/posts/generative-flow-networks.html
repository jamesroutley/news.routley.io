<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yoshuabengio.org/2022/03/05/generative-flow-networks/">Original</a>
    <h1>Generative Flow Networks</h1>
    
    <div id="readability-page-1" class="page"><div>
								
<p><strong>I have rarely been as enthusiastic about a new research direction</strong>. We call them <strong>GFlowNets</strong>, for Generative Flow Networks. They live somewhere at the intersection of reinforcement learning, deep generative models and energy-based probabilistic modelling. They are also related to variational models and inference and I believe open new doors for non-parametric Bayesian modelling, generative active learning, and unsupervised or self-supervised learning of abstract representations to disentangle both the explanatory causal factors and the mechanisms that relate them. What I find exciting is that they open so many doors, but in particular for implementing the system 2 inductive biases I have been discussing in many of my papers and talks since 2017, that I argue are important to incorporate causality and deal with out-of-distribution generalization in a rational way. They allow neural nets to model distributions over data structures like graphs (for example molecules, as in the NeurIPS paper, or explanatory and causal graphs, in current and upcoming work), to sample from them as well as to estimate all kinds of probabilistic quantities (like free energies, conditional probabilities on arbitrary subsets of variables, or partition functions) which otherwise look intractable. </p>



<p>At the same time, this is a new beast which may, at first, look strange, one that we need to tame, for which the appropriate optimization algorithms are still making rapid progress (e.g. see <a href="https://www.notion.so/milayb/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00#0b0c2b1342b74473af1743068f5a218c">this paper</a>), and many opportunities probably waiting for us in upcoming research. It has taken time for many of my students to digest the new concepts, but it is worth it. A volley of new papers are coming out and more are in preparation, as the creative juices are boiling. My son Emmanuel led the first paper on GFlowNets and wrote a <a href="http://folinoid.com/w/gflownet/">blog entry</a> about it to accompany our <a href="https://papers.nips.cc/paper/2021/hash/e614f646836aaed9f89ce58e837e2310-Abstract.html">NeurIPS 2021 paper</a>, the original GFlowNet paper. More recently, I have written a <a href="https://milayb.notion.site/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00">tutorial</a> to explain the basic ideas, and you will also find there all the recent publicly available papers. See also this rather technical <a href="https://bluejeans.com/playback/s/KD9fvEtAYxCC7ta2fKcX51HCJuTnT5Q7CsPc6L6oDGCovJnGNBvX21X6tjtnm4u1">talk</a> I recently gave internally at Mila and this more accessible and philosophical discussion about <a href="https://www.youtube.com/watch?v=M49TMqK5uCE">GFlowNets, causality and consciousness</a> providing a window on upcoming developments of GFlowNets aimed at bridging the gap between SOTA AI and human intelligence by introducing system 2 inductive biases in neural nets. Enjoy!</p>



<figure><img loading="lazy" width="379" height="229" src="https://yoshuabengio.org/wp-content/uploads/2022/03/gflownet_anim.gif" alt=""/></figure>



<p>This figure illustrates why we use the word ‚Äúflow‚Äù in GFlowNets. We think about the flow of unnormalized probabilities, similar to the <span>amount of water flowing</span> from an initial state (<em>s<sub>0</sub></em> on the left) in a directed acyclic graph (that may be exponentially large, so we don‚Äôt need to represent it explicitly in computers) whose trajectories correspond to all the possible sequences of actions, actions that determine state transitions) in order to sequentially construct complicated objects like <a href="https://www.notion.so/milayb/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00#1cc95d78cc75455bbc3a56c5ba7e2a68">molecular graphs</a>, <a href="https://www.notion.so/milayb/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00#00b24daeb5cc4347b4a0ea7535b34f7a">causal graphs</a>, explanations for a scene, or (and this is the real inspiration) <strong>thoughts in our mind</strong>. The <span>square nodes with red transitions</span> correspond to terminal states, completed objects sampled by a policy which chooses stochastically the children of each state (the states accessible by going one step downstream) with a probability proportional to the flow in the corresponding outgoing edge. Interestingly, this makes it possible to generate a diverse set of samples without facing what I used to think was the intractable challenge of mixing between modes with MCMC methods. What is remarkable with this framework is that it tells us how to train a policy that will sample the constructed objects with the desired probability (specified by an energy function or unnormalized probability function or reward function, <a href="https://www.notion.so/milayb/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00#501449fd3de348e8b93769a523d1769d">which we can also learn</a>) and how to estimate the corresponding normalizing constants and <a href="https://www.notion.so/milayb/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00#afe03e54d6db43468f8dee3a3350f98a">conditional probabilities over any subset of abstract variables</a>. See more in the <a href="https://milayb.notion.site/GFlowNet-Tutorial-919dcf0a0f0c4e978916a2f509938b00">tutorial</a> üôÇ</p>
											</div></div>
  </body>
</html>
