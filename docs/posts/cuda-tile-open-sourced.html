<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NVIDIA/cuda-tile">Original</a>
    <h1>CUDA Tile Open Sourced</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">CUDA Tile IR is an MLIR-based intermediate representation and compiler
infrastructure for CUDA kernel optimization, focusing on tile-based computation
patterns and optimizations targeting NVIDIA tensor core units. The project
provides a comprehensive ecosystem for expressing and optimizing tiled
computations for NVIDIA GPUs, simplifying the development of high-performance
CUDA kernels through abstractions for common tiling patterns, memory hierarchy
management, and GPU-specific optimizations.</p>
<p dir="auto">This open-source release is aligned with the <strong>CUDA Toolkit 13.1</strong> release. For
more information about CUDA Tile, visit <a href="https://developer.nvidia.com/cuda/tile" rel="nofollow">https://developer.nvidia.com/cuda/tile</a>.</p>

<p dir="auto">CUDA Tile is composed of:</p>
<ul dir="auto">
<li><strong>CUDA Tile Dialect</strong>: A domain-specific MLIR dialect that provides
first-class operations and types for tile-based computations</li>
<li><strong>Python Bindings</strong>: Complete Python API for programmatic IR construction,
manipulation, and transformation</li>
<li><strong>Bytecode:</strong>: Efficient binary representation with support for serialization
and de-serialization between the CUDA Tile dialect and binary format.</li>
<li><strong>Conformance Test Suite</strong>: Comprehensive tests ensuring compliance with the
CUDA Tile specification and validation of dialect semantics</li>
</ul>

<p dir="auto">CUDA Tile development is driven by the CUDA Tile IR specification, which defines
the formal semantics, operations, and type system for tile-based computations on
NVIDIA GPUs. For detailed information about the CUDA Tile IR specification,
including dialect operations, type system, and transformation passes, please
refer to the <a href="https://docs.nvidia.com/cuda/tile-ir/13.1/index.html" rel="nofollow">CUDA Tile Specification</a>.</p>


<ul dir="auto">
<li>CMake 3.20.0 or higher</li>
<li>C++17 compatible compiler</li>
<li>Python 3.6+ (for Python bindings)</li>
<li>MLIR/LLVM sources or pre-built libraries at a compatible commit (see
<a href="https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29">cmake/IncludeLLVM.cmake</a> for the exact version)</li>
<li>Ninja build system (optional)</li>
</ul>

<p dir="auto">For a quick start, use the following commands from the top of the repository to
configure and build a release version of CUDA Tile with Python bindings enabled.
MLIR/LLVM sources will be automatically downloaded from
<a href="https://github.com/llvm/llvm-project">https://github.com/llvm/llvm-project</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Configure
cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DLLVM_ENABLE_ASSERTIONS=OFF \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON

# Build
cmake --build build

# Run tests
cmake --build build --target check-cuda-tile"><pre><span><span>#</span> Configure</span>
cmake -G Ninja -S <span>.</span> -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DLLVM_ENABLE_ASSERTIONS=OFF \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON

<span><span>#</span> Build</span>
cmake --build build

<span><span>#</span> Run tests</span>
cmake --build build --target check-cuda-tile</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Build Configuration Options</h3><a id="user-content-build-configuration-options" aria-label="Permalink: Build Configuration Options" href="#build-configuration-options"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">MLIR/LLVM Build Configuration</h4><a id="user-content-mlirllvm-build-configuration" aria-label="Permalink: MLIR/LLVM Build Configuration" href="#mlirllvm-build-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">CUDA Tile requires MLIR/LLVM at a specific compatible commit. The exact commit
hash is specified in <a href="https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29">cmake/IncludeLLVM.cmake</a>.
CUDA Tile can be built with MLIR/LLVM in three different ways:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Automatic Download from GitHub</strong> (Default): CMake automatically downloads
MLIR/LLVM sources from the official GitHub repository and builds them at the
compatible commit. This is the slowest option but requires no manual LLVM
setup.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -G Ninja -S . -B build -DCMAKE_BUILD_TYPE=Release"><pre>cmake -G Ninja -S <span>.</span> -B build -DCMAKE_BUILD_TYPE=Release</pre></div>
</li>
<li>
<p dir="auto"><strong>Use Local LLVM Sources</strong>: CMake builds MLIR/LLVM from existing sources on
your system. The commit hash of the source must be compatible with commit
specified in <a href="https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29">cmake/IncludeLLVM.cmake</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_USE_LLVM_SOURCE_DIR=/path/to/llvm/sources"><pre>cmake -G Ninja -S <span>.</span> -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_USE_LLVM_SOURCE_DIR=/path/to/llvm/sources</pre></div>
</li>
<li>
<p dir="auto"><strong>Use Pre-built LLVM Libraries</strong>: CMake links against pre-compiled LLVM
libraries. The commit hash of the source must be compatible with commit
specified in <a href="https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29">cmake/IncludeLLVM.cmake</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_USE_LLVM_INSTALL_DIR=/path/to/llvm/install"><pre>cmake -G Ninja -S <span>.</span> -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_USE_LLVM_INSTALL_DIR=/path/to/llvm/install</pre></div>
</li>
</ol>

<p dir="auto">CUDA Tile provides Python bindings for programmatic IR manipulation (disabled by
default). To enable them, add the <code>-DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON</code> flag
to your cmake configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON"><pre>cmake -G Ninja -S <span>.</span> -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON</pre></div>
<p dir="auto">When building MLIR/LLVM from sources, MLIR Python bindings will be automatically
enabled. However, when using pre-built LLVM libraries, you must ensure they were
built with <code>-DMLIR_ENABLE_BINDINGS_PYTHON=ON</code>.</p>

<p dir="auto">To build with <code>ccache</code> enabled, add <code>-DCUDA_TILE_ENABLE_CCACHE=ON</code> to
your cmake configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -G Ninja -S . -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_ENABLE_CCACHE=ON"><pre>cmake -G Ninja -S <span>.</span> -B build \
  -DCMAKE_BUILD_TYPE=Release \
  -DCUDA_TILE_ENABLE_CCACHE=ON</pre></div>
<p dir="auto">When building LLVM from sources, this setting is automatically propagated to
the LLVM build.</p>

<p dir="auto">CUDA Tile uses LLVM&#39;s lit testing infrastructure for comprehensive testing.
Testing is enabled by default (<code>-DCUDA_TILE_ENABLE_TESTING=ON</code>). To run the test
suite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake --build build --target check-cuda-tile"><pre>cmake --build build --target check-cuda-tile</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Integrating CUDA Tile Into Your Project</h2><a id="user-content-integrating-cuda-tile-into-your-project" aria-label="Permalink: Integrating CUDA Tile Into Your Project" href="#integrating-cuda-tile-into-your-project"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">CUDA Tile can be integrated into your project in two ways, depending on your
build system and requirements.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Option 1: Using Pre-built CUDA Tile Libraries</h3><a id="user-content-option-1-using-pre-built-cuda-tile-libraries" aria-label="Permalink: Option 1: Using Pre-built CUDA Tile Libraries" href="#option-1-using-pre-built-cuda-tile-libraries"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To use pre-built CUDA Tile libraries in your project, include the necessary
headers and link against the required libraries based on your use case. For
example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="include_directories(${CUDA_TILE_INSTALL_DIR}/include)

# CUDA Tile dialect
target_link_libraries(your_target PRIVATE
  CudaTileDialect           # CUDA Tile dialect operations and types
)

# Bytecode support.
target_link_libraries(your_target PRIVATE
  CudaTileBytecodeReader    # Read bytecode format
  CudaTileBytecodeWriter    # Write bytecode format
)"><pre><span>include_directories</span>(<span>${CUDA_TILE_INSTALL_DIR}</span>/<span>include</span>)

<span># CUDA Tile dialect</span>
<span>target_link_libraries</span>(your_target <span>PRIVATE</span>
  CudaTileDialect           <span># CUDA Tile dialect operations and types</span>
)

<span># Bytecode support.</span>
<span>target_link_libraries</span>(your_target <span>PRIVATE</span>
  CudaTileBytecodeReader    <span># Read bytecode format</span>
  CudaTileBytecodeWriter    <span># Write bytecode format</span>
)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Option 2: Integrating CUDA Tile Sources</h3><a id="user-content-option-2-integrating-cuda-tile-sources" aria-label="Permalink: Option 2: Integrating CUDA Tile Sources" href="#option-2-integrating-cuda-tile-sources"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To build CUDA Tile from source as part of your project:</p>
<ol dir="auto">
<li>Integrate CUDA Tile sources into your project with CMake&#39;s FetchContent, Git
submodules, or any other integration method. Example using FetchContent:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="include(FetchContent)

# Define CUDA Tile directories
set(CUDA_TILE_SOURCE_DIR ${CMAKE_BINARY_DIR}/_deps/cuda_tile-src)
set(CUDA_TILE_BINARY_DIR ${CMAKE_BINARY_DIR}/_deps/cuda_tile-build)

FetchContent_Declare(
  cuda_tile
  GIT_REPOSITORY https://github.com/NVIDIA/cuda-tile.git
  GIT_TAG        main
  SOURCE_DIR     ${CUDA_TILE_SOURCE_DIR}
  BINARY_DIR     ${CUDA_TILE_BINARY_DIR}
)"><pre><span>include</span>(FetchContent)

<span># Define CUDA Tile directories</span>
<span>set</span>(CUDA_TILE_SOURCE_DIR <span>${CMAKE_BINARY_DIR}</span>/_deps/cuda_tile-src)
<span>set</span>(<span>CUDA_TILE_BINARY_DIR</span> <span>${CMAKE_BINARY_DIR}</span>/_deps/cuda_tile-<span>build</span>)

FetchContent_Declare(
  cuda_tile
  GIT_REPOSITORY https://github.com/NVIDIA/cuda-tile.git
  GIT_TAG        main
  SOURCE_DIR     <span>${CUDA_TILE_SOURCE_DIR}</span>
  BINARY_DIR     <span>${CUDA_TILE_BINARY_DIR}</span>
)</pre></div>
<ol start="2" dir="auto">
<li>Configure CUDA Tile build options (before calling
<code>FetchContent_MakeAvailable</code>, if using FetchContent):</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="set(CUDA_TILE_USE_LLVM_INSTALL_DIR ${YOUR_LLVM_INSTALL_DIR} CACHE PATH &#34;&#34;)
set(CUDA_TILE_ENABLE_BINDINGS_PYTHON ON CACHE BOOL &#34;&#34;)
set(CUDA_TILE_ENABLE_TESTING OFF CACHE BOOL &#34;&#34;)

FetchContent_MakeAvailable(cuda_tile)"><pre><span>set</span>(CUDA_TILE_USE_LLVM_INSTALL_DIR <span>${YOUR_LLVM_INSTALL_DIR}</span> <span>CACHE</span> <span>PATH</span> <span>&#34;&#34;</span>)
<span>set</span>(CUDA_TILE_ENABLE_BINDINGS_PYTHON <span>ON</span> <span>CACHE</span> <span>BOOL</span> <span>&#34;&#34;</span>)
<span>set</span>(CUDA_TILE_ENABLE_TESTING <span>OFF</span> <span>CACHE</span> <span>BOOL</span> <span>&#34;&#34;</span>)

FetchContent_MakeAvailable(cuda_tile)</pre></div>
<ol start="3" dir="auto">
<li>Include headers from source and build directories, then link libraries as in
Option 1:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="include_directories(${CUDA_TILE_SOURCE_DIR}/include)
include_directories(${CUDA_TILE_BINARY_DIR}/include)"><pre><span>include_directories</span>(<span>${CUDA_TILE_SOURCE_DIR}</span>/<span>include</span>)
<span>include_directories</span>(<span>${CUDA_TILE_BINARY_DIR}</span>/<span>include</span>)</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Example: Writing and Running a CUDA Tile IR Program</h2><a id="user-content-example-writing-and-running-a-cuda-tile-ir-program" aria-label="Permalink: Example: Writing and Running a CUDA Tile IR Program" href="#example-writing-and-running-a-cuda-tile-ir-program"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The following shows how to compile and run a simple Tile IR kernel that prints data from a pointer.</p>
<p dir="auto">Tile IR bytecode can be produced from an MLIR program using the <code>cuda-tile-translate</code> tool.
This can be loaded directly using the CUDA driver API, which will JIT compile the program automatically.
To compile ahead of time, you can use the <code>tileiras</code> tool from the CUDA Toolkit to compile the bytecode
into a cubin for a particular GPU target. This example shows the latter to illustrate the extra step, but the
driver launch API is the same in either case (just substitute the path to the bytecode file).</p>

<p dir="auto">This example assumes you have built the CUDA Tile IR dialect tools according to the instructions above.</p>
<p dir="auto">You will need a supported CUDA device, CUDA Toolkit 13.1+, and a compatible driver.</p>

<p dir="auto">Save the following into a file <code>example.mlir</code>.</p>
<div data-snippet-clipboard-copy-content="cuda_tile.module @example_module {
    entry @example_kernel(%data_pr : tile&lt;ptr&lt;f32&gt;&gt;) {
        print &#34;Running example module\n&#34;
        %offsets = iota : tile&lt;128xi32&gt;
        %data_ptr_reshaped = reshape %data_pr : tile&lt;ptr&lt;f32&gt;&gt; -&gt; tile&lt;1xptr&lt;f32&gt;&gt;
        %data_ptr_broadcasted = broadcast %data_ptr_reshaped : tile&lt;1xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt;
        %data_ptr_tensor = offset %data_ptr_broadcasted, %offsets : tile&lt;128xptr&lt;f32&gt;&gt;, tile&lt;128xi32&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt;
        %data, %token = load_ptr_tko weak %data_ptr_tensor : tile&lt;128xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xf32&gt;, token
        print &#34;Data: %f\n&#34;, %data : tile&lt;128xf32&gt;
        return
    }
}"><pre><code>cuda_tile.module @example_module {
    entry @example_kernel(%data_pr : tile&lt;ptr&lt;f32&gt;&gt;) {
        print &#34;Running example module\n&#34;
        %offsets = iota : tile&lt;128xi32&gt;
        %data_ptr_reshaped = reshape %data_pr : tile&lt;ptr&lt;f32&gt;&gt; -&gt; tile&lt;1xptr&lt;f32&gt;&gt;
        %data_ptr_broadcasted = broadcast %data_ptr_reshaped : tile&lt;1xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt;
        %data_ptr_tensor = offset %data_ptr_broadcasted, %offsets : tile&lt;128xptr&lt;f32&gt;&gt;, tile&lt;128xi32&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt;
        %data, %token = load_ptr_tko weak %data_ptr_tensor : tile&lt;128xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xf32&gt;, token
        print &#34;Data: %f\n&#34;, %data : tile&lt;128xf32&gt;
        return
    }
}
</code></pre></div>

<p dir="auto">Save the following into a file <code>example_host.cpp</code>.</p>
<div data-snippet-clipboard-copy-content="#include &lt;cuda.h&gt;
#include &lt;cuda_runtime_api.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

// Macro to check for errors from CUDA driver API calls.
#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    CUresult err = call;                                                       \
    if (err != CUDA_SUCCESS) {                                                 \
      const char *errStr;                                                      \
      cuGetErrorString(err, &amp;errStr);                                          \
      fprintf(stderr, &#34;CUDA error at %s:%d: %s\n&#34;, __FILE__, __LINE__,         \
              errStr);                                                         \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)

// Data tile to be passed to the kernel.
float data[] = {0,   5,   10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,
                65,  70,  75,  80,  85,  90,  95,  100, 105, 110, 115, 120, 125,
                130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190,
                195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255,
                260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320,
                325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385,
                390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450,
                455, 460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515,
                520, 525, 530, 535, 540, 545, 550, 555, 560, 565, 570, 575, 580,
                585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635};

int main() {
  // Declare and initialize CUDA driver API handles.
  CUdevice cuDevice;
  CUcontext cuContext;
  CUmodule cuModule;
  CUfunction example_kernel;
  CUstream stream;

  CUDA_CHECK(cuInit(0));
  CUDA_CHECK(cuDeviceGet(&amp;cuDevice, 0));
  CUDA_CHECK(cuCtxCreate(&amp;cuContext, NULL, 0, cuDevice));
  CUDA_CHECK(cuStreamCreate(&amp;stream, CU_STREAM_DEFAULT));

  // Load the compiled cubin file and get the entry CUDA Tile IR function.
  // CUDA Tile IR bytecode can also be directly loaded (JIT compilation).
  CUDA_CHECK(cuModuleLoad(&amp;cuModule, &#34;example.cubin&#34;));
  CUDA_CHECK(cuModuleGetFunction(&amp;example_kernel, cuModule, &#34;example_kernel&#34;));

  // Allocate memory on the device and copy the input data to it.
  CUdeviceptr data_ptr;
  CUDA_CHECK(cuMemAlloc(&amp;data_ptr, sizeof(data)));
  CUDA_CHECK(cuMemcpyHtoD(data_ptr, data, sizeof(data)));

  // Launch the kernel.
  void *kernel_args[] = {&amp;data_ptr};
  CUDA_CHECK(cuLaunchKernel(example_kernel, // function
                            1, 1, 1,        // grid dims: must be (1,1,1)
                            1, 1, 1,        // block dims
                            0,              // shared memory bytes: must be 0
                            stream,         // cuda stream
                            kernel_args,    // kernel arguments
                            NULL            // extra parameters
                            ));
  CUDA_CHECK(cuCtxSynchronize());

  // Clean up.
  CUDA_CHECK(cuModuleUnload(cuModule));
  CUDA_CHECK(cuCtxDestroy(cuContext));

  return 0;
}"><pre><code>#include &lt;cuda.h&gt;
#include &lt;cuda_runtime_api.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

// Macro to check for errors from CUDA driver API calls.
#define CUDA_CHECK(call)                                                       \
  do {                                                                         \
    CUresult err = call;                                                       \
    if (err != CUDA_SUCCESS) {                                                 \
      const char *errStr;                                                      \
      cuGetErrorString(err, &amp;errStr);                                          \
      fprintf(stderr, &#34;CUDA error at %s:%d: %s\n&#34;, __FILE__, __LINE__,         \
              errStr);                                                         \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)

// Data tile to be passed to the kernel.
float data[] = {0,   5,   10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,
                65,  70,  75,  80,  85,  90,  95,  100, 105, 110, 115, 120, 125,
                130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190,
                195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255,
                260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320,
                325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385,
                390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450,
                455, 460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515,
                520, 525, 530, 535, 540, 545, 550, 555, 560, 565, 570, 575, 580,
                585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635};

int main() {
  // Declare and initialize CUDA driver API handles.
  CUdevice cuDevice;
  CUcontext cuContext;
  CUmodule cuModule;
  CUfunction example_kernel;
  CUstream stream;

  CUDA_CHECK(cuInit(0));
  CUDA_CHECK(cuDeviceGet(&amp;cuDevice, 0));
  CUDA_CHECK(cuCtxCreate(&amp;cuContext, NULL, 0, cuDevice));
  CUDA_CHECK(cuStreamCreate(&amp;stream, CU_STREAM_DEFAULT));

  // Load the compiled cubin file and get the entry CUDA Tile IR function.
  // CUDA Tile IR bytecode can also be directly loaded (JIT compilation).
  CUDA_CHECK(cuModuleLoad(&amp;cuModule, &#34;example.cubin&#34;));
  CUDA_CHECK(cuModuleGetFunction(&amp;example_kernel, cuModule, &#34;example_kernel&#34;));

  // Allocate memory on the device and copy the input data to it.
  CUdeviceptr data_ptr;
  CUDA_CHECK(cuMemAlloc(&amp;data_ptr, sizeof(data)));
  CUDA_CHECK(cuMemcpyHtoD(data_ptr, data, sizeof(data)));

  // Launch the kernel.
  void *kernel_args[] = {&amp;data_ptr};
  CUDA_CHECK(cuLaunchKernel(example_kernel, // function
                            1, 1, 1,        // grid dims: must be (1,1,1)
                            1, 1, 1,        // block dims
                            0,              // shared memory bytes: must be 0
                            stream,         // cuda stream
                            kernel_args,    // kernel arguments
                            NULL            // extra parameters
                            ));
  CUDA_CHECK(cuCtxSynchronize());

  // Clean up.
  CUDA_CHECK(cuModuleUnload(cuModule));
  CUDA_CHECK(cuCtxDestroy(cuContext));

  return 0;
}
</code></pre></div>

<ol dir="auto">
<li>Compile the textual mlir program to CUDA Tile IR bytecode: <code>cuda-tile-translate example.mlir --bytecode-version=13.1 --mlir-to-cudatilebc --no-implicit-module -o example.tilebc</code>.</li>
<li>For AoT compilation, compile the bytecode file to a cubin: <code>tileiras --gpu-name sm_100 example.tilebc -o example.cubin</code>.
<ol dir="auto">
<li>Substitute <code>sm_100</code> with your supported target architecture.</li>
<li>To JIT compile the bytecode at launch time, skip this step and replace <code>example.cubin</code> with <code>example.tilebc</code> in <code>host_example.cpp</code>.</li>
</ol>
</li>
<li>Compile the host program: <code>g++ example_host.cpp -o example -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcuda</code>.
<ol dir="auto">
<li>Substitute <code>g++</code> with your C++ compiler, and the paths with the correct paths to your CUDA headers and libraries.</li>
</ol>
</li>
<li>Execute: <code>./example</code>.</li>
</ol>
<p dir="auto">You should see the following terminal output:</p>
<div data-snippet-clipboard-copy-content="Running example module
Data: [0.000000, 5.000000, 10.000000, 15.000000, 20.000000, 25.000000, 30.000000, 35.000000, 40.000000, 45.000000, 50.000000, 55.000000, 60.000000, 65.000000, 70.000000, 75.000000, 80.000000, 85.000000, 90.000000, 95.000000, 100.000000, 105.000000, 110.000000, 115.000000, 120.000000, 125.000000, 130.000000, 135.000000, 140.000000, 145.000000, 150.000000, 155.000000, 160.000000, 165.000000, 170.000000, 175.000000, 180.000000, 185.000000, 190.000000, 195.000000, 200.000000, 205.000000, 210.000000, 215.000000, 220.000000, 225.000000, 230.000000, 235.000000, 240.000000, 245.000000, 250.000000, 255.000000, 260.000000, 265.000000, 270.000000, 275.000000, 280.000000, 285.000000, 290.000000, 295.000000, 300.000000, 305.000000, 310.000000, 315.000000, 320.000000, 325.000000, 330.000000, 335.000000, 340.000000, 345.000000, 350.000000, 355.000000, 360.000000, 365.000000, 370.000000, 375.000000, 380.000000, 385.000000, 390.000000, 395.000000, 400.000000, 405.000000, 410.000000, 415.000000, 420.000000, 425.000000, 430.000000, 435.000000, 440.000000, 445.000000, 450.000000, 455.000000, 460.000000, 465.000000, 470.000000, 475.000000, 480.000000, 485.000000, 490.000000, 495.000000, 500.000000, 505.000000, 510.000000, 515.000000, 520.000000, 525.000000, 530.000000, 535.000000, 540.000000, 545.000000, 550.000000, 555.000000, 560.000000, 565.000000, 570.000000, 575.000000, 580.000000, 585.000000, 590.000000, 595.000000, 600.000000, 605.000000, 610.000000, 615.000000, 620.000000, 625.000000, 630.000000, 635.000000]"><pre><code>Running example module
Data: [0.000000, 5.000000, 10.000000, 15.000000, 20.000000, 25.000000, 30.000000, 35.000000, 40.000000, 45.000000, 50.000000, 55.000000, 60.000000, 65.000000, 70.000000, 75.000000, 80.000000, 85.000000, 90.000000, 95.000000, 100.000000, 105.000000, 110.000000, 115.000000, 120.000000, 125.000000, 130.000000, 135.000000, 140.000000, 145.000000, 150.000000, 155.000000, 160.000000, 165.000000, 170.000000, 175.000000, 180.000000, 185.000000, 190.000000, 195.000000, 200.000000, 205.000000, 210.000000, 215.000000, 220.000000, 225.000000, 230.000000, 235.000000, 240.000000, 245.000000, 250.000000, 255.000000, 260.000000, 265.000000, 270.000000, 275.000000, 280.000000, 285.000000, 290.000000, 295.000000, 300.000000, 305.000000, 310.000000, 315.000000, 320.000000, 325.000000, 330.000000, 335.000000, 340.000000, 345.000000, 350.000000, 355.000000, 360.000000, 365.000000, 370.000000, 375.000000, 380.000000, 385.000000, 390.000000, 395.000000, 400.000000, 405.000000, 410.000000, 415.000000, 420.000000, 425.000000, 430.000000, 435.000000, 440.000000, 445.000000, 450.000000, 455.000000, 460.000000, 465.000000, 470.000000, 475.000000, 480.000000, 485.000000, 490.000000, 495.000000, 500.000000, 505.000000, 510.000000, 515.000000, 520.000000, 525.000000, 530.000000, 535.000000, 540.000000, 545.000000, 550.000000, 555.000000, 560.000000, 565.000000, 570.000000, 575.000000, 580.000000, 585.000000, 590.000000, 595.000000, 600.000000, 605.000000, 610.000000, 615.000000, 620.000000, 625.000000, 630.000000, 635.000000]
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Contributions and Support</h2><a id="user-content-contributions-and-support" aria-label="Permalink: Contributions and Support" href="#contributions-and-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Note: We are currently not accepting external contributions.</strong></p>
<p dir="auto">While CUDA Tile is an open-source project, we are not accepting external
contributions at this time. The project is under active development with a
focused roadmap. We encourage you to use GitHub Issues to report bugs, provide
feedback, and share your experiences with CUDA Tile. Your input helps us improve
the project and prioritize future development.</p>

<p dir="auto">CUDA Tile IR is licensed under the
<a href="https://llvm.org/LICENSE.txt" rel="nofollow">Apache License v2.0 with LLVM Exceptions</a>.</p>
</article></div></div>
  </body>
</html>
