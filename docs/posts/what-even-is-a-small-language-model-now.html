<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jigsawstack.com/blog/what-even-is-a-small-language-model-now--ai">Original</a>
    <h1>What even is a small language model now?</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p><img alt="What Even Is a Small Language Model Now?" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1747294969803/b02500d6-a4b9-4950-aa11-2f0d4d37bd19.png?auto=compress,format&amp;format=webp" loading="lazy"/></p><p>If you asked someone in 2018 what a &#34;small model&#34; was, they&#39;d probably say something with a few million parameters that ran on a Raspberry Pi or your phone. Fast-forward to today, and we&#39;re calling 30B parameter models &#34;small&#34;â€”because they <em>only</em> need one GPU to run.</p>
<p>So yeah, the definition of &#34;small&#34; has changed.</p>
<h2>Small Used to Mean... Actually Small</h2>
<p>Back in the early days of machine learning, a &#34;small model&#34; might&#39;ve been a decision tree or a basic neural net that could run on a laptop CPU. Think scikit-learn, not LLMs.</p>
<p>Then came transformers and large language models (LLMs). As these got bigger and better, anything not requiring a cluster of A100s suddenly started to feel... small by comparison.</p>
<p>Today, small is more about <strong>how deployable</strong> the model is, not just its size on paper.</p>
<h2>Types of Small Models (By 2025 Standards)</h2>
<p>We now have two main flavors of small language models:</p>
<h3>1. Edge-Optimized Models</h3>
<p>These are the kind of models you can run on mobile devices or edge hardware. They&#39;re optimized for speed, low memory, and offline use.</p>
<ul role="list"><li><strong>Examples</strong>: Phi-3-mini (3.8B), Gemma 2B, TinyLlama (1.1B)</li><li><strong>Use cases</strong>: voice assistants, translation on phones, offline summarization, chatbots embedded in apps</li></ul>
<h3>2. GPU-Friendly Models</h3>
<p>These still require a GPU, but just <strong>one GPU</strong>â€”not a whole rack. In this category, even 30B or 70B models can qualify as &#34;small&#34;.</p>
<ul role="list"><li><strong>Examples</strong>: Meta Llama 3 70B (quantized), MPT-30B</li><li><strong>Use cases</strong>: internal RAG pipelines, chatbot endpoints, summarizers, code assistants</li></ul>
<p>The fact that you can now run a 70B model on a single 4090 and get decent throughput? That would&#39;ve been science fiction a few years ago.</p>
<h2>Specialization: The Real Power Move</h2>
<p>One big strength of small models is that they don&#39;t need to do everything. Unlike GPT-4 or Claude that try to be general-purpose brains, small models are often <strong>narrow and optimized</strong>.</p>
<p>That gives them a few key advantages:</p>
<ul role="list"><li><strong>They stay lean</strong> â€” no need to carry weights for tasks theyâ€™ll never do.</li><li><strong>Theyâ€™re more accurate in-domain</strong> â€” a small legal model will outperform a general-purpose LLM on legal docs.</li><li><strong>Theyâ€™re easier to fine-tune</strong> â€” less data, faster iteration.</li></ul>
<p>Small models shine when you know what you want. Think: summarizing medical records, identifying security vulnerabilities, parsing invoicesâ€”stuff that doesn&#39;t need general reasoning across the internet.</p>
<h2>30B+ Models: Still Small?</h2>
<p>Sounds weird, but yes. The bar for whatâ€™s considered &#34;small&#34; keeps shifting.</p>
<p>With the right quantization and engineering, even a 70B model can run comfortably on a high-end consumer GPU:</p>
<ul role="list"><li><strong>Llama 3.1 70B</strong> can be shrunk from 140GB (FP16) to 21GB (2-bit), running on a single 24GB VRAM card.</li><li>Throughput? ~60 tokens/sec â€” totally usable for many production workloads.</li></ul>
<p>So now we talk about models being &#34;small&#34; if theyâ€™re:</p>
<ul role="list"><li>Deployable without distributed inference</li><li>Runnable on one GPU (especially consumer-grade)</li><li>Tunable without a lab full of TPUs</li></ul>
<p>Itâ€™s less about size, more about <em>practicality</em>.</p>
<h2>Everyday Small Models: The Unsung Heroes</h2>
<p>Not all small models are new. Some of the most widely used models today have been around for years, quietly powering everyday tools we rely on.</p>
<ul role="list"><li>
<p><strong>Google Translate</strong>: Since 2006, it&#39;s been translating billions of words daily. In 2016, Google switched to a neural machine translation system, GNMT, which uses an encoder-decoder architecture with long short-term memory (LSTM) layers and attention mechanisms. This system, with over 160 million parameters, significantly improved translation fluency and accuracy.</p>
</li><li>
<p><strong>AWS Textract</strong>: This service extracts text and data from scanned documents. It&#39;s been a staple in automating document processing workflows, handling everything from invoices to medical records.</p>
</li></ul>
<p>These models may not be cutting-edge by today&#39;s standards, but they&#39;ve been instrumental in shaping the AI landscape and continue to serve millions daily.</p>
<h2>Why This Matters</h2>
<p>Small models are becoming a huge deal:</p>
<ul role="list"><li><strong>Startups</strong> can deploy LLMs without spending six figures on infra.</li><li><strong>Developers</strong> can run local models for privacy-focused apps.</li><li><strong>Enterprises</strong> can fine-tune task-specific LLMs without massive overhead.</li></ul>
<p>And when a &#34;small model&#34; can hold its own against GPT-3.5 in benchmarks? The game has officially changed.</p>
<h2>TL;DR</h2>
<ul role="list"><li>Small models used to mean tiny. Now they mean &#34;runs without drama.&#34;</li><li>Youâ€™ve got edge models, GPU-ready models, and everything in between.</li><li>Specialization is where small models shine.</li><li>30B and 70B models can be smallâ€”if theyâ€™re optimized well.</li><li>Practicality &gt; parameter count.</li></ul>
<p>In a world chasing ever-bigger models, small ones are quietly doing more with lessâ€”and that&#39;s exactly what makes them powerful.</p>
<h2>ðŸ‘¥ Join the JigsawStack Community</h2>
<p>Have questions or want to show off what youâ€™ve built? Join the JigsawStack developer community on <a href="https://discord.com/invite/dj8fMBpnqd">Discord</a> and <a href="https://x.com/jigsawstack">X/Twitter</a>. Letâ€™s build something amazing together!</p></div></div></div></div>
  </body>
</html>
