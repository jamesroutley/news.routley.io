<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/">Original</a>
    <h1>Today is when the Amazon brain drain sent AWS down the spout</h1>
    
    <div id="readability-page-1" class="page"><div id="body">
<p><span>column</span> &#34;It&#39;s always DNS&#34; is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.</p>
<p>And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who&#39;ve been to this dance before gone? And the answer increasingly is that they&#39;ve left the building — taking decades of hard-won institutional knowledge about how AWS&#39;s systems work at scale right along with them.</p>
<h3>What happened?</h3>
<p>AWS reports that on October 20, at 12:11 AM PDT, it began investigating “increased error rates and latencies for multiple AWS services in the US-EAST-1 Region.” About an hour later, at 1:26 AM, the company confirmed “significant error rates for requests made to the DynamoDB endpoint” in that region. By 2:01 AM, engineers had identified <a target="_blank" href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">DNS resolution of the DynamoDB API endpoint</a> for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a &#34;foundational service&#34; upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.</p>
<p>As a result, <a target="_blank" href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">much of the internet stopped working</a>: banking, gaming, social media, government services, buying things I don&#39;t need on Amazon.com itself, etc.</p>
<p>AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from &#34;things are breaking&#34; to &#34;we&#39;ve narrowed it down to a single service endpoint, but are still researching,&#34; which is something of a bitter pill to swallow. To be clear: I&#39;ve seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.</p>

    

<p>Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an &#34;all is well!&#34; default response. Ah well, it&#39;s not as if AWS had <a target="_blank" href="https://aws.amazon.com/message/12721/" rel="nofollow">previously called out slow outage notification times</a> as an area for improvement. <a target="_blank" href="https://aws.amazon.com/message/11201/" rel="nofollow">Multiple times</a> even. We can <a target="_blank" href="https://aws.amazon.com/message/41926/" rel="nofollow">keep doing this</a> if you&#39;d like.</p>
<h3>The prophecy</h3>
<p>AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being &#34;just another Monday outage.&#34; At AWS&#39;s scale, all of their issues are complex; this isn&#39;t going to be a simple issue that someone should have caught, just because they&#39;ve already hit similar issues years ago and ironed out the kinks in their resilience story.</p>
<p>Once you reach a certain point of scale, there are no simple problems left. What&#39;s more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I&#39;m reminded of something I had tried very hard to forget.</p>

        


        

<p>At the end of 2023, Justin Garrison left AWS and <a target="_blank" href="https://justingarrison.com/blog/2023-12-30-amazons-silent-sacking/" rel="nofollow">roasted them on his way out the door</a>. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn&#39;t slowed — and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.</p>
<p>You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it&#39;s a database), but the one thing you can&#39;t hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it has historically played a contributing role to some outages of yesteryear.</p>

        

<p>When that tribal knowledge departs, you&#39;re left having to reinvent an awful lot of in-house expertise that didn&#39;t want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn&#39;t impact your service reliability — until one day it very much does, in spectacular fashion. I suspect that day is today.</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">AWS outage exposes Achilles heel: central control plane</a></li>

<li><a href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">Major AWS outage across US-East region breaks half the internet</a></li>

<li><a href="https://www.theregister.com/2025/10/17/amazon_nuke_washington/">Amazon spills plan to nuke Washington...with X-Energy mini-reactors</a></li>

<li><a href="https://www.theregister.com/2025/10/06/amazon_007_without_golden_gun/">Amazon turns James Bond into the Man Without the Golden Gun</a></li>
</ul>
<h3>The talent drain evidence</h3>
<p>This is <em>The Register</em>, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that &#34;there is no talent exodus at AWS,&#34; a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.</p>
<ul>
<li>It is a fact that there have been <a target="_blank" href="https://www.cnbc.com/2025/07/17/amazon-web-services-has-some-layoffs.html" rel="nofollow">27,000+ Amazonians impacted by layoffs</a> between 2022 and 2024, continuing into 2025. It&#39;s hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.</li>

<li>Internal documents reportedly say that Amazon <a target="_blank" href="https://www.engadget.com/amazon-attrition-leadership-ctsmd-201800110-201800100.html" rel="nofollow">suffers from 69 percent to 81 percent regretted attrition</a> across all employment levels. In other words, &#34;people quitting who we wish didn&#39;t.&#34;</li>

<li>The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; <a target="_blank" href="https://finance.yahoo.com/news/amazon-back-office-crusade-could-090200105.html/" rel="nofollow">experts have weighed in</a> citing similar concerns.</li>
</ul>
<p>If you were one of the early employees who built these systems, the world is your oyster. There&#39;s little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.</p>
<h3>My take</h3>
<p>This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon&#39;s &#34;Frugality&#34; leadership principle meant doing more with less, not doing everything with basically nothing. AWS&#39;s operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.</p>
<p>I want to be very clear on one last point. This isn&#39;t about the technology being old. It&#39;s about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.</p>
<p>AWS will almost certainly say this was an &#34;isolated incident,&#34; but when you&#39;ve hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It&#39;s just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ®</p>                                
                    </div></div>
  </body>
</html>
