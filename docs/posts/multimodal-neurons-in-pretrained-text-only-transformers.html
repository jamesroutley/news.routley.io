<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/papers/2308.01544">Original</a>
    <h1>Multimodal Neurons in Pretrained Text-Only Transformers</h1>
    
    <div id="readability-page-1" class="page"><section><div data-props="{&#34;comments&#34;:[],&#34;primaryEmailConfirmed&#34;:false,&#34;paper&#34;:{&#34;id&#34;:&#34;2308.01544&#34;,&#34;authors&#34;:[{&#34;_id&#34;:&#34;64cc5f898a16b1748f21b910&#34;,&#34;user&#34;:{&#34;avatarUrl&#34;:&#34;/avatars/e4df93ad4a3a676b44a394967ab7fb7a.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Sarah Schwettmann&#34;,&#34;user&#34;:&#34;generish&#34;},&#34;name&#34;:&#34;Sarah Schwettmann&#34;,&#34;status&#34;:&#34;admin_assigned&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;64cc5f898a16b1748f21b911&#34;,&#34;name&#34;:&#34;Neil Chowdhury&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;64cc5f898a16b1748f21b912&#34;,&#34;name&#34;:&#34;Antonio Torralba&#34;,&#34;hidden&#34;:false}],&#34;publishedAt&#34;:&#34;2023-08-03T05:27:12.000Z&#34;,&#34;title&#34;:&#34;Multimodal Neurons in Pretrained Text-Only Transformers&#34;,&#34;summary&#34;:&#34;Language models demonstrate remarkable capacity to generalize representations\nlearned in one modality to downstream tasks in other modalities. Can we trace\nthis ability to individual neurons? We study the case where a frozen text\ntransformer is augmented with vision using a self-supervised visual encoder and\na single linear projection learned on an image-to-text task. Outputs of the\nprojection layer are not immediately decodable into language describing image\ncontent; instead, we find that translation between modalities occurs deeper\nwithin the transformer. We introduce a procedure for identifying \&#34;multimodal\nneurons\&#34; that convert visual representations into corresponding text, and\ndecoding the concepts they inject into the model&#39;s residual stream. In a series\nof experiments, we show that multimodal neurons operate on specific visual\nconcepts across inputs, and have a systematic causal effect on image\ncaptioning.&#34;,&#34;upvotes&#34;:9},&#34;canReadDatabase&#34;:false,&#34;canManageCommunity&#34;:false,&#34;hasHfLevelAccess&#34;:false,&#34;publishedOnDailyAt&#34;:&#34;2023-08-04T02:16:44.713Z&#34;,&#34;upvoted&#34;:false,&#34;upvoters&#34;:[{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675335466846-63044d41d14428368d1cac3d.png?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Alisher Amantay&#34;,&#34;user&#34;:&#34;ozyman&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Gabriele Sarti&#34;,&#34;user&#34;:&#34;gsarti&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/5b95d2509d1c7640d77a3405ebd53eaf.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Lakshmi Nair&#34;,&#34;user&#34;:&#34;lnair&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/56cc620425f65afdb16b07dd1f6f115d.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Faria&#34;,&#34;user&#34;:&#34;fhuq&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651037596712-61e7c06064d3c6c929057bee.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;蓋瑞王&#34;,&#34;user&#34;:&#34;gary109&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b1955aa8aae48db53e42c7/yfb9vWVpcaDclmKIpgGq-.png?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;af&#34;,&#34;user&#34;:&#34;ArthurFischel&#34;},{&#34;avatarUrl&#34;:&#34;https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675778487155-63d4c8ce13ae45b780792f32.jpeg?w=200&amp;h=200&amp;f=face&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Ohenenoo&#34;,&#34;user&#34;:&#34;PeepDaSlan9&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/241e6051f8fef30e76e8f4e5487c9078.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Sanyam Bhutani&#34;,&#34;user&#34;:&#34;Sanyam&#34;},{&#34;avatarUrl&#34;:&#34;/avatars/c6b7cec309634ab44f30ee1ab16597ea.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Dario Clavijo&#34;,&#34;user&#34;:&#34;daedalus2027&#34;}],&#34;acceptLanguages&#34;:[&#34;*&#34;]}" data-target="PaperContent">

<div><h2>Abstract</h2>
	<p>Language models demonstrate remarkable capacity to generalize representations
learned in one modality to downstream tasks in other modalities. Can we trace
this ability to individual neurons? We study the case where a frozen text
transformer is augmented with vision using a self-supervised visual encoder and
a single linear projection learned on an image-to-text task. Outputs of the
projection layer are not immediately decodable into language describing image
content; instead, we find that translation between modalities occurs deeper
within the transformer. We introduce a procedure for identifying &#34;multimodal
neurons&#34; that convert visual representations into corresponding text, and
decoding the concepts they inject into the model&#39;s residual stream. In a series
of experiments, we show that multimodal neurons operate on specific visual
concepts across inputs, and have a systematic causal effect on image
captioning.</p></div>





</div></section></div>
  </body>
</html>
