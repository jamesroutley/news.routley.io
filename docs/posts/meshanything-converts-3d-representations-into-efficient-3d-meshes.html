<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://buaacyw.github.io/mesh-anything/">Original</a>
    <h1>MeshAnything â€“ Converts 3D representations into efficient 3D meshes</h1>
    
    <div id="readability-page-1" class="page">

<section>
  <div>
    <div>
      <div>
        <div>
          
            

            <div>
              <p><span><sup>1</sup>S-Lab, Nanyang Technological University,</span>
              <span><sup>2</sup>Shanghai AI Lab</span></p></div>


          
        </div>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    <div>
      <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/demo_video.gif" alt="Demo Video"/>
        </p>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <div>
      <h2>
        <p><span>MeshAnything</span> mimics human artist in extracting meshes from any 3D representations. It can be combined with various 3D asset production pipelines, such as 3D reconstruction and generation, to convert their results into Artist-Created Meshes that can be seamlessly applied in 3D industry.
        </p>
      </h2>
    </div>
  </div>
</section>



<section>
  <div>
    <div>
      <h4>Comparison with Previous Methods</h4>
      
    </div>
    <div>
      <div>
        <div>
          <p><img src="https://buaacyw.github.io/mesh-anything/remesh.png" alt="Remesh Image"/></p><p>
            <span>MeshAnything</span> generates meshes with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <div>
          <p>
            Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists.
Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality.
          </p>
          <p>
            To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry.
          </p>
          <p>
            The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section>
  <div>
    

    <div>
      <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/pipline.png" alt="Pipeline Image"/>
        </p>
      </div>
    </div>

    <div>
      <div>
        <p>
           <span>MeshAnything</span> is an autoregressive transformer capable of generating Artist-Created Meshes that adhere to given 3D shapes. We sample point clouds from given 3D assets, encode them into features, and inject them into the decoder-only transformer to achieve shape-conditional mesh generation.
        </p>
        <p>
          Compared to methods like MeshGPT that directly generate Artist-Created Meshes, our approach avoids learning complex 3D shape distributions. Instead, it focuses on efficiently constructing shapes through optimized topology, significantly reducing the training burden and enhancing scalability.
        </p>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    

    <div>
      <div>
        <p><img src="https://buaacyw.github.io/mesh-anything/teaser.png" alt="Teaser Image"/>
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/add.png" alt="Additional Image"/>
        </p>
        <p><img src="https://buaacyw.github.io/mesh-anything/gt.png" alt="Ground Truth Image"/>
        </p>

        <p>
          By integrating with various 3D asset production methods, our approach achieves highly controllable Artist-Created Mesh generation. Besides, we compare our reseults with ground truth in (b) and (c). In (b), MeshAnything generates meshes with better topology and fewer faces than the ground truth. In (c), we produce meshes with a completely different topology while achieving a similar shape, proving that our method does not simply overfit but understands how to construct meshes using efficient topology.
        </p>
      </div>
    </div>
  </div>
</section>



<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@misc{chen2024meshanything,
      title={MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers},
      author={Yiwen Chen and Tong He and Di Huang and Weicai Ye and Sijin Chen and Jiaxiang Tang and Xin Chen and Zhongang Cai and Lei Yang and Gang Yu and Guosheng Lin and Chi Zhang},
      year={2024},
      eprint={2406.10163},
      archivePrefix={arXiv},
      primaryClass={id=&#39;cs.CV&#39; full_name=&#39;Computer Vision and Pattern Recognition&#39; is_active=True alt_name=None in_archive=&#39;cs&#39; is_general=False description=&#39;Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.&#39;}
}</code></pre>
  </div>
</section>






</div>
  </body>
</html>
