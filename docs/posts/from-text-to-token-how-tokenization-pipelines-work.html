<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.paradedb.com/blog/when-tokenization-becomes-token">Original</a>
    <h1>From text to token: How tokenization pipelines work</h1>
    
    <div id="readability-page-1" class="page"><div><article>
<div><div><p><img alt="James Blackwood-Sewell headshot" loading="lazy" width="200" height="200" decoding="async" data-nimg="1" srcset="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fjames_headshot.a20a5a20.jpeg&amp;w=256&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fjames_headshot.a20a5a20.jpeg&amp;w=640&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e 2x" src="https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fjames_headshot.a20a5a20.jpeg&amp;w=640&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e"/></p></div><p><span>By <!-- -->James Blackwood-Sewell<!-- --> on<!-- --> <!-- -->October 10, 2025</span></p></div>
<img alt="From Text to Token: How Tokenization Pipelines Work" loading="lazy" width="1200" height="630" decoding="async" data-nimg="1" srcset="/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fhero.b3404150.png&amp;w=1200&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fhero.b3404150.png&amp;w=3840&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e 2x" src="https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fhero.b3404150.png&amp;w=3840&amp;q=75&amp;dpl=dpl_CUvdUm5QLDEDEkxYPFrhse9czu5e"/>
<p>When you type a sentence into a search box, it’s easy to imagine the search engine seeing the same thing you do. In reality, search engines (or <a href="https://www.paradedb.com/blog/elasticsearch-acid-test">search databases</a>) don’t store blobs of text, and they don’t store sentences. They don’t even store words in the way we think of them. They dismantle input text (both indexed and query), scrub it clean, and reassemble it into something slightly more abstract and far more useful: tokens. These tokens are what you search with, and what is stored in your <a href="https://www.paradedb.com/learn/search-concepts/full-text-search#indexing">inverted indexes</a> to search over.</p>
<p>Let’s slow down and watch that pipeline in action, pausing at each stage to see how language is broken apart and remade, and how that affects results.</p>
<p>We’ll use a twist on &#34;The quick brown fox jumps over the lazy dog&#34; as our test case. It has everything that makes tokenization interesting: capitalization, punctuation, an accent, and words that change as they move through the pipeline. By the end, it’ll look different, but be perfectly prepared for search.</p>
<div><div><p>The full-text database jumped over the lazy café dog</p></div></div>

<div data-slot="callout-root" data-variant="note" role="note" aria-label="note callout"><div data-slot="callout-content"><div data-slot="callout-content-body"><p>This isn’t a complete pipeline, just a look at some of the common filters
you’ll find in lexical search systems. Different databases and search engines
expose many of these filters as composable building blocks that you can
enable, disable, or reorder to suit your needs.</p><p>The same general ideas apply whether you&#39;re using <a href="https://github.com/apache/lucene">Lucene</a>/<a href="https://github.com/elastic/elasticsearch">Elasticsearch</a>,
<a href="https://github.com/quickwit-oss/tantivy">Tantivy</a>/<a href="https://github.com/paradedb/paradedb">ParadeDB</a>, or <a href="https::/postgresql.org">Postgres</a> full-text search.</p></div></div></div>
<h2 id="filtering-text-with-case-and-character-folding">Filtering Text With Case and Character Folding</h2>
<p>Before we even think about breaking our text down we need to think about filtering out anything which isn’t useful. This usually means auditing the characters which make up our text string: transforming all letters to lower-case, and if we know we might have them folding any diacritics (like in résumé, façade, or Noël) to their base letter.</p>
<p>This step ensures that characters are normalized and consistent before tokenization begins. Café becomes cafe, and résumé becomes resume, allowing searches to match regardless of accents. Lowercasing ensures that database matches Database, though it can introduce quirks: like matching Olive (the name) with olive (the snack). Most systems accept this trade-off: false positives are better than missed results. Code search is a notable exception, since it often needs to preserve symbols and respect casing like <em>camelCase</em> or <em>PascalCase</em>.</p>
<p>Let’s take a look at how our input string is transformed. We are replacing the capital T with a lower-case one, and also folding the <code>é</code> to an <code>e</code>. Nothing too surprising here. All of these boxes are interactive, so feel free to put in your own sentences to see the results.</p>
<div><div><p>↓</p><p>lowercase &amp; fold diacritics</p></div><p>the full-text database jumped over the lazy cafe dog</p></div>
<p>Of course, there are many more filters that can be applied here, but for the sake of brevity, let’s move on.</p>
<h2 id="splitting-text-into-searchable-pieces-with-tokenization">Splitting Text Into Searchable Pieces with Tokenization</h2>
<p>The tokenization phase takes our filtered text and splits it up into indexable units. This is where we move from dealing with a sentence as a single unit to treating it as a collection of discrete, searchable parts called tokens.</p>
<p>The most common approach for English text is simple whitespace and punctuation tokenization: split on spaces and marks, and you’ve got tokens. But even this basic step has nuances: tabs, line breaks, or hyphenated words like full-text can all behave differently. Each system has its quirks, the default Lucene tokenizer turns <code>it’s</code> into <code>[it&#39;s]</code>, while the Tantivy splits into <code>[it, s]</code><sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p>
<p>Generally speaking there are three classes of tokenizers:</p>
<ol>
<li>
<p><strong>Word oriented tokenizers</strong> break text into individual words at word boundaries. This includes simple whitespace tokenizers that split on spaces, as well as more sophisticated language-aware tokenizers that understand non-English character sets<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup> . These work well for most search applications where you want to match whole words.</p>
</li>
<li>
<p><strong>Partial Word Tokenizers</strong> split words into smaller fragments, useful for matching parts of words or handling compound terms. <a href="https://en.wikipedia.org/wiki/N-gram">N-gram</a> tokenizers create overlapping character sequences, while edge n-gram tokenizers focus on prefixes or suffixes. These are powerful for autocomplete features and fuzzy matching but can create noise in search results.</p>
</li>
<li>
<p><strong>Structured Text Tokenizers</strong> are designed for specific data formats like URLs, email addresses, file paths, or structured data. They preserve meaningful delimiters and handle domain-specific patterns that would be mangled by general-purpose tokenizers. These can be essential when your content contains non-prose text that needs special handling.</p>
</li>
</ol>
<p>For our example we will be using a simple tokenizer, but you can also toggle to a trigram (an n-gram with a length of 3) tokenizer below to get a feel for how different the output would be (don&#39;t forget you can change the text in the top box to play round).</p>
<div><div><p>↓</p><p>split on whitespace and punctuation</p></div><p><span>the</span><span>full</span><span>text</span><span>database</span><span>jumped</span><span>over</span><span>the</span><span>lazy</span><span>cafe</span><span>dog</span></p></div>
<h2 id="throwing-out-filler-with-stopwords">Throwing Out Filler With Stopwords</h2>
<p>Some words carry little weight. They appear everywhere, diluting meaning: &#34;the&#34;, &#34;and&#34;, &#34;of&#34;, &#34;are&#34;. These are stopwords. Search engines often throw them out entirely<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>, betting that what remains will carry more signal.</p>
<p>This is not without risk. In <em>The Who</em>, &#34;the&#34; matters. That&#39;s why stopword lists are usually configurable<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> and not universal. In systems which support <a href="https://www.paradedb.com/learn/search-concepts/bm25">BM25</a> they are often left out altogether because the ranking formula gives less weight to very common terms, but in systems which don&#39;t support BM25 (like Postgres tsvector) stopwords are critically important.</p>
<div><p><span>full</span><span>text</span><span>database</span><span>jumped</span><span>over</span><span>lazy</span><span>cafe</span><span>dog</span></p></div>
<p>Notice how removing stopwords immediately makes our token list more focused? We&#39;ve gone from ten tokens to eight, and what remains carries more semantic weight.</p>
<h2 id="cutting-down-to-the-root-with-stemming">Cutting Down to the Root with Stemming</h2>
<p><code>Jump</code>, <code>jumps</code>, <code>jumped</code>, <code>jumping</code>. Humans see the connection instantly. Computers don&#39;t, unless we give them a way<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>.</p>
<p>Enter stemming. A stemmer is a rule-based machine that chops words down to a common core. Sometimes this happens elegantly, and sometimes it happens brutally. The foundation for most modern English stemming comes from <a href="https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf">Martin Porter&#39;s 1980 algorithm</a>, which defined the approach that gave search engines consistent rules for stripping suffixes while respecting word structure. Today many stemmers are based on the <a href="https://snowballstem.org/">Snowball</a> variant.</p>
<p>The results can look odd. <code>Database</code> becomes <code>databas,</code> <code>lazy</code> becomes <code>lazi.</code> But that&#39;s okay because stemmers don&#39;t care about aesthetics, they care about consistency. If every form of <code>lazy</code> collapses to <code>lazi,</code> the search engine can treat them as one<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup>. There&#39;s also lemmatization, which uses linguistic knowledge to convert words to their dictionary forms, but it&#39;s more complex and computationally expensive than stemming&#39;s &#34;good enough&#34; approach<sup><a href="#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="true" aria-describedby="footnote-label">7</a></sup>. </p>
<div><p><span>full</span><span>text</span><span>databas</span><span>jump</span><span>over</span><span>lazi</span><span>cafe</span><span>dog</span></p></div>
<p>Here&#39;s the final transformation: our tokens have been reduced to their essential stems. <code>Jumped</code> becomes <code>jump,</code> <code>lazy</code> becomes <code>lazi,</code> and <code>database</code> becomes <code>databas.</code> These stems might not look like real words, but they serve a crucial purpose: they&#39;re consistent. Whether someone searches for <code>jumping,</code> <code>jumped,</code> or <code>jumps,</code> they&#39;ll all reduce to <code>jump</code> and match our indexed content. This is the power of stemming: bridging the gap between the many ways humans express the same concept. </p>
<h2 id="the-final-tokens">The Final Tokens</h2>
<p>Our sentence has traveled through the complete pipeline. What started as <em>&#34;The full-text database jumped over the lazy café dog&#34;</em> has been transformed through each stage: stripped of punctuation and capitalization, split into individual words, filtered of common stopwords, and finally reduced to stems.</p>
<p>The result is a clean set of eight tokens:</p>
<!-- -->
<div><div><p><span>full</span><span>text</span><span>databas</span><span>jump</span><span>over</span><span>lazi</span><span>cafe</span><span>dog</span></p></div></div>
<!-- -->
<p>This transformation is applied to any data we store in our inverted index, and also to our queries. When someone searches for &#34;databases are jumping,&#34; that query gets tokenized: lowercased, split, stopwords removed, and stemmed. It becomes <code>databas</code> and <code>jump</code>, which will match our indexed content perfectly. </p>
<h2 id="why-tokenization-matters">Why Tokenization Matters</h2>
<p>Tokenization doesn’t get the glory. Nobody brags about their stopword filter at conferences. But it’s the quiet engine of search. Without it, <code>dogs</code> wouldn’t match <code>dog</code>, and <code>jumping</code> wouldn’t find <code>jump</code>.</p>
<p>Every search engine invests heavily here because everything else (scoring, ranking, relevance) depends on getting tokens right. It’s not glamorous, but it’s precise, and when you get this part right, everything else in search works better.</p>
<p><a href="https://www.paradedb.com">Get started with ParadeDB</a> and see how modern search databases handle tokenization for you.</p>
<hr/>
</article><!--$--><!--/$--></div></div>
  </body>
</html>
