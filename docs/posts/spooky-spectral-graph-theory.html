<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fjorn.dev/spooky-spectral-graph-theory/">Original</a>
    <h1>Spooky Spectral Graph Theory</h1>
    
    <div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2024-05-28">
            28 May, 2024
        </time>
    </i>
</p>


    <p>Table of contents</p>
<!-- toc -->
<ul>
<li><a href="#motivation">Motivation</a><ul>
<li><a href="#background">Background</a></li>
</ul>
</li>
<li><a href="#the-graph-laplacian">The Graph Laplacian</a><ul>
<li><a href="#eigenvalues">Eigenvalues</a></li>
<li><a href="#graph-properties">Graph Properties</a></li>
<li><a href="#connectivity">Connectivity</a><ul>
<li><a href="#fiedler-vector">Fiedler Vector</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#but-why-the-laplacian">But why the Laplacian?</a><ul>
<li><a href="#the-2nd-derivative">The 2nd Derivative</a></li>
<li><a href="#discretizing-the-number-line">Discretizing the Number Line</a></li>
<li><a href="#vectors-as-functions">Vectors as Functions</a></li>
<li><a href="#eigenfunctions">Eigenfunctions</a></li>
<li><a href="#change-of-change-and-connectivity">Change of Change and Connectivity</a></li>
</ul>
</li>
<li><a href="#further-reading">Further Reading</a>
</li>
</ul>
<!-- tocstop -->
<p>The following is a basic primer and introduction to spectral graph theory. I&#39;ll be using math-y words like &#34;eigenvector&#34; and &#34;graph connectivity.&#34; I would advise knowing at least these 2 words before proceeding.</p>
<h2 id="motivation">Motivation</h2><p>Despite the ghostly üëª name, &#34;spectral graph theory&#34; is just the application of linear algebra to graphs (i.e. collections of vertices and the edges between them). The eigenvalues of a matrix are often referred to as the matrix&#39;s <em>spectra</em> so you can probably infer that our goal will be to study the eigenvalues of matrices derived from graphs.</p>
<p>Usually you&#39;d motivate a new area of math with some specific problem in need of solving. Newton gave us calculus when trying to describe the motion of objects. Galois gave us Galois theory when trying to find an equation for solutions of arbitrary quintic polynomials. I will introduce spectral graph theory under the principle of &#34;just trust me, bro.&#34; It makes the final results much spookier üëª.</p>
<h3 id="background">Background</h3><p>For the remainder of this article I&#39;ll be referring exclusively to unweighted, undirected graphs and as such will be omitting those qualifiers. Given a graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow></math> its adjacency matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>A</mi><mi>G</mi></msub></mrow></math> is defined as:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>A</mi><mi>G</mi></msub><mo>=</mo><mo>(</mo><mtable><mtr><mtd><msub><mi>a</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><msub><mi>a</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>‚ãÆ</mo></mtd><mtd><mo>‚ã±</mo></mtd><mtd><mo>‚ãÆ</mo></mtd></mtr><mtr><mtd><msub><mi>a</mi><mrow><mi>n</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><msub><mi>a</mi><mrow><mi>n</mi><mo>,</mo><mi>n</mi></mrow></msub></mtd></mtr></mtable><mo>)</mo><mspace width="1em"></mspace><mtext>with</mtext><mspace width="1em"></mspace><msub><mi>a</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" fence="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mi>i</mi><mi>~</mi><mi>j</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn></mtd><mtd columnalign="left"><mi>i</mi><mo>‚âÅ</mo><mi>j</mi></mtd></mtr></mtable></mrow></mrow></math><p>Here <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi><mi>~</mi><mi>j</mi></mrow></math> means that the vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> is adjacent to the vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math>. To get an intuitive feel for this matrix imagine labeling every vertex of the graph with some index <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi><mo>‚àà</mo><mi>‚Ñï</mi></mrow></math>. Now the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>j</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow></math> entry of the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow></math> column tells you whether the vertex with label <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> is connected to the vertex with label <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math>. Such labels can be arbitrarily assigned as the core information of the graph is encoded by the connectivity between labelled vertices rather than by the labels themselves.</p>
<p>Concretely, given the following graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math></p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/fjorn-1716265963-0.svg" alt="Hi This Is Fjorn"/></p>
<p>we would write its adjacency matrix as so:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>A</mi><mi>G</mi></msub><mo>=</mo><mo>(</mo><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>)</mo></mrow></math><p>Notice we could shift all the labels clockwise by one vertex (leaving the edges in place) to create a new graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>H</mi></mrow></math>. The adjacency matrix of this new graph is actually equivalent to the old one, i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>A</mi><mi>H</mi></msub><mo>=</mo><mi>P</mi><msub><mi>A</mi><mi>G</mi></msub><msup><mi>P</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow></math> for some other 4x4 matrix P. As such, we can think of relabeling our graph as being some mechanism for changing the basis of the vector space over which the adjacency matrix acts (we&#39;ll discuss what this vector space even represents later).</p>
<p>Similarly to the adjacency matrix, we define the degree matrix of a graph as follows where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math> measures the degree of the vertex labeled <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math></p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>D</mi><mi>G</mi></msub><mo>=</mo><mo>(</mo><mtable><mtr><mtd><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd><mtd><mn>0</mn></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>‚ãÆ</mo></mtd><mtd><mo>‚ãÆ</mo></mtd><mtd><mo>‚ã±</mo></mtd><mtd><mo>‚ãÆ</mo></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>)</mo></mrow></math><h2 id="the-graph-laplacian">The Graph Laplacian</h2><p>The primary matrix of study in spectral graph theory is known as the <em>Graph Laplacian</em>. The Laplacian of a graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> is defined as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>L</mi><mi>G</mi></msub><mo>=</mo><msub><mi>D</mi><mi>G</mi></msub><mo>‚àí</mo><msub><mi>A</mi><mi>G</mi></msub></mrow></math><p>While the Laplacian matrix seems like a contrived definition, bear with me as we work through some of its useful properties to see anyone would care about it.</p>
<h3 id="eigenvalues">Eigenvalues</h3><p>Notice that the adjacency and degree matrices are, by definition, symmetric with real number entries. Therefore the Laplacian is real-symmetric too. It is a well known result of the Spectral Theorem (oooo spooky üëª) that such matrices are diagonalizable and their eigenvectors can form an orthonormal basis for the vector space over which they operate. Let&#39;s try to identify said eigenvectors and eigenvalues.</p>
<p>For a graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> over <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> vertices, given a normalized vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>u</mi><mi>T</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><msub><mi>u</mi><mn>1</mn></msub></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><msub><mi>u</mi><mi>n</mi></msub></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math> we can see that each entry <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mi>L</mi><mi>u</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub></mrow></math> of the Laplacian applied to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>u</mi></mrow></math> has the following form:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo stretchy="false">(</mo><mi>L</mi><mi>u</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><msub><mi>u</mi><mi>i</mi></msub><mo>*</mo><mi>d</mi><mi>e</mi><mi>g</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>‚àí</mo><munder><mo>‚àë</mo><mrow><mi>j</mi><mo>,</mo><mi>j</mi><mi>~</mi><mi>i</mi></mrow></munder><msub><mi>u</mi><mi>j</mi></msub><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>i</mi><mo>,</mo><mi>i</mi><mi>~</mi><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mi>u</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math><p>Plugging this into the equation for a normalized eigenvector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>u</mi></mrow></math> with eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œª</mi></mrow></math> gives us</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œª</mi><mi>u</mi><mo>=</mo><mi>L</mi><mi>u</mi></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œª</mi><mo>=</mo><msup><mi>u</mi><mi>T</mi></msup><mi>L</mi><mi>u</mi></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œª</mi><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>i</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>L</mi><mi>u</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œª</mi><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>i</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><munder><mo>‚àë</mo><mrow><mi>i</mi><mo>,</mo><mi>i</mi><mi>~</mi><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mi>u</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math><p>This nested sum can be simplified by noticing that each vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> contributes a term of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo>‚àí</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>u</mi><mi>j</mi></msub></mrow></math> for each vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> adjacent to it. Similarly an adjacent vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> would contribute a term of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msubsup><mi>u</mi><mi>j</mi><mn>2</mn></msubsup><mo>‚àí</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>u</mi><mi>j</mi></msub></mrow></math>. Combining these gives a familiar difference of squares <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mi>u</mi><mi>j</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></math>. As such we can reduce the equation for an eigenvalue to</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Œª</mi><mo>=</mo><munder><mo>‚àë</mo><mrow><mi>i</mi><mo>&lt;</mo><mi>j</mi><mo>,</mo><mi>i</mi><mi>~</mi><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mi>u</mi><mi>j</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></math><p>Some immediate consequences of this formula are:</p>
<ul>
<li>all eigenvalues are non-negative</li>
<li><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œª</mi><mo>=</mo><mn>0</mn></mrow></math> for &#34;constant&#34; vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mrow><mn mathvariant="double-struck">1</mn></mrow><mi>T</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><mn>1</mn></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math></li>
<li>Each vertex contributes some non-negative amount to the eigenvalue for every connection it has so small eigenvalues for a given vector generally imply the graph is not well connected</li>
</ul>
<h3 id="graph-properties">Graph Properties</h3><p>Ok cool, so we know some stuff about the eigenvalues of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>L</mi></mrow></math>. What do they tell us about the original graph though? Turns out they tell us a lot. A core result of spectral graph theory is that the multiplicity of the smallest eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow></math> is exactly equal to the number of connected components of the graph.</p>
<p>We can actually see this directly from our analysis above. Since we can relabel vertices arbitrarily we can write the adjacency matrix for a graph with multiple connected components in a &#34;block&#34; form. The following graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> has the following adjacency matrix:</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/fjorn-1716476458-0.svg" alt="Hi This Is Fjorn"/></p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>A</mi><mi>G</mi></msub><mo>=</mo><mo>(</mo><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>)</mo></mrow></math><p>Notice that the first 3 columns and rows of the matrix form a block representing the connectivity of the triangular connected component while the 4th and 5th rows and columns represent the connectivity of the other connected component. We can do this in general with arbitrarily many connected components.</p>
<p>In our eigenvalue formula, the only contributions that a vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> can make to an eigenvalue are due to incorporating values from connected vertices. So the &#34;semi-constant&#34; vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msubsup><mi>u</mi><mn>1</mn><mi>T</mi></msubsup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math> is an eigenvector with eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Œª</mi><mo>=</mo><mn>0</mn></mrow></math>. Similarly, the vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msubsup><mi>u</mi><mn>2</mn><mi>T</mi></msubsup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math> is also an eigenvector with eigenvalue 0. Therefore the multiplicity of the zero eigenvalue tells us the number of connected components of the graph.</p>
<h3 id="connectivity">Connectivity</h3><p>Another important property of graphs elucidated by their Laplacian eigenvalues is their connectivity. A standard measurement of general connectivity is the minimum number of edges you need to remove from a connected graph in order to break it into 2 or more connected components. We define the <em>conductance</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>œï</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></math> of a graph cut to be the ratio of the size of the cut (i.e. number of edges removed) to the number of vertices in the smaller partition.</p>
<p>Formally for a cut <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>S</mi></mrow></math> that divides a graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> into sets of vertices <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>S</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mrow><mi>S</mi></mrow><mo stretchy="true">¬Ø</mo></mover></mrow></math> such that all edges <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo>‚àÇ</mo><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="false">)</mo><mo>‚àà</mo><mi>V</mi><mi>:</mi><mi>u</mi><mo>‚àà</mo><mi>S</mi><mtext>and</mtext><mi>v</mi><mo>‚àà</mo><mover><mrow><mi>S</mi></mrow><mo stretchy="true">¬Ø</mo></mover><mo stretchy="false">}</mo></mrow></math> (edges that have one endpoint each in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>S</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mrow><mi>S</mi></mrow><mo stretchy="true">¬Ø</mo></mover></mrow></math>) are removed, its conductance is defined as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>œï</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mo>‚àÇ</mo><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo></mrow><mrow><mo>min</mo><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">|</mo><mo>,</mo><mo stretchy="false">|</mo><mover><mrow><mi>S</mi></mrow><mo stretchy="true">¬Ø</mo></mover><mo stretchy="false">|</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow></math><p>Similarly, the conductance of an entire graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> is defined as the minimum conductance of any cut of the graph.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>œï</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>min</mo><mrow><mi>S</mi><mo>‚àà</mo><mi>G</mi></mrow></munder><mi>œï</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></math><p>Another fundamental result of spectral graph theory is the Cheeger Inequality. The inequality states that for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub><mo>&gt;</mo><mn>0</mn></mrow></math> being the second-smallest eigenvalue of the Laplacian of graph <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>G</mi></mrow></math> with maximum vertex degree <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> we have</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>Œª</mi><mn>2</mn></msub><mo>‚â§</mo><mi>œï</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo><mo>‚â§</mo><msqrt><mrow><mn>2</mn><mi>d</mi><msub><mi>Œª</mi><mn>2</mn></msub></mrow></msqrt></mrow></math><p>The proof is a little too technical to dive into here but let&#39;s instead look what this inequality tells us about a graph at a high level. For small values of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> we see that the graph will have low conductance and as such can be separated by removing relatively few edges. Just like the multiplicity of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow></math> encoded information about the connected components of the graph, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> encodes information about how well connected &#34;nearly&#34; separate components are.</p>
<h4 id="fiedler-vector">Fiedler Vector</h4><p>The eigenvector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>u</mi><mn>2</mn></msub></mrow></math> for eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub><mo>&gt;</mo><mn>0</mn></mrow></math> of a graph&#39;s Laplacian also encodes connectivity information. This eigenvector, sometimes called the Fiedler Vector, will have some positive and some negative coefficients. The coefficients of matching sign actually correspond to vertices in &#34;nearly&#34; separate components that would be divided if one applied a cut whose conductance was close to the graph&#39;s overall conductance. An example may be more illustrative.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/fjorn-1716489795-0.svg" alt="Hi This Is Fjorn"/></p>
<p>The above graph will have Laplacian</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>L</mi><mo>=</mo><mo>(</mo><mtable><mtr><mtd><mn>2</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>3</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>3</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr></mtable><mo>)</mo></mrow></math><p>The second smallest eigenvalue is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub><mo>‚âà</mo><mn>0.438</mn></mrow></math> with eigenvector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msubsup><mi>u</mi><mn>2</mn><mi>T</mi></msubsup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>1</mn></mtd><mtd><mo>‚àí</mo><mn>0.561</mn></mtd><mtd><mn>3.561</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math>. Notice that the first 3 coefficients of the eigenvector are all negative while the last 3 coefficients are positive. This tells us that if we cut the graph using a cut that (nearly) minimized conductance then the vertices labeled 1-3 would form a connected component and the vertices 4-6 would form a separate connected component. Visually we can see that the lone edge between vertices 3 and 4 effectively splits the graph into 2 separate clusters, but with the Laplacian eigenvalues and eigenvectors we can detect this analytically.</p>
<h2 id="but-why-the-laplacian">But why the Laplacian?</h2><p>We&#39;ve shown the graph Laplacian to be a fairly powerful tool, but why the hell is it called the Laplacian? In continuous settings the Laplacian usually denotes something conceptually equivalent to the 2nd derivative. In fact, in single variable calculus the Laplacian is exactly equal to the 2nd derivative. How do we translate this continuous setting concept into something graph-theoretic?</p>
<h3 id="the-2nd-derivative">The 2nd Derivative</h3><p>To start, let&#39;s imagine the function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></math> over the real numbers. Its second derivative <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>f</mi><mi>‚Ä≥</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn></mrow></math> is strictly positive and thus implies that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> is <em>concave up</em> everywhere. Visually, one can intuit that concavity measures whether the graph of a function is bowed upwards like a bowl (positive concavity) or downward like a mountain.</p>
<p>Analytically, however, we can approximate concavity at a point as whether the average value of the function in a small neighborhood of the point is generally greater than or less than the value of the function at that point. In algebra we could express the concavity of a function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi></mrow></math> at some point <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi></mrow></math> as</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><mo>‚àë</mo><mrow><mi>b</mi><mo>,</mo><mo stretchy="false">|</mo><mi>b</mi><mo>‚àí</mo><mi>a</mi><mo stretchy="false">|</mo><mo>&lt;</mo><mi>œµ</mi></mrow></munder><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></math><p>We use <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></math> since we want the concavity measurement to be a smooth function itself and squaring guarantees positive values on the reals.</p>
<h3 id="discretizing-the-number-line">Discretizing the Number Line</h3><p>The above expression for concavity looks eerily similar to the expression for eigenvalues of the graph Laplacian. Let&#39;s try to turn the domain of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math>, i.e. the real number line, into a graph. The most straightforward way to do so would be to just create an infinitely long list of vertices where each is connected to exactly 2 neighbors. Maybe something like the following:</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/fjorn-1716492228-0.svg" alt="Hi This Is Fjorn"/></p>
<p>To approximate the reals we could now label each vertex with arbitrarily small real numbers such that for two connected vertices <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi><mo>&gt;</mo><mi>i</mi></mrow></math> there exists no real number <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>k</mi></mrow></math> such that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi><mo>&lt;</mo><mi>k</mi><mo>&lt;</mo><mi>j</mi></mrow></math>. This graph now looks like a pixelated version of the number line.</p>
<p>Applying <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></math> to this graph would be equivalent to squaring the label on each vertex. The concavity of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> at some vertex <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi></mrow></math> on this graph could be measured the same way as before, except instead of summing over points <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>b</mi></mrow></math> in some <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>œµ</mi></mrow></math> neighborhood of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi></mrow></math>, we sum over vertices <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>b</mi></mrow></math> that are adjacent to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi></mrow></math>.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><munder><mo>‚àë</mo><mrow><mi>b</mi><mo>,</mo><mi>b</mi><mi>~</mi><mi>a</mi></mrow></munder><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>b</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>f</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></math><p>For finer and finer labelings of the graph this will approximate the actual concavity and 2nd derivative of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> on the number line. But what would happen if we chose a different graph? While a straight-line graph does a great job at simulating the real number line, all the graphs we discussed above do not. Does the concavity or 2nd derivative calculation work for functions applied over them too?</p>
<h3 id="vectors-as-functions">Vectors as Functions</h3><p>While we traditionally think of functions as applying over the real number line (or other Euclidean spaces like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>‚Ñù</mi><mn>2</mn></msup></mrow></math>), functions can just as well apply to graphs as shown above. Now the domain would be the set of vertices, usually indexed by some discrete label, and range would be the usual outputs of the function. Since graphs can be arbitrarily connected they allow us to apply functions like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></math> to space more &#34;topologically complex&#34; than a traditional number line.</p>
<p>As discussed before, a point on the number line is only &#34;influenced&#34; by its immediate neighbors directly to its left and right. However, with arbitrary graphs a vertex can be &#34;influenced&#34; by arbitrarily many other vertices that themselves may be connected to points traditionally considered &#34;far away.&#34;</p>
<p>The graph Laplacian thus represents a sort of 2nd derivative on functions of graphs when the functions are represented as vectors onto which the Laplacian may act. The function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></math> could thus be represented by a vector like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>f</mi><mi>T</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnspacing="0.2778em"><mtr><mtd><mn>1</mn></mtd><mtd><mn>4</mn></mtd><mtd><mn>9</mn></mtd><mtd><mo>‚ãØ</mo></mtd><mtd><mn>36</mn></mtd></mtr></mtable></mstyle><mo stretchy="false">)</mo></mrow></math> when acting on a 6 vertex graph.</p>
<p>Just as the traditional derivative measures the change of a function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> as you move along the numberline, the 2nd derivative measures the change of that change. In the graph-theoretic view the vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>g</mi><mo>=</mo><mi>L</mi><mi>f</mi></mrow></math> is itself also a function, in this case the function that measures the change of the change of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>f</mi></mrow></math> as you move from vertex to vertex on the graph.</p>
<h3 id="eigenfunctions">Eigenfunctions</h3><p>An eigenvector of a matrix is a vector that is exclusively scaled (and not &#34;rotated&#34;) by the application of said matrix and its eigenvalue is the scaling factor. If the Laplacian is effectively taking the 2nd derivative of a function over a graph then its eigenvectors are <em>eigenfunctions</em> whose second derivatives are scalar multiples of themselves. This seems like an underwhelming insight on its own, but recall the Spectral Theorem from earlier: these eigenvectors form an orthonormal basis over the vector space of functions on the graph. This means we can represent <em>any</em> function on our graph as a linear combination of the Laplacian&#39;s eigenfunctions.</p>
<p>On the real number line, such functions that are multiples of their 2nd derivatives are of family <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>e</mi><mi>x</mi></msup></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>sin</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>cos</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> (which are all actually the same family for complex numbers). Thinking about these families as an orthonormal basis of the space of all functions over the real number line tells us that any function can be written as a sum of of scalar multiples of sines and cosines. Sound familiar? Such a sum is the Fourier series of a function!</p>
<p>So a graph&#39;s Laplacian eigenfunctions give us an orthonormal basis for <em>all</em> functions over the graph. This lets us find Fourier series any function over a graph. Traditional Fourier series grant us mastery over functions on real (or complex) numbers. But, as mentioned above, graphs let us think of functions over more exotic topologies where points can influence many more neighbors than usual. So the graph Laplacian gives us a starting point for mastering functions over more connected spaces too!</p>
<h3 id="change-of-change-and-connectivity">Change of Change and Connectivity</h3><p>Admittedly the above discussion was fairly &#34;hand-wavy.&#34; But the intuition still holds. Yet one question remains: if the graph Laplacian measures the change of the change of a function then how do its eigenvalues encode graph connectivity?</p>
<p>Let&#39;s start with analyzing how the zero eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow></math> describes graph connectivity. We know its multiplicity tells us the number of connected components, but let&#39;s examine it algebraically. Just like on the number line, the 2nd derivative (i.e. application of the Laplacian) of a constant is the zero function. However, on a graph vertices in disconnected components cannot &#34;influence&#34; each other so there can exist multiple orthonormal constant functions (i.e. semi-constant vectors as discussed earlier). So if a function&#39;s domain can produce orthogonal &#34;constant&#34; functions then the domain contain separated connected components.</p>
<p>The relationship between <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> and a graph&#39;s conductance is a little harder to explain. Algebraically, the eigenfunction <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>f</mi><mn>2</mn></msub></mrow></math> with eigenvalue <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> has a 2nd derivative equal to a scalar multiple of itself. Small <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> would mean the 2nd derivative is &#34;shrunken&#34; by a small multiple so let&#39;s think about it as being near 0. But if <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>f</mi><mn>2</mn></msub></mrow></math> has 2nd derivative of 0 then it must be constant?</p>
<p>Think about the <a href="#fielder-vector">Fiedler Vector example</a>. The eigenvector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Œª</mi><mn>2</mn></msub></mrow></math> actually did &#34;look&#34; constant; the coefficients of matching sign were generally near, if not equal, in value. The coefficients that corresponded to the edge whose removal separates the graph are the &#34;farthest away&#34; from the constant value of the other coefficients on their side of the graph. Just like with a truly disconnected graph, a graph with low conductance has extra dimensions of &#34;constant&#34;-like functions. The &#34;constant-ness&#34; of this extra dimension breaks down for function values on vertices near the &#34;flimsy&#34; connection.</p>
<p>Ultimately, we can parse &#34;change of change&#34; of a function as measuring the function&#39;s smoothness. On the number line smoothness at a point can only be affected by the points immediately greater or less than the point. But on a graph smoothness of a function is influenced by the graph&#39;s connectivity and the connectivity provides greater optionality when thinking about what kinds of function we consider to look &#34;constant.&#34;</p>
<h2 id="further-reading">Further Reading</h2><p>So that was the spooky tale of spectral graph theory. If you&#39;re interested in learning more I highly recommend checking out the following reading material.</p>
<ul>
<li><a href="http://www.cs.yale.edu/homes/spielman/eigs/">Dan Spielman&#39;s course on Spectral Graph Theory</a></li>
<li><a href="http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf">Dan Spielman&#39;s (draft) book on Spectral Graph Theory</a></li>
<li><a href="https://www.youtube.com/watch?v=uTUVhsxdGS8">This video for a computer scientests intro to Spectral Graph Theory</a></li>
<li><a href="https://arxiv.org/pdf/1609.08072">Bogdan Nica&#39;s intro notes to Spectral Graph Theory</a></li>
</ul>





    

    



  </div></div>
  </body>
</html>
