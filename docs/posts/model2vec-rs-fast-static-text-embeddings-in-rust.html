<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/MinishLab/model2vec-rs">Original</a>
    <h1>Show HN: Model2vec-Rs – Fast Static Text Embeddings in Rust</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
      <img width="35%" alt="Model2Vec logo" src="https://github.com/MinishLab/model2vec-rs/raw/main/assets/images/model2vec_rs_logo.png"/>
    </picture></themed-picture>
</div>

<p dir="auto">This crate provides a lightweight Rust implementation for loading and inference of <a href="https://github.com/MinishLab/model2vec">Model2Vec</a> static embedding models. For distillation and training, the <a href="https://github.com/MinishLab/model2vec">Python Model2Vec package</a> can be used.</p>

<p dir="auto">Add the crate:</p>

<p dir="auto">Make embeddings:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use anyhow::Result;
use model2vec_rs::model::StaticModel;

fn main() -&gt; Result&lt;()&gt; {
    // Load a model from the Hugging Face Hub or a local path
    // args = (repo_or_path, token, normalize, subfolder)
    let model = StaticModel::from_pretrained(&#34;minishlab/potion-base-8M&#34;, None, None, None)?;

    // Prepare a list of sentences
    let sentences = vec![
        &#34;Hello world&#34;.to_string(),
        &#34;Rust is awesome&#34;.to_string(),
    ];

    // Create embeddings
    let embeddings = model.encode(&amp;sentences);
    println!(&#34;Embeddings: {:?}&#34;, embeddings);

    Ok(())
}"><pre><span>use</span> anyhow<span>::</span><span>Result</span><span>;</span>
<span>use</span> model2vec_rs<span>::</span>model<span>::</span><span>StaticModel</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>&gt;</span> <span>{</span>
    <span>// Load a model from the Hugging Face Hub or a local path</span>
    <span>// args = (repo_or_path, token, normalize, subfolder)</span>
    <span>let</span> model = <span>StaticModel</span><span>::</span><span>from_pretrained</span><span>(</span><span>&#34;minishlab/potion-base-8M&#34;</span><span>,</span> <span>None</span><span>,</span> <span>None</span><span>,</span> <span>None</span><span>)</span>?<span>;</span>

    <span>// Prepare a list of sentences</span>
    <span>let</span> sentences = <span>vec</span><span>!</span><span>[</span>
        <span>&#34;Hello world&#34;</span><span>.</span>to_string<span>(</span><span>)</span><span>,</span>
        <span>&#34;Rust is awesome&#34;</span><span>.</span>to_string<span>(</span><span>)</span><span>,</span>
    <span>]</span><span>;</span>

    <span>// Create embeddings</span>
    <span>let</span> embeddings = model<span>.</span><span>encode</span><span>(</span><span>&amp;</span>sentences<span>)</span><span>;</span>
    <span>println</span><span>!</span><span>(</span><span>&#34;Embeddings: {:?}&#34;</span><span>,</span> embeddings<span>)</span><span>;</span>

    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto">Make embeddings with the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Single sentence
cargo run -- encode &#34;Hello world&#34; minishlab/potion-base-8M

# Multiple lines from a file
echo -e &#34;Hello world\nRust is awesome&#34; &gt; input.txt
cargo run -- encode input.txt minishlab/potion-base-8M --output embeds.json"><pre># <span>Single</span> sentence
cargo run -- encode <span>&#34;Hello world&#34;</span> minishlab/potion-base-<span>8</span><span>M</span>

# <span>Multiple</span> lines from a file
echo -e <span>&#34;Hello world<span>\n</span>Rust is awesome&#34;</span> &gt; input<span>.</span><span>txt</span>
cargo run -- encode input<span>.</span>txt <span>minishlab</span>/potion-base-<span>8</span><span>M</span> --output embeds<span>.</span><span>json</span></pre></div>
<p dir="auto">Make embeddings with custom encode args:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let embeddings = model.encode_with_args(
    &amp;sentences,     // input texts
    Some(512),  // max length
    1024,       // batch size
);"><pre><span>let</span> embeddings = model<span>.</span><span>encode_with_args</span><span>(</span>
    <span>&amp;</span>sentences<span>,</span>     <span>// input texts</span>
    <span>Some</span><span>(</span><span>512</span><span>)</span><span>,</span>  <span>// max length</span>
    <span>1024</span><span>,</span>       <span>// batch size</span>
<span>)</span><span>;</span></pre></div>

<p dir="auto">We provide a number of models that can be used out of the box. These models are available on the <a href="https://huggingface.co/collections/minishlab/model2vec-base-models-66fd9dd9b7c3b3c0f25ca90e" rel="nofollow">HuggingFace hub</a> and can be loaded using the <code>from_pretrained</code> method. The models are listed below.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Language</th>
<th>Sentence Transformer</th>
<th>Params</th>
<th>Task</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/minishlab/potion-base-32M" rel="nofollow">potion-base-32M</a></td>
<td>English</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5" rel="nofollow">bge-base-en-v1.5</a></td>
<td>32.3M</td>
<td>General</td>
</tr>
<tr>
<td><a href="https://huggingface.co/minishlab/potion-base-8M" rel="nofollow">potion-base-8M</a></td>
<td>English</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5" rel="nofollow">bge-base-en-v1.5</a></td>
<td>7.5M</td>
<td>General</td>
</tr>
<tr>
<td><a href="https://huggingface.co/minishlab/potion-base-4M" rel="nofollow">potion-base-4M</a></td>
<td>English</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5" rel="nofollow">bge-base-en-v1.5</a></td>
<td>3.7M</td>
<td>General</td>
</tr>
<tr>
<td><a href="https://huggingface.co/minishlab/potion-base-2M" rel="nofollow">potion-base-2M</a></td>
<td>English</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5" rel="nofollow">bge-base-en-v1.5</a></td>
<td>1.8M</td>
<td>General</td>
</tr>
<tr>
<td><a href="https://huggingface.co/minishlab/potion-retrieval-32M" rel="nofollow">potion-retrieval-32M</a></td>
<td>English</td>
<td><a href="https://huggingface.co/BAAI/bge-base-en-v1.5" rel="nofollow">bge-base-en-v1.5</a></td>
<td>32.3M</td>
<td>Retrieval</td>
</tr>
<tr>
<td><a href="https://huggingface.co/minishlab/M2V_multilingual_output" rel="nofollow">M2V_multilingual_output</a></td>
<td>Multilingual</td>
<td><a href="https://huggingface.co/sentence-transformers/LaBSE" rel="nofollow">LaBSE</a></td>
<td>471M</td>
<td>General</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">We compared the performance of the Rust implementation with the Python version of Model2Vec. The benchmark was run single-threaded on a CPU.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Implementation</th>
<th>Throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Rust</strong></td>
<td>8000 samples/second</td>
</tr>
<tr>
<td><strong>Python</strong></td>
<td>4650 samples/second</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The Rust version is roughly <strong>1.7×</strong> faster than the Python version.</p>

<p dir="auto">MIT</p>
</article></div></div>
  </body>
</html>
