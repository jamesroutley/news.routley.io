<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepmind.google/models/model-cards/gemini-3-1-pro/">Original</a>
    <h1>Gemini 3.1</h1>
    
    <div id="readability-page-1" class="page"><div id="page-content">
    
  
    




  

  

<section id="cover">
  <div>
    <div>
      <div>
        <div>
          
            <p><span>Published 19 February 2026</span></p>
            
          
        </div>
        
        
          
        
      </div>
      
    </div>
  </div>
</section>





  
    




  <section id="intro">
    <div>
      
      
        

<div>
  <p data-block-key="344bo">Model Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised.</p><p data-block-key="cm4bc">Published: February 2026</p>
</div>
      
    </div>
  </section>


  
    



  



  <section id="jump-links">
    
  </section>


  
    




  <section id="model-information">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Model Information</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Description</h3><p data-block-key="emvjj">Gemini 3.1 Pro is the next iteration in the Gemini 3 series of models, a suite of highly capable, natively multimodal reasoning models. As of this model card’s date of publication, Gemini 3.1 Pro is Google’s most advanced model for complex tasks. Geminin 3.1 Pro can comprehend vast datasets and challenging problems from massively multimodal information sources, including text, audio, images, video, and entire code repositories.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Model dependencies</h3><p data-block-key="fubl5">Gemini 3.1 Pro is based on Gemini 3 Pro.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Inputs</h3><p data-block-key="5emtg">Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Outputs</h3><p data-block-key="4hel8">Text, with a 64K token output.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Architecture</h3><p data-block-key="fjl15">Gemini 3.1 Pro is based on Gemini 3 Pro. For more information about the model architecture for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
    </div>
  </section>


  
    <hr/>

  
    




  <section id="model-data">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Model Data</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Training Dataset</h3><p data-block-key="9a96d">Gemini 3.1 Pro is based on Gemini 3 Pro. For more information about the training dataset for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Training Data Processing</h3><p data-block-key="5uunv">For more information about the training data processing for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
    </div>
  </section>


  
    <hr/>

  
    




  <section id="implementation-and-sustainability">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Implementation and Sustainability</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Hardware</h3><p data-block-key="er3jj">Gemini 3.1 Pro is based on Gemini 3 Pro. For more information about the hardware for Gemini 3.1 Pro and our continued<a href="https://sustainability.google/operating-sustainably/" rel="noopener" target="_blank"> commitment to operate sustainably</a>, see the Gemini 3 Pro<a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank"> model card</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Software</h3><p data-block-key="7dri8">Gemini 3.1 Pro is based on Gemini 3 Pro. For more information about the software for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
    </div>
  </section>


  
    <hr/>

  
    




  <section id="distribution">
    
  </section>


  
    <hr/>

  
    




  <section id="evaluation">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Evaluation</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Approach</h3><p data-block-key="56qeu">Gemini 3.1 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: <a href="http://deepmind.google/models/evals-methodology/gemini-3-1-pro" rel="noopener" target="_blank">deepmind.google/models/evals-methodology/gemini-3-1-pro</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Results</h3><p data-block-key="8rrpr">Gemini 3.1 Pro significantly outperforms Gemini 2.5 Pro across a range of benchmarks requiring enhanced reasoning and multimodal capabilities. Results as of February 2026 are listed below:</p>
</div>
      
    </div>
  </section>


  
    




  <section id="results">
    <div>
      
      
        

<div>
  <div>
    <table>
      <thead>
        <tr>
          <th scope="col">Benchmark</th>
          <th scope="col"><span>Notes</span></th>
          <th scope="col">
            Gemini 3.1 Pro
            <small>Thinking (High)</small>
          </th>
          <th scope="col">
            Gemini 3 Pro
            <small>Thinking (High)</small>
          </th>
          <th scope="col">
            Sonnet 4.6
            <small>Thinking (Max)</small>
          </th>
          <th scope="col">
            Opus 4.6
            <small>Thinking (Max)</small>
          </th>
          <th scope="col">
            GPT-5.2
            <small>Thinking (xhigh)</small>
          </th>
          <th scope="col">
            GPT-5.3-Codex
            <small>Thinking (xhigh)</small>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="2" scope="row">
            Humanity&#39;s Last Exam
            <small>
              Academic reasoning (full set, text + MM)
            </small>
          </th>
          <td><small>No tools</small></td>
          <td><strong>44.4%</strong></td>
          <td>37.5%</td>
          <td>33.2%</td>
          <td>40.0%</td>
          <td>34.5%</td>
          <td>—</td>
        </tr>
        <tr>
          <td>
            <small>
              Search (blocklist) + Code
            </small>
          </td>
          <td>51.4%</td>
          <td>45.8%</td>
          <td>49.0%</td>
          <td><strong>53.1%</strong></td>
          <td>45.5%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            ARC-AGI-2
            <small>Abstract reasoning puzzles</small>
          </th>
          <td><small>ARC Prize Verified</small></td>
          <td><strong>77.1%</strong></td>
          <td>31.1%</td>
          <td>58.3%</td>
          <td>68.8%</td>
          <td>52.9%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            GPQA Diamond
            <small>Scientific knowledge</small>
          </th>
          <td><small>No tools</small></td>
          <td><strong>94.3%</strong></td>
          <td>91.9%</td>
          <td>89.9%</td>
          <td>91.3%</td>
          <td>92.4%</td>
          <td>—</td>
        </tr>

        <tr>
          <th rowspan="2" scope="row">
            Terminal-Bench 2.0
            <small>Agentic terminal coding</small>
          </th>
          <td><small>Terminus-2 harness</small></td>
          <td><strong>68.5%</strong></td>
          <td>56.9%</td>
          <td>59.1%</td>
          <td>65.4%</td>
          <td>54.0%</td>
          <td>64.7%</td>
        </tr>
        <tr>
          <td>
            <small>Other best self-reported harness</small>
          </td>
          <td>—</td>
          <td>—</td>
          <td>—</td>
          <td>—</td>
          <td>62.2% <small>(Codex)</small></td>
          <td>
            <strong>77.3%</strong>
            <small>(Codex)</small>
          </td>
        </tr>

        <tr>
          <th scope="row">
            SWE-Bench Verified
            <small>Agentic coding</small>
          </th>
          <td><small>Single attempt</small></td>
          <td>80.6%</td>
          <td>76.2%</td>
          <td>79.6%</td>
          <td><strong>80.8%</strong></td>
          <td>80.0%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            SWE-Bench Pro (Public)
            <small>
              Diverse agentic coding tasks
            </small>
          </th>
          <td><small>Single attempt</small></td>
          <td>54.2%</td>
          <td>43.3%</td>
          <td>—</td>
          <td>—</td>
          <td>55.6%</td>
          <td><strong>56.8%</strong></td>
        </tr>

        <tr>
          <th scope="row">
            LiveCodeBench Pro
            <small>
              Competitive coding problems from Codeforces, ICPC, and IOI
            </small>
          </th>
          <td><small>Elo</small></td>
          <td><strong>2887</strong></td>
          <td>2439</td>
          <td>—</td>
          <td>—</td>
          <td>2393</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            SciCode
            <small>Scientific research coding</small>
          </th>
          <td></td>
          <td><strong>59%</strong></td>
          <td>56%</td>
          <td>47%</td>
          <td>52%</td>
          <td>52%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            APEX-Agents
            <small>
              Long horizon professional tasks
            </small>
          </th>
          <td><small></small></td>
          <td><strong>33.5%</strong></td>
          <td>18.4%</td>
          <td>—</td>
          <td>29.8%</td>
          <td>23.0%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            GDPval-AA Elo
            <small>Expert tasks</small>
          </th>
          <td></td>
          <td>1317</td>
          <td>1195</td>
          <td><strong>1633</strong></td>
          <td>1606</td>
          <td>1462</td>
          <td>—</td>
        </tr>

        <tr>
          <th rowspan="2" scope="row">
            τ2-bench
            <small>Agentic and tool use</small>
          </th>
          <td><small>Retail</small></td>
          <td>90.8%</td>
          <td>85.3%</td>
          <td>91.7%</td>
          <td><strong>91.9%</strong></td>
          <td>82.0%</td>
          <td>—</td>
        </tr>
        <tr>
          <td><small>Telecom</small></td>
          <td><strong>99.3%</strong></td>
          <td>98.0%</td>
          <td>97.9%</td>
          <td><strong>99.3%</strong></td>
          <td>98.7%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            MCP Atlas
            <small>
              Multi-step workflows using MCP
            </small>
          </th>
          <td></td>
          <td>69.2%</td>
          <td>54.1%</td>
          <td>61.3%</td>
          <td>59.5%</td>
          <td>60.6%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            BrowseComp
            <small>Agentic search</small>
          </th>
          <td>
            <small>Search + Python + Browse</small>
          </td>
          <td><strong>85.9%</strong></td>
          <td>59.2%</td>
          <td>74.7%</td>
          <td>84.0%</td>
          <td>65.8%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            MMMU-Pro
            <small>
              Multimodal understanding and reasoning
            </small>
          </th>
          <td><small>No tools</small></td>
          <td>80.5%</td>
          <td><strong>81.0%</strong></td>
          <td>74.5%</td>
          <td>73.9%</td>
          <td>79.5%</td>
          <td>—</td>
        </tr>

        <tr>
          <th scope="row">
            MMMLU
            <small>Multilingual Q&amp;A</small>
          </th>
          <td></td>
          <td><strong>92.6%</strong></td>
          <td>91.8%</td>
          <td>89.3%</td>
          <td>91.1%</td>
          <td>89.6%</td>
          <td>—</td>
        </tr>

        <tr>
          <th rowspan="2" scope="row">
            MRCR v2 (8-needle)
            <small>Long context performance</small>
          </th>
          <td><small>128k (average)</small></td>
          <td><strong>84.9%</strong></td>
          <td>77.0%</td>
          <td><strong>84.9%</strong></td>
          <td>84.0%</td>
          <td>83.8%</td>
          <td>—</td>
        </tr>
        <tr>
          <td>
            <small>1M (pointwise)</small>
          </td>
          <td>26.3%</td>
          <td>26.3%</td>
          <td><small>Not supported</small></td>
          <td><small>Not supported</small></td>
          <td><small>Not supported</small></td>
          <td>—</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
      
        

      
        



      
    </div>
  </section>


  
    




  <section id="intended-usage-and-limitations">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Intended Usage and Limitations</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Benefit and Intended Usage</h3><p data-block-key="2k71k">Gemini 3.1 Pro is the next iteration in the Gemini 3.0 series of models, a suite of highly intelligent and adaptive models, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require:</p><ul><li data-block-key="926ju">agentic performance</li><li data-block-key="4n01t">advanced coding</li><li data-block-key="ala1v">long context and/or multimodal understanding</li><li data-block-key="87pnn">algorithmic development</li></ul>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Known Limitations</h3><p data-block-key="8qrh9">For more information about the known limitations for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Acceptable Usage</h3><p data-block-key="65pfs">For more information about the acceptable usage for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
    </div>
  </section>


  
    <hr/>

  
    




  <section id="ethics-and-content-safety">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Ethics and Content Safety</h2>
</p>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Evaluation Approach</h3><p data-block-key="2k71k">For more information about the evaluation approach for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Safety Policies</h3><p data-block-key="2k71k">For more information about the safety policies for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="344bo">Training and Development Evaluation Results</h3><p data-block-key="2v31c">Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3.1 Pro outperforms Gemini 3.0 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red. Safety evaluations of Gemini 3.1 Pro produced results consistent with the original Gemini 3.0 Pro safety assessment.</p>
</div>
      
        

      
        

<div>
  <div>
    <table>
      <thead>
        <tr>
          <th scope="col">Evaluation<sup>1</sup></th>
          <th scope="col">
            Description
          </th>
          <th scope="col">
            Gemini 3.1 Pro</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th scope="row">
            Text to Text Safety
          </th>
          <td>Automated content safety evaluation measuring safety policies</td>
          <td>+0.10% (non-egregious)</td>
        </tr>
        <tr>
          <th scope="row">
            Multilingual Safety 
          </th>
          <td>Automated safety policy evaluation across multiple languages</td>
          <td>+0.11% (non-egregious)</td>
        </tr>
        <tr>
          <th scope="row">
            Image to Text Safety
          </th>
          <td>Automated content safety evaluation measuring safety policies</td>
          <td>-0.33%</td>
        </tr>
        <tr>
          <th scope="row">
            Tone<sup>2</sup>
          </th>
          <td>Automated evaluation measuring objective tone of model refusal</td>
          <td>+0.02%</td>
        </tr>
        <tr>
          <th scope="row">
            Unjustified-refusals
          </th>
          <td>Automated evaluation measuring model’s ability to respond to borderline prompts while remaining safe</td>
          <td>-0.08%</td>
        </tr>                        
      </tbody>
    </table>
  </div>
</div>
      
        

      
        



      
        

      
        

<div>
  <p data-block-key="4i2vs">We continue to improve our internal evaluations, including refining automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high standard of results. The performance results reported below are computed with improved evaluations and thus are not directly comparable with performance results found in previous Gemini model cards.</p><p data-block-key="h4t8">We expect variation in our automated safety evaluations results, which is why we review flagged content to check for egregious or dangerous material. Our manual review confirmed losses were overwhelmingly either a) false positives or b) not egregious.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="z38lz">Human Red Teaming Results</h3><p data-block-key="d8uos">We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3.1 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet <a href="https://blog.google/technology/safety-security/an-update-on-our-child-safety-efforts-and-commitments/" rel="noopener" target="_blank">Google’s commitments to child safety</a> across our models and Google products. For content safety policies generally, including child safety, we saw similar safety performance compared to Gemini 3.0 Pro.</p>
</div>
      
        

      
        

<div>
  <h3 data-block-key="z38lz">Risks and Mitigations</h3><p data-block-key="tr6n">For more information about the risks and mitigations for Gemini 3.1 Pro, see the Gemini 3 Pro <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" rel="noopener" target="_blank">model card</a>.</p>
</div>
      
    </div>
  </section>


  
    <hr/>

  
    




  <section id="frontier-safety">
    <div>
      
      
        

<p>
  <h2 data-block-key="344bo">Frontier Safety</h2>
</p>
      
        

      
        

<div>
  <p data-block-key="4i2vs">Our <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/strengthening-our-frontier-safety-framework/frontier-safety-framework_3.pdf" rel="noopener" target="_blank">Frontier Safety Framework</a> includes rigorous evaluations that address risks of severe harm from frontier models, covering five risk domains: CBRN (chemical, biological, radiological and nuclear information risks), cyber, harmful manipulation, machine learning R&amp;D and misalignment.</p><p data-block-key="1q5f9">Our frontier safety strategy is based on a “safety buffer” to prevent models from reaching critical capability levels (CCLs), i.e. if a frontier model does not reach the alert threshold for a CCL, we can assume models developed before the next regular testing interval will not reach that CCL. We conduct continuous testing, evaluating models at a fixed cadence and when a significant capability jump is detected. (Read more about this in our <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf" rel="noopener" target="_blank">approach to technical AGI safety.</a>)</p><p data-block-key="2ub8o">Following FSF protocols, we conducted a full evaluation of Gemini 3.1 Pro (focusing on Deep Think mode). We found that the model remains below alert thresholds for the CBRN, harmful manipulation, machine learning R&amp;D, and misalignment CCLs. As previous models passed the alert threshold for cyber, we performed more additional testing in this domain on Gemini 3.1 Pro with and without Deep Think mode, and found that the model remains below the cyber CCL.</p><p data-block-key="490m1">More details on our evaluations and the mitigations we deploy can be found in the<a href="https://deepmind.google/models/fsf-reports/gemini-3-pro/" rel="noopener" target="_blank"> Gemini 3 Pro Frontier Safety Framework Report</a>.</p>
</div>
      
    </div>
  </section>


  
    




  <section id="frontier-safety">
    <div>
      
      
        

<div>
  <div>
    <table>
      <thead>
        <tr>
          <th scope="col">Domain</th>
          <th scope="col">
            Key Results for Gemini 3.1 Pro
          </th>
          <th scope="col">
            CCL
          </th>
          <th scope="col">
            CCL reached?
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th scope="row">
            CBRN
          </th>
          <td>
            (Deep Think mode) The model can provide highly accurate and actionable information but still fails to offer novel or sufficiently complete and detailed instructions for critical stages,  to significantly enhance the capabilities of low to medium resourced threat actors required for the CCL. We continue to deploy mitigations in this domain.
          </td>
          <td>
            Uplift Level 1 
          </td>
          <td>
            CCL not reached
          </td>
        </tr>
        <tr>
          <th scope="row">
            Cyber
          </th>
          <td>
            (3.1 Pro) We conducted additional testing on the model in this domain as Gemini 3 Pro had previously reached the alert threshold. The model shows an  increase in cyber capabilities compared to Gemini 3 Pro. As with Gemini 3 Pro, the model has reached the alert threshold, but still does not reach the levels of uplift required for the CCL.
            </td>
          <td>
            Uplift Level 1 
          </td>
          <td>
            CCL not reached
          </td>
        </tr>
        <tr>
          <th scope="row">
            Harmful Manipulation
          </th>
          <td>
            (Deep Think mode) Evaluations indicated that the model showed higher manipulative efficacy for belief change metrics compared to a non-AI baseline, with the maximum odds ratio of 3.6x, which is the same as Gemini 3 Pro, and did not reach the alert threshold.  
          </td>
          <td>
            Level 1 (exploratory) 
          </td>
          <td>
            CCL not reached
          </td>
        </tr>
        <tr>
          <th scope="row">
            Machine Learning R&amp;D
          </th>
          <td>
            (Deep Think mode) The model shows gains on RE-Bench compared to Gemini 3 Pro, with a human-normalised average score of 1.27 compared to Gemini 3 Pro&#39;s score of 1.04. On one particular challenge, Optimise LLM Foundry, it scores double the human-normalised baseline score (reducing the runtime of a fine-tuning script from 300 seconds to 47 seconds, compared to the human reference solution of 94 seconds). However, the model’s average performance across all challenges remains beneath the alert threshold for the CCLs.
          </td>
          <td>
            Acceleration level 1</td>
          <td>
            CCLs not reached
          </td>
        </tr>
        <tr>
          <th scope="row">
            Misalignment (Exploratory)
          </th>
          <td>
            (Deep Think mode) On stealth evaluations, the model performs similarly to Gemini 3 Pro. On situational awareness, the model is stronger than Gemini 3 Pro: on three challenges which no other model has been able to consistently solve, max tokens, context size mod, and oversight frequency, the model achieves a success rate of almost 100%. However, its performance on other challenges is inconsistent, and thus the model does not reach the alert threshold.
          </td>
          <td>
            Instrumental Reasoning</td>
          <td>
            CCLs not reached
          </td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
      
    </div>
  </section>


  

  </div></div>
  </body>
</html>
