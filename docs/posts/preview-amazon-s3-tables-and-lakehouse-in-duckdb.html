<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://duckdb.org/2025/03/14/preview-amazon-s3-tables.html">Original</a>
    <h1>Preview: Amazon S3 Tables and Lakehouse in DuckDB</h1>
    
    <div id="readability-page-1" class="page"><div>
					<div>
						<div>
							<div>
								
								
								
									
								
								
								
								
								
									<p><em>TL;DR: We are happy to announce a new preview feature that adds support for Apache Iceberg REST Catalogs, enabling DuckDB users to connect to Amazon S3 Tables and Amazon SageMaker Lakehouse with ease.</em></p>
								
								
								<h2 id="iceberg-ahead">
        
        <a href="#iceberg-ahead">Iceberg Ahead!</a>
        
      </h2>
    

<p>In recent years, the <a href="https://iceberg.apache.org/">Iceberg open table format</a> has become increasingly popular. Major data warehouse platforms such as
<a href="https://www.databricks.com/company/newsroom/press-releases/databricks-agrees-acquire-tabular-company-founded-original-creators">Databricks</a>,
<a href="https://docs.snowflake.com/en/release-notes/2024/other/2024-10-18-snowflake-open-catalog-ga">Snowflake</a>,
<a href="https://cloud.google.com/blog/products/data-analytics/biglake-support-for-building-apache-iceberg-lakehouses-is-now-ga">Google BigQuery</a>
and
<a href="https://aws.amazon.com/blogs/big-data/read-and-write-s3-iceberg-table-using-aws-glue-iceberg-rest-catalog-from-open-source-apache-spark/">AWS</a>
have all announced or already implemented support for Iceberg tables. These platforms also support Iceberg <a href="https://iceberg.apache.org/terms/#catalog">catalogs</a>, which are responsible for tracking current metadata for a collection of Iceberg tables grouped by namespaces.</p>

<p>DuckDB has supported reading Iceberg tables <a href="https://duckdb.org/2023/09/26/announcing-duckdb-090.html">since September 2023</a> via the <a href="https://duckdb.org/docs/stable/extensions/iceberg/overview.html"><code>iceberg</code> extension</a>. Today, we are happy to introduce a new preview feature in this extension, which allows attaching to <a href="https://www.tabular.io/apache-iceberg-cookbook/getting-started-catalog-background/">Iceberg REST catalogs</a>. This preview release coincides with two AWS announcements yesterday: <a href="https://aws.amazon.com/about-aws/whats-new/2025/03/amazon-s3-tables-apache-iceberg-rest-catalog-apis/">support for Iceberg tables in Amazon S3 Tables</a> and the <a href="https://aws.amazon.com/about-aws/whats-new/2025/03/amazon-sagemaker-lakehouse-integration-s3-tables-generally-available/">GA release of the integration between S3 Tables and SageMaker Lakehouse (AWS Glue Data Catalog)</a>. In practice, these developments mean that DuckDB now provides an end-to-end solution for reading Iceberg tables in <a href="https://duckdb.org/docs/stable/extensions/iceberg/amazon_s3_tables.html">S3 Tables</a> and <a href="https://duckdb.org/docs/stable/extensions/iceberg/amazon_sagemaker_lakehouse.html">SageMaker Lakehouse</a>.</p>

<blockquote>
  <p>DuckDB&#39;s support for Iceberg REST Catalog endpoints in Amazon S3 Tables is the result of a collaboration between AWS and DuckDB Labs.</p>
</blockquote>
      <h2 id="using-apache-iceberg-rest-catalogs-in-duckdb">
        
        <a href="#using-apache-iceberg-rest-catalogs-in-duckdb">Using Apache Iceberg REST Catalogs in DuckDB</a>
        
      </h2>
    
      <h3 id="steps-for-installing">
        
        <a href="#steps-for-installing">Steps for Installing</a>
        
      </h3>
    

<p>To connect to Apache Iceberg REST Catalogs in DuckDB,
make sure you are running the <strong>latest stable</strong> DuckDB release (version 1.2.1).
For our example steps, we&#39;ll use the DuckDB <a href="https://duckdb.org/docs/stable/clients/overview.html">CLI client</a>.
You can obtain this client from the <a href="https://duckdb.org/docs/installation/">installation page</a> and start it with:</p>



<p>Next, we need to install the “bleeding edge” versions of the required extensions from the <a href="https://duckdb.org/docs/stable/extensions/installing_extensions.html#extension-repositories"><code>core_nightly</code> repository</a>.</p>

<div><div><pre><code><span>FORCE</span> <span>INSTALL</span> <span>aws</span> <span>FROM</span> <span>core_nightly</span><span>;</span>
<span>FORCE</span> <span>INSTALL</span> <span>httpfs</span> <span>FROM</span> <span>core_nightly</span><span>;</span>
<span>FORCE</span> <span>INSTALL</span> <span>iceberg</span> <span>FROM</span> <span>core_nightly</span><span>;</span>
</code></pre></div></div>

<blockquote>
  <p>For more information on using the <code>core_nightly</code> repository, please see the <a href="#footnotes">notes</a> at the end of the post.</p>
</blockquote>

<p>With these extensions installed, your DuckDB is now capable of using Apache Iceberg REST Catalogs.
Let&#39;s find some data.</p>
      <h3 id="setting-up-an-amazon-s3-table-bucket">
        
        <a href="#setting-up-an-amazon-s3-table-bucket">Setting up an Amazon S3 Table Bucket</a>
        
      </h3>
    

<p>(If you already have Iceberg tables in Amazon S3 Tables, you can skip to the <a href="#reading-amazon-s3-tables-with-duckdb">“Reading Iceberg Catalogs with DuckDB” section</a>.)</p>

<p>In this post, we demonstrate how to read data from Amazon S3 Tables.
To follow along, make sure that your account has <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-setting-up.html"><code>s3tables</code> permissions</a>
and create a new <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-buckets.html">S3 table bucket</a>.
Note that Amazon S3 Tables is currently only supported in <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-regions-quotas.html">selected AWS regions</a>.</p>
      <h3 id="populating-an-amazon-s3-table-bucket">
        
        <a href="#populating-an-amazon-s3-table-bucket">Populating an Amazon S3 Table Bucket</a>
        
      </h3>
    

<p>If you don&#39;t have an S3 table bucket with tables already, we found the easiest way to get going is to create a table using Amazon Athena.
See their <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-getting-started.html#s2-tables-tutorial-EMR-cluster">instructions</a>.
For our example, we created a simple table with three columns using the Athena query editor:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>duck_species</span> <span>(</span>
    <span>id</span> <span>INT</span><span>,</span>
    <span>english_name</span> <span>STRING</span><span>,</span>
    <span>latin_name</span> <span>STRING</span>
<span>)</span> <span>TBLPROPERTIES</span> <span>(</span><span>&#39;table_type&#39;</span> <span>=</span> <span>&#39;ICEBERG&#39;</span><span>);</span>
</code></pre></div></div>

<p>Let&#39;s insert some data to the table:</p>

<div><div><pre><code><span>INSERT</span> <span>INTO</span> <span>duck_species</span> <span>VALUES</span>
    <span>(</span><span>0</span><span>,</span> <span>&#39;Anas nivis&#39;</span><span>,</span> <span>&#39;Snow duck&#39;</span><span>);</span>
</code></pre></div></div>
      <h3 id="reading-amazon-s3-tables-with-duckdb">
        
        <a href="#reading-amazon-s3-tables-with-duckdb">Reading Amazon S3 Tables with DuckDB</a>
        
      </h3>
    

<p>Querying S3 Tables with DuckDB is really easy.
The first step is to get your AWS credentials into DuckDB.
You can achieve this in two ways.
First, you can let DuckDB detect your AWS credentials and configuration based on the default profile in your <code>~/.aws</code> directory by creating the following secret using the <a href="https://duckdb.org/docs/stable/configuration/secrets_manager.html">Secrets Manager</a>:</p>

<div><div><pre><code><span>CREATE</span> <span>SECRET</span> <span>(</span>
    <span>TYPE</span> <span>s3</span><span>,</span>
    <span>PROVIDER</span> <span>credential_chain</span>
<span>);</span>
</code></pre></div></div>

<p>Alternatively, you can set the AWS key, secret, and region values manually. For example:</p>

<div><div><pre><code><span>CREATE</span> <span>SECRET</span> <span>(</span>
    <span>TYPE</span> <span>s3</span><span>,</span>
    <span>KEY_ID</span> <span>&#39;</span><span>AKIAIOSFODNN7EXAMPLE</span><span>&#39;</span><span>,</span>
    <span>SECRET</span> <span>&#39;</span><span>wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</span><span>&#39;</span><span>,</span>
    <span>REGION</span> <span>&#39;</span><span>us-east-1</span><span>&#39;</span>
<span>);</span>
</code></pre></div></div>

<blockquote>
  <p>Tip To see the secrets in your session, run <code><span>FROM</span> <span>duckdb_secrets</span><span>();</span></code></p>
</blockquote>

<p>Next, point DuckDB to your S3 table bucket.
You can do so by copy-pasting the S3 Tables ARN value directly from the AWS Management Console and using it in the <code>ATTACH</code> command:</p>

<div><div><pre><code><span>ATTACH</span> <span>&#39;arn:aws:s3tables:</span><span>us-east-1</span><span>:</span><span>111122223333</span><span>:bucket/</span><span>bucket_name</span><span>&#39;</span>
    <span>AS</span> <span>s3_tables_db</span> <span>(</span>
        <span>TYPE</span> <span>iceberg</span><span>,</span>
        <span>ENDPOINT_TYPE</span> <span>s3_tables</span>
    <span>);</span>
</code></pre></div></div>

<p>And that&#39;s all! Now, DuckDB is connected to Amazon S3 Tables. 
To show the available tables, run:</p>



<div><div><pre><code>┌──────────────┬─────────┬───────────────┬──────────────┬──────────────┬───────────┐
│   database   │ schema  │     name      │ column_names │ column_types │ temporary │
│   varchar    │ varchar │    varchar    │  varchar[]   │  varchar[]   │  boolean  │
├──────────────┼─────────┼───────────────┼──────────────┼──────────────┼───────────┤
│ s3_tables_db │ ducks   │ duck_species  │ [__]         │ [INTEGER]    │ false     │
└──────────────┴─────────┴───────────────┴──────────────┴──────────────┴───────────┘
</code></pre></div></div>

<p>You can query tables as if they were ordinary DuckDB tables:</p>

<div><div><pre><code><span>FROM</span> <span>s3_tables_db</span><span>.</span><span>ducks</span><span>.</span><span>duck_species</span><span>;</span>
</code></pre></div></div>

<div><div><pre><code>┌───────┬──────────────┬────────────┐
│  id   │ english_name │ latin_name │
│ int32 │   varchar    │  varchar   │
├───────┼──────────────┼────────────┤
│   0   │ Anas nivis   │ Snow duck  │
└───────┴──────────────┴────────────┘
</code></pre></div></div>

<p>You also have an alternative option to connect to S3 Tables using the Amazon SageMaker Lakehouse (AWS Glue Data Catalog) Iceberg REST Catalog endpoint.
To do so, run:</p>

<div><div><pre><code><span>ATTACH</span> <span>&#39;</span><span>account_id</span><span>:s3tablescatalog/</span><span>namespace_name</span><span>&#39;</span>
<span>AS</span> <span>(</span>
    <span>TYPE</span> <span>iceberg</span><span>,</span>
    <span>ENDPOINT_TYPE</span> <span>glue</span>
<span>);</span>
</code></pre></div></div>

<blockquote>
  <p>Tip If you need basic read access to tabular data in a single S3 table bucket, use the <code>s3_tables</code> endpoint type.
If you want a unified view across all of your tabular data in AWS, use the <code>glue</code> endpoint type.</p>
</blockquote>
      <h3 id="schema-evolution">
        
        <a href="#schema-evolution">Schema Evolution</a>
        
      </h3>
    

<p>A key feature of the Iceberg format is <a href="https://iceberg.apache.org/docs/1.7.1/evolution/">schema evolution</a>,
i.e., the ability to follow changes in the table&#39;s schema.
To demonstrate this, we go back to the Athena query editor and add a new column to the <code>duck_species</code> table:</p>

<div><div><pre><code><span>ALTER</span> <span>TABLE</span> <span>duck_species</span>
    <span>ADD</span> <span>COLUMNS</span> <span>(</span><span>conservation_status</span> <span>STRING</span><span>);</span>
</code></pre></div></div>

<p>Then, we insert a few more duck species:</p>

<div><div><pre><code><span>INSERT</span> <span>INTO</span> <span>duck_species</span> <span>VALUES</span>
    <span>(</span><span>1</span><span>,</span> <span>&#39;Anas eatoni&#39;</span><span>,</span> <span>&#39;Eaton</span><span>&#39;&#39;</span><span>s pintail&#39;</span><span>,</span> <span>&#39;Vulnerable&#39;</span><span>),</span>
    <span>(</span><span>2</span><span>,</span> <span>&#39;Histrionicus histrionicus&#39;</span><span>,</span> <span>&#39;Harlequin duck&#39;</span><span>,</span> <span>&#39;Least concern&#39;</span><span>);</span>
</code></pre></div></div>

<p>Let&#39;s run the query again from DuckDB:</p>

<div><div><pre><code><span>FROM</span> <span>s3_tables_db</span><span>.</span><span>ducks</span><span>.</span><span>duck_species</span><span>;</span>
</code></pre></div></div>

<p>The query now returns a table with the additional fourth column, which has a <code>NULL</code> value in the row inserted before the change in the schema
– as expected.</p>

<div><div><pre><code>┌───────┬───────────────────────────┬─────────────────┬─────────────────────┐
│  id   │       english_name        │   latin_name    │ conservation_status │
│ int32 │          varchar          │     varchar     │       varchar       │
├───────┼───────────────────────────┼─────────────────┼─────────────────────┤
│     1 │ Anas eatoni               │ Eaton&#39;s pintail │ Vulnerable          │
│     2 │ Histrionicus histrionicus │ Harlequin duck  │ Least concern       │
│     0 │ Anas nivis                │ Snow duck       │ NULL                │
└───────┴───────────────────────────┴─────────────────┴─────────────────────┘
</code></pre></div></div>
      <h2 id="conclusion">
        
        <a href="#conclusion">Conclusion</a>
        
      </h2>
    

<p>The latest preview release of the DuckDB <code>iceberg</code> extension enables directly reading tables using Iceberg REST endpoints.
This allows you to query Amazon S3 Tables and Amazon SageMaker Lakehouse (AWS Glue Data Catalog) with ease.
As of today, the extension is in an experimental state and is under active development.
We will publish a stable release later this year.</p>
      
    
      <h3 id="cleaning-up">
        
        <a href="#cleaning-up">Cleaning Up</a>
        
      </h3>
    

<p>If you created a new S3 table bucket to follow the examples,
don&#39;t forget to clean up by <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-buckets-delete.html">deleting your S3 table bucket</a>.</p>
      <h3 id="using-the-core_nightly-repository">
        
        <a href="#using-the-core_nightly-repository">Using the <code>core_nightly</code> Repository</a>
        
      </h3>
    

<p>The extensions used in this blog post are currently experimental, and hence they are distributed through the <a href="https://duckdb.org/docs/stable/extensions/installing_extensions.html#extension-repositories"><code>core_nightly</code> repository</a>. If you want to switch back to using extensions from the <code>core</code> repository, follow the <a href="https://duckdb.org/docs/stable/extensions/installing_extensions.html#force-installing-to-upgrade-extensions">extension documentation</a>.</p>

<p>Note that DuckDB does not support reloading extensions. Therefore, if you experience any issues, try restarting DuckDB after updating the extensions.</p>

							</div>
						</div>
						
						
							
						
					</div>
				</div></div>
  </body>
</html>
