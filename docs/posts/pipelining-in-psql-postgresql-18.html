<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://postgresql.verite.pro/blog/2025/10/01/psql-pipeline.html">Original</a>
    <h1>Pipelining in psql (PostgreSQL 18)</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="what-is-pipelining-in-postgres">What is pipelining in Postgres?</h2>

<p>Pipelining is a client-side feature supported by the network protocol
that basically consists of
not waiting for the results of previously sent queries before sending the next.
This increases the throughput in two ways:</p>

<ul>
  <li>
    <p>The client, network and server can work in parallel. For instance,
the network may transmit the results of the (N-1)th query while the
server executes the Nth query and the client sends the (N+1)th query,
all this at the same time.</p>
  </li>
  <li>
    <p>The network is better utilized because successive queries can be
grouped in the same network packets, resulting in less packets
overall.</p>
  </li>
</ul>

<p>Pipelining is possible since version 7.4 (released in 2003), which introduced
the extended query protocol. But it’s only since 2021, with PostgreSQL 14,
that it can be used through libpq, the client-side C library.
Since then, some libpq-based drivers like <a href="https://www.psycopg.org/psycopg3/docs/advanced/pipeline.html">psycopg3</a> have started to support it.</p>

<p>With <strong>PostgreSQL 18</strong>, released last week, <code>psql</code>, the command line client
comes equipped with commands to use pipelining in SQL scripts, making it
even more accessible.
While this addition is not part of the highlighted features of that release,
it can provide huge gains in query throughput, as we’re going to see in a simple
test.</p>

<h2 id="psql-commands">psql commands</h2>

<p>The pipeline is started with <code>\startpipeline</code>, and in the most simple case, followed
by the SQL queries and ended with <code>\endpipeline</code>.
If intermediate results are needed, we can use <code>\syncpipeline</code> to force a 
synchronisation point and <code>\getresults</code> to fetch all results up to that point.
Also, starting a pipeline creates an implicit transaction. If a query fails,
all the changes since the start (or before the last synchronization point) will
be rolled back.</p>

<p>If you know about the <a href="https://www.postgresql.org/docs/current/app-psql.html#APP-PSQL-META-COMMAND-SEMICOLON"><code>\;</code> syntax</a> to group several queries in the same request, there are similarities between this technique and pipelining: they’re both used to reduce server round-trips and have the same semantics with regard to transactions. In a way, pipelining is the evolution in the <a href="https://www.postgresql.org/docs/current/protocol-flow.html#PROTOCOL-FLOW-EXT-QUERY">extended query protocol</a> of what multi-statement queries (<code>\;</code> in psql) are in the <a href="https://www.postgresql.org/docs/current/protocol-flow.html#PROTOCOL-FLOW-SIMPLE-QUERY">simple query protocol</a>.</p>

<h2 id="performance-test">Performance test</h2>

<p>Let’s do a simple test where data from devices are imported
with <code>INSERT ... ON CONFLICT</code> queries. Same-device same-date does update the row,
otherwise it inserts a new row.
Note that if we wanted to unconditionally append all rows,
<code>COPY</code> would be preferable and pipelining not necessary, which is why
the more sophisticated insert-or-update is chosen for that test.</p>

<p>The following bash code imports the (random) data, with or without the pipelining
depending on a parameter.</p>

<div><div><pre><code><span>function </span>import_data
<span>{</span>
  <span>local </span><span>count</span><span>=</span><span>$1</span>  <span># how many rows?</span>
  <span>local </span><span>pipeline</span><span>=</span><span>$2</span> <span># 1 or 0</span>
  <span>local </span><span>now_ts</span><span>=</span><span>$(</span><span>date</span> +%s<span>)</span>

  <span>(</span>
    <span>echo</span> <span>&#39;PREPARE s AS insert into events(device, recorded_at, measure)
values($1, to_timestamp($2), $3) on conflict(device,recorded_at) do update set measure=excluded.measure;&#39;</span>
    <span>echo</span> <span>&#34;BEGIN;&#34;</span>
    <span>[[</span> <span>$pipeline</span> <span>=</span> 1 <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>&#34;</span><span>\\</span><span>startpipeline&#34;</span>
    <span>for </span>i <span>in</span> <span>$(</span><span>seq </span>1 <span>$count</span><span>)</span>
    <span>do
      </span><span>device</span><span>=</span><span>$RANDOM</span>
      <span>secs</span><span>=</span><span>$((</span><span>$now_ts</span> <span>+</span> <span>$RANDOM</span><span>*</span><span>50</span><span>))</span>
      <span>measure</span><span>=</span><span>${</span><span>RANDOM</span><span>}</span><span>&#34;.&#34;</span><span>${</span><span>RANDOM</span><span>}</span>
      <span>echo</span> <span>&#34;execute s(</span><span>$device</span><span>, &#39;</span><span>$secs</span><span>&#39;, </span><span>$measure</span><span>);&#34;</span>
    <span>done</span>
    <span>[[</span> <span>$pipeline</span> <span>=</span> 1 <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>&#34;</span><span>\\</span><span>endpipeline&#34;</span>
    <span>echo</span> <span>&#34;COMMIT;&#34;</span>
  <span>)</span> | <span>$psql</span> <span>-q</span> <span>-v</span> <span>ON_ERROR_STOP</span><span>=</span>1
<span>}</span>
</code></pre></div></div>
<p>Let’s try this with batches of 100, 1000, 5000, 10000, 50000, 100000 rows, with and
without pipelining, and compare how fast these batches are processed.</p>

<p>Also, since the network speed matters a lot here, let’s try with three
typical kinds of network connections:</p>
<ul>
  <li>localhost (ping time ~ 0.04ms): client and server are on the same host.</li>
  <li>LAN (ping time ~ 1ms): client and server are separated only by an Ethernet 1GB/s switch.</li>
  <li>WAN (ping time ~ 4ms): the server is reached through a public Internet connection.</li>
</ul>

<p>Finally, each case is run 5 times and we keep only the median time of the runs.</p>

<p><img src="https://postgresql.verite.pro/blog/assets/img/pipeline-psql-localhost-graph.png" alt="graph localhost"/></p>

<p>On the same host, the pipeline acceleration ranges from 1.5x for the smallest batch size, up to 5x.</p>

<p><img src="https://postgresql.verite.pro/blog/assets/img/pipeline-psql-lan-graph.png" alt="graph LAN"/></p>

<p>On a local network connection, the smallest batch size is accelerated by 2.6x, and it goes up to 42x with the bigger sizes.</p>

<p><img src="https://postgresql.verite.pro/blog/assets/img/pipeline-psql-wan-graph.png" alt="psql WAN"/></p>

<p>On the slowest network, it’s even more impressive. The acceleration is between 5.4x and 71x !</p>

<h2 id="conclusion">Conclusion</h2>

<p>These accelerations show how under-utilized the network is when we
send batches of small queries without pipelining: the network packets
are like 50 seater buses that ride with only one passenger.</p>

<p>In our example, all we have to do to optimize on that front
is to add a pair of <code>\startpipeline</code> and <code>\endpipeline</code>.
That’s because our queries do not depend on the results of previous queries
of the same batch, except in the sense that if one fails, the entire batch fails.</p>

<p>Without pipelining, we could still optimize our test by adding many
rows to the <code>VALUES</code> clauses for each query instead of one row per
query. But it’s not easy to find the sweet spot for how many data rows
there needs to be per query, and large queries with thousands of parameters
are not the nicest to handle on the server side.
Also, if the client-side logic is more complicated, for instance
conditionally targeting several tables, running simple statements in
a pipeline while using row-by-row logic might be much easier.</p>

<p>The pipelining meta-commands were added in psql version 18, but they do not
require PostgreSQL 18 on the server side. For those interested in this feature
who can’t upgrade their server soon, you can still upgrade to the latest
version of psql: it’s backward-compatible as much as possible.</p>

  </div>

    


</article>

      </div>
    </div></div>
  </body>
</html>
