<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/karthink/gptel">Original</a>
    <h1>gptel: a simple LLM client for Emacs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://elpa.nongnu.org/nongnu/gptel.svg" rel="nofollow"><img src="https://camo.githubusercontent.com/4b6f2590c77fa98a45d56fbea0332e8d9844f89a8524ad078621a9c638e6b523/68747470733a2f2f656c70612e6e6f6e676e752e6f72672f6e6f6e676e752f677074656c2e737667" alt="https://elpa.nongnu.org/nongnu/gptel.svg" data-canonical-src="https://elpa.nongnu.org/nongnu/gptel.svg"/></a> <a href="https://stable.melpa.org/packages/gptel-badge.svg" rel="nofollow"><img src="https://camo.githubusercontent.com/c595cbac5099d30bb5ce893a701f6bbcd5b99948bca3ce42201938b5ab501845/68747470733a2f2f737461626c652e6d656c70612e6f72672f7061636b616765732f677074656c2d62616467652e737667" alt="https://stable.melpa.org/packages/gptel-badge.svg" data-canonical-src="https://stable.melpa.org/packages/gptel-badge.svg"/></a> <a href="https://melpa.org/#/gptel" rel="nofollow"><img src="https://camo.githubusercontent.com/c607e4545b94060506a5b0f88e06c62839e09a256a49e8ed68780706a411fa99/68747470733a2f2f6d656c70612e6f72672f7061636b616765732f677074656c2d62616467652e737667" alt="https://melpa.org/packages/gptel-badge.svg" data-canonical-src="https://melpa.org/packages/gptel-badge.svg"/></a></p>
<p dir="auto">gptel is a simple Large Language Model chat client for Emacs, with support for multiple models and backends.  It works in the spirit of Emacs, available at any time and uniformly in any buffer.</p>

<p dir="auto"><b>General usage</b>: (<a href="https://www.youtube.com/watch?v=bsRnh_brggM" rel="nofollow">YouTube Demo</a>)</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description intro-demo.mp4">intro-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxMi04NjUxMGEwOS1hMmZiLTRjYmQtYjUzZi1jYzI1MjJkMDVhMTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcxNmYwN2M4ZDViZjZmMjJiMjVmMmRiOGVmNDNmMmQxY2ZiNDE5MWQ1MmMzNzE1MmQxNGRhZjc3N2EwMzcxNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.wb0ZEjoH43OMeQchOUbNS54NjoqRicH6PkbYpsK7Apc" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/230516812-86510a09-a2fb-4cbd-b53f-cc2522d05a13.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxMi04NjUxMGEwOS1hMmZiLTRjYmQtYjUzZi1jYzI1MjJkMDVhMTMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcxNmYwN2M4ZDViZjZmMjJiMjVmMmRiOGVmNDNmMmQxY2ZiNDE5MWQ1MmMzNzE1MmQxNGRhZjc3N2EwMzcxNyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.wb0ZEjoH43OMeQchOUbNS54NjoqRicH6PkbYpsK7Apc" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description intro-demo-2.mp4">intro-demo-2.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxNi1hZTRhNjEzYS00ZDAxLTQwNzMtYWQzZi1iNjZmYTczYzZlNDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzMyODM4NTk5YTIzYmM2OTVlOWNmMDQ5ZjQ4OTlkOWIyYTE0MThkZjJlM2Q3ZWQxNzYyMmI4MDM0Zjk1MDA3YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gfg8bMfFRAG32aljPxL1L9FjyrsyRm9dG9pUS3ojuHw" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/230516816-ae4a613a-4d01-4073-ad3f-b66fa73c6e45.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzIzMDUxNjgxNi1hZTRhNjEzYS00ZDAxLTQwNzMtYWQzZi1iNjZmYTczYzZlNDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NzMyODM4NTk5YTIzYmM2OTVlOWNmMDQ5ZjQ4OTlkOWIyYTE0MThkZjJlM2Q3ZWQxNzYyMmI4MDM0Zjk1MDA3YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.gfg8bMfFRAG32aljPxL1L9FjyrsyRm9dG9pUS3ojuHw" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><b>Media support</b></p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description gptel-image-demo-1.mp4">gptel-image-demo-1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/375931228-1fd947e1-226b-4be2-bc68-7b22b2e3215f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM3NTkzMTIyOC0xZmQ5NDdlMS0yMjZiLTRiZTItYmM2OC03YjIyYjJlMzIxNWYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTE0YTJjYWIyYjBjYmZjMTRhYThhMjdmMDhiZjU1ZTg0NGU5NWNmNTJjZjkyYTMxNjgzMmVhMWYyN2E4MmJmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.6zCl44DaK9XH9WhnpDeBDmR6KAklh8yOZdXLYuqmZuM" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/375931228-1fd947e1-226b-4be2-bc68-7b22b2e3215f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM3NTkzMTIyOC0xZmQ5NDdlMS0yMjZiLTRiZTItYmM2OC03YjIyYjJlMzIxNWYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTE0YTJjYWIyYjBjYmZjMTRhYThhMjdmMDhiZjU1ZTg0NGU5NWNmNTJjZjkyYTMxNjgzMmVhMWYyN2E4MmJmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.6zCl44DaK9XH9WhnpDeBDmR6KAklh8yOZdXLYuqmZuM" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><b>Multi-LLM support demo</b>:</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description gptel-multi.mp4">gptel-multi.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI3ODg1NDAyNC1hZTEzMzZjNC01Yjg3LTQxZjItODNlOS1lNDE1MzQ5ZDZhNDMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yzk5OTY1NGNlYWVlN2ZlYzc3NjVkNzhkYTIyNTEzMTM1NzUzNTU2MmFjMjBiZGRjNGY3YmM3ZjQ5NTg0OGU0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.K6QMjOHQE7-2Ymen-gSO3HPkZe_bkRwz874CguljtVw" data-canonical-src="https://private-user-images.githubusercontent.com/8607532/278854024-ae1336c4-5b87-41f2-83e9-e415349d6a43.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI3ODg1NDAyNC1hZTEzMzZjNC01Yjg3LTQxZjItODNlOS1lNDE1MzQ5ZDZhNDMubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yzk5OTY1NGNlYWVlN2ZlYzc3NjVkNzhkYTIyNTEzMTM1NzUzNTU2MmFjMjBiZGRjNGY3YmM3ZjQ5NTg0OGU0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.K6QMjOHQE7-2Ymen-gSO3HPkZe_bkRwz874CguljtVw" controls="controls" muted="muted">

  </video>
</details>

<ul dir="auto">
  <li>It’s async and fast, streams responses.</li>
  <li>Interact with LLMs from anywhere in Emacs (any buffer, shell, minibuffer, wherever)</li>
  <li>LLM responses are in Markdown or Org markup.</li>
  <li>Supports multiple independent conversations and one-off ad hoc interactions.</li>
  <li>Supports multi-modal models (include images, documents)</li>
  <li>Save chats as regular Markdown/Org/Text files and resume them later.</li>
  <li>You can go back and edit your previous prompts or LLM responses when continuing a conversation. These will be fed back to the model.</li>
  <li>Don’t like gptel’s workflow? Use it to create your own for any supported model/backend with a <a href="https://github.com/karthink/gptel/wiki/Defining-custom-gptel-commands">simple API</a>.</li>
</ul>
<p dir="auto">gptel uses Curl if available, but falls back to url-retrieve to work without external dependencies.</p>

<ul dir="auto">
  <li><a href="#breaking-changes">Breaking changes!</a></li>
  <li><a href="#installation">Installation</a>
    <ul dir="auto">
      <li><a href="#straight">Straight</a></li>
      <li><a href="#manual">Manual</a></li>
      <li><a href="#doom-emacs">Doom Emacs</a></li>
      <li><a href="#spacemacs">Spacemacs</a></li>
    </ul>
  </li>
  <li><a href="#setup">Setup</a>
    <ul dir="auto">
      <li><a href="#chatgpt">ChatGPT</a></li>
      <li><a href="#other-llm-backends">Other LLM backends</a>
        <ul dir="auto">
          <li><a href="#azure">Azure</a></li>
          <li><a href="#gpt4all">GPT4All</a></li>
          <li><a href="#ollama">Ollama</a></li>
          <li><a href="#gemini">Gemini</a></li>
          <li><a href="#llamacpp-or-llamafile">Llama.cpp or Llamafile</a></li>
          <li><a href="#kagi-fastgpt--summarizer">Kagi (FastGPT &amp; Summarizer)</a></li>
          <li><a href="#togetherai">together.ai</a></li>
          <li><a href="#anyscale">Anyscale</a></li>
          <li><a href="#perplexity">Perplexity</a></li>
          <li><a href="#anthropic-claude">Anthropic (Claude)</a></li>
          <li><a href="#groq">Groq</a></li>
          <li><a href="#openrouter">OpenRouter</a></li>
          <li><a href="#privategpt">PrivateGPT</a></li>
          <li><a href="#deepseek">DeepSeek</a></li>
          <li><a href="#cerebras">Cerebras</a></li>
          <li><a href="#github-models">Github Models</a></li>
          <li><a href="#novita-ai">Novita AI</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#usage">Usage</a>
    <ul dir="auto">
      <li><a href="#in-any-buffer">In any buffer:</a></li>
      <li><a href="#in-a-dedicated-chat-buffer">In a dedicated chat buffer:</a>
        <ul dir="auto">
          <li><a href="#including-media-images-documents-with-requests">Including media (images, documents) with requests</a></li>
          <li><a href="#save-and-restore-your-chat-sessions">Save and restore your chat sessions</a></li>
        </ul>
      </li>
      <li><a href="#include-more-context-with-requests">Include more context with requests</a></li>
      <li><a href="#rewrite-refactor-or-fill-in-a-region">Rewrite, refactor or fill in a region</a></li>
      <li><a href="#extra-org-mode-conveniences">Extra Org mode conveniences</a></li>
    </ul>
  </li>
  <li><a href="#faq">FAQ</a>
    <ul dir="auto">
      <li><a href="#i-want-the-window-to-scroll-automatically-as-the-response-is-inserted">I want the window to scroll automatically as the response is inserted</a></li>
      <li><a href="#i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted">I want the cursor to move to the next prompt after the response is inserted</a></li>
      <li><a href="#i-want-to-change-the-formatting-of-the-prompt-and-llm-response">I want to change the formatting of the prompt and LLM response</a></li>
      <li><a href="#i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once">I want the transient menu options to be saved so I only need to set them once</a></li>
      <li><a href="#i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu">I want to use gptel in a way that’s not supported by <code>gptel-send</code> or the options menu</a></li>
      <li><a href="#doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode">(Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode</a></li>
      <li><a href="#chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota">(ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”</a></li>
      <li><a href="#why-another-llm-client">Why another LLM client?</a></li>
    </ul>
  </li>
  <li><a href="#additional-configuration">Additional Configuration</a></li>
  <li><a href="#alternatives">Alternatives</a>
    <ul dir="auto">
      <li><a href="#packages-using-gptel">Packages using gptel</a></li>
    </ul>
  </li>
  <li><a href="#acknowledgments">Acknowledgments</a></li>
</ul>

<ul dir="auto">
  <li><code>gptel-model</code> is now expected to be a symbol, not a string.  Please update your configuration.</li>
</ul>

<p dir="auto">gptel can be installed in Emacs out of the box with <code>M-x package-install</code> ⏎ <code>gptel</code>.  This installs the latest commit.</p>
<p dir="auto">If you want the stable version instead, add NonGNU-devel ELPA or MELPA-stable to your list of package sources (<code>package-archives</code>), then install gptel with <code>M-x package-install⏎</code> <code>gptel</code> from these sources.</p>
<p dir="auto">(Optional: Install <code>markdown-mode</code>.)</p>
<details><summary>

</summary>
<div dir="auto" data-snippet-clipboard-copy-content="(straight-use-package &#39;gptel)"><pre>(<span>straight-use-package</span> <span>&#39;gptel</span>)</pre></div>
<p dir="auto">Installing the <code>markdown-mode</code> package is optional.</p>
</details>
<details><summary>

</summary>
<p dir="auto">Clone or download this repository and run <code>M-x package-install-file⏎</code> on the repository directory.</p>
<p dir="auto">Installing the <code>markdown-mode</code> package is optional.</p>
</details>
<details><summary>

</summary>
<p dir="auto">In <code>packages.el</code></p>

<p dir="auto">In <code>config.el</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="(use-package! gptel
 :config
 (setq! gptel-api-key &#34;your key&#34;))"><pre>(use-package! gptel
 <span>:config</span>
 (setq! gptel-api-key <span><span>&#34;</span>your key<span>&#34;</span></span>))</pre></div>
<p dir="auto">“your key” can be the API key itself, or (safer) a function that returns the key.  Setting <code>gptel-api-key</code> is optional, you will be asked for a key if it’s not found.</p>
</details>
<details><summary>

</summary>
<p dir="auto">In your <code>.spacemacs</code> file, add <code>llm-client</code> to <code>dotspacemacs-configuration-layers</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(llm-client :variables
            llm-client-enable-gptel t)"><pre>(llm-client <span>:variables</span>
            llm-client-enable-gptel <span>t</span>)</pre></div>
</details>


<p dir="auto">Procure an <a href="https://platform.openai.com/account/api-keys" rel="nofollow">OpenAI API key</a>.</p>
<p dir="auto">Optional: Set <code>gptel-api-key</code> to the key. Alternatively, you may choose a more secure method such as:</p>
<ul dir="auto">
  <li>Storing in <code>~/.authinfo</code>. By default, “api.openai.com” is used as HOST and “apikey” as USER.
    <pre lang="authinfo">machine api.openai.com login apikey password TOKEN
    </pre>
  </li>
  <li>Setting it to a function that returns the key.</li>
</ul>

<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-azure &#34;Azure-1&#34;             ;Name, whatever you&#39;d like
  :protocol &#34;https&#34;                     ;Optional -- https is the default
  :host &#34;YOUR_RESOURCE_NAME.openai.azure.com&#34;
  :endpoint &#34;/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15&#34; ;or equivalent
  :stream t                             ;Enable streaming responses
  :key #&#39;gptel-api-key
  :models &#39;(gpt-3.5-turbo gpt-4))"><pre>(gptel-make-azure <span><span>&#34;</span>Azure-1<span>&#34;</span></span>             <span><span>;</span>Name, whatever you&#39;d like</span>
  <span>:protocol</span> <span><span>&#34;</span>https<span>&#34;</span></span>                     <span><span>;</span>Optional -- https is the default</span>
  <span>:host</span> <span><span>&#34;</span>YOUR_RESOURCE_NAME.openai.azure.com<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15<span>&#34;</span></span> <span><span>;</span>or equivalent</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Enable streaming responses</span>
  <span>:key</span> <span>#<span>&#39;gptel-api-key</span></span>
  <span>:models</span> &#39;(gpt-3.5-turbo gpt-4))</pre></div>
<p dir="auto">Refer to the documentation of <code>gptel-make-azure</code> to set more parameters.</p>
<p dir="auto">You can pick this backend from the menu when using gptel. (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model &#39;gpt-3.5-turbo
 gptel-backend (gptel-make-azure &#34;Azure-1&#34;
                 :protocol &#34;https&#34;
                 :host &#34;YOUR_RESOURCE_NAME.openai.azure.com&#34;
                 :endpoint &#34;/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15&#34;
                 :stream t
                 :key #&#39;gptel-api-key
                 :models &#39;(gpt-3.5-turbo gpt-4)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>&#39;gpt-3</span>.5-turbo
 gptel-backend (gptel-make-azure <span><span>&#34;</span>Azure-1<span>&#34;</span></span>
                 <span>:protocol</span> <span><span>&#34;</span>https<span>&#34;</span></span>
                 <span>:host</span> <span><span>&#34;</span>YOUR_RESOURCE_NAME.openai.azure.com<span>&#34;</span></span>
                 <span>:endpoint</span> <span><span>&#34;</span>/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15<span>&#34;</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:key</span> <span>#<span>&#39;gptel-api-key</span></span>
                 <span>:models</span> &#39;(gpt-3.5-turbo gpt-4)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-gpt4all &#34;GPT4All&#34;           ;Name of your choosing
 :protocol &#34;http&#34;
 :host &#34;localhost:4891&#34;                 ;Where it&#39;s running
 :models &#39;(mistral-7b-openorca.Q4_0.gguf)) ;Available models"><pre>(gptel-make-gpt4all <span><span>&#34;</span>GPT4All<span>&#34;</span></span>           <span><span>;</span>Name of your choosing</span>
 <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
 <span>:host</span> <span><span>&#34;</span>localhost:4891<span>&#34;</span></span>                 <span><span>;</span>Where it&#39;s running</span>
 <span>:models</span> &#39;(mistral-7b-openorca.Q4_0.gguf)) <span><span>;</span>Available models</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-gpt4all</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-1" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.  Additionally you may want to increase the response token size since GPT4All uses very short (often truncated) responses by default.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-max-tokens 500
 gptel-model &#39;mistral-7b-openorca.Q4_0.gguf
 gptel-backend (gptel-make-gpt4all &#34;GPT4All&#34;
                 :protocol &#34;http&#34;
                 :host &#34;localhost:4891&#34;
                 :models &#39;(mistral-7b-openorca.Q4_0.gguf)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-max-tokens <span>500</span>
 gptel-model <span>&#39;mistral-7b-openorca</span>.Q4_0.gguf
 gptel-backend (gptel-make-gpt4all <span><span>&#34;</span>GPT4All<span>&#34;</span></span>
                 <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
                 <span>:host</span> <span><span>&#34;</span>localhost:4891<span>&#34;</span></span>
                 <span>:models</span> &#39;(mistral-7b-openorca.Q4_0.gguf)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-ollama &#34;Ollama&#34;             ;Any name of your choosing
  :host &#34;localhost:11434&#34;               ;Where it&#39;s running
  :stream t                             ;Stream responses
  :models &#39;(mistral:latest))          ;List of models"><pre>(gptel-make-ollama <span><span>&#34;</span>Ollama<span>&#34;</span></span>             <span><span>;</span>Any name of your choosing</span>
  <span>:host</span> <span><span>&#34;</span>localhost:11434<span>&#34;</span></span>               <span><span>;</span>Where it&#39;s running</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Stream responses</span>
  <span>:models</span> &#39;(mistral:latest))          <span><span>;</span>List of models</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-ollama</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-2" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-2"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model &#39;mistral:latest
 gptel-backend (gptel-make-ollama &#34;Ollama&#34;
                 :host &#34;localhost:11434&#34;
                 :stream t
                 :models &#39;(mistral:latest)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>&#39;mistral:latest</span>
 gptel-backend (gptel-make-ollama <span><span>&#34;</span>Ollama<span>&#34;</span></span>
                 <span>:host</span> <span><span>&#34;</span>localhost:11434<span>&#34;</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:models</span> &#39;(mistral:latest)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; :key can be a function that returns the API key.
(gptel-make-gemini &#34;Gemini&#34; :key &#34;YOUR_GEMINI_API_KEY&#34; :stream t)"><pre><span><span>;</span>; :key can be a function that returns the API key.</span>
(gptel-make-gemini <span><span>&#34;</span>Gemini<span>&#34;</span></span> <span>:key</span> <span><span>&#34;</span>YOUR_GEMINI_API_KEY<span>&#34;</span></span> <span>:stream</span> <span>t</span>)</pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-gemini</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-3" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-3"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model &#39;gemini-pro
 gptel-backend (gptel-make-gemini &#34;Gemini&#34;
                 :key &#34;YOUR_GEMINI_API_KEY&#34;
                 :stream t))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>&#39;gemini-pro</span>
 gptel-backend (gptel-make-gemini <span><span>&#34;</span>Gemini<span>&#34;</span></span>
                 <span>:key</span> <span><span>&#34;</span>YOUR_GEMINI_API_KEY<span>&#34;</span></span>
                 <span>:stream</span> <span>t</span>))</pre></div>
</details>
<details>
<summary>

</summary>
<p dir="auto">(If using a llamafile, run a <a href="https://github.com/Mozilla-Ocho/llamafile#other-example-llamafiles">server llamafile</a> instead of a “command-line llamafile”, and a model that supports text generation.)</p>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Llama.cpp offers an OpenAI compatible API
(gptel-make-openai &#34;llama-cpp&#34;          ;Any name
  :stream t                             ;Stream responses
  :protocol &#34;http&#34;
  :host &#34;localhost:8000&#34;                ;Llama.cpp server location
  :models &#39;(test))                    ;Any names, doesn&#39;t matter for Llama"><pre><span><span>;</span>; Llama.cpp offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>llama-cpp<span>&#34;</span></span>          <span><span>;</span>Any name</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Stream responses</span>
  <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
  <span>:host</span> <span><span>&#34;</span>localhost:8000<span>&#34;</span></span>                <span><span>;</span>Llama.cpp server location</span>
  <span>:models</span> &#39;(test))                    <span><span>;</span>Any names, doesn&#39;t matter for Llama</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-openai</code> for more.</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-4" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-4"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   &#39;test
 gptel-backend (gptel-make-openai &#34;llama-cpp&#34;
                 :stream t
                 :protocol &#34;http&#34;
                 :host &#34;localhost:8000&#34;
                 :models &#39;(test)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>&#39;test</span>
 gptel-backend (gptel-make-openai <span><span>&#34;</span>llama-cpp<span>&#34;</span></span>
                 <span>:stream</span> <span>t</span>
                 <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
                 <span>:host</span> <span><span>&#34;</span>localhost:8000<span>&#34;</span></span>
                 <span>:models</span> &#39;(test)))</pre></div>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">Kagi (FastGPT &amp; Summarizer)</h4><a id="user-content-kagi-fastgpt--summarizer" aria-label="Permalink: Kagi (FastGPT &amp; Summarizer)" href="#kagi-fastgpt--summarizer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">Kagi’s FastGPT model and the Universal Summarizer are both supported.  A couple of notes:</p>
<ol dir="auto">
  <li>Universal Summarizer: If there is a URL at point, the summarizer will summarize the contents of the URL.  Otherwise the context sent to the model is the same as always: the buffer text upto point, or the contents of the region if the region is active.</li>
  <li>Kagi models do not support multi-turn conversations, interactions are “one-shot”.  They also do not support streaming responses.</li>
</ol>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-kagi &#34;Kagi&#34;                    ;any name
  :key &#34;YOUR_KAGI_API_KEY&#34;)                ;can be a function that returns the key"><pre>(gptel-make-kagi <span><span>&#34;</span>Kagi<span>&#34;</span></span>                    <span><span>;</span>any name</span>
  <span>:key</span> <span><span>&#34;</span>YOUR_KAGI_API_KEY<span>&#34;</span></span>)                <span><span>;</span>can be a function that returns the key</span></pre></div>
<p dir="auto">These are the required parameters, refer to the documentation of <code>gptel-make-kagi</code> for more.</p>
<p dir="auto">You can pick this backend and the model (fastgpt/summarizer) from the transient menu when using gptel.</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-5" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-5"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model &#39;fastgpt
 gptel-backend (gptel-make-kagi &#34;Kagi&#34;
                 :key &#34;YOUR_KAGI_API_KEY&#34;))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>&#39;fastgpt</span>
 gptel-backend (gptel-make-kagi <span><span>&#34;</span>Kagi<span>&#34;</span></span>
                 <span>:key</span> <span><span>&#34;</span>YOUR_KAGI_API_KEY<span>&#34;</span></span>))</pre></div>
<p dir="auto">The alternatives to <code>fastgpt</code> include <code>summarize:cecil</code>, <code>summarize:agnes</code>, <code>summarize:daphne</code> and <code>summarize:muriel</code>.  The difference between the summarizer engines is <a href="https://help.kagi.com/kagi/api/summarizer.html#summarization-engines" rel="nofollow">documented here</a>.</p>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Together.ai offers an OpenAI compatible API
(gptel-make-openai &#34;TogetherAI&#34;         ;Any name you want
  :host &#34;api.together.xyz&#34;
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :stream t
  :models &#39;(;; has many more, check together.ai
            mistralai/Mixtral-8x7B-Instruct-v0.1
            codellama/CodeLlama-13b-Instruct-hf
            codellama/CodeLlama-34b-Instruct-hf))"><pre><span><span>;</span>; Together.ai offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>TogetherAI<span>&#34;</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.together.xyz<span>&#34;</span></span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> &#39;(<span><span>;</span>; has many more, check together.ai</span>
            mistralai/Mixtral-8x7B-Instruct-v0.1
            codellama/CodeLlama-13b-Instruct-hf
            codellama/CodeLlama-34b-Instruct-hf))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-6" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-6"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   &#39;mistralai/Mixtral-8x7B-Instruct-v0.1
 gptel-backend
 (gptel-make-openai &#34;TogetherAI&#34;         
   :host &#34;api.together.xyz&#34;
   :key &#34;your-api-key&#34;                   
   :stream t
   :models &#39;(;; has many more, check together.ai
             mistralai/Mixtral-8x7B-Instruct-v0.1
             codellama/CodeLlama-13b-Instruct-hf
             codellama/CodeLlama-34b-Instruct-hf)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>&#39;mistralai/Mixtral-8x7B-Instruct-v0</span>.1
 gptel-backend
 (gptel-make-openai <span><span>&#34;</span>TogetherAI<span>&#34;</span></span>         
   <span>:host</span> <span><span>&#34;</span>api.together.xyz<span>&#34;</span></span>
   <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   
   <span>:stream</span> <span>t</span>
   <span>:models</span> &#39;(<span><span>;</span>; has many more, check together.ai</span>
             mistralai/Mixtral-8x7B-Instruct-v0.1
             codellama/CodeLlama-13b-Instruct-hf
             codellama/CodeLlama-34b-Instruct-hf)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Anyscale offers an OpenAI compatible API
(gptel-make-openai &#34;Anyscale&#34;           ;Any name you want
  :host &#34;api.endpoints.anyscale.com&#34;
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :models &#39;(;; has many more, check anyscale
            mistralai/Mixtral-8x7B-Instruct-v0.1))"><pre><span><span>;</span>; Anyscale offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>Anyscale<span>&#34;</span></span>           <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.endpoints.anyscale.com<span>&#34;</span></span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> &#39;(<span><span>;</span>; has many more, check anyscale</span>
            mistralai/Mixtral-8x7B-Instruct-v0.1))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-7" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-7"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   &#39;mistralai/Mixtral-8x7B-Instruct-v0.1
 gptel-backend
 (gptel-make-openai &#34;Anyscale&#34;
                 :host &#34;api.endpoints.anyscale.com&#34;
                 :key &#34;your-api-key&#34;
                 :models &#39;(;; has many more, check anyscale
                           mistralai/Mixtral-8x7B-Instruct-v0.1)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>&#39;mistralai/Mixtral-8x7B-Instruct-v0</span>.1
 gptel-backend
 (gptel-make-openai <span><span>&#34;</span>Anyscale<span>&#34;</span></span>
                 <span>:host</span> <span><span>&#34;</span>api.endpoints.anyscale.com<span>&#34;</span></span>
                 <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>
                 <span>:models</span> &#39;(<span><span>;</span>; has many more, check anyscale</span>
                           mistralai/Mixtral-8x7B-Instruct-v0.1)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Perplexity offers an OpenAI compatible API
(gptel-make-openai &#34;Perplexity&#34;         ;Any name you want
  :host &#34;api.perplexity.ai&#34;
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :endpoint &#34;/chat/completions&#34;
  :stream t
  :models &#39;(;; has many more, check perplexity.ai
            pplx-7b-chat
            pplx-70b-chat
            pplx-7b-online
            pplx-70b-online))"><pre><span><span>;</span>; Perplexity offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>Perplexity<span>&#34;</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.perplexity.ai<span>&#34;</span></span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> &#39;(<span><span>;</span>; has many more, check perplexity.ai</span>
            pplx-7b-chat
            pplx-70b-chat
            pplx-7b-online
            pplx-70b-online))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-8" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-8"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   &#39;pplx-7b-chat
 gptel-backend
 (gptel-make-openai &#34;Perplexity&#34;
   :host &#34;api.perplexity.ai&#34;
   :key &#34;your-api-key&#34;
   :endpoint &#34;/chat/completions&#34;
   :stream t
   :models &#39;(;; has many more, check perplexity.ai
             pplx-7b-chat
             pplx-70b-chat
             pplx-7b-online
             pplx-70b-online)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>&#39;pplx-7b-chat</span>
 gptel-backend
 (gptel-make-openai <span><span>&#34;</span>Perplexity<span>&#34;</span></span>
   <span>:host</span> <span><span>&#34;</span>api.perplexity.ai<span>&#34;</span></span>
   <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>
   <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
   <span>:stream</span> <span>t</span>
   <span>:models</span> &#39;(<span><span>;</span>; has many more, check perplexity.ai</span>
             pplx-7b-chat
             pplx-70b-chat
             pplx-7b-online
             pplx-70b-online)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-anthropic &#34;Claude&#34;          ;Any name you want
  :stream t                             ;Streaming responses
  :key &#34;your-api-key&#34;)"><pre>(gptel-make-anthropic <span><span>&#34;</span>Claude<span>&#34;</span></span>          <span><span>;</span>Any name you want</span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>Streaming responses</span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>)</pre></div>
<p dir="auto">The <code>:key</code> can be a function that returns the key (more secure).</p>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-9" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-9"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model &#39;claude-3-sonnet-20240229 ;  &#34;claude-3-opus-20240229&#34; also available
 gptel-backend (gptel-make-anthropic &#34;Claude&#34;
                 :stream t :key &#34;your-api-key&#34;))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model <span>&#39;claude-3-sonnet-20240229</span> <span><span>;</span>  &#34;claude-3-opus-20240229&#34; also available</span>
 gptel-backend (gptel-make-anthropic <span><span>&#34;</span>Claude<span>&#34;</span></span>
                 <span>:stream</span> <span>t</span> <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Groq offers an OpenAI compatible API
(gptel-make-openai &#34;Groq&#34;               ;Any name you want
  :host &#34;api.groq.com&#34;
  :endpoint &#34;/openai/v1/chat/completions&#34;
  :stream t
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :models &#39;(llama-3.1-70b-versatile
            llama-3.1-8b-instant
            llama3-70b-8192
            llama3-8b-8192
            mixtral-8x7b-32768
            gemma-7b-it))"><pre><span><span>;</span>; Groq offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>Groq<span>&#34;</span></span>               <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.groq.com<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/openai/v1/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> &#39;(llama-3.1-70b-versatile
            llama-3.1-8b-instant
            llama3-70b-8192
            llama3-8b-8192
            mixtral-8x7b-32768
            gemma-7b-it))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).  Note that Groq is fast enough that you could easily set <code>:stream nil</code> and still get near-instant responses.</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-10" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-10"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   &#39;mixtral-8x7b-32768
      gptel-backend
      (gptel-make-openai &#34;Groq&#34;
        :host &#34;api.groq.com&#34;
        :endpoint &#34;/openai/v1/chat/completions&#34;
        :stream t
        :key &#34;your-api-key&#34;
        :models &#39;(llama-3.1-70b-versatile
                  llama-3.1-8b-instant
                  llama3-70b-8192
                  llama3-8b-8192
                  mixtral-8x7b-32768
                  gemma-7b-it)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>&#39;mixtral-8x7b-32768</span>
      gptel-backend
      (gptel-make-openai <span><span>&#34;</span>Groq<span>&#34;</span></span>
        <span>:host</span> <span><span>&#34;</span>api.groq.com<span>&#34;</span></span>
        <span>:endpoint</span> <span><span>&#34;</span>/openai/v1/chat/completions<span>&#34;</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>
        <span>:models</span> &#39;(llama-3.1-70b-versatile
                  llama-3.1-8b-instant
                  llama3-70b-8192
                  llama3-8b-8192
                  mixtral-8x7b-32768
                  gemma-7b-it)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OpenRouter offers an OpenAI compatible API
(gptel-make-openai &#34;OpenRouter&#34;               ;Any name you want
  :host &#34;openrouter.ai&#34;
  :endpoint &#34;/api/v1/chat/completions&#34;
  :stream t
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :models &#39;(openai/gpt-3.5-turbo
            mistralai/mixtral-8x7b-instruct
            meta-llama/codellama-34b-instruct
            codellama/codellama-70b-instruct
            google/palm-2-codechat-bison-32k
            google/gemini-pro))
"><pre><span><span>;</span>; OpenRouter offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>OpenRouter<span>&#34;</span></span>               <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>openrouter.ai<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/api/v1/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> &#39;(openai/gpt-3.5-turbo
            mistralai/mixtral-8x7b-instruct
            meta-llama/codellama-34b-instruct
            codellama/codellama-70b-instruct
            google/palm-2-codechat-bison-32k
            google/gemini-pro))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-11" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-11"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   &#39;mixtral-8x7b-32768
      gptel-backend
      (gptel-make-openai &#34;OpenRouter&#34;               ;Any name you want
        :host &#34;openrouter.ai&#34;
        :endpoint &#34;/api/v1/chat/completions&#34;
        :stream t
        :key &#34;your-api-key&#34;                   ;can be a function that returns the key
        :models &#39;(openai/gpt-3.5-turbo
                  mistralai/mixtral-8x7b-instruct
                  meta-llama/codellama-34b-instruct
                  codellama/codellama-70b-instruct
                  google/palm-2-codechat-bison-32k
                  google/gemini-pro)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>&#39;mixtral-8x7b-32768</span>
      gptel-backend
      (gptel-make-openai <span><span>&#34;</span>OpenRouter<span>&#34;</span></span>               <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>&#34;</span>openrouter.ai<span>&#34;</span></span>
        <span>:endpoint</span> <span><span>&#34;</span>/api/v1/chat/completions<span>&#34;</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
        <span>:models</span> &#39;(openai/gpt-3.5-turbo
                  mistralai/mixtral-8x7b-instruct
                  meta-llama/codellama-34b-instruct
                  codellama/codellama-70b-instruct
                  google/palm-2-codechat-bison-32k
                  google/gemini-pro)))
</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content="(gptel-make-privategpt &#34;privateGPT&#34;               ;Any name you want
  :protocol &#34;http&#34;
  :host &#34;localhost:8001&#34;
  :stream t
  :context t                            ;Use context provided by embeddings
  :sources t                            ;Return information about source documents
  :models &#39;(private-gpt))
"><pre>(gptel-make-privategpt <span><span>&#34;</span>privateGPT<span>&#34;</span></span>               <span><span>;</span>Any name you want</span>
  <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
  <span>:host</span> <span><span>&#34;</span>localhost:8001<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:context</span> <span>t</span>                            <span><span>;</span>Use context provided by embeddings</span>
  <span>:sources</span> <span>t</span>                            <span><span>;</span>Return information about source documents</span>
  <span>:models</span> &#39;(private-gpt))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-12" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-12"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   &#39;private-gpt
      gptel-backend
      (gptel-make-privategpt &#34;privateGPT&#34;               ;Any name you want
        :protocol &#34;http&#34;
        :host &#34;localhost:8001&#34;
        :stream t
        :context t                            ;Use context provided by embeddings
        :sources t                            ;Return information about source documents
        :models &#39;(private-gpt)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>&#39;private-gpt</span>
      gptel-backend
      (gptel-make-privategpt <span><span>&#34;</span>privateGPT<span>&#34;</span></span>               <span><span>;</span>Any name you want</span>
        <span>:protocol</span> <span><span>&#34;</span>http<span>&#34;</span></span>
        <span>:host</span> <span><span>&#34;</span>localhost:8001<span>&#34;</span></span>
        <span>:stream</span> <span>t</span>
        <span>:context</span> <span>t</span>                            <span><span>;</span>Use context provided by embeddings</span>
        <span>:sources</span> <span>t</span>                            <span><span>;</span>Return information about source documents</span>
        <span>:models</span> &#39;(private-gpt)))
</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; DeepSeek offers an OpenAI compatible API
(gptel-make-openai &#34;DeepSeek&#34;       ;Any name you want
  :host &#34;api.deepseek.com&#34;
  :endpoint &#34;/chat/completions&#34;
  :stream t
  :key &#34;your-api-key&#34;               ;can be a function that returns the key
  :models &#39;(deepseek-chat deepseek-coder))
"><pre><span><span>;</span>; DeepSeek offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>DeepSeek<span>&#34;</span></span>       <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.deepseek.com<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>               <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> &#39;(deepseek-chat deepseek-coder))
</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-13" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-13"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   &#39;deepseek-chat
      gptel-backend
      (gptel-make-openai &#34;DeepSeek&#34;     ;Any name you want
        :host &#34;api.deepseek.com&#34;
        :endpoint &#34;/chat/completions&#34;
        :stream t
        :key &#34;your-api-key&#34;             ;can be a function that returns the key
        :models &#39;(deepseek-chat deepseek-coder)))
"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>&#39;deepseek-chat</span>
      gptel-backend
      (gptel-make-openai <span><span>&#34;</span>DeepSeek<span>&#34;</span></span>     <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>&#34;</span>api.deepseek.com<span>&#34;</span></span>
        <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>             <span><span>;</span>can be a function that returns the key</span>
        <span>:models</span> &#39;(deepseek-chat deepseek-coder)))
</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Cerebras offers an instant OpenAI compatible API
(gptel-make-openai &#34;Cerebras&#34;
  :host &#34;api.cerebras.ai&#34;
  :endpoint &#34;/v1/chat/completions&#34;
  :stream t                             ;optionally nil as Cerebras is instant AI
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :models &#39;(llama3.1-70b
            llama3.1-8b))"><pre><span><span>;</span>; Cerebras offers an instant OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>Cerebras<span>&#34;</span></span>
  <span>:host</span> <span><span>&#34;</span>api.cerebras.ai<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/v1/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>                             <span><span>;</span>optionally nil as Cerebras is instant AI</span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:models</span> &#39;(llama3.1-70b
            llama3.1-8b))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-14" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-14"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model   &#39;llama3.1-8b
      gptel-backend
      (gptel-make-openai &#34;Cerebras&#34;
        :host &#34;api.cerebras.ai&#34;
        :endpoint &#34;/v1/chat/completions&#34;
        :stream nil
        :key &#34;your-api-key&#34;
        :models &#39;(llama3.1-70b
                  llama3.1-8b)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model   <span>&#39;llama3</span>.1-8b
      gptel-backend
      (gptel-make-openai <span><span>&#34;</span>Cerebras<span>&#34;</span></span>
        <span>:host</span> <span><span>&#34;</span>api.cerebras.ai<span>&#34;</span></span>
        <span>:endpoint</span> <span><span>&#34;</span>/v1/chat/completions<span>&#34;</span></span>
        <span>:stream</span> <span>nil</span>
        <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>
        <span>:models</span> &#39;(llama3.1-70b
                  llama3.1-8b)))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Github Models offers an OpenAI compatible API
(gptel-make-openai &#34;Github Models&#34; ;Any name you want
  :host &#34;models.inference.ai.azure.com&#34;
  :endpoint &#34;/chat/completions&#34;
  :stream t
  :key &#34;your-github-token&#34;
  :models &#39;(gpt-4o))"><pre><span><span>;</span>; Github Models offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>Github Models<span>&#34;</span></span> <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>models.inference.ai.azure.com<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
  <span>:stream</span> <span>t</span>
  <span>:key</span> <span><span>&#34;</span>your-github-token<span>&#34;</span></span>
  <span>:models</span> &#39;(gpt-4o))</pre></div>
<p dir="auto">For all the available models, check the <a href="https://github.com/marketplace/models">marketplace</a>.</p>
<p dir="auto">You can pick this backend from the menu when using (see <a href="#usage">Usage</a>).</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-15" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-15"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq gptel-model  &#39;gpt-4o
      gptel-backend
      (gptel-make-openai &#34;Github Models&#34; ;Any name you want
        :host &#34;models.inference.ai.azure.com&#34;
        :endpoint &#34;/chat/completions&#34;
        :stream t
        :key &#34;your-github-token&#34;
        :models &#39;(gpt-4o))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span> gptel-model  <span>&#39;gpt-4o</span>
      gptel-backend
      (gptel-make-openai <span><span>&#34;</span>Github Models<span>&#34;</span></span> <span><span>;</span>Any name you want</span>
        <span>:host</span> <span><span>&#34;</span>models.inference.ai.azure.com<span>&#34;</span></span>
        <span>:endpoint</span> <span><span>&#34;</span>/chat/completions<span>&#34;</span></span>
        <span>:stream</span> <span>t</span>
        <span>:key</span> <span><span>&#34;</span>your-github-token<span>&#34;</span></span>
        <span>:models</span> &#39;(gpt-4o))</pre></div>
</details>
<details><summary>

</summary>
<p dir="auto">Register a backend with</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Novita AI offers an OpenAI compatible API
(gptel-make-openai &#34;NovitaAI&#34;         ;Any name you want
  :host &#34;api.novita.ai&#34;
  :endpoint &#34;/v3/openai&#34;
  :key &#34;your-api-key&#34;                   ;can be a function that returns the key
  :stream t
  :models &#39;(;; has many more, check https://novita.ai/llm-api
            gryphe/mythomax-l2-13b
            meta-llama/llama-3-70b-instruct
            meta-llama/llama-3.1-70b-instruct))"><pre><span><span>;</span>; Novita AI offers an OpenAI compatible API</span>
(gptel-make-openai <span><span>&#34;</span>NovitaAI<span>&#34;</span></span>         <span><span>;</span>Any name you want</span>
  <span>:host</span> <span><span>&#34;</span>api.novita.ai<span>&#34;</span></span>
  <span>:endpoint</span> <span><span>&#34;</span>/v3/openai<span>&#34;</span></span>
  <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   <span><span>;</span>can be a function that returns the key</span>
  <span>:stream</span> <span>t</span>
  <span>:models</span> &#39;(<span><span>;</span>; has many more, check https://novita.ai/llm-api</span>
            gryphe/mythomax-l2-13b
            meta-llama/llama-3-70b-instruct
            meta-llama/llama-3.1-70b-instruct))</pre></div>
<p dir="auto">You can pick this backend from the menu when using gptel (see <a href="#usage">Usage</a>)</p>
<div dir="auto"><h5 tabindex="-1" dir="auto">(Optional) Set as the default gptel backend</h5><a id="user-content-optional-set-as-the-default-gptel-backend-16" aria-label="Permalink: (Optional) Set as the default gptel backend" href="#optional-set-as-the-default-gptel-backend-16"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The above code makes the backend available to select.  If you want it to be the default backend for gptel, you can set this as the value of <code>gptel-backend</code>.  Use this instead of the above.</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; OPTIONAL configuration
(setq
 gptel-model   &#39;gryphe/mythomax-l2-13b
 gptel-backend
 (gptel-make-openai &#34;NovitaAI&#34;         
   :host &#34;api.novita.ai&#34;
   :endpoint &#34;/v3/openai&#34;
   :key &#34;your-api-key&#34;                   
   :stream t
   :models &#39;(;; has many more, check https://novita.ai/llm-api
             mistralai/Mixtral-8x7B-Instruct-v0.1
             meta-llama/llama-3-70b-instruct
             meta-llama/llama-3.1-70b-instruct)))"><pre><span><span>;</span>; OPTIONAL configuration</span>
(<span>setq</span>
 gptel-model   <span>&#39;gryphe/mythomax-l2-13b</span>
 gptel-backend
 (gptel-make-openai <span><span>&#34;</span>NovitaAI<span>&#34;</span></span>         
   <span>:host</span> <span><span>&#34;</span>api.novita.ai<span>&#34;</span></span>
   <span>:endpoint</span> <span><span>&#34;</span>/v3/openai<span>&#34;</span></span>
   <span>:key</span> <span><span>&#34;</span>your-api-key<span>&#34;</span></span>                   
   <span>:stream</span> <span>t</span>
   <span>:models</span> &#39;(<span><span>;</span>; has many more, check https://novita.ai/llm-api</span>
             mistralai/Mixtral-8x7B-Instruct-v0.1
             meta-llama/llama-3-70b-instruct
             meta-llama/llama-3.1-70b-instruct)))</pre></div>
</details>

<p dir="auto">(There is also a <a href="https://www.youtube.com/watch?v=bsRnh_brggM" rel="nofollow">video demo</a> showing various uses of gptel.)</p>
<markdown-accessiblity-table><table>
  <tbody><tr><th><b>To send queries</b></th><th>Description</th></tr>
  <tr><td><code>gptel-send</code></td><td>Send all text up to <code>(point)</code>, or the selection if region is active.  Works anywhere in Emacs.</td></tr>
  <tr><td><code>gptel</code></td><td>Create a new dedicated chat buffer.  Not required to use gptel.</td></tr>
  <tr><th><b>To Set options</b></th><th></th></tr>
  <tr><td><code>C-u</code> <code>gptel-send</code></td><td>Transient menu for preferences, input/output redirection etc.</td></tr>
  <tr><td><code>gptel-menu</code></td><td><i>(Same)</i></td></tr>
  <tr><th><b>To add context</b></th><th></th></tr>
  <tr><td><code>gptel-add</code></td><td>Add/remove a region or buffer to gptel’s context.  In Dired, add/remove marked files.</td></tr>
  <tr><td><code>gptel-add-file</code></td><td>Add a file (text or supported media type) to gptel’s context.  Also available from the transient menu.</td></tr>
  <tr><th><b>Org mode bonuses</b></th><th></th></tr>
  <tr><td><code>gptel-org-set-topic</code></td><td>Limit conversation context to an Org heading.  (For branching conversations see below.)</td></tr>
  <tr><td><code>gptel-org-set-properties</code></td><td>Write gptel configuration as Org properties, for per-heading chat configuration.</td></tr>
</tbody></table></markdown-accessiblity-table>

<ol dir="auto">
  <li>Call <code>M-x gptel-send</code> to send the text up to the cursor. The response will be inserted below.  Continue the conversation by typing below the response.</li>
  <li>If a region is selected, the conversation will be limited to its contents.</li>
  <li>Call <code>M-x gptel-send</code> with a prefix argument (<code>C-u</code>)
    <ul dir="auto">
      <li>to set chat parameters (GPT model, system message etc) for this buffer,</li>
      <li>include quick instructions for the next request only,</li>
      <li>to add additional context – regions, buffers or files – to gptel,</li>
      <li>to read the prompt from or redirect the response elsewhere,</li>
      <li>or to replace the prompt with the response.</li>
    </ul>
  </li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342004903-3562a6e2-7a5c-4f7e-8e57-bf3c11589c73.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNDkwMy0zNTYyYTZlMi03YTVjLTRmN2UtOGU1Ny1iZjNjMTE1ODljNzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzIzODNmNWUwMjRmOTgxYTM4M2IzMmQ3NTQzNjRlYTc2Y2Q4MjFmZjBjM2JhYzJhNTk0M2JhNmMwYmU5Mjk2NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.GBJxhZQhQe8kfCCZQGfrHIVX6_hClD1y0pEFoPzHO84"><img src="https://private-user-images.githubusercontent.com/8607532/342004903-3562a6e2-7a5c-4f7e-8e57-bf3c11589c73.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNDkwMy0zNTYyYTZlMi03YTVjLTRmN2UtOGU1Ny1iZjNjMTE1ODljNzMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzIzODNmNWUwMjRmOTgxYTM4M2IzMmQ3NTQzNjRlYTc2Y2Q4MjFmZjBjM2JhYzJhNTk0M2JhNmMwYmU5Mjk2NiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.GBJxhZQhQe8kfCCZQGfrHIVX6_hClD1y0pEFoPzHO84" alt="Image showing gptel&#39;s menu with some of the available query options."/></a>
<div dir="auto"><h3 tabindex="-1" dir="auto">In a dedicated chat buffer:</h3><a id="user-content-in-a-dedicated-chat-buffer" aria-label="Permalink: In a dedicated chat buffer:" href="#in-a-dedicated-chat-buffer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
  <li>Run <code>M-x gptel</code> to start or switch to the chat buffer. It will ask you for the key if you skipped the previous step. Run it with a prefix-arg (<code>C-u M-x gptel</code>) to start a new session.</li>
  <li>In the gptel buffer, send your prompt with <code>M-x gptel-send</code>, bound to <code>C-c RET</code>.</li>
  <li>Set chat parameters (LLM provider, model, directives etc) for the session by calling <code>gptel-send</code> with a prefix argument (<code>C-u C-c RET</code>):</li>
</ol>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342005178-eb4867e5-30ac-455f-999f-e17123afb810.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNTE3OC1lYjQ4NjdlNS0zMGFjLTQ1NWYtOTk5Zi1lMTcxMjNhZmI4MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDNiMGRmZjI2MWI3NmEyZjU1MTNmNDVhYjAwZGYyODQyNjFhN2EzMWM0NmViMTQwMDhlMjYwMDQ2MjcwZTkzNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.bZbLHu8LckIC7MT34_mwW-VNAdDcnDYdutAXjpAvTfw"><img src="https://private-user-images.githubusercontent.com/8607532/342005178-eb4867e5-30ac-455f-999f-e17123afb810.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNTE3OC1lYjQ4NjdlNS0zMGFjLTQ1NWYtOTk5Zi1lMTcxMjNhZmI4MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDNiMGRmZjI2MWI3NmEyZjU1MTNmNDVhYjAwZGYyODQyNjFhN2EzMWM0NmViMTQwMDhlMjYwMDQ2MjcwZTkzNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.bZbLHu8LckIC7MT34_mwW-VNAdDcnDYdutAXjpAvTfw" alt="Image showing gptel&#39;s menu with some of the available query options."/></a>
<p dir="auto">That’s it. You can go back and edit previous prompts and responses if you want.</p>
<p dir="auto">The default mode is <code>markdown-mode</code> if available, else <code>text-mode</code>.  You can set <code>gptel-default-mode</code> to <code>org-mode</code> if desired.</p>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">Including media (images, documents) with requests</h4><a id="user-content-including-media-images-documents-with-requests" aria-label="Permalink: Including media (images, documents) with requests" href="#including-media-images-documents-with-requests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">gptel supports sending media in Markdown and Org chat buffers, but this feature is disabled by default.</p>
<ul dir="auto">
  <li>You can enable it globally, for all models that support it, by setting <code>gptel-track-media</code>.</li>
  <li>Or you can set it locally, just for the chat buffer, via the header line:</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/375929517-91f6aaab-2ea4-4806-9cc9-39b4b46a8e6c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM3NTkyOTUxNy05MWY2YWFhYi0yZWE0LTQ4MDYtOWNjOS0zOWI0YjQ2YThlNmMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDc2YmFkMjM2MjljMDRmMDdmNDU2YjM1Mjk5M2ZkMTdkMzRiMWJjMWM1MTEzNTk2NjM4YTI5ZmE4MzExMWM3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.A5i9YeOcbT0ONzaP-TXaF4yP1PhVhQ8tsNb9STHbknA"><img src="https://private-user-images.githubusercontent.com/8607532/375929517-91f6aaab-2ea4-4806-9cc9-39b4b46a8e6c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM3NTkyOTUxNy05MWY2YWFhYi0yZWE0LTQ4MDYtOWNjOS0zOWI0YjQ2YThlNmMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDc2YmFkMjM2MjljMDRmMDdmNDU2YjM1Mjk5M2ZkMTdkMzRiMWJjMWM1MTEzNTk2NjM4YTI5ZmE4MzExMWM3ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.A5i9YeOcbT0ONzaP-TXaF4yP1PhVhQ8tsNb9STHbknA" alt="Image showing a gptel chat buffer&#39;s header line with the button to toggle media support"/></a>
<hr/>
<p dir="auto">There are two ways to include media with requests:</p>
<ol dir="auto">
  <li>Adding media files to the context with <code>gptel-add-file</code>, described further below.</li>
  <li>Including links to media in chat buffers, described here:</li>
</ol>
<p dir="auto">To send media – images or other supported file types – with requests in chat buffers, you can include links to them in the chat buffer.  Such a link must be “standalone”, i.e. on a line by itself surrounded by whitespace.</p>
<p dir="auto">In Org mode, for example, the following are all <b>valid</b> ways of including an image with the request:</p>
<ul dir="auto">
  <li>“Standalone” file link:</li>
</ul>
<pre>Describe this picture

[[file:/path/to/screenshot.png]]

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>“Standalone” file link with description:</li>
</ul>
<pre>Describe this picture

[[file:/path/to/screenshot.png][some picture]]

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>“Standalone”, angle file link:</li>
</ul>
<pre>Describe this picture

&lt;file:/path/to/screenshot.png&gt;

Focus specifically on the text content.
</pre>
<p dir="auto">The following links are <b>not valid</b>, and the text of the link will be sent instead of the file contents:</p>
<ul dir="auto">
  <li>Inline link:</li>
</ul>
<pre>Describe this [[file:/path/to/screenshot.png][picture]].

Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>Link not “standalone”:</li>
</ul>
<pre>Describe this picture: 
[[file:/path/to/screenshot.png]]
Focus specifically on the text content.
</pre>
<ul dir="auto">
  <li>Not a valid Org link:</li>
</ul>
<pre>Describe the picture

file:/path/to/screenshot.png
</pre>
<p dir="auto">Similar criteria apply to Markdown chat buffers.</p>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">Save and restore your chat sessions</h4><a id="user-content-save-and-restore-your-chat-sessions" aria-label="Permalink: Save and restore your chat sessions" href="#save-and-restore-your-chat-sessions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">Saving the file will save the state of the conversation as well.  To resume the chat, open the file and turn on <code>gptel-mode</code> before editing the buffer.</p>
</details>
<div dir="auto"><h3 tabindex="-1" dir="auto">Include more context with requests</h3><a id="user-content-include-more-context-with-requests" aria-label="Permalink: Include more context with requests" href="#include-more-context-with-requests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">By default, gptel will query the LLM with the active region or the buffer contents up to the cursor.  Often it can be helpful to provide the LLM with additional context from outside the current buffer. For example, when you’re in a chat buffer but want to ask questions about a (possibly changing) code buffer and auxiliary project files.</p>
<p dir="auto">You can include additional text regions, buffers or files with gptel’s queries.  This additional context is “live” and not a snapshot.  Once added, the regions, buffers or files are scanned and included at the time of each query.  When using multi-modal models, added files can be of any supported type – typically images.</p>
<p dir="auto">You can add a selected region, buffer or file to gptel’s context from the menu, or call <code>gptel-add</code>.  (To add a file use <code>gptel-add</code> in Dired or use the dedicated <code>gptel-add-file</code> command.)</p>
<p dir="auto">You can examine the active context from the menu:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342006376-63cd7fc8-6b3e-42ae-b6ca-06ff935bae9c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3Ni02M2NkN2ZjOC02YjNlLTQyYWUtYjZjYS0wNmZmOTM1YmFlOWMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTc5MThlOTc1YTZmMmI5NDczMjdmN2I0NTA4YmFhMDAzMDBlNTY0N2RjNTRjYzhiNjE4NGI3ZTZjNzU2NmQ5ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.AxNzBlT7V-xzL51mmzxF9EybktIJg-gjuupWux6ubVY"><img src="https://private-user-images.githubusercontent.com/8607532/342006376-63cd7fc8-6b3e-42ae-b6ca-06ff935bae9c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3Ni02M2NkN2ZjOC02YjNlLTQyYWUtYjZjYS0wNmZmOTM1YmFlOWMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTc5MThlOTc1YTZmMmI5NDczMjdmN2I0NTA4YmFhMDAzMDBlNTY0N2RjNTRjYzhiNjE4NGI3ZTZjNzU2NmQ5ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.AxNzBlT7V-xzL51mmzxF9EybktIJg-gjuupWux6ubVY" alt="Image showing gptel&#39;s menu with the "/></a>
<p dir="auto">And then browse through or remove context from the context buffer:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/342006378-79a5ffe8-3d63-4bf7-9bf6-0457ab61bf2a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3OC03OWE1ZmZlOC0zZDYzLTRiZjctOWJmNi0wNDU3YWI2MWJmMmEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDQ3ZGE2NjFlZjllOWRjZGRmODJmZGQzYTYzMmQxMmY2NjRjYzI3NTVmNzE5Y2UzNmEwOWFkNDYwNTE1MWZiOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Z8eZn5Ixd1lfguHHvu3AWBkf3dsSanEK9YKBYTZpUU0"><img src="https://private-user-images.githubusercontent.com/8607532/342006378-79a5ffe8-3d63-4bf7-9bf6-0457ab61bf2a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM0MjAwNjM3OC03OWE1ZmZlOC0zZDYzLTRiZjctOWJmNi0wNDU3YWI2MWJmMmEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDQ3ZGE2NjFlZjllOWRjZGRmODJmZGQzYTYzMmQxMmY2NjRjYzI3NTVmNzE5Y2UzNmEwOWFkNDYwNTE1MWZiOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Z8eZn5Ixd1lfguHHvu3AWBkf3dsSanEK9YKBYTZpUU0" alt="Image showing gptel&#39;s context buffer."/></a>
<div dir="auto"><h3 tabindex="-1" dir="auto">Rewrite, refactor or fill in a region</h3><a id="user-content-rewrite-refactor-or-fill-in-a-region" aria-label="Permalink: Rewrite, refactor or fill in a region" href="#rewrite-refactor-or-fill-in-a-region"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In any buffer: with a region selected, you can rewrite prose or refactor code from here:</p>
<p dir="auto"><b>Prose</b>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940610-172e75cd-11e9-4f3b-9ab9-b4edfb8f6695.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxMC0xNzJlNzVjZC0xMWU5LTRmM2ItOWFiOS1iNGVkZmI4ZjY2OTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjAwYjk2M2I4NWRkOGZkNjRlZWU0YmRhMThhNzVhYjRiN2EwNjI5NzhlNjBkNDU1ODEyNmQyNWZkNjBhY2E5YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.yJlx2ak3BKXItFu31F6jFetp8ac-zuu24XXQC0XQsJ4"><img src="https://private-user-images.githubusercontent.com/8607532/365940610-172e75cd-11e9-4f3b-9ab9-b4edfb8f6695.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxMC0xNzJlNzVjZC0xMWU5LTRmM2ItOWFiOS1iNGVkZmI4ZjY2OTUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjAwYjk2M2I4NWRkOGZkNjRlZWU0YmRhMThhNzVhYjRiN2EwNjI5NzhlNjBkNDU1ODEyNmQyNWZkNjBhY2E5YyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.yJlx2ak3BKXItFu31F6jFetp8ac-zuu24XXQC0XQsJ4"/></a>
<p dir="auto"><b>Code</b>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940620-8670ddb3-8655-47f4-a70c-8994994e61e3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYyMC04NjcwZGRiMy04NjU1LTQ3ZjQtYTcwYy04OTk0OTk0ZTYxZTMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWNjZGQ1NDQ4OWMwMGZiNzE2NjA0OTM4ZjBjNWEyM2RhYjc0ZDFiMzdlM2I3NmM0NzM1MjQ1MGZjZTBlYWIwZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.UWLultAGYO_sKXpt-afViPhWKEJwKcDGoEaCF2bb8JY"><img src="https://private-user-images.githubusercontent.com/8607532/365940620-8670ddb3-8655-47f4-a70c-8994994e61e3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYyMC04NjcwZGRiMy04NjU1LTQ3ZjQtYTcwYy04OTk0OTk0ZTYxZTMucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWNjZGQ1NDQ4OWMwMGZiNzE2NjA0OTM4ZjBjNWEyM2RhYjc0ZDFiMzdlM2I3NmM0NzM1MjQ1MGZjZTBlYWIwZCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.UWLultAGYO_sKXpt-afViPhWKEJwKcDGoEaCF2bb8JY"/></a>
<p dir="auto">When the refactor is ready, you can apply it or compare against the original:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940614-a0cf33b2-3ad3-4f55-aa7b-4349e54d6771.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxNC1hMGNmMzNiMi0zYWQzLTRmNTUtYWE3Yi00MzQ5ZTU0ZDY3NzEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTE4MDBkYmNjMGI0ZGM0ZWQ5NGExZGQ1ZDE2NDY4MzdkMmJiZTRkZmE4NjYzZGMzYjM3NTUxZGU2YTA4OWQ4NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iA46VUMqsNFPXBTl7SD_PYIBOwuRI4cX9xvaLE6q_VQ"><img src="https://private-user-images.githubusercontent.com/8607532/365940614-a0cf33b2-3ad3-4f55-aa7b-4349e54d6771.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYxNC1hMGNmMzNiMi0zYWQzLTRmNTUtYWE3Yi00MzQ5ZTU0ZDY3NzEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTE4MDBkYmNjMGI0ZGM0ZWQ5NGExZGQ1ZDE2NDY4MzdkMmJiZTRkZmE4NjYzZGMzYjM3NTUxZGU2YTA4OWQ4NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iA46VUMqsNFPXBTl7SD_PYIBOwuRI4cX9xvaLE6q_VQ"/></a>
<p dir="auto">These actions are also available directly when the cursor is inside the pending rewrite region:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/365940607-57e1fff7-f8e4-47f0-9bda-d0b698443559.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYwNy01N2UxZmZmNy1mOGU0LTQ3ZjAtOWJkYS1kMGI2OTg0NDM1NTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjY1YWQyOTkyZTdlMTc2YTc3Mjc4ZmEwM2U1MmU4ZmZlYmJkODY1NTFhMDc4MmU0ODE0ZjFiOTc5MzZkMWRhMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.n5_azw02D-p99axp18J5j7PtQsZoFDS2E1WQXd4fHIs"><img src="https://private-user-images.githubusercontent.com/8607532/365940607-57e1fff7-f8e4-47f0-9bda-d0b698443559.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzM2NTk0MDYwNy01N2UxZmZmNy1mOGU0LTQ3ZjAtOWJkYS1kMGI2OTg0NDM1NTkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NjY1YWQyOTkyZTdlMTc2YTc3Mjc4ZmEwM2U1MmU4ZmZlYmJkODY1NTFhMDc4MmU0ODE0ZjFiOTc5MzZkMWRhMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.n5_azw02D-p99axp18J5j7PtQsZoFDS2E1WQXd4fHIs"/></a>
<div dir="auto"><h3 tabindex="-1" dir="auto">Extra Org mode conveniences</h3><a id="user-content-extra-org-mode-conveniences" aria-label="Permalink: Extra Org mode conveniences" href="#extra-org-mode-conveniences"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">gptel offers a few extra conveniences in Org mode.</p>
<ul dir="auto">
  <li>You can limit the conversation context to an Org heading with the command <code>gptel-org-set-topic</code>.</li>
  <li>You can have branching conversations in Org mode, where each hierarchical outline path through the document is a separate conversation branch.  This is also useful for limiting the context size of each query.  See the variable <code>gptel-org-branching-context</code>.
    Note: using this option requires Org 9.6.7 or higher to be available.  The <a href="https://github.com/ultronozm/ai-org-chat.el">ai-org-chat</a> package uses gptel to provide this branching conversation behavior for older versions of Org.</li>
  <li>You can declare the gptel model, backend, temperature, system message and other parameters as Org properties with the command <code>gptel-org-set-properties</code>.  gptel queries under the corresponding heading will always use these settings, allowing you to create mostly reproducible LLM chat notebooks, and to have simultaneous chats with different models, model settings and directives under different Org headings.</li>
</ul>

<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">I want the window to scroll automatically as the response is inserted</h4><a id="user-content-i-want-the-window-to-scroll-automatically-as-the-response-is-inserted" aria-label="Permalink: I want the window to scroll automatically as the response is inserted" href="#i-want-the-window-to-scroll-automatically-as-the-response-is-inserted"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">To be minimally annoying, gptel does not move the cursor by default.  Add the following to your configuration to enable auto-scrolling.</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-hook &#39;gptel-post-stream-hook &#39;gptel-auto-scroll)"><pre>(<span>add-hook</span> <span>&#39;gptel-post-stream-hook</span> <span>&#39;gptel-auto-scroll</span>)</pre></div>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">I want the cursor to move to the next prompt after the response is inserted</h4><a id="user-content-i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted" aria-label="Permalink: I want the cursor to move to the next prompt after the response is inserted" href="#i-want-the-cursor-to-move-to-the-next-prompt-after-the-response-is-inserted"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">To be minimally annoying, gptel does not move the cursor by default.  Add the following to your configuration to move the cursor:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-hook &#39;gptel-post-response-functions &#39;gptel-end-of-response)"><pre>(<span>add-hook</span> <span>&#39;gptel-post-response-functions</span> <span>&#39;gptel-end-of-response</span>)</pre></div>
<p dir="auto">You can also call <code>gptel-end-of-response</code> as a command at any time.</p>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">I want to change the formatting of the prompt and LLM response</h4><a id="user-content-i-want-to-change-the-formatting-of-the-prompt-and-llm-response" aria-label="Permalink: I want to change the formatting of the prompt and LLM response" href="#i-want-to-change-the-formatting-of-the-prompt-and-llm-response"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">For dedicated chat buffers: customize <code>gptel-prompt-prefix-alist</code> and <code>gptel-response-prefix-alist</code>.  You can set a different pair for each major-mode.</p>
<p dir="auto">Anywhere in Emacs: Use <code>gptel-pre-response-hook</code> and <code>gptel-post-response-functions</code>, which see.</p>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">I want the transient menu options to be saved so I only need to set them once</h4><a id="user-content-i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once" aria-label="Permalink: I want the transient menu options to be saved so I only need to set them once" href="#i-want-the-transient-menu-options-to-be-saved-so-i-only-need-to-set-them-once"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">Any model options you set are saved for the current buffer.  But the redirection options in the menu are set for the next query only:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/298437596-2ecc6be9-aa52-4287-a739-ba06e1369ec2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI5ODQzNzU5Ni0yZWNjNmJlOS1hYTUyLTQyODctYTczOS1iYTA2ZTEzNjllYzIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDRiMzA3NDVhYzNmYzM2YzJkNzI5YmFjNTRjNmQwM2Q3YjJhMzc0YWVmMzYyMTM1NWM0ZTdjMzM4YzY4MDM4MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.qv_unKNDZErzbcI3oIkwFJpG-WB3NGKxHzpzIz9J_yA"><img src="https://private-user-images.githubusercontent.com/8607532/298437596-2ecc6be9-aa52-4287-a739-ba06e1369ec2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI5ODQzNzU5Ni0yZWNjNmJlOS1hYTUyLTQyODctYTczOS1iYTA2ZTEzNjllYzIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDRiMzA3NDVhYzNmYzM2YzJkNzI5YmFjNTRjNmQwM2Q3YjJhMzc0YWVmMzYyMTM1NWM0ZTdjMzM4YzY4MDM4MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.qv_unKNDZErzbcI3oIkwFJpG-WB3NGKxHzpzIz9J_yA" alt="https://github.com/karthink/gptel/assets/8607532/2ecc6be9-aa52-4287-a739-ba06e1369ec2"/></a>
<p dir="auto">You can make them persistent across this Emacs session by pressing <code>C-x C-s</code>:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/8607532/298438187-b8bcb6ad-c974-41e1-9336-fdba0098a2fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI5ODQzODE4Ny1iOGJjYjZhZC1jOTc0LTQxZTEtOTMzNi1mZGJhMDA5OGEyZmUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDY3OWM4NjMxNGMxMGY0OTI4NmQ2Nzk2NGRlNDZkZTJjNTViMTViMzMwYTIyZmNlNmNiMzRlZGM1NDZiNmMyYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Dg11q-L3hTlyykrQfkDtWhjQNPLL5gS1BZ-YHDwxj7U"><img src="https://private-user-images.githubusercontent.com/8607532/298438187-b8bcb6ad-c974-41e1-9336-fdba0098a2fe.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA2OTc3NTAsIm5iZiI6MTczMDY5NzQ1MCwicGF0aCI6Ii84NjA3NTMyLzI5ODQzODE4Ny1iOGJjYjZhZC1jOTc0LTQxZTEtOTMzNi1mZGJhMDA5OGEyZmUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTEwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMDRUMDUxNzMwWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDY3OWM4NjMxNGMxMGY0OTI4NmQ2Nzk2NGRlNDZkZTJjNTViMTViMzMwYTIyZmNlNmNiMzRlZGM1NDZiNmMyYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Dg11q-L3hTlyykrQfkDtWhjQNPLL5gS1BZ-YHDwxj7U" alt="https://github.com/karthink/gptel/assets/8607532/b8bcb6ad-c974-41e1-9336-fdba0098a2fe"/></a>
<p dir="auto">(You can also cycle through presets you’ve saved with <code>C-x p</code> and <code>C-x n</code>.)</p>
<p dir="auto">Now these will be enabled whenever you send a query from the transient menu.  If you want to use these saved options without invoking the transient menu, you can use a keyboard macro:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Replace with your key to invoke the transient menu:
(keymap-global-set &#34;&lt;f6&gt;&#34; &#34;C-u C-c &lt;return&gt; &lt;return&gt;&#34;)"><pre><span><span>;</span>; Replace with your key to invoke the transient menu:</span>
(keymap-global-set <span><span>&#34;</span>&lt;f6&gt;<span>&#34;</span></span> <span><span>&#34;</span>C-u C-c &lt;return&gt; &lt;return&gt;<span>&#34;</span></span>)</pre></div>
<p dir="auto">Or see this <a href="https://github.com/karthink/gptel/wiki/Commonly-requested-features#save-transient-flags">wiki entry</a>.</p>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">I want to use gptel in a way that’s not supported by <code>gptel-send</code> or the options menu</h4><a id="user-content-i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu" aria-label="Permalink: I want to use gptel in a way that’s not supported by gptel-send or the options menu" href="#i-want-to-use-gptel-in-a-way-thats-not-supported-by-gptel-send-or-the-options-menu"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">gptel’s default usage pattern is simple, and will stay this way: Read input in any buffer and insert the response below it.  Some custom behavior is possible with the transient menu (<code>C-u M-x gptel-send</code>).</p>
<p dir="auto">For more programmable usage, gptel provides a general <code>gptel-request</code> function that accepts a custom prompt and a callback to act on the response. You can use this to build custom workflows not supported by <code>gptel-send</code>.  See the documentation of <code>gptel-request</code>, and the <a href="https://github.com/karthink/gptel/wiki/Defining-custom-gptel-commands">wiki</a> for examples.</p>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">(Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode</h4><a id="user-content-doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode" aria-label="Permalink: (Doom Emacs) Sending a query from the gptel menu fails because of a key conflict with Org mode" href="#doom-emacs-sending-a-query-from-the-gptel-menu-fails-because-of-a-key-conflict-with-org-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<p dir="auto">Doom binds <code>RET</code> in Org mode to <code>+org/dwim-at-point</code>, which appears to conflict with gptel’s transient menu bindings for some reason.</p>
<p dir="auto">Two solutions:</p>
<ul dir="auto">
  <li>Press <code>C-m</code> instead of the return key.</li>
  <li>Change the send key from return to a key of your choice:
    <div dir="auto" data-snippet-clipboard-copy-content="(transient-suffix-put &#39;gptel-menu (kbd &#34;RET&#34;) :key &#34;&lt;f8&gt;&#34;)
    "><pre>(transient-suffix-put <span>&#39;gptel-menu</span> (<span>kbd</span> <span><span>&#34;</span>RET<span>&#34;</span></span>) <span>:key</span> <span><span>&#34;</span>&lt;f8&gt;<span>&#34;</span></span>)
    </pre></div>
  </li>
</ul>
</details>
<details><summary>
<div dir="auto"><h4 tabindex="-1" dir="auto">(ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”</h4><a id="user-content-chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota" aria-label="Permalink: (ChatGPT) I get the error “(HTTP/2 429) You exceeded your current quota”" href="#chatgpt-i-get-the-error-http2-429-you-exceeded-your-current-quota"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
</summary>
<blockquote>
  <p dir="auto">(HTTP/2 429) You exceeded your current quota, please check your plan and billing details.</p>
</blockquote>
<p dir="auto">Using the ChatGPT (or any OpenAI) API requires <a href="https://platform.openai.com/account/billing/overview" rel="nofollow">adding credit to your account</a>.</p>
</details>
<details><summary>

</summary>
<p dir="auto">Other Emacs clients for LLMs prescribe the format of the interaction (a comint shell, org-babel blocks, etc).  I wanted:</p>
<ol dir="auto">
  <li>Something that is as free-form as possible: query the model using any text in any buffer, and redirect the response as required.  Using a dedicated <code>gptel</code> buffer just adds some visual flair to the interaction.</li>
  <li>Integration with org-mode, not using a walled-off org-babel block, but as regular text.  This way the model can generate code blocks that I can run.</li>
</ol>
</details>

<markdown-accessiblity-table><table>
  <tbody><tr><th><b>Connection options</b></th><th></th></tr>
  <tr><td><code>gptel-use-curl</code></td><td>Use Curl (default), fallback to Emacs’ built-in <code>url</code>.</td></tr>
  <tr><td><code>gptel-proxy</code></td><td>Proxy server for requests, passed to curl via <code>--proxy</code>.</td></tr>
  <tr><td><code>gptel-api-key</code></td><td>Variable/function that returns the API key for the active backend.</td></tr>
  <tr><th><b>LLM request options</b></th><th><i>(Note: not supported uniformly across LLMs)</i></th></tr>
  <tr><td><code>gptel-backend</code></td><td>Default LLM Backend.</td></tr>
  <tr><td><code>gptel-model</code></td><td>Default model to use, depends on the backend.</td></tr>
  <tr><td><code>gptel-stream</code></td><td>Enable streaming responses, if the backend supports it.</td></tr>
  <tr><td><code>gptel-directives</code></td><td>Alist of system directives, can switch on the fly.</td></tr>
  <tr><td><code>gptel-max-tokens</code></td><td>Maximum token count (in query + response).</td></tr>
  <tr><td><code>gptel-temperature</code></td><td>Randomness in response text, 0 to 2.</td></tr>
  <tr><td><code>gptel-use-context</code></td><td>How/whether to include additional context</td></tr>
  <tr><th><b>Chat UI options</b></th><th></th></tr>
  <tr><td><code>gptel-default-mode</code></td><td>Major mode for dedicated chat buffers.</td></tr>
  <tr><td><code>gptel-track-response</code></td><td>Distinguish between user messages and LLM responses?</td></tr>
  <tr><td><code>gptel-track-media</code></td><td>Send images or other media from links?</td></tr>
  <tr><td><code>gptel-prompt-prefix-alist</code></td><td>Text inserted before queries.</td></tr>
  <tr><td><code>gptel-response-prefix-alist</code></td><td>Text inserted before responses.</td></tr>
  <tr><td><code>gptel-use-header-line</code></td><td>Display status messages in header-line (default) or minibuffer</td></tr>
  <tr><td><code>gptel-display-buffer-action</code></td><td>Placement of the gptel chat buffer.</td></tr>
  <tr><th><b>Org mode UI options</b></th><th></th></tr>
  <tr><td><code>gptel-org-branching-context</code></td><td>Make each outline path a separate conversation branch</td></tr>
  <tr><th><b>Hooks for customization</b></th><th></th></tr>
  <tr><td><code>gptel-save-state-hook</code></td><td>Runs before saving the chat state to a file on disk</td></tr>
  <tr><td><code>gptel-pre-response-hook</code></td><td>Runs before inserting the LLM response into the buffer</td></tr>
  <tr><td><code>gptel-post-response-functions</code></td><td>Runs after inserting the full LLM response into the buffer</td></tr>
  <tr><td><code>gptel-post-stream-hook</code></td><td>Runs after each streaming insertion</td></tr>
  <tr><td><code>gptel-context-wrap-function</code></td><td>To include additional context formatted your way</td></tr>
</tbody></table></markdown-accessiblity-table>

<p dir="auto">Other Emacs clients for LLMs include</p>
<ul dir="auto">
  <li><a href="https://github.com/ahyatt/llm">llm</a>: llm provides a uniform API across language model providers for building LLM clients in Emacs, and is intended as a library for use by package authors.  For similar scripting purposes, gptel provides the command <code>gptel-request</code>, which see.</li>
  <li><a href="https://github.com/s-kostyaev/ellama">Ellama</a>: A full-fledged LLM client built on llm, that supports many LLM providers (Ollama, Open AI, Vertex, GPT4All and more).  Its usage differs from gptel in that it provides separate commands for dozens of common tasks, like general chat, summarizing code/text, refactoring code, improving grammar, translation and so on.</li>
  <li><a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a>: comint-shell based interaction with ChatGPT.  Also supports DALL-E, executable code blocks in the responses, and more.</li>
  <li><a href="https://github.com/rksm/org-ai">org-ai</a>: Interaction through special <code>#+begin_ai ... #+end_ai</code> Org-mode blocks.  Also supports DALL-E, querying ChatGPT with the contents of project files, and more.</li>
</ul>
<p dir="auto">There are several more: <a href="https://github.com/MichaelBurge/leafy-mode">leafy-mode</a>, <a href="https://github.com/iwahbe/chat.el">chat.el</a></p>

<p dir="auto">gptel is a general-purpose package for chat and ad-hoc LLM interaction.  The following packages use gptel to provide additional or specialized functionality:</p>
<ul dir="auto">
  <li><a href="https://github.com/karthink/gptel-quick">gptel-quick</a>: Quickly look up the region or text at point.</li>
  <li><a href="https://github.com/daedsidog/evedel">Evedel</a>: Instructed LLM Programmer/Assistant</li>
  <li><a href="https://github.com/lanceberge/elysium">Elysium</a>: Automatically apply AI-generated changes as you code</li>
  <li><a href="https://github.com/kamushadenes/gptel-extensions.el">gptel-extensions</a>: Extra utility functions for gptel</li>
  <li><a href="https://github.com/kamushadenes/ai-blog.el">ai-blog.el</a>: Streamline generation of blog posts in Hugo</li>
  <li><a href="https://github.com/douo/magit-gptcommit">magit-gptcommit</a>: Generate Commit Messages within magit-status Buffer using gptel</li>
  <li><a href="https://github.com/armindarvish/consult-omni">consult-omni</a>: Versatile multi-source search package.  It includes gptel as one of its many sources.</li>
  <li><a href="https://github.com/ultronozm/ai-org-chat.el">ai-org-chat</a>: Provides branching conversations in Org buffers using gptel.  (Note that gptel includes this feature as well (see <code>gptel-org-branching-context</code>), but requires a recent version of Org mode (9.67 or later) to be installed.)</li>
</ul>

<ul dir="auto">
  <li><a href="https://github.com/algal">Alexis Gallagher</a> and <a href="https://github.com/d1egoaz">Diego Alvarez</a> for fixing a nasty multi-byte bug with <code>url-retrieve</code>.</li>
  <li><a href="https://github.com/tarsius">Jonas Bernoulli</a> for the Transient library.</li>
  <li><a href="https://github.com/daedsidog">daedsidog</a> for adding context support to gptel.</li>
  <li><a href="https://github.com/Aquan1412">Aquan1412</a> for adding PrivateGPT support to gptel.</li>
  <li><a href="https://github.com/r0man">r0man</a> for improving gptel’s Curl integration.</li>
</ul>
</article></div></div>
  </body>
</html>
