<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/">Original</a>
    <h1>Coral NPU: A full-stack platform for Edge AI</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20251015">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="tf5oi">Generative AI has fundamentally reshaped our expectations of technology. We&#39;ve seen the power of large-scale cloud-based models to create, reason and assist in incredible ways. However, the next great technological leap isn&#39;t just about making cloud models bigger; it&#39;s about embedding their intelligence directly into our immediate, personal environment. For AI to be truly assistive — proactively helping us navigate our day, translating conversations in real-time, or understanding our physical context — it must run on the devices we wear and carry. This presents a core challenge: embedding ambient AI onto battery-constrained edge devices, freeing them from the cloud to enable truly private, all-day assistive experiences.</p><p data-block-key="1a1a">To move from the cloud to personal devices, we must solve three critical problems:</p><ul><li data-block-key="eu56"><i>The performance gap:</i> Complex, state-of-the-art machine learning (ML) models demand more compute, far exceeding the limited power, thermal, and memory budgets of an edge device.</li><li data-block-key="7buqm"><i>The fragmentation tax:</i> Compiling and optimizing ML models for a diverse landscape of proprietary processors is difficult and costly, hindering consistent performance across devices.</li><li data-block-key="a3ap1"><i>The user trust deficit:</i> To be truly helpful, personal AI must prioritize the privacy and security of personal data and context.</li></ul><p data-block-key="ejp6o">Today we introduce <a href="https://developers.google.com/coral" target="_blank" rel="noopener noreferrer">Coral NPU</a>, a full-stack platform that builds on our original work from <a href="https://gweb-coral-full.uc.r.appspot.com/" target="_blank" rel="noopener noreferrer">Coral</a> to provide hardware designers and ML developers with the tools needed to build the next generation of private, efficient edge AI devices. Co-designed in partnership with Google Research and Google DeepMind, Coral NPU is an AI-first hardware architecture built to enable the next generation of ultra-low-power, always-on edge AI. It offers a unified developer experience, making it easier to deploy applications like ambient sensing. It&#39;s specifically designed to enable all-day AI on wearable devices while minimizing battery usage and being configurable for higher performance use cases. We’ve released our <a href="https://developers.google.com/coral" target="_blank" rel="noopener noreferrer">documentation and tools</a> so that developers and designers can start building today.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Coral NPU: An AI-first architecture</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">Developers building for low-power edge devices face a fundamental trade-off, choosing between general purpose CPUs and specialized accelerators. General-purpose CPUs offer crucial flexibility and broad software support but lack the domain-specific architecture for demanding ML workloads, making them less performant and power-inefficient. Conversely, specialized accelerators provide high ML efficiency but are inflexible, difficult to program, and ill-suited for general tasks.</p><p data-block-key="1fta9">This hardware problem is magnified by a highly fragmented software ecosystem. With starkly different programming models for CPUs and ML blocks, developers are often forced to use proprietary compilers and complex command buffers. This creates a steep learning curve and makes it difficult to combine the unique strengths of different compute units. Consequently, the industry lacks a mature, low-power architecture that can easily and effectively support multiple ML development frameworks.</p><p data-block-key="a5efi">The Coral NPU architecture directly addresses this by reversing traditional chip design. It prioritizes the ML matrix engine over scalar compute, optimizing architecture for AI from silicon up and creating a platform purpose-built for more efficient, on-device inference.</p><p data-block-key="43d11">As a complete, reference <a href="https://en.wikipedia.org/wiki/Neural_processing_unit" target="_blank" rel="noopener noreferrer">neural processing unit</a> (NPU) architecture, Coral NPU provides the building blocks for the next generation of energy-efficient, ML-optimized <a href="https://en.wikipedia.org/wiki/System_on_a_chip" target="_blank" rel="noopener noreferrer">systems on chip</a> (SoCs). The architecture is based on a set of <a href="https://riscv.org/specifications/ratified/" target="_blank" rel="noopener noreferrer">RISC-V ISA</a> compliant architectural IP blocks and is designed for minimal power consumption, making it ideal for always-on ambient sensing. The base design delivers performance in the 512 <a href="https://en.wikipedia.org/wiki/Floating_point_operations_per_second" target="_blank" rel="noopener noreferrer">giga operations per second</a> (GOPS) range while consuming just a few milliwatts, thus enabling powerful on-device AI for edge devices, hearables, AR glasses, and smartwatches.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="tf5oi">The open and extensible <a href="https://developers.google.com/coral/guides/architecture" target="_blank" rel="noopener noreferrer">architecture</a> based on RISC-V gives SoC designers flexibility to modify the base design, or use it as a pre-configured NPU. The Coral NPU architecture includes the following components:</p><ul><li data-block-key="bclpo"><i>A scalar core:</i> A lightweight, C-programmable RISC-V frontend that manages data flow to the back-end cores, using a simple &#34;run-to-completion&#34; model for ultra-low power consumption and traditional CPU functions.</li><li data-block-key="1u961"><i>A vector execution unit:</i> A robust single instruction multiple data (<a href="https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data" target="_blank" rel="noopener noreferrer">SIMD</a>) co-processor compliant with the RISC-V Vector instruction set (RVV) v1.0, enabling simultaneous operations on large data sets.</li><li data-block-key="2ju85"><i>A matrix execution unit:</i> A highly efficient quantized outer product <a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation" target="_blank" rel="noopener noreferrer">multiply-accumulate</a> (MAC) engine purpose-built to accelerate fundamental neural network operations. Note that the matrix execution unit is still under development and will be released on <a href="https://github.com/google-coral/coralnpu" target="_blank" rel="noopener noreferrer">GitHub</a> later this year.</li></ul>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Unified developer experience</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">The Coral NPU architecture is a simple, C-programmable target that can seamlessly integrate with modern compilers like <a href="https://iree.dev/" target="_blank" rel="noopener noreferrer">IREE</a> and <a href="https://github.com/tensorflow/tflite-micro" target="_blank" rel="noopener noreferrer">TFLM</a>. This enables easy support for ML frameworks like <a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow</a>, <a href="https://docs.jax.dev/en/latest/" target="_blank" rel="noopener noreferrer">JAX</a>, and <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch</a>.</p><p data-block-key="46n8n">Coral NPU incorporates a comprehensive software toolchain, including specialized solutions like the TFLM compiler for TensorFlow, alongside a general-purpose <a href="https://mlir.llvm.org/" target="_blank" rel="noopener noreferrer">MLIR</a> compiler, C compiler, custom kernels, and a simulator. This provides developers with flexible pathways. For example, a model from a framework like JAX is first imported into the MLIR format using the <a href="https://openxla.org/stablehlo" target="_blank" rel="noopener noreferrer">StableHLO</a> dialect. This intermediate file is then fed into the IREE compiler, which applies a hardware-specific plug-in to recognize the Coral NPU&#39;s architecture. From there, the compiler performs progressive lowering — a critical optimization step where the code is systematically translated through a series of dialects, moving closer to the machine&#39;s native language. After optimization, the toolchain generates a final, compact binary file ready for efficient execution on the edge device. This suite of industry-standard developer tools helps simplify the programming of ML models and can allow for a consistent experience across various hardware targets.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="tf5oi">Coral NPU’s co-design process focuses on two key areas. First, the architecture efficiently accelerates the leading encoder-based architectures used in today&#39;s on-device vision and audio applications. Second, we are collaborating closely with the <a href="https://deepmind.google/models/gemma/" target="_blank" rel="noopener noreferrer">Gemma</a> team to optimize Coral NPU for small transformer models, helping to ensure the accelerator architecture supports the next generation of generative AI at the edge.</p><p data-block-key="brs14">This dual focus means Coral NPU is on track to be the first open, standards-based, low-power NPU designed to bring LLMs to wearables. For developers, this provides a single, validated path to deploy both current and future models with maximum performance at minimal power.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Target applications</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">Coral NPU is designed to enable ultra-low-power, always-on edge AI applications, particularly focused on ambient sensing systems. Its primary goal is to enable all day AI-experiences on wearables, mobile phones and <a href="https://en.wikipedia.org/wiki/Internet_of_things" target="_blank" rel="noopener noreferrer">Internet of Things</a> (IoT) devices minimizing battery usage.</p><p data-block-key="54nt5">Potential use cases include:</p><ul><li data-block-key="7himc"><i>Contextual awareness:</i> Detecting user activity (e.g., walking, running), proximity, or environment (e.g., indoors/outdoors, on-the-go) to enable &#34;do-not-disturb&#34; modes or other context-aware features.</li><li data-block-key="ff339"><i>Audio processing:</i> Voice and speech detection, keyword spotting, live translation, transcription, and audio-based accessibility features.</li><li data-block-key="ai106"><i>Image processing:</i> Person and object detection, facial recognition, gesture recognition, and low-power visual search.</li><li data-block-key="9m2mn"><i>User interaction:</i> Enabling control via hand gestures, audio cues, or other sensor-driven inputs.</li></ul>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Hardware-enforced privacy</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">A core principle of Coral NPU is building user trust through hardware-enforced security. Our architecture is being designed to support emerging technologies like <a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/" target="_blank" rel="noopener noreferrer">CHERI</a>, which provides fine-grained memory-level safety and scalable software compartmentalization. With this approach, we hope to enable sensitive AI models and personal data to be isolated in a hardware-enforced sandbox, mitigating memory-based attacks.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Building an ecosystem</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">Open hardware projects rely on strong partnerships to succeed. To that end, we’re collaborating with <a href="https://www.synaptics.com/" target="_blank" rel="noopener noreferrer">Synaptics</a>, our first strategic silicon partner and a leader in embedded compute, wireless connectivity, and multimodal sensing for the IoT. Today, at their Tech Day, Synaptics announced their new <a href="https://www.synaptics.com/sl2610-press-release" target="_blank" rel="noopener noreferrer">Astra™ SL2610</a> line of <a href="https://www.synaptics.com/sl2610-product-line" target="_blank" rel="noopener noreferrer">AI-Native IoT Processors</a>. This product line features their <a href="https://www.synaptics.com/torq-github" target="_blank" rel="noopener noreferrer">Torq™ NPU</a> subsystem, the industry’s first production implementation of the Coral NPU architecture. The NPU’s design is transformer-capable and supports dynamic operators, enabling developers to build future-ready Edge AI systems for consumer and industrial IoT.</p><p data-block-key="4r0e3">This partnership supports our commitment to a unified developer experience. The Synaptics Torq™ Edge AI platform is built on an open-source compiler and runtime based on IREE and MLIR. This collaboration is a significant step toward building a shared, open standard for intelligent, context-aware devices.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Solving core crises of the Edge</h2>
            
        
        
    </p>



    <p data-block-key="tf5oi">With Coral NPU, we are building a foundational layer for the future of personal AI. Our goal is to foster a vibrant ecosystem by providing a common, open-source, and secure platform for the industry to build upon. This empowers developers and silicon vendors to move beyond today&#39;s fragmented landscape and collaborate on a shared standard for edge computing, enabling faster innovation. Learn more about <a href="https://developers.google.com/coral" target="_blank" rel="noopener noreferrer">Coral NPU</a> and start building today.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Acknowledgements</h2>
            
        
        
    </p>



    <p data-block-key="9lzmb"><i>We would like to thank the core contributors and leadership team for this work, particularly Billy Rutledge, Ben Laurie, Derek Chow, Michael Hoang, Naveen Dodda, Murali Vijayaraghavan, Gregory Kielian, Matthew Wilson, Bill Luan, Divya Pandya, Preeti Singh, Akib Uddin, Stefan Hall, Alex Van Damme, David Gao, Lun Dong, Julian Mullings-Black, Roman Lewkow, Shaked Flur, Yenkai Wang, Reid Tatge, Tim Harvey, Tor Jeremiassen, Isha Mishra, Kai Yick, Cindy Liu, Bangfei Pan, Ian Field, Srikanth Muroor, Jay Yagnik, Avinatan Hassidim, and Yossi Matias.</i></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
