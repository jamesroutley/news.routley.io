<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/">Original</a>
    <h1>Nvidia DGX GH200: The First 100 Terabyte GPU Memory System</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>At <a href="https://www.computextaipei.com.tw/en/index.html" data-wpel-link="external" target="_blank" rel="follow external noopener">COMPUTEX 2023</a>, NVIDIA announced <a href="http://www.nvidia.com/dgx-gh200" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA DGX GH200</a>, which marks another breakthrough in GPU-accelerated computing to power the most demanding giant AI workloads. In addition to describing critical aspects of the NVIDIA DGX GH200 architecture, this post discusses how <a href="https://www.nvidia.com/en-us/data-center/base-command/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Base Command</a> enables rapid deployment, accelerates the onboarding of users, and simplifies system management.</p>



<p>The unified memory programming model of GPUs has been the cornerstone of various breakthroughs in complex accelerated computing applications over the last 7 years. In 2016, NVIDIA introduced <a href="https://www.nvidia.com/en-us/data-center/nvlink/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVLink</a> technology and the Unified Memory Programming model with CUDA-6, designed to increase the memory available to GPU-accelerated workloads. </p>



<p>Since then, the core of every DGX system is a GPU complex on a baseboard interconnected with NVLink in which each GPU can access the other’s memory at NVLink speed. Many such DGX with GPU complexes are interconnected with high-speed networking to form larger supercomputers such as the <a href="https://www.youtube.com/watch?v=vY61ExKhnfA&amp;t=5s" data-wpel-link="external" target="_blank" rel="follow external noopener">NVIDIA Selene supercomputer</a>. Yet an emerging class of giant, trillion-parameter AI models will require either several months to train or cannot be solved even on today’s best supercomputers. </p>



<p>To empower the scientists in need of an advanced platform that can solve these extraordinary challenges, NVIDIA paired<a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/" data-wpel-link="internal" target="_self" rel="noopener noreferrer"> NVIDIA Grace Hopper Superchip</a> with the NVLink Switch System, uniting up to 256 GPUs in an NVIDIA DGX GH200 system. In the DGX GH200 system, 144 terabytes of memory will be accessible to the GPU shared memory programming model at high speed over NVLink. </p>



<p>Compared to a single <a href="https://resources.nvidia.com/en-us-dgx-systems/nvidia-dgx-a100-system-40gb-datasheet-web-us" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA DGX A100 320 GB system</a>, NVIDIA DGX GH200 provides nearly 500x more memory to the GPU shared memory programming model over NVLink, forming a giant data center-sized GPU. NVIDIA DGX GH200 is the first supercomputer to break the 100-terabyte barrier for memory accessible to GPUs over NVLink.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1199" height="742" src="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1.png" alt="Linear graph illustrating the gains made in GPU memory as a result of NVLink technology progression." srcset="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1.png 1199w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-300x186.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-625x387.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-179x111.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-768x475.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-645x399.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-485x300.png 485w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-145x90.png 145w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-362x224.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-178x110.png 178w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/gpu-memory-nvlink-1-1024x634.png 1024w" sizes="(max-width: 1199px) 100vw, 1199px"/><figcaption><em>Figure 1. GPU memory gains as a result of NVLink progression</em> </figcaption></figure></div>


<h2>NVIDIA DGX GH200 system architecture</h2>



<p>NVIDIA Grace Hopper Superchip and NVLink Switch System are the building blocks of NVIDIA DGX GH200 architecture. NVIDIA Grace Hopper Superchip combines the Grace and Hopper architectures using<a href="https://www.nvidia.com/en-us/data-center/nvlink-c2c/" data-wpel-link="internal" target="_self" rel="noopener noreferrer"> NVIDIA NVLink-C2C</a> to deliver a CPU + GPU coherent memory model. The NVLink Switch System, powered by the fourth generation of NVLink technology, extends NVLink connection across superchips to create a seamless, high-bandwidth, multi-GPU system.</p>



<p>Each NVIDIA Grace Hopper Superchip in NVIDIA DGX GH200 has 480 GB LPDDR5 CPU memory, at eighth of the power per GB, compared with DDR5 and 96 GB of fast HBM3. NVIDIA Grace CPU and Hopper GPU are interconnected with NVLink-C2C, providing 7x more bandwidth than PCIe Gen5 at one-fifth the power. </p>



<p>NVLink Switch System forms a two-level, non-blocking, fat-tree NVLink fabric to fully connect 256 Grace Hopper Superchips in a DGX GH200 system. Every GPU in DGX GH200 can access the memory of other GPUs and extended GPU memory of all NVIDIA Grace CPUs at 900 GBps. </p>



<p>Compute baseboards hosting Grace Hopper Superchips are connected to the NVLink Switch System using a custom cable harness for the first layer of NVLink fabric. LinkX cables extend the connectivity in the second layer of NVLink fabric. </p>


<div>
<figure><img decoding="async" loading="lazy" width="960" height="540" src="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200.png" alt="Diagram illustrating the topology of a fully connected NVIDIA NVLink Switch System across NVIDIA DGX GH200 consisting of 256 GPUs: 36 NVLink switches." srcset="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200.png 960w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-625x352.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-768x432.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-645x363.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-500x281.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/diagram-topology-nvlink-switch-system-dgx-gh200-196x110.png 196w" sizes="(max-width: 960px) 100vw, 960px"/><figcaption><em>Figure 2. Topology of a fully connected NVIDIA NVLink Switch System across NVIDIA DGX GH200 consisting of 256 GPUs</em></figcaption></figure></div>


<p>In the DGX GH200 system, GPU threads can address peer HBM3 and LPDDR5X memory from other Grace Hopper Superchips in the NVLink network using an NVLink page table. <a href="https://www.nvidia.com/en-us/data-center/magnum-io/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Magnum IO</a> acceleration libraries optimize GPU communications for efficiency, enhancing application scaling with all 256 GPUs. </p>



<p>Every Grace Hopper Superchip in DGX GH200 is paired with one <a href="https://nvdam.widen.net/s/csf8rmnqwl/infiniband-ethernet-datasheet-connectx-7-ds-nv-us-2544471" data-wpel-link="external" target="_blank" rel="follow external noopener">NVIDIA ConnectX-7</a> network adapter and one <a href="https://resources.nvidia.com/en-us-accelerated-networking-resource-library/datasheet-nvidia-bluefield?lx=LbHvpR&amp;topic=networking-cloud" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA BlueField-3 NIC</a>. The DGX GH200 has 128 TBps bi-section bandwidth and 230.4 TFLOPS of NVIDIA SHARP in-network computing to accelerate collective operations commonly used in AI and doubles the effective bandwidth of the NVLink Network System by reducing the communication overheads of collective operations.</p>



<p>For scaling beyond 256 GPUs, ConnectX-7 adapters can interconnect multiple DGX GH200 systems to scale into an even larger solution. The power of BlueField-3 DPUs transforms any enterprise computing environment into a secure and accelerated virtual private cloud, enabling organizations to run application workloads in secure, multi-tenant environments.</p>



<h2>Target use cases and performance benefits</h2>



<p>The generational leap in GPU memory significantly improves the performance of AI and HPC applications bottlenecked by GPU memory size. Many mainstream AI and HPC workloads can reside entirely in the aggregate GPU memory of a single <a href="https://www.nvidia.com/en-us/data-center/dgx-h100/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA DGX H100</a>. For such workloads, the DGX H100 is the most performance-efficient training solution. </p>



<p>Other workloads—such as a deep learning recommendation model (DLRM) with terabytes of embedded tables, a terabyte-scale graph neural network training model, or large data analytics workloads—see speedups of 4x to 7x with DGX GH200. This shows that DGX GH200 is a better solution for the more advanced AI and HPC models requiring massive memory for GPU shared memory programming. </p>



<p>The mechanics of speedup are described in detail in the<a href="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper" data-wpel-link="internal" target="_self" rel="noopener noreferrer"> NVIDIA Grace Hopper Superchip Architecture</a> whitepaper.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1999" height="1129" src="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads.png" alt="Bar graph compares the performance gains between an NVIDIA DGX H100 Cluster with NVIDIA InfiniBand and an NVIDIA DGX GH200 with NVLink Switch System when applied to large AI models that impose giant memory demands for particular workloads, including emerging NLP, larger recommender systems, graph neural networks, graph analytics and data analytics workloads." srcset="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads.png 1999w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-300x169.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-625x353.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-179x101.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-768x434.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-1536x868.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-645x364.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-500x282.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-160x90.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-362x204.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-195x110.png 195w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/performance-comparisons-giant-memory-ai-model-workloads-1024x578.png 1024w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>Figure 3. Performance comparisons for giant memory AI workloads</em></figcaption></figure></div>


<h2>Purpose-designed for the most demanding workloads</h2>



<p>Every component throughout DGX GH200 is selected to minimize bottlenecks while maximizing network performance for key workloads and fully utilizing all scale-up hardware capabilities. The result is linear scalability and high utilization of the massive, shared memory space. </p>



<p>To get the most out of this advanced system, NVIDIA also architected an extremely high-speed storage fabric to run at peak capacity and to handle a variety of data types (text, tabular data, audio, and video)—in parallel and with unwavering performance. </p>



<h2>Full-stack NVIDIA solution</h2>



<p>DGX GH200 comes with <a href="https://www.nvidia.com/en-us/data-center/base-command/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Base Command</a>, which includes an OS optimized for AI workloads, cluster manager, libraries that accelerate compute, storage, and network infrastructure are optimized for DGX GH200 system architecture. </p>



<p>DGX GH200 also includes <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA AI Enterprise</a>, providing a suite of software and frameworks optimized to streamline AI development and deployment. This full-stack solution enables customers to focus on innovation and worry less about managing their IT infrastructure.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1934" height="940" src="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack.png" alt="Diagram illustrates the full stack of software and software platforms that are included with the NVIDIA DGX GH200 AI supercomputer. The stack includes NVIDIA AI Enterprise software suite for developers, NVIDIA Base Command OS platform that includes AI workflow management, enterprise-grade cluster management, libraries that accelerate compute, storage, and network infrastructure, and system software optimized for running AI workloads." srcset="https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack.png 1934w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-300x146.png 300w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-625x304.png 625w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-179x87.png 179w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-768x373.png 768w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-1536x747.png 1536w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-645x313.png 645w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-500x243.png 500w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-160x78.png 160w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-362x176.png 362w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-226x110.png 226w, https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/nvidia-dgx-gh200-ai-supercomputer-full-stack-1024x498.png 1024w" sizes="(max-width: 1934px) 100vw, 1934px"/><figcaption><em>Figure 4. The NVIDIA DGX GH200 AI supercomputer full stack includes NVIDIA Base Command and NVIDIA AI Enterprise</em></figcaption></figure></div>


<h2>Supercharge giant AI and HPC workloads</h2>



<p>NVIDIA is working to make DGX GH200 available at the end of this year. NVIDIA is eager to provide this incredible first-of-its-kind supercomputer and empower you to innovate and pursue your passions in solving today’s biggest AI and HPC challenges. <a href="http://www.nvidia.com/dgx-gh200" data-wpel-link="internal" target="_self" rel="noopener noreferrer">Learn more</a>.</p>
</div></div>
  </body>
</html>
