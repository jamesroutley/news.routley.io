<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/Pearl">Original</a>
    <h1>Pearl: A Production-Ready Reinforcement Learning Agent</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Pearl/blob/main/logo/pearl_long.png"><img src="https://github.com/facebookresearch/Pearl/raw/main/logo/pearl_long.png" alt="alt"/></a></p>

<h3 tabindex="-1" dir="auto"><a id="user-content-proudly-brought-by-applied-reinforcement-learning--meta" aria-hidden="true" tabindex="-1" href="#proudly-brought-by-applied-reinforcement-learning--meta"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Proudly brought by Applied Reinforcement Learning @ Meta</h3>
<ul dir="auto">
<li>v0.1 - Pearl beta-version is now released! Announcements: <a href="https://x.com/ZheqingZhu/status/1732880717263352149?s=20" rel="nofollow">Twitter Post</a>, <a href="https://www.linkedin.com/posts/zheqingzhubill_github-facebookresearchpearl-a-production-ready-activity-7138647748102258688-rz-g?utm_source=share&amp;utm_medium=member_desktop" rel="nofollow">LinkedIn Post</a></li>
</ul>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"/></a>
<a href="https://opensource.fb.com/support-ukraine" rel="nofollow"><img src="https://camo.githubusercontent.com/b4bfe90546cd79d85029a3743921a92fe788c2d802292fe762bc872eda0b4711/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f537570706f72742d556b7261696e652d4646443530303f7374796c653d666c6174266c6162656c436f6c6f723d303035424242" alt="Support Ukraine" data-canonical-src="https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;labelColor=005BBB"/></a></p>
<p dir="auto">More details of the library at our <a href="https://pearlagent.github.io" rel="nofollow">official website</a>.</p>
<p dir="auto">The Pearl paper is <a href="https://chs6.short.gy/pearl_paper" rel="nofollow">available at Arxiv</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-overview" aria-hidden="true" tabindex="-1" href="#overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto">Pearl is a new production-ready Reinforcement Learning AI agent library open-sourced by the Applied Reinforcement Learning team at Meta. Furthering our efforts on open AI innovation, Pearl enables researchers and practitioners to develop Reinforcement Learning AI agents. These AI agents prioritize cumulative long-term feedback over immediate feedback and can adapt to environments with limited observability, sparse feedback, and high stochasticity. We hope that Pearl offers the community a means to build state-of-the-art Reinforcement Learning AI agents that can adapt to a wide range of complex production environments.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" tabindex="-1" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h3>
<p dir="auto">To install Pearl, you can simply clone this repo and pip install</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/facebookresearch/Pearl.git
cd Pearl
pip install -e ."><pre>git clone https://github.com/facebookresearch/Pearl.git
<span>cd</span> Pearl
pip install -e <span>.</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-hidden="true" tabindex="-1" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h3>
<p dir="auto">NeurIPS EXPO Tutorial Colab: <a href="https://github.com/PearlAgent/pearl_neurips_demo/blob/main/demo.ipynb">Link</a></p>
<p dir="auto">To kick off a Pearl agent with a classic reinforcement learning environment, here&#39;s a quick example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pearl.pearl_agent import PearlAgent
from pearl.action_representation_modules.one_hot_action_representation_module import (
    OneHotActionTensorRepresentationModule,
)
from pearl.policy_learners.sequential_decision_making.deep_q_learning import (
    DeepQLearning,
)
from pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (
    FIFOOffPolicyReplayBuffer,
)
from pearl.utils.instantiations.environments.gym_environment import GymEnvironment

env = GymEnvironment(&#34;CartPole-v1&#34;)

num_actions = env.action_space.n
agent = PearlAgent(
    policy_learner=DeepQLearning(
        state_dim=env.observation_space.shape[0],
        action_space=env.action_space,
        hidden_dims=[64, 64],
        training_rounds=20,
        action_representation_module=OneHotActionTensorRepresentationModule(
            max_number_actions=num_actions
        ),
    ),
    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),
)

observation, action_space = env.reset()
agent.reset(observation, action_space)
done = False
while not done:
    action = agent.act(exploit=False)
    action_result = env.step(action)
    agent.observe(action_result)
    agent.learn()
    done = action_result.done"><pre><span>from</span> <span>pearl</span>.<span>pearl_agent</span> <span>import</span> <span>PearlAgent</span>
<span>from</span> <span>pearl</span>.<span>action_representation_modules</span>.<span>one_hot_action_representation_module</span> <span>import</span> (
    <span>OneHotActionTensorRepresentationModule</span>,
)
<span>from</span> <span>pearl</span>.<span>policy_learners</span>.<span>sequential_decision_making</span>.<span>deep_q_learning</span> <span>import</span> (
    <span>DeepQLearning</span>,
)
<span>from</span> <span>pearl</span>.<span>replay_buffers</span>.<span>sequential_decision_making</span>.<span>fifo_off_policy_replay_buffer</span> <span>import</span> (
    <span>FIFOOffPolicyReplayBuffer</span>,
)
<span>from</span> <span>pearl</span>.<span>utils</span>.<span>instantiations</span>.<span>environments</span>.<span>gym_environment</span> <span>import</span> <span>GymEnvironment</span>

<span>env</span> <span>=</span> <span>GymEnvironment</span>(<span>&#34;CartPole-v1&#34;</span>)

<span>num_actions</span> <span>=</span> <span>env</span>.<span>action_space</span>.<span>n</span>
<span>agent</span> <span>=</span> <span>PearlAgent</span>(
    <span>policy_learner</span><span>=</span><span>DeepQLearning</span>(
        <span>state_dim</span><span>=</span><span>env</span>.<span>observation_space</span>.<span>shape</span>[<span>0</span>],
        <span>action_space</span><span>=</span><span>env</span>.<span>action_space</span>,
        <span>hidden_dims</span><span>=</span>[<span>64</span>, <span>64</span>],
        <span>training_rounds</span><span>=</span><span>20</span>,
        <span>action_representation_module</span><span>=</span><span>OneHotActionTensorRepresentationModule</span>(
            <span>max_number_actions</span><span>=</span><span>num_actions</span>
        ),
    ),
    <span>replay_buffer</span><span>=</span><span>FIFOOffPolicyReplayBuffer</span>(<span>10_000</span>),
)

<span>observation</span>, <span>action_space</span> <span>=</span> <span>env</span>.<span>reset</span>()
<span>agent</span>.<span>reset</span>(<span>observation</span>, <span>action_space</span>)
<span>done</span> <span>=</span> <span>False</span>
<span>while</span> <span>not</span> <span>done</span>:
    <span>action</span> <span>=</span> <span>agent</span>.<span>act</span>(<span>exploit</span><span>=</span><span>False</span>)
    <span>action_result</span> <span>=</span> <span>env</span>.<span>step</span>(<span>action</span>)
    <span>agent</span>.<span>observe</span>(<span>action_result</span>)
    <span>agent</span>.<span>learn</span>()
    <span>done</span> <span>=</span> <span>action_result</span>.<span>done</span></pre></div>
<p dir="auto">More detailed tutorial will be presented at NeurIPS 2023 EXPO presentation (12/10/2023, 4 pm to 6 pm). Users can replace the environment with any real-world problems.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-design-and-features" aria-hidden="true" tabindex="-1" href="#design-and-features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Design and Features</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/Pearl/blob/main/logo/agent_interface.png"><img src="https://github.com/facebookresearch/Pearl/raw/main/logo/agent_interface.png" alt="alt"/></a>
Pearl was built with a modular design so that industry practitioners or academic researchers can select any subset and flexibly combine features below to construct a Pearl agent customized for their specific use cases. Pearl offers a diverse set of unique features for production environments, including dynamic action spaces, offline learning, intelligent neural exploration, safe decision making, history summarization, and data augmentation.</p>
<p dir="auto">You can find many Pearl agent candidates with mix-and-match set of reinforcement learning features in utils/scripts/benchmark_config.py</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-adoption-in-real-world-applications" aria-hidden="true" tabindex="-1" href="#adoption-in-real-world-applications"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Adoption in Real-world Applications</h2>
<p dir="auto">Pearl is in progress supporting real-world applications, including recommender systems, auction bidding systems and creative selection. Each of them requires a subset of features offered by Pearl. To visualize the subset of features used by each of the applications above, see the table below.</p>

<table>
<thead>
<tr>
<th>Pearl Features</th>
<th>Recommender Systems</th>
<th>Auction Bidding</th>
<th>Creative Selection</th>
</tr>
</thead>
<tbody>
<tr>
<td>Policy Learning</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Intelligent Exploration</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Safety</td>
<td></td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td>History Summarization</td>
<td></td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td>Replay Buffer</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>Contextual Bandit</td>
<td></td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td>Offline RL</td>
<td>✅</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td>Dynamic Action Space</td>
<td>✅</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td>Large-scale Neural Network</td>
<td>✅</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<h2 tabindex="-1" dir="auto"><a id="user-content-comparison-to-other-libraries" aria-hidden="true" tabindex="-1" href="#comparison-to-other-libraries"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Comparison to Other Libraries</h2>

<table>
<thead>
<tr>
<th>Pearl Features</th>
<th>Pearl</th>
<th>ReAgent (Superseded by Pearl)</th>
<th>RLLib</th>
<th>SB3</th>
<th>Tianshou</th>
<th>Dopamine</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agent Modularity</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Dynamic Action Space</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Offline RL</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>Intelligent Exploration</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>⚪ (limited support)</td>
<td>❌</td>
</tr>
<tr>
<td>Contextual Bandit</td>
<td>✅</td>
<td>✅</td>
<td>⚪ (only linear support)</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Safe Decision Making</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>History Summarization</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>❌</td>
<td>⚪ (requires modifying environment state)</td>
<td>❌</td>
</tr>
<tr>
<td>Data Augmented Replay Buffer</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
</tr>
</tbody>
</table>

<h2 tabindex="-1" dir="auto"><a id="user-content-cite-us" aria-hidden="true" tabindex="-1" href="#cite-us"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Cite Us</h2>
<div data-snippet-clipboard-copy-content="@misc{pearl2023paper,
    title = {Pearl: A Production-ready Reinforcement Learning AI Agent Library},
    author = {Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Ruiyang Xu, Liyuan Wang, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu},
    year = 2023,
    eprint = {arXiv:2310.07786}
}"><pre><code>@misc{pearl2023paper,
    title = {Pearl: A Production-ready Reinforcement Learning AI Agent Library},
    author = {Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Ruiyang Xu, Liyuan Wang, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu},
    year = 2023,
    eprint = {arXiv:2310.07786}
}
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" tabindex="-1" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">Pearl is MIT licensed, as found in the LICENSE file.</p>
</article>
          </div></div>
  </body>
</html>
