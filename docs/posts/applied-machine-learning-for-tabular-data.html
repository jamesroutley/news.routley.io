<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aml4td.org/">Original</a>
    <h1>Applied Machine Learning for Tabular Data</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-document-content">




<section id="preface">

<p>Welcome! This is a work in progress. We want to create a practical guide to developing quality predictive models from tabular data. We’ll publish materials here as we create them and welcome community contributions in the form of discussions, suggestions, and edits.</p>
<p>We also want these materials to be reusable and open. The sources are in the source <a href="https://github.com/aml4td/website">GitHub repository</a> with a Creative Commons license attached (see below).</p>
<p>Our intention is to write these materials and, when we feel we’re done, pick a publishing partner to produce a print version.</p>
<p>The book takes a holistic view of the predictive modeling process and focuses on a few areas that are usually left out of similar works. For example, the effectiveness of the model can be driven by how the predictors are represented. Because of this, we tightly couple feature engineering methods with machine learning models. Also, quite a lot of work happens after we have determined our best model and created the final fit. These post-modeling activities are an important part of the model development process and will be described in detail.</p>
<p>We deliberately avoid using the term “artificial intelligence.” Eugen Rochko’s (<code>@Gargron@mastodon.social</code>) comment on <a href="https://mastodon.social/@Gargron/111554885513300997">Mastodon</a> does a good job of summarizing our reservations regarding the term:</p>
<blockquote>
<p>It’s hard not to say “AI” when everybody else does too, but technically calling it AI is buying into the marketing. There is no intelligence there, and it’s not going to become sentient. It’s just statistics, and the danger they pose is primarily through the false sense of skill or fitness for purpose that people ascribe to them.</p>
</blockquote>
<p>To cite this website, we suggest:</p>
<div data-layout-align="center">
<pre><code>@online{aml4td,
  author = {Kuhn, M and Johnson, K},
  title = {{Applied Machine Learning for Tabular Data}},
  year = {2023},
  url = { https://aml4td.org},
  urldate = {2024-05-06}
}</code></pre>
</div>
<section id="license">
<h2 data-anchor-id="license">License</h2>
<p xmlns:cc="http://creativecommons.org/ns#">
This work is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0<img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"/><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"/><img src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"/><img src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"/></a>
</p>
<p>This license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only. If others modify or adapt the material, they must license the modified material under identical terms.</p>
<ul>
<li>BY: Credit must be given to you, the creator.</li>
<li>NC: Only noncommercial use of your work is permitted. Noncommercial means not primarily intended for or directed towards commercial advantage or monetary compensation.</li>
<li>SA: Adaptations must be shared under the same terms.</li>
</ul>
<p>Our goal is to have an open book where people can reuse and reference the materials but can’t just put their names on them and resell them (without our permission).</p>
</section>
<section id="intended-audience">
<h2 data-anchor-id="intended-audience">Intended Audience</h2>
<p>Our intended audience includes data analysts of many types: statisticians, data scientists, professors and instructors of machine learning courses, laboratory scientists, and anyone else who desires to understand how to create a model for prediction. We don’t expect readers to be experts in these methods or the math behind them. Instead, our approach throughout this work is applied. That is, we want readers to use this material to build intuition about the predictive modeling process. What are good and bad ideas for the modeling process? What pitfalls should we look out for? How can we be confident that the model will be predictive for new samples? What are advantages and disadvantages of different types of models? These are just some of the questions that this work will address.</p>
<p>Some background in modeling and statistics will be extremely useful. Having seen or used basic regression models is good, and an understanding of basic statistical concepts such as variance, correlation, populations, samples, etc., is needed. There will also be some mathematical notation, so you’ll need to be able to grasp these abstractions. But we will keep this to those parts where it is absolutely necessary. There are a few more statistically sophisticated sections for some of the more advanced topics.</p>
<p>If you would like a more theoretical treatment of machine learning models, then we recommend <span data-cites="HastieEtAl2017">Hastie, Tibshirani, and Friedman (<a href="#ref-HastieEtAl2017" role="doc-biblioref">2017</a>)</span>. Other books for gaining a more in-depth understanding of machine learning are <span data-cites="bishop2006pattern">Bishop and Nasrabadi (<a href="#ref-bishop2006pattern" role="doc-biblioref">2006</a>)</span>, <span data-cites="arnold2019computational">Arnold, Kane, and Lewis (<a href="#ref-arnold2019computational" role="doc-biblioref">2019</a>)</span> and, for more of a deep learning focus, <span data-cites="goodfellow2016deep">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016deep" role="doc-biblioref">2016</a>)</span> and/or <span data-cites="udl2023">Prince (<a href="#ref-udl2023" role="doc-biblioref">2023</a>)</span>.</p>
</section>
<section id="is-there-code">
<h2 data-anchor-id="is-there-code">Is there code?</h2>
<p>We definitely want to decouple the content of this work from specific software. <a href="http://appliedpredictivemodeling.com/">One of our other books</a> on modeling had computing sections. Many people found these sections to be a useful resource at the time of the book’s publication. However, code can quickly become outdated in today’s computational environment. In addition, this information takes up a lot of page space that would be better used for other topics.</p>
<p>We will create <em>computing supplements</em> to go along with the materials. Since we use R’s tidymodels framework for calculations, the supplement currently in-progress is:</p>
<ul>
<li><a href="https://tidymodels.aml4td.org"><code>tidymodels.aml4td.org</code></a></li>
</ul>
<p>If you are interested in working on a python/scikit-learn supplement, please <a href="https://github.com//aml4td/website/issues">file an issue</a></p>
</section>
<section id="are-there-exercises">
<h2 data-anchor-id="are-there-exercises">Are there exercises?</h2>
<p>Many readers found the Exercise sections of <em>Applied Predictive Modeling</em> to be helpful for solidifying the concepts presented in each chapter. The current set can be found at <a href="https://exercises.aml4td.org"><code>exercises.aml4td.org</code></a></p>
</section>
<section id="how-can-i-ask-questions">
<h2 data-anchor-id="how-can-i-ask-questions">How can I ask questions?</h2>
<p>If you have questions about the content, it is probably best to ask on a public forum, like <a href="https://stats.stackexchange.com/">cross-validated</a>. You’ll most likely get a faster answer there if you take the time to ask the questions in the best way possible.</p>
<p>If you want a direct answer from us, you should follow what I call <a href="https://yihui.org/en/2017/08/so-gh-email/"><em>Yihui’s Rule</em></a>: add an issue to GitHub (labeled as “Discussion”) first. It may take some time for us to get back to you.</p>
<p>If you think there is a bug, please <a href="https://github.com//aml4td/website/issues">file an issue</a>.</p>
</section>
<section id="can-i-contribute">
<h2 data-anchor-id="can-i-contribute">Can I contribute?</h2>
<p>There is a <a href="https://aml4td.org/chapters/contributing.html">contributing page</a> with details on how to get up and running to compile the materials (there are a lot of software dependencies) and suggestions on how to help.</p>
<p>If you just want to fix a typo, you can make a pull request to alter the appropriate <code>.qmd</code> file.</p>
<p>Please feel free to improve the quality of this content by submitting <strong>pull requests</strong>. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are <strong>not considered an author, copyright holder, or owner</strong> of the content once it has been merged in.</p>
</section>
<section id="computing-notes">
<h2 data-anchor-id="computing-notes">Computing Notes</h2>
<p><a href="https://quarto.org/">Quarto</a> was used to compile and render the materials</p>
<div data-layout-align="center">
<pre><code>Quarto 1.4.521
[✓] Checking versions of quarto binary dependencies...
      Pandoc version 3.1.9: OK
      Dart Sass version 1.69.5: OK
      Deno version 1.37.2: OK
[✓] Checking versions of quarto dependencies......OK
[✓] Checking Quarto installation......OK
      Version: 1.4.521
[✓] Checking tools....................OK
      TinyTeX: (external install)
      Chromium: (not installed)
[✓] Checking LaTeX....................OK
      Using: TinyTex
      Version: 2023
[✓] Checking basic markdown render....OK
[✓] Checking Python 3 installation....OK
      Version: 3.11.7
      Jupyter: 5.7.1
      Kernels: python3
[✓] Checking Jupyter engine render....OK
[✓] Checking R installation...........OK
      Version: 4.3.2
      LibPaths:
      knitr: 1.46
      rmarkdown: 2.25
[✓] Checking Knitr engine render......OK</code></pre>
</div>
<p><a href="https://en.wikipedia.org/wiki/R_(programming_language)">R version 4.3.2 (2023-10-31)</a> was used for the majority of the computations. <a href="https://en.wikipedia.org/wiki/Torch_(machine_learning)">torch</a> 2.0.1 was also used. The versions of the primary R modeling and visualization packages used here are:</p>
<div data-layout-align="center">
<div>
<table>
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr>
<td><code>aorsf</code> (0.1.4)</td>
<td><code>applicable</code> (0.1.1)</td>
<td><code>aspline</code> (0.2.0)</td>
</tr>
<tr>
<td><code>baguette</code> (1.0.2)</td>
<td><code>bestNormalize</code> (1.9.1)</td>
<td><code>bibtex</code> (0.5.1)</td>
</tr>
<tr>
<td><code>bonsai</code> (0.2.1)</td>
<td><code>broom</code> (1.0.5)</td>
<td><code>brulee</code> (0.3.0)</td>
</tr>
<tr>
<td><code>C50</code> (0.1.8)</td>
<td><code>Cubist</code> (0.4.2.1)</td>
<td><code>DALEXtra</code> (2.3.0)</td>
</tr>
<tr>
<td><code>dbarts</code> (0.9-28)</td>
<td><code>ddalpha</code> (1.3.15)</td>
<td><code>desirability2</code> (0.0.1)</td>
</tr>
<tr>
<td><code>dials</code> (1.2.1)</td>
<td><code>dimRed</code> (0.2.6)</td>
<td><code>discrim</code> (1.0.1)</td>
</tr>
<tr>
<td><code>doMC</code> (1.3.8)</td>
<td><code>dplyr</code> (1.1.4)</td>
<td><code>e1071</code> (1.7-14)</td>
</tr>
<tr>
<td><code>earth</code> (5.3.3)</td>
<td><code>embed</code> (1.1.4)</td>
<td><code>fastICA</code> (1.2-4)</td>
</tr>
<tr>
<td><code>finetune</code> (1.2.0)</td>
<td><code>GA</code> (3.2.4)</td>
<td><code>gganimate</code> (1.0.9)</td>
</tr>
<tr>
<td><code>ggforce</code> (0.4.2)</td>
<td><code>ggiraph</code> (0.8.9)</td>
<td><code>ggplot2</code> (3.5.1)</td>
</tr>
<tr>
<td><code>glmnet</code> (4.1-8)</td>
<td><code>gt</code> (0.10.1)</td>
<td><code>hardhat</code> (1.3.1)</td>
</tr>
<tr>
<td><code>ipred</code> (0.9-14)</td>
<td><code>irlba</code> (2.3.5.1)</td>
<td><code>kernlab</code> (0.9-32)</td>
</tr>
<tr>
<td><code>kknn</code> (1.3.1)</td>
<td><code>klaR</code> (1.7-3)</td>
<td><code>lightgbm</code> (4.3.0)</td>
</tr>
<tr>
<td><code>mda</code> (0.5-4)</td>
<td><code>mgcv</code> (1.9-1)</td>
<td><code>mixOmics</code> (6.25.1)</td>
</tr>
<tr>
<td><code>modeldata</code> (1.3.0)</td>
<td><code>modeldatatoo</code> (0.3.0)</td>
<td><code>pamr</code> (1.56.2)</td>
</tr>
<tr>
<td><code>parsnip</code> (1.2.1)</td>
<td><code>partykit</code> (1.2-20)</td>
<td><code>patchwork</code> (1.2.0)</td>
</tr>
<tr>
<td><code>plsmod</code> (1.0.0)</td>
<td><code>probably</code> (1.0.3)</td>
<td><code>pROC</code> (1.18.5)</td>
</tr>
<tr>
<td><code>purrr</code> (1.0.2)</td>
<td><code>ragg</code> (1.3.1)</td>
<td><code>ranger</code> (0.16.0)</td>
</tr>
<tr>
<td><code>recipes</code> (1.0.10)</td>
<td><code>rpart</code> (4.1.23)</td>
<td><code>rsample</code> (1.2.1)</td>
</tr>
<tr>
<td><code>RSpectra</code> (0.16-1)</td>
<td><code>rstudioapi</code> (0.16.0)</td>
<td><code>rules</code> (1.0.2)</td>
</tr>
<tr>
<td><code>shinylive</code> (0.1.1)</td>
<td><code>sparsediscrim</code> (0.3.0)</td>
<td><code>sparseLDA</code> (0.1-9)</td>
</tr>
<tr>
<td><code>spatialsample</code> (0.5.1)</td>
<td><code>splines2</code> (0.5.1)</td>
<td><code>stacks</code> (1.0.4)</td>
</tr>
<tr>
<td><code>stopwords</code> (2.3)</td>
<td><code>textrecipes</code> (1.0.6.9000)</td>
<td><code>themis</code> (1.0.2)</td>
</tr>
<tr>
<td><code>tidymodels</code> (1.2.0)</td>
<td><code>tidyposterior</code> (1.0.1)</td>
<td><code>tidyr</code> (1.3.1)</td>
</tr>
<tr>
<td><code>torch</code> (0.12.0)</td>
<td><code>tune</code> (1.2.1)</td>
<td><code>usethis</code> (2.2.3)</td>
</tr>
<tr>
<td><code>uwot</code> (0.2.2)</td>
<td><code>VBsparsePCA</code> (0.1.0)</td>
<td><code>viridis</code> (0.6.5)</td>
</tr>
<tr>
<td><code>workflows</code> (1.1.4)</td>
<td><code>workflowsets</code> (1.1.0)</td>
<td><code>xgboost</code> (1.7.7.1)</td>
</tr>
<tr>
<td><code>xrf</code> (0.2.2)</td>
<td><code>yardstick</code> (1.3.1)</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="chapter-references">
<h2 data-anchor-id="chapter-references">Chapter References</h2>


<div id="refs" data-entry-spacing="0" role="list">
<p>
Arnold, T, M Kane, and B Lewis. 2019. <em>A Computational Approach to Statistical Learning</em>. Chapman; Hall/CRC.
</p>
<p>
Bishop, C M, and N M Nasrabadi. 2006. <em><span>Pattern Recognition and Machine Learning</span></em>. Vol. 4. 4. Springer.
</p>
<p>
Goodfellow, I, Y Bengio, and A Courville. 2016. <em><a href="https://www.deeplearningbook.org">Deep Learning</a></em>. MIT press.
</p>
<p>
Hastie, T, R Tibshirani, and J Friedman. 2017. <em><span>The Elements of Statistical Learning: Data Mining, Inference and Prediction</span></em>. Springer.
</p>
<div id="ref-udl2023" role="listitem"><p>
Prince, S. 2023. <em><a href="https://udlbook.github.io/udlbook/">Understanding Deep Learning</a></em>. MIT press.
</p></div>
</div>
</section>
</section>

</div></div>
  </body>
</html>
