<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2305.10947">Original</a>
    <h1>In Defense of Pure 16-Bit Floating-Point Neural Networks</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2305.10947">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Reducing the number of bits needed to encode the weights and activations of
neural networks is highly desirable as it speeds up their training and
inference time while reducing memory consumption. For these reasons, research
in this area has attracted significant attention toward developing neural
networks that leverage lower-precision computing, such as mixed-precision
training. Interestingly, none of the existing approaches has investigated pure
16-bit floating-point settings. In this paper, we shed light on the overlooked
efficiency of pure 16-bit floating-point neural networks. As such, we provide a
comprehensive theoretical analysis to investigate the factors contributing to
the differences observed between 16-bit and 32-bit models. We formalize the
concepts of floating-point error and tolerance, enabling us to quantitatively
explain the conditions under which a 16-bit model can closely approximate the
results of its 32-bit counterpart. This theoretical exploration offers
perspective that is distinct from the literature which attributes the success
of low-precision neural networks to its regularization effect. This in-depth
analysis is supported by an extensive series of experiments. Our findings
demonstrate that pure 16-bit floating-point neural networks can achieve similar
or even better performance than their mixed-precision and 32-bit counterparts.
We believe the results presented in this paper will have significant
implications for machine learning practitioners, offering an opportunity to
reconsider using pure 16-bit networks in various applications.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Juyoung Yun [<a href="https://arxiv.org/show-email/004de542/2305.10947">view email</a>]
      </p></div></div>
  </body>
</html>
