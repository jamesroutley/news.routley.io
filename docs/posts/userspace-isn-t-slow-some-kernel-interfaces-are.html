<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tailscale.com/blog/throughput-improvements/">Original</a>
    <h1>Userspace isn&#39;t slow, some kernel interfaces are</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p><strong>We made significant improvements to the throughput of <a href="https://git.zx2c4.com/wireguard-go/about/">wireguard-go</a>, which is the userspace <a href="https://www.wireguard.com/">WireGuard</a>® implementation that Tailscale uses. What this means for you: improved performance of the Tailscale client on Linux.</strong> We intend to <a href="https://github.com/WireGuard/wireguard-go/pull/64">upstream these changes</a> to WireGuard as well.</p>
<p>You can experience these improvements in the current <a href="https://pkgs.tailscale.com/unstable/">unstable Tailscale client release</a>, and also in Tailscale v1.36, available in early 2023. Read on to learn how we did it, or jump down to the <a href="#results">Results</a> section if you just want numbers.</p>
<h2 id="background">Background</h2>
<p>The Tailscale client leverages wireguard-go, a userspace WireGuard implementation written in Go, for dataplane functionality. In Tailscale, wireguard-go receives unencrypted packets from the kernel, encrypts them, and sends them over a UDP socket to another WireGuard peer. The inverse flow is flipped — when receiving communications from a peer, wireguard-go first reads encrypted packets from a UDP socket, then decrypts them, and writes them back to the kernel. This is a simplified view of the pipeline inside of wireguard-go — the Tailscale client <a href="https://tailscale.com/blog/how-tailscale-works/">adds additional functionality</a>, such as NAT traversal, access control, and key distribution.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/wireguard-go-data-flow.svg"/>
    
</figure>

<h2 id="baseline">Baseline</h2>
<p>Network performance is a complicated topic in large part because networked applications can have drastically different requirements and goals. In this post, we will focus on throughput. By throughput, we mean the amount of data that can be transferred between Tailscale clients within a given timeframe.</p>
<p><strong>Disclaimer about benchmarks</strong>: This post contains benchmarks! These benchmarks are reproducible at the time of writing, and we provide details about the environments we ran them in. Benchmark results tend to vary across environments, and they also tend to go stale as time progresses. Your mileage may vary.</p>
<p>We’ll start with some baseline numbers for wireguard-go and in-kernel WireGuard. Toward the end we will show results of our changes. Throughput tests are conducted using <a href="https://github.com/esnet/iperf">iperf3</a> over a single TCP stream, with cubic-flavored congestion control. Ubuntu 22.04 is the operating system on all hosts.</p>
<p>For these baseline tests, we’ll use two c6i.8xlarge virtual hosts in AWS. These instances have fast network interfaces and sufficient CPU capacity to handle encryption at network speeds. They are in the same region and availability zone:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ ec2metadata <span>|</span> grep -E <span>&#39;instance-type:|availability-zone:&#39;</span>
</span></span><span><span>availability-zone: us-west-2d
</span></span><span><span>instance-type: c6i.8xlarge
</span></span><span><span>
</span></span><span><span>ubuntu@thru7:~$ ec2metadata <span>|</span> grep -E <span>&#39;instance-type:|availability-zone:&#39;</span>
</span></span><span><span>availability-zone: us-west-2d
</span></span><span><span>instance-type: c6i.8xlarge
</span></span><span><span>
</span></span><span><span>ubuntu@thru6:~$ ping 172.31.56.191 -c <span>5</span> -q
</span></span><span><span>PING 172.31.56.191 <span>(</span>172.31.56.191<span>)</span> 56<span>(</span>84<span>)</span> bytes of data.
</span></span><span><span>
</span></span><span><span>--- 172.31.56.191 ping statistics ---
</span></span><span><span><span>5</span> packets transmitted, <span>5</span> received, 0% packet loss, <span>time</span> 4099ms
</span></span><span><span>rtt min/avg/max/mdev <span>=</span> 0.098/0.119/0.150/0.017 ms
</span></span></code></pre></div><p>This first benchmark does not use wireguard-go. It sets a throughput baseline without any WireGuard overhead:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ iperf3 -i <span>0</span> -c 172.31.56.191 -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux thru6 5.15.0-1026-aws <span>#30-Ubuntu SMP Wed Nov 23 14:15:21 UTC 2022 x86_64</span>
</span></span><span><span>Control connection MSS <span>8949</span>
</span></span><span><span>Time: Thu, <span>08</span> Dec <span>2022</span> 19:29:39 GMT
</span></span><span><span>Connecting to host 172.31.56.191, port <span>5201</span>
</span></span><span><span>      Cookie: dcnjnuzjeobo4dne6djnj3waeq4dugc2fh7a
</span></span><span><span>      TCP MSS: <span>8949</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 172.31.51.101 port <span>40158</span> connected to 172.31.56.191 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  11.1 GBytes  9.53 Gbits/sec    <span>0</span>   1.29 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  11.1 GBytes  9.53 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  11.1 GBytes  9.49 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 10.0% <span>(</span>0.2%u/9.7%s<span>)</span>, remote/receiver 4.4% <span>(</span>0.3%u/4.0%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>This second benchmark uses in-kernel WireGuard:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ iperf3 -i <span>0</span> -c thru7-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux thru6 5.15.0-1026-aws <span>#30-Ubuntu SMP Wed Nov 23 14:15:21 UTC 2022 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>08</span> Dec <span>2022</span> 19:58:24 GMT
</span></span><span><span>Connecting to host thru7-wg, port <span>5201</span>
</span></span><span><span>      Cookie: o5iu6xoxq47swoubx5un32monokel573kj6i
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.6 port <span>46284</span> connected to 10.9.9.7 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.09 GBytes  2.66 Gbits/sec   <span>81</span>    <span>987</span> KBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  3.09 GBytes  2.66 Gbits/sec   <span>81</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.05  sec  3.09 GBytes  2.64 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 5.2% <span>(</span>0.2%u/5.0%s<span>)</span>, remote/receiver 5.9% <span>(</span>0.1%u/5.8%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>Now, over <a href="https://git.zx2c4.com/wireguard-go/tree/?id=bb719d3a6e2cd20ec00f26d65c0073c1dde6b529">wireguard-go@bb719d3</a>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ iperf3 -i <span>0</span> -c thru7-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux thru6 5.15.0-1026-aws <span>#30-Ubuntu SMP Wed Nov 23 14:15:21 UTC 2022 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>08</span> Dec <span>2022</span> 19:30:49 GMT
</span></span><span><span>Connecting to host thru7-wg, port <span>5201</span>
</span></span><span><span>      Cookie: zg7hsb2jrbklpaqez2gzhdi2kyxr4ibne5lf
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.6 port <span>51660</span> connected to 10.9.9.7 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  2.82 GBytes  2.42 Gbits/sec  <span>4711</span>    <span>415</span> KBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  2.82 GBytes  2.42 Gbits/sec  <span>4711</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  2.82 GBytes  2.41 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 5.7% <span>(</span>0.2%u/5.5%s<span>)</span>, remote/receiver 7.3% <span>(</span>0.6%u/6.7%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>One thing that’s interesting to note: The TCP MSS is much higher on the first test (more on MSS/MTU later if you are unfamiliar). AWS supports a 9001 byte IP MTU. What happens when we increase the MTU on the wireguard-go interface?</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ iperf3 -i <span>0</span> -c thru7-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux thru6 5.15.0-1026-aws <span>#30-Ubuntu SMP Wed Nov 23 14:15:21 UTC 2022 x86_64</span>
</span></span><span><span>Control connection MSS <span>8869</span>
</span></span><span><span>Time: Thu, <span>08</span> Dec <span>2022</span> 19:33:21 GMT
</span></span><span><span>Connecting to host thru7-wg, port <span>5201</span>
</span></span><span><span>      Cookie: ov4nsnsdfxomict4cu2cxy2iwt4ncpi364d4
</span></span><span><span>      TCP MSS: <span>8869</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.6 port <span>43416</span> connected to 10.9.9.7 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  9.17 GBytes  7.88 Gbits/sec  <span>1854</span>   1.54 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  9.17 GBytes  7.88 Gbits/sec  <span>1854</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  9.17 GBytes  7.84 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 14.6% <span>(</span>0.2%u/14.4%s<span>)</span>, remote/receiver 26.6% <span>(</span>1.4%u/25.3%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div><p>Interesting! More than a 3x improvement in throughput. This suggests that per-packet overhead is quite high. Let’s grab some Linux perf data to confirm.</p>
<h3 id="linux-perf-and-flame-graphs">Linux perf and flame graphs</h3>
<p>We can analyze performance using <a href="https://perf.wiki.kernel.org/index.php/Main_Page">Linux perf</a> to better understand where CPU time is spent. <a href="https://www.brendangregg.com/flamegraphs.html">Flame graphs</a> can be rendered from the perf data, and they help us visualize the stack traces. The wider the function, the more expensive it (and/or its children) are. The flame graphs below are <a href="https://github.com/spiermar/d3-flame-graph">interactive</a>. You can click to zoom and hover to see percentages. The first flame graph is from the sender:</p>
<figure data-flamegraph="" data-flamegraph-json="sender.json" data-flamegraph-title="Sender"></figure>

<p>A large portion of CPU time (unrelated to crypto) on the sender is spent in:</p>
<ol>
<li>
<p><code>sendmsg()</code> on the UDP socket</p>
</li>
<li>
<p><code>write()</code> towards the TUN driver</p>
</li>
<li>
<p><code>read()</code> from the TUN driver</p>
</li>
</ol>
<p>Now, for the receiver:</p>
<figure data-flamegraph="" data-flamegraph-json="receiver.json" data-flamegraph-title="Receiver" data-flamegraph-min-frame-size="12"></figure>

<p>A large portion of CPU time (unrelated to crypto) on the receiver is spent in:</p>
<ol>
<li>
<p><code>write()</code> towards the TUN driver</p>
</li>
<li>
<p><code>recvmsg()</code> on the UDP socket</p>
</li>
<li>
<p><code>sendmsg()</code> on the UDP socket</p>
</li>
</ol>
<p>This confirms that per-packet overhead is high. By increasing the MTU of the TUN interface, we reduced the frequency of the system calls for I/O on the TUN and UDP socket. Since those dominate these profiles, it makes sense that throughput would increase as a result. So how do we reduce the frequency of these system calls while still maintaining an MTU that will work across the general internet?</p>
<h3 id="tcp-segmentation">TCP Segmentation</h3>
<p>TCP enables transmission of an arbitrary stream of bytes between two sockets. A userspace application interacts with a TCP socket using <code>write()</code>- and <code>read()</code>-like kernel interfaces once it is in a connected state. The application may <code>write()</code> 2 bytes, followed later by a 2,000 byte <code>write()</code>. The applications on either end do not need to be involved in the retransmission, ordering, or framing of messages between them. These are all handled by the TCP implementation on the kernel side of the system calls.</p>
<p>While the application writes arbitrary-sized data per system call, the TCP implementation must segment the data before it gets sent over the network. There is a finite limit to the size of a TCP segment, called the maximum segment size (MSS). MSS is advertised during the TCP three-way handshake. The lowest value wins, and neither side will transmit a segment exceeding it. MSS is typically derived from the maximum transmission unit (MTU) of the egress network interface. The MTU represents the maximum size of a single packet at the network layer. MSS describes the segment size limit at a higher layer protocol (TCP) than MTU (typically IP), and should always be less than it as a result.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/mtu-limited.svg"/>
    
</figure>

<p>There are consequences when a packet exceeds the MTU of a network path. The network device enforcing the limit may choose to fragment the packet into multiple smaller packets, or just drop the oversized packet. Both of these outcomes have negative impacts on performance. There are also mechanisms for signaling to the endpoints that their frames are too large, which we will not go into here. In summary, there is a finite packet size for packet-switched networks, and TCP implementations try to respect it.</p>
<p>Knowing that TCP is responsible for segmenting the data, we can picture the journey of packets through the host to look roughly something like this:</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/data-transmission.svg"/>
    
</figure>

<p>There is a nonzero cost for each of these layers to handle an individual TCP segment. If we can reduce the number of traversals through this stack, we can win back a lot of CPU time and pipeline latency. Enter TCP segmentation offload (TSO).</p>
<h3 id="tcp-segmentation-offload-tso">TCP Segmentation Offload (TSO)</h3>
<p>TSO is an offloading technique where the final fit-within-MSS segmentation is performed by the network interface device. This enables up to 64KB sized segments to traverse the stack, while still fitting within MSS before entering the network. <a href="https://lwn.net/Articles/883713/">Recent work in the Linux kernel</a> extends the 64KB limit to 256KB. TSO requires extra metadata to be passed along with the oversized segment describing the size to segment to, along with where to find the TCP checksum field. The TCP checksum must be recomputed post-segmentation as the payload is shortened, and various TCP header fields will differ between segments.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/data-transmission-tso.svg"/>
    
</figure>

<p>With a typical MTU of 1500 bytes and IPv4 TCP MSS of 1460 bytes, TSO could be used to reduce stack traversals by up to 44x.</p>
<p>Note: Modern Linux makes use of generic segmentation offload (GSO). This enables the networking stack to postpone segmentation as late as possible, even if the driver and device do not support TSO.</p>
<p>But, what about the other direction?</p>
<p>Generic receive offload (GRO) is the inverse of TSO. The network interface device coalesces packets together that belong to the same TCP stream, following a set of rules to prevent breaking the TCP implementation.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/data-receive-gro.svg"/>
    
</figure>

<h3 id="discovery-of-tso-and-gro-in-the-tun-driver">(Discovery of) TSO and GRO in the TUN driver</h3>
<p>The Linux kernel contains a <a href="https://docs.kernel.org/networking/tuntap.html">network device driver referred to as TUN/TAP</a>. This driver is used in wireguard-go in order to present the application as a network device to the kernel. That is, a packet sent out of the TUN interface is received by the userspace application, and the userspace application can inject packets back toward the kernel by writing in the other direction (received by the TUN interface).</p>
<p>When we set out to improve performance, we started by reading the TUN driver code. We had initially hoped to use a multi-message API via a <a href="https://github.com/torvalds/linux/blob/f443e374ae131c168a065ea1748feac6b2e76613/drivers/net/tun.c#L3666">packet socket</a>, but unfortunately the kernel does not expose this to userspace. Instead, we started to explore the <a href="https://github.com/torvalds/linux/blob/f443e374ae131c168a065ea1748feac6b2e76613/drivers/net/tun.c#L2803">set_offload() function</a>. This function is responsible for controlling the offloads supported by the TUN device, and we could enable TSO/GRO through it via <code>ioctl()</code>. This functionality has been in the <a href="https://github.com/torvalds/linux/commit/f43798c27684ab925adde7d8acc34c78c6e50df8">Linux kernel since v2.6.27</a> (2008), but seems to have gone largely unnoticed outside of the kernel-side virtio framework uses that it was originally added for.</p>
<p>With TSO/GRO enabled on the TUN, the application acting as the TUN device becomes responsible for implementing the offload techniques (segmentation and coalescing). Once we’ve segmented, where do we transmit the smaller segments? What represents the “network” sitting on the other side of wireguard-go and tailscaled? The answer is “it depends” in the Tailscale client, but typically it’s a UDP socket. After receiving packets from the TUN device, wireguard-go handles encryption prior to transmission out of said UDP socket. The same is true in reverse: We receive packets from a UDP socket, they are decrypted, and then written back toward the TUN.</p>
<h3 id="sendmmsg-and-recvmmsg"><code>sendmmsg()</code> and <code>recvmmsg()</code></h3>
<p>The <code>sendmmsg()</code> and <code>recvmmsg()</code> system calls enable the transmission and reception of multiple messages in a single system call. With a vector of packets now available from reads at the TUN driver, we can leverage <code>sendmmsg()</code> when transmitting out of the UDP socket. The inverse direction starts with <code>recvmmsg()</code> at the UDP socket, potentially returning multiple packets, which are candidates for coalescing just before writing to the TUN driver. Putting this together with TSO and GRO, we are able to reduce I/O system calls on both ends of the pipeline.</p>

    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/improved-throughput.svg"/>
    
</figure>

<h2 id="results">Results</h2>
<p>And now for the results!</p>
<p>Applying TCP segmentation offload, generic receive offload, and the mmsg() system calls resulted in significant throughput performance improvements in wireguard-go, and so also in the Tailscale client. Using the same tests we conducted previously, we delivered a best case 2.2x improvement to wireguard-go. And, we improved the throughput of Tailscale on Linux by up to 33%. We intend to continue working on improving the performance of Tailscale in all areas, including throughput, as well as across more platforms.</p>
<p>wireguard-go with TSO, GRO, and mmsg():</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>ubuntu@thru6:~$ iperf3 -i <span>0</span> -c thru7-wg -t <span>10</span> -C cubic -V
</span></span><span><span>iperf 3.9
</span></span><span><span>Linux thru6 5.15.0-1026-aws <span>#30-Ubuntu SMP Wed Nov 23 14:15:21 UTC 2022 x86_64</span>
</span></span><span><span>Control connection MSS <span>1368</span>
</span></span><span><span>Time: Thu, <span>08</span> Dec <span>2022</span> 19:39:43 GMT
</span></span><span><span>Connecting to host thru7-wg, port <span>5201</span>
</span></span><span><span>      Cookie: y4x75uvr2uupa3urdguks6m5ovn2ucdrjxrs
</span></span><span><span>      TCP MSS: <span>1368</span> <span>(</span>default<span>)</span>
</span></span><span><span><span>[</span>  5<span>]</span> <span>local</span> 10.9.9.6 port <span>58314</span> connected to 10.9.9.7 port <span>5201</span>
</span></span><span><span>Starting Test: protocol: TCP, <span>1</span> streams, <span>131072</span> byte blocks, omitting <span>0</span> seconds, <span>10</span> second test, tos <span>0</span>
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr  Cwnd
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  6.24 GBytes  5.36 Gbits/sec    <span>0</span>   3.02 MBytes
</span></span><span><span>- - - - - - - - - - - - - - - - - - - - - - - - -
</span></span><span><span>Test Complete. Summary Results:
</span></span><span><span><span>[</span> ID<span>]</span> Interval           Transfer     Bitrate         Retr
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.00  sec  6.24 GBytes  5.36 Gbits/sec    <span>0</span>             sender
</span></span><span><span><span>[</span>  5<span>]</span>   0.00-10.04  sec  6.24 GBytes  5.33 Gbits/sec                  receiver
</span></span><span><span>CPU Utilization: local/sender 9.1% <span>(</span>0.1%u/9.0%s<span>)</span>, remote/receiver 0.5% <span>(</span>0.0%u/0.5%s<span>)</span>
</span></span><span><span>snd_tcp_congestion cubic
</span></span><span><span>rcv_tcp_congestion cubic
</span></span></code></pre></div>
    
    

<figure>
        <img src="https://tailscale.com/blog/throughput-improvements/results.svg" alt="Single TCP stream iperf3 test, OS: Ubuntu 22.04, Instance Type: c6i.8xlarge, Kernel: 5.15.0-1026-aws"/>
    
</figure>

<p>Surprisingly, we improved the performance of wireguard-go (running in userspace) enough to make it faster than WireGuard (running in the kernel) in the best conditions. But, this point of comparison likely won’t be long-lived: we expect the kernel can do similar things.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In our journey to overcome our biggest overhead in packet processing, we came very close to wanting a new or different kernel interface. We gladly found that one was already available in the Linux kernel — and one that has been around long enough for us to use everywhere. Performance can always become somewhat of an arms race, but our results here demonstrate that we can keep up with our kernel counterparts provided that we are using the right kind of kernel interface – userspace isn’t slow, some kernel interfaces are!</p>
<p>Thanks to <a href="https://github.com/sailorfrag">Adrian Dewhurst</a> for his detailed review and thanks to <a href="https://www.zx2c4.com/">Jason A. Donenfeld</a> for his ongoing review of our patches. Thanks to our designers <a href="https://dannypagano.com/">Danny Pagano</a> for the illustrations, and <a href="https://rosszurowski.com">Ross Zurowski</a> for incorporating d3-flame-graph.</p>
<p><em>To learn more, watch <a href="https://www.youtube.com/watch?v=wCqXYPQFNuE">our discussion with Jordan Whited and James Tucker</a> on improving Tailscale’s throughput.</em></p>
    </div></div>
  </body>
</html>
