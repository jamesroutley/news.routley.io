<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bigscience.huggingface.co/blog/bloom">Original</a>
    <h1>BLOOM: The largest open multilingual language model</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Large language models (LLMs) have made a significant impact on AI research. These powerful, general models can take on a wide variety of new language tasks from a user’s instructions. However, academia, nonprofits and smaller companies&#39; research labs find it difficult to create, study, or even use LLMs as only a few industrial labs with the necessary resources and exclusive rights can fully access them. Today, we release <a href="https://huggingface.co/bigscience/bloom">BLOOM</a>, the first multilingual LLM trained in complete transparency, to change this status quo — the result of the largest collaboration of AI researchers ever involved in a single research project.</p>
<p>With its 176 billion parameters, BLOOM is able to generate text in 46 natural languages and 13 programming languages. For almost all of them, such as Spanish, French and Arabic, BLOOM will be the first language model with over 100B parameters ever created. This is the culmination of a year of work involving over 1000 researchers from 70+ countries and 250+ institutions, leading to a final run of 117 days (March 11 - July 6) training the BLOOM model on the <a href="http://www.idris.fr/eng/info/missions-eng.html">Jean Zay supercomputer</a> in the south of Paris, France thanks to a compute grant worth an estimated €3M from French research agencies CNRS and GENCI.</p>
<p>Researchers can <a href="https://huggingface.co/bigscience/bloom">now download, run and study BLOOM</a> to investigate the performance and behavior of recently developed large language models down to their deepest internal operations. More generally, any individual or institution who agrees to the terms of the model’s <a href="https://bigscience.huggingface.co/blog/the-bigscience-rail-license">Responsible AI License</a> (developed during the BigScience project itself) can use and build upon the model on a local machine or on a cloud provider - since it&#39;s embedded in the Hugging Face ecosystem, it&#39;s as easy as importing it with <tt>transformers</tt> and running it with <tt>accelerate</tt>. In this spirit of collaboration and continuous improvement, we’re also releasing, for the first time, the intermediary checkpoints and optimizer states of the training. Don’t have 8 A100s to play with? We&#39;re finalizing an inference API for large-scale use even without dedicated hardware or engineering. In the meantime, for quick tests, prototyping, and lower-scale use, you can already <a href="https://huggingface.co/bigscience/bloom">play with an early version</a> on the HF hub.</p></div><p>This is only the beginning. BLOOM’s capabilities will continue to improve as the workshop continues to experiment and tinker with the model. We’ve started work to make it as instructable as our earlier effort T0++ was and are slated to add more languages, compress the model into a more usable version with the same level of performance, and use it as a starting point for more complex architectures… All of the experiments researchers and practitioners have always wanted to run, starting with the power of a 100+ billion parameter model, are now possible. BLOOM is the seed of a living family of models that we intend to grow, not just a one-and-done model, and we’re ready to support community efforts to expand it.</p></div>
  </body>
</html>
