<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://andrewkchan.dev/posts/yalm.html">Original</a>
    <h1>Fast LLM Inference From Scratch (using CUDA)</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p id="toc"><h4>Contents</h4></p>
    
    <h2>Pushing single-GPU inference throughput to the edge without libraries</h2>
    <dt-byline></dt-byline>
    <ul>
      <li><i>Source code for this article on <a href="https://github.com/andrewkchan/yalm">GitHub</a>.</i></li>
    </ul>
    <p>
      This post is about building an LLM inference engine using C++ and CUDA from scratch without libraries.
    </p>
    <p>
      Why? In doing so, we can learn about the full stack of LLM inference - which is becoming increasingly important<dt-fn>Especially as inference compute becomes a new axis with which AI models scale,
      and models are increasingly deployed locally to devices on the edge.</dt-fn> - from CUDA kernels to model architecture, and get a real sense of how different optimizations affect inference speed.
      And one of the most important use cases is <i>running fast on a single prompt on consumer devices</i>.
    </p>
    <p>
      That&#39;s what we&#39;ll focus on: building a program that can load weights of common open models and do single-batch inference on them on a single CPU + GPU server, and iteratively 
      improving the token throughput until it surpasses <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Readers should have basic familiarity with large language 
      models, attention, and transformers. The full source code is available on GitHub: <a href="https://github.com/andrewkchan/yalm">yalm (Yet Another Language Model)</a>.
    </p>
    <h3>Acknowledgements</h3>
    <ul>
      <li>
        <a href="https://github.com/zeux/calm">calm</a> - Much of my implementation is inspired by Arseny Kapoulkine&#39;s inference engine.
        In a way, this project was kicked off by “understand calm and what makes it so fast.” I&#39;ve tried to keep my code more readable for myself though, and as much as possible 
        scientifically understanding optimizations, which means foregoing some advanced techniques used in calm like dynamic parallelism.
      </li>
      <li>
        <a href="https://github.com/karpathy/llama2.c">llama2.c</a> - Parts of the CPU backend come from Andrej Karpathy&#39;s excellent C implementation of Llama inference.
      </li>
    </ul>
    <dt-byline></dt-byline>
    
    <p>
      Let&#39;s recap how LLMs work, starting with their architecture and then moving onto inference mechanics. This will provide a starting point for an optimized implementation and help us establish benchmarks.
    </p>
    <p>
      Almost<dt-fn>There are also state-space models like <a href="https://arxiv.org/abs/2312.00752">Mamba</a> which purport to be more efficient and scalable to long sequences than transformers, but they don&#39;t appear to have found much success outside of
      niches like <a href="https://www.cartesia.ai/">low-power ML and non-discrete data domains like audio/video</a>.</dt-fn> every major open-weights LLM uses the same architecture<dt-fn>To the point where new foundation models explicitly state that they 
      use a &#34;standard architecture&#34;, with the value added being the training. This has led to some misunderstandings, see for instance <a href="https://blog.eleuther.ai/nyt-yi-34b-response/#how-all-llms-are-similar">https://blog.eleuther.ai/nyt-yi-34b-response/#how-all-llms-are-similar</a></dt-fn> 
      (sequential transformer blocks), with some minor variations/innovations since GPT-2:
      </p><ul>
        <li><a href="https://arxiv.org/abs/2305.13245v3">Grouped query attention (and multi-query attention)</a></li>
        <li><a href="https://arxiv.org/abs/2401.04088">Mixture-of-experts-based feedforward networks</a></li>
        <li><a href="https://arxiv.org/abs/2002.05202">GLU-based instead of MLP-based feedforward networks</a></li>
        <li><a href="https://arxiv.org/abs/2002.05202">Different activation functions for feedforward networks</a></li>
        <li><a href="https://arxiv.org/abs/1910.07467">Different layer normalizations</a></li>
        <li><a href="https://arxiv.org/abs/2104.09864">Rotary position embeddings</a></li>
      </ul>
    
    <p>
      Loading models from different architectures is thus essentially defining a customizable transformer block class, then creating a sequence of these configured with the 
      right bells and whistles and initializing them with the <a href="https://huggingface.co/docs/safetensors/en/index">safetensors</a> weights. This article will focus on just 
      one architecture - Mistral v0.2 - but if you&#39;re curious you can read 
      <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/development/HOWTO-add-model.md">how llama.cpp adds support for new models</a>.
    </p>
    <h2 id="section-1.1">1.1 Inference overview</h2>
    <p>
      At a high level, inference looks like the C++ pseudocode below:
    </p>
    <!-- <dt-code block language="clike"> -->
      <pre>        <code>
/* PSUEDOCODE */

void generate(Model&amp; model, std::string prompt, int steps) {
  std::vector&lt;int&gt; encoded = tokenizer.encode(prompt);
  InferenceState s(model);
  
  // 1. Prefill step: Forward the model on each prompt token, discarding 
  // the output. This lets the model read the prompt and hydrates the KV 
  // cache.
  for (int token : encoded) {
    model.forward(s, token);
  }
  // 2. Decode step: Forward the model repeatedly, generating 1 token at a time.
  for (int i = 0; i &lt; steps; i++) {
    model.forward(s, encoded.back());
    int next_token = sampler.sample(s.logits);
    encoded.push_back(next_token);
    std::cout &lt;&lt; tokenizer.decode_one(next_token) &lt;&lt; std::flush;
    if (next_token == tokenizer.EOS) {
      break;
    }
  }
}
        </code>
    </pre>
  <!-- </dt-code> -->
    <p>
      We can start to see differences between training and inference immediately. Inference - at least the local kind that we care about - is usually single batch. 
      For prompt completion and use cases like generating essays, the “decode phase” takes up the majority of execution and involves computing attention between the past context 
      and just a single token (or query timestep). 
    </p>
    <p>
      The prefill step is more similar to training in that we&#39;re given a complete sequence to attend over, but more on that later. In chatbots there&#39;s also an “append” step when 
      passing the model additional user messages which is like prefill, but I won&#39;t talk about that in this article as our implementation will support only completions. 
    </p>
    <p>
      The model forward pass looks like so:
    </p>
    <!-- <dt-code block language="clike"> -->
      <pre>        <code>
/* PSUEDOCODE */

// InferenceState is the minimum set of buffers needed to
// hold state during the forward pass and exists to avoid
// extra allocations
void Model::forward(InferenceState&amp; s, int token) {
  // The embedding table maps token IDs to embedding vectors,
  // which are copied into a buffer of the inference state
  s.x = copy_embedding(token, this-&gt;token_embedding_table);
  
  // Models consist of a sequence of transformer blocks which
  // mutate the inference state in order
  for (Block&amp; b : this-&gt;blocks) {
    b-&gt;block(s);
  }
  
  // Usually there is a layer norm right before the final classifier
  s.x = layernorm(s.x, this-&gt;lm_head_prenorm_weights);
  // Typically we end with a linear transform from (dim) -&gt; (vocab_size)
  s.logits = linear(s.x, this-&gt;lm_head_classifier_weights); 
}

void Block::block(InferenceState&amp; s) {
  s.x_resid = layernorm(s.x, this-&gt;att_prenorm_weights);
  // Multi-head attention typically includes: 
  // 1. RoPE on input (element-wise mutation w/ sines/cosines)
  // 2. QKV matmuls and updating the KV cache
  // 3. Causal self-attention, softmax, and value mixing
  // 4. Projection back into the residual stream
  s.x_resid = multi_head_attn(
    s.x_resid,
    this-&gt;wq, 
    this-&gt;wk, 
    this-&gt;wv, 
    this-&gt;key_cache,
    this-&gt;value_cache
  );
  s.x += s.x_resid;
  s.x_resid = layernorm(s.x, this-&gt;ffn_prenorm_weights);
  // On modern architectures like Llama, this is a GLU feedforward 
  // with 3 linear transforms, not a simple MLP:
  // -&gt; w2(F.silu(w1(x)) * w3(x))
  // Some architectures also split the FFN into a mixture of experts.
  s.x_resid = ffn(s.x_resid, this-&gt;w1, this-&gt;w2, this-&gt;w3);
  s.x += s.x_resid;
}
        </code>
    </pre>
  <!-- </dt-code> -->
    <p>
      This should look roughly familiar, if expressed a bit more low-level in typical C++ fashion. 
    </p>
    <p>
      The main thing worth noting is that unlike training, inference can use a KV cache to store past keys and values for each block. 
      We keep things simple and implement this as a simple ring buffer (known as sliding window attention in the literature), which is sufficient to 
      support exact attention up to some maximum context length. Some exact attention implementations like PagedAttention use more complex KV caches to 
      improve aspects like memory footprint.
    </p>
    <h2 id="section-1.2">1.2 Bottlenecks and benchmarks</h2>
    <p>
      Now we are ready to discuss bottlenecks and benchmarks. First, a fact: inference is memory-bandwidth-bound on modern hardware. For more, see 
      this <a href="https://zeux.io/2024/03/15/llm-inference-sol">excellent blog post</a> by Arseny Kapoulkine, but the gist of it is:
      </p><ul>
        <li>
          Every time we generate a token we need to read the entire model, performing only a few floating point operations per weight.
        </li>
        <li>
          Modern CPUs and GPUs are <i>extremely</i> fast at floating point operations. The key metric is the FLOPs/s-to-memory-bandwidth-ratio (FLOPs/byte). 
          For instance, the AMD Ryzen 7950X has about a 40:1 ratio, while the RTX 4090 has an 82:1 ratio. The AMD EPYC 7702P on my server has a less impressive, 
          but still significant 10:1 ratio.
        </li>
      </ul>
    
    <p>
      This is why <a href="https://huggingface.co/docs/optimum/en/concept_guides/quantization">model quantization</a> is so effective at improving inference speed. 
      It&#39;s not just allowing the hardware to use faster instructions (which is sometimes true), 
      but also shrinking the input that we need to fit through the bandwidth bottleneck.
    </p>
    <p>
      We can use bandwidth to establish a theoretical “speed of light”, or the max token throughput we can achieve. On my machine with an AMD EPYC 7702P and RTX 4090:
      </p><ul>
        <li>
          EPYC 7702P max bandwidth<dt-fn><a href="https://www.amd.com/content/dam/amd/en/documents/products/epyc/amd-epyc-7002-series-datasheet.pdf">See the official datasheet</a>.</dt-fn>: <code>204.8 GB/s</code>
        </li>
        <li>
          RTX 4090 max bandwidth<dt-fn>See <a href="https://en.wikipedia.org/wiki/GeForce_40_series#Desktop">wikipedia</a>.</dt-fn>: <code>1008 GB/s</code>
        </li>
        <li>
          <p>
            Mistral-7B-Instruct-v0.2-FP32 with a 4k context window and FP32 KV-cache is <code>29516398592 bytes</code>
          </p>
          <ul>
            <li>
              <code>204.8e9 bytes/s / 29516398592 bytes/tok = ~6.9 tok/s</code> for EPYC 7702P
            </li>
            <li>
              This won&#39;t fit in the 24GB of RTX 4090 VRAM so we&#39;ll skip it.
            </li>
          </ul>
        </li>
        <li>
          <p>
            Mistral-7B-Instruct-v0.2-FP16 with a 4k context window and FP16 KV-cache is <code>15020875776 bytes</code>
          </p>
          <ul>
            <li>
              <code>204.8e9 bytes/s / 15020875776 bytes/tok = ~13.6 tok/s</code> for EPYC 7702P
            </li>
            <li>
              <code>1008e9 bytes/s / 15020875776 bytes/tok = ~67.1 tok/s</code> for RTX 4090
            </li>
          </ul>
        </li>
      </ul>
    
    <p>
      Note that how close we can actually come to the theoretical bounds varies a bit depending on the hardware. We fortunately have a few popular inference 
      engines that we can look at to set more realistic targets. On my machine<dt-fn>Unfortunately I wasn&#39;t able to test calm CPU as my machine doesn&#39;t support the extensions 
      needed for it to compile.</dt-fn> using Mistral-7B-Instruct-v0.2 in FP16 with 4k context I&#39;m able to get:
    </p>
    <table>
      <tbody><tr>
        <th>Program</th>
        <th>Avg. throughput (~120 tokens)</th>
        <th>Avg. throughput (~4800 tokens)</th>
      </tr>
      <tr>
        <td>llama.cpp, CPU<dt-fn>With number of threads tuned.</dt-fn></td>
        <td>8.7 tok/s</td>
        <td>7.6 tok/s</td>
      </tr>
      <tr>
        <td>huggingface transformers, GPU<dt-fn>See <a href="#appendix">appendix</a> for benchmark code. For some reason, this was the highest variance of all. Shown are a best of 3 run.</dt-fn></td>
        <td>25.9 tok/s</td>
        <td>25.7 tok/s</td>
      </tr>
      <tr>
        <td>llama.cpp, GPU</td>
        <td>61.0 tok/s</td>
        <td>58.8 tok/s</td>
      </tr>
      <tr>
        <td>calm, GPU</td>
        <td>66.0 tok/s</td>
        <td>65.7 tok/s</td>
      </tr>
    </tbody></table>
    <dt-byline></dt-byline>

    
    <p>
      We begin with a naive implementation on CPU (the code is available <a href="https://github.com/andrewkchan/yalm/blob/ec8c8fec911794c788c50dfe5d42ae9e1ef0e905/src/infer.cpp#L240">here</a>). 
      It&#39;s a straightforward single-threaded implementation with a 4k KV cache that only supports FP32 weights and no explicit SIMD of any kind. It achieves a blazing fast 
      throughput of 0.6 tok/s. Here&#39;s what that looks like:
    </p>
    
    <h2 id="section-2.1">2.1 Multithreading</h2>
    <p>
      The first optimization step we can do is to begin parallelizing our code on the thread level. Equipped with our handy OpenMP pragmas, we go hunting for embarrassingly parallel 
      opportunities. We&#39;ll optimize the same spots as llama2.c, and I&#39;ll go over each one to show the improvement.
    </p>
    <p>
      First, <a href="https://github.com/andrewkchan/yalm/commit/2a65dcbdff106976aeff1f08a037c6b6ece5b80b">adding a single line of code</a> parallelizes our widely matrix-vector multiplication 
      function so that each thread handles a row of the output:
    </p>
    <!-- <dt-code block language="clike"> -->
      <pre>        <code>
static void matmul(float* xout, float* x, float* w, int n, int d) {
  // W (d,n) @ x (n,) -&gt; xout (d,)
  int i;
#pragma omp parallel for private(i)
  for (i = 0; i &lt; d; i++) {
    float val = 0.0f;
    for (int j = 0; j &lt; n; j++) {
      val += w[i * n + j] * x[j];
    }
    xout[i] = val;
  }
}
        </code>
    </pre>
  <!-- </dt-code> -->
    <p>
      This is a big improvement that takes us to 4.2 tok/s with a bit of tuning to find the right number of threads:
    </p>
    
    <p>
      Next, we can parallelize our multi-head attention computation (<a href="https://github.com/andrewkchan/yalm/compare/2a65dcbdff106976aeff1f08a037c6b6ece5b80b..f39081de410ced856a4ba301234ab3fc6948e4a4">code here</a>) 
      so that each thread gets an attention head to compute. This is a less immediately clear improvement, but gets us to 
      4.4 tok/s for short context generations, and likely much better for long contexts.
    </p>
    
    <h2 id="section-2.2">2.2 Weight quantization and SIMD</h2>
    <p>
      The next potential opportunity is to use SIMD. The EPYC 7702P CPU supports AVX and AVX2, which let us work with 256-bit vectors of 8 packed float32 values at a time. 
      In our ubiquitous <code>matmul</code> function, we could try loading, multiplying, and accumulating 8 values at a time in the inner loop, which would let each thread finish 
      its row-column dot product up to 8 times faster!
    </p>
    <p>
      Unfortunately, inspecting our compiled code via <code>objdump</code> reveals that <code>matmul</code> is in fact already using AVX instructions (notice <code>vmovups</code>) to perform a vectorized dot 
      product in the case that the inputs are large enough. It seems GCC is too smart:
    </p>
    <!-- <dt-code block language="plaintext"> -->
      <pre>        <code>
1f5:       c4 e3 7d 19 c1 01       vextractf128 xmm1,ymm0,0x1
1fb:       c5 f0 58 c0             vaddps xmm0,xmm1,xmm0
1ff:       c5 f8 12 c8             vmovhlps xmm1,xmm0,xmm0
203:       c5 f0 58 c8             vaddps xmm1,xmm1,xmm0
207:       c5 f0 c6 c1 55          vshufps xmm0,xmm1,xmm1,0x55
20c:       c5 f8 58 c1             vaddps xmm0,xmm0,xmm1
        </code>
    </pre>
  <!-- </dt-code> -->
    <p>
      Let&#39;s turn to quantization. We won&#39;t explore the full gamut of quantized formats, since the goal of this article is to explore the breadth of optimizations, and we&#39;ve 
      chosen our benchmarks with fixed formats. Instead, we&#39;ll just quantize our weights to FP16, which is the bare minimum needed to get it loaded onto the RTX 4090 VRAM anyway.
    </p>
    <p>
      One hiccup is that many CPUs do not support native float16 math. But barring that, we&#39;d like to keep our calculations in float32 as much as possible anyway to mitigate 
      effects on accuracy, and we should be able to without trading off performance as long as bandwidth remains a bottleneck. 
    </p>
    <p>
      So instead we leverage the fact that many CPUs do still support converting float16 values to float32 via the F16C x86 extension (which has been well-supported for over a 
      decade now) to load float16 weights and convert them to float32 just-in-time for calculations. Among other things, this requires us to explicitly vectorize the loads in 
      our matmul function from before because GCC doesn&#39;t know how to handle the half-precision arrays:
    </p>
    <!-- <dt-code block language="clike"> -->
      <pre>        <code>
// F16C code technically operates on 16-bit unsigned short integers
typedef uint16_t f16_t;

// matmul supporting float16 weights via the F16C extension, which allows
// conversion into float32 values before calculations.
static void matmul(float* xout, float* x, f16_t* w, int n, int d) {
#if defined(__AVX2__) &amp;&amp; defined(__F16C__)
  // W (d,n) @ x (n,) -&gt; xout (d,)
  assert(n % 16 == 0);
  int i;
#pragma omp parallel for private(i)
  for (i = 0; i &lt; d; i++) {
    // Vectorized dot product of w[i][:] and x[:] where w is a packed float16 array.
    __m256 sumlo = _mm256_setzero_ps();
    __m256 sumhi = _mm256_setzero_ps();
    for (int j = 0; j &lt; n; j+=16) {
      // Extract the next set of 16 float16 weights from `w` and store them
      // to two separate float32 vectors of width 8 (`wveclo_ps`, `wvechi_ps`)
      __m256i wvec = _mm256_loadu_si256((__m256i*)&amp;w[i * n + j]);
      __m128i wveclo = _mm256_extractf128_si256(wvec, 0);
      __m128i wvechi = _mm256_extractf128_si256(wvec, 1);
      __m256 wveclo_ps = _mm256_cvtph_ps(wveclo);
      __m256 wvechi_ps = _mm256_cvtph_ps(wvechi);
      // Extract the next two float32 vectors of width 8 `xveclo`, `xvechi` from `x`
      __m256 xveclo = _mm256_loadu_ps(&amp;x[j]);
      __m256 xvechi = _mm256_loadu_ps(&amp;x[j + 8]);
      // Compute vectorized FMAs: sumlo += wveclo * xveclo, sumhi += wvechi * xvechi
      sumlo = _mm256_fmadd_ps(wveclo_ps, xveclo, sumlo);
      sumhi = _mm256_fmadd_ps(wvechi_ps, xvechi, sumhi);
    }
    // Horizontally reduce width-8 float32 vectors sumlo, sumhi to a scalar.
    __m256 sum8 = _mm256_add_ps(sumlo, sumhi);              // sum8[0:8] = sumlo[0:8] + sumhi[0:8]
    __m128 sum4 = _mm_add_ps(                               // sum4[0:4] = sum8[0:4] + sum8[4:8]
      _mm256_extractf128_ps(sum8, 0), 
      _mm256_extractf128_ps(sum8, 1)
    );
    __m128 sum1 = _mm_dp_ps(sum4, _mm_set1_ps(1.0f), 0xf1); // sum1[0] = dot(sum4, [1,1,1,1])
    xout[i] = _mm_cvtss_f32(sum1);
  }
#else
  assert(false &amp;&amp; &#34;float16 not supported on this platform&#34;);
#endif
}
        </code>
    </pre>
  <!-- </dt-code> -->
    <p>
      <a href="https://github.com/andrewkchan/yalm/blob/462b3705a2dc75d2ae22e3b6a4c45d9b92bc20ea/src/infer.cpp">The resulting implementation</a> does not yield any difference in perplexity for short texts. It&#39;s also nearly twice as fast at 8.2-8.4 tok/s:
    </p>
    

    <dt-byline></dt-byline>

    
    <!-- <dt-code block language="clike"> -->
      
  <!-- </dt-code> -->
    
    <!-- <dt-code block language="clike"> -->
      
  <!-- </dt-code> -->
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
  <!-- </dt-code> -->
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
  <!-- </dt-code> -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    <!-- <dt-code block language="plaintext"> -->
      
  <!-- </dt-code> -->
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
  <!-- </dt-code> -->
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    <!-- <dt-code block language="plaintext"> -->
      
    <!-- </dt-code> -->
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    <!-- <dt-code block language="clike"> -->
    
    <!-- </dt-code> -->
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <!-- <dt-code block language="clike"> -->
      
    <!-- </dt-code> -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </div></div>
  </body>
</html>
