<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://agi-sphere.com/llama-models/">Original</a>
    <h1>A brief history of LLaMA models</h1>
    
    <div id="readability-page-1" class="page"><div><p>The LLaMA base model was released in February 2023.  Now we have seen a handful of new fine-tuned LLaMA models released.</p><p>It is literally a brief history, but a lot has happened for sure. So let’s do a brief review.</p><p>I will cover some developments in models and briefly touch on tools.</p><ul><li>LLaMA base model</li><li>Alpaca model</li><li>Vicuna model</li><li>Koala model</li><li>GPT4-x-Alpaca model</li><li>WizardLM model</li><li>Software to run LLaMA models locally</li></ul><p>Below is an overview of the models.</p><figure><table><thead><tr><th>Model</th><th>Size</th><th>Training data</th></tr></thead><tbody><tr><td>LLaMA (base model)</td><td>7B, 13B, 33B, 65B</td><td>Various</td></tr><tr><td>Alpaca</td><td>7B, 13B</td><td>52k GPT-3 instructions</td></tr><tr><td>Vicuna</td><td>7B, 13B</td><td>70k ChatGPT conversations</td></tr><tr><td>Koala-distill</td><td>7B, 13B</td><td>117k cleaned ChatGPT conversations</td></tr><tr><td>GPT4-x-Alpaca</td><td>13B</td><td>20k GPT4 instructions</td></tr><tr><td>WizardML</td><td>7B</td><td>70k instructions synthesized with ChatGPT/GPT-3</td></tr></tbody></table><figcaption>Model comparison</figcaption></figure><h2><span id="LLaMA_base_model">LLaMA base model</span></h2><ul><li>Paper: <a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener">LLaMA: Open and Efficient Foundation Language Models</a></li><li><a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" target="_blank" rel="noopener">Release blog post</a></li><li>Release date: February 2023</li></ul><p><strong>LLaMA</strong> (Large Language Model Meta AI) is a language model <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" target="_blank" rel="noopener">released</a> by Meta (Facebook). It is Meta’s answer to OpenAI’s GPT models.</p><p>Like GPT, LLaMA is intended to be a general-purpose foundational model suitable for further fine-tuning.</p><p>LLaMA models have the following variants</p><ul><li>7B parameters</li><li>13B parameters</li><li>33B parameters</li><li>65B parameters</li></ul><p>The larger the number of parameters, the more powerful the model, but it also takes up more resources to run.</p><h3><span id="Accessibility">Accessibility</span></h3><p>Unlike GPT, LLaMA is an open-source model. You can download, study and run them locally. Officially, you will need to use a <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform" target="_blank" rel="noopener">Google form</a> to request the model weights.</p><p>However, the models were leaked on Torrent in March 2023, less than a month after its release.</p><h3><span id="Objective">Objective</span></h3><p>The objective of LLaMA is to build the best-performing model for a given <em>inference budget</em>, for example, running on an NVIDIA 3090 using less than 10GB VRAM.</p><h3><span id="Model_architecture">Model architecture</span></h3><p>LLaMA is a transformer model similar to GPT with the following modifications.</p><ul><li>Normalize the input of each transformer sub-layer to improve training stability.</li><li>Use SwiGLU instead of ReLU to improve performance.</li><li>Use rotary embedding instead of absolute positioning to improve performance.</li></ul><p>The table below summarizes the model parameters.</p><figure><table><tbody><tr><td></td><td><strong>Parameters</strong></td><td><strong>Layers</strong></td><td><strong>Attention heads</strong></td><td><strong>Embedding dimension</strong></td></tr><tr><td>7B</td><td>6.7B</td><td>32</td><td>32</td><td>4,096</td></tr><tr><td>13B</td><td>13B</td><td>40</td><td>40</td><td>5,120</td></tr><tr><td>33B</td><td>33B</td><td>60</td><td>52</td><td>6,656</td></tr><tr><td>65B</td><td>65B</td><td>80</td><td>64</td><td>8,192</td></tr></tbody></table><figcaption>Model parameters</figcaption></figure><p>For reference, GPT-3 has 175B parameters. LLaMA models are small.</p><h3><span id="Training">Training</span></h3><p>The pre-training data used in LLaMA are</p><ul><li><strong>English <a href="https://commoncrawl.org/" target="_blank" rel="noopener">CommonCrawl</a> (67%)</strong>: Removed non-English text and duplicated content. Only includes pages used as references in Wikipedia.</li><li><strong><a href="https://huggingface.co/datasets/c4" target="_blank" rel="noopener">C4</a> (15%)</strong>: A cleaned version of CommonCrawl. The same filters were applied.</li><li><strong>Github (4.5%)</strong>: Public GitHub dataset available on Google BigQuery.</li><li><strong>Wikipedia (4.5%)</strong>: From June-August 2022 period covering 20 languages.</li><li><strong>Gutenberg and Books3 (4.5%)</strong>: Both are book datasets.</li><li><strong>ArXiv (45%)</strong>: Scientific data.</li><li><strong>StackExchange (2%)</strong>: High-quality Q&amp;As covering science and engineering topics.</li></ul><p>The tokenizer is with byte-pair encoding using <a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece</a>.</p><p>The training data has 1.4T tokens.</p><h3><span id="Performance">Performance</span></h3><p>They evaluated the models with tasks such as common sense reasoning, reading comprehension, and code generation.</p><p>Summary of performance:</p><ul><li><strong>Larger is better</strong>: Larger models perform better in most tasks.</li></ul><ul><li><strong>More examples in the prompt are better</strong>: Give 5 examples to LLaMA 7B model is almost as good as not giving any to a 65B model in Natural Questions tasks.</li></ul><ul><li><strong>Smaller performant model</strong>. LLaMA 13B’s performance is similar to GPT-3, despite 10 times smaller. (13B vs 175B parameters)</li></ul><ul><li>LLaMA is <strong>not very good at quantitative reasoning</strong>, especially the smaller 7B and 13B models.</li></ul><ul><li>LLaMA is not tuned for instruction following like ChatGPT. However, the 65B model can follow basic instructions. We will wait for Alpaca (not for long).</li></ul><h3><span id="Model_size_comparison">Model size comparison</span></h3><p>How much do you gain by using a bigger LLaMA model? The following table summarizes the performance of tasks in different categories. They are calculated based on the scores provided in the research article, assuming linear scales.</p><figure><table><thead><tr><th></th><th>Average</th><th>Common sense reasoning</th><th>Natural Questions</th><th>Reading comprehension</th><th>TriviaQA</th><th>Quantitative reasoning</th><th>Code generation</th><th>Multitask language understanding</th></tr></thead><tbody><tr><td><strong>7B</strong></td><td><strong>65%</strong></td><td>92%</td><td>65%</td><td>90%</td><td>76%</td><td>27%</td><td>53%</td><td>56%</td></tr><tr><td><strong>13B</strong></td><td><strong>76%</strong></td><td>95%</td><td>80%</td><td>91%</td><td>86%</td><td>39%</td><td>69%</td><td>74%</td></tr><tr><td><strong>33B</strong></td><td><strong>91%</strong></td><td>99%</td><td>95%</td><td>94%</td><td>96%</td><td>72%</td><td>89%</td><td>91%</td></tr><tr><td><strong>65B</strong></td><td><strong>100%</strong></td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr></tbody></table><figcaption>Performance of LLaMA models (normalized to 65B as 100%).</figcaption></figure><p>Is it worth using a bigger model? You can expect a ~50% generic improvement when switching from the 7B to the 65B model.</p><p>But it also depends on what you use the models for. You will only see a small gain for common sense reasoning and reading comprehension tasks. You will see a big gain for code generation and technical reading tasks.</p><h3><span id="Summary_for_LLaMA">Summary for LLaMA</span></h3><p>The take-home message in this study is small models can perform well if you train them with enough data. This opens up the possibility of running a “<a href="https://agi-sphere.com/vicuna-mac/" data-type="post" data-id="521">local ChatGPT</a>” on a PC.</p><p>But the LLaMA base model was not trained to follow instructions. This is saved for later development.</p><p>To sum up, LLaMA is designed to be a base model for further fine-tuning. Its advantages are</p><ul><li>Small size</li><li>Performant – thanks to extensive training</li><li>Open source</li></ul><h2><span id="Alpaca_model">Alpaca model</span></h2><ul><li><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener">Alpaca model page</a></li><li><a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" rel="noopener">Alpaca Github page</a></li></ul><p>Alpaca is a fine-tuned LLaMA model, meaning that the model architecture is the same, but the weights are slightly different. It is aimed at resolving the lack of instruction-following capability of LLaMA models.</p><p>It behaves like ChatGPT and can follow conversations and instructions.</p><p>The 7B and 13B Alpaca models are available.</p><h3><span id="Training-2">Training</span></h3><p>It was trained to follow instructions like ChatGPT.</p><p>The authors first generate the training data using OpenAI’s GPT-3, then convert them to 52k instruction-following conversational data using the <a href="https://arxiv.org/abs/2212.10560" target="_blank" rel="noopener">Self-Instruct</a> pipeline.</p><figure><img decoding="async" width="1024" height="426" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://agi-sphere.com/wp-content/uploads/2023/04/image-58-1024x426.png" alt="Training workflow for Alpaca model" data-srcset="https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-58.png?resize=1024%2C426&amp;ssl=1 1024w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-58.png?resize=300%2C125&amp;ssl=1 300w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-58.png?resize=768%2C320&amp;ssl=1 768w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-58.png?resize=1536%2C639&amp;ssl=1 1536w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-58.png?resize=2048%2C852&amp;ssl=1 2048w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Training pipeline of Alpaca (Source: <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noopener">Alpaca model page</a>)</figcaption></figure><p>As a result, Alpaca is fine-tuned to respond to conversations like ChatGPT.</p><h3><span id="Performance-2">Performance</span></h3><p>A blinded evaluation for instruction-following ability performed by some of the authors ranked the responses of Alpaca 7B and GPT-3 (text-davinci-003 specifically, which is also trained with instructions) roughly equally.</p><p>This is a surprising result because Alpaca is 26 times smaller than GPT-3.</p><p>Of course, this is just a narrow aspect of performance. It doesn’t mean Alpaca performs equally with GPT-3 in other areas like code generation and scientific knowledge, which were not tested in the study.</p><h3><span id="Summary_for_Alpaca">Summary for Alpaca</span></h3><p>Alpaca is a nice first step in fine-tuning the LLaMA model. As we see in the next section, it is outperformed by a similar fine-tuning effort, Vicuna.</p><h2><span id="Vicuna_model">Vicuna model</span></h2><ul><li><a href="https://vicuna.lmsys.org/" target="_blank" rel="noopener">Vicuna model page</a></li></ul><p>Vicuna is trained by fine-tuning the LLaMA base models on user-shared conversations collected from <a href="https://sharegpt.com/" target="_blank" rel="noopener">ShareGPT.com</a>. So it is basically fine-tuned with ChatGPT conversations.</p><p>It comes in two sizes: 7B and 13B.</p><h3><span id="Training-3">Training</span></h3><p>The model was fine-tuned by an academic team from UC Berkeley, CMU, Stanford, and UC San Diego.</p><p>It was trained with user-contributed <strong>ChatGPT conversations</strong>. So you can expect its behavior mimics ChatGPT. Precisely, it is trained with 70,000 ChatGPT conversations users shared on <a href="https://sharegpt.com/" target="_blank" rel="noreferrer noopener">ShareGPT.com</a>.</p><p>It only costed $140 to train the 7B model and $300 to train the 13B model.</p><h3><span id="Performance-3">Performance</span></h3><p>How good is Vicuna? According to their website, the output quality (as judged by GPT-4…) is about 90% of ChatGPT, making it the best language model you can run locally.</p><figure><img decoding="async" width="599" height="256" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://agi-sphere.com/wp-content/uploads/2023/04/image-50.png" alt="" data-srcset="https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-50.png?w=599&amp;ssl=1 599w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-50.png?resize=300%2C128&amp;ssl=1 300w" sizes="(max-width: 599px) 100vw, 599px"/><figcaption>Response quality as judged by GPT-4. (from <a href="https://vicuna.lmsys.org/" target="_blank" rel="noopener">Vicuna</a> site)</figcaption></figure><p>The authors used an interesting method to evaluate the model’s performance: Using GPT-4 as the judge. They asked GPT-4 to generate some challenging questions and let Vicuna and some other best language models answer them.</p><p>They then ask GPT-4 to evaluate the quality of the answers in different aspects, such as helpfulness and accuracy.</p><p>Here’s the result for comparing <a href="https://agi-sphere.com/install-llama-mac/">LLaMA</a>, <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="noreferrer noopener">Alpaca</a>, <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/" target="_blank" rel="noreferrer noopener">Bard</a>, and ChatGPT. In the eyes of GPT-4, Vicuna is almost as good as ChatGPT, beating LLaMA and Alpaca by a large margin.</p><figure><img decoding="async" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://agi-sphere.com/wp-content/uploads/2023/04/image-59-1024x589.png" alt="" width="598" height="344" data-srcset="https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-59.png?resize=1024%2C589&amp;ssl=1 1024w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-59.png?resize=300%2C172&amp;ssl=1 300w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-59.png?resize=768%2C442&amp;ssl=1 768w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-59.png?w=1536&amp;ssl=1 1536w" sizes="(max-width: 598px) 100vw, 598px"/><figcaption>GPT-4’s judgment. (source: <a href="https://vicuna.lmsys.org/" target="_blank" rel="noreferrer noopener">Vicuna</a> model page)</figcaption></figure><h3><span id="Summary">Summary</span></h3><p>The Vicuna model is considered to be one of the best LLaMA models that you can <a href="https://agi-sphere.com/vicuna-mac/" data-type="post" data-id="521">run locally</a>. But I won’t be surprised if things change in the coming weeks.</p><h2><span id="Koala">Koala</span></h2><ul><li><a href="https://bair.berkeley.edu/blog/2023/04/03/koala/" target="_blank" rel="noopener">Koala model page</a></li><li>Release date: April 2023</li></ul><p>Koala is a LLaMA 7B and 13B models fine-tuned with publicly available dialog data by an academic team at UC Berkeley.</p><h3><span id="Training-4">Training</span></h3><p>The training data includes filtered data from multiple datasets.</p><ul><li><a href="https://sharegpt.com/" target="_blank" rel="noopener">ShareGPT</a> – 30k</li><li><a href="https://arxiv.org/abs/2301.07597" target="_blank" rel="noopener">Human ChatGPT Comparison Corpus</a> – 87k</li><li><a href="https://laion.ai/blog/oig-dataset/" target="_blank" rel="noopener">Open Instruction Generalist</a> – 30k</li><li><a href="https://github.com/tatsu-lab/stanford_alpaca#data-release" target="_blank" rel="noopener">Stanford Alpaca</a> (Training dataset for Alpaca) – 52k</li><li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf" target="_blank" rel="noopener">Anthropic HH</a> – 50k</li><li><a href="https://huggingface.co/datasets/openai/webgpt_comparisons" target="_blank" rel="noopener">OpenAI WebGPT</a> – 20k</li><li><a href="https://huggingface.co/datasets/openai/summarize_from_feedback" target="_blank" rel="noopener">OpenAI summarization</a> – 93k</li></ul><p>They trained two models</p><ol><li><strong>Koala-All</strong>: Used all datasets</li><li><strong>Koala-Distill</strong>: Used the first two datasets (i.e., data distilled from ChatGPT)</li></ol><h3><span id="Performance-4">Performance</span></h3><p>They evaluated the performance of Koala-All and Koala-Distill by comparing them with Alpaca and ChatGPT.  100 evaluators from Amazon Mechanical Turk judged the responses of these models from the same prompts.</p><p>The results are</p><ul><li>Koala-All is better than Alpaca but worse than ChatGPT.</li><li>Koala-Distill is slightly better than Koala-All. — This is surprising as Koala-All was fine-tuned with more data.</li></ul><h3><span id="Summary-2">Summary</span></h3><p>The take-home message is quality of the data is more important than quantity. Koala-Distll fine-tuned with ChatGPT data alone outperforms  Koala-All trained with additional data.</p><p>Going forward, finding or generating high-quality data to fine-tune LLaMA models is going to be important.</p><h2><span id="GPT4-x-Alpaca">GPT4-x-Alpaca</span></h2><ul><li><a href="https://huggingface.co/chavinlo/gpt4-x-alpaca" target="_blank" rel="noopener">HuggingFace page</a></li><li>Release date: April 2023</li></ul><p>GPT4-x-Alpaca is a LLaMA 13B model fine-tuned with a collection of GPT4 conversations, <a href="https://github.com/teknium1/GPTeacher" target="_blank" rel="noopener">GPTeacher</a>. There’s not a lot of information on its training and performance.</p><p>Below are some community efforts to evaluate the model</p><ul><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/12lksqo/ai_showdown_gpt4xalpaca_vs_vicuna_gpt4_as_the/" target="_blank" rel="noopener">GPT-4-x-Alpaca vs. Vicuna, GPT-4 as the judge</a></li><li><a href="https://github.com/oobabooga/text-generation-webui/discussions/727" target="_blank" rel="noopener">Discussion on performance</a></li></ul><h2><span id="WizardLM">WizardLM</span></h2><ul><li><a href="https://arxiv.org/abs/2304.12244" target="_blank" rel="noopener">WizardLM Paper</a></li><li><a href="https://github.com/nlpxucan/WizardLM" target="_blank" rel="noopener">WizardLM GitHub Page</a></li><li>Released date: April 2023.</li></ul><p>WizardLM is a fine-tuned 7B LLaMA model. It was fine-tuned with a large amount of instruction-following conversations with varying difficulties. The novelty of this model is using an LLM to generate instructions automatically.</p><h3><span id="Training-5">Training</span></h3><p>The WizardLM model was trained with 70k computer-generated instructions with a new method called Evol-Instruct. The method produces instructions with varying levels of difficulty.</p><p>Evol-Instruct expands a prompt with these five operations</p><ul><li>Add constraints</li><li>Deepening</li><li>Concretizing</li><li>Increase reasoning steps</li><li>Complicate input</li></ul><p>These operations were applied sequentially to an initial instruction to make it more complex.</p><p>The responses were generated by an LLM.</p><h3><span id="Performance-5">Performance</span></h3><p>The authors compared the performance of WizardLM with Alpaca 7B, Vicuna 7B, and ChatGPT.  They recruited 10 people to judge the responses of WizardLM and other models in five aspects blindly: Relevance, knowledge, reasoning, calculation, and accuracy.</p><p>The authors conclude that:</p><ul><li>The instructions generated by Evol-Instruct are superior to ShareGPT (used by Vicuna).</li><li>WizardLM significantly outperforms Alpca and Vicuna.</li><li>ChatGPT is better overall, but WizardLM excels in high-complexity questions.</li></ul><figure><img decoding="async" width="706" height="274" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://agi-sphere.com/wp-content/uploads/2023/04/image-60.png" alt="" data-srcset="https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-60.png?w=706&amp;ssl=1 706w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-60.png?resize=300%2C116&amp;ssl=1 300w" sizes="(max-width: 706px) 100vw, 706px"/><figcaption>WizardLM excels in answering complex instructions. (Source: WizardLM paper)</figcaption></figure><p>The community generally agrees that WizardLM is the current state-of-the-art for 7B models.</p><h2><span id="Software_tools">Software tools</span></h2><p>The development on the software engineering side is equally breathtaking. Currently, the two main ways to run LLaMA model on your PC are</p><ul><li>llama.cpp (for Mac or CPU only)</li><li>Oobabooga text-generation-webui</li></ul><h3><span id="llamacpp">llama.cpp</span></h3><p><a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener">llama.cpp</a> is written in C++ from the ground up. The goal is to enable running LLaMA models on Macbooks.  It is optimized for Apple Silicon M1/M2.</p><p>It supports <strong>4-bit quantization</strong> to reduce the resources needed for LLaMA models. These quantitation models reduce the storage and RAM usage of the models at the expense of a slight loss in quality.</p><p>A 7B model originally takes 13GB of disk space and RAM to load.  It only takes about 4 GB after 4-bit quantization.</p><p>Due to its native Apple Silicon support, llama.cpp is an excellent choice for running LLaMA models on Mac M1/M2.</p><p>However, it only supports usage in a text terminal. Technically, you can use text-generation-webui as a GUI for llama.cpp, but as of writing, it could be a lot slower.</p><h3><span id="text-generation-webui">text-generation-webui</span></h3><p><a href="https://github.com/oobabooga/text-generation-webui" target="_blank" rel="noopener">Oobabooga text-generation-webui</a> is a GUI for using LLaMA models. It can be run on Windows, Linux and Mac.</p><figure><img decoding="async" width="1024" height="641" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://agi-sphere.com/wp-content/uploads/2023/04/image-61-1024x641.png" alt="" data-srcset="https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-61.png?resize=1024%2C641&amp;ssl=1 1024w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-61.png?resize=300%2C188&amp;ssl=1 300w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-61.png?resize=768%2C481&amp;ssl=1 768w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-61.png?resize=1536%2C962&amp;ssl=1 1536w, https://i0.wp.com/agi-sphere.com/wp-content/uploads/2023/04/image-61.png?w=1724&amp;ssl=1 1724w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure><p>You should go with this GUI if you have a GPU card on Windows or Linux.</p><p>Like llama.cpp, it supports 4-bit quantization (but in a different file format) for model size reduction.</p> <!-- Simple Share Buttons Adder (8.4.6) simplesharebuttons.com --></div></div>
  </body>
</html>
