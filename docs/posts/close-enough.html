<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://impracticalprogrammer.com/2022/07/05/close-enough.html">Original</a>
    <h1>Close Enough</h1>
    
    <div id="readability-page-1" class="page"><div>
      

<p>You can get a good-enough answer to some questions without all the work.</p>



<p>Some days, you just have too much data, you know? Big heaps of data, piling up, each just a little different than the last. And the function you’re supposed to run on all that data is kinda complicated, and a little slow, and you’re just not feeling it. What’s a poor CPU to do?</p>

<p>Well don’t just throw it out! Give each data point the individual attention it deserves, while giving yourself grace to approximate the function instead of worrying about every little step by gradient approximation. Here, I’ll show you:</p>

<ul id="markdown-toc">
  <li><a href="#close-enough" id="markdown-toc-close-enough">Close Enough</a></li>
  <li><a href="#a-gradient-approximation-story-with-zygote-and-julia" id="markdown-toc-a-gradient-approximation-story-with-zygote-and-julia">A Gradient Approximation Story with Zygote and Julia</a></li>
  <li><a href="#an-example-problem--black-scholes" id="markdown-toc-an-example-problem--black-scholes">An example problem- Black-Scholes</a></li>
  <li><a href="#the-idea--gradient-approximation" id="markdown-toc-the-idea--gradient-approximation">The idea- Gradient Approximation</a></li>
  <li><a href="#the-surprisingly-easy-work--building-the-approximator" id="markdown-toc-the-surprisingly-easy-work--building-the-approximator">The surprisingly easy work- building the approximator</a></li>
  <li><a href="#the-digression--some-data" id="markdown-toc-the-digression--some-data">The digression- some data</a></li>
  <li><a href="#the-lie--nothing-in-the-computer-is-real" id="markdown-toc-the-lie--nothing-in-the-computer-is-real">The lie- nothing in the computer is real</a></li>
  <li><a href="#the-results--things-are-good" id="markdown-toc-the-results--things-are-good">The results- things are good</a></li>
  <li><a href="#the-lesson--make-tradeoffs" id="markdown-toc-the-lesson--make-tradeoffs">The lesson- make tradeoffs</a></li>
  <li><a href="#footnotes" id="markdown-toc-footnotes">Footnotes</a></li>
</ul>



<p>Let’s work through an example, in Julia for today. Gradient approximation works on anything with a gradient, but we’ll pick an example that’s pure math. Let’s say you’re supposed to figure out exactly how much a bucket of stock options are worth, and how risky it is, at different times in a day. Luckily, there are formulas for that, which I’ll call Black-Scholes and the greeks. Luckier still, lots of people use that formula, so you don’t even need to write it, you can <a href="https://rcalxrc08.github.io/FinancialToolbox.jl/stable/bls/#Black-and-Scholes-related-functions-1">borrow someone else’s work</a>. It’s got a handful of arguments that change throughout the day- how much the stock is worth, how wiggly the stock is, how long the option lives, even how much other people are selling Treasury bonds for.</p>

<p>So, what you’re supposed to do is for e.g. every minute of the day, look up all these arguments, run Black-Scholes and its siblings on the data, and come back with a vector of results.</p>



<p>But that sucks. Let’s do something easier. All those arguments? They don’t <em>really</em> change much every minute, or even throughout the day. So let’s ignore the nasty <a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model">exponents and distributions and such that make Black-Scholes slow</a>, and pretend it’s a friendly linear function. After all, when you zoom in enough, <a href="https://brilliant.org/wiki/taylor-series-approximation/">most functions look pretty linear</a>. So the idea is:</p>

<p><strong>Convert a complex function into a linear function, by taking its gradient at a nearby point</strong></p>

<p>We’ll see how useful this idea is, by making a “function approximation measurement function”:</p>
<pre><code>function do_approx_check(func, center, delta_size, time = false)
</code></pre>
<p>This will take in any function, along with a representative point for arguments to that function and a guess about how much “real” values vary from that point. Then we’ll try approximating it, optionally timing the “real” and approximate version.</p>


<p>Building a gradient approximation system of arbitrary functions in Julia is surprisingly easy, thanks to systems like <a href="https://fluxml.ai/Zygote.jl/latest/">Zygote</a> to differentiate arbitrary code. We first calculate the “real” result and the gradient of the function at the center of our mess of points. Then for each point in our collection, we update the result by the change in that point from the center, times the gradient.</p>

<div><div><pre><code><span>using</span> <span>Zygote</span>
<span>compute_approx_from_deltas</span><span>(</span><span>deltas</span><span>,</span> <span>center_grad</span><span>,</span> <span>center_val</span><span>)</span> <span>=</span> <span>(</span><span>deltas</span> <span>*</span> <span>center_grad</span><span>)</span> <span>.+</span> <span>center_val</span>
<span>function</span><span> compute_approx</span><span>(</span><span>collection</span><span>,</span> <span>func</span><span>)</span>
  <span>center</span> <span>=</span> <span>mean</span><span>(</span><span>collection</span><span>,</span> <span>dims</span><span>=</span><span>2</span><span>)</span>
  <span>deltas</span> <span>=</span> <span>transpose</span><span>(</span><span>collection</span> <span>.-</span> <span>center</span><span>[</span><span>:</span><span>,</span><span>1</span><span>])</span>
  <span>center_grad</span> <span>=</span> <span>gradient</span><span>(</span><span>func</span><span>,</span> <span>center</span><span>)[</span><span>1</span><span>]</span>
  <span>center_val</span> <span>=</span> <span>func</span><span>(</span><span>center</span><span>)</span>
  <span>return</span> <span>compute_approx_from_deltas</span><span>(</span><span>deltas</span><span>,</span> <span>center_grad</span><span>,</span> <span>center_val</span><span>)</span>
<span>end</span>
</code></pre></div></div>



<p>Real market data is expensive, so for this example, we’ll just pretend. First, let’s pick a specific function from <a href="https://rcalxrc08.github.io/FinancialToolbox.jl/stable/bls/#Black-and-Scholes-related-functions-1">the Julia Financial Toolbox</a>. We’ll go with <code>FinancialToolbox.blslambda</code>,<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> which is documented as:</p>

<div><div><pre><code><span>Black</span> <span>&amp;</span> <span>Scholes</span> <span>Lambda</span> <span>for</span> <span>European</span> <span>Options</span>

	<span>Λ</span><span>=</span><span>blslambda</span><span>(</span><span>S0</span><span>,</span><span>K</span><span>,</span><span>r</span><span>,</span><span>T</span><span>,</span><span>σ</span><span>,</span><span>d</span><span>=</span><span>0.0</span><span>,</span><span>FlagIsCall</span><span>=</span><span>true</span><span>)</span>
<span>Where</span><span>:</span>

	<span>S0</span>         <span>=</span> <span>Value</span> <span>of</span> <span>the</span> <span>Underlying</span><span>.</span>
	<span>K</span>          <span>=</span> <span>Strike</span> <span>Price</span> <span>of</span> <span>the</span> <span>Option</span><span>.</span>
	<span>r</span>          <span>=</span> <span>Zero</span> <span>Rate</span><span>.</span>
	<span>T</span>          <span>=</span> <span>Time</span> <span>to</span> <span>Maturity</span> <span>of</span> <span>the</span> <span>Option</span><span>.</span>
	<span>σ</span>          <span>=</span> <span>Implied</span> <span>Volatility</span>
	<span>d</span>          <span>=</span> <span>Implied</span> <span>Dividend</span> <span>of</span> <span>the</span> <span>Underlying</span><span>.</span>
	<span>FlagIsCall</span> <span>=</span> <span>true</span> <span>for</span> <span>Call</span> <span>Options</span><span>,</span> <span>false</span> <span>for</span> <span>Put</span> <span>Options</span><span>.</span>

	<span>Λ</span>          <span>=</span> <span>lambda</span> <span>of</span> <span>the</span> <span>European</span> <span>Option</span><span>.</span>
<span>Example</span>

<span>julia</span><span>&gt;</span> <span>blslambda</span><span>(</span><span>10.0</span><span>,</span><span>10.0</span><span>,</span><span>0.01</span><span>,</span><span>2.0</span><span>,</span><span>0.2</span><span>,</span><span>0.01</span><span>)</span>
<span>4.945909973725978</span>
</code></pre></div></div>

<p>Look, it comes with an example set of arguments, that’s nice. We’ll use that as our center point.</p>

<p>For ease of code, it would be nice to have our computation logic take a vector/1-d matrix instead of distinct arguments. Easy enough, we’ll just make a wrapper function:</p>

<div><div><pre><code><span>blsvec</span><span>(</span><span>v</span><span>)</span> <span>=</span> <span>blslambda</span><span>(</span><span>v</span><span>[</span><span>1</span><span>],</span> <span>v</span><span>[</span><span>2</span><span>],</span> <span>v</span><span>[</span><span>3</span><span>],</span> <span>v</span><span>[</span><span>4</span><span>],</span> <span>v</span><span>[</span><span>5</span><span>],</span> <span>v</span><span>[</span><span>6</span><span>]);</span>
<span>#blsvec(v) = blslambda(v...) # This works equivilantly, but is super slow, maybe because it triggers complex unboxing logic?</span>
</code></pre></div></div>

<p>Next, we’ll have our do_approx_check function build a collection of data around that representative point:</p>

<div><div><pre><code><span>function</span><span> do_approx_check</span><span>(</span><span>func</span><span>,</span> <span>center</span><span>,</span> <span>delta_size</span><span>,</span> <span>time</span> <span>=</span> <span>false</span><span>)</span>
  <span># Set up samples</span>
  <span>num_samples</span> <span>=</span> <span>1_000_000</span>
  <span>deltas</span> <span>=</span> <span>transpose</span><span>((</span>
            <span>rand</span><span>(</span><span>Float32</span><span>,</span> <span>length</span><span>(</span><span>center</span><span>),</span> <span>num_samples</span><span>)</span>
                <span>.*</span> <span>(</span><span>delta_size</span> <span>*</span> <span>2</span><span>)</span> <span>.-</span> <span>delta_size</span><span>)</span> <span>.*</span> <span>center</span><span>)</span>
  <span>collection</span> <span>=</span> <span>center</span> <span>.+</span> <span>transpose</span><span>(</span><span>deltas</span><span>)</span>
</code></pre></div></div>
<p>So, we’ll pick 1,000,000 random points, where each dimension of each point is at the center +- delta_size percent.</p>

<p>And now we’re ready to compare running the “real” function with our approximation, and optionally time it too:</p>
<div><div><pre><code>  <span># Compute results correctly and approximately, compare them </span>
  <span>results</span> <span>=</span> <span>compute_exact</span><span>(</span><span>collection</span><span>,</span> <span>func</span><span>)</span>
  <span>results_approx</span> <span>=</span> <span>compute_approx</span><span>(</span><span>collection</span><span>,</span> <span>func</span><span>)</span>
  <span>error</span> <span>=</span> <span>sum</span><span>(</span><span>abs</span><span>.</span><span>(</span><span>results</span> <span>-</span> <span>results_approx</span><span>))</span> <span>/</span> <span>sum</span><span>(</span><span>abs</span><span>.</span><span>(</span><span>results</span><span>))</span>
  <span>center_val</span> <span>=</span> <span>func</span><span>(</span><span>center</span><span>)</span>

  <span>error_from_no_change</span> <span>=</span> <span>sum</span><span>(</span><span>abs</span><span>.</span><span>(</span><span>results</span> <span>.-</span> <span>center_val</span><span>))</span> <span>/</span> <span>sum</span><span>(</span><span>abs</span><span>.</span><span>(</span><span>results</span><span>))</span>
  <span>println</span><span>(</span><span>&#34;Gradient approximation error for delta size &#34;</span><span>,</span> <span>delta_size</span> <span>*</span> <span>100</span><span>,</span> <span>&#34;%: &#34;</span><span>,</span> <span>error</span><span>*</span><span>100</span><span>,</span> <span>&#34;%, vs error assuming no change of &#34;</span><span>,</span> <span>error_from_no_change</span> <span>*</span> <span>100</span><span>,</span> <span>&#34;%&#34;</span> <span>)</span>
</code></pre></div></div>
<p>Note that for reference, we compare to an even faster, more approximate method: Ignore the specifics of each point, and just pretend they all have the same result as the center point.</p>

<div><div><pre><code>  <span>if</span> <span>time</span>
    <span>center_grad</span> <span>=</span> <span>gradient</span><span>(</span><span>func</span><span>,</span> <span>center</span><span>)[</span><span>1</span><span>]</span>
    <span># Neat ways to look closer:</span>
    <span>#@code_llvm compute_approx(collection, func)</span>
    <span>#@code_native syntax=:intel debuginfo=:none compute_approx(deltas, center_grad, center_val)</span>
    <span>@btime</span> <span>compute_exact</span><span>(</span><span>$</span><span>collection</span><span>,</span> <span>$</span><span>func</span><span>)</span>
    <span>@btime</span> <span>compute_approx</span><span>(</span><span>$</span><span>collection</span><span>,</span> <span>$</span><span>func</span><span>)</span>
    <span>@btime</span> <span>compute_approx_from_deltas</span><span>(</span><span>$</span><span>deltas</span><span>,</span> <span>$</span><span>center_grad</span><span>,</span> <span>$</span><span>center_val</span><span>)</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>


<p>Before we see how “wrong” the approximation is, ask yourself, “compared to what?”. The intuitive thing is to see the blslambda() function as the absolute ground truth- after all, it’s commented, has an official looking github, and doesn’t say anything about approximation. But the real Black-Scholes formula uses real valued numbers and exponents and Gaussian error functions, and you’re asking a computer to evaluate it using only discrete physical states. Every line of code the blslambda() function relies on is at best an approximation of the true mathematical function- sometimes <a href="https://randomascii.wordpress.com/2014/10/09/intel-underestimates-error-bounds-by-1-3-quintillion/">a surprisingly bad approximation</a>.</p>

<p>If you’re worried that using a gradient approximation means you’re at best computing a rough sketch of the real function, take comfort: your hope was lost before you even began. Rejoice in the freedom that failure gives you to make your own choices about a speed/accuracy tradeoff.</p>


<p>So, how close is the approximation? When the points don’t move much, very close:</p>
<div><div><pre><code><span>do_approx_check</span><span>.</span><span>(</span><span>blsvec</span><span>,</span> <span>[</span><span>0.01</span><span>,</span> <span>0.05</span><span>,</span> <span>0.1</span><span>,</span> <span>0.2</span><span>,</span> <span>0.4</span><span>],</span> <span>Ref</span><span>([</span><span>10.0</span><span>,</span><span>10.0</span><span>,</span><span>0.01</span><span>,</span><span>2.0</span><span>,</span><span>0.2</span><span>,</span><span>0.01</span><span>]));</span>
</code></pre></div></div>

<div><div><pre><code>Gradient approximation error for delta size 1.000%: 0.013%, vs error assuming no change of 1.062%
Gradient approximation error for delta size 5.000%: 0.315%, vs error assuming no change of 5.315%
Gradient approximation error for delta size 10.000%: 1.259%, vs error assuming no change of 10.639%
Gradient approximation error for delta size 20.000%: 5.062%, vs error assuming no change of 21.437%
Gradient approximation error for delta size 40.000%: 20.430%, vs error assuming no change of 43.637%
</code></pre></div></div>

<p>Roughly 1/100th the error of just taking the average for small changes. Error increases substantially the bigger the deltas, which makes sense- the more you move, the less exponents look linear.</p>

<p>Great, so we’re close, but did we save any time?</p>
<div><div><pre><code><span>do_approx_check</span><span>(</span><span>blsvec</span><span>,</span> <span>[</span><span>0.05</span><span>],</span> <span>[</span><span>10.0</span><span>,</span><span>10.0</span><span>,</span><span>0.01</span><span>,</span><span>2.0</span><span>,</span><span>0.2</span><span>,</span><span>0.01</span><span>],</span> <span>true</span><span>);</span>
</code></pre></div></div>

<div><div><pre><code>  75.240 ms (6 allocations: 45.78 MiB)
  16.934 ms (416 allocations: 61.05 MiB)
  5.786 ms (4 allocations: 15.26 MiB)
</code></pre></div></div>
<p>The approximations are a <em>lot</em> faster- we get a 5x speedup even if we include the time to compute the average and the gradient. If we take those as given, and look at the time for a marginal point, we see a <em>13x</em> speedup.</p>


<p>Performance vs complexity is a classic software engineering tradeoff. Others include bandwidth, latency, space, downtime. But a system falls somewhere on <a href="https://en.wikipedia.org/wiki/Approximate_computing">an accuracy spectrum</a> too, and consciously deciding on an application’s accuracy requirement can offer overwhelming gains in other areas.</p>


<div role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In my testing, I actually remove an argument bounds check from the Financial Toolbox- I’m really only interested in comparing the actual computation. <a href="#fnref:1" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>


    </div></div>
  </body>
</html>
