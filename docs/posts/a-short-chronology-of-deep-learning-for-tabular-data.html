<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html">Original</a>
    <h1>A Short Chronology of Deep Learning for Tabular Data</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        




<div>
  

  <article>
    <p>In my lectures, I emphasize that deep learning is really good for unstructured data (essentially, that’s the opposite of tabular data). Deep learning is sometimes referred to as “representation learning” because its strength is the ability to learn the feature extraction pipeline. Most tabular datasets already represent (typically manually) extracted features, so there shouldn’t be a significant advantage using deep learning on these.</p>

<p><img src="https://sebastianraschka.com/images/blog/2022/deep-learning-for-tabular-data/unstructured-structured.jpeg" alt="Deep learning for tabular data"/></p>

<p>Nonetheless, many researchers recently tried developing special-purpose deep learning methods for tabular datasets. Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional methods or counterexamples. So, with this short post, I aim to briefly summarize the major papers on deep tabular learning I am currently aware of. It is possible that I skipped or forgot a few. I am happy to curate and update this list for future reference, so <a href="https://twitter.com/rasbt/status/1551252319689400324">please let me know</a> if there is something I missed.</p>

<p>(By the way, many earlier papers use multilayer perceptrons on tabular datasets and refer to it as “deep learning” – several computational biology papers that train multilayer perceptrons on molecular fingerprint data come to mind. However, to me, multilayer perceptrons are not really “deep learning,” so I am not listing those.)</p>

<p>I want to emphasize that no matter how interesting or promising deep tabular methods look, I still recommend using a conventional machine learning method as a baseline. There is a reason why I cover conventional machine learning before deep learning in <a href="https://sebastianraschka.com/books/">my book</a>.</p>
      <h2 id="deep-learning-for-tabular-data-in-reverse-chronological-order">
        
        
          Deep Learning For Tabular Data In Reverse Chronological Order <a href="#deep-learning-for-tabular-data-in-reverse-chronological-order">#</a>
        
        
      </h2>
    

<p>Below is a (growing) list of relevant papers along with links and short summaries. I will try to keep this list up to date as new publications arrive. <a href="https://twitter.com/rasbt/status/1551252319689400324">Please let me know</a> if you know of some additional papers I may have missed!</p>

<p>(Last updated: August 31, 2022)</p>


      <h3 id="sections">
        
        
          Sections <a href="#sections">#</a>
        
        
      </h3>
    

<ul id="markdown-toc">
  <li><a href="#deep-learning-for-tabular-data-in-reverse-chronological-order" id="markdown-toc-deep-learning-for-tabular-data-in-reverse-chronological-order">Deep Learning For Tabular Data In Reverse Chronological Order</a>    <ul>
      <li><a href="#sections" id="markdown-toc-sections">Sections</a></li>
      <li><a href="#why-do-tree-based-models-still-outperform-deep-learning-on-tabular-data" id="markdown-toc-why-do-tree-based-models-still-outperform-deep-learning-on-tabular-data">Why do tree-based models still outperform deep learning on tabular data?</a></li>
      <li><a href="#gate-gated-additive-tree-ensemble-for-tabular-classification-and-regression" id="markdown-toc-gate-gated-additive-tree-ensemble-for-tabular-classification-and-regression">GATE: Gated Additive Tree Ensemble for Tabular Classification and Regression</a></li>
      <li><a href="#transfer-learning-with-deep-tabular-models" id="markdown-toc-transfer-learning-with-deep-tabular-models">Transfer Learning with Deep Tabular Models</a></li>
      <li><a href="#scalable-interpretability-via-polynomials" id="markdown-toc-scalable-interpretability-via-polynomials">Scalable Interpretability via Polynomials</a></li>
      <li><a href="#hopular-modern-hopfield-networks-for-tabular-data" id="markdown-toc-hopular-modern-hopfield-networks-for-tabular-data">Hopular: Modern Hopfield Networks for Tabular Data</a></li>
      <li><a href="#neural-basis-models-for-interpretability" id="markdown-toc-neural-basis-models-for-interpretability">Neural Basis Models for Interpretability</a></li>
      <li><a href="#on-embeddings-for-numerical-features-in-tabular-deep-learning" id="markdown-toc-on-embeddings-for-numerical-features-in-tabular-deep-learning">On Embeddings for Numerical Features in Tabular Deep Learning</a></li>
      <li><a href="#danets-deep-abstract-networks-for-tabular-data-classification-and-regression-new-since-last-edit-on-aug-31" id="markdown-toc-danets-deep-abstract-networks-for-tabular-data-classification-and-regression-new-since-last-edit-on-aug-31">DANETs: Deep Abstract Networks for Tabular Data Classification and Regression (New Since Last Edit on Aug 31)</a></li>
      <li><a href="#deep-neural-networks-and-tabular-data-a-survey" id="markdown-toc-deep-neural-networks-and-tabular-data-a-survey">Deep Neural Networks and Tabular Data: A Survey</a></li>
      <li><a href="#arm-net-adaptive-relation-modeling-network-for-structured-data" id="markdown-toc-arm-net-adaptive-relation-modeling-network-for-structured-data">ARM-Net: Adaptive Relation Modeling Network for Structured Data</a></li>
      <li><a href="#scarf-self-supervised-contrastive-learning-using-random-feature-corruption" id="markdown-toc-scarf-self-supervised-contrastive-learning-using-random-feature-corruption">SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption</a></li>
      <li><a href="#revisiting-deep-learning-models-for-tabular-data" id="markdown-toc-revisiting-deep-learning-models-for-tabular-data">Revisiting Deep Learning Models for Tabular Data</a></li>
      <li><a href="#xbnet-an-extremely-boosted-neural-network" id="markdown-toc-xbnet-an-extremely-boosted-neural-network">XBNet: An Extremely Boosted Neural Network</a></li>
      <li><a href="#tabular-data-deep-learning-is-not-all-you-need" id="markdown-toc-tabular-data-deep-learning-is-not-all-you-need">Tabular Data: Deep Learning is Not All You Need</a></li>
      <li><a href="#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning" id="markdown-toc-self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning">Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning</a></li>
      <li><a href="#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training" id="markdown-toc-saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training">SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training</a></li>
      <li><a href="#denoising-autoencoders-daes-for-tabular-data" id="markdown-toc-denoising-autoencoders-daes-for-tabular-data">Denoising Autoencoders (DAEs) for Tabular Data</a></li>
      <li><a href="#converting-tabular-data-into-images-for-deep-learning-with-convolutional-neural-networks" id="markdown-toc-converting-tabular-data-into-images-for-deep-learning-with-convolutional-neural-networks">Converting Tabular Data Into Images for Deep Learning with Convolutional Neural Networks</a></li>
      <li><a href="#tabtransformer-tabular-data-modeling-using-contextual-embeddings" id="markdown-toc-tabtransformer-tabular-data-modeling-using-contextual-embeddings">TabTransformer: Tabular Data Modeling Using Contextual Embeddings</a></li>
      <li><a href="#vime-extending-the-success-of-self--and-semi-supervised-learning-to-tabular-domain" id="markdown-toc-vime-extending-the-success-of-self--and-semi-supervised-learning-to-tabular-domain">VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain</a></li>
      <li><a href="#a-novel-method-for-classification-of-tabular-data-using-convolutional-neural-networks" id="markdown-toc-a-novel-method-for-classification-of-tabular-data-using-convolutional-neural-networks">A Novel Method for Classification of Tabular Data Using Convolutional Neural Networks</a></li>
      <li><a href="#neural-additive-models-interpretable-machine-learning-with-neural-nets" id="markdown-toc-neural-additive-models-interpretable-machine-learning-with-neural-nets">Neural Additive Models: Interpretable Machine Learning with Neural Nets</a></li>
      <li><a href="#neural-oblivious-decision-ensembles-for-deep-learning-on-tabular-data" id="markdown-toc-neural-oblivious-decision-ensembles-for-deep-learning-on-tabular-data">Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data</a></li>
      <li><a href="#tabnet-attentive-interpretable-tabular-learning" id="markdown-toc-tabnet-attentive-interpretable-tabular-learning">TabNet: Attentive Interpretable Tabular Learning</a></li>
      <li><a href="#supertml-two-dimensional-word-embedding-for-the-precognition-on-structured-tabular-data" id="markdown-toc-supertml-two-dimensional-word-embedding-for-the-precognition-on-structured-tabular-data">SuperTML: Two-Dimensional Word Embedding for the Precognition on Structured Tabular Data</a></li>
      <li><a href="#autoint-automatic-feature-interaction-learning-via-self-attentive-neural-networks" id="markdown-toc-autoint-automatic-feature-interaction-learning-via-self-attentive-neural-networks">AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
</ul><hr/>



<p>[18 Jul 2022]</p>
      <h3 id="why-do-tree-based-models-still-outperform-deep-learning-on-tabular-data">
        
        
          Why do tree-based models still outperform deep learning on tabular data? <a href="#why-do-tree-based-models-still-outperform-deep-learning-on-tabular-data">#</a>
        
        
      </h3>
    

<p>by Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2207.08815">https://arxiv.org/abs/2207.08815</a></p>

<p>🖥 Code: <a href="https://github.com/LeoGrin/tabular-benchmark">https://github.com/LeoGrin/tabular-benchmark</a></p>

<ul>
  <li>
    <p>The main takeaway is that tree-based models (random forests and XGBoost) outperform deep learning methods for tabular data on medium-sized datasets (10k training examples).</p>
  </li>
  <li>
    <p>The gap between tree-based models and deep learning becomes narrower as the dataset size increases (here: 10k -&gt; 50k).</p>
  </li>
  <li>
    <p>Solid experiments and thorough investigation into the role of uninformative features: uninformative features harm deep learning methods more than tree-based methods.</p>
  </li>
  <li>
    <p>Small caveats: some of the recent tabular methods for deep learning were not considered; “large” datasets are only 50k training examples (small in many industry domains.)</p>
  </li>
  <li>
    <p>Experiments based on 45 tabular datasets; numerical and mixed numerical-categorical; classification and regression datasets; 10k training examples with balanced classes for main experiments; 50k datasets for “large” dataset experiments.</p>
  </li>
</ul>

<hr/>



<p>[18 Jul 2022]</p>
      <h3 id="gate-gated-additive-tree-ensemble-for-tabular-classification-and-regression">
        
        
          GATE: Gated Additive Tree Ensemble for Tabular Classification and Regression <a href="#gate-gated-additive-tree-ensemble-for-tabular-classification-and-regression">#</a>
        
        
      </h3>
    

<p>by Manu Joseph, Harsh Raj</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2207.08548">https://arxiv.org/abs/2207.08548</a></p>

<p>🖥 Code: Not available</p>

<ul>
  <li>
    <p>In essense, GATE can be understood as a hierachical stacking of decision tree stumps.</p>
  </li>
  <li>
    <p>Similar to <a href="https://arxiv.org/abs/1909.06312">NODE</a> (described further below) the proposed GATE method is based on differentiable decision trees. In addition, the GATE architecture is inspired by the <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a> gating mechanisms and also includes self-attention for (re)weighting the outputs.</p>
  </li>
  <li>
    <p>Across  5 large datasets (3 classification and 2 regression datasets, ranging from 26M to 800 M training examples), GATE achieved the highest average rank (tied with LightGBM).</p>
  </li>
  <li>
    <p>The paper comes without code, so we have to accept the results with some reservations.</p>
  </li>
</ul>

<hr/>



<p>[30 Jun 2022]</p>
      <h3 id="transfer-learning-with-deep-tabular-models">
        
        
          Transfer Learning with Deep Tabular Models <a href="#transfer-learning-with-deep-tabular-models">#</a>
        
        
      </h3>
    

<p>by Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, Micah Goldblum</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2206.15306">https://arxiv.org/abs/2206.15306</a></p>

<p>🖥 Code: <a href="https://github.com/LevinRoman/tabular-transfer-learning">https://github.com/LevinRoman/tabular-transfer-learning</a></p>

<ul>
  <li>
    <p>In contrast to gradient boosting, deep learning methods for tabular data can be pretrained on upstream data to increase performance on the target dataset.</p>
  </li>
  <li>
    <p>Supervised pretraining is better than self-supervised pretraining in a tabular dataset context.</p>
  </li>
  <li>
    <p>Multilayer perceptrons outperform transformer-based deep neural networks if target data is scarce.</p>
  </li>
  <li>
    <p>Proposes a pseudo-feature method for cases where the upstream and target feature sets differ.</p>
  </li>
  <li>
    <p>Medical diagnosis benchmark dataset; patient data with 11 diagnosis targets where features between upstream and target data are related but may differ.</p>
  </li>
</ul>

<hr/>



<p>[16 Jun 2022]</p>
      <h3 id="scalable-interpretability-via-polynomials">
        
        
          Scalable Interpretability via Polynomials <a href="#scalable-interpretability-via-polynomials">#</a>
        
        
      </h3>
    

<p>by Filip Radenovic, Abhimanyu Dubey, Dhruv Mahajan</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2205.14120">https://arxiv.org/abs/2205.14120</a></p>

<p>🖥 Code: <a href="https://github.com/facebookresearch/nbm-spam">https://github.com/facebookresearch/nbm-spam</a></p>

<ul>
  <li>
    <p>Similar to the <a href="https://arxiv.org/abs/2004.13912">Neural Additive Model (NAM)</a> (further below), the proposed Scalable Polynomial Additive Models (SPAM) is a type of generalized additive model and its primary goal is interpretability. However, in contrast to NAM, SPAM is easier to scale since it require an individual neural network for each feature.</p>
  </li>
  <li>
    <p>The SPAM-Neural model (which shared the same multilayer perceptron architecture as NAM) outperformed NAM on all 3 tabular datasets and outperformed XGBoost in 2 out of 3 cases.</p>
  </li>
</ul>

<hr/>



<p>[1 Jun 2022]</p>
      <h3 id="hopular-modern-hopfield-networks-for-tabular-data">
        
        
          Hopular: Modern Hopfield Networks for Tabular Data <a href="#hopular-modern-hopfield-networks-for-tabular-data">#</a>
        
        
      </h3>
    

<p>by Bernhard Schäfl, Lukas Gruber, Angela Bitto-Nemling, Sepp Hochreiter</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2206.00664">https://arxiv.org/abs/2206.00664</a></p>

<p>🖥 Code: <a href="https://github.com/ml-jku/hopular">https://github.com/ml-jku/hopular</a></p>

<ul>
  <li>
    <p>Proposes a new deep learning architecture for small- to medium-sized datasets based on <a href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield Networks</a> – a form of recurrent neural networks.</p>
  </li>
  <li>
    <p>Across 21 UCI datasets, the proposed Hopular network has the best median rank (7.5); the closest median rank is by a <a href="https://arxiv.org/abs/2106.02584">Non-Parametric Transformers</a> (11.0). XGBoost has a median rank of 12.0.</p>
  </li>
  <li>
    <p>Experiments were done on 21 UCI datasets ranging from 208 to 1000 examples. Nine datasets also contain categorical features.</p>
  </li>
  <li>
    <p>In the article <a href="https://medium.com/@tunguz/trouble-with-hopular-6649f22fa2d3">Trouble with Hopular</a> Bojan Tunguz describes that the tree-based reference methods such as HistGradientBoosting and XGBoost were insufficiently tuned. However, when properly tuned, tree-based gradient boosting methods outperform the proposed Hopular net.</p>
  </li>
</ul>

<hr/>



<p>[27 May 2022]</p>
      <h3 id="neural-basis-models-for-interpretability">
        
        
          Neural Basis Models for Interpretability <a href="#neural-basis-models-for-interpretability">#</a>
        
        
      </h3>
    

<p>📝 Paper: <a href="https://arxiv.org/abs/2205.14120">https://arxiv.org/abs/2205.14120</a></p>

<p>🖥 Code: <a href="https://github.com/facebookresearch/nbm-spam">https://github.com/facebookresearch/nbm-spam</a></p>

<ul>
  <li>
    <p>Similar to the <a href="https://arxiv.org/abs/2004.13912">Neural Additive Model (NAM)</a> (further below), the Neural Basis Model (NBM) is a type of generalized additive model. Similar to NAM, its primary goal is interpretability. However, in contrast to NAM, NBM is easier to scale since it is a single neural network (vs one neural network per feature).</p>
  </li>
  <li>
    <p>The experiments include 4 tabular datasets, 1 regression, 1 binary classification, and 2 multi-class classification datasets. The dataset sizes range from 7k to 406k training examples.</p>
  </li>
  <li>
    <p>The NBM outperforms the NAM on all tabular datasets (except 1 dataset for which NAM results were not available). While the NBM predictive performance is substantially worse than XGBoost, it outperforms <a href="https://dl.acm.org/doi/10.1145/2487575.2487579">Explainable Boosting Machines</a> in all cases.</p>
  </li>
</ul>

<hr/>



<p>[15 Mar 2022]</p>
      <h3 id="on-embeddings-for-numerical-features-in-tabular-deep-learning">
        
        
          On Embeddings for Numerical Features in Tabular Deep Learning <a href="#on-embeddings-for-numerical-features-in-tabular-deep-learning">#</a>
        
        
      </h3>
    

<p>by Yury Gorishniy, Ivan Rubachev, Artem Babenko</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2203.05556">https://arxiv.org/abs/2203.05556</a></p>

<p>🖥 Code: <a href="https://github.com/Yura52/tabular-dl-num-embeddings">https://github.com/Yura52/tabular-dl-num-embeddings</a></p>

<ul>
  <li>
    <p>Instead of designing new architectures for end-to-end learning, the authors focus on embedding methods for tabular data: (1) a piecewise linear encoding of scalar values and (2) periodic activation-based embeddings.</p>
  </li>
  <li>
    <p>Experiments show that the embeddings are not only beneficial for transformers but other methods as well – multilayer perceptrons are competitive to transformers when trained on the proposed embeddings.</p>
  </li>
  <li>
    <p>Using the proposed embeddings, ResNet, multilayer perceptrons, and transformers outperform CatBoost and XGBoost on several (but not all) datasets.</p>
  </li>
  <li>
    <p>Small caveat: I would have liked to see a control experiment where the authors trained CatBoost and XGboost on the proposed embeddings.</p>
  </li>
</ul>

<hr/>



<p>[06 Dec 2021]</p>
      <h3 id="danets-deep-abstract-networks-for-tabular-data-classification-and-regression-new-since-last-edit-on-aug-31">
        
        
          DANETs: Deep Abstract Networks for Tabular Data Classification and Regression (New Since Last Edit on Aug 31) <a href="#danets-deep-abstract-networks-for-tabular-data-classification-and-regression-new-since-last-edit-on-aug-31">#</a>
        
        
      </h3>
    

<p>by Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z. Chen, Jian Wu</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2112.02962">https://arxiv.org/abs/2112.02962</a></p>

<p>🖥 Code: <a href="https://github.com/WhatAShot/DANet">https://github.com/WhatAShot/DANet</a></p>

<p>🐦 Twitter: <a href="https://twitter.com/rasbt/status/1564982156887310339?s=20&amp;t=AI1HRigiz4JdvDweTD7TMg">Explanation thread</a></p>

<ul>
  <li>
    <p>The DANets architecture is centered around a newly proposed Abstract Layer (ABSTLAY) that is focused on two main steps: 1) feature selection and 2) feature abstraction.</p>
  </li>
  <li>
    <p>The core idea behind ABSTLAY is to group correlated features (via a sparse learnable mask) and create higher-level, abstract features from these.</p>
  </li>
  <li>
    <p>Steps 1 and 2 above are grouped into a block, and the DANet architecture then consists of a stacking of such blocks.</p>
  </li>
  <li>
    <p>The ABSTLAY itself consists of the learnable sparse mask as mentioned above; in addition it has a shortcut connection that adds the raw features back to each block (it is analogous to the shortcut connection in ResNet, however, ResNet adds the previous instead of the raw features back).</p>
  </li>
  <li>
    <p>While the architecture can’t handle categorical features implicitely, the authors used a leave-one-out encoding from scikit-learn for categorical feature encoding.</p>
  </li>
  <li>
    <p>The method was evaluated on 4 classification and 3 regression datasets, and the DANet-32 architecture achived the best performance on 4 out of 7 datasets.</p>
  </li>
</ul>

<hr/>



<p>[5 Oct 2021]</p>
      <h3 id="deep-neural-networks-and-tabular-data-a-survey">
        
        
          Deep Neural Networks and Tabular Data: A Survey <a href="#deep-neural-networks-and-tabular-data-a-survey">#</a>
        
        
      </h3>
    

<p>by Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, Gjergji Kasneci</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2110.01889">https://arxiv.org/abs/2110.01889</a></p>

<p>🖥 Code: <a href="https://github.com/kathrinse/TabSurvey">https://github.com/kathrinse/TabSurvey</a></p>

<ul>
  <li>
    <p>A survey paper that investigates and compares deep neural networks proposed for tabular datasets.</p>
  </li>
  <li>
    <p>The paper is accompanied by benchmark code, and since the paper does <em>not</em> propose a new method for tabular data, the results may be more objective than others.</p>
  </li>
  <li>
    <p>Based on the results, gradient-boosted tree ensembles still mostly outperform deep learning methods on tabular datasets.</p>
  </li>
</ul>

<hr/>



<p>[5 Jul 2021]</p>
      <h3 id="arm-net-adaptive-relation-modeling-network-for-structured-data">
        
        
          ARM-Net: Adaptive Relation Modeling Network for Structured Data <a href="#arm-net-adaptive-relation-modeling-network-for-structured-data">#</a>
        
        
      </h3>
    

<p>by Shaofeng Cai, Kaiping Zheng, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Meihui Zhang</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2107.01830">https://arxiv.org/abs/2107.01830</a></p>

<p>🖥 Code: <a href="https://github.com/nusdbsystem/ARM-Net">https://github.com/nusdbsystem/ARM-Net</a></p>

<ul>
  <li>
    <p>The proposed method transforms inputs into an exponential space and uses a sparse attention method to generate interaction weights to obtain cross-features for prediction.</p>
  </li>
  <li>
    <p>This is essentially a transformer-inspired embedding method followed by a multilayer perceptron for prediction.</p>
  </li>
  <li>
    <p>Besides logistic regression, the paper does not include comparisons with conventional machine learning methods.</p>
  </li>
</ul>

<hr/>



<p>[29 Jun 2021]</p>
      <h3 id="scarf-self-supervised-contrastive-learning-using-random-feature-corruption">
        
        
          SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption <a href="#scarf-self-supervised-contrastive-learning-using-random-feature-corruption">#</a>
        
        
      </h3>
    

<p>by Dara Bahri, Heinrich Jiang, Yi Tay, Donald Metzler</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2106.15147">https://arxiv.org/abs/2106.15147</a></p>

<p>🖥 Code: N/A</p>

<ul>
  <li>
    <p>No code available, so results are not directly reproducible and must be taken with a grain of salt.</p>
  </li>
  <li>
    <p>The paper proposes a contrastive loss for self-supervised learning for tabular data</p>
  </li>
  <li>
    <p>Experiments were done on 69 classification datasets, and the results showed that the self-supervised approach is an improvement compared to purely supervised approaches.</p>
  </li>
</ul>

<hr/>



<p>[22 Jun 2021]</p>
      <h3 id="revisiting-deep-learning-models-for-tabular-data">
        
        
          Revisiting Deep Learning Models for Tabular Data <a href="#revisiting-deep-learning-models-for-tabular-data">#</a>
        
        
      </h3>
    

<p>Paper: <a href="https://arxiv.org/abs/2106.11959">https://arxiv.org/abs/2106.11959</a></p>

<p>Code: <a href="https://github.com/Yura52/rtdl">https://github.com/Yura52/rtdl</a></p>

<ul>
  <li>
    <p>In this paper, the researchers discuss the issue of improper baselines in the deep learning for tabular data literature.</p>
  </li>
  <li>
    <p>The main contributions of this paper are centered around two strong baselines: one is a ResNet-like architecture, and the other is a transformer-based architecture called FT-Transformer (Feature Tokenizer + Transformer).</p>
  </li>
  <li>
    <p>Across all 11 datasets considered in this study, the FT-Transformer outperforms other deep tabular methods in 6 cases and has the best overall rank. The most competitive deep tabular method is NODE (further below), which outperforms other methods in 4 out of 11 cases.</p>
  </li>
  <li>
    <p>In comparison with gradient boosted trees such as XGBoost and CatBoost, the FT-Transformer outperforms the former in 7 out of 11 cases; the authors conclude there is no universally superior method.</p>
  </li>
</ul>

<hr/>



<p>[9 Jun 2021]</p>
      <h3 id="xbnet-an-extremely-boosted-neural-network">
        
        
          XBNet: An Extremely Boosted Neural Network <a href="#xbnet-an-extremely-boosted-neural-network">#</a>
        
        
      </h3>
    

<p>by Tushar Sarkar</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2106.05239">https://arxiv.org/abs/2106.05239</a></p>

<p>🖥 Code: <a href="https://github.com/tusharsarkar3/XBNet">https://github.com/tusharsarkar3/XBNet</a></p>

<ul>
  <li>
    <p>The method is centered around using XGBoost models and their feature importances to initialize neural network layers.</p>
  </li>
  <li>
    <p>For the initialization, it first trains XGBoost models and derives the feature importances. Secondly, it uses the feature importances to initialize the weights of a multilayer perceptron</p>
  </li>
  <li>
    <p>After the initialization, the method trains an additional XGBoost model for each intermediate layer. Consequently, each layer consists of a weight matrix (corresponding to the fully-connected neural network layer) and a XGBoost model; during backpropagation the feature importances are used to update the neural network weights.</p>
  </li>
  <li>
    <p>The proposed XGBnet method was evaluated on 8 small datasets, including the classic Iris and Wine datasets. XGBNet outperformed XGBoost on 3 out of the 8 datasets.</p>
  </li>
</ul>

<hr/>



<p>[6 Jun 2021]</p>
      <h3 id="tabular-data-deep-learning-is-not-all-you-need">
        
        
          Tabular Data: Deep Learning is Not All You Need <a href="#tabular-data-deep-learning-is-not-all-you-need">#</a>
        
        
      </h3>
    

<p>by Ravid Shwartz-Ziv, Amitai Armon</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2106.03253">https://arxiv.org/abs/2106.03253</a></p>

<p>🖥 Code: N/A</p>

<ul>
  <li>
    <p>This paper compares XGBoost and deep learning architectures for tabular data; no new method is proposed here.</p>
  </li>
  <li>
    <p>The results show that XGBoost performs better than most deep learning methods across all datasets; however, while no deep learning dataset performs well across <em>all</em> datasets, a deep learning method usually performs better than XGBoost (except on one dataset). The takeaway is that across different tasks, XGBoost performs most consistently well.</p>
  </li>
  <li>
    <p>Another takeaway is that XGBoost requires substantially less hyperparameter tuning to perform well, which is a significant benefit in many real-life scenarios.</p>
  </li>
  <li>
    <p>The experiments with various ensembles are worth highlighting: The best results are achieved when deep neural networks are combined with XGBoost.</p>
  </li>
  <li>
    <p>No code examples are available, so everything in this paper must be taken with a large grain of salt.</p>
  </li>
</ul>

<hr/>



<p>[4 Jun 2021]</p>
      <h3 id="self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning">
        
        
          Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning <a href="#self-attention-between-datapoints-going-beyond-individual-input-output-pairs-in-deep-learning">#</a>
        
        
      </h3>
    

<p>by Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, Yarin Gal</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2106.02584">https://arxiv.org/abs/2106.02584</a></p>

<p>🖥 Code: <a href="https://github.com/OATML/Non-Parametric-Transformers">https://github.com/OATML/Non-Parametric-Transformers</a></p>

<ul>
  <li>
    <p>Proposes a deep learning method (non-parametric transformers, NPT) that processes the whole dataset simultaneously. (Note that <a href="https://arxiv.org/abs/2106.01342">SAINT</a>, uploaded two days earlier, also performs attention across both rows and columns.)</p>
  </li>
  <li>
    <p>In the NPT, self-attention is used across data points (rows) and features (columns)</p>
  </li>
  <li>
    <p>On binary classification datasets, the NPT has the best average rank among all methods; on multi-classification datasets, NPT and XGBoost are tied; on regression tasks, NPT and XGBoost are also tied but outperformed by CatBoost.</p>
  </li>
  <li>
    <p>Considering the self-attention between data points is a paradigm shift that appears strange and limited at first glance. However, making predictions on new, single data points is possible. The requirement is that the training dataset needs to be used as context. This is somewhat analogous to nearest-neighbor methods such as k-nearest neighbors and hence not an entirely new paradigm.</p>
  </li>
  <li>
    <p>Beyond tabular datasets, the authors also compare their self-attention-based architecture on small image datasets such as CIFAR-10.</p>
  </li>
</ul>

<hr/>



<p>[2 Jun 2021]</p>
      <h3 id="saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training">
        
        
          SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training <a href="#saint-improved-neural-networks-for-tabular-data-via-row-attention-and-contrastive-pre-training">#</a>
        
        
      </h3>
    

<p>by Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, Tom Goldstein</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2106.01342">https://arxiv.org/abs/2106.01342</a></p>

<p>🖥 Code: <a href="https://github.com/somepago/saint">https://github.com/somepago/saint</a></p>

<ul>
  <li>
    <p>The Self-Attention and Intersample Attention Transformer (SAINT) hybrid architecture is based on self-attention that applies attention across both rows and columns.</p>
  </li>
  <li>
    <p>Also proposes a self-supervised learning technique for pre-training under scarce data regimes.</p>
  </li>
  <li>
    <p>When looking at the average performance across all nine datasets, the proposed SAINT method tends to outperform gradient-boosted trees. The datasets ranged from 200 to 495,000 examples.</p>
  </li>
</ul>

<hr/>



<p>[Apr 2021]</p>
      <h3 id="denoising-autoencoders-daes-for-tabular-data">
        
        
          Denoising Autoencoders (DAEs) for Tabular Data <a href="#denoising-autoencoders-daes-for-tabular-data">#</a>
        
        
      </h3>
    

<p>📝 References: <a href="https://www.kaggle.com/competitions/tabular-playground-series-apr-2021/discussion/230013…">https://www.kaggle.com/competitions/tabular-playground-series-apr-2021/discussion/230013…</a></p>

<p>🖥 Code: <a href="https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder">https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder</a></p>

<ul>
  <li>
    <p>This method is based on using a <a href="https://en.wikipedia.org/wiki/Autoencoder">Denoising Autoencoder</a> (DAE) to encode tabular data so that it can be used by a linear classification layer or any other classification model.</p>
  </li>
  <li>
    <p>The DAE-based embeddings are produced by deep neural networks. For example, the DAE can be based on Transformer-based encoding modules.</p>
  </li>
  <li>
    <p>While I can’t find an original article introducing this method, it has <a href="https://www.kaggle.com/competitions/tabular-playground-series-apr-2021/discussion/230013…">won several Kaggle competions</a> in previous years. If someone knows an article that introduces this method, please let me know.</p>
  </li>
</ul>

<hr/>



<p>[1 Feb 2021]</p>
      <h3 id="converting-tabular-data-into-images-for-deep-learning-with-convolutional-neural-networks">
        
        
          Converting Tabular Data Into Images for Deep Learning with Convolutional Neural Networks <a href="#converting-tabular-data-into-images-for-deep-learning-with-convolutional-neural-networks">#</a>
        
        
      </h3>
    

<p>by Yitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne A. Evrard, James H. Doroshow, Rick L. Stevens</p>

<p>📝 Paper: <a href="https://www.nature.com/articles/s41598-021-90923-y">https://www.nature.com/articles/s41598-021-90923-y</a></p>

<p>🖥 Code: <a href="https://github.com/zhuyitan/IGTD">https://github.com/zhuyitan/IGTD</a></p>

<ul>
  <li>
    <p>The image generator for tabular data (IGTD) seems to take a similar approach to <a href="https://arxiv.org/abs/1903.06246">SuperTML</a> (further below) in encoding tabular datasets as 2D images as input to convolutional networks. It also seems to be similar to the earlier <a href="https://www.biorxiv.org/content/10.1101/2020.05.02.074203v1.abstract">TAC</a> (TAbular Convolution) method (also below).</p>
  </li>
  <li>
    <p>Interestingly, the authors omitted a comparison with SuperTML.</p>
  </li>
  <li>
    <p>A CNN with the IGTD-based images outperforms XGBoost and LightGBM.</p>
  </li>
</ul>

<hr/>



<p>[11 Dec 2020]</p>
      <h3 id="tabtransformer-tabular-data-modeling-using-contextual-embeddings">
        
        
          TabTransformer: Tabular Data Modeling Using Contextual Embeddings <a href="#tabtransformer-tabular-data-modeling-using-contextual-embeddings">#</a>
        
        
      </h3>
    

<p>by Xin Huang, Ashish Khetan, Milan Cvitkovic, Zohar Karnin</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/2012.06678">https://arxiv.org/abs/2012.06678</a></p>

<p>🖥 Code: N/A</p>

<ul>
  <li>
    <p>Several open-source implementations are available on GitHub, however, I could not find the official implementation, so the results from this paper must be taken with a grain of salt.</p>
  </li>
  <li>
    <p>The paper proposes a transformer-based architecture based on self-attention that can be applied to tabular data.</p>
  </li>
  <li>
    <p>In addition to the purely supervised regime, the authors propose a semi-supervised approach leveraging unsupervised pre-training.</p>
  </li>
  <li>
    <p>Looking at the average AUC across 15 datasets, the proposed TabTransformer (82.8) is on par with gradient-boosted trees (82.9).</p>
  </li>
</ul>

<hr/>



<p>[5 Jun 2020]</p>
      <h3 id="vime-extending-the-success-of-self--and-semi-supervised-learning-to-tabular-domain">
        
        
          VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain <a href="#vime-extending-the-success-of-self--and-semi-supervised-learning-to-tabular-domain">#</a>
        
        
      </h3>
    

<p>by Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar</p>

<p>📝 Paper: <a href="https://proceedings.neurips.cc/paper/2020/hash/7d97667a3e056acab9aaf653807b4a03-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/7d97667a3e056acab9aaf653807b4a03-Abstract.html</a></p>

<p>🖥 Code: <a href="https://github.com/jsyoon0823/VIME">https://github.com/jsyoon0823/VIME</a></p>

<ul>
  <li>
    <p>VIME (Value Imputation and Mask Estimation) includes self- and semi-supervised learning frameworks for tabular data.</p>
  </li>
  <li>
    <p>The authors provide good ablation studies showing that the semi-supervised learning variant of VIME is better than the supervised-only and self-supervised-only variants. The best VIME variant uses both self- and semi-supervised learning and outperforms XGBoost on all datasets.</p>
  </li>
  <li>
    <p>The comparison is based on only five datasets.</p>
  </li>
</ul>

<hr/>



<p>[3 May 2020]</p>
      <h3 id="a-novel-method-for-classification-of-tabular-data-using-convolutional-neural-networks">
        
        
          A Novel Method for Classification of Tabular Data Using Convolutional Neural Networks <a href="#a-novel-method-for-classification-of-tabular-data-using-convolutional-neural-networks">#</a>
        
        
      </h3>
    

<p>by Ljubomir Buturović, Dejan Miljković</p>

<p>📝 Paper: <a href="https://www.biorxiv.org/content/10.1101/2020.05.02.074203v1.abstract">https://www.biorxiv.org/content/10.1101/2020.05.02.074203v1.abstract</a></p>

<p>🖥 Code: N/A</p>

<ul>
  <li>
    <p>Similar to <a href="https://arxiv.org/abs/1903.06246">SuperTML</a> (listed below), this method converts tabular data into an image format as input to conventional convolutional neural networks for image data.</p>
  </li>
  <li>
    <p>The researchers applied this method to a gene classification dataset. They found that the TAC approach (91.1% accuracy) slightly outperforms other non-deep learning methods: Linear SVM (89.6% accuracy), XGBoost (87.6% accuracy), and others.</p>
  </li>
  <li>
    <p>TAC was pretrained on a combination of image datasets to achieve this performance – a TAC baseline without pre-training is not provided.</p>
  </li>
  <li>
    <p>The paper comes without code, so we have to accept the results with some reservations.</p>
  </li>
</ul>

<hr/>



<p>[29 Apr 2020]</p>
      <h3 id="neural-additive-models-interpretable-machine-learning-with-neural-nets">
        
        
          Neural Additive Models: Interpretable Machine Learning with Neural Nets <a href="#neural-additive-models-interpretable-machine-learning-with-neural-nets">#</a>
        
        
      </h3>
    

<p>📝 Paper: <a href="https://arxiv.org/abs/2004.13912">https://arxiv.org/abs/2004.13912</a></p>

<p>🖥 Code: <a href="https://github.com/google-research/google-research/tree/master/neural_additive_models">https://github.com/google-research/google-research/tree/master/neural_additive_models</a></p>

<ul>
  <li>
    <p>The proposed Neural Additive Models (NAMs) are essentially an ensemble of multilayer perceptrons (MLPs); here, one MLP is used per input feature.</p>
  </li>
  <li>
    <p>Each MLP has precisely one input node and one output node, but it can have an arbitrary number of hidden layers and nodes. The output values are then summed and passed to a logistic sigmoid function for binary classification. (For regression, the sigmoid activation can be omitted).</p>
  </li>
  <li>
    <p>The main advantage of this method is its ease of interpretability since each input feature is handled independently by a different neural network; i.e., the contribution of each individual feature towards the overall output (which is computed as a simple sum) can be easily assessed.</p>
  </li>
  <li>
    <p>The NAM was evaluated on 4 datasets (2 classification and 2 regression datasets). While it performed slightly worse than XGBoost across all four datasets, it performed better than <a href="https://dl.acm.org/doi/10.1145/2487575.2487579">Explainable Boosting Machines</a> on 2 out of the 4 datasets.</p>
  </li>
</ul>

<hr/>



<p>[13 Sep 2019]</p>
      <h3 id="neural-oblivious-decision-ensembles-for-deep-learning-on-tabular-data">
        
        
          Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data <a href="#neural-oblivious-decision-ensembles-for-deep-learning-on-tabular-data">#</a>
        
        
      </h3>
    

<p>by Sergei Popov, Stanislav Morozov, Artem Babenko</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/1909.06312">https://arxiv.org/abs/1909.06312</a></p>

<p>🖥 Code: <a href="https://github.com/Qwicen/node">https://github.com/Qwicen/node</a></p>

<ul>
  <li>
    <p>The proposed Neural Oblivious Decision Ensembles (NODE) method combines decision trees and deep neural networks such that they are trainable (via gradient-based optimization) in an end-to-end fashion.</p>
  </li>
  <li>
    <p>The method is based on so-called oblivious decision trees (ODTs), a particular type of decision tree that “use the same splitting feature and splitting threshold in all internal nodes of the same depth.”</p>
  </li>
  <li>
    <p>The experiments were conducted on six large datasets, ranging from 400K to 10.5M training examples.</p>
  </li>
  <li>
    <p>NODE slightly outperforms XGBoost on all six datasets when default hyperparameters are used; with tuned hyperparameters, NODE outperforms XGBoost in 4 out of 6 cases.</p>
  </li>
  <li>
    <p>The training speed of NODE (7min 42s) is about 7x slower than XGBoost (1min 13s) using a 1080Ti GPU; in inference, NODE (8.56s) is about 2x slower than XGBoost (4.45s).</p>
  </li>
</ul>

<hr/>



<p>[20 Aug 2019]</p>
      <h3 id="tabnet-attentive-interpretable-tabular-learning">
        
        
          TabNet: Attentive Interpretable Tabular Learning <a href="#tabnet-attentive-interpretable-tabular-learning">#</a>
        
        
      </h3>
    

<p>by Sercan O. Arik, Tomas Pfister</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/1908.07442">https://arxiv.org/abs/1908.07442</a></p>

<p>🖥 Code: <a href="https://github.com/google-research/google-research/tree/master/tabnet">https://github.com/google-research/google-research/tree/master/tabnet</a></p>

<ul>
  <li>
    <p>Based on my personal experience, TabNet is the first deep learning architecture for tabular data that gained widespread attention (no pun intended).</p>
  </li>
  <li>
    <p>TabNet is based on a sequential attention mechanism, showing that self-supervised learning with unlabeled data can improve the performance over purely supervised training regimes in tabular settings.</p>
  </li>
  <li>
    <p>Across six synthetic datasets, TabNet outperforms other methods on 3 out of 6 cases. However, XGBoost was omitted, and the tree-based reference method is <a href="https://link.springer.com/article/10.1007/s10994-006-6226-1">extremely randomized trees</a> rather than random forests.</p>
  </li>
  <li>
    <p>Across 4 KDD datasets, TabNet ties with CatBoost and XGboost on 1 dataset and performs almost as well as the gradient-boosted tree methods on the remaining three datasets.</p>
  </li>
</ul>

<hr/>



<p>[26 Feb 2019]</p>
      <h3 id="supertml-two-dimensional-word-embedding-for-the-precognition-on-structured-tabular-data">
        
        
          SuperTML: Two-Dimensional Word Embedding for the Precognition on Structured Tabular Data <a href="#supertml-two-dimensional-word-embedding-for-the-precognition-on-structured-tabular-data">#</a>
        
        
      </h3>
    

<p>by Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, Jason Dong</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/1903.06246">https://arxiv.org/abs/1903.06246</a></p>

<p>🖥 Code: No official implementation</p>

<ul>
  <li>
    <p>The proposed SuperTML method creates 2D (image-like) embeddings from tabular data</p>
  </li>
  <li>
    <p>The 2D embeddings are then used as input for conventional convolutional neural networks.</p>
  </li>
  <li>
    <p>The SuperTML method outperforms XGBoost on all three datasets Iris (150 examples), Wine (178 examples), and Adult (48,842 examples).</p>
  </li>
  <li>
    <p>Caveat: this is a very limited selection of datasets.</p>
  </li>
  <li>
    <p>While there are several implementations available on GitHub, there is no official implementation by the authors, so the results cannot be easily reproduced.</p>
  </li>
</ul>

<hr/>



<p>[29 Oct 2018]</p>
      <h3 id="autoint-automatic-feature-interaction-learning-via-self-attentive-neural-networks">
        
        
          AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks <a href="#autoint-automatic-feature-interaction-learning-via-self-attentive-neural-networks">#</a>
        
        
      </h3>
    

<p>by Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang</p>

<p>📝 Paper: <a href="https://arxiv.org/abs/1810.11921">https://arxiv.org/abs/1810.11921</a></p>

<p>🖥 Code: <a href="https://github.com/DeepGraphLearning/RecommenderSystems">https://github.com/DeepGraphLearning/RecommenderSystems</a></p>

<ul>
  <li>
    <p>The AutoInt model consists of an embedding layer that processes sparse input features and is followed by a multi-head self-attention module with skip connection. This main idea behind this architecture design is to learn “combinatorial features” (or in other words, the interaction between features.)</p>
  </li>
  <li>
    <p>The datasets the AutoInt method is aimed at are particularly large sparse datasets commonly encountered in recommender systems. In particular, the researchers investigate four datasets ranging from 740k to 150M examples. The number of sparse features ranges between 3.5k and 6M.</p>
  </li>
  <li>
    <p>The AutoInt method compares favorably to a large range of other methods for learning feature interactions. It is worth noting that the comparison does not include conventional and tree-based machine learning methods such as gradient-boosting classifiers that are commonly used for “classic” tabular datasets.</p>
  </li>
</ul>

<hr/>


      <h2 id="conclusion">
        
        
          Conclusion <a href="#conclusion">#</a>
        
        
      </h2>
    

<p>Personally, I find the idea of using deep learning algorithms on tabular datasets weird but interesting. That’s because I am perhaps bored with using the same methods all the time. Don’t get me wrong, these methods work. However, I am also a tinkerer and like to try new things.</p>

<p>Should you use deep learning for tabular datasets? Probably not. But it also probably depends on whether you are adventurous and have some time to waste.</p>

<p>My recommendation is always to start with a solid baseline. I would pick a random forest to begin with. Then, I would try HistGradientBoosting in scikit-learn (a powerful but easy-to-use gradient boosting implementation inspired by LightGBM). If that works well and you have the extra time, I highly recommend trying XGBoost. Only then, if you have even more extra time, feel experimental, and enjoy tinkering, I would try deep learning methods on the tabular dataset.</p>

<hr/>



<p>Thank you for reading. If you liked this article, you can also <a href="https://twitter.com/rasbt">find me on Twitter</a>, where I share more helpful content.</p>
  </article>


  <!--<strong>Have feedback on this post? I would love to hear it. Let me know and send me a <a href="https://twitter.com/intent/tweet?text=. @rasbt http://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html">tweet</a> or <a href="http://sebastianraschka.com/email.html">email</a>.</strong>-->

</div>
      </div>
    </div></div>
  </body>
</html>
