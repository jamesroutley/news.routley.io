<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles">Original</a>
    <h1>Rediscovering Transaction Processing from History and First Principles</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Post" name="Post"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>In July 2020, I stumbled into a fundamental limitation in the general-purpose database design for transaction processing.</p><p>This insight led to the creation of TigerBeetle, and attracted a special team who would engineer a financial transactions database from first principles and bring it to production.</p><p>Today, as we announce our Series A of $24 million, I want to tell you the story of how TigerBeetle came to solve this fundamental limitation, but first I want to take you back to the future of transaction processing.</p><h2 id="what-is-a-transaction"><a href="#what-is-a-transaction">What is a Transaction?</a></h2><p>40 years ago, Turing Award-winner Jim Gray, who is regrettably no longer with us, wrote a paper that would be published as <a href="https://jimgray.azurewebsites.net/papers/AMeasureOfTransactionProcessingPower.pdf" rel="noopener">A Measure of Transaction Processing Power</a>.</p><p>Jim Gray&#39;s paper was striking to me for three reasons.</p><p>First, he proposed a metric to evaluate the performance of Online Transaction Processing (OLTP) databases in terms of &#34;Transactions per Second&#34; (TPS).</p><p>We take the term &#34;TPS&#34; for granted today, but DeWitt (the researcher who <a href="https://www.eweek.com/development/db-test-pioneer-makes-history/" rel="noopener">benchmarked</a> a certain database company) and Levine (who worked with Gray at Tandem), <a href="https://sigmodrecord.org/publications/sigmodRecord/0806/p45.dewitt.pdf" rel="noopener">would recognize that</a>:</p><blockquote><p>“By specifying the key metric for evaluating the performance of database systems [..], Jim launched a benchmark war that drove the industry forward at a frantic pace for more than 15 years. At the time of the publication of this seminal paper, database systems that could deliver 100 transactions/second were considered state of the art. Obtaining 1000 transactions/second was viewed as unreachable.”</p></blockquote><p>In fact, the benchmark war was so intense that it <a href="https://www.tpc.org/information/about/history5.asp" rel="noopener">led to the formation</a> of the Transaction Processing Performance Council (TPC).</p><p>But the second thing I appreciated was how Gray (like McSherry et al. with <a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf" rel="noopener">Scalability! But at what COST?</a>) brought it all back to cost-efficiency, to tie TPS to cost, and benchmark the cost per transaction.</p><p>Before Gray, not only was there no &#34;standard transaction&#34; so that it was &#34;difficult to verify or compare TPS claims&#34;, there was also &#34;no accepted way to price a system supporting a desired TPS rate&#34;. And Gray emphasized that this cost has as much to do with hardware, as with developer productivity:</p><blockquote><p>“One can implement [our benchmark] transaction in a day or two on some systems. On others it takes months to get started. There are huge differences in productivity between different systems. [..] We estimate the leanest-meanest system would require six months of expert time to get [our benchmark] operational.”</p></blockquote><p>So Jim taught the industry to think not only about &#34;performance&#34;, but crucially also &#34;price/performance&#34;.</p><p>Beautifully simple in hindsight.</p><blockquote><p>Jim Gray&#39;s contributions never cease to amaze me. Did you know that he helped coin the term <a href="https://www.youtube.com/watch?v=lGyMiW6PnKI" rel="noopener">ACID</a>, gave us <a href="https://en.wikipedia.org/wiki/Five-minute_rule" rel="noopener">the 5-minute rule</a>, and <a href="https://www.amazon.com/Transaction-Processing-Concepts-Techniques-Management/dp/1558601902" rel="noopener">wrote the book</a> on transaction processing?</p></blockquote><p>And yet, what surprised me most, was how Gray defined &#34;transaction&#34;—the T in TPS—not as a SQL transaction (as we might assume), but rather as a business transaction derived from the real world.</p><p>Indeed, the OLTP benchmark that propelled the industry forward by an order of magnitude, and that gave rise to the TPC, would define &#34;the standard measure of transaction processing&#34; (<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2005/04/tr-2005-39.doc" rel="noopener">Jim’s words 20 years later</a>) as a &#34;DebitCredit&#34;.</p><p>In other words, to use &#34;a database system to debit a bank account, do the standard double-entry bookkeeping and then reply to the terminal&#34;:</p><p><em>The transaction as defined by Jim Gray in the &#34;DebitCredit&#34; benchmark, where TPS is &#34;Peak DebitCredit transactions per second with 95% of the transactions having less than one second response time&#34;.</em></p><h2 id="the-power-of-simplicity"><a href="#the-power-of-simplicity">The Power of Simplicity</a></h2><p>It’s worth noting that SQL had been around for 10 years by this point, and that Jim Gray had worked on System R, for which SQL was <a href="https://en.wikipedia.org/wiki/SQL#History" rel="noopener">originally designed</a>. And yet he chose debit/credit as the measure of transaction processing power:</p><blockquote><p>“Most of us have spent our careers making high-function systems. It is painful to see a metric which rewards simplicity; simple systems are faster than fancy ones. [...] Surprisingly, these minimal requirements disqualify many purported transaction processing systems, but there is a very wide range of function and usability among the systems that have these minimal functions.”</p></blockquote><p>So it wasn&#39;t that the SQL transaction didn&#39;t exist, but rather, that the debit/credit transaction was <a href="https://www.postgresql.org/docs/current/tutorial-transactions.html" rel="noopener">the textbook example</a> of an everyday transaction; the reason for a database to provide guarantees such as atomicity, consistency, isolation and durability in the first place.</p><p>While SQL turns 50 this year, debit/credit has for centuries been the simple schema selected to represent the myriad of financial state transitions for any business, in any sector. If SQL is the language of databases, then to quote Warren Buffett, debit/credit is &#34;the language of business&#34;. The essential &#34;who&#34;, &#34;what&#34;, &#34;when&#34;, &#34;where&#34;, &#34;why&#34; and &#34;how much&#34; to record the movement of anything of value, from one person or place to another.</p><p>The rigor of debit/credit is flexible, since the simplicity of debit/credit is composable. You can represent a universe of business. And indeed, you can think of debit/credit like the first law of thermodynamics:</p><blockquote><p>Money cannot be created or destroyed, but is transferred from one account to another, so that the sum of all debits and the sum of all credits remains equal.</p></blockquote><p>Nevertheless, you can imagine my surprise when I discovered the history of Jim Gray&#39;s DebitCredit benchmark for the first time last year.</p><p>I had already created <a href="https://github.com/tigerbeetle/tigerbeetle" rel="noopener">TigerBeetle</a>, and the set of developer primitives we chose to process transactions was designed around (you guessed it) debit/credit. Not because the pioneer of OLTP had picked debit/credit as the canonical transaction processing workload. But because we had stumbled into a fundamental limitation in the general-purpose database design.</p><h2 id="a-fundamental-limitation"><a href="#a-fundamental-limitation">A Fundamental Limitation</a></h2><p>In 2020, I had been consulting on a <a href="https://mojaloop.io/" rel="noopener">central bank payments switch</a> by the Bill and Melinda Gates Foundation. The switch was at heart a transaction system, but consisting of thousands of lines of complex code around a general-purpose database, to process debits and credits as money moved.</p><p>My job was to find the bottleneck. So we traced all the SQL queries processed by the general-purpose database as financial transactions were processed by the switch.</p><p>Immediately, we saw an impedance mismatch:</p><blockquote><p>For each financial transaction, there would be 10-20 SQL queries back and forth across the network, while holding row locks.</p></blockquote><p>For example, for one debit/credit, there might be:</p><ol><li data-preset-tag="p"><p>a query to select the account balances from two rows,</p></li><li data-preset-tag="p"><p>locks held on these rows for the duration of the SQL transaction,</p></li><li data-preset-tag="p"><p>network traffic moving the data to the code in the application layer,</p></li><li data-preset-tag="p"><p>a decision outside the database on whether there was enough money,</p></li><li data-preset-tag="p"><p>more network traffic to write the new balances back to the database,</p></li><li data-preset-tag="p"><p>and finally, a write to record the debit/credit.</p></li></ol><p>No matter how much hardware we gave the database, no matter how much money we spent, we couldn&#39;t improve throughput.</p><p>The limitation was fundamental:</p><blockquote><p>A function of the concurrency control of the general-purpose database design: row locks interacting with the network round-trip time (RTT).</p></blockquote><p>For example, if a debit/credit required a minimum of 2 network RTTs while holding locks, and with an RTT of 0.5ms, then other transactions would not be able to obtain the held locks for at least 1ms.</p><p>Worse, there are only 1000 milliseconds in a second, and if you look again at Gray’s benchmark, you will see that there is the concept of a &#34;branch&#34; account—a row touched by most transactions, since a bank (or business) may have ten million customers to debit, but only a few internal accounts to credit.</p><p>In other words, a large set of rows interacts with a small set of rows, with these hot rows locked in most transactions, serializing system performance, and reversing gains from horizontally scaled systems (in fact, rendering them cost-prohibitive!).</p><p>As Jim Gray understood 40 years ago, and as we learned, the OLTP workload is characterized by intrinsic contention.</p><p>For years, this fundamental limitation of the general-purpose database design to scale transaction processing had been latent. But as the world has become more transactional, the problem has become existential.</p><h2 id="the-world-is-becoming-more-transactional"><a href="#the-world-is-becoming-more-transactional">The World is Becoming More Transactional</a></h2><p>In the last 7 years, instant payments in India increased by <a href="https://www.npci.org.in/what-we-do/upi/product-statistics" rel="noopener">7000x</a> to 14 billion transactions per month, with these expected to triple in the next 3 years.</p><p>The trend is global, with Brazil&#39;s Pix <a href="https://www.reuters.com/business/finance/brazils-pix-payments-are-killing-cash-are-credit-cards-next-2024-04-02/" rel="noopener">replacing the country’s cash payments</a>, with the introduction of FedNow in the US, and with transactions increasing also in other sectors:</p><ul><li data-preset-tag="p"><p>energy, as the world moves to clean energy and the supply of energy begins to follow the sun, energy providers want to arbitrage energy prices more frequently,</p></li><li data-preset-tag="p"><p>gaming, as games become more connected with massively multiplayer in game economies, and</p></li><li data-preset-tag="p"><p>cloud, as server prices move from monthly to per-second billing, and real time spend caps become a challenge.</p></li></ul><p>In less than a decade, the world has become at least three orders of magnitude more transactional. And yet the three most popular general-purpose database designs, Postgres, MySQL and SQLite, while great for building apps, are 20-30 years old, designed for a different era of transaction processing.</p><h2 id="transaction-processing-from-first-principles"><a href="#transaction-processing-from-first-principles">Transaction Processing From First Principles</a></h2><p>Therefore, we asked the question:</p><blockquote><p>What if we could take the four primary colors of computer science (network, storage, memory and compute) and <a href="https://www.youtube.com/watch?v=sC1B3d9C_sI" rel="noopener">blend them from first principles</a> into a transaction processing database for the future?</p></blockquote><p>We saw that there were greater gains to be had than settling for a Postgres extension or stored procedures.</p><p>While we couldn’t completely eliminate network round-trips (i.e. embedding the database in a single application process would still have required replication and consensus for durability and availability)—we could amortize them and completely eliminate row locks.</p><p>With the insight that 1 debit/credit is 128 bytes, or 2 CPU cache lines of information:</p><blockquote><p>We designed TigerBeetle to provide debit/credit as a first-class primitive, to pack up to 8000 debit/credits in a 1 MiB query, and so process up to 8000 transactions in a single round-trip to the database, without row locks.</p><p>By fixing the impedance mismatch, what before took at least 8000 SQL queries with a general-purpose database could now take 1 query with TigerBeetle.</p></blockquote><p>This is the big 1000x performance idea in TigerBeetle. We did nothing special.</p><p>But this also solves cost-efficiency and developer productivity. TigerBeetle can power orders of magnitude more TPS with a fraction of the hardware, and with a richer set of primitives.</p><p>For example, to send a pending debit/credit to reserve funds, and then a debit/credit to post these funds, or else have TigerBeetle roll back the funds after a timeout. (TigerBeetle makes a perfect two-phase commit coordinator!).</p><h2 id="advancing-safety"><a href="#advancing-safety">Advancing Safety</a></h2><p>Finally, much has changed in technology in the last thirty years. While Postgres, MySQL and SQLite are &#34;tried and tested&#34;, this also means that some of the ways they can lose data are now <a href="https://www.usenix.org/conference/atc20/presentation/rebello" rel="noopener">known in the literature</a>.</p><p>Therefore, beyond performance, there was the opportunity also to advance <a href="https://docs.tigerbeetle.com/about/safety" rel="noopener">safety</a>, which becomes crucial especially as scale increases.</p><p>We could tap into a <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/about/README.md#references" rel="noopener">rich reservoir</a> of research to make TigerBeetle:</p><ul><li data-preset-tag="p"><p>one of the first databases to apply 2018 <a href="https://www.usenix.org/conference/fast18/presentation/alagappan" rel="noopener">Protocol-Aware Recovery</a> for storage fault-tolerance,</p></li><li data-preset-tag="p"><p>one of the first to adapt <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md#safety" rel="noopener">NASA’s Power of Ten Rules for Safety-Critical Code</a>, with static memory allocation and 6000+ assertions to verify correctness at runtime, and</p></li><li data-preset-tag="p"><p>one of three (along with FoundationDB) to run in a <a href="https://tigerbeetle.com/blog/2023-07-11-we-put-a-distributed-database-in-the-browser" rel="noopener">deterministic simulator</a> and apply model checking techniques—but on the actual code.</p></li></ul><h2 id="back-to-the-future-of-transaction-processing"><a href="#back-to-the-future-of-transaction-processing">Back to the Future of Transaction Processing</a></h2><p>The past four years of TigerBeetle have seen many highlights:</p><ul><li data-preset-tag="p"><p>The thrill of realizing that debit/credit could unlock orders of magnitude more performance.</p></li><li data-preset-tag="p"><p>The magic of running TigerBeetle in a deterministic simulator—the <a href="https://www.youtube.com/watch?v=w3WYdYyjek4" rel="noopener">velocity and quality</a> we were able to achieve, to ship production in four years.</p></li><li data-preset-tag="p"><p>The validation of our first customer, <a href="https://www.senapt.co.uk/" rel="noopener">an energy provider</a> transacting clean energy more efficiently.</p></li><li data-preset-tag="p"><p>And of course, the joy of discovering that Jim Gray saw the future of transaction processing, long before we did.</p></li></ul><p>Today, it is my great pleasure, as we look to serve the next thirty years of transaction processing, to welcome our long term partner <a href="https://www.sparkcapital.com/team-members/natalie-vais" rel="noopener">Natalie Vais</a> to the board, and to announce our $24 million Series A led by <a href="https://www.sparkcapital.com/" rel="noopener">Spark Capital</a>, with participation from <a href="https://www.amplifypartners.com/" rel="noopener">Amplify Partners</a> and <a href="https://coil.com/content/coil-x-tigerbeetle-databases-of-the-future" rel="noopener">Coil</a>, as well as angels, including: Alex Gallego, Founder and CEO of Redpanda; Uriel Cohen, Co-Founder and Executive Chairman of Clear Street; Sachin Kumar, Co-Founder and CTO of Clear Street; and Alex Rattray, former Stripe, to whom we owe much gratitude.</p><p>Thanks to <a href="https://hannes.muehleisen.org/" rel="noopener">Hannes Mühleisen</a>, <a href="https://www.databass.dev/" rel="noopener">Alex Petrov</a> and <a href="https://sirupsen.com/" rel="noopener">Simon Eskildsen</a> for reading drafts of this post.</p></div></div></div>
  </body>
</html>
