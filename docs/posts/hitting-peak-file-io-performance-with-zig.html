<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://steelcake.com/blog/nvme-zig/">Original</a>
    <h1>Hitting Peak File IO Performance with Zig</h1>
    
    <div id="readability-page-1" class="page"><div>
    <h2 id="intro">Intro</h2>
<p>This post goes through how to maximize file IO performance on linux using zig with io_uring.</p>
<p>All code related to this post can be found in this <a href="https://github.com/steelcake/csio">repo</a>.</p>
<h2 id="a-benchmark">a) Benchmark</h2>
<p>We are comparing fio and the zig code which can be found <a href="https://github.com/steelcake/csio/blob/main/examples/diorw.zig">here</a>.</p>
<h4 id="test-system">test system</h4>
<p>We are using a machine with:</p>
<ul>
<li>ubuntu 24.04 (6.14 kernel, HWE).</li>
<li>kernel parameter <code>nvme.poll_queues=16</code>.</li>
<li>&#34;datacenter&#34; NVMe SSD without any RAID. </li>
<li>756 GB of RAM. This amount of RAM should be irrelevant for this test. Since we are using direct_io hence there is no caching for the file.</li>
<li>Ryzen EPYC CPU with 32 cores (64 threads). CPU should be mostly irrelevant for this test since we are using only one thread. </li>
</ul>
<h4 id="fio-config">fio config</h4>
<pre><code><span>[global]
</span><span>direct=1
</span><span>bs=512K
</span><span>rw=write
</span><span>ioengine=io_uring
</span><span>iodepth=64
</span><span>size=16g
</span><span>hipri=1
</span><span>fixedbufs=1
</span><span>registerfiles=0
</span><span>sqthread_poll=1
</span><span>numjobs=1
</span><span>
</span><span>[job0]
</span><span>filename=testfile
</span></code></pre>
<p>Just changing <code>rw=write</code> to <code>rw=read</code> to do the read benchmark.</p>
<h4 id="zig-code-config">zig code config</h4>
<p>The example does 64 512KB sequential reads at a time, same as the fio configuration. It also writes/reads the entire 16GB file same as fio.</p>
<p>The underlying library uses the exact same features as configured in the fio config as well.</p>
<p>Zig code also uses a single thread same as fio.</p>
<h4 id="benchmark-numbers">benchmark numbers</h4>
<p>The result I get from fio is <code>4.083 GB/s</code> write, and <code>7.33 GB/s</code> read. Not including more detailed statistics that fio gives since the benchmark script doesn&#39;t have those.</p>
<p>The result for the <code>diorw</code> example in zig is <code>3.802 GB/s</code> write, and <code>6.996 GB/s</code> read.</p>
<p>The zig code is a bit slower than fio but it manages to hit expected numbers for the SSD. Also the fio <code>run=</code> timing exactly (to the millisecond) matches the zig code&#39;s timings so there
might be some difference in bandwidth measurement.</p>
<h2 id="b-implementation">b) Implementation</h2>
<p>Most of the implementation follows concepts found in <a href="https://steelcake.com/blog/nvme-zig/github.com/DataDog/glommio">glommio</a> which is a similar library written in Rust.</p>
<p>io_uring usage follows the <a href="https://kernel.dk/io_uring.pdf">io_uring document</a>.</p>
<h4 id="1-how-to-use-io-uring-for-file-io">1) How to use io_uring for file IO</h4>
<p>In my last <a href="https://steelcake.com/blog/steelcake.com/blog/comparing-io-uring/">blog post</a> I benchmarked different io_uring parameters using fio to find which parameters gave a significant performance benefit.</p>
<p>This was crucial to designing the IO library so it is using the most significant features that make file IO performant.</p>
<p>What I found in that article shows that using polled io instead of interrupt driven. And using the kernel side busy polling (SQ_THREAD_POLL) feature of io_uring is crucial for performance.</p>
<p>Also using registered buffers intead of regular buffers gives a significant performance boost.</p>
<h4 id="2-using-polled-io">2) Using polled IO</h4>
<p>We need an io_uring instance with the <code>IOPOLL</code> feature enabled in order to make use of this feature. But if we enable <code>IOPOLL</code> then we can&#39;t do any operation except direct_io reads and writes.</p>
<p>So we need two io_uring instances, one with <code>IOPOLL</code> enabled and one without it.</p>
<p>I don&#39;t think <a href="https://github.com/DataDog/glommio">glommio</a> or other similar mainstream libraries use this feature since using this feature requires that <code>nvme.poll_queues</code> are enabled in linux kernel parameters and
I didn&#39;t see any errors when I was using glommio even though I didn&#39;t have this feature enabled in my setups before.</p>
<p>I opted to enable this feature because it is important for performance.</p>
<h4 id="3-using-registered-buffers-for-file-io">3) Using registered buffers for file IO</h4>
<p>Since we need to pre-register the buffer to make this effective. Most sensible approach seems to be to allocate the amount of memory we need for this beforehand and use that memory for all operations.
This is the same approach that <a href="https://github.com/DataDog/glommio">glommio</a> uses.</p>
<p>This means user of the library can&#39;t pass in memory that they allocated for doing read/write. And we will have a relatively low limit for the amount of io memory we can use at any given time.</p>
<p>So we implement a buffer interface, this buffer is returned by the read call and the user has to release it back so the library can reuse the memory. And they have to allocate and fill one of these
buffers before doing a write and pass it in.</p>
<p>On the read path we can make it easy for user and handle alignment inside the library by over allocating to meet the alignment requirements of direct_io. Only caveat the user needs to keep in mind is the fact 
that there can be read amplification if they issue unaligned reads and this approach works with perfect efficiency as long as the user is issuing aligned reads.</p>
<p>On the write path, we want to make alignment explicit and require the user passes a perfectly aligned and sized buffer.
It doesn&#39;t seem possible to make alignement internal to the library without incurring overhead transparent to the user when doing writes. This is because we have to write a multiple of
the alignment but user might request to write less than the alignment, then we have to read the missing part of the alignment, combine it with user data and then write it.
This can be much worse then just reading some extra data like in the read path case.</p>
<p>I also left out doing automatic merging of io inside the library like glommio does in order to keep it simple and because I&#39;m concerned the application will have to implement something complicated on top anyway and
this can be implement there in a better way. This perspective comes from my previous use of glommio.</p>
<ul>
<li><a href="https://github.com/steelcake/csio/blob/b46041ec8c94c4317b1763ea31c0a4177245ee77/src/fs.zig#L36">write implementation</a>.</li>
<li><a href="https://github.com/steelcake/csio/blob/b46041ec8c94c4317b1763ea31c0a4177245ee77/src/fs.zig#L197">read implementation</a>.</li>
</ul>
<h4 id="4-using-the-sqthread-poll-feature">4) Using the <code>SQTHREAD_POLL</code> feature</h4>
<p>This feature creates a kernel side busy thread that is constantly checking if there are any requests submitted by the user space into the submission queue and also in our case it makes this thread poll the SSD for io completions
so it ends up making the library code a bit simpler.</p>
<p>This feature doesn&#39;t seem to be enabled in other similar libraries probably because it used to require user privilages in OS level on older versions of the kernel, and also it runs a busy thread and maxes out a CPU core completely.</p>
<p>The maxing of the CPU core can sound bad but you can actually use a single busy kernel thread for multiple of your user space io_uring instances using the <code>WQ_ATTACH</code> flag. It doesn&#39;t sound so bad
if you are running 32 threads for the application and if you consider the performance benefit this feature gives.</p>

  </div></div>
  </body>
</html>
