<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2306.03423">Original</a>
    <h1>I&#39;m Afraid I Can&#39;t Do That: Prompt Refusal in Generative Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2306.03423">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Since the release of OpenAI&#39;s ChatGPT, generative language models have
attracted extensive public attention. The increased usage has highlighted
generative models&#39; broad utility, but also revealed several forms of embedded
bias. Some is induced by the pre-training corpus; but additional bias specific
to generative models arises from the use of subjective fine-tuning to avoid
generating harmful content. Fine-tuning bias may come from individual engineers
and company policies, and affects which prompts the model chooses to refuse. In
this experiment, we characterize ChatGPT&#39;s refusal behavior using a black-box
attack. We first query ChatGPT with a variety of offensive and benign prompts
(n=1,730), then manually label each response as compliance or refusal. Manual
examination of responses reveals that refusal is not cleanly binary, and lies
on a continuum; as such, we map several different kinds of responses to a
binary of compliance or refusal. The small manually-labeled dataset is used to
train a refusal classifier, which achieves an accuracy of 92%. Second, we use
this refusal classifier to bootstrap a larger (n=10,000) dataset adapted from
the Quora Insincere Questions dataset. With this machine-labeled data, we train
a prompt classifier to predict whether ChatGPT will refuse a given question,
without seeing ChatGPT&#39;s response. This prompt classifier achieves 76% accuracy
on a test set of manually labeled questions (n=1,009). We examine our
classifiers and the prompt n-grams that are most predictive of either
compliance or refusal. Datasets and code are available at
<a href="https://github.com/maxwellreuter/chatgpt-refusals" rel="external noopener nofollow">this https URL</a>.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Max Reuter [<a href="https://arxiv.org/show-email/47d80fe0/2306.03423">view email</a>]
      </p></div></div>
  </body>
</html>
