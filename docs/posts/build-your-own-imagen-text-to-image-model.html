<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.assemblyai.com/blog/build-your-own-imagen-text-to-image-model/">Original</a>
    <h1>Build Your Own Imagen Text-to-Image Model</h1>
    
    <div id="readability-page-1" class="page"><article>
    <section>
      <figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-36.png" alt="" loading="lazy" width="1000" height="500" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-36.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-36.png 1000w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://openai.com/dall-e-2/">source</a>)</figcaption></figure><p>Just a month after DALL-E 2&#39;s release, Google announced a competing model <strong><a href="https://www.assemblyai.com/blog/how-imagen-actually-works/">Imagen</a></strong> that was found to be <em>even better than DALL-E 2</em>. Here are some example images:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/iamges.png" alt="" loading="lazy" width="1000" height="500" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/iamges.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/iamges.png 1000w" sizes="(min-width: 720px) 720px"/><figcaption><em>&#34;A dragon fruit wearing karate belt in the snow</em>&#34; and &#34;<em>a photo of a Corgi dog riding a bike in Times Square. It is wearing sunglasses and a beach hat</em>&#34; (<a href="https://imagen.research.google/">source</a>).</figcaption></figure><p>The impressive results of both DALL-E 2 and Imagen rely on cutting-edge Deep Learning research. While necessary for attaining State-of-the-Art results, the usage of such cutting-edge research in models like Imagen renders them harder to understand for non-specialist researchers, in turn hindering the widespread adoption of these models and techniques. </p><p>Therefore, in the spirit of democratization, we will learn in this article <strong>how to build Imagen with PyTorch</strong>. In particular, we will construct a minimal implementation of Imagen - called <strong>MinImagen</strong> - that isolates the salient features of Imagen so that we can focus on understanding Imagen&#39;s integral operating principles, disentangling implementation aspects that are essential from those which are incidental.</p><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Package Note</p>
  </header>
	<div>
    <p>N.B. if you are not interested in implementation details and want only to use MinImagen, it has been packaged up and can be installed with</p>
    <center><code>pip install minimagen</code></center>
        <p>Check out the <a href="#training-and-sampling-from-minimagen">section below</a> or the corresponding <a href="https://github.com/AssemblyAI-Examples/MinImagen">GitHub repository</a> for usage tips. The <a href="https://assemblyai-examples.github.io/MinImagen/minimagen.html">documentation</a> contains additional details and information about using the package.</p>
  </div>
</div>
   <!--kg-card-end: html--><h2 id="introduction">Introduction</h2><p>Text-to-image models have made great strides in the past few years, as evidenced by models like GLIDE, DALL-E 2, Imagen, and more. These strides are in large part due to the recent flourishing wave of research into <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Diffusion Models</a>, a new paradigm/framework for generative models.</p><p>While there are some good resources on the <em>theoretical</em> aspects of Diffusion Models and text-to-image models, <em>practical </em>information on how to actually build these models is not as abundant. This is especially true for models that incorporate Diffusion Models as just <em>one</em> component of a larger system, common in text-to-image models, like the <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/#how-dall-e-2-works-a-birds-eye-view">encoding-prior-generator</a> chain in <strong>DALL-E 2</strong>, or the <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#how-imagen-works-a-birds-eye-view">super-resolution chain</a> in <strong>Imagen</strong>.</p><p><strong>MinImagen</strong> strips off the bells and whistles of current best practices in order to isolate Imagen&#39;s salient features for educational purposes. The remainder of this article is structured as follows:</p><ol><li><strong>Review of Imagen / Diffusion Models:</strong> In order to orient ourselves before we begin to code, we will briefly review both Imagen itself and Diffusion Models more generally. These reviews are intended to serve only as a refresher, so you should already have a working understanding of both of these topics when reading the refresher. You can check out our <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Introduction to Diffusion Models for Machine Learning</a> and our dedicated guide to <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/">How Imagen Actually Works</a> to learn more.</li><li><strong>Building the Diffusion Model: </strong>After our recap, we&#39;ll first build the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/ad5fab36e810ed0c4c3cba7ba9df2c892d7ef2c4/minimagen/diffusion_model.py#L8">GaussianDiffusion</a></code> class in PyTorch, which defines the Diffusion Models used in Imagen.</li><li><strong>Building the Denoising U-Net:</strong> We&#39;ll then build the denoising U-Net on which the Diffusion Models rely, manifested in the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/ad5fab36e810ed0c4c3cba7ba9df2c892d7ef2c4/minimagen/Unet.py#L25">Unet</a></code> class. </li><li><strong>Building MinImagen:</strong> Next, we will put all of these pieces together using a T5 text encoder and a Diffusion Model chain in order to build our (Min)Imagen class, <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/ad5fab36e810ed0c4c3cba7ba9df2c892d7ef2c4/minimagen/Imagen.py#L22">Imagen</a></code>.</li><li><strong>Using MinImagen: </strong>Finally, we will learn how to train and sample from Imagen once it is fully defined.</li></ol><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Model Weights</p>
  </header>
	<p>Stay tuned! We&#39;ll be training MinImagen over the coming weeks and releasing a checkpoint so you can generate your own images. Make sure to <a href="https://assemblyai.us17.list-manage.com/subscribe?u=cb9db7b18b274c2d402a56c5f&amp;id=2116bf7c68">follow our newsletter</a> to stay up to date on our content releases.</p>
</div><!--kg-card-end: html--><p>Without further ado, it&#39;s time to jump into the recaps of both Imagen and Diffusion Models. If you are already familiar with Imagen and Diffusion Models from a theoretical perspective and want to jump to the PyTorch implementation details, click <a href="#build-your-own-imagen-in-pytorch">here</a>.</p><h2 id="what-is-imagen">What is Imagen?</h2><p>Imagen is a text-to-image model that was released by Google just a couple of months ago. It takes in a <strong>textual prompt</strong> and outputs an <strong>image </strong>which reflects the semantic information contained within the prompt.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-25.png" alt="" loading="lazy" width="960" height="324" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-25.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-25.png 960w" sizes="(min-width: 720px) 720px"/></figure><p>To generate an image, Imagen first uses a <strong>text encoder </strong>to generate a representative encoding of the prompt. Next, an <strong>image generator</strong>, conditioned on the encoding, starts with Gaussian noise (&#34;TV static&#34;) and progressively denoises it to generate a small image that reflects the scene described by the caption. Finally, two <strong>super-resolution</strong> models sequentially upscale the image to higher resolutions, again conditioning on the encoding information.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/4.png" alt="" loading="lazy" width="960" height="540" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/4.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/4.png 960w" sizes="(min-width: 720px) 720px"/></figure><p>The text encoder is a <strong>pre-trained T5 text encoder </strong>that is frozen during training. Both the base image generation model and the super-resolution models are <strong>Diffusion Models</strong>.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h2 id="what-is-a-diffusion-model">What is a Diffusion Model?</h2><p>Diffusion Models are a class of generative models, meaning that they are used to <em>generate </em>novel data, often images. Diffusion Models train by corrupting training images with Gaussian Noise in a series of timesteps, and then learning to undo this noising process.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-28.png" alt="" loading="lazy" width="1000" height="162" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-28.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-28.png 1000w" sizes="(min-width: 720px) 720px"/></figure><p>In particular, a model is trained to predict the noise component of an image at a given timestep.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-29.png" alt="" loading="lazy" width="737" height="263" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-29.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-29.png 737w" sizes="(min-width: 720px) 720px"/></figure><p>Once trained, this denoising model can then be iteratively applied to randomly sampled Gaussian noise, &#34;denoising&#34; it in order to generate a novel image.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-37.png" alt="" loading="lazy" width="1025" height="173" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-37.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/image-37.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/image-37.png 1025w" sizes="(min-width: 720px) 720px"/></figure><p>Diffusion Models constitute a sort of <em>metamodel</em> that orchestrates the training of <em>another </em>model - <strong>the noise prediction model</strong>. We therefore still have the task of deciding what type of model to actually use for the noise prediction itself. In general, U-Nets are chosen for this role. The U-Net in Imagen has a structure like this:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/arch-4.png" alt="" loading="lazy" width="1536" height="1152" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/arch-4.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/arch-4.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/arch-4.png 1536w" sizes="(min-width: 720px) 720px"/></figure><p>The architecture is based off of the model in the <a href="https://openreview.net/pdf?id=AAWuCvzaVt">Diffusion Models Beat GANs on Image Synthesis</a> paper. For <strong>MinImagen</strong>, we make some small changes to this architecture, including </p><ol><li>Removing the global attention layer (not pictured),</li><li>Replacing the attention layers with transformer encoders, and </li><li>Placing the transformer encoders at the end of the sequence at each layer rather than in between the residual blocks in order to allow for a variable number of residual blocks.</li></ol><!--kg-card-begin: html--><!--kg-card-end: html--><h2 id="build-your-own-imagen-in-pytorch">Build Your Own Imagen in PyTorch</h2><p>With our Imagen/Diffusion Model recap complete, we are finally ready to start building out our Imagen implementation. To get started, first open up a terminal and clone the <a href="https://github.com/AssemblyAI-Examples/MinImagen">project repository</a>:</p><pre><code>git clone https://github.com/AssemblyAI-Examples/MinImagen.git</code></pre><p>In this tutorial, we will <strong>isolate the important parts of the source code</strong> that are relevant to the Imagen implementation itself, omitting details like argument validation, device handling, etc. Even a minimal implementation of Imagen is relatively large, so this approach is necessary in order to isolate instructive information. <strong>MinImagen&#39;s source code is thoroughly commented </strong>(with associated documentation <a href="https://assemblyai-examples.github.io/MinImagen/minimagen.html">here</a>), so information regarding any omitted details should be easy to find.</p><p>Each big component of the project - the Diffusion Model, the Denoising U-Net, and Imagen - has been placed into its own section below. We&#39;ll start by building the <code><a href="https://assemblyai-examples.github.io/MinImagen/minimagen.html#minimagen.diffusion_model.GaussianDiffusion">GaussianDiffusion</a></code> class.</p><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Attribution Note</p>
  </header>
	<p>This implementation is in large part a simplified version of Phil Wang&#39;s Imagen implementation, which you can find on GitHub <a target="_blank" rel="noopener noreferrer" href="https://github.com/lucidrains/imagen-pytorch">here</a>.</p>
</div>
   <!--kg-card-end: html--><h2 id="building-the-diffusion-model">Building the Diffusion Model</h2><p>The Diffusion Model <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L8">GaussianDiffusion</a></code> class can be found in <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/minimagen/diffusion_model.py">minimagen.diffusion_model</a></code>. To jump to a summary of this section, click <a href="#summary">here</a>.</p><h3 id="initialization">Initialization</h3><p>The <code>GaussianDiffusion</code> initialization function takes only one argument - the number of timesteps in the diffusion process.</p><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	super().__init__()</code></pre><p>First, Diffusion Models require a <strong>variance schedule</strong>, which specifies the variance of the Gaussian noise that is added to image at a given timestep in the diffusion process. The variance schedule should be increasing, but there is <a href="https://arxiv.org/pdf/2205.11487.pdf#page=20">some flexibility</a> in how this schedule is defined. For our purposes we implement the variance schedule from the original <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a> (DDPM) paper, which is a linearly spaced schedule from <strong>0.0001 at t=0</strong> to <strong>0.02 at t=T</strong>.</p><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	super().__init__()
        
        self.num_timesteps = timesteps

        scale = 1000 / timesteps
        beta_start = scale * 0.0001
        beta_end = scale * 0.02
        betas = torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)</code></pre><p>From this schedule, we calculate a few values (again specified in the DDPM paper) that will be used in calculations later:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-4.png" alt="" loading="lazy" width="1980" height="161" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-4.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/image-4.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/image-4.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/image-4.png 1980w" sizes="(min-width: 720px) 720px"/></figure><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	# ...
        
        alphas = 1. - betas
        alphas_cumprod = torch.cumprod(alphas, axis=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)</code></pre><p>The <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L36">rest of the initialization function</a> registers the above values and some derived values as buffers, which are like parameters except that they <em>don&#39;t require gradients</em>. <strong>All of the values are ultimately derived from the variance schedule</strong> and exist to make some calculations easier and cleaner down the line. The specifics calculating the derived values are not important, but we will point out below any time one of these derived values is utilized.</p><h3 id="forward-diffusion-process">Forward Diffusion Process</h3><p>Now we can move on to define <code>GaussianDiffusion</code>&#39;s <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L125">q_sample</a></code> method, which is responsible for the <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#diffusion-modelsa-deep-dive">forward diffusion</a> process. Given an input image <em>x_0</em>, we noise it to a given timestep <em>t</em> in the diffusion process by sampling from the below distribution:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/q_sample.png" alt="" loading="lazy" width="1447" height="121" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/q_sample.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/q_sample.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/q_sample.png 1447w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>Sampling from the above distribution is equivalent to the below computation, where we have highlighted <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L45">two of</a> the buffers defined in <code>__init__</code>.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/x_t_boxed.png" alt="" loading="lazy" width="2000" height="233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/x_t_boxed.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/x_t_boxed.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/x_t_boxed.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/x_t_boxed.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption>See the &#34;mathematical note&#34; dropdown <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#diffusion-modelsa-deep-dive">here</a> for details on this equivalence</figcaption></figure><p>That is, <strong>the noisy version of the image at time <em>t</em> can be sampled by simply adding noise to the image</strong>, where both the original image and the noise are scaled by their respective coefficients as dictated by the timestep. Let&#39;s implement this calculation in PyTorch now by adding the method <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L125">q_sample</a></code> to the <code>GaussianDiffusion</code> class:</p><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	# ...

    def q_sample(self, x_start, t, noise=None):
        noise = default(noise, lambda: torch.randn_like(x_start))

        noised = (
                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +
                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise
        )

        return noised</code></pre><p><code>x_start</code> is a PyTorch tensor of shape <code>(b, c, h, w)</code>, <code>t</code> is a PyTorch tensor of shape <code>(b,)</code> that gives, for each image, the timestep to which we would like to noise each image to, and <code>noise</code> allows us to optionally supply custom noise rather than sample Gaussian noise.</p><p>We simply perform and return the calculation in the equation above, using elements from the aforementioned buffers as coefficients. The <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/helpers.py#L25">default</a></code> function samples random Gaussian noise when <code>None</code> is supplied, and <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/helpers.py#L56">extract</a></code> extracts the proper values from the buffers according to <code>t</code>. </p><h3 id="reverse-diffusion-process">Reverse Diffusion Process</h3><p>Ultimately, our goal is to sample from this distribution:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/q_posterior.png" alt="" loading="lazy" width="1774" height="145" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/q_posterior.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/q_posterior.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/q_posterior.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/q_posterior.png 1774w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>Given an image and its noised counterpart, <strong>this distribution tells us how to take a step &#34;back in time&#34; in the diffusion process</strong>, slightly denoising the noisy image. Similarly to above, sampling from this distribution is equivalent to calculating</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-24.png" alt="" loading="lazy" width="945" height="113" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-24.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-24.png 945w" sizes="(min-width: 720px) 720px"/></figure><p>To perform this calculation, we require the distribution&#39;s mean and variance. The variance is a <strong>deterministic function of the variance schedule</strong>:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/posterior_var_wide-1.png" alt="" loading="lazy" width="1925" height="257" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/posterior_var_wide-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/posterior_var_wide-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/posterior_var_wide-1.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/posterior_var_wide-1.png 1925w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>On the other hand, the <strong>mean depends on the original and noised images </strong>(although the coefficients are again deterministic functions of the variance schedule). The form of the mean is:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean-1.png" alt="" loading="lazy" width="1984" height="252" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/posterior_mean-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/posterior_mean-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/posterior_mean-1.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean-1.png 1984w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>At inference, <strong>we will not have <em>x_0</em></strong>, the &#34;original image&#34;, because it is what we are <strong>trying to generate</strong> (a novel image). <strong>This is where our trainable U-Net comes into the picture</strong> - we use it to predict<em> x_0</em> from <em>x_t</em>.</p><p>In practice, <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#final-objective">better results are seen</a> when the U-Net learns to predict the <em>noise component</em> of the image, from which we can calculate <em>x_0</em>. Once we have <em>x_0</em>, we can calculate the distribution mean with the formula above, giving us what we need to sample from the posterior (i.e. denoise the image back one timestep). Visually, the overall process looks like this:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/inference.png" alt="" loading="lazy" width="960" height="540" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/inference.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/inference.png 960w" sizes="(min-width: 720px) 720px"/></figure><p>The function to sample from the posterior (green block in the diagram) will be defined in the <code><a href="https://assemblyai-examples.github.io/MinImagen/minimagen.html#minimagen.Imagen.Imagen">Imagen</a></code> class, but we will define the two remaining functions now. First, we implement the function that calculates <em>x_0 </em>given a noised image and its noisy component (red block in the diagram). From above, we have: </p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/xt-1.png" alt="" loading="lazy" width="2000" height="233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/xt-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/xt-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/xt-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/xt-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>Rearranging it in order to isolate <em>x_0 </em>yields the below, where two <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/diffusion_model.py#L48">buffers</a> have again been highlighted.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/x0_boxes.png" alt="" loading="lazy" width="2000" height="302" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/x0_boxes.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/x0_boxes.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/x0_boxes.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/x0_boxes.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>That is, to calculate <em>x_0 </em>we simply subtract the noise (predicted by the U-Net) from <em>x_t</em>, where both noisy image and noise itself are scaled by their respective coefficients as dictated by the timestep. Let&#39;s implement this function <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L147">predict_start_from_noise</a></code> in PyTorch now:</p><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	# ...

    def q_sample(self, x_start, t, noise=None):
        # ...
        
    def predict_start_from_noise(self, x_t, t, noise):
        return (
                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -
                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise
        )</code></pre><p>Now that we have a function for calculating <em>x_0</em>, we can go back and calculate the posterior mean and variance (yellow block in the diagram). We repeat below their functional definitions from above, highlighting <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/diffusion_model.py#L63">buffers</a> defined in <code>_init__</code> as needed. </p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean_coeffs-1.png" alt="" loading="lazy" width="1925" height="366" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/posterior_mean_coeffs-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/posterior_mean_coeffs-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/posterior_mean_coeffs-1.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean_coeffs-1.png 1925w" sizes="(min-width: 720px) 720px"/></figure><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/posterior_variance_wide.png" alt="" loading="lazy" width="1925" height="366" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/posterior_variance_wide.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/posterior_variance_wide.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/posterior_variance_wide.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/posterior_variance_wide.png 1925w" sizes="(min-width: 720px) 720px"/></figure><p>Let&#39;s implement a function <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L87">q_posterior</a></code> to calculate these variables in PyTorch:</p><pre><code>class GaussianDiffusion(nn.Module):

    def __init__(self, *, timesteps: int):
    	# ...

    def q_sample(self, x_start, t, noise=None):
        # ...
        
    def predict_start_from_noise(self, x_t, t, noise):
        return (
                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -
                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise
        )
     
    def q_posterior(self, x_start: torch.tensor, x_t: torch.tensor, t: torch.tensor, **kwargs):
        posterior_mean = (
                extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +
                extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
        )
        posterior_variance = extract(self.posterior_variance, t, x_t.shape)
        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)
        return posterior_mean, posterior_variance, posterior_log_variance_clipped</code></pre><p>In practice, we return both the variance and log of the variance (<code>posterior_log_variance_clipped</code>, where &#34;clipped&#34; means that we push values of 0 to 1e-20 before taking the log). The reason for using the log of the variance is numerical stability in our calculations, which we will point out later when relevant.</p><h3 id="summary">Summary</h3><p>To recap, in this section we defined the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L8">GaussianDiffusion</a></code> class, which is responsible for defining the diffusion process operations. We first implemented <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L125">q_sample</a></code>, which<em> </em>performs the <em>forward diffusion process</em>, noising images to a given timestep in the diffusion process. We also implemented  <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L147">predict_start_from_noise</a></code> and <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/diffusion_model.py#L87">q_posterior</a></code>, which are used to calculate parameters that are used in the <em>reverse diffusion</em> <em>process</em>.</p><h2 id="building-the-noise-prediction-model">Building the Noise Prediction Model</h2><p>Now it&#39;s time to denoise our noise prediction model - the U-Net. This model is fairly complicated, so to be concise we will examine its forward pass, introducing relevant objects in the <code>__init__</code> where relevant. Examining only the forward pass will help us understand how the U-Net works operationally while omitting unnecessary details that are not instructive in our learning how to build Imagen.</p><p>The U-Net class <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L25">Unet</a></code> can be found in <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/minimagen/Unet.py">minimagen.Unet</a></code>.  To jump to a summary of this section, click <a href="#summary-1">here</a>.</p><h3 id="overview">Overview</h3><p>Recall that the U-Net architecture for Imagen is similar to the one seen in the below diagram. We make a few modifications, most notably placing the attention block (which is a Transformer encoder for us) at the <em>end</em> of each layer in the U-Net.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/arch-4.png" alt="" loading="lazy" width="1536" height="1152" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/arch-4.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/arch-4.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/arch-4.png 1536w" sizes="(min-width: 720px) 720px"/></figure><h3 id="generating-time-conditioning-vectors">Generating Time Conditioning Vectors</h3><p>Remember that our U-Net is a <em>conditional</em> model, meaning it depends on our input text captions. Without this conditioning, <em>there would be no way to tell the model what we want to be present</em> in the generated images. Additionally, since we are using the same U-Net for all timesteps, we need to condition on the timestep information so the model knows what magnitude of noise it should be removing at any given time (remember, our variance schedule varies with <em>t</em>). Let&#39;s take a look at how we <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L506">generate this time conditioning signal</a> now. A <a href="https://www.assemblyai.com/blog/content/images/2022/07/time_encoding-2.png">diagram</a> of these calculations can be seen at the end of this section.</p><p>Input to the model we receive a <strong>time vector</strong> of shape <code>(b,)</code>, which provides the timestep for each image in the batch. We first pass this vector through a module which generates <strong>hidden states</strong> from them:</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        
        self.to_time_hiddens = nn.Sequential(
            SinusoidalPosEmb(dim),
            nn.Linear(dim, time_cond_dim),
            nn.SiLU()
        )
                
    def forward(self, *args, **kwargs):
    
    	time_hiddens = self.to_time_hiddens(time)</code></pre><p>First, for each time a unique <strong>positional encoding vector </strong>is generated (<code>SinusoidalPostEmb()</code>), which maps the integer value of the timestep for a given image into a representative vector that we can use for timestep conditioning. For a recap on positional encodings, see the dropdown <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#timestep-conditioning">here</a>. Next, these encodings are projected to a higher dimensional space (<code>time_cond_dim</code>), and passed through the <code>SiLU</code> nonlinearity. The result is a tensor of size <code>(b, time_cond_dim)</code> that constitutes our hidden states for the timesteps.</p><p>These hidden states are then used in <strong>two ways</strong>. First, a time conditioning tensor <code>t</code> is generated, which we will use to provide timestep conditioning at each step in the U-Net. These are generated from <code>time_hiddens</code> with a simple linear layer. Second, time <em>tokens</em> <code>time_tokens</code> are generated again from <code>time_hiddens</code> with a simple linear layer, which are concatenated to the main text-conditioning tokens we will generate momentarily. The reason we have these two uses is because the <strong>time conditioning is necessarily provided</strong> <strong>everywhere </strong>in the U-Net (via simple addition), while the main conditioning tokens are used <strong>only in the cross-attention</strong> operation in specific blocks/layers of the U-Net. Let&#39;s see how to implement these functions in PyTorch:</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        
        self.to_time_cond = nn.Sequential(
            nn.Linear(time_cond_dim, time_cond_dim)
        )
        
        self.to_time_tokens = nn.Sequential(
            nn.Linear(time_cond_dim, cond_dim * NUM_TIME_TOKENS),
            Rearrange(&#39;b (r d) -&gt; b r d&#39;, r=NUM_TIME_TOKENS)
        )
                
    def forward(self, *args, **kwargs):
    	# ...
        t = self.to_time_cond(time_hiddens)
        time_tokens = self.to_time_tokens(time_hiddens)</code></pre><p>The shape of <code>t</code> is <code>(b, time_cond_dim)</code>, the same as <code>time_hiddens</code>. The shape of <code>time_tokens</code> is <code>(b, NUM_TIME_TOKENS, cond_dim)</code>, where <code>NUM_TIME_TOKENS</code> defines how many time tokens should be generated that will be concatenated on the main conditioning text tokens. The default value is 2. The <a href="https://github.com/arogozhnikov/einops/blob/4ee1c9fa9d758cd5d544f4f9cd1b0bcff98f571b/einops/layers/torch.py#L12">einops Rearrange</a> layer reshapes the tensor from <code>(b, NUM_TIME_TOKENS*cond_dim)</code> to <code>(b, NUM_TIME_TOKENS, cond_dim)</code>.</p><p>The time encoding process is summarized in this figure:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/time_encoding-2.png" alt="" loading="lazy" width="2000" height="1123" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/time_encoding-2.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/time_encoding-2.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/time_encoding-2.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/time_encoding-2.png 2400w" sizes="(min-width: 720px) 720px"/></figure><h3 id="generating-text-conditioning-vectors">Generating Text Conditioning Vectors</h3><p>Now it is time to <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L536">generate our <strong>text conditioning</strong></a> objects. From our text encoder we have two tensors - the <strong>text embeddings</strong> of the batch captions, <code>text_embeds</code>, and the <strong>text mask</strong>, <code>text_mask</code>, which tells us how many words are in each caption in the batch. These tensors are size <code>(b, max_words, enc_dim)</code>, and <code>(b, max_words)</code> respectively, where <code>max_words</code> is the number of words in the longest caption in the batch, and <code>enc_dim</code> is the encoding dimension of the text encoder. </p><p>We also incorporate <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#classifier-free-guidance">classifier-free guidance</a> at this point; so, given all of the moving parts, let&#39;s take a look at a visual example to understand what&#39;s going at a high level. All of the calculations are again summarized in a <a href="https://raw.githubusercontent.com/AssemblyAI-Examples/MinImagen/main/images/conditioning_diagram.png">diagram</a> below.</p><!--kg-card-begin: html--><h4>Visual Example</h4><!--kg-card-end: html--><p>Let&#39;s assume that we have three captions - &#39;<em>a very big red house</em>&#39;, &#39;<em>a man</em>&#39;, and &#39;<em>a happy dog</em>&#39;. Our text encoder provides the following:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/tc_1.png" alt="" loading="lazy" width="1026" height="913" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/tc_1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/tc_1.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/tc_1.png 1026w" sizes="(min-width: 720px) 720px"/></figure><p>We project the embedding vectors to a higher dimension (greater horizontal width), and pad both the mask and embedding tensors (extra entry vertically) to the maximum number of words allowed in a caption, <strong>a value we choose</strong> and which we let be 6 here:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/tc_2.png" alt="" loading="lazy" width="1026" height="1056" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/tc_2.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/tc_2.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/tc_2.png 1026w" sizes="(min-width: 720px) 720px"/></figure><p>From here, we incorporate <strong><a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#classifier-free-guidance">classifier-free guidance</a></strong> by <em>randomly deciding which batch instances to drop</em> with a fixed probability. Let&#39;s just assume that the last instance is dropped, which is implemented by alterting the text mask.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/tc_3.png" alt="" loading="lazy" width="1026" height="1056" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/tc_3.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/tc_3.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/tc_3.png 1026w" sizes="(min-width: 720px) 720px"/></figure><p>Continuing with classifier-free guidance, we generate NULL vectors to use for the dropped elements.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/tc_4.png" alt="" loading="lazy" width="1152" height="294" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/tc_4.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/tc_4.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/tc_4.png 1152w" sizes="(min-width: 720px) 720px"/></figure><p>We replace the encodings will NULL wherever the text mask is red:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/tc_5.png" alt="" loading="lazy" width="1026" height="1056" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/tc_5.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/tc_5.png 1000w, https://www.assemblyai.com/blog/content/images/2022/07/tc_5.png 1026w" sizes="(min-width: 720px) 720px"/></figure><p>To get the final main conditioning token <code>c</code>, we simple concatenate the <code>time_tokens</code> generated above to these text conditioning tensors. The concatenation happens along the num_tokens/word dimension to leave a final main conditioning token of shape <code>(b, NUM_TIME_TOKENS + max_text_len, cond_dim)</code>.</p><p>Finally, we also mean pool across the word dimension to acquire a tensor of shape <code>(b, cond_dim)</code>, and then project to the time conditioning vector dimension to yield a tensor of shape <code>(b, 4*cond_dim)</code>. After dropping the necessary instances along the batch dimensions according to the classifier-free guidance vector, we <strong>add </strong>this to <code>t</code> to get the final timestep conditioning <code>t</code>.</p><!--kg-card-begin: html--><h4>Corresponding Code</h4><!--kg-card-end: html--><p>The corresponding code for these operations is a bit cumbersome and just reiterates implements the above process, so the code will be omitted here. Feel free to check out the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L536">Unet._text_condition</a></code> method in the source code to explore how this function is implemented. The below image summarizes the entire conditioning generation process, so feel free to open <a href="https://raw.githubusercontent.com/AssemblyAI-Examples/MinImagen/main/images/conditioning_diagram.png">this image</a> in a new tab and follow along visually while going through the code in order to stay oriented.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/conditioning_diagram3-1.png" alt="" loading="lazy" width="2000" height="935" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/conditioning_diagram3-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/conditioning_diagram3-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/conditioning_diagram3-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/conditioning_diagram3-1.png 2400w" sizes="(min-width: 720px) 720px"/><figcaption>(this image is compressed - see a full resolution version <a href="https://raw.githubusercontent.com/AssemblyAI-Examples/MinImagen/main/images/conditioning_diagram.png">here</a>)</figcaption></figure><h3 id="building-the-u-net">Building the U-Net</h3><p>Now that we have the two conditioning tensors we need - the main conditioning tensor <code>c</code> applied via attention and the time conditioning tensor <code>t</code> applied via addition - we can move on to defining the U-Net itself. As above, we continue by examining <code>Unet</code>&#39;s <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L354">forward</a></code> method, introducing objects in <code>__init__</code> as needed.</p><!--kg-card-begin: html--><h4>Initial Convolution</h4><!--kg-card-end: html--><p>First, we need to perform an <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L168">initial convolution</a> to get our input images to the expected number of channels for the network. We utilize <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L254">minimagen.layers.CrossEmbedLayer</a></code>, which is essentially an Inception layer.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-16.png" alt="" loading="lazy" width="1631" height="1158" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-16.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/image-16.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/image-16.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/image-16.png 1631w" sizes="(min-width: 720px) 720px"/></figure><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.init_conv = CrossEmbedLayer(channels, dim_out=dim, kernel_sizes=(3, 7, 15), stride=1)
        
    def forward(self, *args, **kwargs):
    	# ...
        x = self.init_conv(x)</code></pre><!--kg-card-begin: html--><h4>Initial ResNet Block</h4><!--kg-card-end: html--><p>Next, we pass the images into the initial ResNet block (<code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L371">minimagen.layers.ResnetBlock</a></code>) for this layer of the U-Net, called <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L418">init_block</a></code>.</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.init_block = ResnetBlock(current_dim, current_dim, cond_dim=layer_cond_dim, time_cond_dim=time_cond_dim, groups=groups)
        
    def forward(self, *args, **kwargs):
    	# ...
        x = init_block(x, t, c)</code></pre><p>The ResNet block first passes the images through an initial <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L412">block1</a></code> (<code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L107">minimagen.layers.block</a></code>), resulting in an output tensor of the same size as the input.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/ib_0.png" alt="" loading="lazy" width="2000" height="1233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/ib_0.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/ib_0.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/ib_0.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/ib_0.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>Next, residual cross attention (<code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L180">minimagen.layers.CrossAttention</a></code>) is performed with the main conditioning tokens</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/ib_1.png" alt="" loading="lazy" width="2000" height="1233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/ib_1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/ib_1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/ib_1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/ib_1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>After that we pass the time encodings through a simple <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L396">MLP</a> to attain the proper dimensionality, and then split it into two sizes <code>(b, c, 1, 1)</code> tensors.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/ib_2.png" alt="" loading="lazy" width="2000" height="1233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/ib_2.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/ib_2.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/ib_2.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/ib_2.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>We finally pass the images through another <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L107">convolution block</a> that is identical to <code>block1</code>, except for the fact that it incorporates the timestep information via a <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L140">scale-shift</a> using the timestep embeddings.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/ib_3.png" alt="" loading="lazy" width="2000" height="1233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/ib_3.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/ib_3.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/ib_3.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/ib_3.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>The final resulting output of <code>init_block</code> has the same shape as the input tensor.</p><!--kg-card-begin: html--><h4>Remaining ResNet Blocks</h4><!--kg-card-end: html--><p>Next, we pass the images through a sequence of <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L371">ResNet blocks</a> that are identical to <code>init_block</code>, except for the fact that they only condition on the timestep. We save the outputs in <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L408">hiddens</a></code> for the skip connections later on.</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.resnet_blocks = nn.ModuleList(
                    [
                        ResnetBlock(current_dim, current_dim, time_cond_dim=time_cond_dim, groups=groups)
                        for _ in range(layer_num_resnet_blocks)
                    ]
        
    def forward(self, *args, **kwargs):
    	# ...
        hiddens = []
        for resnet_block in self.resnet_blocks:
                x = resnet_block(x, t)
                hiddens.append(x)</code></pre><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/resnet_block.png" alt="" loading="lazy" width="2000" height="1233" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/resnet_block.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/resnet_block.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/resnet_block.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/resnet_block.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><h4>Final Transformer Block</h4><!--kg-card-end: html--><p>After processing with the ResNet blocks, we optionally pass the images through a <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L426">Transformer encoder</a> (<code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L468">minimagen.layers.TransformerBlock</a></code>).</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.transformer_block = TransformerBlock(dim=current_dim, heads=ATTN_HEADS, dim_head=ATTN_DIM_HEAD)
        
    def forward(self, *args, **kwargs):
    	# ...
        x = self.transformer_block(x)
        hiddens.append(x)</code></pre><p>The transformer block applies multi-headed attention (purple block below), and then passes the output through a <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L148">minimagen.layers.ChanFeedForward</a></code> layer, a sequence of convolutions with layer norms between them and GeLU between them.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/transformer_simple.png" alt="" loading="lazy" width="2000" height="834" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/transformer_simple.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/transformer_simple.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/transformer_simple.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/transformer_simple.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Detailed Diagram</p>
  </header>
	<p><img src="https://raw.githubusercontent.com/AssemblyAI-Examples/MinImagen/main/images/transformer_full.png" alt_text="Detailed Transformer encoder diagram"/>
  </p>
</div>
   <!--kg-card-end: html--><!--kg-card-begin: html--><h4>Downsample</h4><!--kg-card-end: html--><p>As the final step for this layer of the U-Net, the images are <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L430">downsampled</a> to half the spatial width.</p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.post_downsample = Downsample(current_dim, dim_out)
        
    def forward(self, *args, **kwargs):
    	# ...
        x = post_downsample(x)</code></pre><p>Where the <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L308">downsampling operation</a> is a simple fixed convolution.</p><pre><code>def Downsample(dim, dim_out=None):
    dim_out = default(dim_out, dim)
    return nn.Conv2d(dim, dim_out, kernel_size=4, stride=2, padding=1)</code></pre><!--kg-card-begin: html--><h4>Middle Layers</h4><!--kg-card-end: html--><p>The above sequence of ResNet blocks, (possible) Transformer encoder, and Downsampling is repeated for each layer of the U-Net until we reach the lowest spatial resolution / greatest channel depth. At this point, we <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/Unet.py#L436">pass the images through two more ResNet blocks</a>, which <strong>do</strong> condition on the main conditioning tokens (like the <code>init_block</code> of each Resnet Layer). Optionally, we pass the images through a residual <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L14">Attention</a> layer between these blocks. </p><pre><code>class Unet(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
                self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=time_cond_dim,
                                      groups=resnet_groups[-1])
        self.mid_attn = EinopsToAndFrom(&#39;b c h w&#39;, &#39;b (h w) c&#39;,
                                        Residual(Attention(mid_dim, heads=ATTN_HEADS,
                                                           dim_head=ATTN_DIM_HEAD))) if attend_at_middle else None
        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim=cond_dim, time_cond_dim=time_cond_dim,
                                      groups=resnet_groups[-1])
        
    def forward(self, *args, **kwargs):
    	# ...
        x = self.mid_block1(x, t, c)
        if exists(self.mid_attn):
            x = self.mid_attn(x)
        x = self.mid_block2(x, t, c)</code></pre><!--kg-card-begin: html--><h4>Upsampling Trajectory</h4><!--kg-card-end: html--><p>The upsampling trajectory of the U-Net is largely a mirror-inverse of the downsampling trajectory, except for the fact that we (a) <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Unet.py#L444">concatenate</a> the corresponding skip connections from the downsampling trajectory before each resnet block at any given layer, and (b) we use an <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/0f305c29922274e1faefe9e93be441fdb7ed0efa/minimagen/layers.py#L502">upsampling</a> operation rather than a downsampling one. This upsampling operation is a nearest-neighbor upsampling followed by a spatial size preserving convolution</p><pre><code>def Upsample(dim, dim_out=None):
    dim_out = default(dim_out, dim)

    return nn.Sequential(
        nn.Upsample(scale_factor=2, mode=&#39;nearest&#39;),
        nn.Conv2d(dim, dim_out, 3, padding=1)
    )
</code></pre><p>For the sake of brevity, the upsampling trajectory code is not repeated here, but <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Unet.py#L446">can be found</a> in the source code.</p><p>At the end of the upsampling trajectory, a <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Unet.py#L326">final convolution layer</a> brings the images to the proper output channel depth (generally 3).</p><h3 id="summary-1">Summary</h3><p>To recap, in this section we defined the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Unet.py#L25">Unet</a></code> class, which is responsible for defining the denoising U-Net that is trained via Diffusion. We first learned how to <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Unet.py#L389">generate conditioning tensors</a> for a given timestep and caption, and then incorporate this conditioning information into the U-Net&#39;s <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Unet.py#L354">forward pass</a>, which sends images through a series of <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/layers.py#L371">ResNet blocks</a> and <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/layers.py#L468">Transformer encoders</a> in order to predict the noise component of a given image</p><h2 id="building-imagen">Building Imagen</h2><p>To recap, we have constructed a <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/diffusion_model.py#L8">GaussianDiffusion</a></code> object which defines and implements the diffusion process &#34;metamodel&#34;, which in turn utilizes our <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Unet.py#L25">Unet</a></code> class to train. Let&#39;s now take a look at how we put these pieces together to build <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L22">Imagen</a> itself. We&#39;ll again look at the two primary functions within Imagen - <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L575">forward</a></code> for training and <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L424">sample</a></code> for image generation, again introducing objects in <code>__init__</code> as needed.</p><p>The <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Imagen.py#L22">Imagen</a></code> class can be found in <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/minimagen/Imagen.py">minimagen.Imagen</a></code>.  To jump to a summary of this section, click <a href="#summary-2">here</a>.</p><h3 id="imagen-forward-pass">Imagen Forward Pass</h3><p>The Imagen forward pass consists of (1) noising the training images, (2) predicting the noise components with the U-Net, and then (3) returning the loss between the predicted noise and the true noise.</p><p>To begin, we randomly sample timesteps to noise the training images to, and then encoding the conditioning text, placing the embeddings and masks on the same device as the input image tensor:</p><pre><code>from minimagen.t5 import t5_encode_text

class Imagen(nn.Module):
    def __init__(self, timesteps):
        self.noise_scheduler = GaussianDiffusion(timesteps=timesteps)
        self.text_encoder_name = &#39;t5_small&#39;
        
    def forward(self, images, texts):
        times = self.noise_scheduler.sample_random_times(b, device=device)
        
        text_embeds, text_masks = t5_encode_text(texts, name=self.text_encoder_name)
        text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))</code></pre><p>Recall that Imagen has a <strong>base</strong> model that generates small images and <strong>super-resolution</strong> models that upscale the images. We therefore need to resize the images to the proper size for the U-Net in use. If the U-Net is a super-resolution model, we additionally need to rescale the training images <strong>first </strong>down to the low-resolution conditioning size, <strong>and then</strong> up to the proper size for the U-Net. This simulates the upsampling of one U-Net&#39;s output to the size of the next U-Net&#39;s input in Imagen&#39;s super-resolution chain (allowing the latter U-Net to condition on the former U-Net&#39;s output). </p><p>We also <strong>add noise to the low-resolution conditioning </strong>images for <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#robust-cascaded-diffusion-models">noise conditioning augmentation</a>, picking one noise level for the whole batch.</p><pre><code>#...
from minimagen.helpers import resize_image_to
from einops import repeat

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
        self.lowres_noise_schedule = GaussianDiffusion(timesteps=timesteps)

       
    def forward(self, images, texts):
    	# ...
        lowres_cond_img = lowres_aug_times = None
        if exists(prev_image_size):
            lowres_cond_img = resize_image_to(images, prev_image_size, pad_mode=&#39;reflect&#39;)
            lowres_cond_img = resize_image_to(lowres_cond_img, target_image_size, 
            					pad_mode=&#39;reflect&#39;)
            
            lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device=device)
            lowres_aug_times = repeat(lowres_aug_time, &#39;1 -&gt; b&#39;, b=b)
        
        images = resize_image_to(images, target_image_size)</code></pre><p>Finally, we calculate and return the loss:</p><pre><code>#...
from minimagen.helpers import resize_image_to
from einops import repeat

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
    
    def forward(self, images, texts, unet):
    	# ...
        return self._p_losses(unet, images, times, text_embeds=text_embeds, 
        			text_mask=text_masks, 
                            	lowres_cond_img=lowres_cond_img, 
                            	lowres_aug_times=lowres_aug_times)</code></pre><p>Let&#39;s take a look at <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L512">_p_losses</a></code> to see how we calculate the loss. </p><p>First, we use the Diffusion Model forward process to noise both the input images and, if the U-Net is a super-resolution model, the low-resolution conditioning images as well.</p><pre><code>#...

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
    
    def p_losses(self, x_start, times, lowres_cond_img=None):
    	# ...
        noise = torch.randn_like(x_start)
        
        x_noisy = self.noise_scheduler.q_sample(x_start=x_start, t=times, noise=noise)

        lowres_cond_img_noisy = None
        if exists(lowres_cond_img):
            lowres_aug_times = default(lowres_aug_times, times)
            lowres_cond_img_noisy = self.lowres_noise_schedule.q_sample(
                            		x_start=lowres_cond_img, t=lowres_aug_times, 
                        		noise=torch.randn_like(lowres_cond_img))</code></pre><p>Next, we use the U-Net to <strong>predict the noise component </strong>of the noisy images, taking in text embeddings as conditioning information, in addition to the low-resolution images if the U-Net is for super-resolution. <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Unet.py#L376"><code>cond_drop_prob</code></a> gives the probability of dropout for <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#classifier-free-guidance">classifier-free guidance</a>.</p><pre><code>#...

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
        self.cond_drop_prob = 0.1
    
    def p_losses(self, x_start, times, text_embeds, text_mask, lowres_cond_img=None):
    	# ...
        pred = unet.forward(
            x_noisy,
            times,
            text_embeds=text_embeds,
            text_mask=text_mask,
            lowres_noise_times=lowres_aug_times,
            lowres_cond_img=lowres_cond_img_noisy,
            cond_drop_prob=self.cond_drop_prob,
        )</code></pre><p>We then calculate the loss between the actual noise that was added and the U-Net&#39;s prediction of the noise according to <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L67">self.loss_fn</a></code>, which is L2 loss by default.</p><pre><code>#...

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
        self.loss_fn = torch.nn.functional.mse_loss
    
    def p_losses(self, x_start, times, text_embeds, text_mask, lowres_cond_img=None):
    	# ...
        return self.loss_fn(pred, noise)       </code></pre><p>That&#39;s all it takes to get the loss with Imagen! It is quite a simple process once we have built the Diffusion Model/U-Net backbone.</p><h3 id="sampling-with-imagen">Sampling with Imagen</h3><p>Ultimately, what we want to do is <strong>sample with Imagen</strong>. That is, we want to be able to generate novel images given textual captions. Recall from above that this requires calculating the forward process posterior mean:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/inference-1.png" alt="" loading="lazy" width="960" height="540" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/inference-1.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/inference-1.png 960w" sizes="(min-width: 720px) 720px"/></figure><p>Now that we have defined our U-Net that predicts the noise component, we have all of the pieces we need to compute the posterior mean. </p><p>First, we get the noise prediction (blue) using our U-Net&#39;s <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Unet.py#L354">forward</a></code> (or <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Unet.py#L473">forward_with_cond_scale</a></code>) method, and then calculate <em>x_0 </em>from it (red) using the U-Net&#39;s <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/diffusion_model.py#L147">predict_start_from_noise</a></code> method introduced previously which performs the below calculation:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/x0_boxes_no_dist.png" alt="" loading="lazy" width="2000" height="302" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/x0_boxes_no_dist.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/x0_boxes_no_dist.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/x0_boxes_no_dist.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/07/x0_boxes_no_dist.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>Where <em>x_t </em>is a noisy image and epsilon is the U-Net&#39;s noise prediction. Next, <em>x_0 </em>is <a href="https://www.assemblyai.com/blog/how-imagen-actually-works/#large-guidance-weight-samplers">dynamically thresholded</a> and then passed, along with <em>x_t</em>, into the into the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/diffusion_model.py#L87">q_posterior</a></code> method of the U-Net (yellow) to get the distribution mean.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean_coeffs-1.png" alt="" loading="lazy" width="1925" height="366" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/posterior_mean_coeffs-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/posterior_mean_coeffs-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/posterior_mean_coeffs-1.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/posterior_mean_coeffs-1.png 1925w" sizes="(min-width: 720px) 720px"/></figure><p>This whole process is wrapped up in <code>Imagen</code>&#39;s <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L261">_p_mean_variance</a></code> function.</p><pre><code>#...

class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
        self.dynamic_thresholding_percentile = 0.9


    def _p_mean_variance(self, unet, x, t, *, noise_scheduler 
    			text_embeds=None, text_mask=None):
    
        # Get the noise prediction from the unet (blue block)
        pred = unet.forward_with_cond_scale(x, t, text_embeds=text_embeds, text_mask=text_mask)

        # Calculate the starting images from the noise (yellow block)
        x_start = noise_scheduler.predict_start_from_noise(x, t=t, 

        # Dynamically threshold
        s = torch.quantile(
            rearrange(x_start, &#39;b ... -&gt; b (...)&#39;).abs(),
            self.dynamic_thresholding_percentile,
            dim=-1
        )

        s.clamp_(min=1.)
        s = right_pad_dims_to(x_start, s)
        x_start = x_start.clamp(-s, s) / s

        # Return the forward process posterior parameters (green block)
        return noise_scheduler.q_posterior(x_start=x_start, x_t=x, t=t, t_next=t_next)</code></pre><p>From here we have everything we need to sample from the posterior, which is to say &#34;go back one timestep&#34; in the diffusion process. That is, we are seeking to sample from the below distribution:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/q_posterior-1.png" alt="" loading="lazy" width="1774" height="145" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/q_posterior-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/07/q_posterior-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/07/q_posterior-1.png 1600w, https://www.assemblyai.com/blog/content/images/2022/07/q_posterior-1.png 1774w" sizes="(min-width: 720px) 720px"/></figure><p>We saw above that sampling from this distribution is equivalent to calculating</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/07/image-24.png" alt="" loading="lazy" width="945" height="113" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/07/image-24.png 600w, https://www.assemblyai.com/blog/content/images/2022/07/image-24.png 945w" sizes="(min-width: 720px) 720px"/></figure><p>Since we calculated the posterior mean and (log) variance with <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L261">_p_mean_variance</a></code>, we can now implement the above calculation with <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L329">_p_sample</a></code>, calculating the square root of the variance as such for numerical stability.</p><pre><code>class Imagen(nn.Module):
    def __init__(self, timesteps):
    	# ...
        self.dynamic_thresholding_percentile = 0.9

    @torch.no_grad()
    def _p_sample(self, unet, x, t, *, text_embeds=None, text_mask=None):
        
        b, *_, device = *x.shape, x.device
        
        # Get posterior parameters
        model_mean, _, model_log_variance = self.p_mean_variance(unet, x=x, t=t, 
        					text_embeds=text_embeds, text_mask=text_mask)
        
        # Get noise which we will use to calculate the denoised image
        noise = torch.randn_like(x)
        
        # No more denoising when t == 0
        is_last_sampling_timestep = (t == 0)
        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, 
        							*((1,) * (len(x.shape) - 1)))
        
        # Get the denoised image. Equivalent to mean * sqrt(variance) but calculate this way to be more numerically stable
        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise
    </code></pre><p>At this point, we have denoised the random noise input into Imagen <strong>one timestep</strong>. To generate images, we need to do this for <em>every</em> timestep, starting with randomly sampled Gaussian noise at <em>t=T</em> and going &#34;back in time&#34; until we reach <em>t=0</em>. Therefore, we run <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L329">_p_sample</a></code> in a loop over timesteps with <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L373">_p_sample_loop</a></code>:</p><pre><code>class Imagen(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        
    @torch.no_grad()
    def p_sample_loop(self, unet, shape, *, lowres_cond_img=None, lowres_noise_times=None, 
    			noise_scheduler=None, text_embeds=None, text_mask=None):
        
        device = self.device

        # Get starting noisy images
        img = torch.randn(shape, device=device)

	# Get sampling timesteps (final_t, final_t-1, ..., 2, 1, 0)
	batch = shape[0]
        timesteps = noise_scheduler.get_sampling_timesteps(batch, device=device)

	# For each timestep, denoise the images slightly
        for times in tqdm(timesteps, desc=&#39;sampling loop time step&#39;, total=len(timesteps)):
            img = self.p_sample(
                unet,
                img,
                times,
                text_embeds=text_embeds,
                text_mask=text_mask)

	# Clamp the values to be in the allowed range and potentialy
   	# unnormalize back into the range (0., 1.)
        img.clamp_(-1., 1.)
        unnormalize_img = self.unnormalize_img(img)
        return unnormalize_img</code></pre><p><code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L373">_p_sample_loop</a></code> is how we generate images with <strong>one unet</strong>. Imagen contains a <em>chain</em><strong> </strong>of U-Nets, so, finally, the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/Imagen.py#L424">sample</a></code> function iteratively passes the generated images through <em>each</em> U-Net in the chain, and handles other sampling requirements like generating text encodings/masks, placing the currently-sampling U-Net on the GPU if available, etc. <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/8445a5aeff34d5f818b9c659d9bd71941533f999/minimagen/helpers.py#L35">eval_decorator</a></code> sets the model to be in evaluation mode if it is not upon calling <code>sample</code>.</p><pre><code>class Imagen(nn.Module):
    def __init__(self, *args, **kwargs):
    	# ...
        self.noise_schedulers = nn.ModuleList([])
        for i in num_unets:
        	self.noise_schedulers.append(GaussianDiffusion(timesteps=timesteps))
    
    @torch.no_grad()
    @eval_decorator
    def sample(self, texts=None, batch_size=1, cond_scale=1., lowres_sample_noise_level=None, return_pil_images=False, device=None):
    	# Put all Unets on the same device as Imagen
        device = default(device, self.device)
        self.reset_unets_all_one_device(device=device)

	# Get the text embeddings/mask from textual captions (`texts`)
        text_embeds, text_masks = t5_encode_text(texts, name=self.text_encoder_name)
        text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))

        batch_size = text_embeds.shape[0]

        outputs = None

        is_cuda = next(self.parameters()).is_cuda
        device = next(self.parameters()).device

        lowres_sample_noise_level = default(lowres_sample_noise_level, 
        					self.lowres_sample_noise_level)

		# Iterate through each Unet
        for unet_number, unet, channel, image_size, noise_scheduler, dynamic_threshold in tqdm(
                zip(range(1, len(self.unets) + 1), self.unets, self.sample_channels, 
                	self.image_sizes, self.noise_schedulers, self.dynamic_thresholding)):

	  # If GPU is available, place the Unet currently being sampled from on the GPU
            context = self.one_unet_in_gpu(unet=unet) if is_cuda else null_context()

            with context:
                lowres_cond_img = lowres_noise_times = None
                shape = (batch_size, channel, image_size, image_size)

                # If on a super-res model, noise the previous unet&#39;s images for conditioning
                if unet.lowres_cond:
                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, 
                                            			lowres_sample_noise_level,
                          		                 	device=device)

                    lowres_cond_img = resize_image_to(img, image_size, pad_mode=&#39;reflect&#39;)
                    lowres_cond_img = self.lowres_noise_schedule.q_sample(
                    		x_start=lowres_cond_img,
                    		t=lowres_noise_times,
                    		noise=torch.randn_like(lowres_cond_img))
                    		shape = (batch_size, self.channels, image_size, image_size)

				# Generate an image with the current U-Net
                img = self.p_sample_loop(
                    unet,
                    shape,
                    text_embeds=text_embeds,
                    text_mask=text_masks,
                    cond_scale=cond_scale,
                    lowres_cond_img=lowres_cond_img,
                    lowres_noise_times=lowres_noise_times,
                    noise_scheduler=noise_scheduler,
                )
                
                # Output the image if at the end of the super-resolution chain
                outputs = img if unet_number == len(self.unets) + 1 else None

        # Return tensors or PIL images
        if not return_pil_images:
            return outputs
            
        pil_images = list(map(T.ToPILImage(), img.unbind(dim=0)))

        return pil_images</code></pre><h3 id="summary-2">Summary</h3><p>To recap, in this section we defined the <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Imagen.py#L22">Imagen</a></code> class, first examining its <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Imagen.py#L575">forward</a></code> pass which noises training images, predicts their noise components, and then returns the average L2 loss between the predictions and true noise values. Then, we looked at <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/556ce32679903ff59baf0bf0dd890dc5434f51d0/minimagen/Imagen.py#L424">sample</a></code>, which is used to generate images via the successive application of the U-Nets which compose the Imagen instance.</p><h2 id="training-and-sampling-from-minimagen">Training and Sampling from MinImagen</h2><p>MinImagen can be installed with</p><pre><code>pip install minimagen</code></pre><p>The MinImagen package hides all of the implementation details discussed above, and exposes a high-level API for working with Imagen, documented <a href="https://assemblyai-examples.github.io/MinImagen/minimagen.html">here</a>. Let&#39;s check out how to use the <code>minimagen</code> package to train and sample from a MinImagen instance. You can alternatively check out MinImagen&#39;s <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/README.md#usage---command-line">GitHub repo</a> to see information on using the provided scripts for training/generation.</p><h3 id="training-minimagen">Training MinImagen</h3><p>To train Imagen, we need to first perform some imports.</p><pre><code>import os
from datetime import datetime

import torch.utils.data
from torch import optim

from minimagen.Imagen import Imagen
from minimagen.Unet import Unet, Base, Super, BaseTest, SuperTest
from minimagen.generate import load_minimagen, load_params
from minimagen.t5 import get_encoded_dim
from minimagen.training import get_minimagen_parser, ConceptualCaptions, get_minimagen_dl_opts, \
    create_directory, get_model_size, save_training_info, get_default_args, MinimagenTrain, \
    load_testing_parameters</code></pre><p>Next, we determine the device the training will happen on, using a GPU if one is available, and then instantiate a MinImagen argument parser. The parser will allow us to specify <a href="https://github.com/AssemblyAI-Examples/MinImagen#trainpy">relevant parameters</a> when running the script from the command line.</p><pre><code># Get device
device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

# Command line argument parser
parser = get_minimagen_parser()
args = parser.parse_args()</code></pre><p>Now we&#39;ll create a timestamped training directory that will store all of the information from the training. The <code>create_directory()</code> function returns a <a href="https://docs.python.org/3/library/contextlib.html">context manager</a> that allows us to temporarily enter the directory to read files, save files, etc.</p><pre><code># Create training directory
timestamp = datetime.now().strftime(&#34;%Y%m%d_%H%M%S&#34;)
dir_path = f&#34;./training_{timestamp}&#34;
training_dir = create_directory(dir_path)</code></pre><p>Since this is an example script, we replace some command-line arguments with alternative values that will lower the computational load so that we can quickly train and see the results to understand how MinImagen trains.</p><pre><code># Replace some cmd line args to lower computational load.
args = load_testing_parameters(args)</code></pre><p>Next, we&#39;ll create our DataLoaders, using a subset of the <a href="https://ai.google.com/research/ConceptualCaptions/">Conceptual Captions</a> dataset. Check out <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/training.py#L214">MinimagenDataset</a></code> if you want to use a different dataset.</p><pre><code># Replace some cmd line args to lower computational load.
args = load_testing_parameters(args)

# Load subset of Conceptual Captions dataset.
train_dataset, valid_dataset = ConceptualCaptions(args, smalldata=True)

# Create dataloaders
dl_opts = {**get_minimagen_dl_opts(device), &#39;batch_size&#39;: args.BATCH_SIZE, &#39;num_workers&#39;: args.NUM_WORKERS}
train_dataloader = torch.utils.data.DataLoader(train_dataset, **dl_opts)
valid_dataloader = torch.utils.data.DataLoader(valid_dataset, **dl_opts)</code></pre><p>It&#39;s now time to create the U-Net&#39;s that will be used in the MinImagen&#39;s U-Net chain. The base model that generates the image is a <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/Unet.py#L695">BaseTest</a></code> instance, and the super-resolution model that upscales the image is a <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/Unet.py#L725">SuperTest</a></code> instance. These models are intentionally tiny so that we can quickly train them to see how training a MinImagen instance works. See <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/Unet.py#L637">Base</a></code> and <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/Unet.py#L667">Super</a></code> for models closer to the original Imagen implementation.</p><p>We load the parameters for these U-Nets, and then instantiate the instances with a list comprehension.</p><pre><code># Use small U-Nets to lower computational load.
unets_params = [get_default_args(BaseTest), get_default_args(SuperTest)]
unets = [Unet(**unet_params).to(device) for unet_params in unets_params]</code></pre><p>Now we can finally instantiate the actual MinImagen instance. We first specify some parameters, and then create the instance.</p><pre><code># Specify MinImagen parameters
imagen_params = dict(
    image_sizes=(int(args.IMG_SIDE_LEN / 2), args.IMG_SIDE_LEN),
    timesteps=args.TIMESTEPS,
    cond_drop_prob=0.15,
    text_encoder_name=args.T5_NAME
)

# Create MinImagen from UNets with specified imagen parameters
imagen = Imagen(unets=unets, **imagen_params).to(device)</code></pre><p>For record keeping, we fill in the <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/training.py#L660">default values</a> for unspecified arguments, <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/training.py#L584">get the size</a> of the MinImagen instance, and then <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/training.py#L596">save all of this info</a> and more.</p><pre><code># Fill in unspecified arguments with defaults to record complete config (parameters) file
unets_params = [{**get_default_args(Unet), **i} for i in unets_params]
imagen_params = {**get_default_args(Imagen), **imagen_params}

# Get the size of the Imagen model in megabytes
model_size_MB = get_model_size(imagen)

# Save all training info (config files, model size, etc.)
save_training_info(args, timestamp, unets_params, imagen_params, model_size_MB, training_dir)</code></pre><p>Finally, we can train the MinImagen instance using <code><a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/cc470d3354867a2f534bcb0d7206cf466fc91a88/minimagen/training.py#L344">MinimagenTrain</a></code>:</p><pre><code># Create optimizer
optimizer = optim.Adam(imagen.parameters(), lr=args.OPTIM_LR)

# Train the MinImagen instance
MinimagenTrain(timestamp, args, unets, imagen, train_dataloader, valid_dataloader, training_dir, optimizer, timeout=30)</code></pre><p>In order to train the instance, save the script as <code>minimagen_train.py</code> and then run the following in the terminal:</p><pre><code>python minimagen_train.py</code></pre><p>N.B. - you may have to change <code>python</code> to <code>python3</code>, and/or <code>minimagen_train.py</code> to <code>-m minimagen_train</code>.</p><p>After training is complete, you will see a new <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/README.md#user-content-training-directory">Training Directory</a>, which stores all of the information from the training including model configurations and weights. To see how this Training Directory can be used to generate images, move on to the next section.</p><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p><code>train.py</code></p>
  </header>
	<p>Note that the above script is a stripped down version of the provided <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/train.py">train</a> file. You can read more about training MinImagen instances using this script <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/README.md#trainpy">here</a>.</p>
</div>
   <!--kg-card-end: html--><h3 id="generating-images-with-minimagen">Generating Images with MinImagen</h3><p>Now that we have a &#34;trained&#34; MinImagen instance, we can use it to generate images. Luckily, this process is much more straightforward. First, we&#39;ll again perform necessary imports and define an argument parser so that we can specify the location of the <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/README.md#user-content-training-directory">Training Directory</a> that contains the trained MinImagen weights.</p><pre><code>from argparse import ArgumentParser
from minimagen.generate import load_minimagen, sample_and_save

# Command line argument parser
parser = ArgumentParser()
parser.add_argument(&#34;-d&#34;, &#34;--TRAINING_DIRECTORY&#34;, dest=&#34;TRAINING_DIRECTORY&#34;, help=&#34;Training directory to use for inference&#34;, type=str)
args = parser.parse_args()</code></pre><p>Next, we can define a list of captions that we want to generate images for. We just specify one caption for now.</p><pre><code># Specify the caption(s) to generate images for
captions = [&#39;a happy dog&#39;]</code></pre><p>Now all we have to do is run <code>sample_and_save()</code>, specifying the captions and Training Directory to use, and an image for each caption will be generated and saved.</p><pre><code># Use `sample_and_save` to generate and save the iamges
sample_and_save(captions, training_directory=args.TRAINING_DIRECTORY)</code></pre><p>Alternatively, you can load a MinImagen instance and input this (rather than a Training Directory) to <code>sample_and_save()</code>, but in this case information about the MinImagen instance used to generate the images will not be saved, so this is not recommended.</p><pre><code>minimagen = load_minimagen(args.TRAINING_DIRECTORY)
sample_and_save(captions, minimagen=minimagen)    </code></pre><p>That&#39;s it! Once the generation is complete, you will see a new directory called <code>generated_images_&lt;TIMESTAMP&gt;</code> that stores the captions used to generate the images, the Training Directory used to generate images, and the images themselves. The number in each image&#39;s filename corresponds to the index of the caption that was used to generate it.</p><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p><code>inference.py</code></p>
  </header>
	<p>Note that the above script is a stripped down version of the provided <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/inference.py">inference</a> file. You can read more about training MinImagen instances using this script <a href="https://github.com/AssemblyAI-Examples/MinImagen/blob/main/README.md#inferencepy">here</a>.</p>
</div>
   <!--kg-card-end: html--><h2 id="final-words">Final Words</h2><p>The impressive results of State-of-the-Art text-to-image models speak for themselves, and MinImagen serves as a solid foundation for understanding the practical workings of such models. For more Machine Learning content, feel free to check out more of our <a href="https://www.assemblyai.com/blog/">blog</a> or <a href="https://www.youtube.com/c/AssemblyAI">YouTube</a> channel. Alternatively, follow us on <a href="https://twitter.com/AssemblyAI">Twitter</a> or follow our newsletter to stay in the loop for future content we drop.</p><!--kg-card-begin: html--><center><a data-template="button" href="https://assemblyai.us17.list-manage.com/subscribe?u=cb9db7b18b274c2d402a56c5f&amp;id=2116bf7c68">Follow the AssemblyAI Newsletter</a></center><!--kg-card-end: html-->
    </section>
    
  </article></div>
  </body>
</html>
