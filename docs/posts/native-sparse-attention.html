<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aclanthology.org/2025.acl-long.1126/">Original</a>
    <h1>Native Sparse Attention</h1>
    
    <div id="readability-page-1" class="page"><div id="main-container"><section id="main"><div><p><a href="https://aclanthology.org/people/j/jingyang-yuan/">Jingyang Yuan</a>,
<a href="https://aclanthology.org/people/h/huazuo-gao/">Huazuo Gao</a>,
<a href="https://aclanthology.org/people/d/damai-dai/">Damai Dai</a>,
<a href="https://aclanthology.org/people/j/junyu-luo/">Junyu Luo</a>,
<a href="https://aclanthology.org/people/l/liang-zhao/">Liang Zhao</a>,
<a href="https://aclanthology.org/people/z/zhengyan-zhang/">Zhengyan Zhang</a>,
<a href="https://aclanthology.org/people/z/zhenda-xie/">Zhenda Xie</a>,
<a href="https://aclanthology.org/people/y/yuxing-wei/">Yuxing Wei</a>,
<a href="https://aclanthology.org/people/l/lean-wang/">Lean Wang</a>,
<a href="https://aclanthology.org/people/z/zhiping-xiao/">Zhiping Xiao</a>,
<a href="https://aclanthology.org/people/y/yuqing-wang/">Yuqing Wang</a>,
<a href="https://aclanthology.org/people/c/chong-ruan/">Chong Ruan</a>,
<a href="https://aclanthology.org/people/m/ming-zhang/">Ming Zhang</a>,
<a href="https://aclanthology.org/people/w/wenfeng-liang/">Wenfeng Liang</a>,
<a href="https://aclanthology.org/people/w/wangding-zeng/">Wangding Zeng</a></p></div><hr/><div><div><div><div><h5>Abstract</h5><p><span>Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.</span></p></div></div><dl><dt>Anthology ID:</dt><dd>2025.acl-long.1126</dd><dt>Volume:</dt><dd><a href="https://aclanthology.org/volumes/2025.acl-long/">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2025</dd><dt>Address:</dt><dd>Vienna, Austria</dd><dt>Editors:</dt><dd><a href="https://aclanthology.org/people/w/wanxiang-che/">Wanxiang Che</a>,
<a href="https://aclanthology.org/people/j/joyce-nabende/">Joyce Nabende</a>,
<a href="https://aclanthology.org/people/e/ekaterina-shutova/">Ekaterina Shutova</a>,
<a href="https://aclanthology.org/people/m/mohammad-taher-pilehvar/">Mohammad Taher Pilehvar</a></dd><dt>Venue:</dt><dd><a href="https://aclanthology.org/venues/acl/">ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>23078–23097</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href="https://aclanthology.org/2025.acl-long.1126/">https://aclanthology.org/2025.acl-long.1126/</a></dd><dt>DOI:</dt><dd></dd><dt>Award:</dt><dd><i></i> Best Paper</dd><dt>Bibkey:</dt><dd></dd><dt>Cite (ACL):</dt><dd><span id="citeACL">Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. <a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>. In <i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pages 23078–23097, Vienna, Austria. Association for Computational Linguistics.</span></dd><dt>Cite (Informal):</dt><dd><span id="citeRichText"><a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a> (Yuan et al., ACL 2025)</span></dd><dt>Copy Citation:</dt><dd>



</dd><dt>PDF:</dt><dd><a href="https://aclanthology.org/2025.acl-long.1126.pdf">https://aclanthology.org/2025.acl-long.1126.pdf</a></dd></dl></div></div><hr/><div id="citeModal" tabindex="-1" role="dialog" aria-labelledby="citeModalLabel" aria-hidden="true"><div role="document"><div><div><ul id="citeFormats" role="tablist"><li><a data-toggle="list" href="#citeBibtex" role="tab" aria-controls="citeBibtex" aria-selected="true">BibTeX</a></li><li><a data-toggle="list" href="#citeMods" role="tab" aria-controls="citeMods" aria-selected="false">MODS XML</a></li><li><a data-toggle="list" href="#citeEndnote" role="tab" aria-controls="citeEndnote" aria-selected="false">Endnote</a></li><li><a data-toggle="list" href="#citeMarkdown" role="tab" aria-controls="citeMarkdown" aria-selected="false">Preformatted</a></li></ul><div id="citeFormatsContent"><div id="citeBibtex" role="tabpanel"><pre id="citeBibtexContent">@inproceedings{yuan-etal-2025-native,
    title = &#34;Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention&#34;,
    author = &#34;Yuan, Jingyang  and
      Gao, Huazuo  and
      Dai, Damai  and
      Luo, Junyu  and
      Zhao, Liang  and
      Zhang, Zhengyan  and
      Xie, Zhenda  and
      Wei, Yuxing  and
      Wang, Lean  and
      Xiao, Zhiping  and
      Wang, Yuqing  and
      Ruan, Chong  and
      Zhang, Ming  and
      Liang, Wenfeng  and
      Zeng, Wangding&#34;,
    editor = &#34;Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher&#34;,
    booktitle = &#34;Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
    month = jul,
    year = &#34;2025&#34;,
    address = &#34;Vienna, Austria&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://aclanthology.org/2025.acl-long.1126/&#34;,
    pages = &#34;23078--23097&#34;,
    ISBN = &#34;979-8-89176-251-0&#34;,
    abstract = &#34;Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.&#34;
}</pre></div><div id="citeMods" role="tabpanel"><pre id="citeModsContent">&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;yuan-etal-2025-native&#34;&gt;
    &lt;titleInfo&gt;
        &lt;title&gt;Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention&lt;/title&gt;
    &lt;/titleInfo&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Jingyang&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Yuan&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Huazuo&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Gao&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Damai&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Dai&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Junyu&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Luo&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Liang&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Zhao&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Zhengyan&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Zhang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Zhenda&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Xie&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Yuxing&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Wei&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Lean&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Wang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Zhiping&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Xiao&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Yuqing&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Wang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Chong&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Ruan&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Ming&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Zhang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Wenfeng&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Liang&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Wangding&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Zeng&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;originInfo&gt;
        &lt;dateIssued&gt;2025-07&lt;/dateIssued&gt;
    &lt;/originInfo&gt;
    &lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
    &lt;relatedItem type=&#34;host&#34;&gt;
        &lt;titleInfo&gt;
            &lt;title&gt;Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/title&gt;
        &lt;/titleInfo&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Wanxiang&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Che&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Joyce&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Nabende&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Ekaterina&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Shutova&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Mohammad&lt;/namePart&gt;
            &lt;namePart type=&#34;given&#34;&gt;Taher&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Pilehvar&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;originInfo&gt;
            &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt;
            &lt;place&gt;
                &lt;placeTerm type=&#34;text&#34;&gt;Vienna, Austria&lt;/placeTerm&gt;
            &lt;/place&gt;
        &lt;/originInfo&gt;
        &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt;
        &lt;identifier type=&#34;isbn&#34;&gt;979-8-89176-251-0&lt;/identifier&gt;
    &lt;/relatedItem&gt;
    &lt;abstract&gt;Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.&lt;/abstract&gt;
    &lt;identifier type=&#34;citekey&#34;&gt;yuan-etal-2025-native&lt;/identifier&gt;
    &lt;location&gt;
        &lt;url&gt;https://aclanthology.org/2025.acl-long.1126/&lt;/url&gt;
    &lt;/location&gt;
    &lt;part&gt;
        &lt;date&gt;2025-07&lt;/date&gt;
        &lt;extent unit=&#34;page&#34;&gt;
            &lt;start&gt;23078&lt;/start&gt;
            &lt;end&gt;23097&lt;/end&gt;
        &lt;/extent&gt;
    &lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
</pre></div><div id="citeEndnote" role="tabpanel"><pre id="citeEndnoteContent">%0 Conference Proceedings
%T Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention
%A Yuan, Jingyang
%A Gao, Huazuo
%A Dai, Damai
%A Luo, Junyu
%A Zhao, Liang
%A Zhang, Zhengyan
%A Xie, Zhenda
%A Wei, Yuxing
%A Wang, Lean
%A Xiao, Zhiping
%A Wang, Yuqing
%A Ruan, Chong
%A Zhang, Ming
%A Liang, Wenfeng
%A Zeng, Wangding
%Y Che, Wanxiang
%Y Nabende, Joyce
%Y Shutova, Ekaterina
%Y Pilehvar, Mohammad Taher
%S Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
%D 2025
%8 July
%I Association for Computational Linguistics
%C Vienna, Austria
%@ 979-8-89176-251-0
%F yuan-etal-2025-native
%X Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.
%U https://aclanthology.org/2025.acl-long.1126/
%P 23078-23097</pre></div><div id="citeMarkdown" role="tabpanel"><h5>Markdown (Informal)</h5><p id="citeMarkdownContent">[Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://aclanthology.org/2025.acl-long.1126/) (Yuan et al., ACL 2025)</p><ul><li><a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a> (Yuan et al., ACL 2025)</li></ul><h5>ACL</h5><ul><li id="citeACLstyleContent">Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. <a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>. In <i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pages 23078–23097, Vienna, Austria. Association for Computational Linguistics.</li></ul></div></div></div></div></div></div></section></div></div>
  </body>
</html>
