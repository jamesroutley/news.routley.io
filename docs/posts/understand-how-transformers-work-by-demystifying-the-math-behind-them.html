<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/">Original</a>
    <h1>Understand how transformers work by demystifying the math behind them</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>In this blog post, we’ll do an end-to-end example of the math within a transformer model. The goal is to get a good understanding of how the model works. To make this manageable, we’ll do lots of simplification. As we’ll be doing quite a bit of the math by hand, we’ll reduce the dimensions of the model. For example, rather than using embeddings of 512 values, we’ll use embeddings of 4 values. This will make the math easier to follow! We’ll use random vectors and matrices, but you can use your own values if you want to follow along.</p>
<p>As you’ll see, the math is not that complicated. The complexity comes from the number of steps and the number of parameters. <strong>I recommend you to read the <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> blog before reading this blog post (or reading in parallel)</strong>. It’s a great blog post that explains the transformer model in a very intuitive (and illustrative!) way and I don’t intend to explain what it’s already explained there. My goal is to explain the “how” of the transformer model, not the “what”. If you want to dive even deeper, check out the famous original paper: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>.</p>
<p><strong>Prerequisites</strong></p>
<p>A basic understanding of linear algebra is required - we’ll mostly do simple matrix multiplications, so no need to be an expert. Apart from that, basic understanding of Machine Learning and Deep Learning will be useful.</p>
<p><strong>What is covered here?</strong></p>
<ul>
<li>An end-to-end example of the math within a transformer model during inference</li>
<li>An explanation of attention mechanisms</li>
<li>An explanation of residual connections and layer normalization</li>
<li>Some code to scale it up!</li>
</ul>
<p>Without further ado, let’s get started! Our goal will be to use the transformer model as a translation tool, so we’ll pass an input to the model expecting it to generate the translation. For example, we could pass “Hello World” in English and expect “Hola Mundo” in Spanish.</p>
<p>Let’s take a look at the diagram of the transformer beast (don’t be intimidatd by it, you’ll soon understand it!):</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"/></a></p>
<figcaption>Transformer model from the original “attention is all you need” paper</figcaption>
</figure>
</div>
<p>The original transformer model has two parts: encoder and decoder. The encoder focus is in “understanding” or “capturing the meaning” of the input text, while the decoder focus is in generating the output text. We’ll first focus on the encoder part.</p>
<section id="encoder">
<h2 data-anchor-id="encoder">Encoder</h2>
<p>The whole goal of the encoder is to generate a rich embedding representation of the input text. This embedding will capture semantic information about the input, and will then be passed to the decoder to generate the output text. The encoder is composed of a stack of N layers. Before we jump into the layers, we need to see how to pass the words (or tokens) into the model.</p>
<div>

<p>Embeddings are a somewhat overused term. We’ll first create an embedding that will be the input to the encoder. The encoder also outputs an embedding (also called hidden states sometimes). The decoder will also receive an embedding! 😅 The whole point of an embedding is to represent a token as a vector.</p>
</div>
<section id="tokenization">
<h3 data-anchor-id="tokenization">0. Tokenization</h3>
<p>ML models can process numbers, not text. soo we need to turn our input text into numbers. That’s what <strong>tokenization</strong> does! This is the process of splitting the input text into tokens, each with an associated ID. For example, we could split the text “Hello World” into two tokens: “Hello” and “World”. We could also split it into characters: “H”, “e”, “l”, “l”, “o”, ” “,”W”, “o”, “r”, “l”, “d”. The choice of tokenization is up to us and depends on the data we’re working with.</p>
<p>Word-based tokenization (splitting the text into words) will require a very large <strong>vocabulary</strong> (all possible tokens). It will also represent words like “dog” and “dogs” or “run” and “running” as different tokens. Character-based vocabulary will require a smaller vocabulary, but will provide less meaning (in can be useful for languages such as Chinese where each character carries more information).</p>
<p>The field has moved towards subword tokenization. This is a middle ground between word-based and character-based tokenization. We’ll split the words into subwords. For example, we could split “tokenization” into “token” and “ization”. How do we decide how to split the words? This is part of training a tokenizer through a statistical process that tries to identify which subwords are the best to pick given a dataset. It’s a deterministic process (unlike training a ML model).</p>
<p>For this blog post, let’s go with word tokenization for simplicity. Our goal will be to translate “Hello World” from English to Spanish. Given an example “Hello World”, we’ll split into tokens: “Hello” and “World”. Each token has an associated ID defined in the model’s vocabulary. For example, “Hello” could be token 1 and “World” could be token 2.</p>
</section>
<section id="embedding-the-text">
<h3 data-anchor-id="embedding-the-text">1. Embedding the text</h3>
<p>Although we could pass the token IDs to the model (e.g. 1 and 2), these numbers don’t carry any meaning. We need to turn them into vectors (list of numbers). This is what <strong>embedding</strong> does! The token embeddings map a token ID to a fixed-size vector with some <strong>semantic meaning</strong> of the tokens**. These brings some interesting properties: similar tokens will have a similar embedding (in other words, calculating the cosine similarity between two embeddings will give us a good idea of how similar the tokens are).</p>
<p>Note that the mapping from a token to an embedding is learned. Although we could use a pre-trained embedding such as word2vec or GloVe, transformers models learn these embeddings as part of their training. This is a big advantage as the model can learn the best representation of the tokens for the task at hand. For example, the model could learn that “dog” and “dogs” should have similar embeddings.</p>
<p>All embeddings in a single model have the same size. The original transformer used a size of 512, but let’s do 4 for our example so we can keep the maths manageable. I’ll assign some random values to each token (as mentioned, this mapping is usually learned by the model).</p>
<p>Hello -&gt; [1,2,3,4]</p>
<p>World -&gt; [2,3,4,5]</p>
<div>

<div>
<p>After releasing this blog post, multiple persons raised questions about the embeddings above. I was a bit lazy and just wrote down some numbers that will make for some nice math below. In practice, these numbers would be learned by the model. I’ve updated the blog post to make this clearer. Thanks to everyone who raised this question!</p>
<p>We can estimate how similar these vectors are using cosine similarity, which would be too high for the vectors above. In practice, a vector would likely look something like [-0.071, 0.344, -0.12, 0.026, …, -0.008].</p>
</div>
</div>
<p>We can represent our input as a single matrix</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 3 &amp; 4 &amp; 5
\end{bmatrix}
\]</span></p>
<div>

<p>Although we could manage the two embeddings as separate vectors, it’s easier to manage them as a single matrix. This is because we’ll be doing matrix multiplications as we move forward!</p>
</div>
</section>
<section id="positional-encoding">
<h3 data-anchor-id="positional-encoding">2 Positional encoding</h3>
<p>The embedding above has no information about the position of the word in the sentence, so we need to feed some positional information. The way we do this is by adding a positional encoding to the embedding.</p>
<p>There are different choices on how to obtain these - we could use a learned embedding or a fixed vector. The original paper uses a fixed vector as they see almost no difference between the two approaches (see section 3.5 of the original paper). We’ll use a fixed vector as well. Sine and cosine functions have a wave-like pattern, and they repeat over time. By using these functions, <strong>each position in the sentence gets a unique</strong> yet consistent positional encoding. Given they repeat over time, it can help the model more easily learn patterns like proximity and distance between elements. These are the functions they use in the paper (section 3.5):</p>
<p><span>\[
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p><span>\[
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p>The idea is to interpolate between sine and cosine for each value in the embedding (even indices will use sine, odd indices will use cosine). Let’s calculate them for our example!</p>
<p>For “Hello”</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
<p>For “World”</p>
<ul>
<li>i = 0 (even): PE(1,0) = sin(1 / 10000^(0 / 4)) = sin(1 / 10000^0) = sin(1) ≈ 0.84</li>
<li>i = 1 (odd): PE(1,1) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^0.5) ≈ cos(0.01) ≈ 0.99</li>
<li>i = 2 (even): PE(1,2) = sin(1 / 10000^(2*2 / 4)) = sin(1 / 10000^1) ≈ 0</li>
<li>i = 3 (odd): PE(1,3) = cos(1 / 10000^(2*3 / 4)) = cos(1 / 10000^1.5) ≈ 1</li>
</ul>
<p>So concluding</p>
<ul>
<li>“Hello” -&gt; [0, 1, 0, 1]</li>
<li>“World” -&gt; [0.84, 0.99, 0, 1]</li>
</ul>
<p>Note that these encodings have the same dimension as the original embedding.</p>
<div>

<p>While we use sine and cosine as the original paper, there are other ways to do this. BERT, a very popular transformer, use trainable positional embeddings.</p>
</div>
</section>
<section id="add-positional-encoding-and-embedding">
<h3 data-anchor-id="add-positional-encoding-and-embedding">3. Add positional encoding and embedding</h3>
<p>We now add the positional encoding to the embedding. This is done by adding the two vectors together.</p>
<p>“Hello” = [1,2,3,4] + [0, 1, 0, 1] = [1, 3, 3, 5] “World” = [2,3,4,5] + [0.84, 0.99, 0, 1] = [2.84, 3.99, 4, 6]</p>
<p>So our new matrix, which will be the input to the encoder, is:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\]</span></p>
<p>If you look at the original paper’s image, what we just did is the bottom left part of the image (the embedding + positional encoding).</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"/></a></p>
<figcaption>Transformer model from the original “attention is all you need” paper</figcaption>
</figure>
</div>
</section>
<section id="self-attention">
<h3 data-anchor-id="self-attention">4. Self-attention</h3>
<section id="matrices-definition">
<h4 data-anchor-id="matrices-definition">4.1 Matrices Definition</h4>
<p>We’ll now introduce the concept of multi-head attention. Attention is a mechanism that allows the model to focus on certain parts of the input. Multi-head attention is a way to allow the model to jointly attend to information from different representation subspaces. This is done by using multiple attention heads. Each attention head will have its own K, V, and Q matrices.</p>
<p>Let’s use 2 attention heads for our example. We’ll use random values for these matrices. Each matrix will be a 4x3 matrix. With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. Note that using a too small attention size will hurt the performance of the model. Let’s use the following values (just random values):</p>
<p><strong>For the first head</strong></p>
<p><span>\[
\begin{align*}
WK1 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV1 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1  &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WQ1 &amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{bmatrix}
\end{align*}
\]</span></p>
<p><strong>For the second head</strong></p>
<p><span>\[
\begin{align*}
WK2 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}, \quad
WQ2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\end{align*}
\]</span></p>
</section>
<section id="keys-queries-and-values-calculation">
<h4 data-anchor-id="keys-queries-and-values-calculation">4.2 Keys, queries, and values calculation</h4>
<p>We now need to multiply our input embeddings with the weight matrices to obtain the keys, queries, and values.</p>
<p><strong>Key calculation</strong></p>
<p><span>\[
\begin{align*}
E \times WK1 &amp;= \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix} \\
&amp;= \begin{bmatrix}
(1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) &amp; (1 \times 0) + (3 \times 1) + (3 \times 0) + (5 \times 1) &amp; (1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) \\
(2.84 \times 1) + (3.99 \times 0) + (4 \times 1) + (6 \times 0) &amp; (2.84 \times 0) + (4 \times 1) + (4 \times 0) + (6 \times 1) &amp; (2.84 \times 1) + (4 \times 0) + (4 \times 1) + (6 \times 0)
\end{bmatrix} \\
&amp;= \begin{bmatrix}
4 &amp; 8 &amp; 4 \\
6.84 &amp; 9.99 &amp; 6.84
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Ok, I actually do not want to do the math by hand for all of these - it gets a bit repetitive plus it breaks the site. So let’s cheat and use NumPy to do the calculations for us.</p>
<p>We first define the matrices</p>
<div>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>import</span> numpy <span>as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>WK1 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>WV1 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>WQ1 <span>=</span> np.array([[<span>0</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>WK2 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>WV2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>WQ2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>]])</span></code></pre></div>
</div>
<p>And let’s confirm that I didn’t make any mistakes in the calculations above.</p>
<div>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>embedding <span>=</span> np.array([[<span>1</span>, <span>3</span>, <span>3</span>, <span>5</span>], [<span>2.84</span>, <span>3.99</span>, <span>4</span>, <span>6</span>]])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>K1 <span>=</span> embedding <span>@</span> WK1</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>K1</span></code></pre></div>
<div>
<pre><code>array([[4.  , 8.  , 4.  ],
       [6.84, 9.99, 6.84]])</code></pre>
</div>
</div>
<p>Phew! Let’s now get the values and queries</p>
<p><strong>Value calculations</strong></p>
<div>

<div>
<pre><code>array([[6.  , 6.  , 4.  ],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p><strong>Query calculations</strong></p>
<div>

<div>
<pre><code>array([[8.  , 3.  , 3.  ],
       [9.99, 3.99, 4.  ]])</code></pre>
</div>
</div>
<p>Let’s skip the second head for now and focus on the first head final score. We’ll come back to the second head later.</p>
</section>
<section id="attention-calculation">
<h4 data-anchor-id="attention-calculation">4.3 Attention calculation</h4>
<p>Calculating the attention score requires a couple of steps:</p>
<ol type="1">
<li>Calculate the dot product of the query with each key</li>
<li>Divide the result by the square root of the dimension of the key vector</li>
<li>Apply a softmax function to obtain the attention weights</li>
<li>Multiply each value vector by the attention weights</li>
</ol>
<section id="dot-product-of-query-with-each-key">
<h5 data-anchor-id="dot-product-of-query-with-each-key">4.3.1 Dot product of query with each key</h5>
<p>The score for “Hello” requires calculating the dot product of q1 with each key vector (k1 and k2)</p>
<p><span>\[
\begin{align*}
q1 \cdot k1 &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \end{bmatrix} \cdot \begin{bmatrix} 4 \\ 8 \\ 4 \end{bmatrix} \\
&amp;= 8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 \\
&amp;= 68
\end{align*}
\]</span></p>
<p>In matrix world, that would be Q1 multiplied by the transpose of K1</p>
<p><span>\[\begin{align*}
Q1 \times K1^\top &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \\ 9.99 &amp; 3.99 &amp; 4 \end{bmatrix} \times \begin{bmatrix} 4 &amp; 6.84 \\ 8 &amp; 9.99 \\ 4 &amp; 6.84 \end{bmatrix} \\
&amp;= \begin{bmatrix}
    8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 &amp; 8 \cdot 6.84 + 3 \cdot 9.99 + 3 \cdot 6.84 \\
    9.99 \cdot 4 + 3.99 \cdot 8 + 4 \cdot 4 &amp; 9.99 \cdot 6.84 + 3.99 \cdot 9.99 + 4 \cdot 6.84
    \end{bmatrix} \\
&amp;= \begin{bmatrix}
    68 &amp; 105.21 \\
    87.88 &amp; 135.5517
    \end{bmatrix}
\end{align*}\]</span></p>
<p>I’m prone to do mistakes, so let’s confirm with Python once again</p>
<div>
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>scores1 <span>=</span> Q1 <span>@</span> K1.T</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>scores1</span></code></pre></div>
<div>
<pre><code>array([[ 68.    , 105.21  ],
       [ 87.88  , 135.5517]])</code></pre>
</div>
</div>
</section>
<section id="divide-by-square-root-of-dimension-of-key-vector">
<h5 data-anchor-id="divide-by-square-root-of-dimension-of-key-vector">4.3.2 Divide by square root of dimension of key vector</h5>
<p>We then divide the scores by the square root of the dimension (d) of the keys (3 in this case, but 64 in the original paper). Why? For large values of d, the dot product grows too large (we’re adding the multiplication of a bunch of numbers, after all, leading to high values). And large values are bad! We’ll discuss soon more about this.</p>
<div>
<div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>scores1 <span>=</span> scores1 <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>scores1</span></code></pre></div>
<div>
<pre><code>array([[39.2598183 , 60.74302182],
       [50.73754166, 78.26081048]])</code></pre>
</div>
</div>
</section>
<section id="apply-softmax-function">
<h5 data-anchor-id="apply-softmax-function">4.3.3 Apply softmax function</h5>
<p>We then softmax to normalize so they are all positive and add up to 1.</p>
<div title="What is softmax?">

<div>
<p>Softmax is a function that takes a vector of values and returns a vector of values between 0 and 1, where the sum of the values is 1. It’s a nice way of obtaining probabilities. It’s defined as follows:</p>
<p><span>\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
\]</span></p>
<p>Don’t be intimidated by the formula - it’s actually quite simple. Let’s say we have the following vector:</p>
<p><span>\[
x = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}
\]</span></p>
<p>The softmax of this vector would be:</p>
<p><span>\[
\text{softmax}(x) = \begin{bmatrix} \frac{e^1}{e^1 + e^2 + e^3} &amp; \frac{e^2}{e^1 + e^2 + e^3} &amp; \frac{e^3}{e^1 + e^2 + e^3} \end{bmatrix} = \begin{bmatrix} 0.09 &amp; 0.24 &amp; 0.67 \end{bmatrix}
\]</span></p>
<p>As you can see, the values are all positive and add up to 1.</p>
</div>
</div>
<div>
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span>def</span> softmax(x):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.exp(x) <span>/</span> np.<span>sum</span>(np.exp(x), axis<span>=</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>scores1 <span>=</span> softmax(scores1)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>scores1</span></code></pre></div>
<div>
<pre><code>array([[4.67695573e-10, 1.00000000e+00],
       [1.11377182e-12, 1.00000000e+00]])</code></pre>
</div>
</div>
</section>
<section id="multiply-value-matrix-by-attention-weights">
<h5 data-anchor-id="multiply-value-matrix-by-attention-weights">4.3.4 Multiply value matrix by attention weights</h5>
<p>We then multiply times the value matrix</p>
<div>
<div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>attention1 <span>=</span> scores1 <span>@</span> V1</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>attention1</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>Let’s combine 4.3.1, 4.3.2, 4.3.3, and 4.3.4 into a single formula using matrices (this is from section 3.2.1 of the original paper):</p>
<p><span>\[
Attention(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
\]</span></p>
<p>Yes, that’s it! All the math we just did can easily be encapsulated in the attention formula above! Let’s now translate this to code!</p>
<div>
<div id="cb16"><pre><code><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    K <span>=</span> x <span>@</span> WK</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    V <span>=</span> x <span>@</span> WV</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> softmax(scores)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span>return</span> scores</span></code></pre></div>
</div>
<div>
<div id="cb17"><pre><code><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>attention(embedding, WQ1, WK1, WV1)</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>We confirm we got same values as above. Let’s chear and use this to obtain the attention scores the second attention head:</p>
<div>
<div id="cb19"><pre><code><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>attention2</span></code></pre></div>
<div>
<pre><code>array([[8.84, 3.99, 7.99],
       [8.84, 3.99, 7.99]])</code></pre>
</div>
</div>
<p>If you’re wondering how come the attention is the same for the two embeddings, it’s because the softmax is taking our scores to 0 and 1. See this:</p>
<div>
<div id="cb21"><pre><code><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>softmax(((embedding <span>@</span> WQ2) <span>@</span> (embedding <span>@</span> WK2).T) <span>/</span> np.sqrt(<span>3</span>))</span></code></pre></div>
<div>
<pre><code>array([[1.10613872e-14, 1.00000000e+00],
       [4.95934510e-20, 1.00000000e+00]])</code></pre>
</div>
</div>
<p>This is due to bad initialization of the matrices and small vector sizes. Large differences in the scores before applying softmax will just be amplified with softmax, leading to one value being close to 1 and others close to 0. In practice, our initial embedding matrices’ values were maybe too high, leading to high values for the keys, values, and queries, which just grew larger as we multiplied them.</p>
<p>Remember when we were dividing by the square root of the dimension of the keys? This is why we do that. If we don’t do that, the values of the dot product will be too large, leading to large values after the softmax. In this case, though, it seems it wasn’t enough given our small values! As a short-term hack, we can scale down the values by a larger amount than the square root of 3. Let’s redefine the attention function but scaling down by 30. This is not a good long-term solution, but it will help us get different values for the attention scores. We’ll get back to a better solution later.</p>
<div>
<div id="cb23"><pre><code><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    K <span>=</span> x <span>@</span> WK</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    V <span>=</span> x <span>@</span> WV</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>/</span> <span>30</span>  <span># we just changed this</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> softmax(scores)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span>return</span> scores</span></code></pre></div>
</div>
<div>
<div id="cb24"><pre><code><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>attention1 <span>=</span> attention(embedding, WQ1, WK1, WV1)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>attention1</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657],
       [7.65266185, 8.35857269, 6.35857269]])</code></pre>
</div>
</div>
<div>
<div id="cb26"><pre><code><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>attention2</span></code></pre></div>
<div>
<pre><code>array([[8.45589591, 3.85610456, 7.72085664],
       [8.63740591, 3.91937741, 7.84804146]])</code></pre>
</div>
</div>
</section>
<section id="heads-attention-output">
<h5 data-anchor-id="heads-attention-output">4.3.5 Heads’ attention output</h5>
<p>The next layer of the encoder will expect a single matrix, not two. The first step will be to concatenate the two heads’ outputs (section 3.2.2 of the original paper)</p>
<div>
<div id="cb28"><pre><code><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>attentions <span>=</span> np.concatenate([attention1, attention2], axis<span>=</span><span>1</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>attentions</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657, 8.45589591, 3.85610456,
        7.72085664],
       [7.65266185, 8.35857269, 6.35857269, 8.63740591, 3.91937741,
        7.84804146]])</code></pre>
</div>
</div>
<p>We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. This weight matrix is also learned! The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case).</p>
<div>
<div id="cb30"><pre><code><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span># Just some random values</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>W <span>=</span> np.array(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        [<span>0.79445237</span>, <span>0.1081456</span>, <span>0.27411536</span>, <span>0.78394531</span>],</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        [<span>0.29081936</span>, <span>-</span><span>0.36187258</span>, <span>-</span><span>0.32312791</span>, <span>-</span><span>0.48530339</span>],</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        [<span>-</span><span>0.36702934</span>, <span>-</span><span>0.76471963</span>, <span>-</span><span>0.88058366</span>, <span>-</span><span>1.73713022</span>],</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        [<span>-</span><span>0.02305587</span>, <span>-</span><span>0.64315981</span>, <span>-</span><span>0.68306653</span>, <span>-</span><span>1.25393866</span>],</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        [<span>0.29077448</span>, <span>-</span><span>0.04121674</span>, <span>0.01509932</span>, <span>0.13149906</span>],</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        [<span>0.57451867</span>, <span>-</span><span>0.08895355</span>, <span>0.02190485</span>, <span>0.24535932</span>],</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>Z <span>=</span> attentions <span>@</span> W</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>Z</span></code></pre></div>
<div>
<pre><code>array([[ 11.46394285, -13.18016471, -11.59340253, -17.04387829],
       [ 11.62608573, -13.47454936, -11.87126395, -17.4926367 ]])</code></pre>
</div>
</div>
<p>The image from <a href="https://jalammar.github.io/illustrated-transformer/">The Ilustrated Transformer</a> encapsulates all of this in a single image <img src="http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="Attention"/></p>
</section>
</section>
</section>
<section id="feed-forward-layer">
<h3 data-anchor-id="feed-forward-layer">5. Feed-forward layer</h3>
<section id="basic-feed-forward-layer">
<h4 data-anchor-id="basic-feed-forward-layer">5.1 Basic feed-forward layer</h4>
<p>After the self-attention layer, the encoder has a feed-forward neural network (FFN). This is a simple network with two linear transformations and a ReLU activation in between. The Illustrated Transformer blog post does not dive into it, so let me briefly explain a bit more. The goal of the FFN is to process and transformer the representation produced by the attention mechanism. The flow is usually as follows (see section 3.3 of the original paper):</p>
<ol type="1">
<li><strong>First linear layer:</strong> this usually expands the dimensionality of the input. For example, if the input dimension is 512, the output dimension might be 2048. This is done to allow the model to learn more complex functions. In our simple of example with dimension of 4, we’ll expand to 8.</li>
<li><strong>ReLU activation:</strong> This is a non-linear activation function. It’s a simple function that returns 0 if the input is negative, and the input if it’s positive. This allows the model to learn non-linear functions. The math is as follows:</li>
</ol>
<p><span>\[
\text{ReLU}(x) = \max(0, x)
\]</span></p>
<ol start="3" type="1">
<li><strong>Second linear layer:</strong> This is the opposite of the first linear layer. It reduces the dimensionality back to the original dimension. In our example, we’ll reduce from 8 to 4.</li>
</ol>
<p>We can represent all of this as follows</p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p>Just as a reminder, the input for this layer is the Z we calculated in the self-attention above. Here are the values as a reminder</p>
<p><span>\[
Z =
\begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix}
\]</span></p>
<p>Let’s now define some random values for the weight matrices and bias vectors. I’ll do it with code, but you can do it by hand if you feel patient!</p>
<div>
<div id="cb32"><pre><code><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>b2 <span>=</span> np.random.randn(<span>4</span>)</span></code></pre></div>
</div>
<p>And now let’s write the forward pass function</p>
<div>
<div id="cb33"><pre><code><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span>def</span> relu(x):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.maximum(<span>0</span>, x)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span></code></pre></div>
</div>
<div>
<div id="cb34"><pre><code><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>output_encoder <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>output_encoder</span></code></pre></div>
<div>
<pre><code>array([[ -3.24115016,  -9.7901049 , -29.42555675, -19.93135286],
       [ -3.40199463,  -9.87245924, -30.05715408, -20.05271018]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-encoder">
<h4 data-anchor-id="encapsulating-everything-the-random-encoder">5.2 Encapsulating everything: The Random Encoder</h4>
<p>Let’s now write some code to have the multi-head attention and the feed-forward, all together in the encoder block.</p>
<div>

<p>The code optimizes for understanding and educational purposes, not for performance! Don’t judge too hard!</p>
</div>
<div>
<div id="cb36"><pre><code><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>d_embedding <span>=</span> <span>4</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    K <span>=</span> x <span>@</span> WK</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    V <span>=</span> x <span>@</span> WV</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> softmax(scores)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span>return</span> scores</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span>def</span> multi_head_attention(x, WQs, WKs, WVs):</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    attentions <span>=</span> np.concatenate(</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        [attention(x, WQ, WK, WV) <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)], axis<span>=</span><span>1</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span>return</span> attentions <span>@</span> W</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Z</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span>def</span> random_encoder_block(x):</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>    WQs <span>=</span> [</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>    WKs <span>=</span> [</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    WVs <span>=</span> [</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>    <span>return</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)</span></code></pre></div>
</div>
<p>Recall that our input is the matrix E which has the positional encoding and the embedding.</p>
<div>

<div>
<pre><code>array([[1.  , 3.  , 3.  , 5.  ],
       [2.84, 3.99, 4.  , 6.  ]])</code></pre>
</div>
</div>
<p>Let’s now pass this to our <code>random_encoder_block</code> function</p>
<div>
<div id="cb39"><pre><code><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>random_encoder_block(embedding)</span></code></pre></div>
<div>
<pre><code>array([[ -71.76537515, -131.43316885,   13.2938131 ,   -4.26831998],
       [ -72.04253781, -131.84091347,   13.3385937 ,   -4.32872015]])</code></pre>
</div>
</div>
<p>Nice! This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:</p>
<div>
<div id="cb41"><pre><code><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        x <span>=</span> random_encoder_block(x)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span>return</span> x</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>encoder(embedding)</span></code></pre></div>
<div>
<pre><code>/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: overflow encountered in exp
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)
/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: invalid value encountered in divide
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)</code></pre>
</div>
<div>
<pre><code>array([[nan, nan, nan, nan],
       [nan, nan, nan, nan]])</code></pre>
</div>
</div>
</section>
<section id="residual-and-layer-normalization">
<h4 data-anchor-id="residual-and-layer-normalization">5.3 Residual and Layer Normalization</h4>
<p>Uh oh! We’re getting NaNs! It seems our values are too high, and when being passed to the next encoder, they end up being too high and exploding! This issue of having values that are too high is a common issue when training models. For example, when doing the backpropagation (the technique through which the models learn), the gradients can become too large and end up exploding; this is called <strong>gradient explosion</strong>. Without any kind of normalization, small changes in the input of early layers end up being amplified in later layers. This is a common problem in deep neural networks. There are two common techniques to mitigate this problem: residual connections and layer normalization (section 3.1 of the paper, barely mentioned).</p>
<ul>
<li><strong>Residual connections:</strong> Residual connections are simply adding the input of the layer to it output. For example, we add the initial embedding to the output of the attention. Residual connections mitigate the vanishing gradient problem. The intuition is that if the gradient is too small, we can just add the input to the output and the gradient will be larger. The math is very simple:</li>
</ul>
<p><span>\[
\text{Residual}(x) = x + \text{Layer}(x)
\]</span></p>
<p>That’s it! We’ll do this to the output of the attention and the output of the feed-forward layer.</p>
<ul>
<li><strong>Layer normalization</strong> Layer normalization is a technique to normalize the inputs of a layer. It normalizes across the embedding dimension. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. This helps with the gradient flow. The math does not look so simple at a first glance.</li>
</ul>
<p><span>\[
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \times \gamma + \beta
\]</span></p>
<p>Let’s explain each parameter:</p>
<ul>
<li><span>\(\mu\)</span> is the mean of the embedding</li>
<li><span>\(\sigma\)</span> is the standard deviation of the embedding</li>
<li><span>\(\epsilon\)</span> is a small number to avoid division by zero. In case the standard deviation is 0, this small epsilon saves the day!</li>
<li><span>\(\gamma\)</span> and <span>\(\beta\)</span> are learned parameters that control scaling and shifting steps.</li>
</ul>
<p>Unlike batch normalization (no worries if you don’t know what it is), layer normalization normalizes across the embedding dimension - that means that each embedding will not be affected by other samples in the batch. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1.</p>
<p>Why do we add the learnable parameters <span>\(\gamma\)</span> and <span>\(\beta\)</span>? The reason is that we don’t want to lose the representational power of the layer. If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values.</p>
<p>Combining the equations, the equation for the whole encoder could look like this</p>
<p><span>\[
\text{Z}(x) = \text{LayerNorm}(x + \text{Attention}(x))
\]</span></p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p><span>\[
\text{Encoder}(x) = \text{LayerNorm}(Z(x) + \text{FFN}(Z(x) + x))
\]</span></p>
<p>Let’s try with our example! Let’s go with E and Z values from before</p>
<p><span>\[
\begin{align*}
\text{E} + \text{Attention(E)} &amp;= \begin{bmatrix}
1.0 &amp; 3.0 &amp; 3.0 &amp; 5.0 \\
2.84 &amp; 3.99 &amp; 4.0 &amp; 6.0
\end{bmatrix} + \begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix} \\
&amp;= \begin{bmatrix}
12.46394281 &amp; -10.18016469 &amp; -8.59340253 &amp; -12.04387833 \\
14.46608569 &amp; -9.48454934 &amp; -7.87126395 &amp; -11.49263674
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Let’s now calculate the layer normalization, we can divide it into three steps:</p>
<ol type="1">
<li>Compute mean and variance for each embedding.</li>
<li>Normalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).</li>
<li>Scale and shift by multiplying by gamma and adding beta.</li>
</ol>
<section id="mean-and-variance">
<h5 data-anchor-id="mean-and-variance">5.3.1 Mean and variance</h5>
<p>For the first embedding</p>
<p><span>\[
\begin{align*}
\mu_1 &amp;= \frac{12.46394281-10.18016469-8.59340253-12.04387833}{4} = -4.58837568 \\
\sigma^2 &amp;= \frac{\sum (x_i - \mu)^2}{N} \\
&amp;= \frac{(12.46394281 - (-4.588375685))^2 + \ldots + (-12.04387833 - (-4.588375685))^2}{4} \\
&amp;= \frac{393.67443005013}{4} \\
&amp;= 98.418607512533 \\
\sigma &amp;= \sqrt{98.418607512533} \\
&amp;= 9.9206152789297
\end{align*}
\]</span></p>
<p>We can do the same for the second embedding. We’ll skip the calculations but you get the hang of it.</p>
<p><span>\[
\begin{align*}
\mu_2 &amp;= -3.59559109 \\
\sigma_2 &amp;= 10.50653018
\end{align*}
\]</span></p>
<p>Let’s confirm with Python</p>
<div>
<div id="cb44"><pre><code><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>(embedding <span>+</span> Z).mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[-4.58837567],
       [-3.59559107]])</code></pre>
</div>
</div>
<div>
<div id="cb46"><pre><code><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>(embedding <span>+</span> Z).std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[ 9.92061529],
       [10.50653019]])</code></pre>
</div>
</div>
<p>Amazing! Let’s now normalize</p>
</section>
<section id="normalize">
<h5 data-anchor-id="normalize">5.3.2 Normalize</h5>
<p>For normalization, for each value in the embedding, we subsctract the mean and divide by the standard deviation. Epsilon is a very small value, such as 0.00001. We’ll assume <span>\(\gamma=1\)</span> and <span>\(\beta=0\)</span>, it simplifies things.</p>
<p><span>\[\begin{align*}
\text{normalized}_1 &amp;= \frac{12.46394281 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{17.05231849}{9.9206152789297} \\
&amp;= 1.718 \\
\text{normalized}_2 &amp;= \frac{-10.18016469 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-5.59178901}{9.9206152789297} \\
&amp;= -0.564 \\
\text{normalized}_3 &amp;= \frac{-8.59340253 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-4.00502685}{9.9206152789297} \\
&amp;= -0.404 \\
\text{normalized}_4 &amp;= \frac{-12.04387833 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-7.45550265}{9.9206152789297} \\
&amp;= -0.752
\end{align*}\]</span></p>
<p>We’ll skip the calculations by hand for the second embedding. Let’s confirm with code! Let’s re-define our <code>encoder_block</code> function with this change</p>
<div>
<div id="cb48"><pre><code><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span>def</span> layer_norm(x, epsilon<span>=</span><span>1e-6</span>):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    mean <span>=</span> x.mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    std <span>=</span> x.std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span>return</span> (x <span>-</span> mean) <span>/</span> (std <span>+</span> epsilon)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    output <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    <span>return</span> layer_norm(output <span>+</span> Z)</span></code></pre></div>
</div>
<div>
<div id="cb49"><pre><code><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>layer_norm(Z <span>+</span> embedding)</span></code></pre></div>
<div>
<pre><code>array([[ 1.71887693, -0.56365339, -0.40370747, -0.75151608],
       [ 1.71909039, -0.56050453, -0.40695381, -0.75163205]])</code></pre>
</div>
</div>
<p>It works! Let’s retry to pass the embedding through the six encoders.</p>
<div>
<div id="cb51"><pre><code><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>        x <span>=</span> random_encoder_block(x)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span>return</span> x</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>encoder(embedding)</span></code></pre></div>
<div>
<pre><code>array([[-0.335849  , -1.44504571,  1.21698183,  0.56391289],
       [-0.33583947, -1.44504861,  1.21698606,  0.56390202]])</code></pre>
</div>
</div>
<p>Amazing! These values make sense and we don’t get NaNs! The idea of the stack of encoders is that they output a continuous representation, z, that captures the meaning of the input sequence. This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time.</p>
<p>Before diving into the decoder, here’s an image from Jay’s amazing blog post:</p>
<div>
<figure>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"/></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
<p>You should be able to explain each component at the left side! Quite impressive, right? Let’s now move to the decoder.</p>
</section>
</section>
</section>
</section>
<section id="decoder">
<h2 data-anchor-id="decoder">Decoder</h2>
<p>Most of the thing we learned for encoders will be used in the decoder as well! The decoder has two self-attention layers, one for the encoder and one for the decoder. The decoder also has a feed-forward layer. Let’s go through each of these.</p>
<p>The decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!</p>
<p>Given the embedding generated by the encoder and the SOS token, the decoder will then generate the next token of the sequence, e.g. “hola”. The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.</p>
<ul>
<li>Iteration 1: Input is SOS, output is “hola”</li>
<li>Iteration 2: Input is SOS + “hola”, output is “mundo”</li>
<li>Iteration 3: Input is SOS + “hola” + “mundo”, output is EOS</li>
</ul>
<p>Here, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder.</p>
<div>

<p><strong>This autoregressive design makes decoder slow.</strong> The encoder is able to generate its embedding in a single forward pass while the decoder needs to do many forward passes. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).</p>
</div>
<p>Let’s dive into each step! Just as the encoder, the decoder is composed of a stack of decoder blocks. The decoder block is a bit more complex than the encoder block. The general structure is:</p>
<ol type="1">
<li>(Masked) Self-attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Encoder-decoder attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Feed-forward layer</li>
<li>Residual connection and layer normalization</li>
</ol>
<p>We’re already familiar with all the math from 1, 2, 3, 5 and 6. See the right side of the image below, you’ll see that all these blocks you already know (the right part):</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"/></a></p>
<figcaption>Transformer model from the original “attention is all you need” paper</figcaption>
</figure>
</div>
<section id="embedding-the-text-1">
<h3 data-anchor-id="embedding-the-text-1">1. Embedding the text</h3>
<p>The first text of the decoder is to embed the input tokens. The input token is <code>SOS</code>, so we’ll embed it. We’ll use the same embedding dimension as the encoder. Let’s assume the embedding vector is the following:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
\]</span></p>
</section>
<section id="positional-encoding-1">
<h3 data-anchor-id="positional-encoding-1">2. Positional encoding</h3>
<p>We’ll now add the positional encoding to the embedding, just as we did for the encoder. Given it’s the same position as “Hello”, we’ll have same positional encoding as we did before:</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
</section>
<section id="add-positional-encoding-and-embedding-1">
<h3 data-anchor-id="add-positional-encoding-and-embedding-1">3. Add positional encoding and embedding</h3>
<p>Adding the positional encoding to the embedding is done by adding the two vectors together:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
</section>
<section id="self-attention-1">
<h3 data-anchor-id="self-attention-1">4. Self-attention</h3>
<p>The first step within the decoder block is the self-attention mechanism. Luckily, we have some code for this and can just use it!</p>
<div>
<div id="cb53"><pre><code><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>d_embedding <span>=</span> <span>4</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>E <span>=</span> np.array([[<span>1</span>, <span>1</span>, <span>0</span>, <span>1</span>]])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>Z_self_attention <span>=</span> multi_head_attention(E, WQs, WKs, WVs)</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 2.19334924, 10.61851198, -4.50089666, -2.76366551]])</code></pre>
</div>
</div>
<div>

<div>
<p>Things are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder’s goal is to capture all information of the input, the decoder’s goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).</p>
<p>Because of this, we use masked self-attention: we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1). We’ll skip this for now, but it’s important to keep in mind that the decoder is a bit more complex during training.</p>
</div>
</div>
</section>
<section id="residual-connection-and-layer-normalization">
<h3 data-anchor-id="residual-connection-and-layer-normalization">5. Residual connection and layer normalization</h3>
<p>Nothing magical here, we just add the input to the output of the self-attention and apply layer normalization. We’ll use the same code as before.</p>
<div>
<div id="cb55"><pre><code><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>Z_self_attention <span>=</span> layer_norm(Z_self_attention <span>+</span> E)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 0.17236212,  1.54684892, -1.0828824 , -0.63632864]])</code></pre>
</div>
</div>
</section>
<section id="encoder-decoder-attention">
<h3 data-anchor-id="encoder-decoder-attention">6. Encoder-decoder attention</h3>
<p><strong>This part is the new one!</strong> If you were wondering where do the encoder-generated embeddings come in, this is their moment to shine!</p>
<p>Let’s assume the output of the encoder is the following matrix</p>
<p><span>\[
\begin{bmatrix}
-1.5 &amp; 1.0 &amp; -0.8 &amp; 1.5 \\
1.0 &amp; -1.0 &amp; -0.5 &amp; 1.0
\end{bmatrix}
\]</span></p>
<p>In the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.</p>
<p>In the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let’s look at some code</p>
<div>
<div id="cb57"><pre><code><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span>def</span> encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span># The next three lines are the key difference!</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    K <span>=</span> encoder_output <span>@</span> WK    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    V <span>=</span> encoder_output <span>@</span> WV    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    Q <span>=</span> attention_input <span>@</span> WQ   <span># Same as self-attention</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span># This stays the same</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> softmax(scores)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span>return</span> scores</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span>def</span> multi_head_encoder_decoder_attention(</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>    encoder_output, attention_input, WQs, WKs, WVs</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>    attentions <span>=</span> np.concatenate(</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>            encoder_decoder_attention(</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>                encoder_output, attention_input, WQ, WK, WV</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>            <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)</span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>        axis<span>=</span><span>1</span>,</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>    <span>return</span> attentions <span>@</span> W</span></code></pre></div>
</div>
<div>
<div id="cb58"><pre><code><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    encoder_output, Z_self_attention, WQs, WKs, WVs</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[ 1.57651431,  4.92489307, -0.08644448, -0.46776051]])</code></pre>
</div>
</div>
<p>This worked! You might be asking “why do we do this?”. The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., “hello world”). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens. This is a very powerful mechanism!</p>
</section>
<section id="residual-connection-and-layer-normalization-1">
<h3 data-anchor-id="residual-connection-and-layer-normalization-1">7. Residual connection and layer normalization</h3>
<p>Same as before!</p>
<div>
<div id="cb60"><pre><code><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z_self_attention)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[-0.44406723,  1.6552893 , -0.19984632, -1.01137575]])</code></pre>
</div>
</div>
</section>
<section id="feed-forward-layer-1">
<h3 data-anchor-id="feed-forward-layer-1">8. Feed-forward layer</h3>
<p>Once again, same as before! I’ll also do the residual connection and layer normalization after it.</p>
<div>
<div id="cb62"><pre><code><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>b2 <span>=</span> np.random.randn(<span>4</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>output <span>=</span> layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) <span>+</span> Z_encoder_decoder)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>output</span></code></pre></div>
<div>
<pre><code>array([[-0.97650182,  0.81470137, -2.79122044, -3.39192873]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-decoder">
<h3 data-anchor-id="encapsulating-everything-the-random-decoder">9. Encapsulating everything: The Random Decoder</h3>
<p>Let’s write the code for a single decoder block. The main change is that we now have an additional attention mechanism.</p>
<div>
<div id="cb64"><pre><code><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>d_embedding <span>=</span> <span>4</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span>def</span> decoder_block(</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    x,</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>    encoder_output,</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>    W1, b1, W2, b2,</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>    <span># Same as before</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> multi_head_attention(</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>        x, WQs_self_attention, WKs_self_attention, WVs_self_attention</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    <span># The next three lines are the key difference!</span></span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z)</span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>    <span># Same as before</span></span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a>    output <span>=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2)</span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>    <span>return</span> layer_norm(output <span>+</span> Z_encoder_decoder)</span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a><span>def</span> random_decoder_block(x, encoder_output):</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>    <span># Just a bunch of random initializations</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>    WQs_self_attention <span>=</span> [</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>    WKs_self_attention <span>=</span> [</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a>    WVs_self_attention <span>=</span> [</span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-42"><a href="#cb64-42" aria-hidden="true" tabindex="-1"></a>    WQs_ed_attention <span>=</span> [</span>
<span id="cb64-43"><a href="#cb64-43" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-44"><a href="#cb64-44" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-45"><a href="#cb64-45" aria-hidden="true" tabindex="-1"></a>    WKs_ed_attention <span>=</span> [</span>
<span id="cb64-46"><a href="#cb64-46" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-47"><a href="#cb64-47" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-48"><a href="#cb64-48" aria-hidden="true" tabindex="-1"></a>    WVs_ed_attention <span>=</span> [</span>
<span id="cb64-49"><a href="#cb64-49" aria-hidden="true" tabindex="-1"></a>        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-50"><a href="#cb64-50" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb64-51"><a href="#cb64-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-52"><a href="#cb64-52" aria-hidden="true" tabindex="-1"></a>    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb64-53"><a href="#cb64-53" aria-hidden="true" tabindex="-1"></a>    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb64-54"><a href="#cb64-54" aria-hidden="true" tabindex="-1"></a>    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb64-55"><a href="#cb64-55" aria-hidden="true" tabindex="-1"></a>    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb64-56"><a href="#cb64-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-57"><a href="#cb64-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-58"><a href="#cb64-58" aria-hidden="true" tabindex="-1"></a>    <span>return</span> decoder_block(</span>
<span id="cb64-59"><a href="#cb64-59" aria-hidden="true" tabindex="-1"></a>        x, encoder_output,</span>
<span id="cb64-60"><a href="#cb64-60" aria-hidden="true" tabindex="-1"></a>        WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-61"><a href="#cb64-61" aria-hidden="true" tabindex="-1"></a>        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-62"><a href="#cb64-62" aria-hidden="true" tabindex="-1"></a>        W1, b1, W2, b2,</span>
<span id="cb64-63"><a href="#cb64-63" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<div>
<div id="cb65"><pre><code><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span>def</span> decoder(x, decoder_embedding, n<span>=</span><span>6</span>):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        x <span>=</span> random_decoder_block(x, decoder_embedding)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span>return</span> x</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>decoder(E, encoder_output)</span></code></pre></div>
<div>
<pre><code>array([[ 0.71866458, -1.72279956,  0.57735876,  0.42677623]])</code></pre>
</div>
</div>
</section>
</section>
<section id="generating-the-output-sequence">
<h2 data-anchor-id="generating-the-output-sequence">Generating the output sequence</h2>
<p>We have all the building blocks! Let’s now generate the output sequence.</p>
<ul>
<li>We have the <strong>encoder</strong>, which takes the input sequence and generates its rich representation. It’s composed of a stack of encoder blocks.</li>
<li>We have the <strong>decoder</strong>, which takes the encoder output and generated tokens, and generates the output sequence. It’s composed of a stack of decoder blocks.</li>
</ul>
<p>How do we go from the decoder’s output to a word? We need to add a final linear layer and a softmax layer on top of the decoder. The whole algorithm looks like this:</p>
<ol type="1">
<li>The encoder receives the input sequence and generates a representation of it.</li>
<li>The decoder begins with the SOS token and the encoder output. It generates the next token of the output sequence.</li>
<li>We then apply a linear layer to generate the logits.</li>
<li>We then apply a softmax layer to generate the probabilities.</li>
<li>The decoder uses the encoder output and the previously generated token to generate the next token of the output sequence.</li>
<li>We repeat steps 2-5 until we generate the EOS token.</li>
</ol>
<p>This is mentioned in the section 3.4 of the paper.</p>
<section id="linear-layer">
<h3 data-anchor-id="linear-layer">1. Linear layer</h3>
<p>The linear layer is a simple linear transformation. It takes the decoder’s output and transforms it into a vector of size <code>vocab_size</code>. This is the size of the vocabulary. For example, if we have a vocabulary of 10000 words, the linear layer will transform the decoder’s output into a vector of size 10000. This vector will contain the probability of each word being the next word in the sequence. For simplicity, let’s go with a vocabulary of 10 words and assume the first decoder output is a very simple vector: [1, 0, 1, 0]. We’ll use random weights and biases matrices of the size <code>vocab_size</code> x <code>decoder_output_size</code>.</p>
<div>
<div id="cb67"><pre><code><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span>def</span> linear(x, W, b):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span>return</span> np.dot(x, W) <span>+</span> b</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>x <span>=</span> linear([[<span>1</span>, <span>0</span>, <span>1</span>, <span>0</span>]], np.random.randn(<span>4</span>, <span>10</span>), np.random.randn(<span>10</span>))</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<div>
<pre><code>array([[-0.39929948,  0.96345013,  2.77090264,  0.25651866, -0.84738762,
        -1.67834992, -0.29583529, -3.55515281,  2.97453801, -1.10682376]])</code></pre>
</div>
</div>
</section>
<section id="softmax">
<h3 data-anchor-id="softmax">2. Softmax</h3>
<p>These are called logits but they are not easily interpretable. We need to apply a softmax function to obtain the probabilities.</p>
<div>

<div>
<pre><code>array([[0.01602618, 0.06261303, 0.38162024, 0.03087794, 0.0102383 ,
        0.00446011, 0.01777314, 0.00068275, 0.46780959, 0.00789871]])</code></pre>
</div>
</div>
<p>This is giving us probabilities! Let’a assume the vocabulary is the following:</p>
<p><span>\[
\text{vocab} = \begin{bmatrix}
\text{hello} &amp; \text{mundo} &amp; \text{world} &amp; \text{how} &amp; \text{?} &amp; \text{EOS} &amp; \text{SOS} &amp; \text{a} &amp; \text{hola} &amp; \text{c}
\end{bmatrix}
\]</span></p>
<p>The above tells us that the probabilities are</p>
<ul>
<li>hello: 0.01602618</li>
<li>mundo: 0.06261303</li>
<li>world: 0.38162024</li>
<li>how: 0.03087794</li>
<li>?: 0.0102383</li>
<li>EOS: 0.00446011</li>
<li>SOS: 0.01777314</li>
<li>a: 0.00068275</li>
<li>hola: 0.46780959</li>
<li>c: 0.00789871</li>
</ul>
<p>From these, the most likely next token is “hola”. Picking always the most likely token is called greedy decoding. This is not always the best approach, as it might lead to suboptimal results, but we won’t dive into generation techniques at the moment. If you want to learn more about it, check out this amazing <a href="https://huggingface.co/blog/how-to-generate">blog post</a>.</p>
</section>
<section id="the-random-encoder-decoder-transformer">
<h3 data-anchor-id="the-random-encoder-decoder-transformer">3. The Random Encoder-Decoder Transformer</h3>
<p>Let’s write the whole code for this! Let’s define a dictionary that maps the words to their initial embeddings. Note that this is also learned during training, but we’ll use random values for now.</p>
<div>
<div id="cb71"><pre><code><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>vocabulary <span>=</span> [</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;hello&#34;</span>,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    <span>&#34;mundo&#34;</span>,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    <span>&#34;world&#34;</span>,</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    <span>&#34;how&#34;</span>,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    <span>&#34;?&#34;</span>,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    <span>&#34;EOS&#34;</span>,</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    <span>&#34;SOS&#34;</span>,</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    <span>&#34;a&#34;</span>,</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span>&#34;hola&#34;</span>,</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>    <span>&#34;c&#34;</span>,</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>embedding_reps <span>=</span> np.random.randn(<span>10</span>, <span>1</span>, <span>4</span>)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>vocabulary_embeddings <span>=</span> {</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    word: embedding_reps[i] <span>for</span> i, word <span>in</span> <span>enumerate</span>(vocabulary)</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>vocabulary_embeddings</span></code></pre></div>
<div>
<pre><code>{&#39;hello&#39;: array([[-1.19489531, -1.08007463,  1.41277762,  0.72054139]]),
 &#39;mundo&#39;: array([[-0.70265064, -0.58361306, -1.7710761 ,  0.87478862]]),
 &#39;world&#39;: array([[ 0.52480342,  2.03519246, -0.45100608, -1.92472193]]),
 &#39;how&#39;: array([[-1.14693176, -1.55761929,  1.09607545, -0.21673596]]),
 &#39;?&#39;: array([[-0.23689522, -1.12496841, -0.03733462, -0.23477603]]),
 &#39;EOS&#39;: array([[ 0.5180958 , -0.39844119,  0.30004136,  0.03881324]]),
 &#39;SOS&#39;: array([[ 2.00439161,  2.19477149, -0.84901634, -0.89269937]]),
 &#39;a&#39;: array([[ 1.63558337, -1.2556952 ,  1.65365362,  0.87639945]]),
 &#39;hola&#39;: array([[-0.5805717 , -0.93861149,  1.06847734, -0.34408367]]),
 &#39;c&#39;: array([[-2.79741142,  0.70521986, -0.44929098, -1.66167776]])}</code></pre>
</div>
</div>
<p>And now let’s write our random <code>generate</code> method that generates tokens autorergressively.</p>
<div>
<div id="cb73"><pre><code><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span>def</span> generate(input_sequence, max_iters<span>=</span><span>10</span>):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    <span># We first encode the inputs into embeddings</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    <span># This skips the positional encoding step for simplicity</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    embedded_inputs <span>=</span> [</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        vocabulary_embeddings[token][<span>0</span>] <span>for</span> token <span>in</span> input_sequence</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Embedding representation (encoder input)&#34;</span>, embedded_inputs)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    <span># We then generate an embedding representation</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    encoder_output <span>=</span> encoder(embedded_inputs)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(<span>&#34;Embedding generated by encoder (encoder output)&#34;</span>, encoder_output)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>    <span># We initialize the decoder output with the embedding of the start token</span></span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    sequence <span>=</span> vocabulary_embeddings[<span>&#34;SOS&#34;</span>]</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>    output <span>=</span> <span>&#34;SOS&#34;</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>    <span># Random matrices for the linear layer</span></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>    W_linear <span>=</span> np.random.randn(d_embedding, <span>len</span>(vocabulary))</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>    b_linear <span>=</span> np.random.randn(<span>len</span>(vocabulary))</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>    <span># We limit number of decoding steps to avoid too long sequences without EOS</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>    <span>for</span> i <span>in</span> <span>range</span>(max_iters):</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        <span># Decoder step</span></span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        decoder_output <span>=</span> decoder(sequence, encoder_output)</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>        logits <span>=</span> linear(decoder_output, W_linear, b_linear)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>        probs <span>=</span> softmax(logits)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>        <span># We get the most likely next token</span></span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>        next_token <span>=</span> vocabulary[np.argmax(probs)]</span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a>        sequence <span>=</span> vocabulary_embeddings[next_token]</span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a>        output <span>+=</span> <span>&#34; &#34;</span> <span>+</span> next_token</span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>        <span>print</span>(</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a>            <span>&#34;Iteration&#34;</span>, i, </span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>            <span>&#34;next token&#34;</span>, next_token,</span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>            <span>&#34;with probability of&#34;</span>, np.<span>max</span>(probs),</span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a>        <span># If the next token is the end token, we return the sequence</span></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a>        <span>if</span> next_token <span>==</span> <span>&#34;EOS&#34;</span>:</span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a>            <span>return</span> output</span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a>    <span>return</span> output</span></code></pre></div>
</div>
<p>Let’s run this now!</p>
<div>
<div id="cb74"><pre><code><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>generate([<span>&#34;hello&#34;</span>, <span>&#34;world&#34;</span>])</span></code></pre></div>
<div>
<pre><code>Embedding representation (encoder input) [array([-1.19489531, -1.08007463,  1.41277762,  0.72054139]), array([ 0.52480342,  2.03519246, -0.45100608, -1.92472193])]
Embedding generated by encoder (encoder output) [[-0.15606365  0.90444064  0.82531037 -1.57368737]
 [-0.15606217  0.90443936  0.82531082 -1.57368802]]
Iteration 0 next token how with probability of 0.6265258176587956
Iteration 1 next token a with probability of 0.42708031743571
Iteration 2 next token c with probability of 0.44288777368698484</code></pre>
</div>

</div>
<p>Ok, so we got the tokens “how”, “a”, and “c”. This is not a good translation, but it’s expected! We only used random weights!</p>
<p>I suggest you to look again in detail at the whole encoder-decoder architecture from the original paper:</p>
<div>
<figure>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"/></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusions">
<h2 data-anchor-id="conclusions">Conclusions</h2>
<p>I hope that was fun and informational! We covered a lot of ground. Wait…was that it? And the answer is, mostly, yes! New transformer architectures add lots of tricks, but the core of the transformer is what we just covered. Depending on what task you want to solve, you can also only the encoder or the decoder. For example, for understanding-heavy tasks such as classification, you can use the encoder stack with a linear layer on top. For generation-heavy tasks such as translation, you can use the encoder and decoder stacks. And finally, for free generation, as in ChatGPT or Mistral, you can use only the decoder stack.</p>
<p>Of course, we also did lots of simplifications. Let’s briefly check which were the numbers in the original transformer paper:</p>
<ul>
<li>Embedding dimension: 512 (4 in our example)</li>
<li>Number of encoders: 6 (6 in our example)</li>
<li>Number of decoders: 6 (6 in our example)</li>
<li>Feed-forward dimension: 2048 (8 in our example)</li>
<li>Number of attention heads: 8 (2 in our example)</li>
<li>Attention dimension: 64 (3 in our example)</li>
</ul>
<p>We just covered lots of topics, but it’s quite interesting we can achieve impressive results by scaling up this math and doing smart training. We didn’t cover training in this blog post as the goal was to understand the math when using an existing model, but I hope this provided strong foundations for jumping into the training part. I hope you enjoyed this blog post!</p>
<p>You can also find a more formal document with the math in <a href="https://johnthickstun.com/docs/transformers.pdf">this PDF</a> (recommended by HackerNews folks).</p>
</section>
<section id="exercises">
<h2 data-anchor-id="exercises">Exercises</h2>
<p>Here are some exercises to practice your understanding of the transformer.</p>
<ol type="1">
<li>What is the purpose of the positional encoding?</li>
<li>How does self-attention and encoder-decoder attention differ?</li>
<li>What would happen if our attention dimension was too small? What about if it was too large?</li>
<li>Briefly describe the structure of a feed-forward layer.</li>
<li>Why is the decoder slower than the encoder?</li>
<li>What is the purpose of the residual connections and layer normalization?</li>
<li>How do we go from the decoder output to probabilities?</li>
<li>Why is picking the most likely next token every single time problematic?</li>
</ol>
</section>
<section id="resources">
<h2 data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">Hugging Face free NLP course</a></li>
</ul>


</section>

</main> <!-- /main -->


</div></div>
  </body>
</html>
