<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rfd.shared.oxide.computer/rfd/0026">Original</a>
    <h1>Why Oxide Chose Illumos</h1>
    
    <div id="readability-page-1" class="page"><div><p>Static metrics are valuable when engineers can determine interesting values to
collect and store in advance of a problem occurring.  Unfortunately, many of
the most pernicious software defects are not found or fixed by inspecting
existing metrics.</p><p>Complex defects are often not easily reproducible; their presentation may
depend on ambient and difficult to catalogue background activity that is unique
to a specific customer site at a specific time.  In order to chase these
defects to root cause and make a fix, we must be able to ask questions about
the dynamic state of the system and iteratively refine those questions as we
hone in on the problem.  New instrumentation should be able to be enabled on a
running production system without updating or even restarting the software.</p><p>Human intervention in systems can lead to unforced errors, and even the most
cautious engineers engaging in debugging exercises are not immune.  Our
instrumentation must be constructed to be safe to engage on a production system
without fear of a crash, or data corruption, or serious and widely felt
performance degradation.  When not engaged, the machinery for instrumentation
should have no impact on system operation or performance.  In the limit, it
should be safe to accidentally request the enabling of all possible system
instrumentation without prolonged deleterious effect; it is better to report
an error to the debugging engineer than to make the system unsafe.</p><p>Modern systems generally consist of a lot of software, performing operations at
a dizzying rate.  A naive firehose approach to system observability where
post-processing is used to locate the subset of interesting events is unlikely
to broadly useful.  In order to have minimal impact on system performance, and
to answer very specific questions, the instrumentation system should be able to
efficiently and safely filter and aggregate relevant events <em>in-situ</em>; that is,
to make the decision to record the event, and which data to record, at the time
of the event.</p><p>Our hypervisor and other critical services, are likely to be implemented in a
split between user processes and kernel modules.  We will need instrumentation
that can cleanly measure events that occur across more than one process, and
which extend into the kernel.</p><p>In general, we expect to able to gather at least the following information:</p><div data-lineno="767"><ul><li><p>scheduling events; e.g., when a thread or process goes to sleep and why, or
what caused a thread or process to wake up</p></li><li><p>timer-based profiling; e.g., to allow us to sample system state such as the
kernel or user stack at regular intervals, possibly with other constraints
such as limiting to a particular process</p></li><li><p>system call entry and return, both per-process and globally</p></li><li><p>kernel function entry and return, with guard rails that prevent the tracing
of functions that are not safe to trace</p></li><li><p>user process function entry and return, and breakpoint-style tracing of
specific instructions or specific offsets within a function</p></li><li><p>creation of new processes and threads, and termination (planned or
unplanned) of existing processes and threads</p></li><li><p>latency distributions of operations, some of which may be defined as
the composite of multiple system-level operations by the engineer</p></li></ul></div><p>In some cases it can also be desirable to allow instrumentation to take limited
mutating actions against the system, such as invoke a data collector program
once a particular sequence of trace events has been detected.  In order to aid
in the reproduction of races in multi-threaded software, it can be helpful to
be able to inject small delays (hundreds of microseconds) at specific moments
in order to widen a suspected race window.  Facilities that perturb the system
may present a trade-off in safety, and it’s possible we might want to be able
to restrict these separately from general instrumentation in customer
environments.</p><div><h5 data-sectnum="2.5.3.1."><a id="_comparison_dtrace_ebpf" aria-hidden="true"></a><a href="#_comparison_dtrace_ebpf">Comparison: DTrace &amp; eBPF<svg width="16" height="16"><use href="/build/_assets/sprite-KDLRSXCD.svg#link-16"></use></svg></a></h5><div><p>DTrace, as described in
<a href="https://www.usenix.org/legacy/event/usenix04/tech/general/full_papers/cantrill/cantrill_html/index.html">
the 2004 paper</a>, is a system for the dynamic instrumentation of production
systems.  A prominent goal in its construction is to be perfectly safe, and
over more than a decade of use on a wide variety of production workloads it has
proven to be sufficiently robust that engineers and operators need not worry
when enabling it on a critical system.  Many key design decisions stem from
safety as a core goal; e.g., the instruction set for the tracing virtual
machine allows no backwards branches, so infinite loops are impossible by
construction.</p><p>Joyent hosted arbitrary customer workloads within
<a href="https://illumos.org/man/5/zones">zones</a>, an isolation and virtualisation
technology similar in principle to FreeBSD jails or Docker containers.  DTrace
was sufficiently safe that access could be granted to customers to instrument
software running within their container, with only restricted visibility into
global system behaviour.  In addition to raw DTrace access, part of the <em>Cloud
Analytics</em> product was built on top of DTrace instrumentation.  This product
was able to collect statistics both from probes that fired in user applications,
and from the underlying operating system, aggregating them in a live graphical
view.  Finally, countless real production problems were solved by long-running
DTrace enablings distributed throughout the fleet, waiting to log data about
the events leading up to some critical fault, but without otherwise affecting
the regular operation of the system.</p><p>In the more distant past, DTrace was a critical underpinning of the Analytics
feature of the Fishworks appliance at Sun.  Analytics enabled customers to
drill down into fine detail while analysing the performance of the system,
providing more abstract control over DTrace enablings and presenting an
interactive graphical view of the resultant telemetry.</p><p>The Berkeley (née BSD) Packet Filter (BPF) was introduced in 1992, to provide a
safe virtual machine that could be included in the kernel for selective packet
capture.  By allowing the filtering engine to run safely in the kernel, the
performance overhead of copying every packet into a user address space for
filtering could be avoided.  It followed from similar approaches taken in
earlier systems.</p><p>In 2014, an <em>extended</em> BPF (eBPF) was introduced to the mainline Linux kernel
for a variety of uses.  In contrast to many prior approaches, the eBPF virtual
machine makes use of a just-in-time (JIT) compiler to convert eBPF instructions
into native program text as they are loaded into the kernel.  This choice
appears to be the result of an attempt to build one system for two masters:</p><div data-lineno="838"><ol><li><p>Adding new behaviours to the system, even in the data path, where
performance is of paramount performance and programs must run to completion for
system correctness even if they have an outsized impact on the rest of the
system; e.g.,</p><div data-lineno="842"><ul><li><p>filtering, configuring, or redirecting socket connections</p></li><li><p>classifying and shaping network traffic</p></li><li><p>system call security policy, resource, and quota management in cgroups</p></li><li><p>network encapsulation protocol implementation</p></li></ul></div></li><li><p>Tracing and performance measurement of the system; e.g., by allowing eBPF
programs to hook various trace points and events from the perf subsystem</p></li></ol></div><p>The first use case, extending the data path, requires high performance at all
costs.  Without low latency operations, eBPF would not be an attractive target
when implementing new network filtering or encapsulation facilities.  The
robustness and security of eBPF appear to depend fundamentally on a component
called the &#34;verifier&#34;, which scans the eBPF program upon load into the kernel.
The verifier attempts to determine (before execution) whether an eBPF program
will do anything unsafe, and seeks to ensure that it will terminate.  There
have been some serious vulnerabilities found in the verifier, and it is not
clear the extent to which it has been proven to work.  Indeed,
<a href="https://github.com/torvalds/linux/blob/master/kernel/bpf/verifier.c">
<code>kernel/<wbr/>bpf/<wbr/>verifier.c</code></a> is (according to <code>cloc</code>) eight thousand lines of
non-comment C code running in the kernel.
<a href="https://www.openwall.com/lists/oss-security/2020/03/30/3">CVE-2020-8835</a> from
earlier this year is one such example of a security issue in the verifier.</p><p>By contrast, DTrace has a more constrained programming model which has allowed
a more readily verified implementation.  A byte code interpreter is used, with
security checks directly at the site of operations like loads or stores that
allow a D program to impact or observe the rest of the system.  The instruction
format does not allow for backwards branches, so constraining the program
length has a concomitant impact on execution time and thus impact on the
broader system.  Each action is limited in the amount of work it can perform;
e.g., by caps on the number of bytes of string data that will be read or
copied, and by the overall principal buffer size.  Explicit choices have been
made to favour limiting system impact — one could not implement a reliable
auditing or accounting system in DTrace, as the system makes no guarantee that
an enabling won’t be thrown overboard to preserve the correct execution of the
system.</p><p>In addition to issues of implementation complexity and verifier tractability,
there is the small matter of binary size.  The <code>bpftrace</code> tool, analogous on
some level to <code>dtrace</code>, depends directly on the library form of BCC, Clang, and
LLVM.  This puts the directly linked text size (as visible via <code>ldd</code>) at around
160MB, which was more than half of the size of the entire SmartOS RAM disk.
This doesn’t account for other parts of those toolchains that generally come
along for the ride, or the debugging information that is often stripped from
binaries in desktop Linux distributions.  By contrast, <code>dtrace</code> and supporting
libraries run to around 11MB total including CTF.  In 2020, disks, memory, and
network bandwidth, are relatively cheap.  That said, in contexts within the
system where we choose to execute the OS from a pinned RAM image, program text
size may still be an issue.  Lighter debugging infrastructure is easier to
include in more contexts without other trade-offs.</p><p>Finally, the tools in the eBPF ecosystem continue to present a number of
opportunities for improvement with respect to user experience.  A relatively
easy task with DTrace is to trace all system calls being made on the system or
by a particular process, and to then aggregate them by system call type, or
obtain a distribution of the latency of each call, or some combination of those
and other things.  By contrast, on a modern Ubuntu system, repeated attempts to
do the same basic thing resulted in hitting any number of walls; e.g.,</p><div data-lineno="901"><ul><li><p>Probe names have not been selected to be terribly ergonomic; e.g.,
what would in DTrace be <code>syscall::read:entry</code>, where each of the
colon-separated portions of the tuple are available in separate variables, the
analogous probe available to <code>bpftrace</code> appears to be
<code>tracepoint:syscalls:sys_enter_read</code>.  There is only one variable, <code>probe</code>,
which then contains the whole string.  Useful output appears likely to require
post-processing for even simple tracing activities.</p></li><li><p>When trying to enable all <code>tracepoint:syscalls:sys_*</code> probes and
count the distinct <code>probe</code> values, it becomes apparent that enabling probes <em>at
all</em> results in sufficient risk to the system that you may enable at most 500
of them; following the instructions in the scary warning to increase the probe
count results instead in tens of seconds of the system hanging and then pages
of viscous and apparently diagnostic output from the eBPF verifier.</p></li><li><p>Upon further inspection, there is instead a probe site that fires for the
entry of every different kind of system call,
<code>tracepoint:raw_syscalls:sys_enter</code>, though it is difficult to actually use
this effectively: the only way to identify the system call is then by its
number, which though stable is also different per architecture (e.g., even 32-
or 64-bit processes on x86 have different numbers).</p></li></ul></div><p>Conversely, it is possible to cheaply enable tens of thousands of probe sites
with DTrace.  On a current system, there are 236 system calls and something
like 34000 kernel function probe sites, not including a number of other probes.
A simple enabling that counts the firings of all probes over a ten second run
is responsive and easy:</p><div data-lineno="927"><div><pre># time dtrace -n &#39;::: { @ = count(); }&#39; -c &#39;sleep 10&#39;
dtrace: description &#39;::: &#39; matched 80872 probes
dtrace: pid 28326 has exited

         39354882

real    0m18.393s
user    0m0.564s
sys     0m8.115s</pre></div></div></div></div></div></div>
  </body>
</html>
