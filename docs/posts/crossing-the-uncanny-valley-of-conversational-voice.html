<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice">Original</a>
    <h1>Crossing the uncanny valley of conversational voice</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><div><div><div><div><div><p>February 27, 2025</p><p><span>Brendan Iribe</span>,<!-- --> <span>Ankit Kumar</span>, and the Sesame team</p></div></div></div><div><p><span>How do we know when someone truly understands us? It is rarely just our words—it is in the subtleties of voice: the rising excitement, the thoughtful pause, the warm reassurance.</span></p><div><h3>Achieving voice presence</h3><p><span>At Sesame, our goal is to achieve “voice presence”—the magical quality that makes spoken interactions feel real, understood, and valued. We are creating conversational partners that do not just process requests; they engage in genuine dialogue that builds confidence and trust over time. In doing so, we hope to realize the untapped potential of voice as the ultimate interface for instruction and understanding.</span></p></div><div><h3>Key components</h3><div><div><ul><li>Emotional intelligence: reading and responding to emotional contexts.</li><li>Conversational dynamics: natural timing, pauses, interruptions and emphasis.</li><li>Contextual awareness: adjusting tone and style to match the situation.</li><li>Consistent personality: maintaining a coherent, reliable and appropriate presence.</li></ul></div></div></div></div><div><div><h3>We’re not there yet </h3><p><span>Building a digital companion with voice presence is not easy, but we are making steady progress on multiple fronts, including personality, memory, expressivity and appropriateness. This demo is a showcase of some of our work in conversational speech generation. The companions shown here have been optimized for friendliness and expressivity to illustrate the potential of our approach.</span></p></div><div><div><h3>Conversational voice demo</h3><p>1. Microphone permission is required. 2. Calls are recorded for quality review but not used for ML training and are deleted within 30 days. 3. By using this demo, you are agreeing to our<!-- --> <a data-sentry-element="NextLink" data-sentry-source-file="Link.tsx" data-sentry-component="Link" href="https://www.sesame.com/terms">Terms of Use<!-- --> </a> <!-- -->and<!-- --> <a data-sentry-element="NextLink" data-sentry-source-file="Link.tsx" data-sentry-component="Link" href="https://www.sesame.com/privacy">Privacy Policy</a>. 4. We recommend using Chrome (Audio quality may be degraded in iOS/Safari 17.5).</p></div></div></div><div><p>Technical post</p><div><div><div><p>Authors</p><p><span>Johan Schalkwyk</span>,<!-- --> <span>Ankit Kumar</span>,<!-- --> <span>Dan Lyth</span>,<!-- --> <span>Sefik Emre Eskimez</span>, <span>Zack Hodari</span>,<!-- --> <span>Cinjon Resnick</span>,<!-- --> <span>Ramon Sanabria</span>,<!-- --> <span>Raven Jiang</span></p></div></div></div><p>To create AI companions that feel genuinely interactive, speech generation must go beyond producing high-quality audio—it must understand and adapt to context in real time. Traditional text-to-speech (TTS) models generate spoken output directly from text but lack the contextual awareness needed for natural conversations. Even though recent models produce highly human-like speech, they struggle with the one-to-many problem: there are countless valid ways to speak a sentence, but only some fit a given setting. Without additional context—including tone, rhythm, and history of the conversation—models lack the information to choose the best option. Capturing these nuances requires reasoning across multiple aspects of language and prosody.<!-- --> <!-- --> </p></div><div><h3>Background</h3><p>One approach to modeling audio with transformers is to convert continuous waveforms into discrete audio token sequences using tokenizers. Most contemporary approaches (<a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2306.12925">[1]</a>,<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2107.03312">[2]</a>) rely on two types of audio tokens:</p><ol><li><span>Semantic tokens</span>: Compact speaker-invariant representations of semantic and phonetic features. Their compressed nature enables them to capture key speech characteristics at the cost of high-fidelity representation.</li><li><span>Acoustic tokens</span>: Encodings of fine-grained acoustic details that enable high-fidelity audio reconstruction. These tokens are often generated using Residual Vector Quantization (RVQ)<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2107.03312">[2]</a>. In contrast to semantic tokens, acoustic tokens retain natural speech characteristics like speaker-specific identity and timbre.</li></ol><p>A common strategy first models semantic tokens and then generates audio using RVQ or diffusion-based methods. Decoupling these steps allows for a more structured approach to speech synthesis—the semantic tokens provide a compact, speaker-invariant representation that captures high-level linguistic and prosodic information, while the second-stage reconstructs the fine-grained acoustic details needed for high-fidelity speech. However, this approach has a critical limitation; semantic tokens are a bottleneck that must fully capture prosody, but ensuring this during training is challenging.</p><p>RVQ-based methods introduce their own set of challenges. Models must account for the sequential dependency between codebooks in a frame. One method, the delay pattern (figure below)<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2309.08804">[3]</a>, shifts higher codebooks progressively to condition predictions on lower codebooks within the same frame. A key limitation of this approach is that the time-to-first-audio scales poorly because an RVQ tokenizer with N codebooks requires N backbone steps before decoding the first audio chunk. While suitable for offline applications like audiobooks, this delay is problematic in a real-time scenario.</p><p>Example of delayed pattern generation in an RVQ tokenizer with 4 codebooks</p></div><div><h3>Conversational Speech Model</h3><p>CSM is a multimodal, text and speech model that operates directly on RVQ tokens. Inspired by the RQ-Transformer<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2203.01941">[4]</a>, we use two autoregressive transformers. Different from the approach in<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2410.00037">[5]</a>, we split the transformers at the zeroth codebook. The first<span> multimodal backbone</span> processes interleaved text and audio to model the zeroth codebook. The second <span>audio decoder</span> uses a distinct linear head for each codebook and models the remaining N – 1 codebooks to reconstruct speech from the backbone’s representations. The decoder is significantly smaller than the backbone, enabling low-latency generation while keeping the model end-to-end.</p><p>CSM model inference process. Text (T) and audio (A) tokens are interleaved and fed sequentially into the Backbone, which predicts the zeroth level of the codebook. The Decoder then samples levels 1 through N – 1 conditioned on the predicted zeroth level. The reconstructed audio token (A) is then autoregressively fed back into the Backbone for the next step, continuing until the audio EOT symbol is emitted. This process begins again on the next inference request, with the interim audio (such as a user utterance) being represented by interleaved audio and text transcription tokens.</p><p>Both transformers are variants of the Llama architecture. Text tokens are generated via a Llama tokenizer<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2407.21783">[6]</a>, while audio is processed using Mimi, a split-RVQ tokenizer, producing one semantic codebook and N – 1 acoustic codebooks per frame at 12.5 Hz.<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2410.00037">[5]</a> <!-- -->Training samples are structured as alternating interleaved patterns of text and audio, with speaker identity encoded directly in the text representation.</p></div><div><h3>Compute amortization</h3><p>This design introduces significant infrastructure challenges during training. The audio decoder processes an effective batch size of B × S and N codebooks autoregressively, where B is the original batch size, S is the sequence length, and N is the number of RVQ codebook levels. This high memory burden even with a small model slows down training, limits model scaling, and hinders rapid experimentation, all of which are crucial for performance.</p><p>To address these challenges, we use a compute amortization scheme that alleviates the memory bottleneck while preserving the fidelity of the full RVQ codebooks. The audio decoder is trained on only a random 1/16 subset of the audio frames, while the zeroth codebook is trained on every frame. We observe no perceivable difference in audio decoder losses during training when using this approach.</p><p>Amortized training process. The backbone transformer models the zeroth level across all frames (highlighted in blue), while the decoder predicts the remaining N – 31 levels, but only for a random 1/16th of the frames (highlighted in green). The top section highlights the specific frames modeled by the decoder for which it receives loss.</p></div><div><h3>Experiments</h3><p><span>Dataset</span>: We use a large dataset of publicly available audio, which we transcribe, diarize, and segment. After filtering, the dataset consists of approximately one million hours of predominantly English audio.</p><p><span>Model Sizes</span>: We trained three model sizes, delineated by the backbone and decoder sizes:</p><ul><li><span>Tiny</span>: 1B backbone, 100M decoder</li><li><span>Small</span>: 3B backbone, 250M decoder</li><li><span>Medium</span>: 8B backbone, 300M decoder</li></ul><p>Each model was trained with a 2048 sequence length (~2 minutes of audio) over five epochs.</p></div><div><h3>Samples</h3><p>Paralinguistics</p><p>Sentences from<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2402.08093">Base TTS</a></p><p>Foreign words</p><p>Sentences from<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2402.08093">Base TTS</a></p><p>Contextual expressivity</p><p>Samples from Expresso, continuation after chime</p><p>Pronunciation correction</p><p>Pronunciation correction sentence is a recording, all other audio is generated.</p><p>Conversations with multiple speakers</p><p>Single generation using audio prompts from two speakers</p></div><div><h3>Evaluation</h3><p>Our evaluation suite measures model performance across four key aspects: faithfulness to text, context utilization, prosody, and latency. We report both objective and subjective metrics—objective benchmarks include word error rate and novel tests like homograph disambiguation, while subjective evaluation relies on a Comparative Mean Opinion Score (CMOS) human study using the <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2308.05725">Expresso</a> <!-- -->dataset.</p></div><div><h3>Objective metrics</h3><p>Traditional benchmarks, such as word error rate (WER) and speaker similarity (SIM), have become saturated—modern models, including CSM, now achieve near-human performance on these metrics.</p><p>Objective metric results for Word Error Rate (top) and Speaker Similarity (bottom) tests, showing the metrics are saturated (matching human performance).</p><p>To better assess pronunciation and contextual understanding, we introduce a new set of phonetic transcription-based benchmarks.</p><ul><li><span>Text understanding through Homograph Disambiguation:</span> <!-- -->Evaluates whether the model correctly pronounced different words with the same orthography (e.g., “lead” /lɛd/ as in “metal” vs. “lead” /liːd/ as in “to guide”).</li><li><span>Audio understanding through Pronunciation Continuation Consistency:</span> <!-- -->Evaluates whether the model maintains pronunciation consistency of a specific word with multiple pronunciation variants in multi-turn speech. One example is “route” (/raʊt/ or /ruːt/), which can vary based on region of the speaker and context.</li></ul><p>Objective metric results for Homograph Disambiguation (left) and Pronunciation Consistency (right) tests, showing the accuracy percentage for each model’s correct pronunciation. Play.ht, Elevenlabs, and OpenAI generations were made with default settings and voices from their respective API documentation.</p><p>The graph above compares objective metric results across three model sizes. For Homograph accuracy we generated 200 speech samples covering 5 distinct homographs—lead, bass, tear, wound, row—with 2 variants for each and evaluated pronunciation consistency using<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://huggingface.co/facebook/wav2vec2-lv-60-espeak-cv-ft">wav2vec2-lv-60-espeak-cv-ft</a>. For Pronunciation Consistency we generated 200 speech samples covering 10 distinct words that have common pronunciation variants—aunt, data, envelope, mobile, route, vase, either, adult, often, caramel.</p><p>In general, we observe that performance improves with larger models, supporting our hypothesis that scaling enhances the synthesis of more realistic speech.</p></div><div><h3>Subjective metrics</h3><p>We conducted two Comparative Mean Opinion Score (CMOS) studies using the<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://arxiv.org/abs/2308.05725">Expresso</a> <!-- -->dataset to assess the naturalness and prosodic appropriateness of generated speech for CSM-Medium. Human evaluators were presented with pairs of audio samples—one generated by the model and the other a ground-truth human recording. Listeners rated the generated sample on a 7-point preference scale relative to the reference. Expresso’s diverse expressive TTS samples, including emotional and prosodic variations, make it a strong benchmark for evaluating appropriateness to context.</p><p>In the first CMOS study we presented the generated and human audio samples with no context and asked listeners to<!-- --> <span>“choose which rendition feels more like human speech.”</span> In the second CMOS study we also provide the previous 90 seconds of audio and text context, and ask the listeners to<!-- --> <span>“choose which rendition feels like a more appropriate continuation of the conversation.”</span> Eighty people were paid to participate in the evaluation and rated on average 15 examples each.</p><p>Subjective evaluation results on the Expresso dataset. No context: listeners chose<!-- --> <span>“which rendition feels more like human speech”</span> <!-- -->without knowledge of the context. Context: listeners chose<!-- --> <span>“which rendition feels like a more appropriate continuation of the conversation”</span> <!-- -->with audio and text context. 50:50 win–loss ratio suggests that listeners have no clear preference.</p><p>The graph above shows the win-rate of ground-truth human recordings vs CSM-generated speech samples for both studies. Without conversational context (top), human evaluators show no clear preference between generated and real speech, suggesting that naturalness is saturated. However, when context is included (bottom), evaluators consistently favor the original recordings. These findings suggest a noticeable gap remains between generated and human prosody in conversational speech generation.</p></div><div><h3>Open-sourcing our work</h3><p>We believe that advancing conversational AI should be a collaborative effort. To that end, we’re committed to open-sourcing key components of our research, enabling the community to experiment, build upon, and improve our approach. Our models will be available under an Apache 2.0 license.</p></div><div><h3>Limitations and future work</h3><p>CSM is currently trained on primarily English data; some multilingual ability emerges due to dataset contamination, but it does not perform well yet. It also does not take advantage of the information present in the weights of pre-trained language models.</p><p>In the coming months, we intend to scale up model size, increase dataset volume, and expand language support to over 20 languages. We also plan to explore ways to utilize pre-trained language models, working towards large multimodal models that have deep knowledge of both speech and text.</p><p>Ultimately, while CSM generates high quality conversational prosody, it can only model the text and speech content in a conversation—not the structure of the conversation itself. Human conversations are a complex process involving turn taking, pauses, pacing, and more. We believe the future of AI conversations lies in fully duplex models that can implicitly learn these dynamics from data. These models will require fundamental changes across the stack, from data curation to post-training methodologies, and we’re excited to push in these directions.</p></div><div><h3>Join us</h3><p>If you’re excited about building the most natural, delightful, and inspirational voice interfaces out there, reach out—we’re hiring. Check our<!-- --> <a target="_blank" rel="noopener noreferrer" data-sentry-element="NextLink" data-sentry-source-file="crossing_the_uncanny_valley_of_voice.tsx" href="https://jobs.ashbyhq.com/sesame">open roles</a>.</p></div></div></div></div></div></div></div></div>
  </body>
</html>
