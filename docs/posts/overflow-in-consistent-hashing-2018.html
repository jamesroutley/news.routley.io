<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rmarcus.info/blog/2018/09/14/consistent-hashing-overflow.html">Original</a>
    <h1>Overflow in consistent hashing (2018)</h1>
    
    <div id="readability-page-1" class="page"><div>
  
  <p>14 Sep 2018</p>
  <p><a href="https://en.wikipedia.org/wiki/Consistent_hashing">Consistent hashing</a> was first proposed <a href="https://dl.acm.org/citation.cfm?doid=258533.258660">in 1997 by David Karger et al.</a>, and is used today in many large-scale data management systems, including (for example) <a href="https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf">Apache Cassandra</a>. Consistent hashing helps reduce the number of items that need to be moved from one machine to another when the number of machines in a cluster changes.</p>

<p>The basic idea is to use two hash functions<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">1</a></sup> – one, <span id="il153"></span>, which maps each <em>data item</em> to a point on a circle (generally represented as an angle between <span id="il154"></span> and <span id="il155"></span>), and another, <span id="il156"></span>, which maps each <em>machine</em> to a point on the circle. Each machine <span id="il157"></span> is responsible for serving all the data items with hash values that fall between <span id="il158"></span> and the machine with the next angle along the circle (clockwise).</p>



<p>In the example above, each server is hashed by <span id="il156"></span> to three coordinates on the circle (server 1 is hashed to the east-most point on the circle). Items that fall between server A’s hashed position and the next server (moving clockwise) are assigned to server A. Click and drag the data items to change their assigned segment.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">2</a></sup></p>

<p>As shown above, this technique generally produces a uniform distribution of data items onto servers. Of course, bad luck might end up putting far more data items onto one server than another. But that’s pretty unlikely… right?</p>

<p>As far as I can tell, this problem was first given an efficient answer in 1987 by <a href="https://rmarcus.info/blog/assets/overflow/ramakrishna1987.pdf">M.V. Ramakrishna</a>. Ramakrishna phrases the problem as an “urn” problem (paraphrased):</p>

<blockquote>
  <p>There are <span id="il159"></span> balls tossed uniformly at random  into one of <span id="il157"></span> urns, each urn having a capacity of at most <span id="il160"></span> balls. What is the probability that no bin overflows?</p>
</blockquote>

<p>It turns out this probability is difficult enough to compute that an instance of it appears in a <a href="https://projecteuler.net/problem=307">high-numbered (307) Project Euler problem</a>. This blog post won’t go into the details behind efficiently computing the exact probability, but it will:</p>

<ul>
  <li>explore some unintuitive results that arise from these probabilities,</li>
  <li>discuss some (very good) approximations that can be computed near-instantly,</li>
  <li>provide a handy calculator for computing the probability,</li>
  <li>and provide some general pointers for cluster sizing and tuning.</li>
</ul>

<h3 id="it-cant-overflow-that-often-can-it">It can’t overflow that often, can it?</h3>

<p>Since an example is worth a thousand equations, here’s an animated figure:</p>

<div id="overflowWrapper">
<svg id="overflowSVG" width="300px" height="250px" style="padding-bottom: 15px;">
  <g id="overflowSVGArcs"></g>
  <g id="overflowSVGDividers"></g>
  <g id="overflowSVGCircles"></g>
</svg><p>

Each time a segment turns red, that segment is overflowing. The table below controls the parameters of the experiment:

</p><table id="parameters">
  <thead>
    <tr><th>Parameter</th><th>Value</th></tr>
  </thead>
  <tbody>
    <tr><td>Number of items</td><td>
        
        <span id="numItems"></span>
        
    </td></tr>
    <tr><td>Number of bins</td><td>
        
        <span id="numBins"></span>
        
    </td></tr>
    <tr><td>Bin capacity</td><td>
        
        <span id="binCapacity"></span>
        
    </td></tr>
    <tr><td>Overflow probability</td>
      <td><span id="overflowProb"></span></td>
    </tr>
  </tbody>  
</table>
</div>

<p>The probability listed in the table above is computed precisely using Ramakrishna’s technique. With 5 servers, and a server capacity of 4, you have a theoretical maximum capacity of 40 items. However, with just 10 items, there’s a 16.37% chance of one server overflowing!</p>

<p>Perhaps even more counter-intuitively, with 10 bins of size 3 and 15 items, there’s a <em>nearly 50%</em> chance of an overflow! That’s even with the theoretical capacity of the cluster being twice as large as the number of items we’re trying to store (a load factor of 0.5).</p>

<p>When we talk about hash tables, distributed or otherwise, we often talk about the <em>load factor</em>, which is the number of items divided by the total number of spots an item could occupy. In our case, the load factor is <span id="il161"></span>. However, playing with extreme cases makes it clear that there’s more to this problem than the load factor. For example, consider two scenarios: 5 bins with a capacity of 2 and 2 bins with a capacity of 5. With 5 items, the overflow probability for the second scenario is zero: no arrangement of items will cause any bin to overflow. The first scenario, however, overflows about 25% of the time.</p>

<p>Next, we’ll look at a way to approximate the probability of a bin overflowing. This process will also teach us quite a bit about what is really going on.</p>

<h3 id="approximations">Approximations</h3>
<p>Ramakrishna works out a few nice approximate closed-form expression for the overflow probability. To build this approximation, we first note that the probability of an arbitrary <em>but fixed</em> bin overflowing is binomial (e.g., the probability of at least <span id="il162"></span> successful trials occurring within <span id="il159"></span> total trials with success probability <span id="il163"></span>). Letting <span id="il164"></span> be the load factor (the number of items divided by the maximum capacity), we can <a href="https://en.wikipedia.org/wiki/Binomial_distribution#Poisson_approximation">approximate this probability using a Poisson distribution</a>:</p>



<p>When our bin capacity <span id="il160"></span> is large, and our load factor <span id="il165"></span> is not too close to one, the tail of the distribution will collapse rapidly, so we can approximate this sum by taking the first few terms (and applying some algebra):</p>



<p>Ramakrishna next makes the assumption that each bin overflows independently of other bins (which is definitely false, but remember, we’re building an approximation), which allows us to write the probability that no bin overflows as:</p>



<p><a name="diff"></a>
Ramakrishna shows that this approximation is very good in the case of <span id="il166"></span> and <span id="il167"></span>, and larger values of <span id="il160"></span> allow for lower values of <span id="il157"></span>. In addition to being a useful approximation for calculation, we can also differentiate this approximation to gain a better understanding of how the variables work.</p>



<p>Imagine we had a load factor of 0.5 and <span id="il168"></span>. This equation tells us that a unit increase in <span id="il165"></span> will result in a 20-fold increase to <span id="il169"></span>! So, for example, if we were to increase our load factor from 0.5 to 0.6, we could expect <span id="il170"></span>. Counter-intuitively, this means that larger bin sizes are <em>more</em> sensitive to changes in load factor.</p>

<p>Let’s take a closer look at how the load factor, number of items, bin capacity, and number of bins interact. We’ll begin by fixing the capacity of each bin at 20. The sliders below allow you to vary the number of bins represented by each curve.</p>

<div>
<svg id="approxGraph" width="300px" height="300px"></svg>
<div><p>Fixed bin capacity (20)</p>
<table id="graph-parameters">
  <thead>
    <tr><th>Curve</th><th>Bins</th></tr>
  </thead>
  <tbody>
    <tr><td>Red</td><td>
        
        <span id="red"></span>
    </td></tr>
    <tr><td>Green</td><td>
        
        <span id="green"></span>
    </td></tr>
    <tr><td>Blue</td><td>
        
        <span id="blue"></span>
    </td></tr>
  </tbody>
</table></div>
</div>

<p>The graph can be a little perplexing at first glance. To interpret it, first set the values to 5, 25, and 200. Notice that the red curve, representing 5 bins, at a load factor of 0.8, has an overflow probability of around 50%. This means that in a cluster configuration with 5 nodes, a node capacity of 20, and 80 items, there is (approximately) a 50% chance of a node overflowing. Compare this to the blue curve, representing 200 bins – if I have the same node capacity (20) and the same load factor of 0.8 (implying that I have 3200 items), my overflow probability is practically one. This is very counter-intuitive if you are used to thinking of the load factor as something that should be kept constant as a system grows. However, as we’ve seen here, keeping the load factor constant as you increase the number of nodes and data items will cause your overflow probability to skyrocket.</p>

<p>Imagine you have 5 cups, each currently filled with 8 pieces of candy. You instruct your friend, a computer scientist, to take the remaining 10 pieces of candy and evenly distribute them into each cup, such that each cup will have 10 pieces of candy. The computer scientist, having recently taken a data structures course, begins dropping each remaining piece of candy into a cup at random. Confused and unsatisfied, you are forced to redistribute the candies yourself (the computer scientist’s explanation of an “amortized cup” does not comfort you).</p>



<p>Clearly, the computer scientist’s strategy would’ve worked if your friend had gotten a little lucky. But exactly how lucky? Well, getting it exactly right would’ve been like rolling 10 dice (each representing a piece of candy), each with 5 sides (each representing a cup), and getting exactly two “ones”, two “twos”, two “threes”, two “fours”, and two “fives.” We know that there are <span id="il171"></span> possible ways the dice could land, and we know that the total number of ways the dice could land in the desired configuration is:</p>



<p>Taking the quotient, we end up with a success rate of around 1%. Not very good odds.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>Now imagine that instead of 5 cups filled with 8 pieces of candy with 10 pieces of candy leftover, you instead have 50 cups, each with 8 pieces of candy, with 100 pieces of candy still to be distributed. Each cup is still 80% full, but do you think your friend’s odds of success have changed? Now, your friend must roll exactly 20 “ones”, 20 “twos”, 20 “threes”, 20 “fours”, and 20 “fives.” Again, there are <span id="il172"></span> different ways for the dice to land. This time, the number of valid configurations is:</p>



<p>Since we’re starting to get some 60+ digit numbers, <a href="http://www.wolframalpha.com/input/?i=(%7B100+%5Cchoose+20%7D+%5Ctimes+%7B80+%5Cchoose+20%7D+%5Ctimes+%7B60+%5Cchoose+20%7D+%5Ctimes+%7B40+%5Cchoose+20%7D)+%2F+5%5E100">we’ll let Wolfram Alpha do the math for us</a>. The result? A smashing 0.013% success rate. Just like keeping the cup only 80% full didn’t keep the success probability the same, keeping the load factor the same for consistent hashing schemes of different scales doesn’t give you the same overflow probability.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>Next, we will fix the number of bins, and instead alter the bin capacity.</p>


<p>Interpreting the graph is much the same. First, reset the graph to its default values. Notice that, with a bin capacity of 5 (red line), the overflow probability goes above 10% at about a load factor of 0.25. This corresponds to around 63 items. However, with a bin size of 25 (green line), the overflow probability does not go over 10% until a load factor of 0.55! This corresponds to around 688 items. So, by increasing each bin size by a factor of 5 (from 5 to 25), the total number of items I could store (with a fixed overflow probability) went up by a factor of 10!</p>

<p>We can also learn quite a bit from the slopes of the curves. As we predicted <a href="#diff">before, based on the derivative of the approximation</a>, the blue curve (representing a bin size of 200) has a much steeper slope than the red curve (representing a bin size of 5). So, on the one hand, increasing bin capacity gives a very nice improvement to the maximum load factor that can be tolerated, but it also makes the system more sensitive to fluctuations in that load factor.</p>

<h3 id="handy-calculator">Handy calculator</h3>

<table>
    <thead>
        <tr><th>Parameter</th><th>Value</th></tr>
    </thead>
    <tbody>
        <tr><td>Number of items</td>
            <td></td></tr>
        <tr><td>Number of bins</td>
            <td></td></tr>
        <tr><td>Bin capacity</td>
            <td></td></tr>
        <tr><td>Result type</td>
            <td id="result-type">exact</td></tr>
        <tr><td>Overflow probability</td>
            <td id="overflow-prob">0.2095</td></tr>
    </tbody>
</table>

<p>The calculator will use an exact method when the inputs are small enough, and fall back to using an estimate when the values become too large.</p>

<h3 id="takeaways--suggestions">Takeaways &amp; suggestions</h3>
<p>When you are using a consistent hashing system, it’s good to keep overflows in mind. Unless any node can hold all your data, there’s a possibility that <strong>you’ll have overflow</strong>.</p>

<p>Since there’s (almost) always going to be some chance of overflow, you should probably <strong>do something about it</strong>. <a href="https://arxiv.org/abs/1608.01350">Google has put out an interesting paper</a> showing that a simple strategy of “pushing” extra items from one bin to the next doesn’t require too many “pushes.” <a href="https://arxiv.org/abs/1608.01350">Vimeo put this idea into practice</a> to great success.</p>

<p>Regardless of how you handle them, overflowing nodes are never a good thing. First, <strong>decide what your system needs</strong>: tolerance to changes in load factors, or maximal storage with low overflow probability. If you need the former, build many, smaller nodes. If you need the later, build larger nodes, but keep in mind that a small change in load factor may cause your overflow rate to skyrocket!</p>

<p>This problem gets more interesting when you consider what might happen in a heterogeneous cluster… it certainly makes the math a lot more complicated!</p>







</div></div>
  </body>
</html>
