<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.greptile.com/blog/semantic">Original</a>
    <h1>Searching a Codebase in English</h1>
    
    <div id="readability-page-1" class="page"><article>
<p>I&#39;m Daksh, one of the co-founders of Greptile. We&#39;re building AI that understands codebases, which you can query using an API. To do this, we have to, depending on the task, provide an LLM with snippets from the codebase, ideally the fewest number of snippets that give it <em>sufficient</em> information to respond to the query.</p>
<p><img src="https://i.imgur.com/iHO15Hg.png" alt="Semantic search illustration"/></p>
<p>We found out that this problem is harder than it looks. Before we get into all the ways we tried to make codebase semantic search work, it&#39;s interesting to see why it&#39;s different from semantically searching a book.</p>
<h3 id="semantically-searching-a-book"><a href="#semantically-searching-a-book"></a>Semantically searching a book</h3>
<p>First, we index the corpus we are trying to search over:</p>
<ol>
<li>Split it up into units (or &#34;chunks&#34;), each one ideally about distinct topics. Splitting by paragraph is a good approximation of this.</li>
<li>Generate semantic vector embeddings for each &#34;chunk&#34;.</li>
</ol>
<p>Semantic vector embeddings capture the semantic meaning of a piece of text as an <em>n</em>-dimensional vector. The algorithm to create them is quite fascinating and is explained very well in chapter 6 of Speech and Language Processing, available for free <a target="_blank" rel="noopener noreferrer" href="https://web.stanford.edu/~jurafsky/slp3/6.pdf"><strong>here</strong></a>.</p>
<p>When two pieces of text are semantically similar, their vectors are too, which can be quantified by their cosine similarity or normalized dot product. You could say, therefore, that the semantic similarity between two pieces of text <strong><em>t₁</em>,</strong> <strong><em>t₂</em></strong> is:</p>
<p><img src="https://i.imgur.com/45cqysn.png" alt="dot product of v_1 and v_2"/></p>
<p>Where <strong><em>v₁</em></strong> is the normalized semantic embedding vector for text <strong><em>t₁</em></strong> and <strong><em>v₂</em></strong> is the normalized semantic embedding vector for text <strong><em>t₂</em></strong>.</p>
<p><img src="https://i.imgur.com/MwUlyRy.png" alt="Indexing"/></p>
<p>To retrieve, we start by generating a semantic vector embedding for the query, in this case, &#34;Word storing frequently accessed key-value pairs&#34;. We then compare that against our database of vectors and find the one(s) that match the closest, i.e., have the lowest dot product and highest similarity.</p>
<p><img src="https://i.imgur.com/9gOntK6.png" alt="Retrieval"/></p>
<p>In theory, this should work for codebases too. Surely you could split up a codebase into files or functions as the “chunks”, embed them, and do a similar semantic similarity-based search. We tried this, and got fairly poor results even with simple queries like “Session management code”, which should have retrieved the files associated with session management, and instead naively picked up files that mentioned “management” or “session”, since those files were the most semantically similar.</p>
<h3 id="code-and-natural-language-are-not-semantically-similar"><a href="#code-and-natural-language-are-not-semantically-similar"></a>Code and natural language are not semantically similar</h3>
<p>Here I have a natural language query, with which I want to search a codebase to find where the code for HFT fraud detection is.</p>
<pre><div><pre><span>query = &#34;Get the code that detects potential fraud in HFT transactions&#34;
</span></pre></div></pre>
<p>This is the code I am hoping the search will return:</p>
<pre><div><pre><span>from collections import defaultdict

def analyze_high_frequency_transactions(
    transactions: List[Dict[str, any]],
    time_window: timedelta,
    amount_threshold: float,
    frequency_threshold: int
) -&gt; Tuple[List[Dict[str, any]], Dict[str, List[datetime]]]:

    def is_suspicious(times: List[datetime]) -&gt; bool:
        if len(times) &lt; frequency_threshold:
            return False
        times.sort()
        for i in range(len(times) - frequency_threshold + 1):
            if times[i + frequency_threshold - 1] - times[i] &lt;= time_window:
                return True
        return False

    suspicious_transactions = []
    account_timestamps = defaultdict(list)

    for transaction in transactions:
        account = transaction[&#39;account_id&#39;]
        amount = transaction[&#39;amount&#39;]
        timestamp = datetime.fromisoformat(transaction[&#39;timestamp&#39;])

        if amount &gt;= amount_threshold:
            account_timestamps[account].append(timestamp)

            if is_suspicious(account_timestamps[account]):
                suspicious_transactions.append(transaction)

    flagged_accounts = {
        account: timestamps
        for account, timestamps in account_timestamps.items()
        if is_suspicious(timestamps)
    }

    return suspicious_transactions, flagged_accounts

</span></pre></div></pre>
<p>Here is an English description of what the code does:</p>
<pre><div><pre><span>This function analyzes a list of financial transactions to identify potentially suspicious high-frequency trading patterns. It flags transactions and accounts that meet specific criteria:

1. Transactions with amounts above a certain threshold.
2. Accounts with a high frequency of such transactions within a specified time window.

The function takes four parameters:

- A list of transaction dictionaries
- A time window for analysis
- An amount threshold for transactions
- A frequency threshold for the number of transactions

It returns two items:

1. A list of suspicious transactions
2. A dictionary of flagged accounts with their transaction timestamps

The function uses nested helper functions and defaultdict for efficient processing. It&#39;s designed to handle large datasets and could be part of a larger financial monitoring system.
</span></pre></div></pre>
<p>I computed the semantic similarity between the query and code, and then between the query and the natural language code description. Here is what I found:</p>
<div><table><thead><tr><th><p>Comparison</p></th><th><p>Similarity</p></th></tr></thead><tbody><tr><td><p>Query and code</p></td><td><p>0.7280</p></td></tr><tr><td><p>Query and description</p></td><td><p>0.8152</p></td></tr></tbody></table></div>
<p>The similarity between the query and the description is meaningfully higher (12%) than the similarity between query and description. This <em>should</em> mean that searching over a natural language summary of the codebase should yield better results than searching over the code.</p>
<h3 id="chunking-and-noise"><a href="#chunking-and-noise"></a>Chunking and noise</h3>
<p>Another aspect is the signal-to-noise ratio of the chunk that was embedded.</p>
<p>In the indexing step if you chunk the codebase by file (every file being one vector), you might include a lot of code that isn’t relevant to the query.</p>
<p>Here we examine three scenarios, and see the semantic similarity each one has with the query </p><div><pre><span>&#34;Get the code that detects potential fraud in HFT transactions&#34;</span></pre></div>
<ol>
<li>Full file of random code</li>
<li>Full file of random code with correct function buried in between</li>
<li>Just the correct function</li>
</ol>
<p>Interestingly, adding noise dramatically reduces the semantic similarity, to a point where the performance is closer to if it were just noise than if it was just the correct function.</p>
<div><table><thead><tr><th><p>Scenario</p></th><th><p>Similarity</p></th></tr></thead><tbody><tr><td><p>Query and full file</p></td><td><p>0.718032</p></td></tr><tr><td><p>Query and full file with function</p></td><td><p>0.739421</p></td></tr><tr><td><p>Query and just the function</p></td><td><p>0.768347</p></td></tr></tbody></table></div>
<h3 id="summary"><a href="#summary"></a>Summary</h3>
<ul>
<li>Semantic search on codebases works better if you first translate the code to natural language, before generating embedding vectors.</li>
<li>It also works better if you chunk more “tightly” - on a per-function level rather than a per-file level. This is because noise negatively impacts retrieval quality in a huge way.</li>
</ul></article></div>
  </body>
</html>
