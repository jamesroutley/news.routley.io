<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://reubenson.com/recurse/week-6/">Original</a>
    <h1>Wrapping up</h1>
    
    <div id="readability-page-1" class="page"><div>
      
<h3>Week 6 at Recurse Center</h3>
<p>It&#39;s been such a fast six weeks at Recurse, and all the more so for getting so absorbed in getting my MIDI Archive project into shape. It&#39;s been a journey ... which I think I&#39;ll need some time to sit with before writing out a proper reflection of my wonderful and expansive time here.</p>
<p>For now, I&#39;m still figuring out a path to an MVP version of this project, which I&#39;m hoping to chip away at the next few weeks as I adjust to a new post-RC equilibrium. I&#39;m also struggling with uncertainty and doubt about the technical and cultural value of this project, and whether it makes sense to put much more effort into an MVP. At worst, it feels unclear to me whether I&#39;m just re-enacting the algorithmic instrumentalization of music (a l√° Spotify) in a more amateur way, on a more campy substrate.</p>
<p>But for the moment, I&#39;ll just limit my scope to documenting and preserving the groundwork I&#39;ve laid out thus far. And while still in very skeletal form, the project has at least been shipped to prod! <a href="http://reubenson.com/midi-archive/">http://reubenson.com/midi-archive/</a></p>
<h4>THE PROJECT</h4>
<p>At the core of this project, I&#39;ve been driven by two broad questions:</p>
<ul>
<li>What, if anything, does the pre-MP3 history of music on the web say about the present moment? Or its future?</li>
<li>How the heck does machine learning actually work?</li>
</ul>
<p>In the six weeks of my residency at Recurse, I feel like I&#39;m still just scratching the surface of these questions. But as I&#39;ve heard writers say, you write a story in order to figure out how it ends.</p>
<p>In my approach to these two questions, I&#39;ve been building a neural net model alongside an archive, in which the model informs my curation of the archive, and the curation of the archive influences how the model behaves. In more straightforward terms, what I&#39;ve been doing is the following:</p>
<ul>
<li>Explore MIDI websites on the early web, and scraping ones that feel culturally/aesthetically interesting and/or representative. I&#39;ve eschewed using canonical datasets like <a href="https://magenta.tensorflow.org/datasets/maestro">Maestro</a> or large existing archives like the <a href="https://archive.org/details/archiveteam-geocities-midi-collection-2009">Geocities MIDI collection</a>. Thus far, I&#39;ve scraped together ~3000 MIDI files, which is not a lot. In some sense, I&#39;ve been loosely following principles described by Everest Pipkin in their <a href="https://www.youtube.com/watch?v=IYNKs8vfocc">&#34;Corpora as Medium&#34; talk</a> on the importance of curation in working with large language models. Given that, I&#39;ve been taking a more iterative approach to building up the archive.</li>
<li>Use these MIDI files to train a neural net model built on PyTorch in order to generate new MIDI files. My main goal with this model is to be of educational use, not necessarily trying to make it as sophisticated as possible, so I&#39;ve been pretty happy with the relatively naive results I&#39;ve gotten thus far. I find the results amusing, but I&#39;m probably biased. It reminds me a bit of the main plot-point in <a href="https://en.wikipedia.org/wiki/The_Fly_(1986_film)">The Fly</a>, in which Jeff Goldblum fuses with a housefly ...</li>
<li>The MIDI Archive and the ML output is featured alongside each other on the <a href="https://reubenson.com/midi-archive">MIDI Archive website</a>. I had originally planned on having the ML output presented as a &#39;daily broadcast/performance&#39;, but haven&#39;t gotten around to implementing that bit yet. At the moment, the loosely prototyped website allows visitors to see and listen to all the MIDI files that were used for training and validating the model, as well as listen to MIDI music generated (daily) by the model.</li>
</ul>
<p>As for what&#39;s next for this project ... the main thing that feels lacking at the moment is a sense for what visitors, who may or may not be familiar with music on the early web, should expect to find here. I&#39;m not particularly interested in just exploring the nostalgic valence of this work, but want to dig deeper into why I may have been drawn to this project in the first place ...</p>
<p>But for now, feel free to take a look at the two repositories I&#39;ve spun up for this project, but be warned that documentation and cleanup is still in a messy state</p>
<ul>
<li><a href="https://github.com/reubenson/midi-archive">https://github.com/reubenson/midi-archive</a> contains the scraper and 11ty sit generator</li>
<li><a href="https://github.com/reubenson/midi-archive-lambda">https://github.com/reubenson/midi-archive-lambda</a> contains the lambda functions used for deployed model, and the notebook used to train the model</li>
</ul>

    </div></div>
  </body>
</html>
