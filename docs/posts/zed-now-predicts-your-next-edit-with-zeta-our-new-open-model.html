<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zed.dev/blog/edit-prediction">Original</a>
    <h1>Zed now predicts your next edit with Zeta, our new open model</h1>
    
    <div id="readability-page-1" class="page"><article><p>Zed is built for speed. We&#39;ve always strived for an editing experience that feels <em>instant</em>. But what&#39;s faster than instant?  A tool that anticipates your next move. That&#39;s why we&#39;re introducing <a href="https://theleo.zone/edit-prediction">edit prediction</a> in Zed, powered by <a href="https://huggingface.co/zed-industries/zeta">Zeta</a>, our new open source model.</p>
<p>Here&#39;s a quick walkthrough:</p>
<p><figure><video src="https://customer-snccc0j9v3kfzkif.cloudflarestream.com/32e5feacc10b8e63292918b21e9ae15c/downloads/default.mp4" width="640" height="360" controls=""></video><figcaption>Edit Prediction in action.</figcaption></figure></p>
<p>As you work, Zed now predicts your next edit, so you can apply it just by hitting <code>tab</code>. Once you accept a prediction, you can perform multiple follow-up edits by pressing <code>tab</code> repeatedly, saving you time and keystrokes. We&#39;ve received <em>a ton</em> of requests for this functionality, and we&#39;ve poured our hearts into making it feel like a natural extension of the Zed experience.</p>
<p>You can use Zeta for free during this public beta by <a href="https://theleo.zone/download">downloading Zed</a> and signing in with your GitHub account. Edit prediction won&#39;t be free forever, but right now we&#39;re just excited to share and learn.</p>
<h2 id="thoughtful-integration"><a href="#thoughtful-integration" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Thoughtful Integration</span></a></h2>
<p>Edit prediction transforms <code>tab</code> into a magical, universal key. But what about the existing uses of <code>tab</code>, such as indenting lines? And what happens when there&#39;s both an edit prediction <em>and</em> suggestions from your language server? We didn&#39;t want a powerful new feature to come at the expense of the existing editing experience in Zed.</p>
<p><figure><video loop="" muted="" autoplay="" preload="metadata" controls="" playsinline=""><source src="https://customer-snccc0j9v3kfzkif.cloudflarestream.com/4f9b29a42d9bfa64e5457dec716f59be/downloads/default.mp4" type="video/webm; codecs=&#34;vp8.0, vorbis&#34;"/><source src="https://customer-snccc0j9v3kfzkif.cloudflarestream.com/4f9b29a42d9bfa64e5457dec716f59be/downloads/default.mp4" type="video/mp4; codecs=&#34;avc1.4D401E, mp4a.40.2&#34;"/></video><figcaption>Zeta predictions together with language server completions.</figcaption></figure></p>
<p>When language server completions are visible, Zed won&#39;t preview the predicted edit until you press <code>option</code> or <code>alt</code>. As soon as you press the modifier, Zed previews the edit and hides the menu to enable an unobstructed review. On macOS, you can just hit <code>tab</code> to confirm, or back out by releasing <code>option</code> to restore the language server completions menu.</p>
<p>On Linux, <code>alt-tab</code> is often reserved by the window manager, so we offer <code>alt-l</code> as an alternative default. We chose <code>l</code> because it&#39;s on the QWERTY home row and represents rightward movement in Vim. If your Linux window manager doesn&#39;t claim <code>alt-tab</code>, you&#39;re free to use that binding as well.</p>
<h2 id="introducing-zeta-zeds-open-source-edit-prediction-model"><a href="#introducing-zeta-zeds-open-source-edit-prediction-model" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Introducing Zeta: Zed&#39;s Open Source Edit Prediction Model</span></a></h2>
<p><a href="https://huggingface.co/zed-industries/zeta">Zeta</a> is derived from Qwen2.5-Coder-7B, and is fully open source, including <a href="https://huggingface.co/datasets/zed-industries/zeta">an open dataset</a>. If you&#39;re working in an open source repository, we&#39;d love your help improving Zeta by contributing to its dataset. Please bear with us initially, as we will be reviewing the submitted data before publishing to ensure everyone&#39;s safety and privacy. We&#39;re excited to figure this out and see a community effort form to make edit prediction better everywhere, most especially in Zed!</p>
<!-- -->
<div><div><div><p>Companion Video</p><p>How Zed&#39;s Open-Source Edit Predictions Work</p></div><p>Richard Feldman and Antonio Scandurra talk about how Zed&#39;s new Edit Prediction feature works under the hood. This includes how the Zed team developed and open-sourced both the code and the dataset behind the fine-tuned Zeta language model that powers it!</p><p><a href="https://youtu.be/r1A268kA1uM">Watch the video here →</a></p></div><p><a href="https://youtu.be/r1A268kA1uM"><img src="https://theleo.zone/img/edit-prediction/decoded-banner.webp" width="230" height="150"/></a></p></div>
<h3 id="editing-by-rewriting"><a href="#editing-by-rewriting" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Editing By Rewriting</span></a></h3>
<p>Most coding models are trained on a &#34;fill in the middle&#34; task. You give them a prefix and a suffix, and they generate what goes in between.</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="rs" data-theme="dark-plus light-plus"><span data-line=""><span>&lt;|</span><span>fim_prefix</span><span>|&gt;</span><span>fn</span><span> quicksort</span><span>(</span><span>array</span><span>: &amp;</span><span>mut</span><span> [</span><span>T</span><span>]) {</span></span>
<span data-line=""><span>    if</span><span> array</span><span>.</span><span>len</span><span>() &lt;= </span><span>1</span><span> {</span></span>
<span data-line=""><span>        return</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>    let</span><span> pivot</span><span> = </span><span>partition</span><span>(</span><span>array</span><span>);</span></span>
<span data-line=""><span>    &lt;|</span><span>fim_suffix</span><span>|&gt;</span></span>
<span data-line=""><span>    quicksort</span><span>(&amp;</span><span>mut</span><span> array</span><span>[</span><span>pivot</span><span> + </span><span>1</span><span>..]);</span></span>
<span data-line=""><span>}&lt;|</span><span>fim_middle</span><span>|&gt;</span></span></code></pre></div></figure>
<p>This works for completing text at the cursor, but we wanted Zeta to predict edits at arbitrary locations, which doesn&#39;t fit into this structure.</p>
<p>In our experience, models aren&#39;t very good at producing granular edits, but they do excel at rewriting larger chunks of code. So that&#39;s where we started: given a list of recent edits and the cursor position, we asked the model to rewrite a snippet of text around the cursor, incorporating one or more edit predictions in the rewritten text.</p>
<h3 id="evaluating-predictions"><a href="#evaluating-predictions" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Evaluating Predictions</span></a></h3>
<p>Before writing a single line of code, we created a set of tests to check if our idea worked. Testing the output of a large language model is tricky because, on every run, you can get slightly different results even when feeding it the exact same input. This can be mitigated by using a temperature of <code>0</code> and, for providers that support it, providing a seed for the RNG.</p>
<p>That said, code can often be written in many different but equally valid ways. So even when Zeta&#39;s output differs from our expected answer, it might still be doing exactly what we want—just taking a different path to get there. This makes traditional unit testing approaches particularly challenging when working with LLMs.</p>
<p>This led us to take a different approach—instead of strict assertions, we used a larger LLM to evaluate Zeta&#39;s edits. By writing our test assertions in plain English and having Claude check if the results matched our intent, we could validate that Zeta was making sensible edits, even when its exact output differed between runs. This ended up being much more practical than trying to make brittle assertions about specific tokens.</p>
<p>Here&#39;s an example taken from our eval suite:</p>
<figure data-rehype-pretty-code-figure=""><div><pre><code data-language="rs" data-theme="dark-plus light-plus"><span data-line=""><span>// Input:</span></span>
<span data-line=""><span>pub</span><span> fn</span><span> quicksort</span><span>&lt;</span><span>T</span><span>: </span><span>Ord</span><span>&gt;(</span><span>arr</span><span>: &amp;</span><span>mut</span><span> [</span><span>T</span><span>]) {</span></span>
<span data-line=""><span>    let</span><span> len</span><span> = </span><span>arr</span><span>.</span><span>len</span><span>();</span></span>
<span data-line=""><span>    if</span><span> len</span><span> &lt;= </span><span>1</span><span> {</span></span>
<span data-line=""><span>        return</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    let</span><span> pivot_index</span><span> = </span><span>partition</span><span>(</span><span>arr</span><span>);</span></span>
<span data-line=""><span>    &lt;|</span><span>user_cursor_is_here</span><span>|&gt;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>// Assertion: Ensure that the quicksort function recurses to the left and to the right of the pivot.</span></span></code></pre></div></figure>
<h3 id="prompt-engineering"><a href="#prompt-engineering" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Prompt Engineering</span></a></h3>
<p>We took our first stab at making those tests pass by using Qwen2.5-Coder-32B and giving it clear instructions for which types of edits we wanted it to predict. <a href="https://github.com/zed-industries/zed/blob/79a70b72b3968d102c6171f8bd2738ec7be8e94f/crates/zeta/src/complete_prompt.md">Here&#39;s the initial system prompt</a> we used and you can look through the history to see how we kept changing it to pass the eval suite.</p>
<p>This worked out surprisingly well for the first 4-5 evals. However, as soon as we introduced more, we started noticing that it got harder and harder to pass them all consistently. Changing the prompt caused the new evals to pass, but made the old ones fail. Overall, it felt like a flaky process and we didn&#39;t feel confident this would lead to the system being robust enough to be used in production.</p>
<p>Moreover, using a 32b model wasn&#39;t really compatible with our strict latency requirements (more on that later).</p>
<h3 id="supervised-fine-tuning"><a href="#supervised-fine-tuning" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Supervised Fine-Tuning</span></a></h3>
<p>After playing around with different approaches, we decided to go with supervised fine-tuning using <a href="https://unsloth.ai">Unsloth</a> and LoRA. The idea was to teach Zeta two key things: figuring out what changes a developer might want next based on their recent edits, and then actually applying those changes cleanly to the code without introducing weird side effects.</p>
<p>But we had a classic chicken-and-egg problem—we needed data to train the model, but we didn&#39;t have any real examples yet. So we started by having Claude generate about 50 synthetic examples that we added to <a href="https://huggingface.co/datasets/zed-industries/zeta-dataset">our dataset</a>. We then used that initial fine-tune to ship an early version of Zeta behind a feature flag and started collecting examples from our own team&#39;s usage.</p>
<p>This approach let us quickly build up a solid dataset of around 400 high-quality examples, which improved the model a lot! However, we kept running into edge cases that would trip the model up. The most annoying ones were when Zeta was working with a small piece of code in a larger file—it would sometimes get confused and make random deletions or insertions that had nothing to do with what the user was trying to do, and it didn&#39;t seem like adding more examples steered the model away from those mistakes.</p>
<h3 id="direct-preference-optimization"><a href="#direct-preference-optimization" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Direct Preference Optimization</span></a></h3>
<p>To handle these edge cases, we conducted another pass using direct preference optimization (DPO). This technique let us go beyond simply showing the model what good edits look like—we could also teach it what edits <em>to avoid</em>. With DPO, we could fine-tune Zeta by providing both positive and negative examples, helping it learn the subtle differences between helpful and problematic edits.</p>
<p>We found that just ~150 carefully selected examples were enough to significantly improve Zeta&#39;s behavior on tricky cases. Of course, we think we can make it even better by expanding our training data with more diverse examples, and we&#39;re excited to keep pushing the boundaries here.</p>
<h3 id="minimizing-latency-speculative-decoding"><a href="#minimizing-latency-speculative-decoding" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Minimizing Latency: Speculative Decoding</span></a></h3>
<p>Like every feature in Zed, latency was a critical factor for edit prediction. When we started, we set aggressive performance targets: predictions should be delivered in under 200ms for the median case (p50) and under 500ms for the 90th percentile (p90). The challenge was that rewriting complete excerpts, while enabling multi-location edits, requires generating significantly more tokens than simple fill-in-middle approaches. Initially, this put us way over our latency budget.</p>
<p>However, there&#39;s a fascinating insight about how edit predictions work. When we rewrite a text snippet, the output often mirrors the input closely, with changes concentrated in specific spots. This pattern lets us parallelize token generation by using the input as a reference—a technique known as speculative decoding. We use n-gram search to identify promising jumping-off points in the input where we can start parallel token generation, giving us a significant speedup without sacrificing quality.</p>
<h3 id="minimizing-latency-serving-the-model"><a href="#minimizing-latency-serving-the-model" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Minimizing Latency: Serving The Model</span></a></h3>
<p>For edit predictions to feel responsive, we needed to solve multiple latency challenges in parallel. As discussed above, we tackled the model execution time through speculative decoding, but serving the model at scale presented its own set of hurdles. This was by far the most compute-intensive problem our team has ever tackled.</p>
<p>A few weeks out from launch, we ran a brief competitive process, and we ended up being really impressed with <a href="https://www.baseten.co">Baseten</a>. Their performance engineers quickly optimized our open source model to run on their flexible infrastructure, achieving our target latencies while letting us retain full visibility into the details of the deployment, both for the Zed team and the entire Zed community. We plan to follow up with a guest post about what they learned optimizing <a href="https://huggingface.co/zed-industries/zeta">our model</a>.</p>
<p>Latency is not just a function of compute; network transit time is a key driver of perceived speed. To cooperate with the laws of physics, we&#39;re launching with GPUs in both North America and Europe, and we hope to add more regions soon. We&#39;re also using <a href="https://workers.cloudflare.com/">Cloudflare Workers</a> to handle your requests in a data center located as close to you as possible.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Copy heading link"><span><svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="4" x2="20" y1="9" y2="9"></line><line x1="4" x2="20" y1="15" y2="15"></line><line x1="10" x2="8" y1="3" y2="21"></line><line x1="16" x2="14" y1="3" y2="21"></line></svg></span><span>Conclusion</span></a></h2>
<p>There&#39;s plenty more to explore to make edit predictions more powerful. We&#39;ll be fast-following with more experiments. We plan on sending more kinds of context to the model and continuing our experiments with fine-tuning, and we&#39;ll share updates as we grow and evolve the Zeta dataset.</p>
<p>We&#39;ve learned a lot since we launched Zed AI last fall. The world is changing fast, and we&#39;re having a blast exploring and learning to build features that developers love. We&#39;re also excited to build with AI the Zed way. From our early days, we&#39;ve been proponents of an open approach to building software, even when hard, and we see no reason to change that approach when it comes to working with AI. We hope you&#39;ll join us as a user, a contributor, or an employee, as we hustle to ship a golden future.</p><hr/></article></div>
  </body>
</html>
