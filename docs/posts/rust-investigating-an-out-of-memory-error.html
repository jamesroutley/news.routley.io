<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.qovery.com/blog/rust-investigating-a-strange-out-of-memory-error/">Original</a>
    <h1>Rust: Investigating an Out of Memory Error</h1>
    
    <div id="readability-page-1" class="page"><div><p>At Qovery, we have a fairly small service called <strong>engine-gateway</strong> that handles connections and data transmission with our clients&#39; deployers. This service has a relatively low memory footprint, as its responsibilities are mainly authentication/authorization and streaming bytes forward into a data store for processing.</p><p>This engine-gateway has been running fine for months in production without any intervention at a memory usage &lt; 50Mib. Until one dayâ€¦</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4Za5JbqstJ99bLZ_oomrust.png?auto=format,compress" alt="Oops.. Out of Memory crash ðŸ˜¬"/></p></div></div><figcaption>Oops.. Out of Memory crash ðŸ˜¬</figcaption></figure><p>That day, we were automatically alerted on Slack that this service had been restarted abruptly and that someone must investigate why.</p><p>We reached the service dashboard, but nothing stood up. CPU, memory, network, threads, file descriptors, number of requestsâ€¦ all resources were checked before the crash. Also, there was nothing in the application logs just before; it was all as if the app had been restarted without any reason.</p><p>This service runs on Kubernetes, so we checked the pod&#39;s status and found out the last reason for the restart was an out-of-memory (OOM), meaning that the application used more memory than the limit we set.</p><p>But it felt strange because in our dashboard, the memory consumption of the engine gateway was pretty stable before the crash, and even in general, there was no spike. Also, no slow increase/march of death would indicate a memory leak.</p><p>So, to convince ourselves, we dug deeper and connected directly to the node to retrieve the kernel message regarding the OOM. And indeed, <strong>dmesg</strong> command was greeting us with this.</p><div><pre><code>tokio<span>-</span>runtime<span>-</span>w invoked oom<span>-</span>killer<span>:</span> gfp_mask<span>=</span><span>0</span><span>xcc0</span><span>(</span><span>GFP_KERNEL</span><span>)</span><span>,</span> order<span>=</span><span>0</span><span>,</span> oom_score_adj<span>=</span><span>-</span><span>997</span>
<span>CPU</span><span>:</span> <span>6</span> <span>PID</span><span>:</span> <span>1974311</span> <span>Comm</span><span>:</span> tokio<span>-</span>runtime<span>-</span>w Not tainted <span>5.10</span><span>.210</span><span>-</span><span>201.852</span><span>.</span>amzn2<span>.</span>x86_64 #<span>1</span>
Hardware name<span>:</span> Amazon <span>EC2</span> t3a<span>.</span>2xlarge<span>/</span><span>,</span> <span>BIOS</span> <span>1.0</span> <span>10</span><span>/</span><span>16</span><span>/</span><span>2017</span>
Call Trace<span>:</span>
 dump_stack<span>+</span><span>0x57</span><span>/</span><span>0x70</span>
 dump_header<span>+</span><span>0x4a</span><span>/</span><span>0x1f4</span>
 oom_kill_process<span>.</span>cold<span>+</span><span>0xb</span><span>/</span><span>0x10</span>
 out_of_memory<span>+</span><span>0xed</span><span>/</span><span>0x2d0</span>
 mem_cgroup_out_of_memory<span>+</span><span>0x138</span><span>/</span><span>0x150</span>
 try_charge<span>+</span><span>0x65d</span><span>/</span><span>0x6e0</span>
 mem_cgroup_charge<span>+</span><span>0x7f</span><span>/</span><span>0x240</span>
 do_anonymous_page<span>+</span><span>0xf0</span><span>/</span><span>0x520</span>
 __handle_mm_fault<span>+</span><span>0x499</span><span>/</span><span>0x640</span>
 handle_mm_fault<span>+</span><span>0xbe</span><span>/</span><span>0x2a0</span>
 do_user_addr_fault<span>+</span><span>0x1b3</span><span>/</span><span>0x3f0</span>
 exc_page_fault<span>+</span><span>0x68</span><span>/</span><span>0x130</span>
 <span>?</span> asm_exc_page_fault<span>+</span><span>0x8</span><span>/</span><span>0x30</span>
 asm_exc_page_fault<span>+</span><span>0x1e</span><span>/</span><span>0x30</span>
<span>RIP</span><span>:</span> <span>0033</span><span>:</span><span>0x7f6eabf85bd8</span>
<span>Code</span><span>:</span> ae c0 <span>13</span> <span>00</span> <span>31</span> d2 <span>48</span> 8d <span>34</span> <span>29</span> <span>48</span> <span>39</span> fb <span>48</span> <span>89</span> <span>73</span> <span>60</span> 0f <span>95</span> c2 <span>48</span> <span>29</span> e8 <span>48</span> c1 e2 <span>02</span> <span>48</span> <span>83</span> c8 <span>01</span> <span>48</span> <span>09</span> ea <span>48</span> <span>83</span> ca <span>01</span> <span>48</span> <span>89</span> <span>51</span> <span>08</span> <span>&lt;</span><span>48</span><span>&gt;</span> <span>89</span> <span>46</span> <span>08</span> <span>48</span> 8d <span>41</span> <span>10</span> <span>48</span> 8b <span>54</span> <span>24</span> <span>28</span> <span>64</span>
<span>RSP</span><span>:</span> 002b<span>:</span>00007f6eabedd110 <span>EFLAGS</span><span>:</span> <span>00010202</span>
<span>RAX</span><span>:</span> <span>0000000000000241</span> <span>RBX</span><span>:</span> 00007f6ea4000030 <span>RCX</span><span>:</span> 00007f6e8032cdb0
<span>RDX</span><span>:</span> <span>0000000000007015</span> <span>RSI</span><span>:</span> 00007f6e80333dc0 <span>RDI</span><span>:</span> 00007f6eac0c1c60
<span>RBP</span><span>:</span> <span>0000000000007010</span> <span>R08</span><span>:</span> <span>0000000000007251</span> <span>R09</span><span>:</span> 00007f6e80000000
<span>R10</span><span>:</span> <span>0000000000000000</span> <span>R11</span><span>:</span> 000000000032f000 <span>R12</span><span>:</span> <span>0000000000002250</span>
<span>R13</span><span>:</span> 00007f6e8032cdb0 <span>R14</span><span>:</span> <span>0000000000001000</span> <span>R15</span><span>:</span> <span>0000000000007030</span>
<span>memory</span><span>:</span> usage 262144kB<span>,</span> limit 262144kB<span>,</span> failcnt <span>57</span>
memory<span>+</span>swap<span>:</span> usage 262144kB<span>,</span> limit 9007199254740988kB<span>,</span> failcnt <span>0</span>
<span>kmem</span><span>:</span> usage 1272kB<span>,</span> limit 9007199254740988kB<span>,</span> failcnt <span>0</span>
Memory cgroup stats <span>for</span> <span>/</span>kubepods<span>.</span>slice<span>/</span>kubepods<span>-</span>pod7aef5bea_2046_431a_9237_7aa567fe652f<span>.</span>slice<span>:</span>
anon <span>266821632</span>
file <span>0</span>
kernel_stack <span>98304</span>
percpu <span>0</span>
sock <span>0</span>
shmem <span>0</span>
file_mapped <span>0</span>
file_dirty <span>0</span>
file_writeback <span>0</span>
anon_thp <span>0</span>
inactive_anon <span>266551296</span>
active_anon <span>0</span>
inactive_file <span>0</span>
active_file <span>0</span>
unevictable <span>0</span>
slab_reclaimable <span>131272</span>
slab_unreclaimable <span>0</span>
slab <span>131272</span>
workingset_refault_anon <span>0</span>
workingset_refault_file <span>0</span>
workingset_activate_anon <span>0</span>
workingset_activate_file <span>0</span>
workingset_restore_anon <span>0</span>
workingset_restore_file <span>0</span>
workingset_nodereclaim <span>0</span>
pgfault <span>128568</span>
pgmajfault <span>0</span>
pgrefill <span>0</span>
pgscan <span>35</span>
pgsteal <span>0</span>
pgactivate <span>0</span>
pgdeactivate <span>0</span>
pglazyfree <span>0</span>
pglazyfreed <span>0</span>
thp_fault_alloc <span>0</span>
thp_collapse_alloc <span>0</span>
Tasks <span>state</span> <span>(</span>memory values <span>in</span> pages<span>)</span><span>:</span>
<span>[</span>  pid  <span>]</span>   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
<span>[</span><span>1974229</span><span>]</span> <span>65535</span> <span>1974229</span>      <span>243</span>        <span>1</span>    <span>28672</span>        <span>0</span>          <span>-</span><span>998</span> pause
<span>[</span><span>1974298</span><span>]</span>  <span>1000</span> <span>1974298</span>      <span>583</span>      <span>225</span>    <span>36864</span>        <span>0</span>          <span>-</span><span>997</span> dumb<span>-</span>init
<span>[</span><span>1974310</span><span>]</span>  <span>1000</span> <span>1974310</span>   <span>154306</span>    <span>76896</span>   <span>741376</span>        <span>0</span>          <span>-</span><span>997</span> engine<span>-</span>gateway
oom<span>-</span>kill<span>:</span>constraint<span>=</span><span>CONSTRAINT_MEMCG</span><span>,</span>nodemask<span>=</span><span>(</span><span>null</span><span>)</span><span>,</span>cpuset<span>=</span>cri<span>-</span>containerd<span>-</span>ce4373c476b64aa312287f39ace9fdc4720c0ab83c4db76c035cb05ea7f32f7f<span>.</span>scope<span>,</span>mems_allowed<span>=</span><span>0</span><span>,</span>oom_memcg<span>=</span><span>/</span>kub
Memory cgroup out <span>of</span> <span>memory</span><span>:</span> Killed process <span>1974310</span> <span>(</span>engine<span>-</span>gateway<span>)</span> total<span>-</span>vm<span>:</span>617224kB<span>,</span> anon<span>-</span>rss<span>:</span>260492kB<span>,</span> file<span>-</span>rss<span>:</span>47092kB<span>,</span> shmem<span>-</span>rss<span>:</span>0kB<span>,</span> <span>UID</span><span>:</span><span>1000</span> <span>pgtables</span><span>:</span>724kB oom_score_a</code></pre></div><p>Letâ€™s unpack it a bit. The stack trace is reasonably normal for an OOM. Reading from bottom to top.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4Zbw5bqstJ99bMY_stacktrace1.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p><a href=" https://en.wikipedia.org/wiki/Memory_paging#Page_faults" target="_self" rel="noopener">This page</a> explains page faults and how they are linked to virtual memory in Linux.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4Zb1JbqstJ99bMi_stacktrace2.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p>The OOM trace gives some helpful information to narrow down the culprit.</p><ol><li>If we look at the breakdown by process, we see that the engine-gateway process is clearly using too much memory. <a href="https://stackoverflow.com/a/48654926" target="_self" rel="noopener">Pause</a> is an internal process for Kubernetes, and <a href="https://github.com/Yelp/dumb-init" target="_self" rel="noopener">dumb-init</a>, which helps manage signals, is not in cause.</li><li>The 2nd information is that the memory allocation is for anon(ymous) page, which is the kind of memory used by malloc/free standard libc allocator.</li></ol><p>This OOM trace proves that this is the code of our engine gateway that is allocating too much for some unknown reason. More importantly, our monitoring is hiding the truth. The memory surge is so quick that the process is instantly killed without our monitoring being able to report the increase. Our monitoring frequency is set at 10sec, so everything happens under those 10sec.</p><p>At this point, we were stuck investigating further. We are running this service with multiple instances, so the crash of one instance is not causing any outage. We control both ends of the service, so we donâ€™t expect any big payload that would cause such an abrupt spike, and clients are able to resume even after an error.</p><p>So without any traces to dig in deeper in the code, we decided to let the problem sit, as we were busy with other matters at the time. We only doubled the memory limit of the service in order to catch the issue live if it happens again.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4Zrw5bqstJ99baf_rustinserttoken.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p>Several weeks later, it happened again. This time, in spite of doubling the memory limit, the OOM propagated at the same time during the night to all running instances of the engine gateway, creating a real outageâ€¦ To add to the worst, we were left without much more information than the first time.</p><p>This time, we took the problem at hand. We increased the engine gateway&#39;s memory limit to 4Gib and changed our code to investigate the memory allocation issue.</p><p>By default, the libc memory allocator does not provide profiling information that is easily exploitable, so we changed to using the venerable jemalloc so that when we catch the issue live, we can investigate where the allocations are coming from.</p><p>For that, we used the crate <a href="https://www.polarsignals.com/blog/posts/2023/12/20/rust-memory-profiling" target="_self" rel="noopener">rust-jemalloc-pprof</a> to retrieve profiling information in pprofâ€™s format and generate a flame graph of allocations.</p><p>The change is pretty straightforward in Rust, and well explained in the article. Adding a crate and those lines is enough to use jemalloc in place of the default allocator.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4ZsMZbqstJ99ba3_rustmalloc.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p>We released everything, and one more time, we waited for blood to be spilledâ€¦</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4a5upbqstJ99dgL_lll.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p>This time, we got lucky. We only had to wait several days for the issue to arise again. We saw in our dashboard that the memory increased by more than 400Mib, only to discover that the root cause of the issue was this line of code, which only logged an error.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4a6EZbqstJ99dgP_singlelineofcodeoom.png?auto=format,compress" alt="The cause of our OOM"/></p></div></div><figcaption>The cause of our OOM</figcaption></figure><p>Looking at the profiling information of the allocation thanks to jemalloc, we can now clearly see what is happening.</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4a6LpbqstJ99dgR_profiling1.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4a6L5bqstJ99dgT_profiling2.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure><p>We use <strong>anyhow</strong> crate to manage errors in our application, and we see that calling the Debug implementation for anyhow::Error triggers the symbolization of a backtrace and caches it. The implementation is trying to resolve the symbols of a backtrace to give them human-readable names so that they can be printed.</p><p>But why? How? Our first surprise was to discover that when backtrace is enabled, the anyhow library is capturing one on every error created<strong>.</strong> The second surprise was that we use <strong>anyhow</strong> pretty much everywhere in this service, and it is not the single path in the code where we log an error. So why does this code path trigger a symbolization and lead to OOM? Because it is the single path where we print it in <strong>Debug</strong> and not in <strong>Display</strong>, so only this code path triggers the issueâ€¦</p><p>This line would not have caused the issue.</p><div><pre><code>error<span>!</span><span>(</span><span>&#34;Error while getting git token: {}&#34;</span><span>,</span> err<span>)</span><span>;</span></code></pre></div><p>But this line, yes</p><div><pre><code>error<span>!</span><span>(</span><span>&#34;Error while getting git token: {:?}&#34;</span><span>,</span> err<span>)</span><span>;</span></code></pre></div><p>see the issue? Pretty subtle</p><p>After looking at <strong>anyhow</strong> issues, we discovered that it was a <a href="https://github.com/dtolnay/anyhow/issues/342" target="_self" rel="noopener">common pitfall</a> and that it is even mentioned in the documentation, but with unclear consequences for the profane</p><div><pre><code>If using Rust â‰¥ <span>1.65</span><span>,</span> a backtrace is captured and printed <span>with</span> the error <span>if</span> the underlying error type does not already provide its own<span>.</span> In order to see backtraces<span>,</span> they must be enabled through the environment variables described <span>in</span> <span>std</span><span>:</span><span>:</span>backtrace<span>:</span>
    If you want panics and errors to both have backtraces<span>,</span> <span>set</span> <span>RUST_BACKTRACE</span><span>=</span><span>1</span><span>;</span>
    If you want only errors to have backtraces<span>,</span> <span>set</span> <span>RUST_LIB_BACKTRACE</span><span>=</span><span>1</span><span>;</span>
    If you want only panics to have backtraces<span>,</span> <span>set</span> <span>RUST_BACKTRACE</span><span>=</span><span>1</span> and <span>RUST_LIB_BACKTRACE</span><span>=</span><span>0.</span></code></pre></div><p>So the fix was easy, not even involving changing code, as we are only interested in backtraces during panic; we restarted the engine-gateway with this extra environment variable <strong>RUST_LIB_BACKTRACE=0</strong></p><h2 id="conclusion"><a href="#conclusion"><span role="button" tabindex="0">#</span>Conclusion</a></h2><p>To conclude:</p><ul><li>Your monitoring can lie to you if events happen in between its sampling frequency</li><li>The documentation you donâ€™t understand is going to bite you in the future</li><li>Surprisingly, anyhow/eyre library is leaning by default to capture a backtrace on every error when not explicitly disabled</li></ul><p>VoilÃ ! I hope it helps</p><figure><div><div aria-owns="rmiz-modal-" data-rmiz=""><p><img loading="lazy" src="https://images.prismic.io/qovery/Z4a7DpbqstJ99dgh_theend.png?auto=format,compress"/></p></div></div><figcaption></figcaption></figure></div><div><div><h2>Your Favorite DevOps Automation Platform</h2><p>Qovery is a DevOps Automation Platform Helping 200+ Organizations To Ship Faster and Eliminate DevOps Hiring Needs</p><p><a href="https://start.qovery.com">Try it out now!</a></p></div><p><img src="https://eieio.games/images/img-started-in-seconds.svg" alt="Your Favorite DevOps Automation Platform"/></p></div></div>
  </body>
</html>
