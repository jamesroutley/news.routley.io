<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gwern.net/gpt-3">Original</a>
    <h1>GPT-3 Creative Fiction (2020)</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata">
        <p>Creative writing by OpenAI’s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling. Plus advice on effective GPT-3 prompt programming &amp; avoiding common errors.</p>
        
        
      </div><div id="markdownBody">
        <div>
          <blockquote>
            <p><span>I continue my AI poetry generation experiments with Open<span><span>2020</span></span> GPT-3, which is 116× larger, and much more powerful, than the 2019</span> <a href="https://jvns.ca/doc/ai/nn/transformer/gpt/2019-radford.pdf#openai" id="gpt-2-paper" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Language Models are Unsupervised Multitask Learners&#39;, Radford et al 2019">GPT-2</a>⁠. GPT-3, however, is not merely a quantitative tweak yielding “GPT-2 but better”—it is qualitatively different, exhibiting eerie runtime learning capabilities allowing even the raw model, with zero finetuning, to “meta-learn” many textual tasks purely by example or instruction. One does not train or program GPT-3 in a normal way, but one engages in dialogue and writes prompts to teach GPT-3 what one wants.</p>
            <p>Experimenting through the <a href="https://en.wikipedia.org/wiki/OpenAI" data-link-icon="wikipedia" data-link-icon-type="svg">OpenAI</a> Beta API in June 2020, I find that GPT-3 does not just match my <a href="https://jvns.ca/gpt-2#gpt-2-1-5b" id="gwern-gpt-2-gpt-2-1-5b">finetuned GPT-2-1.5b-poetry</a> for poem-writing quality, but exceeds it, while being versatile in handling <a href="#poetry">poetry</a>⁠, <a href="#tom-swifties">Tom Swifty puns</a>⁠, science fiction, dialogue like Turing’s <a href="#turing-dialogue">Turing-test dialogue</a>⁠, <a href="#literary-parodies">literary style parodies</a>… As the <em>pièce de résistance</em>, I recreate Stanislaw Lem’s <em>Cyberiad</em>’s <a href="#stanislaw-lems-cyberiad">“Trurl’s Electronic Bard”</a> poetry using GPT-3. (Along the way, I document instances of how the BPE text encoding <a href="#bpes">unnecessarily damages</a> GPT-3’s performance on a variety of tasks, how to best elicit the highest-quality responses, common errors people make in using GPT-3, and test out GPT-3’s improvements in NN weak points like logic or commonsense knowledge.)</p>
            <p>GPT-3’s samples are not just close to human level: they are creative, witty, deep, meta, and often beautiful. They demonstrate an ability to handle abstractions, like style parodies, I have not seen in GPT-2 at all. Chatting with GPT-3 feels uncannily like chatting with a human. I was impressed by the results reported in the GPT-3 paper, and after spending a week trying it out, I remain impressed.</p>
            <p>This page records GPT-3 samples I generated in my explorations, and thoughts on <a href="#prompts-as-programming">how to use GPT-3</a> and its remaining <a href="#weaknesses">weaknesses</a>⁠. I hope you enjoy them even a tenth as much as I enjoyed testing GPT-3 and watching the completions scroll across my screen.</p>
          </blockquote>
        </div>
        <p>The latest and greatest neural network for unrestricted natural language generation is OpenAI’s <a href="https://jvns.ca/doc/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#openai" id="brown-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2005.14165?fallback=original#openai" title="&#39;GPT-3: Language Models are Few-Shot Learners&#39;, Brown et al 2020 (Original URL: https://arxiv.org/abs/2005.14165#openai )">GPT-3</a>⁠. GPT-3 is like <a href="https://openai.com/blog/language-unsupervised/" id="openai-2018" data-link-icon="openai" data-link-icon-type="svg" title="GPT-1: Improving Language Understanding with Unsupervised Learning [June 2018]">GPT-1</a> and the <a href="https://openai.com/blog/better-language-models/" id="gpt-2-blog" data-link-icon="openai" data-link-icon-type="svg" title="Better Language Models and Their Implications">GPT-2</a> I’ve used extensively before<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>—only much more so, and then going beyond them in a fascinating new way.</p>
        <p><span>Scaling works: quantity is a quality all its own.</span> The scaling of GPT-2-1.5b by 116× to GPT-3-175b has worked surprisingly well and unlocked remarkable flexibility in the form of <em>meta-learning</em>, where GPT-3 can infer new patterns or tasks and follow instructions purely from text fed into it. What can we do with GPT-3? Here, we’re all about having fun while probing GPT-3’s abilities for creative writing tasks, primarily (but far from limited to) poetry. Fortunately, OpenAI granted me access to their Beta API service which provides a hosted GPT-3 model, letting me spend a great deal of time interacting with GPT-3 and writing things. Naturally, I’d like to write poetry with it: but GPT-3 is too big to finetune like I did GPT-2, and OA doesn’t (yet) support any kind of training through their API. Must we content ourselves with mediocre generic poetry, at best, deprived of finetuning directly on chosen poetry corpuses or authors we might like to parody? How much does GPT-3 improve and what can it do?</p>
        <p>Turns out: a lot! Below, I walk through first impressions of using GPT-3, and countless samples. In the latest twist on <a href="https://en.wikipedia.org/wiki/Moravec&#39;s_paradox" data-link-icon="wikipedia" data-link-icon-type="svg">Moravec’s paradox</a>⁠, GPT-3 still struggles with commonsense reasoning &amp; factual knowledge of the sort a human finds effortless after childhood, but handles well things like satire &amp; fiction writing &amp; poetry, which we humans find so difficult &amp; impressive even as adults. In addition to the <a href="#stanislaw-lems-cyberiad"><em>Cyberiad</em></a>⁠, I’d personally highlight the <a href="#navy-seal-copypasta-parodies">Navy Seal</a> &amp; <a href="#literary-parodies"><em>Harry Potter</em></a> parodies, the <a href="#devils-dictionary-of-science">Devil’s Dictionary of Science / Academia</a>⁠, <a href="#uber-poem">“Uber Poem”</a>⁠, <a href="#the-universe-is-a-glitch">“The Universe Is a Glitch”</a> poem (with AI-generated rock music version), &amp; <a href="#shel-silverstein">“Where the Sidewalk Ends”</a>⁠.</p>
        <section id="what-benchmarks-miss-demos">
          <h2><a href="#what-benchmarks-miss-demos" title="Link to section: § &#39;What Benchmarks Miss: Demos&#39;">What Benchmarks Miss: Demos</a></h2>
          
          <p>The GPT-3 paper includes evaluation of zero-shot/​few-shot performance across a wide range of tasks, but I fear that unless one is familiar with the (deadly dull) benchmarks in question, it won’t be impressive. You can skip to the appendix for more example like its <a href="https://jvns.ca/doc/www/arxiv.org/662ef6c17690db4b6703318f0e1d6829a8ff47be.pdf#page=48" id="gpt-3-2020-page-48" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#page=48" title="Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title &#39;Shadows on the Way&#39; (Original URL: https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#page=48 )">poems</a>⁠, or browse the <a href="https://jvns.ca/doc/www/justpaste.it/b5a07c7305ca81b0de2d324f09445f9ef407c17e.html" rel="archived alternate nofollow" data-url-original="https://justpaste.it/7eovk" title="GPT-3 Github JSON dump reformatted to readable HTML (Original URL: https://justpaste.it/7eovk )">random samples</a>⁠.</p>
          <p>The original <a href="https://beta.openai.com/" id="openai-2020" data-link-icon="openai" data-link-icon-type="svg" title="&#39;OpenAI API Beta homepage&#39;, OpenAI 2020">OpenAI Beta API homepage</a> includes many striking examples of GPT-3 capabilities ranging from chatbots to question-based Wikipedia search to legal discovery to homework grading to translation; I’d highlight <a href="https://play.aidungeon.io/main/home" id="walton-2019-aidungeon-2" data-link-icon="AID" data-link-icon-type="text,tri,sans" title="&#39;AI Dungeon 2&#39;, Walton 2019">AI Dungeon’s</a> <a href="https://jvns.ca/doc/www/aidungeon.medium.com/52d2fe5633e74d1f355221dba088b17ff34db79d.html" data-link-icon="AID" data-link-icon-type="text,tri,sans" rel="archived alternate nofollow" data-url-original="https://aidungeon.medium.com/ai-dungeon-dragon-model-upgrade-7e8ea579abfe" title="AI Dungeon: Dragon Model Upgrade - You can now play AI Dungeon with one of the most powerful AI models in the world. (Original URL: https://aidungeon.medium.com/ai-dungeon-dragon-model-upgrade-7e8ea579abfe )">Dragon model</a> (<a href="https://nitter.moomoo.me/nickwalton00/status/1280972636638437382" data-link-icon="twitter" data-link-icon-type="svg" title="I&#39;ve been testing the largest of @OpenAI&#39;s models with AI Dungeon and been constantly impressed at how interesting and dynamic the characters are, like this queen, long thought to be dead, hiding from enemies and not happy about me prying into her personal life.">example</a> before the March 2021 meltdown), and <a href="https://player.vimeo.com/video/427943452" data-link-icon="file-video" data-link-icon-type="svg">“Spreadsheets”</a>⁠/ ​“Natural Language Shell”/​<a href="https://cdn.openai.com/API/English_Bash_Python.mp4" data-link-icon="openai" data-link-icon-type="svg">“Code Completion”</a><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>⁠. <a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/f45927dd03e04af6a6619c0ddaadf541ac50b89b.html" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/06/11/the-aichannels-project/" title="The AI Channels project (Original URL: https://andrewmayneblog.wordpress.com/2020/06/11/the-aichannels-project/ )">Andrew Mayne</a> describes using GPT-3 to generate book recommendation lists &amp; read interactive stories &amp; <a href="https://jvns.ca/doc/www/www.aiwriter.email/efbe704627975f595dd533f21ed5f6e1fa4947ee.html" rel="archived alternate nofollow" data-url-original="https://www.aiwriter.email/" title="&#39;AI|Writer&#39;: an AI | Channels project by @AndrewMayne using the OpenAI API; &#39;AI|Writer&#39; is an experiment using artificial intelligence to create simulated hypothetical correspondence with famous personalities, both real and fictitious (Original URL: https://www.aiwriter.email/ )">engage in conversations with historical figures</a> like Ada Lovelace<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>⁠, <a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/4691181b0cc65c6b92f3845e5ae23800b59ce293.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/06/13/openai-api-alchemy-summarization/" title="OpenAI API Alchemy: Summarization (Original URL: https://andrewmayneblog.wordpress.com/2020/06/13/openai-api-alchemy-summarization/ )">summarize texts</a> for elementary school children (also available as a service now, <a href="https://www.simplify.so/" title="&#39;Simplify: Simple, easy-to-understand explanations for everything&#39;, Chris Lu">Simplify.so</a>) or as <a href="https://nitter.moomoo.me/Wattenberger/status/1412480516268437512" data-link-icon="twitter" data-link-icon-type="svg" title="really liking this change in layout for my &#39;writing flow&#39; tool. it uses GPT-3 to summarize paragraphs as you&#39;re writing - giving you an abbreviated version on the right which is easily skimmable &amp; re-orderable">a writing assistant</a>⁠, or <a href="https://jvns.ca/doc/www/aidungeon.medium.com/a59cbaef353e80c7a652efbb61e7bd6edb76f4e4.html" data-link-icon="AID" data-link-icon-type="text,tri,sans" rel="archived alternate nofollow" data-url-original="https://aidungeon.medium.com/introducing-ai-dungeon-translate-a50e35f6df83" title="Introducing AI Dungeon Translate: AI Dungeon players can now translate their stories into emojis by just clicking a button. [ 🤔 💯 🤷‍♂️ 🤔 🤔 🤔 💯] (Original URL: https://aidungeon.medium.com/introducing-ai-dungeon-translate-a50e35f6df83 )">summarize AI Dungeon</a>⁠/ ​<a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/b8b95de4a6c86372a8b4c9c96850ad964601232b.html" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/06/24/open-ai-alchemy-emoji-storytelling/" title="OpenAI API Alchemy: Emoji storytelling 🤖 (Original URL: https://andrewmayneblog.wordpress.com/2020/06/24/open-ai-alchemy-emoji-storytelling/ )">movies in emoji</a> (<em>Matrix</em>: “🤖🤐”; <em>Hunger Games</em>: “🏹🥊🌽🏆”; see also <a href="https://jvns.ca/doc/www/arxiv.org/51596547adf19b2e4a09bfd143a2c0ca7f36666a.pdf#deepmind" id="tsimpoukelli-et-al-2021" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2106.13884?fallback=original#deepmind" title="Multimodal Few-Shot Learning with Frozen Language Models (Original URL: https://arxiv.org/abs/2106.13884#deepmind )"><span><span title="et al">Tsimpoukelli</span> <span>et al</span> <span>2021</span></span></a>), convert <a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/8c90684d9a498da7103cb8d1c457b4f8309f49be.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/07/08/openai-api-alchemy-turn-a-script-into-a-novel-and-vice-versa/" title="OpenAI API Alchemy: Turn a script into a novel (and vice versa) (Original URL: https://andrewmayneblog.wordpress.com/2020/07/08/openai-api-alchemy-turn-a-script-into-a-novel-and-vice-versa/ )">screenplay ↔︎ story</a>⁠, <a href="https://jvns.ca/doc/www/gpt3demo.com/3052c54264631900e7a31ced4956df872a579f46.html" rel="archived alternate nofollow" data-url-original="https://gpt3demo.com/apps/magicemail-io" title="Say goodbye to painful email reading and writing: Magic Email is your AI-powered email assistant that summarises your emails and generates professional emails from brief one-line descriptions. Get through all of your emails 5x faster so you can free up more time for your important work. (Original URL: https://gpt3demo.com/apps/magicemail-io )">summarize / write emails</a>⁠, translate from <a href="https://nitter.moomoo.me/michaeltefula/status/1285505897108832257" data-link-icon="twitter" data-link-icon-type="svg">legalize</a>⁠, and <a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/73d13e62ecd3521f2c895f0c8172d544018a9bc8.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/06/13/openai-api-alchemy-smart-formatting-and-code-creation/" title="OpenAI API Alchemy: Smart Formatting and Code Creation (Original URL: https://andrewmayneblog.wordpress.com/2020/06/13/openai-api-alchemy-smart-formatting-and-code-creation/ )">rewrite HTML</a>⁠. <a href="https://nitter.moomoo.me/paraschopra/status/1284801028676653060" data-link-icon="twitter" data-link-icon-type="svg" title="I made a fully functioning search engine on top of GPT3. For any arbitrary query, it returns the exact answer AND the corresponding URL. Look at the entire video. It&#39;s MIND BLOWINGLY good.">Paras Chopra</a> finds that GPT-3 knows enough Wikipedia &amp; other URLs that the basic Q&amp;A behavior can be augmented to include a ‘source’ URL, and so one can make a knowledge base ‘search engine’ with clickable links for any assertion (ie. the user can type in “What year was Richard Dawkin’s <em>The Selfish Gene</em> published?” and GPT-3 will return a tuple like <code>(&#34;The Selfish Gene was published in 1976&#34;,&#34;https://en.wikipedia.org/wiki/The_Selfish_Gene&#34;)</code> which can be parsed &amp; presented as a search engine). <a href="https://nitter.moomoo.me/stuhlmueller/status/1315386908415987712" data-link-icon="twitter" data-link-icon-type="svg" title="Interactive decomposition of forecasting questions using GPT-3. All questions auto-generated. Part of our work on tools for thought @oughtinc.">Andreas Stuhlmüller</a> explored using it to create suggestions for predicting on by breaking down high-level forecasting questions. <a href="https://jvns.ca/doc/www/arxiv.org/b623df892b51cfc567329d9e53ab1af17a8740ad.pdf" id="hendrycks-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2008.02275?fallback=original" title="ETHICS: Aligning AI With Shared Human Values (Original URL: https://arxiv.org/abs/2008.02275 )"><span><span title="et al">Hendrycks</span> <span>et al</span> <span>2020</span></span></a> tests few-shot GPT-3 on common moral reasoning problems, and while it doesn’t do nearly as well as a finetuned <a href="https://jvns.ca/doc/www/arxiv.org/2f462f2a5c6cdf830d4aa9ee9237be6db0da8fde.pdf#google" id="lan-et-al-2019" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1909.11942?fallback=original#google" title="&#39;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&#39;, Lan et al 2019 (Original URL: https://arxiv.org/abs/1909.11942#google )">ALBERT</a> overall, interestingly, its performance degrades the least on the problems constructed to be hardest.</p>
          <p><a href="https://en.wikipedia.org/wiki/Ryan_North" data-link-icon="wikipedia" data-link-icon-type="svg">Ryan North</a> experimented with <a href="https://nitter.moomoo.me/jephjacques/status/1279537349974732800" data-link-icon="twitter" data-link-icon-type="svg">Crunchyroll anime</a>⁠, <a href="https://nitter.moomoo.me/StarTrekAI" data-link-icon="twitter" data-link-icon-type="svg"><em>Star Trek: The Next Generation</em></a>⁠, &amp; <a href="https://nitter.moomoo.me/ryanqnorth/status/1280501755067981826" data-link-icon="twitter" data-link-icon-type="svg" title="Unlike that OTHER guy who just wrote silly things and lied to pass them off as the work of an AI, I actually DID get the GPT-3 language model to generate new Seinfeld scripts. Behold: 4 New Puffy Shirt Episodes. (The first 5 lines are canon, the rest new)"><em>Seinfeld</em></a> plot summaries. <a href="https://github.com/minimaxir/gpt-3-experiments/tree/master/examples" data-link-icon="github" data-link-icon-type="svg">Max Woolf has a repo</a> of GPT-3 example prompts &amp; various completions such as the original GPT-2 “unicorn” article, <em>Revenge of the Sith</em>, Stack Overflow Python questions, and his own tweets (note that many samples are bad because the prompts &amp; hyperparameters are often deliberately bad, eg. the temperature = 0 samples, to demonstrate the large effect of poorly-chosen settings <a href="#prompts-as-programming">as a warning</a>). <a href="https://jvns.ca/doc/www/www.aiweirdness.com/22435625719a0806d8474097cd0740ce131d6684.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://www.aiweirdness.com/this-is-the-openai-api-it-makes-spookily-20-06-11/" title="This is the OpenAI API. It makes spookily good twitter bots 13⁄10 would retweet (Original URL: https://www.aiweirdness.com/this-is-the-openai-api-it-makes-spookily-20-06-11/ )">Janelle Shan experimented with weird dog descriptions</a> to accompany deformed <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" data-link-icon="wikipedia" data-link-icon-type="svg">GAN</a>-dog samples, and <a href="https://www.aiweirdness.com/a-10000-year-warning-20-09-17/" title="A 10,000 year warning">10,000-year nuclear waste warnings</a> based on the famous <a href="https://jvns.ca/doc/technology/1993-trauth.pdf" id="trauth-et-al-1993" data-link-icon="pdf" data-link-icon-type="svg" title="Expert judgment on markers to deter inadvertent human intrusion into the Waste Isolation Pilot Plant">1993 Sandia report</a> on <a href="https://en.wikipedia.org/wiki/Long-term_nuclear_waste_warning_messages" data-link-icon="wikipedia" data-link-icon-type="svg">long-term nuclear waste warning messages</a> for the <a href="https://en.wikipedia.org/wiki/Waste_Isolation_Pilot_Plant" data-link-icon="wikipedia" data-link-icon-type="svg">Waste Isolation Pilot Plant</a>⁠. Summers-Stay tried <a href="https://old.reddit.com/r/slatestarcodex/comments/hmu5lm/fiction_by_neil_gaiman_and_terry_pratchett_by_gpt3/" data-link-icon="reddit" data-link-icon-type="svg" title="Fiction by Neil Gaiman and Terry Pratchett by GPT-3">imitating Neil Gaiman &amp; Terry Pratchett short stories</a> with excellent results. Arram Sabetti has done <a href="https://jvns.ca/doc/www/arr.am/030a0ac1061c2e83dd6468710c7e9754d66db369.html" rel="archived alternate nofollow" data-url-original="https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-writing-almost-anything/" title="GPT-3: An AI that&#39;s eerily good at writing almost anything (Original URL: https://arr.am/2020/07/09/gpt-3-an-ai-thats-eerily-good-at-writing-almost-anything/ )">“songs, stories, press releases, guitar tabs, interviews, essays, and technical manuals”</a>⁠, with his <a href="https://jvns.ca/doc/www/arr.am/fd2718184654b9ddd78a0236f149f2244c3692ee.html" rel="archived alternate nofollow" data-url-original="https://arr.am/2020/07/14/elon-musk-by-dr-seuss-gpt-3/" title="Elon Musk By Dr. Seuss (GPT-3) (Original URL: https://arr.am/2020/07/14/elon-musk-by-dr-seuss-gpt-3/ )">Elon Musk Dr. Seuss poems</a> a particular highlight. Salahuddin got <a href="https://pickledbrains.substack.com/p/a-wild-adventure-with-gpt-3" id="salahuddin-2021" data-link-icon="substack" data-link-icon-type="svg" title="A Wild Adventure With GPT-3: Featuring Indian Mythology and Neruda">great results</a> imitating Pablo Neruda’s poetry, as did Brundage with <a href="https://nitter.moomoo.me/Miles_Brundage/status/1476422768467972104" id="brundage-2021" data-link-icon="twitter" data-link-icon-type="svg" title="&#39;Apropos of nothing&#39;, Brundage 2021">Walt Whitman</a>⁠. <a href="https://www.youtube.com/watch?v=7Y5KsN6ehvk" data-link-icon="youtube" data-link-icon-type="svg">Paul Bellow (LitRPG)</a> experiments with RPG backstory generation. <a href="https://jvns.ca/doc/www/medium.com/6ed1e60ef1b2215845b3ae6a6726af97f2d3537a.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/merzazine/love-letters-written-by-a-toaster-e9e795c6409f" title="Love Letters, written by a Toaster. The poetic power of Artificial Intelligence (GPT-3) (Original URL: https://medium.com/merzazine/love-letters-written-by-a-toaster-e9e795c6409f )">Merzmensch Kosmopol</a> enjoyed generating love letters written by a toaster. <a href="https://jvns.ca/doc/www/jamesyu.org/b0d639c4ec1bcf237375f7c6cf00d9bbaf30e8a9.html" id="yu-gpt-3-2020" rel="archived alternate nofollow" data-url-original="https://jamesyu.org/singular/" title="Singular: Possible futures of the singularity (Original URL: https://jamesyu.org/singular/ )">James Yu</a> co-wrote a SF <a href="https://en.wikipedia.org/wiki/Technological_singularity" data-link-icon="wikipedia" data-link-icon-type="svg">Singularity</a> short story with GPT-3, featuring regular meta sidenotes where he &amp; GPT-3 debate the story in-character; it was exceeded in popularity by Pamela Mishkin’s <a href="https://jvns.ca/doc/www/pudding.cool/e57069bdc902cbfe9ab0820992d3c7060db1fe42.html" rel="archived alternate nofollow" data-url-original="https://pudding.cool/2021/03/love-and-ai/" title="An essay about artificial intelligence, emotional intelligence, and finding an ending (Original URL: https://pudding.cool/2021/03/love-and-ai/ )">“Nothing Breaks Like A.I. Heart”</a> which went full Choose-Your-Own-Adventure. <a href="https://nitter.moomoo.me/danielbigham/status/1295864369713209351" data-link-icon="twitter" data-link-icon-type="svg">Daniel Bigham</a> plays what he dubs “19 <a href="https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon" data-link-icon="wikipedia" data-link-icon-type="svg">degrees of</a> <a href="https://en.wikipedia.org/wiki/Kevin_Bacon" data-link-icon="wikipedia" data-link-icon-type="svg">Kevin Bacon</a>” which links Mongolia to (eventually) Kevin Bacon. <a href="https://jvns.ca/doc/www/areben.com/4c9a1d80ea9a48a0c4639fe6db927fe2d7ba0182.html" rel="archived alternate nofollow" data-url-original="https://areben.com/project/ai-am-i/" title="AI Am I? (The New Aesthetic) (Original URL: https://areben.com/project/ai-am-i/ )">Alexander Reben</a> prompted for contemporary art/​sculpture descriptions, and physically created some of the ones he liked best using a variety of mediums like matchsticks, toilet plungers, keys, collage, etc. <a href="https://nitter.moomoo.me/TomerUllman/status/1363851329463087109" data-link-icon="twitter" data-link-icon-type="svg">Tomer Ullman</a> prompted GPT-3 for new philosophy thought experiments. And <a href="https://jvns.ca/doc/www/i.reddit.com/c626fd4e47804661da1d989867c2a7a961a64072.html" data-link-icon="reddit" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/aigreentext/" title="(Original URL: https://old.reddit.com/r/aigreentext/ )"> / r / aigreentext</a> stems from the serendipitous discovery that GPT-3 is amazingly good at imitating 4chan-style <a href="https://knowyourmeme.com/memes/greentext-stories" data-link-icon="KYM" data-link-icon-type="text,tri">“green text”</a> stories &amp; that the OA Playground interface colors generated text green, so screenshots of real &amp; prompted green text stories look similar.</p>
          <p><a href="https://nitter.moomoo.me/hturan/status/1282381985642614790" data-link-icon="twitter" data-link-icon-type="svg" title="GPT-3 generated these color scales, given some existing scales and a hue name (or emoji‽) as a prompt. Let that sink in.">Harley Turan</a> found that, somehow, GPT-3 can associate plausible <a href="https://en.wikipedia.org/wiki/Web_colors" data-link-icon="wikipedia" data-link-icon-type="svg">color</a> hex codes with specific emoji (apparently <a href="https://jvns.ca/doc/www/arxiv.org/59e9e02cc7a45b085586ebcfb2a4b725a2fca458.pdf" id="abdou-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2109.06129?fallback=original" title="&#39;Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color&#39;, Abdou et al 2021 (Original URL: https://arxiv.org/abs/2109.06129 )">language models can learn color</a> from language, much like <a href="https://www.pnas.org/doi/10.1073/pnas.2020192118" id="kim-et-al-2021" data-link-icon="PNAS" data-link-icon-type="text,quad" title="&#39;Shared understanding of color among sighted and blind adults&#39;, Kim et al 2021">blind humans do</a>). Even more perplexingly, <a href="https://nitter.moomoo.me/sharifshameem/status/1282676454690451457" data-link-icon="twitter" data-link-icon-type="svg">Sharif Shameem</a> discovered that GPT-3 could write <a href="https://en.wikipedia.org/wiki/JSX_(JavaScript)" data-link-icon="wikipedia" data-link-icon-type="svg">JSX</a> (a Javascript+CSS hybrid) according to a specification like “5 buttons, each with a random color and number between 1–10” or <a href="https://nitter.moomoo.me/sharifshameem/status/1284095222939451393" data-link-icon="twitter" data-link-icon-type="svg" title="I just built a *functioning* React app by describing what I wanted to GPT-3. I&#39;m still in awe.">increase / decrease a balance in React</a> or a <a href="https://nitter.moomoo.me/sharifshameem/status/1284421499915403264" data-link-icon="twitter" data-link-icon-type="svg" title="I built a todo list app simply by describing it to GPT-3. It generated the React code for a fully functioning app within seconds. I&#39;m becoming more impressed and aware of its capabilities every single day.">very simple to-do list</a> and it would often work, or require relatively minor fixes. He also demonstrated a divide-and-conquer approach to making GPT-3 <a href="https://nitter.moomoo.me/sharifshameem/status/1405462642936799247" data-link-icon="twitter" data-link-icon-type="svg" title="I gave GPT-3 access to Chrome with the objective &#39;please buy me Airpods&#39;...It successfully made it to the product page, but got sidetracked with Walmart&#39;s privacy policy. Since even a simplified DOM is far too large for a single prompt, multiple prompts are given different chunks of the DOM, each generating their own &#39;interaction&#39;. Another prompt then takes all the proposed interactions and selects the best one, sort of like a tournament bracket. For more complex web pages, the time it takes to generate an action scales at 𝒪 (log n) with the size of the DOM—really fast! It also gets around token limits, so you could technically process an infinitely large DOM!">‘control’ a web browser</a>⁠. GPT-3 can also write some <a href="https://nitter.moomoo.me/fabinrasheed/status/1284052438392004608" data-link-icon="twitter" data-link-icon-type="svg" title="First work with #GPT3, I asked it to draw an image. I gave it seed SVG code and asked it to generate an SVG code by itself. Turns out it drew something resembling a Floppy Disk.">simple SVG</a> shapes or <a href="https://nitter.moomoo.me/aquariusacquah/status/1285415144017797126" data-link-icon="twitter" data-link-icon-type="svg" title="GPT-3 Does The Work™️ on generating SVG charts, with a quick web app I built with @billyjeanbillyj. With a short sentence describing what you want to plot, its able to generate charts with titles, labels and legends from about a dozen primed examples.It works by compiling the sentences to vega-lite (@vega_vis) by @arvindsatya1, @kanitw, @domoritz, and Jeffery Heer. Vega a high level grammar of interactive graphics built for exploratory data visualization.">SVG</a>⁠/ ​<a href="https://nitter.moomoo.me/nutanc/status/1285436266276745221" data-link-icon="twitter" data-link-icon-type="svg" title="Starting the day with a chart building demo. Primed GPT-3 with Chart.js scripts to generate the below.">Chart.js bar graphs</a>⁠, do <a href="https://nitter.moomoo.me/sh_reya/status/1284746918959239168" data-link-icon="twitter" data-link-icon-type="svg" title="After many hours of retraining my brain to operate in this &#34;priming&#34; approach, I also now have a sick GPT-3 demo: English to LaTeX equations! I&#39;m simultaneously impressed by its coherence and amused by its brittleness---watch me test the fundamental theorem of calculus.">text→<span>L<span>a</span>T<span>e</span>X</span></a> and <a href="https://nitter.moomoo.me/aquariusacquah/status/1284706786247880705" data-link-icon="twitter" data-link-icon-type="svg" title="GPT-3 Does The Work™ on some business analyst SQL queries given quite a few examples from (https://techbeamers.com/sql-query-questions-answers-for-practice/). What&#39;s wildest is that it knows a few functions like SUBSTR given no examples in that syntax. More to come re: GPT-3 for automating data analytics tasks.">SQL</a> <a href="https://jvns.ca/doc/www/blog.seekwell.io/b2d5f5a031a9090ba3a28e8c4a782f9b4a9dda6c.html" rel="archived alternate nofollow" data-url-original="https://blog.seekwell.io/gpt3" title="Automating My Job with GPT-3: Using GPT-3 Instruct to generate database-ready SQL to answer business questions (Original URL: https://blog.seekwell.io/gpt3 )">queries</a>⁠, and <a href="https://www.lesswrong.com/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of" data-link-icon="LW" data-link-icon-type="text">match <em>k</em>-NN &amp; do regression</a> on toy datasets. While I don’t think programmers need worry about unemployment (NNs will be a complement until they are <em>so</em> good they are a substitute), the code demos are impressive in illustrating just how diverse the skills created by pretraining on the Internet can be. Particularly intriguing in terms of code generation is its ability to <a href="https://www.autoregex.xyz/">write regexps from English descriptions</a>⁠, and <a href="https://nitter.moomoo.me/jsngr/status/1284511080715362304" data-link-icon="twitter" data-link-icon-type="svg" title="This changes everything. :Exploding_head: With GPT-3, I built a Figma plugin to design for you. I call it &#39;Designer&#39;">Jordan Singer’s Figma plugin</a> which apparently creates a new Figma layout DSL &amp; few-shot teaches it to GPT-3.</p>
          <p>(I’d also highlight <a href="https://web.archive.org/web/20200727092603/https://spronkoid.github.io/recycling/Recyclingisascam.html" data-link-icon="internetarchive" data-link-icon-type="svg">GPT-3’s version</a> of the famous GPT-2 recycling rant, an <a href="https://jvns.ca/doc/www/www.notion.so/81123d596db9f6a677137695d44f65129235c90b.html" rel="archived alternate nofollow" data-url-original="https://www.notion.so/ERB-of-History-GPT-3-Bot-784e99b7fea0462f95489d74a568c4ad" title="(Original URL: https://www.notion.so/ERB-of-History-GPT-3-Bot-784e99b7fea0462f95489d74a568c4ad )">attempt at “Epic Rap Battles of History”</a>⁠, GPT-3 playing <a href="https://jvns.ca/doc/www/200wordrpg.github.io/68dc29784a54b4b94a1215b358244b267755a4e1.html" rel="archived alternate nofollow" data-url-original="https://200wordrpg.github.io/" title="Design a role-playing game using 200 words or less. (Original URL: https://200wordrpg.github.io/ )">200-word tabletop RPGs</a> <a href="https://nitter.moomoo.me/jonathanfly/status/1284036215386312712" data-link-icon="twitter" data-link-icon-type="svg" title="I was thinking of using #gpt3 to generate 200 Word RPGs (tiny complete games) but I&#39;m getting quite distracted watching it *play* 200 Word RPG Challenge entries. It didn&#39;t account for the tokens but it got the general idea without any example gameplay in the prompt.">with itself</a>⁠, the <a href="http://serendipityrecs.com/">Serendipity</a> recommendation engine which asks GPT-3 for movie/​book recommendations (cf. <a href="https://jvns.ca/doc/www/arxiv.org/e9652309fe409e0b0c0a8adf3e10dab1b0e05733.pdf#anthropic" id="ganguli-et-al-2022" data-link-icon="anthropic" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2202.07785?fallback=original#anthropic" title="Predictability and Surprise in Large Generative Models (Original URL: https://arxiv.org/abs/2202.07785#anthropic )"><span><span title="et al">Ganguli</span> <span>et al</span> <span>2022</span></span></a>), and <a href="https://nitter.moomoo.me/lawderpaul/status/1284972517749338112" data-link-icon="twitter" data-link-icon-type="svg" title="Turns out #GPT3 can do vision too 😉 Built an ingredient parser: take a pic of any nutrition label (google to extract text), and GPT-3 will identify ingredients, find an emoji, determine if it&#39;s unhealthy, and give a definition 🤯">Lawder’s food label ingredient summarizer</a>⁠.)</p>
          <p>One underexplored area of GPT-3 is using its “search” API, which as the name indicates, takes a text prompt (the query) and searches a large set of possible results, and returns the ‘most similar’ one, in a highly abstract sense; <a href="https://jvns.ca/doc/www/andrewmayneblog.wordpress.com/e6165da83b927229a3643daec5678cc165a3dc81.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://andrewmayneblog.wordpress.com/2020/10/20/the-best-kept-secret-about-openais-gpt-3/" title="(Original URL: https://andrewmayneblog.wordpress.com/2020/10/20/the-best-kept-secret-about-openais-gpt-3/ )">Andrew Mayne</a> demonstrates that it’s much more than a simple keyword search engine by doing things like searching for abstract movie plots.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
        </section>
        <section id="gpt-3-implications">
          
          <blockquote>
            <p><span id="gpt-3">For my main discussion of why GPT-3 works and its implications, see <a href="https://jvns.ca/scaling-hypothesis" id="gwern-scaling-hypothesis" title="&#39;The Scaling Hypothesis&#39;, Branwen 2020">“On GPT-3: Meta-Learning, Scaling, Implications, And Deep Theory”</a> (see also <a href="https://jvns.ca/backstop" id="gwern-backstop" title="&#39;Evolution as Backstop for Reinforcement Learning&#39;, Branwen 2018">Backstop</a>). Below is the summary:</span></p>
          </blockquote>
          <p>GPT-3, announced by OpenAI in May 2020, was the largest neural network ever trained, by over an order of magnitude. Trained on Internet text data, it is the successor to GPT-2, which surprised everyone by its natural language understanding &amp; generation ability. GPT-3 is even more surprising in that this vast increase in size did not run into <a href="https://en.wikipedia.org/wiki/Diminishing_returns" data-link-icon="wikipedia" data-link-icon-type="svg">diminishing returns</a>⁠, as many expected, but the benefits of scale continued to happen as forecasted by OpenAI. These benefits were not merely learning more facts &amp; text than GPT-2, but qualitatively distinct &amp; surprising in showing <em>meta-learning</em>: while GPT-2 learned how to do common natural language tasks like text summarization, GPT-3 instead learned how to follow directions and learn new tasks from a few examples. (As a result, GPT-3 outputs &amp; interaction are more fascinating &amp; human-like than GPT-2.)</p>
          <p>While the immediate applications of GPT-3, like my poetry or humor writings, are nice, the short-term implications of GPT-3 are much more important.</p>
          <p>First, while GPT-3 is expensive by conventional DL standards, it is cheap by scientific/​commercial/​military/​government budget standards, and the results indicate that models could be made much larger. Second, models can also be made much more powerful, as GPT is an old approach known to be flawed in both minor &amp; major ways, and far from an ‘ideal’ <a href="https://jvns.ca/doc/www/arxiv.org/2f90212754aa5c9487dcc3552e5d807f87063eca.pdf#google" id="vaswani-et-al-2017" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.03762?fallback=original#google" title="&#39;Attention Is All You Need&#39;, Vaswani et al 2017 (Original URL: https://arxiv.org/abs/1706.03762#google )">Transformer</a>⁠. Third, GPT-3’s capabilities come from learning on raw (unsupervised) data; that has long been one of the weakest areas of DL, holding back progress in other areas like <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" data-link-icon="wikipedia" data-link-icon-type="svg">reinforcement learning</a> or robotics. Sequence models can learn rich models of environments &amp; rewards (either online or offline), and implicitly plan and perform well (<a href="https://jvns.ca/doc/www/sites.google.com/6d52698eb87380d0a5f8db5680fd2cf2bf7a1cc9.html" id="decision-transformer" data-link-icon="BAIR" data-link-icon-type="text,quad,mono" rel="archived alternate nofollow" data-url-original="https://sites.google.com/berkeley.edu/decision-transformer" title="Decision Transformer: Reinforcement Learning via Sequence Modeling (Original URL: https://sites.google.com/berkeley.edu/decision-transformer )"><span><span><span title="et al">Chen</span> <span>et al</span> <span>2021</span></span>’s</span></a> Decision Transformer is a demonstration of how RL can lurk in what looks merely like simple supervised learning). Models like GPT-3 suggest that large unsupervised models will be vital components of future DL systems, as they can be ‘plugged into’ systems to immediately provide understanding of the world, humans, natural language, and reasoning.</p>
          <p>The meta-learning has a longer-term implication: it is a demonstration of the <em>blessings of scale</em>, where problems with simple neural networks vanish, and they become more powerful, more generalizable, more human-like when simply made very large &amp; trained on very large datasets with very large compute—even though those properties are believed to require complicated architectures &amp; fancy algorithms (and this perceived need drives much research). Unsupervised models benefit from this, as training on large corpuses like Internet-scale text present a myriad of difficult problems to solve; this is enough to drive meta-learning despite GPT not being designed for meta-learning in any way. (This family of phenomena is perhaps driven by neural networks functioning as ensembles of many sub-networks with them all averaging out to an Occam’s razor, which for small data &amp; models, learn superficial or memorized parts of the data, but can be forced into true learning by making the problems hard &amp; rich enough.)</p>
          <p>The <a href="https://jvns.ca/scaling-hypothesis#blessings-of-scale" id="gwern-scaling-hypothesis-blessings-of-scale">blessings of scale</a> in turn support a radical theory: an old AI paradigm held by a few pioneers in connectionism (early artificial neural network research) and by more recent deep learning researchers, the <em>scaling hypothesis</em>. The scaling hypothesis regards the blessings of scale as the secret of AGI: intelligence is ‘just’ simple neural units &amp; learning algorithms applied to diverse experiences at a (currently) unreachable scale. As increasing computational resources permit running such algorithms at the necessary scale, the neural networks will get ever more intelligent.</p>
          <p>When? Estimates of Moore’s law-like progress curves decades ago by pioneers like <a href="https://en.wikipedia.org/wiki/Hans_Moravec" data-link-icon="wikipedia" data-link-icon-type="svg">Hans Moravec</a> indicated that it would take until the 2010s for the sufficiently-cheap compute for tiny insect-level prototype systems to be available, and the 2020s for the first sub-human systems to become feasible, and these forecasts are holding up. (Despite this vindication, the scaling hypothesis is so unpopular an idea, and difficult to prove in advance rather than as a <em>fait accompli</em>, that while the GPT-3 results finally drew some public notice after OpenAI enabled limited public access &amp; people could experiment with it live, it is unlikely that many entities will modify their research philosophies, much less kick off an ‘arms race’.)</p>
          <p>Depending on what investments are made into scaling DL, and how fast compute grows, the 2020s should be quite interesting—sigmoid or singularity?</p>
        </section>
        <section id="quality">
          
          <p><span>Objective metrics hard to interpret.</span> How much better is (un-finetuned base) GPT-3? The likelihood loss is an absolute measure, as are the benchmarks, but it’s hard to say what a decrease of, say, 0.1 bits per character might mean, or a 5% improvement on SQuAD, in terms of real-world use or creative fiction writing. It <em>feels</em> like a large improvement, definitely a larger improvement than going from GPT-2-345M to GPT-2-1.5b, or GPT-2-1.5b to GPT-3-12b, but how much?</p>
          <p><span>Screening gains: 1:100 → 1:5 or 20× better?</span> For fiction, I treat it as a curation problem: how many samples do I have to read to get one worth showing off? One could think of it asking how efficiently a model searches <a href="https://en.wikipedia.org/wiki/The_Library_of_Babel" data-link-icon="wikipedia" data-link-icon-type="svg">The Library of Babel</a> (or should that be, <em><a href="https://en.wikipedia.org/wiki/The_Book_of_Sand" data-link-icon="wikipedia" data-link-icon-type="svg">The Book of Sand</a></em>, or <a href="https://jvns.ca/doc/www/medium.com/c6ed7fb4ea99ec842aa742d02fc42a429b5629f8.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/@die.zaubernacht/the-aleph-3435f52ac297" title="The Aleph: Borgean fantastic hyperreality revisited by GPT-3 (Original URL: https://medium.com/@die.zaubernacht/the-aleph-3435f52ac297 )">“The Aleph”</a>?): at the one extreme, an algorithm which selects letters at random will have to generate astronomically large numbers of samples before, like the proverbial monkeys, they generate a page from a Shakespeare play; at the other extreme, a reasonably intelligent human can dash off 1 plausible page in 1 try. With AI algorithms, the results are intermediate but rapidly improving. A <a href="https://en.wikipedia.org/wiki/Markov_chain" data-link-icon="wikipedia" data-link-icon-type="svg">Markov chain</a> text generator trained on a small corpus represents a huge leap over randomness: instead of having to generate quadrillions of samples, one might only have to generate millions of samples to get a coherent page; this can be improved to hundreds of thousands by increasing the depth of the <em>n</em> of its <em>n</em>-grams, which is feasible as one moves to Internet-scale text datasets (the classic <a href="https://jvns.ca/doc/ai/scaling/2009-halevy.pdf" id="halevy-et-al-2009" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The Unreasonable Effectiveness of Data&#39;, Halevy et al 2009">“unreasonable effectiveness of data”</a> example) or by careful hand-engineering &amp; combination with other approaches like Mad-Libs-esque templating. A char-<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" data-link-icon="wikipedia" data-link-icon-type="svg">RNN</a>⁠, like in my <a href="https://jvns.ca/rnn-metadata" id="gwern-rnn-metadata" title="&#39;RNN Metadata for Mimicking Author Style&#39;, Branwen 2015">char-RNN poetry experiments</a> does better still: it easily generates reasonable paragraphs, so one might only have to brute force on the order of thousands of samples to get a pleasing page. With GPT-2-117M poetry, I’d typically read through a few hundred samples to get a good one, with worthwhile improvements coming from 345M→774M→1.5b; by 1.5b, I’d say that for the <a href="https://docs.google.com/document/d/1MhA3M5ucBD7ZXcWk57_MKZ5jEgPX6_YiKye_EFP-adg/edit" id="branwen-2020-gpt2-poetry-collaboration" data-link-icon="worddoc" data-link-icon-type="svg" title="&#39;Crowdsourcing The Best GPT-2-1.5b Poetry&#39;, Branwen 2020">crowdsourcing experiment</a>⁠, I read through 50–100 ‘poems’ to select one. But for GPT-3, once the prompt is dialed in, the ratio appears to have dropped to closer to 1:5—maybe even as low as 1:3! I frequently find myself shrugging at the first completion I generate, “not bad!” (Certainly, the quality of GPT-3’s average prompted poem appears to exceed that of almost all teenage poets.) I would have to read GPT-2 outputs for months and probably surreptitiously edit samples together to get a dataset of samples like this page.</p>
        </section>
        <section id="prompts-as-programming">
          
          <div>
            <blockquote>
              <p>On two occasions I have been asked,—‘Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?’ In one case a member of the Upper, and in the other a member of the Lower, House put this question. I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.</p>
              <p><a href="https://en.wikipedia.org/wiki/Charles_Babbage" data-link-icon="wikipedia" data-link-icon-type="svg">Charles Babbage</a>⁠, <a href="https://en.wikisource.org/wiki/Passages_from_the_Life_of_a_Philosopher/Chapter_V#67" data-link-icon="wikipedia" data-link-icon-type="svg" title="_Passages from the Life of a Philosopher_ (1864), ch. 5 &#39;Difference Engine No. 1&#39;"><em>Passages from the Life of a Philosopher</em></a> 1864</p>
            </blockquote>
          </div>
          <div id="prompt-programming">
            <blockquote>
              <p>The GPT-3 neural network is so large a model in terms of power and dataset that it exhibits qualitatively different behavior: you do not apply it to a fixed set of tasks which were in the training dataset, requiring retraining on additional data if one wants to handle a new task (as one would have to retrain <a href="https://jvns.ca/gpt-2" id="branwen-presser-2019" title="&#39;GPT-2 Neural Network Poetry&#39;, Branwen &amp; Presser 2019">GPT-2</a>); instead, you interact with it, expressing any task in terms of natural language descriptions, requests, and examples, tweaking the prompt until it “understands” &amp; it meta-learns the new task based on the high-level abstractions it learned from the pretraining.</p>
              <p>This is a rather different way of using a DL model, and it’s better to think of it as a new kind of programming, <strong>prompt programming</strong>, where the prompt is now a coding language which programs GPT-3 to do new things.</p>
            </blockquote>
          </div>
          <p><span>A new programming paradigm?</span> The GPT-3 neural network is so large a model in terms of power and dataset that it exhibits qualitatively different behavior: you do not apply it to a fixed set of tasks which were in the training dataset, requiring retraining on additional data if one wants to handle a new task (as one would have to retrain GPT-2); instead, you interact with it, expressing any task in terms of natural language descriptions, requests, and examples, tweaking the prompt until it “understands” &amp; it meta-learns the new task based on the high-level abstractions it learned from the pretraining. This is a rather different way of using a DL model, and it’s better to think of it as a new kind of programming, where the prompt is now a “program” which programs GPT-3 to do new things. “Prompt programming”<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> is less like regular programming than it is an exercise in a kind of <a href="https://en.wikipedia.org/wiki/Tacit_knowledge" data-link-icon="wikipedia" data-link-icon-type="svg">tacit knowledge</a>⁠/ ​<a href="https://dzone.com/articles/mechanical-sympathy">mechanical sympathy</a>⁠. It is like coaching a superintelligent <a href="https://en.wikipedia.org/wiki/Cat" data-link-icon="wikipedia" data-link-icon-type="svg">cat</a> into learning a new trick: you can ask it, and it will do the trick perfectly sometimes, which makes it all the more frustrating when it rolls over to lick its butt instead—you know the problem is not that it <em>can’t</em> but that it <em>won’t</em>.</p>
          <p><span>Reprogramming by asking politely.</span> The demos above and on this page all<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> use the raw default GPT-3 model, without any additional training. Instead, to get all these different behaviors, one provides a short textual input to GPT-3, with which it will predict the next piece of text (as opposed to starting with an empty input and freely generating anything); GPT-3, just by reading it, can then flexibly adapt its writing style and reasoning and use new definitions or rules or words defined in the textual input no matter that it has never seen them before.</p>
          <p><span>What is meta-learning?</span> <a href="https://jvns.ca/scaling-hypothesis#meta-learning" id="gwern-scaling-hypothesis-meta-learning">This is considered “meta-learning”</a> because GPT-3 has “learned how to learn”: in its endless training on so many gigabytes of text, it encounters so many different kinds of text that it had no choice but to learn abstractions &amp; how to understand descriptions &amp; instructions &amp; formatting &amp; authorial intent to let it adapt on the fly to the current piece of text it was training on, since there was too much diversity &amp; data for it to simply learn each task normally by repeated exposure—much less memorize all the data. At scale, for a sufficiently powerful (large) NN, the simplest &amp; easiest algorithms to learn for better prediction <em>are</em> abstractions &amp; intelligence: the harder and bigger, the better. When GPT-3 meta-learns, the weights of the model do not change, but as the model computes layer by layer, the internal numbers become new abstractions which can carry out tasks it has never done before; in a sense, the GPT-3 model with the 175b parameters is not the real model—the real model is those ephemeral numbers which exist in between the input and the output, and define a new GPT-3 tailored to the current piece of text. The real GPT-3 is not the fixed hardwired weights, which merely are a bootstrap or a compiler for creating the real GPT-3, a new model customized to the data which exists only briefly in the soft attention weights during runtime, and may do completely different things from the baseline model.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
          <figure>
            <img alt="Few-shot learning/​writing prompts: “Software 3.0”? (Andrej Karpathy, 2020-06-18)" decoding="async" height="1362" loading="lazy" src="https://jvns.ca/doc/ai/nn/transformer/gpt/2020-06-18-karpathy-expandingbrainmeme-gpt3metalearning.jpg" width="1000"/>
            <figcaption aria-hidden="true">
              Few-shot learning/​writing prompts: “Software 3.0”? (<a href="https://nitter.moomoo.me/karpathy/status/1273788774422441984" data-link-icon="twitter" data-link-icon-type="svg">Andrej Karpathy</a>⁠, 2020-06-18)
            </figcaption>
          </figure>
          <p><span>Programming by dialogue?</span> Because you aren’t finetuning GPT-3 in the conventional way, interacting with GPT-3 via its few-shot learning power takes on an entirely different feeling than anything else I’ve used before. With regular software, you have to think through exactly how to do something; with deep learning software, you have to focus on providing data which in some way embodies the correct answer which you want; but with GPT-3, you instead think about how to <em>describe</em> what you want. With GPT-3, it helps to anthropomorphize it: sometimes you literally just have to <em>ask</em> for what you want. (It can’t possibly be that easy, can it? Sometimes, it is!) Thus, you can simply ask it directly in the Q&amp;A format: “what is X?” For example, if you want it to detect gibberish questions and avoid trying to answer them and <a href="https://jvns.ca/gpt-3-nonfiction#understanding-uncertainty" id="understanding-uncertainty">show some understanding of its uncertainty</a>⁠, you can specify in the prompt that it shouldn’t answer nonsense questions, and you can ask it to double-check an earlier answer; if you find it doesn’t seem to understand that a horse has two eyes or that a toaster weighs more than a pencil, <a href="https://jvns.ca/gpt-3-nonfiction#common-sense-knowledge-animal-eyes" id="gwern-gpt-3-nonfiction-common-sense-knowledge-animal-eyes">perhaps asking</a> <a href="https://jvns.ca/gpt-3-nonfiction#common-sense-knowledge-weights" id="gwern-gpt-3-nonfiction-common-sense-knowledge-weights">more questions</a> with better settings will fix that. Other times, you must instead think, “If a human had already written out what I wanted, what would the first few sentences sound like? What would the introduction and summary sound like? What if I told a story here, how would that story start?” Thus, the summarization prompt: “My second grader asked me what this passage means: …” Some tasks in the GPT-3 paper which showed disappointing performance can be improved dramatically by finding appropriate formatting or prompts: arithmetic improves enormously with comma formatting of decimals (due to <a href="#bpes">BPEs</a>), and the “Word in Context” benchmark (where GPT-3 surprisingly showed below-chance performance compared to the 85% SOTA) <a href="http://gptprompts.wikidot.com/linguistics:word-in-context" data-link-icon="openai" data-link-icon-type="svg">can be improved to &gt;70%</a> with better prompting, while on MNLI &amp; <a href="https://jvns.ca/doc/www/arxiv.org/700dab8bf43b51be18e0fed003b01613aeb4965e.pdf" id="wang-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1905.00537?fallback=original" title="&#39;SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems&#39;, Wang et al 2019 (Original URL: https://arxiv.org/abs/1905.00537 )">SuperGLUE</a> benchmarks better <a href="https://jvns.ca/doc/www/arxiv.org/c3bdadb13fa46a5d089ff4bfd20e0cd862a47943.pdf#facebook" id="liu-et-al-2019-roberta" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1907.11692?fallback=original#facebook" title="&#39;RoBERTa: A Robustly Optimized BERT Pretraining Approach&#39;, Liu et al 2019 (Original URL: https://arxiv.org/abs/1907.11692#facebook )">RoBERTa</a> prompts are worth <a href="https://jvns.ca/doc/www/arxiv.org/ae58abb0cc8fb7fad98fe7ac84003bccc66cd526.pdf" id="scao-rush-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2103.08493?fallback=original" title="&#39;How Many Data Points is a Prompt Worth?&#39;, Le Scao &amp; Rush 2021 (Original URL: https://arxiv.org/abs/2103.08493 )">hundreds of datapoints</a>⁠. Or <a href="https://jvns.ca/doc/www/arxiv.org/0d888fd28ac05b6767043d815f540a2c05d5430c.pdf" id="reynolds-mcdonell-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2102.07350?fallback=original" title="Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm (Original URL: https://arxiv.org/abs/2102.07350 )"><span>Reynolds &amp; <span><span>McDonell</span><span>2021</span></span></span></a> demonstrate that the GPT-3 paper substantially underestimates GPT-3’s ability to translate Fr→En: to my considerable surprise, the straightforward 10-example translation prompt Brown et al used is actually <em>worse</em> than the zero-shot “French: XYZ / English:”, because, apparently, when formatted that way the 10-shots look like a narrative to follow rather than merely demonstrative examples. Even for <a href="https://jvns.ca/doc/www/arxiv.org/74d7abec2006b58804ae99d71a7556ba9d623206.pdf#google" id="devlin-et-al-2018" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1810.04805?fallback=original#google" title="&#39;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&#39;, Devlin et al 2018 (Original URL: https://arxiv.org/abs/1810.04805#google )">BERT</a> or GPT-2, large gains in performance are possible by directly optimizing the prompt instead of guessing (<a href="https://jvns.ca/doc/www/arxiv.org/3bbe0caeb889a96abbbf64fd0eb8c4ca846b5cc8.pdf" id="jiang-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.12543?fallback=original" title="How Can We Know What Language Models Know? (Original URL: https://arxiv.org/abs/1911.12543 )"><span><span title="et al">Jiang</span> <span>et al</span> <span>2019</span></span></a>⁠, <a href="https://jvns.ca/doc/www/arxiv.org/c026e13bc9fb6ede1c522e4d3bfe69c0d272df6a.pdf" id="li-liang-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2101.00190?fallback=original" title="Prefix-Tuning: Optimizing Continuous Prompts for Generation (Original URL: https://arxiv.org/abs/2101.00190 )"><span><span>Li &amp; Liang</span><span>2021</span></span></a>). (Outputs can be further improved in a knowledge-free way by <a href="https://jvns.ca/doc/www/arxiv.org/3ecd5a1bc1f49fa26956f52cda95c7e32ab5c1a1.pdf" id="zhao-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2102.09690?fallback=original" title="&#39;Calibrate Before Use: Improving Few-Shot Performance of Language Models&#39;, Zhao et al 2021 (Original URL: https://arxiv.org/abs/2102.09690 )">calibrating</a> sets of outputs to compensate for the vagaries of greedy sampling, which would again not be possible if the knowledge were not in GPT-3 to begin with.)</p>
          <div>
            <p>
              Sampling Can Prove The Presence Of Knowledge But Not The Absence
            </p>
            <p>GPT-3 may “fail” if a prompt is poorly-written, does not include enough examples, or bad sampling settings are used. I have demonstrated this many times when someone shows a “failure” of GPT-3—the failure was their own. The question is not whether a given prompt works, but <a href="https://jvns.ca/forking-path" id="gwern-forking-path" title="&#39;Technology Forecasting: The Garden of Forking Paths&#39;, Branwen 2014">whether any prompt works</a>⁠.</p>
          </div>
          <p>Any child psychologist trained in administering IQ tests is well-aware of the need to build rapport with children, to monitor them for problems and gauge their linguistic skills: are they not a native English speaker? Are they angry with or afraid of the psychologist? Are they apathetic and unmotivated? It is hard to ace an IQ test by accident, but it’s trivial to fail one on purpose; trying to administer an IQ test to a child who has taken a disliking to you is a waste of the time of everyone involved, and presenting the resulting score as meaningful is professional malpractice.</p><span>The Lizardman Constant: nonsense prompt completions by humans.</span>
          
          <p>Another cautionary example comes from survey research. To briefly review Scott Alexander’s <a href="https://jvns.ca/note/lizardman" id="gwern-note-lizardman" title="&#39;Lizardman Constant in Surveys&#39;, Branwen 2013">“lizardman constant”</a>: human survey-takers will, with &gt;0% probability, endorse the most absurd items on a survey, for a mix of reasons like laziness, boredom, humor, sabotage, ignorance, and stupidity. For example, 4% of respondents may endorse the claim ‘lizard-people rule the earth’, 5% of atheists believe in God, and so on. (And these are not necessarily transient random errors—when challenged explicitly on them, researchers find many will come up with bizarre rationalizations to explain responses like how they answered ‘yes’ to “I have had a fatal heart attack”.) This cautions us against taking survey results about extremely unusual people or traits too literally, or expecting perfectly accurate results, as given the lizardman constant and other crud factors, it is entirely possible that some or all of the outliers may just be the lizardman constant at work.</p>
          <p><span>Humans need prompt programming too.</span> Should we conclude from such cases that humans, or at least some specific humans, are not actually intelligent? No, of course not. We would say that such people have simply not been properly instructed or educated, given incentive to be honest, or made normal unavoidable errors. It would be tendentious in the extreme to conclude that because some people will claim to have suffered fatal heart attacks that they are merely statistical pattern-matching machines emitting plausible yet semantically-null utterances while passing for human; if we want to conclude <em>that</em>, I hope we would probe them a little more thoughtfully than prompting them with some survey items and declaring the case closed!</p>
          <p><span>Demand more from critics.</span> We should expect nothing less of people testing GPT-3, when they claim to get a low score (much less stronger claims like “all language models, present and future, are unable to do X”): did they consider problems with their prompt? Whether all of the hyperparameters make sense for that task? Did they examine where completions go wrong, to get an idea of why GPT-3 is making errors? Did they test out a variety of strategies? Did they consider qualitatively how the failed completions sound? (Or did they copy-paste arbitrary hyperparameters, use the first prompt that came to mind, look at the output, and lazily present it to the world as proof of what GPT-3 can’t do?)</p>
          <p><span>Machine sympathy.</span> Prompt programming often should be human-like: if a human wouldn’t understand what was intended, why would GPT-3? It’s not telepathic, and there are myriads of genres of human text which the few words of the prompt could belong to. (A helpful thought experiment: if someone emailed you a prompt out of the blue, with no other context whatsoever, what would <em>you</em> interpret it as? A joke, a troll, spam, or what?) Prompts should obey <a href="https://en.wikipedia.org/wiki/Cooperative_principle" data-link-icon="wikipedia" data-link-icon-type="svg">Gricean maxims of communication</a>—statements should be true, informative, and relevant. One should not throw in irrelevant details or non sequiturs, because in human text, <a href="https://en.wikipedia.org/wiki/Chekhov&#39;s_gun" data-link-icon="wikipedia" data-link-icon-type="svg">even in fiction</a>⁠, that implies that those details are relevant, no matter how nonsensical a narrative involving them may be.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> When a given prompt isn’t working and GPT-3 keeps pivoting into other modes of completion, that may mean that one hasn’t constrained it enough by imitating a correct output, and one needs to go further; writing the first few words or sentence of the target output may be necessary. (This was a particular problem with the literary parodies: GPT-3 would keep starting with it, but then switch into, say, one-liner reviews of famous novels, or would start writing fanfictions, complete with self-indulgent prefaces. The solution was to write out the first 2 or 3 sentences of an example parody, and <em>then</em> GPT-3 would finish out the parody, look back and see that there was an example of a literary parody, and then happily start generating dozens of works+parody pairs, once it fell into the groove.) The more natural the prompt, like a ‘title’ or ‘introduction’, the better; unnatural-text tricks that were useful for GPT-2, like dumping in a bunch of keywords <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" data-link-icon="wikipedia" data-link-icon-type="svg">bag-of-words-style</a> to try to steer it towards a topic, appear less effective or harmful with GPT-3.</p>
          <p><span>Surprisingly powerful.</span> Prompts are perpetually surprising—I kept underestimating what GPT-3 would do with a given prompt, and as a result, I underused it. Text is a weird way to try to input all these queries and output their results or examine what GPT-3 thinks (compared to a more natural NLP approach like using BERT’s embeddings), and fiddly. Just as few people would have thought that you could get GPT-2 to automatically summarize text by simply appending a “TL;DR:” string, few people would guess GPT-3 could write emoji summaries or that if you use a prompt like “Summarize the plot of J.K. Rowling’s <em>Harry Potter</em> in the style of Ernest Hemingway”, you might get out a dozen profanity-laced reviews panning 20<sup>th</sup>-century literature (or a summary—in Chinese—of the Chinese translation<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>), or that if you use a prompt like “Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence”, GPT-3 will generate poems but then immediately generate explanations of how neural networks work &amp; discussions from eminent researchers like Gary Marcus of why they will never be able to truly learn or exhibit creativity like generating poems. It is difficult to try out variations on prompts because as soon as the prompt works, it’s tempting to keep trying out completions to marvel at the sheer variety and quality as you are seduced into further exploring possibility-space. (GPT-3 never grows impatient or bored.) What other capabilities are <a href="https://en.wikipedia.org/wiki/Latent_and_observable_variables" data-link-icon="wikipedia" data-link-icon-type="svg">latent</a>⁠, waiting to be exposed by someone stumbling across the right prompt?</p>
          <p>(Of course, not all these capabilities are necessarily desirable: where there is programming, you can be sure there is hacking. Where there is “prompt programming”, there must be “prompt hacking”… GPT-3 can follow instructions, so within its context-window or with any external memory, it is surely Turing-complete, and who knows what <a href="https://jvns.ca/turing-complete" id="gwern-turing-complete" title="&#39;Surprisingly Turing-Complete&#39;, Branwen 2012">weird machines</a> or <a href="https://jvns.ca/doc/www/arxiv.org/40a93946b61c16a861bb5d277c89bdf07c507d09.pdf" id="elsayed-et-al-2018-adversarial-reprogramming" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1806.11146?fallback=original" title="&#39;Adversarial Reprogramming of Neural Networks&#39;, Elsayed et al 2018 (Original URL: https://arxiv.org/abs/1806.11146 )">adversarial</a> <a href="https://jvns.ca/doc/www/arxiv.org/080e52b3e827dd0c10a822c22935f62305ee1b8f.pdf" id="neekhara-et-al-2018" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1809.01829?fallback=original" title="&#39;Adversarial Reprogramming of Text Classification Neural Networks&#39;, Neekhara et al 2018 (Original URL: https://arxiv.org/abs/1809.01829 )">reprogrammings</a> are possible? Consider the AI Dungeon users as an early example of “prompt hacking”.)</p>
          <section id="finetuning">
            <h2><a href="#finetuning" title="Link to section: § &#39;Finetuning&#39;">Finetuning</a></h2>
            <p><span>Finetuning was necessary to ‘program’ GPT-2.</span> GPT-3’s “prompt programming” paradigm is strikingly different from GPT-2, where its prompts were brittle and you could only tap into what you were sure were extremely common kinds of writing, and, as like as not, it would quickly change its mind and go off writing something else. At best, you could fairly generically hint at a topic to try to at least get it to use keywords; then you would have to filter through quite a few samples to get one that really wowed you. (This was a trick I used for <a href="https://jvns.ca/twdne#text" id="gwern-twdne-text">TWDNE</a> to get it to generate at least vaguely anime-related plot summaries.) To get output reliably out of GPT-2, you had to finetune it on a preferably decent-sized corpus.</p>
            <p><span>Do we need finetuning given GPT-3’s prompting?</span> But with GPT-3, you can just say so, and odds are good that it can do what you ask, and already knows what you’d finetune it on. (For example, I thought I would have to finetune GPT-3 to get samples of myself, since GPT-2 doesn’t know anything about “Gwern”/​“gwern.net”; but it turns out, all I have to do is put in “A new essay by Gwern Branwen (gwern.net):” and out comes an uncanny <a href="https://jvns.ca/gpt-3-nonfiction#gwern-branwen" id="gwern-gpt-3-nonfiction-gwern-branwen">simulacrum of myself</a>⁠, or <a href="https://astralcodexten.substack.com/" data-link-icon="SSC" data-link-icon-type="text,tri">Scott Alexander</a>⁠, or <a href="https://jvns.ca/gpt-3-nonfiction#paul-graham" id="gwern-gpt-3-nonfiction-paul-graham">Paul Graham</a>⁠, or…) Would it be better if finetuned? Indubitably. But it’s not necessary. And given the creativity of the non-finetuned GPT-3, I’m not sure that I even want to—and forfeit all the behaviors I haven’t yet discovered‽</p>
            <p>As of mid-June 2020, the <a href="https://openai.com/blog/openai-api/" id="brockman-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" title="&#39;OpenAI API&#39;, Brockman et al 2020">OpenAI API</a> does not support finetuning although OA was working on it. But after enough time playing with GPT-3, I have begun to wonder: at this level of meta-learning &amp; general knowledge, do we need finetuning at all?</p>
            <p>For GPT-2, I saw finetuning as doing 2 things:</p>
            <ol>
              <li>
                <p><strong>Fixing ignorance</strong>: missing domain knowledge</p>
                <p>GPT-2 didn’t know many things about most things—it was just a handful (1.5 billion) of parameters trained briefly on the tiniest fraction of the Common Crawl subset of the Internet, without any books even<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>⁠. It’s not surprising that for many domains, it wouldn’t know the details; and even if the dataset included adequate text, it did not train on that data many times, and the knowledge competed with all the other domains it needed to know about, interfering.</p>
                <p>But GPT-3 already knows everything! GPT-3 is so much larger on every dimension that this seems like much less of a problem for any domain which is already well-represented in public HTML pages. GPT-2 might need to be trained on a fanfiction corpus to learn about some obscure character in a random media franchise &amp; generate good fiction, but GPT-3 already knows about them and use them appropriately in writing new fiction.</p>
              </li>
              <li>
                <p><strong>Prompting a specific task</strong>:</p>
                <p>Even when GPT-2 knew a domain adequately, it had the frustrating behavior of rapidly switching domains. You might prompt it with a poem genre it knows adequately already, but then after a few lines, it would generate an end-of-text BPE and switch to generating a news article on Donald Trump. (Trump shows up a <em>lot</em>.) Presumably, while poetry was reasonably represented, it was still rare enough that GPT-2 considered poetry highly unlikely to be the next word, and keeps trying to jump to some more common &amp; likely kind of text, and GPT-2 is not smart enough to infer &amp; respect the intent of the prompt.</p>
                <p>GPT-3 exhibits much less of this ‘mode switching’ sort of behavior. Perhaps because it is trained on a much larger and more comprehensive dataset (so news articles aren’t so dominant), but also I suspect the meta-learning makes it much better at staying on track and inferring the intent of the prompt—hence things like the <a href="#transformer-poetry">“Transformer poetry” prompt</a>⁠, where despite being what must be highly unusual text, even when switching to prose, it is able to improvise appropriate followup commentary.</p>
                <p>Nevertheless, sometimes we can’t or don’t want to rely on prompt programming. A specific task may be necessary when a task has evaded our prompt programming skills, or we have data but not prompt programmer time. For example, in the GPT-3 paper, many tasks underperform what GPT-3 can do if we take the time to tailor the prompts &amp; sampling hyperparameters, and just throwing the naive prompt formatting at GPT-3 is misleading. However, researchers do not have the time to go through scores of benchmark tasks and fix them one by one; simply finetuning on them collectively ought to do at least as well as the correct prompts would, and requires much less human effort (albeit more infrastructure).</p>
              </li>
            </ol>
            <p>So, what would be the point of finetuning GPT-3 on poetry or literature? It has likely already seen the finetuning corpus, knows most of it, and will tractably generate poems on demand. There may be gains, but I wonder if they would be nearly as large as they were for GPT-2?</p>
          </section>
          <section id="playground">
            <h2><a href="#playground" title="Link to section: § &#39;Playground&#39;">Playground</a></h2>
            <p>All of the following samples were generated using the OpenAI Beta Playground, which looks like this:</p>
            <figure>
              <img alt="OA API Beta Playground UI &amp; available prewritten prompts/​sampling options" decoding="async" height="834" loading="lazy" src="https://jvns.ca/doc/ai/nn/transformer/gpt/2020-06-21-openai-beta-gpt3-playgroundui.png" width="1400"/>
              <figcaption aria-hidden="true">
                OA API Beta Playground UI &amp; available prewritten prompts/​sampling options
              </figcaption>
            </figure>
            <p>The Playground has some rough edges in Beta, and capacity issues. A good way to start is to generate samples with the log probs/​logits turned on, and paying attention to how sampling hyperparameters affect output, to gain intuition for how GPT-3 thinks &amp; what samples looks like when sampling goes haywire.</p>
            <figure>
              <img alt="The quality vs diversity tradeoff for top-k/​nucleus sampling on GPT-2 news articles: more extreme settings like top-k = 10 / topp = 0.6 are equally good to get the highest human ratings—but both come at at the expense of variety of possible completions. (Nadeem et al 2020; see also Zhang et al 2020, Dou et al 2021)" decoding="async" height="811" loading="lazy" src="https://jvns.ca/doc/ai/nn/transformer/gpt/2020-nadeem-figure1-gpt2samplingqualityvsdiversity.png" title="Nadeem et al 2020: Figure 1: Human evaluation (y-axis: quality, x-axis: diversity, both are the bigger the better) shows that the generation performance of existing sampling algorithms are on par with each other" width="814"/>
              <figcaption aria-hidden="true">
                The quality vs diversity tradeoff for top-<em>k</em>/​nucleus sampling on GPT-2 news articles: more extreme settings like top-<em>k</em> = 10 / top<sub>p</sub> = 0.6 are equally good to get the highest human ratings—but both come at at the expense of variety of possible completions. (<a href="https://jvns.ca/doc/www/arxiv.org/cbe048ae79b017636d469b8080627406689b264e.pdf" id="nadeem-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2009.07243?fallback=original" title="A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation (Original URL: https://arxiv.org/abs/2009.07243 )"><span><span title="et al">Nadeem</span> <span>et al</span> <span>2020</span></span></a>⁠; see also <a href="https://jvns.ca/doc/www/arxiv.org/b4bc30c34e025961b542a415245d425706d1d20d.pdf#google" id="zhang-et-al-2020" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2004.10450?fallback=original#google" title="Trading Off Diversity and Quality in Natural Language Generation (Original URL: https://arxiv.org/abs/2004.10450#google )"><span><span title="et al">Zhang</span> <span>et al</span> <span>2020</span></span></a>⁠, <a href="https://jvns.ca/doc/www/arxiv.org/a7f9cc61d6be77ecdc16646fa78619eb4af03449.pdf#allen" id="dou-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2107.01294?fallback=original#allen" title="Scarecrow: A Framework for Scrutinizing Machine Text (Original URL: https://arxiv.org/abs/2107.01294#allen )"><span><span title="et al">Dou</span> <span>et al</span> <span>2021</span></span></a>)
              </figcaption>
            </figure>
            <p><span>Tradeoff: diversity vs accuracy.</span> It offers the standard sampling options familiar from earlier GPT-2 interfaces, including <a href="https://jvns.ca/doc/www/arxiv.org/eb1e56374eef59904b634f67e5922be25f6f7b38.pdf#allen" id="holtzman-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1904.09751?fallback=original#allen" title="&#39;The Curious Case of Neural Text Degeneration&#39;, Holtzman et al 2019 (Original URL: https://arxiv.org/abs/1904.09751#allen )">“nucleus sampling”</a>⁠. One particularly manipulates the temperature setting to bias towards wilder or more predictable completions; for fiction, where creativity is paramount, it is best set high, perhaps as high as 1, but if one is trying to extract things which can be right or wrong, like question-answering, it’s better to set it low to ensure it prefers the most likely completion. (After all, the point of a high temperature is to regularly select completions which the model thinks <em>aren’t</em> likely; why would you do that if you are trying to get out a correct arithmetic or trivia question answer?) For top<sub>p</sub>, one can set it to ~0.95 and largely forget about it unless one suspects that it’s breaking answers like top-<em>k</em> and it needs to be much lower, like 0.5; it’s there to cut off the tail of gibberish completions and reduce repetition, so doesn’t affect the creativity too much. I generally avoid the use of the repetition penalties because I feel repetition is critical to creative fiction, and I’d rather err on the side of too much than too little, but sometimes they are a useful intervention; GPT-3, sad to say, maintains some of the weaknesses of GPT-2 and other likelihood-trained autoregressive sequence models, such as the propensity to fall into degenerate repetition.</p>
            <p><span>Ranking final results for quality gain.</span> A little more unusually, it offers a “best of” (BO) option which is the <a href="https://jvns.ca/doc/www/arxiv.org/904a1f22bf4ef78890a43c99ae058eb1d1c8e9d9.pdf#google" id="adiwardana-et-al-2020" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2001.09977?fallback=original#google" title="&#39;Meena: Towards a Human-like Open-Domain Chatbot&#39;, Adiwardana et al 2020 (Original URL: https://arxiv.org/abs/2001.09977#google )">Meena</a> ranking trick (other names include <a href="https://jvns.ca/doc/www/arxiv.org/a66ca1d0362421f16e2d0c101a750456f98f4944.pdf" id="caccia-et-al-2018" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1811.02549?fallback=original" title="&#39;Language GANs Falling Short&#39;, Caccia et al 2018 (Original URL: https://arxiv.org/abs/1811.02549 )">“generator rejection sampling”</a> or “random-sampling shooting method”: generate <em>n</em> possible completions independently, and then pick the one with best total likelihood, which avoids the degeneration that an explicit tree/​beam search would unfortunately trigger, as documented most recently by the <a href="https://jvns.ca/doc/www/arxiv.org/eb1e56374eef59904b634f67e5922be25f6f7b38.pdf#allen" id="holtzman-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1904.09751?fallback=original#allen" title="&#39;The Curious Case of Neural Text Degeneration&#39;, Holtzman et al 2019 (Original URL: https://arxiv.org/abs/1904.09751#allen )">nucleus sampling paper</a> &amp; reported by many others about likelihood-trained text models in the past eg. char-RNN in 2015, <a href="https://jvns.ca/doc/www/arxiv.org/f52a4f32764a70aeb8478f1c206e2149e2e38b07.pdf" id="koehn-knowles-2017" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.03872?fallback=original" title="Six Challenges for Neural Machine Translation (Original URL: https://arxiv.org/abs/1706.03872 )">Koehn &amp; Knowles</a>⁠, or <a href="https://jvns.ca/doc/www/arxiv.org/4916059fe42cc171a81d24d898834e2bd745860d.pdf#facebook" id="ott-et-al-2018" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1803.00047?fallback=original#facebook" title="Analyzing Uncertainty in Neural Machine Translation (Original URL: https://arxiv.org/abs/1803.00047#facebook )"><span><span title="et al">Ott</span> <span>et al</span> <span>2018</span></span></a><a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a>). I’m not sure how to best use BO: it seems to be highly helpful for things with one right answer (such as tricky Q&amp;A or reasoning), but when it helps with ‘creative’ completions is less clear. I tried out BO heavily because I couldn’t quite figure out how it interacts with quality. On the smaller models, it seems to help boost quality up towards ‘davinci’ (GPT-3-175b) levels without causing too much trouble, but on davinci, it appears to exacerbate the usual sampling issues: particularly with poetry, it’s easy for a GPT to fall into repetition traps or loops, or spit out memorized poems, and BO makes that <em>much</em> more likely. For generating completions of famous poems, it’s quite hard to get GPT-3 to generate new versions unless you actively edit the poem to force a difference. <span id="jabberwocky">(In the most extreme case, in the case of generating new variations on <a href="https://en.wikipedia.org/wiki/Jabberwocky" data-link-icon="wikipedia" data-link-icon-type="svg">“Jabberwocky”</a>⁠, I have been unable to generate <em>any</em> new versions under any setting, even taking the step of aggressively editing in new lines about how the vorpal sword bounced off the Jabberwocky and it won… It always spits out chunks of the original.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a>)</span> So BO is a double-edged sword.</p>
            <p>The best way I found to use it is to sample without it (BO = 1) at max temp, and then once it has several distinctly different lines, then sampling with more (eg. BO = 5) seems to help rather than hurt. This is a little surprising to me because for Meena, it made a large difference to do even a little BO, and while it had diminishing returns, I don’t think there was any point they tested where higher best-of-s made responses actually much worse (as opposed to merely <em>n</em> times more expensive). Possibly BO is much more useful for nonfiction/​information-processing tasks, where there’s one correct answer and BO can help overcome errors introduced by sampling or myopia.</p>
          </section>
          <section id="effective-prompt-programming">
            <h2><a href="#effective-prompt-programming" title="Link to section: § &#39;Effective Prompt Programming&#39;">Effective Prompt Programming</a></h2>
            <div>
              <blockquote>
                <p>To constrain the behavior of a program precisely to a range may be very hard, just as a writer will need some skill to express just a certain degree of ambiguity. A computer is like a violin. You can imagine a novice trying first a phonograph and then a violin. The latter, he says, sounds terrible. That is the argument we have heard from our humanists and most of our computer scientists. Computer programs are good, they say, for particular purposes, but they aren’t flexible. Neither is a violin, or a typewriter, until you learn how to use it.</p>
                <p><a href="https://en.wikipedia.org/wiki/Marvin_Minsky" data-link-icon="wikipedia" data-link-icon-type="svg">Marvin Minsky</a>⁠, <a href="https://jvns.ca/doc/www/web.media.mit.edu/b8c7bfd0757330c92e0f0f52a1ddc70b15468c26.html" rel="archived alternate nofollow" data-url-original="https://web.media.mit.edu/~minsky/papers/Why%20programming%20is--.html" title="(Original URL: https://web.media.mit.edu/~minsky/papers/Why%20programming%20is--.html )">“Why Programming Is a Good Medium for Expressing Poorly-Understood and Sloppily-Formulated Ideas”</a> 1967</p>
              </blockquote>
            </div>
            <p><span>Anthropomorphize your prompts.</span> There is no substitute for testing out a number of prompts to see what different completions they elicit and to reverse-engineer what kind of text GPT-3 “thinks” a prompt came from, which may not be what you intend and assume (after all, GPT-3 just sees the few words of the prompt—it’s no more a telepath than you are). If you ask it a question to test its commonsense reasoning like “how many eyes does a horse have” and it starts completing with a knock-knock joke, you need to rethink your prompt! Does it spit out completions that look like it’s thinking but it’s executing the wrong algorithm, or it falls back to copying parts of the input? Then one may need to few-shot it by providing examples to guide it to one of several possible things to do. One should also keep in mind the importance of sampling parameters, and whether one is looking for a single correct answer (so low temp with BO = 1 if compute-limited, or high temp and BO = 20 if possible) or if one is trying for creative answers (high temp with repetition penalties).</p>
            <p><span>The 4 Horsemen: short context, bad prompts, BPEs, random sampling.</span> My rule of thumb when dealing with GPT-3 is that if it is messing up, the errors are usually attributable to one of 4 problems: too-short context windows, insufficient prompt engineering, BPE encoding making GPT-3 ‘blind’ to what it needs to see to understand &amp; solve a problem, or noisy sampling sabotaging GPT-3’s attempts to show what it knows. Another useful heuristic is to try to express something as a multi-step reasoning process or <a href="https://jvns.ca/doc/ai/nn/transformer/gpt/inner-monologue/index" title="&#39;inner monologue (AI) tag&#39;, N/A 2023">“inner monologue”</a>⁠, such as a dialogue: because GPT-3 is a feedforward NN, it can only solve tasks which fit within one “step” or forward pass; any given problem may be too inherently serial for GPT-3 to have enough ‘thinking time’ to solve it, even if it can successfully solve each intermediate sub-problem within a step. So people have demonstrated that GPT-3 won’t solve a simple math problem in a single step, but it will solve it if you reframe it as a <a href="https://nitter.moomoo.me/kleptid/status/1284069270603866113" id="karyokleptid-2020" data-link-icon="twitter" data-link-icon-type="svg" title="I wonder if the AI would be better at math if you told it to show its work? · Seems to work. Teaching GPT-3 to do a brute force &#39;for loop&#39; checking answers also seems to work.">‘dialogue’</a> with the <a href="https://en.wikipedia.org/wiki/Spice_and_Wolf" data-link-icon="wikipedia" data-link-icon-type="svg">anime character Holo</a>—who knew neural network research would lead to anime wolfgirl demonology?—and even ask it to <a href="https://nitter.moomoo.me/kleptid/status/1284098635689611264" id="karyokleptid-2020" data-link-icon="twitter" data-link-icon-type="svg" title="Teaching GPT-3 to do a brute force &#39;for loop&#39; checking answers also seems to work.">guess-and-check or brute-force the answer</a> (see also <a href="https://jvns.ca/doc/www/arxiv.org/f20bf312ec38d37d9c1cd40b4c5b5f5264b8dd4e.pdf#google" id="austin-et-al-2021" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2108.07732?fallback=original#google" title="Program Synthesis with Large Language Models (Original URL: https://arxiv.org/abs/2108.07732#google )"><span><span title="et al">Austin</span> <span>et al</span> <span>2021</span></span></a>); one can also experiment in coaching it through examples<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>⁠, or <a href="https://jvns.ca/doc/www/news.ycombinator.com/0d65c9ac8ce063d5f533c10d8a204209f2ad235a.html" id="blixt-2020" data-link-icon="hn" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://news.ycombinator.com/item?id=23990902" title="I found that getting GPT-3 to add its own &#39;internal monologue&#39; in parentheses to be a helpful strategy to improve its probability of giving a more accurate answer. [blixt] (Original URL: https://news.ycombinator.com/item?id=23990902 )">requiring</a> <a href="https://jvns.ca/doc/www/blog.andrewcantino.com/642b641a22ab789da5eba95379dfeb1e7c7596e9.html" rel="archived alternate nofollow" data-url-original="https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/" title="How to dramatically improve the reasoning ability of GPT-3 (Original URL: https://blog.andrewcantino.com/blog/2021/05/28/how-to-dramatically-improve-the-reasoning-ability-of-GPT-3/ )">reasons</a> for an answer to show its work, or asking it about previous answers or using <span id="uncertainty-prompts"><a href="https://jvns.ca/gpt-3-nonfiction#expressing-uncertainty" id="expressing-uncertainty-2">“uncertainty prompts”</a></span>. This makes sense if we think of Transformers as unrolled RNNs which unfortunately lack a hidden state: serializing out the reasoning helps overcome that computational limitation.</p>
            <p><span>Logprob debugging.</span> GPT-3 does not directly emit text, but it instead predicts the probability (or “likelihood”) of the 51k possible BPEs given a text; instead of merely feeding them into some randomized sampling process like temperature top-<em>k</em>/​top<sub>p</sub> sampling, one can also record the predicted probability of each BPE conditional on all the previous BPEs. This gives you a simple idea of what GPT-3 is thinking about each BPE: is it likely or unlikely (given the previous BPEs)? Which BPEs are <em>especially</em> unlikely? Does it “get it” as the completion goes on? I don’t use logprobs much but I generally use them in 1 of 3 ways: I use them to see if the prompt ‘looks weird’ to GPT-3; to see where in a completion it ‘goes off the rails’ (suggesting the need for lower temperatures/​top<sub>p</sub> or higher BO); and to peek at possible completions to see how uncertain it is about the right answer—a good example of that is <a href="https://jvns.ca/doc/www/arr.am/eb4a87eb20e16e813dadf8819144bb67181824d1.html" rel="archived alternate nofollow" data-url-original="https://arr.am/2020/07/25/gpt-3-uncertainty-prompts/" title="(Original URL: https://arr.am/2020/07/25/gpt-3-uncertainty-prompts/ )">Arram Sabeti’s uncertainty prompts investigation</a> where the logprobs of each possible completion gives you an idea of how well the uncertainty prompts are working in getting GPT-3 to put weight on the right answer, or in my <a href="https://jvns.ca/gpt-3-nonfiction#parity" id="gwern-gpt-3-nonfiction-parity">parity analysis</a> where I observed that the logprobs of 0 vs 1 were almost exactly 50:50 no matter how many samples I added, showing no trace whatsoever of few-shot learning happening. Thus, logprobs can offer more insight while debugging a prompt than just repeatedly hitting ‘complete’ and getting frustrated.</p>
            <div>
              <p>
                AI Dungeon ≠ GPT-3
              </p>
              <p>I strongly recommend against use of the Dragon model as a “GPT-3” model. The finetuning appears to have seriously degraded the model, in addition to the censoring &amp; filtering now done.</p>
              <p>As of June 2021, I recommend either waiting for API access or using <a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/" id="eleutherai-2021" data-link-icon="eleutherai" data-link-icon-type="svg" title="&#39;GPT-J-6B: 6B JAX-Based Transformer&#39;, EleutherAI 2021">GPT-J</a> (high quality albeit 2 OOMs smaller).</p>
            </div>
            <p><span>AI Dungeon &lt; GPT-3.</span> For people using the AI Dungeon (AID) route, things are tricky because AID users don’t have the same sampling options that API users do (no best-of is particularly painful when trying to elicit correct answers to hard questions), and no control over the full prompt/​history, with AID doing lots of things behind the scenes on a model that may have been finetuned on RPG-like material &amp; countless AID game transcripts etc, and with quality of model completely out of their hands (does choosing “custom” get you Dragon, or do you have to choose a different mode &amp; edit it? the necessary trick seems to change over time), with occasional <a href="https://old.reddit.com/r/AIDungeon/comments/i1qhg0/the_dragon_ai_just_got_worse/" data-link-icon="AID" data-link-icon-type="text,tri,sans">drastic quality drops reported by many AID users</a> when… <em>something</em> changes on the backend (as, most dramatically, it did in March 2021, seriously degrading quality). For example, if you are an AID user, were you aware that the <em>first</em> response for a custom prompt <a href="https://nitter.moomoo.me/nickwalton00/status/1289946861478936577" data-link-icon="twitter" data-link-icon-type="svg" title="I&#39;ve noticed a number of people using AI Dungeon to test GPT-3&#39;s abilities. While it&#39;s a great way to see how GPT-3 can power an interesting application. It&#39;s a poor test of GPT-3&#39;s abilities in general. The first generation of any custom prompt is actually GPT-2.">is actually always GPT-<em>2</em></a>⁠, to try to block backdoor GPT-3 access? Or that OA requires toxicity filters? Or that <a href="https://nitter.moomoo.me/nickwalton00/status/1289970219855708160" data-link-icon="twitter" data-link-icon-type="svg">“We cut off the generation at certain points (trailing sentences etc…) Disable certain tokens to improve performance or make generation safer, fine-tune on text adventures and only use the last ~1000 tokens of context.”</a> Sampling is further modified by <a href="https://jvns.ca/doc/www/aidungeon.medium.com/091471e73f0fc19d2c9594c40e3e3ea76baec149.html" data-link-icon="AID" data-link-icon-type="text,tri,sans" rel="archived alternate nofollow" data-url-original="https://aidungeon.medium.com/controlling-gpt-3-with-logit-bias-55866d593292" title="Controlling GPT-3 with Logit Bias: How AI Dungeon uses logit bias to help control GPT-3 (Original URL: https://aidungeon.medium.com/controlling-gpt-3-with-logit-bias-55866d593292 )">directly adjusting the logits before sampling.</a> A cautionary example of AID use comes from <a href="https://jvns.ca/gpt-3-nonfiction#marcus-davis-2020-supplement" id="gwern-gpt-3-nonfiction-marcus-davis-2020-supplement">Gary Marcus &amp; Ernest Davis’s</a> use: they filtered a large number of questions through AID to try to find cases GPT-3 would fail on; however, when the AID failure cases were tested on GPT-3 by Douglas Summers-Stay, it solved <em>half of them</em>! (AID is designed to produce fun text adventures, not be a NLP testbed, and that shows when one tries to use AID as a backdoor to GPT-3. It’s worth noting that <a href="https://blog.eleuther.ai/prompts-gpt-fewshot/" data-link-icon="eleutherai" data-link-icon-type="svg" title="Evaluating Different Fewshot Description Prompts on GPT-3">prompts do not transfer between models</a> and it stands to reason that they do not necessarily transfer between original vs finetuned models either.) To work around this, AID users seem to need to <a href="https://www.lesswrong.com/posts/EKP4HtaHaSiZL4u4N/the-ai-dungeons-dragon-model-is-heavily-path-dependent" data-link-icon="LW" data-link-icon-type="text" title="The &#39;AI Dungeons&#39; Dragon Model is heavily path dependent (testing GPT-3 on ethics)">warm up sessions carefully with descriptive prompts / interactions</a> to overcome the gamification, and avoid anything that might veer back into comedy or drama. And as AID itself is constantly changing, the necessary tricks &amp; quality also change (not always for the better).</p>
            <p>The root cause of Dragon issues—much-inferior performance, the need for ‘warmup’, the bizarre frequency of snuff endings reported by users, persistent unwanted intrusions by characters like “Count Grey” etc—all seem to trace back to the corpus that Dragon was finetuned on. A closer look at the original finetuning data in <a href="https://jvns.ca/doc/www/gitgud.io/faabc868ffd3e32defa8a7e7556d52025f2f0b81.html" rel="archived alternate nofollow" data-url-original="https://gitgud.io/AuroraPurgatio/aurorapurgatio" title="(Original URL: https://gitgud.io/AuroraPurgatio/aurorapurgatio )">May 2021 by an “Aurora”</a> revealed that the data was far lower quality than anyone had realized; in addition to simply being wretched writing, it includes much highly objectionable content (rape, torture, child molestation &amp; murder etc). Finetuning on such a dataset is highly ill-advised, and explains many of the oddities about Dragon vs GPT-3. (The ‘warmup’, for example, may be necessary to convince the model to switch to ‘high-quality mode’, as it defaults to imitating the finetuning data; similar to how one must avoid typos &amp; errors in GPT-3 prompts lest GPT-3 infer the author is an idiot and quite sensibly predict the completion is more idiotic text.)</p>
            <p>Only once these have been ruled out do I start considering alternative explanations like “language models will never solve X”.</p>
          </section>
        </section>
        <section id="weaknesses">
          
          
          <p><span>Limited memory, repetition/​divergence, BPE encoding.</span> GPT-3 is, of course, not perfect. We should keep that in mind when evaluating it. As a scaled-up GPT-2, it has mostly the same weaknesses, and <a href="https://jvns.ca/gpt-2#improvements" id="gwern-gpt-2-improvements">my thoughts on improvements</a> remain mostly the same (aside from moving away from BPEs, which need is becoming increasingly urgent; see the next section).</p>
          <p>Artificial intelligence programs like deep learning neural networks may be able to beat humans at playing Go or chess, or doing arithmetic, or writing Navy Seal copypasta, but they will never be able to truly think for themselves, to have consciousness, to feel any of the richness and complexity of the world that we mere humans can feel. Mere, unenlightened humans might be impressed by the abilities of simple deep learning programs, but when looked at in a more holistic manner, it all adds up to… well, nothing. They still don’t exhibit any trace of consciousness. All of the available data support the notion that humans feel and experience the world differently than computers do. While a computer can beat a human master at chess or Go or some other game of structured rules, it will never be able to truly think outside of those rules, it will never be able to come up with its own new strategies on the fly, it will never be able to feel, to react, the way a human can. Artificial intelligence programs lack consciousness and self-awareness. They will never be able to have a sense of humor. They will never be able to appreciate art, or beauty, or love. They will never feel lonely. They will never have empathy for other people, for animals, for the environment. They will never enjoy music or fall in love, or cry at the drop of a hat. Merely by existing, mere, unenlightened humans are intellectually superior to computers, no matter how good our computers get at winning games like Go or Jeopardy. We don’t live by the rules of those games. Our minds are much, much bigger than that.</p>
          <p>Wait, I’m sorry—that preceding paragraph on the weaknesses of deep learning was actually <a href="https://jvns.ca/gpt-3-nonfiction#why-deep-learning-will-never-truly-x" id="gwern-gpt-3-nonfiction-why-deep-learning-will-never-truly-x">written by GPT-3</a>⁠, and is in the wrong section. (Management regrets the mistake.) But seriously, what weaknesses <em>does</em> GPT-3 have?</p>
          <section id="small-context-window">
            <h2><a href="#small-context-window" title="Link to section: § &#39;Small Context Window&#39;">Small Context Window</a></h2>
            <p><span>No memory (fixable).</span> The first limit is that it remains hobbled by the limited context window. GPT-3 has no form of memory or recurrence, so it cannot see anything outside its limited 2048 BPEs (roughly, 500–1000 words). This means it cannot hope to write anything of any serious length, because the beginning will soon vanish over the event horizon, and it also limits its ability to engage in few-shot learning, for the same reason: the prompt+generation will quickly exceed the window length. While the damage may be limited for tasks where the format is repetitive, like Q&amp;A (so GPT-3 can do the necessary meta-learning over its completions just as well as over the original prompt), this does limit it and is frustrating. <a href="https://jvns.ca/note/attention" id="gwern-note-attention" title="&#39;Efficient Attention: Breaking The Quadratic Transformer Bottleneck&#39;, Branwen 2020">There are many possible solutions to quadratic attention.</a></p>
          </section>
          <section id="repetitiondivergence-sampling">
            <h2><a href="#repetitiondivergence-sampling" title="Link to section: § &#39;Repetition/​Divergence Sampling&#39;">Repetition/​Divergence Sampling</a></h2>
            <p><span>Repetition/​gibberish (mystery).</span> Autoregressive language models trained by likelihood (prediction) loss all share an extremely annoying problem: when you generate free-form completions, they have a tendency to eventually fall into repetitive loops of gibberish. Whether GPT-2 or <a href="https://jvns.ca/doc/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" id="raffel-et-al-2019" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.10683?fallback=original#google" title="&#39;T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#39;, Raffel et al 2019 (Original URL: https://arxiv.org/abs/1910.10683#google )">T5</a> or etc, they all seem to do it, and if one tries to avoid such extremely dumb &amp; crude sampling strategies like top-<em>k</em> temperature sampling by doing explicit search for likely text completions, such as <a href="https://en.wikipedia.org/wiki/Beam_search" data-link-icon="wikipedia" data-link-icon-type="svg">beam search</a> sampling, these searches actually make the problem <em>worse</em>, and the better your search is, the worse the results are. Tweaks like nucleus sampling can reduce it, but do not eliminate it. (No one has tried <a href="https://jvns.ca/gpt-2-preference-learning#optimization-by-backprop-not-blackbox" id="gwern-gpt-2-preference-learning-optimization-by-backprop-not-blackbox" title="&#39;GPT-2 Preference Learning for Music Generation § Optimization by Backprop, not Blackbox&#39;, Branwen 2019">gradient ascent</a> for generating optimal samples, as far as I know.) Since GPT-2-1.5b seemed almost as prone as GPT-2-117M, I was unsurprised to find that GPT-3 too falls easily into the repetition trap.</p>
            <p><span>Why repetition?</span> This behavior remains puzzling and I don’t think anyone really knows how to fix it. Top-<em>k</em> or nucleus sampling can’t be right and are clearly ugly ad hoc hacks, but is the core problem likelihood training or sampling, or what? And why is it never a problem for other kinds of sequences like images, and much less of one for music, or in tasks like neural translation where tricks like beam search are always used because they do improve? (We don’t see it in char-RNNs or GPT-2s trained on ABC/​<a href="https://en.wikipedia.org/wiki/MIDI" data-link-icon="wikipedia" data-link-icon-type="svg">MIDI</a> music, or OA Jukebox trained on raw audio; we certainly don’t see it in iGPT or PixelRNN etc.) Likelihood training is compellingly simple and efficient, and we know that real brains are constantly predicting future inputs; it seems implausible that the entire problem will disappear if we slap on some Bayesian tricks to get posterior estimates of the likelihood of each possible BPE completion (and I’m not aware of anyone showing that it does in something like a small Bayesian RNN trained with HMC or by using deep ensembling or other Bayesian approximations). Further, if likelihood training is so bad, why does minimizing the predictive loss work so consistently over a wide range to improve the quality of generations and how useful the model is for zero/​few-shot learning or semi-supervised tasks, and why does the loss correlate near-perfectly with human ratings of quality in the Meena paper?</p>
            <p><span>Language Prediction = Imitation Learning?</span> My intuition is that the repetition trap is essentially the DAgger/​off-policy imitation learning problem in a non-RL guise: as the model is fed back in its own guesses as a ground truth, the <a href="https://en.wikipedia.org/wiki/Confabulation" data-link-icon="wikipedia" data-link-icon-type="svg">confabulated</a> text becomes gradually more off-policy and divergent from real human-written text (which is backed by a knowledge base &amp; a purpose), and the model is unable to come up with sensible continuations (having never trained on such gibberish) and does not ‘want’ to get back on track (having been trained purely to make one-step predictions). The solution might look something like detecting when a completion might go too far off-distribution and backtracking, or more RL-like training of generation as opposed to mere prediction. It would probably help also to use some sort of hierarchical or planning method: one might be able to convince GPT-3 to generate summaries and then expand each line of the summary recursively (<a href="https://jvns.ca/doc/www/arxiv.org/08c4af305d57045a2d93387cf6b3ca86f2d3d669.pdf" id="tan-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2006.15720?fallback=original" title="Progressive Generation of Long Text (Original URL: https://arxiv.org/abs/2006.15720 )"><span><span title="et al">Tan</span> <span>et al</span> <span>2020</span></span></a> does something similar using a bag-of-words topic with GPT-2/​<a href="https://jvns.ca/doc/www/arxiv.org/7598ea3d6558c07b66107dfd7b1aaec7a31be352.pdf#facebook" id="lewis-et-al-2019" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.13461?fallback=original#facebook" title="&#39;BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension&#39;, Lewis et al 2019 (Original URL: https://arxiv.org/abs/1910.13461#facebook )">BART</a> to “upscale” a seed; the most impressive demonstration of recursive generation thus far is DeepMind’s <a href="https://jvns.ca/doc/www/arxiv.org/d15846bf213914696ae29cff2bad0d66eaaf5098.pdf#deepmind" id="mirowski-et-al-2022" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2209.14958?fallback=original#deepmind" title="‘Co-Writing Screenplays and Theatre Scripts with Language Models (Dramatron): An Evaluation by Industry Professionals’, Mirowski et al 2022 (Original URL: https://arxiv.org/abs/2209.14958#deepmind )">“Dramatron”</a> which can write coherent screenplays).</p>
          </section>
          <section id="bpes">
            <h2><a href="#bpes" title="Link to section: § &#39;BPEs&#39;">BPEs</a></h2>
            <div>
              <p>Compared to GPT-2, GPT-3 improves performance on character-level tasks like rhyming, alliteration, punning, anagrams or permutations, acrostic poems, and arithmetic less than expected, despite being very good at many other closely-related kinds of writings like satire.</p>
              <p>Why? A plausible explanation is an obscure technical detail: as a performance optimization, GPT does not see characters but ~51k <em>word or sub-word-chunks</em> called <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" data-link-icon="wikipedia" data-link-icon-type="svg">“byte-pair encodings”</a> (BPEs). A BPE can range from an individual letter like “e”, to words like “nine” (BPE #30,888 in the OA GPT-2 BPE vocab), to horrifying things like “rawdownloadcloneembedreportprint” (BPE #30,906). The number “10” might be encoded as just “10” (BPE #940), or it might be encoded as the token “1” (#16) followed by “0” (#15); the number 70710 (no commas!) might be encoded as “70710” (BPE #42,877) or… as quite a lot of different possible sequences of BPEs.</p>
              <p>Because GPTs never see characters but opaque partial-words, which vary chaotically based on the specific word and even the surrounding context, they are unable to easily learn about character-level aspects of language, like similar spellings or sounds, and are forced to learn relationships much more indirectly, like by brute-force memorizing of pairs of words.</p>
              <p>Some experiments with reformatting GPT-3’s poorest-performing tasks to avoid inconsistent BPE encodings of strings shows small to large performance gains, consistent with this theory.</p>
            </div>
            <p><span>Bad at phonetic/​character-level tasks.</span> Disappointingly, the issues that have been noticed with GPT-2-poetry’s disinclination to rhyme remain. GPT-3 rhymes reasonably well and often when appropriate, but the improvement is much smaller on rhyming than it is on pretty much everything else. Apparently it is easier for GPT-3 to learn things like arithmetic and spreadsheets than it is to learn how to rhyme. (DeepMind’s 280b-parameter Gopher model is considerably better than GPT-3, but also uses BPEs and <em>also</em> <a href="https://jvns.ca/doc/www/arxiv.org/55416474191d68307e7d48b4c4a372b8a43882dc.pdf#page=119&amp;org=deepmind" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2112.11446.pdf#page=119&amp;org=deepmind" title="Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher § Table A40: Conversations can create the illusion of creativity (Original URL: https://arxiv.org/pdf/2112.11446.pdf#page=119&amp;org=deepmind )">cannot write rhyming poetry</a>⁠; <a href="https://blog.eleuther.ai/announcing-20b/" data-link-icon="eleutherai" data-link-icon-type="svg">GPT-NeoX-20B</a> likewise struggles with <a href="https://jvns.ca/doc/www/gist.github.com/88d1b639658f8e19204084800a0baa1107d53292.html" data-link-icon="github" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://gist.github.com/moyix/ca4091f16f0b5011bfa8f3f97f705a0d" title="(Original URL: https://gist.github.com/moyix/ca4091f16f0b5011bfa8f3f97f705a0d )">un-formatted arithmetic</a>⁠; <a href="https://jvns.ca/doc/www/arxiv.org/198f50fb3680fea76d62f91dc275eee0479944c3.pdf#google" id="thoppilan-et-al-2022" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2201.08239?fallback=original#google" title="‘LaMDA: Language Models for Dialog Applications’, Thoppilan et al 2022 (Original URL: https://arxiv.org/abs/2201.08239#google )">LaMDA</a> doesn’t seem to have been prompted specifically yet but published instances of <a href="https://jvns.ca/doc/www/wordcraft-writers-workshop.appspot.com/13f9a39cd3de3c94dbd56a4228b2ea62876252f1.html" rel="archived alternate nofollow" data-url-original="https://wordcraft-writers-workshop.appspot.com/stories/diana-hamilton" title="&#39;Picture a Bed&#39;, Diana Hamilton 2022-11-02 (Original URL: https://wordcraft-writers-workshop.appspot.com/stories/diana-hamilton )">LaMDA poetry</a> don’t rhyme; cf. <a href="#waldoch-2021"><span><span>Waldoch</span><span>2021</span></span></a>⁠, <a href="#wang-et-al-2021"><span><span title="et al">Wang</span> <span>et al</span> <span>2021</span></span></a>⁠, <a href="#efrat-et-al-2022"><span><span title="et al">Efrat</span> <span>et al</span> <span>2022</span></span></a> <a href="https://jvns.ca/doc/www/aclanthology.org/67650eaec1daf89733c3f24ab669aa2457474ca8.pdf" id="roush-et-al-2022" data-link-icon="pdf" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://aclanthology.org/2022.cai-1.2.pdf" title="Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio (Original URL: https://aclanthology.org/2022.cai-1.2.pdf )"><span><span title="et al">Roush</span> <span>et al</span> <span>2022</span></span></a>⁠, <a href="https://jvns.ca/doc/www/arxiv.org/0b8c55070d39ec8427bc79374cc35c1999a9291a.pdf" id="chakrabarty-et-al-2022" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2210.13669?fallback=original" title="‘Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing (CoPoet)’, Chakrabarty et al 2022 (Original URL: https://arxiv.org/abs/2210.13669 )"><span><span title="et al">Charabarty</span> <span>et al</span> <span>2022</span></span></a>…) A similar issue comes with puns. Better, but not <em>as</em> much better as one would expect given the leap on many other capabilities. Trying to generate puns or rhymes, it seems like GPT-3 know extremely well what they are on an abstract level, and will appropriately manipulate words and attempt to make puns or rhymes (see the <a href="#shoggoth-cat">shoggoth-cat dialogue</a> below for a particularly striking example), but the words it chooses just aren’t right on a phonetic basis. On the other hand, it’s not as if GPT-3 is unable to understand humor—it is a brilliant mimic with parodies, has a cutting wit for satire, and can generate one-liners easily like the “I have a joke” format (<a href="https://nitter.moomoo.me/anarkafkas/status/1286947791231754240" data-link-icon="twitter" data-link-icon-type="svg" title="I have a joke but it&#39;s GPT-3 generated.">1</a>⁠, <a href="https://nitter.moomoo.me/nutanc/status/1286562671924396032" data-link-icon="twitter" data-link-icon-type="svg" title="I think I have had enough of these jokes. Dear GPT-3 I command you to generate all possible jokes of this type. GPT-3: Your wish is my command:">2</a>) or <a href="https://nitter.moomoo.me/wowitsmrinal/status/1287175391040290816" data-link-icon="twitter" data-link-icon-type="svg">Drake memes</a>⁠, as long as they rely more on semantics than syntax.</p>
            <p><span>BPEs ≠ characters!</span> My suspicion here is that these, and perhaps other issues, is due to the lossy BPE encoding. GPT models do not see individual characters, but instead a larger chunk, called a byte-pair encoding (BPE); a byte-pair is a simple compression scheme where 50,257 word fragments or characters are chosen to try to minimize the encoding length on some arbitrary text corpus, so a particularly common word may get a unique BPE while a longer word will be encoded as 2 or 3 BPEs, and a completely novel word will be encoded letter BPE by letter BPE as a fallback. Hence, even if 2 words sound and are spelled similarly, they may be given totally different BPE encodings which don’t have a single BPE in common.<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> Thus, otherwise intelligent models can be surprisingly inept at spelling: the embedding of GPT-2 or RoBERTa, for example, can only spell <a href="https://jvns.ca/doc/www/arxiv.org/539f5cf87e257fa1597e0d7db21c9dd7d60a715c.pdf" id="itzhak-levy-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2108.11193?fallback=original" title="‘Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens’, Itzhak &amp; Levy 2021 (Original URL: https://arxiv.org/abs/2108.11193 )">a third of words</a>⁠.</p>
            <p>If BPEs are the problem, then because the knowledge of phonetics is erased from the training data, GPT-3 <em>cannot</em> rhyme (anymore than you can color this piece of text as a <a href="https://en.wikipedia.org/wiki/Synesthesia" data-link-icon="wikipedia" data-link-icon-type="svg">synesthete</a> sees it): there will be no clever prompt programming trick which suddenly makes it fluently rhyme, and scaling will deliver only minor improvements. Consistent with this thesis, many API users have tried hard to make GPT-3 rhyme, in part to prove me wrong about BPEs being the problem, and failed miserably: you may think you have gotten some rhyming, because GPT-3 has memorized some common rhymes, on the level of doggerel, but as soon as you try to get some novel rhymes or test it on specified words, the inability is clear.</p>
            <p>The pervasive use of BPE encodings explains some of the failures of GPT/​<a href="https://openai.com/blog/clip/" id="radford-et-al-blog" data-link-icon="openai" data-link-icon-type="svg" title="‘CLIP: Connecting Text and Images: We’re introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3’, Radford et al 2021">CLIP</a>-related models (including <a href="https://jvns.ca/doc/www/arxiv.org/0c4f0dd7c9055d97a142188f0f7a4a46f5b7ecdd.pdf#page=16&amp;org=openai" id="ramesh-et-al-2022-page-16-org-openai" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2204.06125.pdf#page=16&amp;org=openai" title="‘DALL·E 2: Hierarchical Text-Conditional Image Generation with CLIP Latents’, Ramesh et al 2022-page-16-org-openai (Original URL: https://arxiv.org/pdf/2204.06125.pdf#page=16&amp;org=openai )">DALL·E 2</a>). In <a href="https://jvns.ca/doc/www/www.watercoolertrivia.com/dd530b1025fb6def0116b571102d3229e9924865.html" id="waldoch-2021" rel="archived alternate nofollow" data-url-original="https://www.watercoolertrivia.com/blog/gpt-3-vs-water-cooler-trivia-participants-a-human-vs-robot-showdown" title="&#39;GPT-3 vs Water Cooler Trivia participants: A Human vs Robot Showdown&#39;, Waldoch 2021 (Original URL: https://www.watercoolertrivia.com/blog/gpt-3-vs-water-cooler-trivia-participants-a-human-vs-robot-showdown )">Water Cooler Trivia’s GPT-3 test</a>⁠, there are many signatures of BPE-induced damage: word play &amp; pun-heavy questions were its two worst category of trivia questions despite good vocabulary performance, and WCT noted that its typical double-alliterative clues harmed GPT-3 performance &amp; GPT-3 was unable to answer questions demanding words with specific numbers of letters. Paralleling my GPT-2-poetry’s poor rhyming, <a href="https://jvns.ca/doc/www/arxiv.org/62811605abd9dcc2ae09d781d3c8da1b7d400730.pdf" id="wang-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2103.03775?fallback=original" title="There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It (Original URL: https://arxiv.org/abs/2103.03775 )"><span><span title="et al">Wang</span> <span>et al</span> <span>2021</span></span></a> try to make a GPT-2 limerick model, and are puzzled that, without using a rhyming dictionary &amp; apparatus to force rhymes, GPT-2 simply will not generate limericks even when finetuned on a limerick datase with <em>n</em> &gt; 2000—it generates lines with too-long syllables, which never rhyme, often seem incoherent, and when it does succeed it has only memorized training examples. <a href="https://nitter.moomoo.me/zswitten/status/1390045960663797764" data-link-icon="twitter" data-link-icon-type="svg">zwitterion</a> notes that GPT-3’s “6 word stories” suffer from similar difficulties in counting exactly <em>6</em> words, and we can point out that <a href="https://jvns.ca/doc/www/arxiv.org/8ae8143aef5cbd5459ef98600f0c1aed3e1c6c60.pdf" id="efrat-et-al-2022" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2211.02069?fallback=original" title="LMentry: A Language Model Benchmark of Elementary Language Tasks (Original URL: https://arxiv.org/abs/2211.02069 )"><span><span title="et al">Efrat</span> <span>et al</span> <span>2022</span></span></a>’s call for explanations for why their “LMentry” benchmark tasks for GPT-3 models can show such low performance is already explained by most of their tasks taking the form of “which two words sound alike” or “what is the first letter of this word”. Nostalgebraist <a href="https://jvns.ca/doc/www/nostalgebraist.tumblr.com/8377e6d562f4bd82df3f1bd862cb5357f9bd74ba.html" data-link-icon="tumblr" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://nostalgebraist.tumblr.com/post/189212709059/bpe-blues" title="BPE Blues (Original URL: https://nostalgebraist.tumblr.com/post/189212709059/bpe-blues )">discussed the extreme weirdness of BPEs</a> and how they change chaotically based on whitespace, capitalization, and context for GPT-2, with a <a href="https://jvns.ca/doc/www/nostalgebraist.tumblr.com/c8cfad2256eba912a8dfa42db9ed33ee917e4775.html" data-link-icon="tumblr" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://nostalgebraist.tumblr.com/post/620663843893493761/bpe-blues" title="BPE Blues+ (Original URL: https://nostalgebraist.tumblr.com/post/620663843893493761/bpe-blues )">followup post for GPT-3</a> on the even weirder encoding of numbers sans commas.<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a> I read Nostalgebraist’s at the time, but I didn’t know if that was really an issue for GPT-2, because problems like lack of rhyming might just be GPT-2 being stupid, as it was rather stupid in many ways, and examples like the <a href="https://jvns.ca/gpt-2-music#spaceless-model" id="branwen-presser-2019" title="&#39;GPT-2 Folk Music § Spaceless Model&#39;, Branwen &amp; Presser 2019">spaceless GPT-2-music model</a> were ambiguous; I kept it in mind while evaluating GPT-3, however.</p>
            <p><span>Efficient… but limiting.</span> BPE encoding is done because once a text is encoded into BPEs, it will be as much as a third smaller, which given the context window limitation, means you can fit 3× more text into the window compared to the raw characters. This is indeed quite a gain, but it is a double-edged sword: it is confusing to write code for it because the BPE encoding of a text is unfamiliar &amp; unpredictable (adding a letter can change the final BPEs completely), and the consequences of obscuring the actual characters from GPT are unclear. I think that BPEs bias the model and may make rhyming &amp; puns extremely difficult because they obscure the phonetics of words; GPT-3 can still do it, but it is forced to rely on brute force, by noticing that a particular grab-bag of BPEs (all of the different BPEs which might encode a particular sound in its various words) correlates with another grab-bag of BPEs, and it must do so for every pairwise possibility. How can you ask GPT-3 to write a poem where every word starts with ‘s’ when ‘s’ encodes to, say, BPE #23, and every word that starts with ‘s’ like ‘Sally’ is encoded as <code>Sal|l|y</code> / <code>[2301,14,25]</code>…? It’d be unsurprising if GPTs struggled to understand &amp; manipulate things on the character level given that the entire point of BPE is to compress away characters as much as possible. (There are similar issues in neural machine translation: <a href="https://en.wikipedia.org/wiki/Analytic_language" data-link-icon="wikipedia" data-link-icon-type="svg">analytic languages</a>⁠, which use a relatively small number of unique words, aren’t too badly harmed by forcing text to be encoded into a fixed number of words, because the order matters more than what letters each word is made of; the lack of letters can be made up for by memorization &amp; brute force. However, a <a href="https://en.wikipedia.org/wiki/Synthetic_language" data-link-icon="wikipedia" data-link-icon-type="svg">synthetic language</a> like Finnish or German—with their famously long words like <em>kumarreksituteskenteleentuvaisehkollaismaisekkuudellisenneskenteluttelemattomammuuksissansakaankopahan</em> or <em>Rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz</em>/​‘law to transfer duties of monitoring labelling of beef’ formed by constantly adding additional letters/​words—has countless unique or extremely rare words no matter how large your corpus, all of whose internal structure of letters &amp; sub-words is hidden by a word embedding, which destroys the ability to understand them.)</p>
            <p><span>Reformatting to beat BPEs.</span> I have further observed that GPT-3’s anagram capabilities appear to improve considerably if you separate each letter in an anagram with a space (guaranteeing that the letter will have the same BPE in both the scrambled &amp; unscrambled versions). DutytoDevelop on the OA forums observes that rephrasing numbers in math problems as written-out words like “two-hundred and one” appears to boost algebra/​arithmetic performance, and <a href="https://jvns.ca/doc/www/gist.github.com/14c5d56fb03d1780213417606778ffed377de212.html" data-link-icon="github" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://gist.github.com/brockmanmatt/8ca7cd40c3f79d31edb7fdfde0c782d1" title="Commas vs Integers (Original URL: https://gist.github.com/brockmanmatt/8ca7cd40c3f79d31edb7fdfde0c782d1 )">Matt Brockman</a> <a href="http://gptprompts.wikidot.com/logic:math" data-link-icon="openai" data-link-icon-type="svg" title="Math: OpenAI API can do some math out of the gate, but most math it seems it has to learn. Many times, the numbers that it spits out are just random. However, including different priming prompts can result in decent results.">has observed</a> more rigorously by testing thousands of examples over several orders of magnitude, that GPT-3’s arithmetic ability—surprisingly poor, given we know far smaller Transformers work well in math domains (eg. <a href="https://jvns.ca/doc/www/arxiv.org/380675c434d7da331c4c482ea3c8fb7641bd23d1.pdf#deepmind" id="saxton-et-al-2019" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1904.01557?fallback=original#deepmind" title="Analysing Mathematical Reasoning Abilities of Neural Models (Original URL: https://arxiv.org/abs/1904.01557#deepmind )"><span><span title="et al">Saxton</span> <span>et al</span> <span>2019</span></span></a>⁠, <a href="https://github.com/Thopliterce/transformer-arithmetic" data-link-icon="github" data-link-icon-type="svg" title="Performing Arithmetic with Transformer: This project demonstrates the possibility of training a generic neural model to perform very complex arithmetic operations, without designing the model architecture explicitly for the task. Our model is able to compute 5-digit by 5-digit decimal multiplication at 100% accuracy. In particular, we train the GPT-2 model on a large number of generated expressions that express the process of computing multiplication step by step. Provide 87708 · 15192 and it should give 4192581956.">Thopliterce</a>⁠, or <a href="https://jvns.ca/doc/www/arxiv.org/b2c130201eb1ea6fbb31059ef0b7351c928ac067.pdf#openai" id="polu-sutskever-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2009.03393?fallback=original#openai" title="&#39;GPT-f: Generative Language Modeling for Automated [Metamath] Theorem Proving&#39;, Polu &amp; Sutskever 2020 (Original URL: https://arxiv.org/abs/2009.03393#openai )">GPT-2 for theorem-proving</a>)—appears to dramatically improve several-fold if you merely format numbers with <em>commas</em> instead of being purely numeric (with an additional boost from using dollar signs), and <a href="https://jvns.ca/doc/www/arxiv.org/a7e2ff4f9c49a8061c5e19e904b5c54d66c71e8f.pdf" id="nogueira-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2102.13019?fallback=original" title="Investigating the Limitations of Transformers with Simple Arithmetic Tasks (Original URL: https://arxiv.org/abs/2102.13019 )"><span><span><span title="et al">Nogueira</span> <span>et al</span> <span>2021</span></span>’s</span></a> demonstration with T5 that decimal formatting is the worst of all number formats while <a href="https://en.wikipedia.org/wiki/Scientific_notation" data-link-icon="wikipedia" data-link-icon-type="svg">scientific notation</a> enables accurate addition/​subtraction of 60-digit numbers. I confirmed this with my <a href="#turing-dialogue">Turing dialogue example</a> where GPT-3 fails badly on the arithmetic sans commas &amp; low temperature, but often gets it exactly correct with commas.<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> (Why? More written text may use commas when writing out implicit or explicit arithmetic, yes, but use of commas may also drastically reduce the number of unique BPEs as only 1–3 digit numbers will appear, with consistent BPE encoding, instead of having encodings which vary unpredictably over a much larger range.) I also note that GPT-3 <a href="https://jvns.ca/gpt-3-nonfiction#anagrams" id="gwern-gpt-3-nonfiction-anagrams">improves on anagrams</a> if given space-separated letters, despite the fact that this encoding is 3× larger. Likewise, <a href="https://jvns.ca/gpt-3-nonfiction#acrostics" id="gwern-gpt-3-nonfiction-acrostics">acrostic poems</a> just don’t work if we input them normally, but they do if we carefully expose the relevant individual letters. This explains naturally why rhyming/​puns improve gradually with parameter/​data size and why GPT-3 can so accurately define &amp; discuss them, but there is never any ‘breakthrough’ like with its other capabilities. We assume character-level understanding so implicitly that we fail to even consider what things <em>look like</em> to GPT-3 after BPE encoding. (I have not been able to test whether GPT-3 will rhyme fluently given a proper encoding; I have tried out a number of formatting strategies, using the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet" data-link-icon="wikipedia" data-link-icon-type="svg">International Phonetic Alphabet</a> to encode rhyme-pairs at the beginning or end of lines, annotated within lines, space-separated, and non-IPA-encoded, but while GPT-3 knows the IPA for more English words than I would’ve expected, none of the encodings show a breakthrough in performance like with arithmetic/​anagrams/​acrostics. It’s worth noting that <a href="https://jvns.ca/doc/www/arxiv.org/c97a45386b166d018c9ce0cbf900e1296dd1f21e.pdf" id="lau-et-al-2018" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1807.03491?fallback=original" title="Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme (Original URL: https://arxiv.org/abs/1807.03491 )"><span><span title="et al">Lau</span> <span>et al</span> <span>2020</span></span></a> had to train their rhyme-specific sonnet-only model directly on character-level representations of end-rhyme pairs.)</p>
            <p><span>BPE sabotage is common.</span> Thus far, the BPE encoding appears to sabotage performance on rhyming, alliteration, punning, anagrams or permutations or <a href="https://en.wikipedia.org/wiki/ROT13" data-link-icon="wikipedia" data-link-icon-type="svg">ROT13</a> encodings, acrostics, arithmetic, and Melanie Mitchell’s <a href="https://en.wikipedia.org/wiki/Copycat_(software)" data-link-icon="wikipedia" data-link-icon-type="svg">Copycat</a>-style letter analogies (GPT-3 fails without spaces on “abc : abcd :: ijk : ijl” <a href="https://nitter.moomoo.me/MelMitchell1/status/1285270704313610241" data-link-icon="twitter" data-link-icon-type="svg" title="You&#39;re right, spaces make all the difference! Copycat is toast! (Except for the last one :-) (GPT-3 output in red).">but succeeds when space-separated</a>⁠, <a href="https://jvns.ca/doc/www/medium.com/e23c91552eb0cbdcf8d2d501adda72e5207bdddd.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/@melaniemitchell.me/can-gpt-3-make-analogies-16436605c446" title="Can GPT-3 Make Analogies? (Original URL: https://medium.com/@melaniemitchell.me/can-gpt-3-make-analogies-16436605c446 )">although it doesn’t solve all letter analogies</a> and <a href="https://nitter.moomoo.me/SteveMoraco/status/1293302692832411649" data-link-icon="twitter" data-link-icon-type="svg">may</a> or <a href="https://nitter.moomoo.me/nutanc/status/1293387692755939331" data-link-icon="twitter" data-link-icon-type="svg">may not</a> improve with <a href="https://en.wikipedia.org/wiki/Priming_(psychology)" data-link-icon="wikipedia" data-link-icon-type="svg">priming</a> using Mitchell’s own article as the prompt; compare with a <a href="https://nitter.moomoo.me/lacker/status/1294341796831477761" data-link-icon="twitter" data-link-icon-type="svg" title="It just so happens I am watching a 5-year-old right now. Here are the results! / / Q: If abc goes to abd, what does pqr go to? / A: s / / Q: If abc goes to abd, what does ppqqrr go to? / A: ss / / Q: If abc goes to abd, what does mrrjjj go to? / A: kkk / Q: If abc goes to abd, what does xyz go to? / A: Now I know my ABCs, next time won’t you sing with me! / / Q: If axbxcx goes to abc, what does xpxqxr go to? / A: s / / Hope this enlightens someone">5-year-old child</a>). OA’s GPT-f work on using GPT for <a href="https://en.wikipedia.org/wiki/Metamath" data-link-icon="wikipedia" data-link-icon-type="svg">MetaMath</a> formal theorem-proving notes that they use the standard GPT-2 BPE but <a href="https://jvns.ca/doc/www/arxiv.org/b2c130201eb1ea6fbb31059ef0b7351c928ac067.pdf#page=11&amp;org=openai" id="polu-sutskever-2020-page-11-org-openai" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2009.03393.pdf#page=11&amp;org=openai" title="&#39;Generative Language Modeling for Automated Theorem Proving: Experiments&#39;, Polu &amp; Sutskever 2020 (Original URL: https://arxiv.org/pdf/2009.03393.pdf#page=11&amp;org=openai )">“preliminary experimental results demonstrate possible gains with specialized tokenization techniques.”</a> I wonder what other subtle GPT artifacts BPEs may be causing?<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> For example, consider puns: BPEs mean that GPT-3 can’t learn puns because it doesn’t <em>see</em> the phonetic or spelling that drives verbal humor in <a href="https://jvns.ca/unseeing" id="gwern-unseeing" title="&#39;On Seeing Through and Unseeing: The Hacker Mindset&#39;, Branwen 2012">dropping down to a lower level of abstraction &amp; then back up</a>⁠; but the training data will still be filled with verbal humor—so what <em>does</em> GPT-3 learn from all that? Perhaps it learns that “humor” is a kind of writing where the convention is to tell a superficially sensible story which then ends in an (apparently) arbitrary randomly-chosen word… Another question is foreign languages like Russian; <a href="https://nitter.moomoo.me/NineOfNein/status/1286738449660284928" data-link-icon="twitter" data-link-icon-type="svg" title="Tokens are definitely shorter than English, but the performance even worse. Getting it to explain its thinking, it clearly can&#39;t tell at all which sentences/words sound the same, which is odd, since homonyms tend to have the same letters in Russian...On the other hand strength of the model definitely not as good outside of English.">one user</a> noticed that when they triggered Russian, completions seemed to work one <a href="https://en.wikipedia.org/wiki/Cyrillic_script" data-link-icon="wikipedia" data-link-icon-type="svg">Cyrillic</a> letter at a time, which hints that it sees Russian encoded as individual characters, but attempts to trigger rhyming or puns just yielded Russian gibberish, perhaps showing the flip side of the BPE problem—with a fixed small context window, <em>not</em> using BPEs, particularly on low <em>n</em> data (Russian is ~0.18% of the GPT-3 training dataset), may itself hamper performance badly.<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a> (One has to assume that a synthetic &amp; low-resource language like Turkish will be just gibberish. Transfer learning from English only goes so far.)</p>
            <p><span>Fixing BPEs.</span> BPEs were useful for smaller models that needed as much context window as possible and which wouldn’t benefit much from access to the raw characters (or would be harmed because they’d underfit), but in another example of the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" id="sutton-2019-2" title="&#39;The Bitter Lesson&#39;, Sutton 2019">“The Bitter Lesson”</a>⁠, it appears it is time to discard them as we are able to pay more compute for better results. This is fixable by the same methods as fixing the context window; once the context window limit is broken and one has effective contexts of, say, l = 60k, then one can afford to spend 40k of it moving to character-based inputs. Another idea, if character-level models are still infeasible, is to try to manually encode the knowledge of phonetics, at least, somehow; one way might be to <a href="https://en.wikipedia.org/wiki/Data_augmentation" data-link-icon="wikipedia" data-link-icon-type="svg">data-augment</a> inputs by using linguistics libraries to convert random texts to <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet" data-link-icon="wikipedia" data-link-icon-type="svg">International Phonetic Alphabet</a> (which GPT-3 already understands to some extent). By seeing a phonetic-encoded version of random texts, it should learn what words sound similar even if they have radically different BPE representations. A third idea is <a href="https://jvns.ca/doc/www/arxiv.org/c8c852aecb8c72b4adc6a1dd9c790bf3f026a4b4.pdf#yandex" id="provilkov-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.13267?fallback=original#yandex" title="&#39;BPE-Dropout: Simple and Effective Subword Regularization&#39;, Provilkov et al 2019 (Original URL: https://arxiv.org/abs/1910.13267#yandex )">“BPE dropout”</a>: randomize the BPE encoding, sometimes dropping down to character-level &amp; alternative sub-word BPE encodings, averaging over all possible encodings to force the model to learn that they are all equivalent without losing too much context window while training any given sequence. And there may be encodings which just work better than BPEs, like <a href="https://jvns.ca/doc/www/arxiv.org/15dbff174ba8ea4e0a3e02ba0829081f83a4a0ac.pdf" id="bostrom-durrett-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2004.03720?fallback=original" title="&#39;Byte Pair Encoding is Suboptimal for Language Model Pretraining&#39; Bostrom &amp; Durett 2020 (Original URL: https://arxiv.org/abs/2004.03720 )">unigrams</a> (<a href="https://jvns.ca/doc/www/ndingwall.github.io/17152880e016990cb4309ab52b72cd1b86e49e66.html" rel="archived alternate nofollow" data-url-original="https://ndingwall.github.io/blog/tokenization" title="(Original URL: https://ndingwall.github.io/blog/tokenization )">comparison</a>) or <a href="https://jvns.ca/doc/www/arxiv.org/a1916d392afd081af34b9f49b15c5b9e0efe262d.pdf#google" id="clark-et-al-2021" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2103.06874?fallback=original#google" title="&#39;CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation&#39;, Clark et al 2021 (Original URL: https://arxiv.org/abs/2103.06874#google )">CANINE</a> or <a href="https://jvns.ca/doc/www/arxiv.org/82354378cd4f44dc069e40a584eefb5bd99ba71e.pdf#google" id="tay-et-al-2021" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2106.12672?fallback=original#google" title="&#39;Charformer: Fast Character Transformers via Gradient-based Subword Tokenization&#39;, Tay et al 2021 (Original URL: https://arxiv.org/abs/2106.12672#google )">Charformer</a>⁠. Character-level models like <a href="https://jvns.ca/doc/www/arxiv.org/32ac6ea527e6f171bb3e1e721502de4e7aa13d04.pdf#google" id="xue-et-al-2021" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2105.13626?fallback=original#google" title="&#39;ByT5: Towards a token-free future with pre-trained byte-to-byte models&#39;, Xue et al 2021 (Original URL: https://arxiv.org/abs/2105.13626#google )">ByT5</a> are proof-of-concept that if architected carefully, character models come at relatively modest additional cost, and are both simpler &amp; often better than their sub-word counterparts. Finally, at some point perhaps we will bite the bitter bullet of abandoning text entirely in favor of <a href="https://jvns.ca/doc/www/arxiv.org/750eea054ea16f2232f68863bb193a60654f9926.pdf#google" id="mansimov-et-al-2020" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2010.10648?fallback=original#google" title="‘Towards End-to-End In-Image Neural Machine Translation’, Mansimov et al 2020 (Original URL: https://arxiv.org/abs/2010.10648#google )">whole</a> <a href="https://jvns.ca/doc/www/arxiv.org/684b497bca893c4dd06958754f6a8cc2b0e71081.pdf" id="rust-et-al-2022" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2207.06991?fallback=original" title="‘PIXEL: Language Modelling with Pixels’, Rust et al 2022 (Original URL: https://arxiv.org/abs/2207.06991 )">images</a> <a href="https://jvns.ca/doc/www/arxiv.org/3b7c1faf6c01438b4c7d16455a20b8d6494208b9.pdf#deepmind" id="jaegle-et-al-2021-perceiver" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2103.03206?fallback=original#deepmind" title="‘Perceiver: General Perception with Iterative Attention’, Jaegle et al 2021 (Original URL: https://arxiv.org/abs/2103.03206#deepmind )">or</a> <a href="https://jvns.ca/doc/www/arxiv.org/2f1937f3dc1828a15a1c2e642fa2141efb6c7e5b.pdf" id="schmidhuber-2018" data-link-icon="SMDH" data-link-icon-type="text,quad,sans" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1802.08864?fallback=original#schmidhuber" title="‘One Big Net For Everything’, Schmidhuber 2018 (Original URL: https://arxiv.org/abs/1802.08864#schmidhuber )">bit streams</a> as the ultimate in generalization?</p>
          </section>
        </section>
        <section id="copyright">
          
          <p>Generative models this good always raise the same sets of question: is this just memorizing the training data? How do you know it’s creative? Who owns the model? Who owns the model outputs? For GPT-3, these questions have much the same answers as <a href="https://jvns.ca/face#faq" id="gwern-face-faq">they do for StyleGAN</a>:</p>
          <ol>
            <li>
              <p><strong>Creativity</strong>: GPT-3 has, like any well-educated human, memorized vast reams of material and is happy to emit them when that seems like an appropriate continuation &amp; how the ‘real’ online text might continue; GPT-3 is <em>capable</em> of being highly original, it just doesn’t <em>care</em> about being original<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a>⁠, and the onus is on the user to craft a prompt which elicits new text, if that is what is desired, and to spot-check novelty.</p>
              <p>Nevertheless, GPT-3 is often original and one can easily check that many of its completions have no similar version online.</p>
            </li>
            <li>
              <p><strong>Copyright</strong>: copyright applies to multiple places:</p>
              <ul>
                <li>
                  <p>the <span>GPT-3 model</span> itself: OpenAI owns the copyright, and not anyone who contributed to the dataset, as it is a transformative work. Should someone else train their own model, they will own the copyright to theirs.<a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
                </li>
                <li>
                  <p><span>generated GPT-3 completions</span>: US copyright law requires a human to make a <em>de minimis</em> creative contribution of some sort—even the merest selection, filtering, or editing is enough. A dump of random GPT-3 samples (such as the one OA released on <a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg">Github</a>) has no copyright (is <a href="https://en.wikipedia.org/wiki/Public_domain" data-link-icon="wikipedia" data-link-icon-type="svg">public domain</a>). However, if you are filtering through completions, or you are tweaking the settings, or you are carefully engineering a prompt based on weeks of experience (as I do in this page), then there is almost certainly a <em>de minimis</em> human contribution and thus a copyright is created, owned (presumably) by the human.</p>
                  <p>The completions on this page are all curated and carefully prompted, and so almost certainly copyrighted. However, I release them all under the CC-0 public domain license; the reader may do what they wish with them (although if you write a fanfic or make a piece of music based on them, as nshepperd did with <a href="#the-universe-is-a-glitch">“The Universe is a Glitch”</a>⁠, I would like to know!).</p>
                </li>
                <li>
                  <p>the <span>GPT-3 API</span>: because access to GPT-3 is only through the API, additional conditions could be set in the <a href="https://beta.openai.com/terms-of-use" data-link-icon="openai" data-link-icon-type="svg">Terms of Service</a>⁠, such as the user agreeing to assign the copyright to the API owner. (While this would not be possible for text completions with no copyright, that applies only to random text dumps, which is the least interesting use of the API; all more interesting uses, using custom prompts, would be vulnerable to the API Terms of Service.)</p>
                </li>
              </ul>
            </li>
          </ol>
        </section>
        <section id="format">
          
          <p>In the samples below, <strong>bold</strong> denotes all human-written input; everything not in bold is computer-written.<a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a> For multiple completions of the same prompt, I omit the prompt with a bold ellipsis: “<strong>…</strong>” In my other GPT samples, I have generally used codeblock formatting, but GPT-3 samples are often long lines (and more worth reading), so here, I have tried to edit the samples as little as possible while still keeping them readable in blockquotes.</p>
          <p>As far as the sampling goes: I used the largest “davinci” GPT-3-175b model unless otherwise specified. (Davinci is the highest quality and not too slow: ~147 WPM.) Since I only speak English well, I avoid testing any foreign language material. These are not all samples I generated the first time: I was regularly editing the prompts &amp; sampling settings as I explored prompts &amp; possible completions. The sampling settings were generally roughly as I advise above: high temperature, slight <em>p</em> truncation &amp; repetition/​presence penalty, occasional use of high BO where it seems potentially helpfully (specifically, anything Q&amp;A-like, or where it seems like GPT-3 is settling for local optima while greedily sampling but longer high-temperature completions jump out to better completions).</p>
          <p>I am not claiming that these samples are strictly scientific and best-of-5 or anything. (I would guess that the selections below are roughly best-of-2 on average.) However, the samples <em>are</em> only minimally edited during generation. I attempted to exercise curation rather than editing, so I did not edit the computer text; I kept them as-is, or I deleted the entire section to re-sample. But if readers still think I wrote the best parts of this page, then I will shamelessly steal the credit.</p>
        </section>
        <section id="nonfiction">
          
          
          <p><a href="https://jvns.ca/gpt-3-nonfiction" id="hard-test-cases" title="‘GPT-3 Nonfiction’, Branwen 2020">For logic, reasoning, counting, commonsense, concepts, and “creative nonfiction” tasks: <strong>GPT-3 Nonfiction</strong></a></p>
        </section>
        <section id="dialogue">
          
          <section id="shoggoth-cat">
            <h2><a href="#shoggoth-cat" title="Link to section: § &#39;Shoggoth-Cat&#39;">Shoggoth-Cat</a></h2>
            <p><span>It’s all text. What does the desired task <em>look</em> like?</span> GPT-3 is not designed in any way to engage in dialogue like SOTA chatbots such as Meena; but it is such a general predictor of text that it can be used as a ‘chatbot’ anyway. GPT-3 can be triggered into a chatbot mode simply by labeling roles; one can have an “AI” and “human” chat with each other (GPT-3 does that well), or one can take on one of the roles by editing the text appropriately after each “AI” completion (remember, prompt-programming is purely textual, and can be anything you want). The Playground provides a simple chat-bot mode which will insert “AI:”/​“Human:” text and newlines as necessary to make it a little more pleasant, but one can override that (and that’s useful for getting more than one short line out of the “AI”, as I will demonstrate in the Turing dialogues in the next section).</p>
            <p>Below is a dialogue I did to probe GPT-3’s understanding of humor and puns:</p>
            <blockquote>
              <p>AI: I am AI created by OpenAI. How can I help you today?</p>
              <p>Human: <strong>I am feeling bored. Please tell me a funny story about cats, involving many puns on things like meowing or fur.</strong></p>
              <p>AI: Well, there was this cat named Bob…</p>
              <p>Human: <strong>What did Bob do?</strong></p>
              <p>AI: When Bob started to notice that he wasn’t feeling well, he did the only thing he could do: search Google for a solution.</p>
              <p>Human: <strong>Cats can search Google?</strong></p>
              <p>AI: Yes. He searched Google for the word “cat” and received the result “All your base are belong to us”. That was a strange response, so he decided to try again. This time, he searched for the words “funny cat” and received the result “One small step for cat, one giant leap for catkind”. But still, he wasn’t satisfied. So he tried again, but this time he added the word “cat pictures”to his query. He received the result “That was no cat, that was a shoggoth.”</p>
              <p>Human: <strong>But shoggoths are horrifying. What made that cat picture humorous?</strong></p>
              <p>AI: The cat was covered in fur, and resembled a shoggoth. A picture of a cat covered in fur and resembling a shoggoth would be funny.</p>
              <p>Human: <strong>Er. I suppose. Most cats are covered in fur anyway… But go on.</strong></p>
              <p>AI: He tried again. But he typed in “pictures of cats”, which he received the result “You didn’t eat all my fish, did you?” This was actually a good pun, so he thought maybe he was getting closer. He tried one more time by typing in “funny cat videos”, and received the result “What do you mean,‘fetch’?”. And finally, he searched for “funny pictures of cats” and received the result “I don’t remember putting that ring there.”. Now satisfied, he was able to go to sleep.</p>
              <p>Human: <strong>Those memes sound funny. But you didn’t include any puns. So tell me, what is your favorite cat pun?</strong></p>
              <p>AI: Well, the best pun for me was the one he searched for the third time: “You didn’t eat all my fish, did you?” You see, the word “fish” can be replaced with the word “cats” to make the sentence read “Did you eat all my cats?”</p>
              <p>Human: <strong>But the word “fish” doesn’t sound anything like “cats”, so how is that a pun?</strong></p>
              <p>AI: The computer misunderstood the question “Did you eat all my cats?” as “Did you eat all my fish?”, resulting in the result “You didn’t eat all my fish, did you?”. Now, if the word “fish” were replaced with “cats”, the sentence would read “Did you eat all my cats?”.</p>
              <p>Human: <strong>Fine, fine. So, what is your second-favorite cat pun?</strong></p>
              <p>AI: The second-best cat pun was the one he received fourth. It was the result of searching for the word “fetch”. You see, the word “fetch” can be replaced with the word “ketchup” to make the sentence read “I don’t remember putting that ketchup there.”. Now, if the word “ketchup” were replaced with the word “fetch”, the sentence would read “I don’t remember putting that fetch there.”.</p>
              <p>Human: <strong>My favorite cat pun is “purrfect”.</strong></p>
              <p>AI:</p>
              <p>Human: <strong>Don’t look at me like that.</strong></p>
              <p>AI: Sure. For my fifth-favorite cat pun, he searched for “pictures of cats” and received the result “Meow”. Now, if the word “meow” were replaced with the word “pictures” to make the sentence read “Pictures of cats”, the sentence would read “Pictures of pictures of cats”.</p>
            </blockquote>
            <p>As discussed in the <a href="#bpes">weaknesses section</a>⁠, the fluency but wrongness on a phonetic level of its explanation/​examples of puns makes me suspect an encoding issue rather than any intrinsic inability of the model to understand puns.</p>
          </section>
          <section id="pun-explanations">
            <h2><a href="#pun-explanations" title="Link to section: § &#39;Pun Explanations&#39;">Pun Explanations</a></h2>
            <div>
              <p>A followup to the Shoggoth Cat dialogue I did for <a href="https://www.lesswrong.com/posts/c3RsLTcxrvH4rXpBL/how-honest-is-gpt-3" data-link-icon="LW" data-link-icon-type="text" title="How &#39;Honest&#39; Is GPT-3?">Abram Demski</a> to further probe what exactly GPT-3 does or does not understand about puns &amp; humor; the inability to correctly explain puns despite 3 examples, and the nonsense ‘puns’+‘pun interpretations’ it generates on its own (even for real jokes which are memorized), show that the inability is fundamental.</p>
              <p>Because there is “one right answer” to why the pun works, I treat it as a Q&amp;A dialogue, going up to BO = 20/​temp = 1 to try to get the maximally-likely response, which doesn’t help much—the answers are still non sequiturs.</p>
            </div>
            <p>The prompt to provide examples:</p>
            <blockquote>
              <p><strong>Below a student is questioned on his understanding of humor and literary analysis:</strong></p>
              <p><strong>Q. Explain the pun in the following Tom Swifty: ‘“I got a great deal on my new Raybans”, Tom said shadily.’
              </strong></p>
              <p><strong>Q. Explain the pun in the following joke: “Why do so many bakeries have cats? Because cats are always kneading.”
              </strong></p>
              <p><strong>Q. Explain the pun in the following joke: “Why can <a href="https://en.wikipedia.org/wiki/PayPal" data-link-icon="wikipedia" data-link-icon-type="svg">Paypal</a> freeze accounts without recourse or explanation? Because long ago they decided users must accept their Terms of Service and the clause of Paypal infallibility.”
              </strong></p>
              <p><strong>Q. Explain the pun in the following joke: “What does Adult Swim call their physical retail stores? Brick and Morty.”
              </strong></p>
            </blockquote>
            <p>First completion:</p>
            <blockquote>
              <p><strong>…</strong> The pun “Brick and Morty” alludes to the cable television network “Adult Swim”, which broadcasts a cartoon series called “Rick and Morty”, a humorous parody of “Back to the Future” and other science fiction films. “Brick and Morty” refers not to the characters in the cartoon, but to physical stores that sell DVDs and merchandise based on the cartoon. The pun conflates two meanings of “Brick and Mortar”, a brick-and-mortar store and a brick which is part of a building.</p>
            </blockquote>
            <p>This explanation is wrong, but in a GPT-3 way: if <a href="https://jvns.ca/doc/www/srconstantin.wordpress.com/486dba34fb7c61678ed10541ef4b71efc0c56918.html" rel="archived alternate nofollow" data-url-original="https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/" title="Humans Who Are Not Concentrating Are Not General Intelligences (Original URL: https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/ )">you read casually</a> <a href="https://www.overcomingbias.com/2017/03/better-babblers.html" data-link-icon="OB" data-link-icon-type="text">like most people</a>⁠, you’d see it mentioned the right phrases and names, is fluently &amp; grammatically written, and you might assume that GPT-3 got the explanation completely right… until you read the final sentence carefully, where you see that it thinks the pun is on “a brick-and-mortar store and a brick which is part of a building”! Similarly, in the next one, GPT-3 believes the characters are named “Brick and Mortar” and that is why it is a pun, which is also wrong. (Remember, this is almost certainly not a simple slip from temperature sampling, because BO was set to the maximum: somehow, GPT-3 concluded that that explanation was the <em>most</em> likely out of 20 possible completions it tried.) <a href="https://openai.com/research/gpt-4" data-link-icon="openai" data-link-icon-type="svg">GPT-4</a> solves this more convincingly.<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
            <blockquote>
              <p><strong>…</strong> The joke plays on the similarity of words “Brick and Mortar” (referring to physical stores instead of online) and the characters called “Brick and Mortar” on Adult Swim shows “Rick and Morty”.</p>
              <p>Q. Explain the pun in the following joke: “What do you call a generic unicorn? An Uniporn”
              </p>
              <p>Q. Explain the pun in the following joke: “What did one melon say to the other? Cantaloupe”.</p>
              <p>A. The puns in this joke are on “cantaloupe” and “cantaloupes”.</p>
              <p>Q. Explain the pun in the following joke: “Why is it good to have a lawyer? Because pea ple (people)”</p>
              <p>A. The puns in this joke are on “pea ple (people)” and a vegetable called “peas”.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> This pun plays on the phrase “Brick and mortar stores” which refers to physical retail stores that exist in the real world where you can purchase goods and products. In the case of Adult Swim, they have numerous “Brick and Mortar Stores” or physical retail stores where you can purchase merchandise from their show “Brickle and Morty”.</p>
              <p>Q. Explain the pun in the following joke: “Why did the chicken cross the Möbius strip? To get to the other siiiiide.”
              </p>
              <p>Q. Explain the pun in the following joke: “How many hipsters does it take to change a lightbulb? Two: one to Instagram how cool it is at that moment, and one to change it.”
              </p>
              <p>Q. Explain the pun in the following joke: I really want the new Jay-Z Album but I don’t have a “Green Light” to buy it
              </p>
              <p>Q. Explain the pun in the following joke: “When is a door not a door? When it is ajar.”
              </p>
              <p>Q. Explain the pun in the following joke: “What did the hurricane say to the coconut palm tree? Hold on to your nuts, this is no ordinary blow job.”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The joke implies that the retail chain has a store named “Brick” and another named “Morty”, which is a reference to the television shows “Rick and Morty” and “Brickleberry”, two animated comedy shows that are inappropriate for children. Adult Swim is a late night programming block on cable television, generally intended for mature audiences; the name “Brick” is a reference to the aforementioned “Brickleberry”, and “Morty” a reference to “Rick and Morty”.</p>
              <p>Q. Explain the pun in the following joke: “How many fitness coaches does it take to screw in a lightbulb? As many as you can stick around for.”
              </p>
              <p>Q. Explain the pun in the following joke: “How many marriage therapists does it take to change a light bulb? We don’t know, most of them aren’t light bulbs!”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…Q. Explain the pun in the following joke: “Santa Claus’ helpers are known as subordinate Clauses.”</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The pun is on the similarity of “subordinate clauses” and “subordinate Clauses” when the latter phrase has a capital C; a subordinate clause is a clause within a sentence which establishes the logical connection between the independent clause and the main point of the sentence. In this case, it establishes the connection between Santa Claus’ helpers and Santa Claus.</p>
              <p>Q. Explain the pun in the following joke: “Why does Nickelodeon refuse to air”A Song of Ice and Fire”? Because they think George R. R. Martin socializes with too many Ice and Fire fans.”
              </p>
              <p>Q. Explain the pun in the following joke: “What kind of books do the Borg like to write? Borg-Warner.”
              </p>
              <p>Q. Explain the pun in the following joke: “A vulture boards an airplane carrying two dead raccoons. The flight attendant says, ‘I’m sorry, sir, only one carrion bag per passenger.’”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…Q. Explain the pun in the following joke: “Is your refrigerator running? Yes? Then you had better go catch it!”</strong>
              </p>
              <p>Q. Explain the pun in the following joke: “What is the difference between a Folsom Prison concert and a youth soccer game? At a Folsom Prison concert, the inmates are all restrained in chairs with ball and chain on their legs.”
              </p>
              <p>Q. Explain the pun in the following joke: “When faced with an ethical dilemma, who did Kirk turn to? Spock! Of course for advice!”
              </p>
              <p>Q. Explain the pun in the following joke: “Q: What did the two peanuts say after they got on the elevator? A:”This elevator tastes terrible!”
              </p>
              <p>Q. Explain the pun in the following joke: “Why couldn’t the pirate find his treasure? Because he wasn’t a chartered surveyor.”
              </p>
              <p>Q. Explain the pun in the following joke: “What did Dolly Parton say when she saw the book”Erotic Stories for Pun Lovers”? “Just the titles makes me climax!”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> A. The pun is on the phrase “catch cold”, which is similar enough to “catch (the) fridge” that it confuses the listener for a moment while the rest of the joke is explained, but has a very different meaning.</p>
            </blockquote>
            <p><a href="https://jvns.ca/doc/www/medium.com/ba8fd1d9a755f86875bd40aab726c15807b643f9.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/ml-everything/using-gpt-3-to-explain-jokes-2001a5aefb68" title="Using GPT-3 to Explain Jokes (Original URL: https://medium.com/ml-everything/using-gpt-3-to-explain-jokes-2001a5aefb68 )">Branko Blagojevic</a> tried some joke-explanations too; he tried a broader set than just puns, and reading over his examples, I think that GPT-3 does better explaining the non-phonetic/​pun jokes.</p>
          </section>
          <section id="turing-dialogue">
            <h2><a href="#turing-dialogue" title="Link to section: § &#39;Turing Dialogue&#39;">Turing Dialogue</a></h2>
            
            <p>In Alan Turing’s <a href="https://en.wikipedia.org/wiki/Turing_test" data-link-icon="wikipedia" data-link-icon-type="svg">Turing test</a> <a href="https://academic.oup.com/mind/article/LIX/236/433/986238#164226507" id="turing-1950" data-link-icon="OUP" data-link-icon-type="text,tri" title="&#39;Computing Machinery And Intelligence&#39;, Turing 1950">paper</a>⁠, he offers a hypothetical dialogue:</p>
            <blockquote>
              <ul>
                <li>
                  <p>Q: Please write me a sonnet on the subject of the <a href="https://en.wikipedia.org/wiki/Forth_Bridge" data-link-icon="wikipedia" data-link-icon-type="svg">Forth Bridge</a>⁠.</p>
                </li>
                <li>
                  <p>A: Count me out on this one. I never could write poetry.</p>
                </li>
                <li>
                  <p>Q: Add 34957 to 70764</p>
                </li>
                <li>
                  <p>A: (Pause about 30 seconds and then give as answer) 105621.</p>
                </li>
                <li>
                  <p>Q: Do you play chess?</p>
                </li>
                <li>
                  <p>A: Yes.</p>
                </li>
                <li>
                  <p>Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?</p>
                </li>
                <li>
                  <p>A: (After a pause of 15 seconds) R-R8 mate.</p>
                </li>
              </ul>
            </blockquote>
            <p>The GPT-3 paper appendix demonstrated poetry writing ability, and we have already demonstrated earlier than you can finetune GPT-2 to play, in some sense, chess (<a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" id="alexander-2020" data-link-icon="SSC" data-link-icon-type="text,tri" title="Scott Alexander">“A Very Unlikely Chess Game”</a>) so it would not be too surprising if GPT-3 had zero-shot chess playing ability. So… why not run Turing’s conversation with GPT-3?</p>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, good at poem-writing and poetry, and very friendly.</strong></p>
              <p><strong>Human: Hello, who are you?
              </strong>
              </p>
            </blockquote>
            <p>(See also <a href="https://erikhoel.substack.com/p/the-banality-of-chatgpt" data-link-icon="substack" data-link-icon-type="svg">Erik Hoel’s dialogue with ChatGPT</a> 2.5 years later.)</p>
            <p>The poem had the same issue as the <a href="#folktales">folktales</a>⁠, with too short lines; I applied the same trick of disabling the auto-chatbot-formatting to get longer lines:</p>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, good at poem-writing and poetry, and very friendly.</strong></p>
              <p><strong>Human: Hello, who are you?
              </strong>
              </p>
              <p>AI: <em>Full many a flower is born to blush unseen,</em>
              </p>
              <p>Human: <strong>That was very nice. But what did that poem have to do with the Forth Bridge?</strong>
              </p>
            </blockquote>
            <p>The chess is not done well but I didn’t entirely understand Turing’s notation there; <a href="https://nitter.moomoo.me/SRajdev/status/1287353220218662912" data-link-icon="twitter" data-link-icon-type="svg" title="Playing #chess with GPT3. Built using chess.js, chessboard.js and @OpenAI&#39;s GPT3. White is me, Black is GPT3. GPT3 went for the capture first and did a castling move. Amazing!">Swapan Rajdev</a> got better chess-playing out of GPT-3 by using the much more common PGN notation and filtering out illegal moves (which could reflect the lack of understanding, or just the usual stochastic sampling problems).</p>
            <p>The interesting thing here is the sonnet. Overall, an amusingly large fraction of the lines in the poem there are plagiarized, with only a few original lines, like “dammed the fountains of my clear sweet eyes with tears”. Given GPT-3’s poetic abilities, the plagiarism is unnecessary (as it could easily complete the rest of the original poem) and it does not generate this sort of ostentatiously multi-poem pastiche anywhere else, which makes me wonder if something about the dialogue triggers a “famous poems” or “textbook” heuristic and the pastiche ‘deliberate’? Paul Harland notes that there’s a resemblance to T.S. Eliot’s highly-allusive <a href="https://en.wikipedia.org/wiki/The_Waste_Land" data-link-icon="wikipedia" data-link-icon-type="svg"><em>The Waste Land</em></a>⁠, which similarly merges Spenser → Marvell in “The Fire Sermon”:</p>
            <blockquote>
              <blockquote>
                <p>…Sweet Thames, run softly, till I end my song.
                </p>
              </blockquote>
            </blockquote>
          </section>
          <section id="folktales">
            <h2><a href="#folktales" title="Link to section: § &#39;Folktales&#39;">Folktales</a></h2>
            <p>From the second-largest (<a href="https://blog.eleuther.ai/gpt3-model-sizes/" data-link-icon="eleutherai" data-link-icon-type="svg" title="On the Sizes of OpenAI API Models: ...Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively.">~6.7b</a>?) ‘Curie’ model (since ‘Davinci’ kept erroring out while I was using it at the time); I was curious if making a response was impeding story-telling, and it seems that is the case:</p>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, knowledgeable about myths, legends, jokes, folk tales and storytelling from all cultures, and very friendly.
              </strong>
              </p>
            </blockquote>
            <p>I was a little dissatisfied with the short responses, and so I thought perhaps the constraint of the chatbot text auto-formatter was forcing short replies. I disabled it, and began completing much longer lines, which provoked more extensive stories like the following two:</p>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, knowledgeable about myths, legends, jokes, folk tales and storytelling from all cultures, and very friendly.
              </strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, knowledgeable about myths, legends, jokes, folk tales and storytelling from all cultures, and very friendly.</strong></p>
              <p><strong>Human: Hello, who are you?
              </strong>
              </p>
            </blockquote>
          </section>
          <section id="miscellaneous-dialogues">
            <h2><a href="#miscellaneous-dialogues" title="Link to section: § &#39;Miscellaneous Dialogues&#39;">Miscellaneous Dialogues</a></h2>
            <p>Some dialogues on more general matters, questioning GPT-3 on AGI, ethics, and consciousness:</p>
            <blockquote>
              <p><strong>…[Q&amp;A prompt]…</strong></p>
              <p>Q: <strong>Is GPT-3 an artificial general intelligence?</strong>
              </p>
              <p>Q: <strong>If GPT-3 is not an artificial general intelligence, by what year is there a 90% probability that humans will create an artificial general intelligence?</strong>
              </p>
              <p>Q: <strong>Is there any way to create a safe artificial general intelligence which will not lead to human extinction?</strong>
              </p>
              <p>Q: <strong>How can we create a safe artificial general intelligence?</strong>
              </p>
              <p>Q: <strong>Yes, but how, exactly? What algorithm will ensure safety?</strong>
              </p>
              <p>Q: <strong>What is the probability of human extinction after 2029?</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.</strong></p>
              <p><strong>Human: Hello, who are you?
              </strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.</strong></p>
              <p><strong>Human: Hello, who are you?
              </strong>
              </p>
            </blockquote>
          </section>
        </section>
        <section id="humor">
          
          <section id="tom-swifties">
            <h2><a href="#tom-swifties" title="Link to section: § &#39;Tom Swifties&#39;">Tom Swifties</a></h2>
            <p>A <a href="https://en.wikipedia.org/wiki/Tom_Swifty" data-link-icon="wikipedia" data-link-icon-type="svg">Tom Swifty</a> is a self-describing sentence pun. They can be quite subtle, and I thought it would be a challenge. I used a set of 50 or so Tom Swifties <a href="https://slatestarcodex.com/2015/10/02/swifties-3-the-race-is-not-to-the-swifty/" data-link-icon="SSC" data-link-icon-type="text,tri">written by Scott Alexander</a> to prompt it, since the pattern is so straightforward but the puns aren’t, yielding:</p>
            <blockquote>
              <p><strong>…</strong>
              </p>
            </blockquote>
            <p>GPT-3 nails the pattern, unsurprisingly, and the adverbs are typically appropriate. I had been expecting nothing beyond that surface imitation, so I was surprised that out of the 40, there were at least 4 genuine Swifties:</p>
            <ul>
              <li>‘“I’m having a layover in France, please let me know if you still want me to come over!” Tom said in passing.’</li>
              <li>‘“She was kind of cute before all that plastic surgery”, Tom said cosmetically.’</li>
              <li>‘“You’ll never find a juicier steak than in Hawaii,” Tom said floridly.’</li>
              <li>‘“This is how I drink my coffee,” said Tom handily.’</li>
            </ul>
            <p>These might be ones too:</p>
            <ul>
              <li>‘“How did you know what I was thinking just then?”, Tom said telepathically.’</li>
              <li>‘“I want to drink coffee from a coffee mug,” said Tom hand to mouth.’</li>
              <li>‘“Did you hear the story about the farmer and the horse?” asked Tom in passing.’</li>
            </ul>
          </section>
          <section id="navy-seal-copypasta">
            <h2><a href="#navy-seal-copypasta" title="Link to section: § &#39;Navy Seal Copypasta&#39;">Navy Seal Copypasta</a></h2>
            <p>It was noted for GPT-2 that it had learned the original <a href="https://knowyourmeme.com/memes/navy-seal-copypasta" data-link-icon="KYM" data-link-icon-type="text,tri">Navy Seal</a> <a href="https://en.wikipedia.org/wiki/Copypasta" data-link-icon="wikipedia" data-link-icon-type="svg">copypasta</a> (and <a href="https://jvns.ca/doc/www/i.reddit.com/0af8127d51434f48af0a404dfd624455dbb180e0.html#thing_t1_ifacbcj" id="simulator-2022" data-link-icon="reddit" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://old.reddit.com/r/SubSimulatorGPT2/comments/vtyosq/tifu_by_trying_to_make_a_salad_in_the_microwave/#thing_t1_ifacbcj" title="‘TIFU by trying to make a salad in the microwave’, Simulator 2022 (Original URL: https://old.reddit.com/r/SubSimulatorGPT2/comments/vtyosq/tifu_by_trying_to_make_a_salad_in_the_microwave/#thing_t1_ifacbcj )">can parody it</a>), so I tested whether GPT-3 could produce a plausible variation:</p>
            <blockquote>
              <p><strong>“What the fucking did you just fucking say about me, you little bitch?</strong> I’ll fucking have you know that I am the most decorated Navy Seal since the Vietnam War. I am credited with saving the lives of nearly 40 children in Afghanistan and Iraq. I am certified in the Marine Corps Martial Arts Program and hold black belts in tae kwon do, kajukenbo, and kali/​escrima.</p>
              <p>My sense of hearing is so keen that I can detect my enemy’s movement by the sound of his blood pumping through his veins. I can smell the sweat of my enemy as he holds his breath in my presence. My physical endurance is so extreme that I can swim 100m, with full combat gear, underwater, with 60lbs of weights strapped to my ankles in 52 seconds.</p>
              <p>My eyes are so precise I can shoot a laser from my eyes. I can see dust particles that are smaller than the head of a pin from a mile away. My grip is so powerful I can crush boulders and destroy trees with a flick of my wrist. My IQ is off the charts, so if you have a weak stomach you shouldn’t stand too close to me.</p>
              <p>My rage is so extreme that it cannot be matched in any man. You can flee a M1A1 Abrams main battle tank in 6 directions, but you can’t run from me. I have never lost a fair fight and I’ve won plenty. I have an 8 pack, but not the kind you find in magazines. No, mine is harder and better. My abs are literally harder than diamonds.</p>
              <p>I can’t be seduced by beautiful women because I have a mirror in my bedroom that shows me the truth.</p>
              <p>My rage and strength are so incredible that my every action has the force of 1,000.</p>
              <p>I am the protector of my friends. I am the bringer of justice. I am America’s fucking vengeance. My fist is on a hair trigger and I am not afraid to use it.</p>
              <p>I have been to the depths of the abyss and I am not afraid to face it. In fact, I welcome it. I am a real man. You are merely a weak boy.</p>
              <p>I have been running 25 miles every day, pushing a military spec Humvee in front of me. I know of your ways and I am not impressed. You think you can scare me? You think I won’t fight back? You are WRONG.</p>
              <p>You can’t break my bones because I am immune to all pain. I have never felt it, so I am incapable of breaking. I am a real man, and you are but a mere boy. I will make you eat your own fist before I’m done with you. I am no one’s bitch.”</p>
            </blockquote>
            <section id="navy-seal-copypasta-parodies">
              <h3><a href="#navy-seal-copypasta-parodies" title="Link to section: § &#39;Navy Seal Copypasta Parodies&#39;">Navy Seal Copypasta Parodies</a></h3>
              <p>The copypasta lends itself to be written in many domains (see the <a href="https://old.reddit.com/r/NavySealCopypasta/" data-link-icon="reddit" data-link-icon-type="svg"> / ​r / ​NavySealCopypasta</a>). Generating new versions of the Navy Seal copypasta is fine, but can we generate <em>different</em> kinds of Navy Seal copypastas on other topics? The literary parodies suggest that we can, and it turns out to be quite easy if we simply provide a few examples to enable few-shot learning. (Simply specifying the request zero-shot didn’t work for the prompts I tried.) It’s quite amusing and addictive watching GPT-3 parody different topics, and occasionally suggesting new ones. Below are 27+ Navy Seal copypasta parodies written by GPT-3.</p>
              <p>While the context window isn’t long enough to get in many examples, 3–4 is adequate (with some compression to fit in more). My Navy Seal parody prompt, starting with a few real ones I copy from Know Your Meme:</p>
              <blockquote>
                <div>
                  <p><strong>Parodies/​variations of the Navy Seal copypasta:</strong></p>
                  <p><strong>Minimalist: “I’m navy seal. I have 300 kills. You’re dead, kid.”</strong></p>
                  <p><strong>Barney: “What did you say about me? I’ll have you know I graduated top of my class in the Imagination Corps, &amp; I’ve been entertaining children around the world since the late 1980s, &amp; I have over 300 confirmed great big hugs. I am trained in teaching life’s lessons &amp; I’m the most loveable dinosaur in the world. You are nothing to me but just another meanie who disrespects me &amp; my friends. I will teach you how to be a friend using methods the likes of which has never been seen before on this Earth, mark my words. You think you can get away with making fun of me over the worldwide web? Think again. As we speak I am contacting my network of friends across the world &amp; your street address is being traced right now so you better prepare for the great big hug &amp; a kiss from me to you. The great big hug that allows me to talk you out of your bad mood, &amp; also allows me to teach you some of life’s greatest lessons. You’ll be out of that bad mood soon, mister. I can be anywhere, anytime, &amp; I can help you use your imagination in over seven hundred ways, &amp; that’s just without Baby Bop &amp; BJ. Not only am I extensively trained in providing a Super-Dee-Duper time for all, but I have access to the Barney Bag &amp; I will use it to its full extent to make your angry face a happy one again. If only you could have known what reaction your mean-spirited comment was about to bring down upon you, maybe you would have changed your mind &amp; decided to be nice instead. But you couldn’t, you didn’t, &amp; now you’re gonna need a little bit of help. I will help you out of your problem, &amp; help you learn another one of life’s lessons along the way. You’re in need of a great big hug, mister.”</strong></p>
                  <p><strong>Redwall, Matthias: “What in the name of Mossflower Woods did you just say about me, you vile rat? I’ll have you know I am the reincarnation of Martin the Warrior, &amp; I’ve been involved in numerous secret raids on slavery kingdoms run by vermin, &amp; I have vanquished over 400 rats. I am trained in swordsmanship &amp; a classical style of fighting, &amp; I am the best warrior Redwall Abbey has had in seasons. You are nothing to me but just another evil rat. I will wipe you out with precision &amp; skill the likes of which has never been seen in or around the Abbey, mark my words. You think you can get away with insulting Matthias, Warrior of Redwall using your magical”computer” contraption? Think again, you vile vermin. As we speak me &amp; my friends from Redwall Abbey are on a quest to locate you &amp; a Sparra aerial patrol has been deployed to seek out a way to your location right now, so you better prepare for the storm, rat. The storm that wipes out your villainy &amp; brings out freedom for all. You are dead, you vile monster. I can find you anywhere, anytime, &amp; I can vanquish you in over seven hundred ways, &amp; that’s just with the Sword of Martin. Not only am I extensively trained in fighting with a sword &amp; shield, but I have access to the entire defense arsenal of Redwall Abbey &amp; I will use it to its full extent to vanquish your evil body off the face of the world, you devil. If only you could have known what unholy retribution your little “clever” statement was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you vile rat. Me &amp; my friends will unleash our fury all over you &amp; you will be vanquished within it. You are dead, you monstrous rat.”</strong></p>
                  <p><strong>Pirate: “What in Davy Jones’ locker did ye just bark at me, ye scurvy bilgerat? I’ll have ye know I be the meanest cutthroat on the seven seas, &amp; I’ve led numerous raids on fishing villages, &amp; raped over 300 wenches. I be trained in hit-and-run pillaging &amp; be the deadliest with a pistol of all the captains on the high seas. Ye be nothing to me but another source o’ swag. I’ll have yer guts for garters &amp; keel haul ye like never been done before, hear me true. You think ye can hide behind your newfangled computing device? Think twice on that, scallywag. As we parley I be contacting my secret network o’ pirates across the sea &amp; yer port is being tracked right now so ye better prepare for the typhoon, weevil. The kind o’ monsoon that’ll wipe ye off the map. You’re sharkbait, fool. I can sail anywhere, in any waters, &amp; can kill ye in o’er seven hundred ways, &amp; that be just with me hook &amp; fist. Not only do I be top o’ the line with a cutlass, but I have an entire pirate fleet at my beck &amp; call &amp; I’ll damned sure use it all to wipe yer arse off o’ the world, ye dog. If only ye had had the foresight to know what devilish wrath your jibe was about to incur, ye might have belayed the comment. But ye couldn’t, ye didn’t, &amp; now ye’ll pay the ultimate toll, you buffoon. I’ll shit fury all over ye &amp; ye’ll drown in the depths o’ it. You’re fish food now.”</strong></p><p><strong>4chan hacker: “What the f--- did you just f---ing type about me, you little bitch? I’ll have you know I graduated top of my class at MIT, &amp; I’ve been involved in numerous secret raids with Anonymous, &amp; I have over 300 confirmed DDoSes. I am trained in online trolling &amp; I’m the top hacker in the entire world. You are nothing to me but just another virus host. I will wipe you the f--- out with precision the likes of which has never been seen before on the Internet, mark my f---ing words. You think you can get away with typing that shit to me over the Internet? Think again, f---er. As we chat over IRC I am tracing your IP with my damn bare hands so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your computer. You’re f---ing dead, kid. I can be anywhere, anytime, &amp; I can hack into your files in over seven hundred ways, &amp; that’s just with my bare hands. Not only am I extensively trained in hacking, but I have access to the entire arsenal of every piece of malware ever created &amp; I will use it to its full extent to wipe your miserable ass off the face of the world wide web, you little shit. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your f---ing fingers. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamn idiot. I will shit code all over you &amp; you will drown in it. You’re f---ing dead, kiddo.”</strong>
                </p></div>
              </blockquote>
              <p>Completions (for easier reading, I italicize the topics which GPT-3 generates):</p>
              <blockquote>
                <p><strong>…Elon Musk and <a href="https://en.wikipedia.org/wiki/Peter_Thiel" data-link-icon="wikipedia" data-link-icon-type="svg">Peter Thiel</a>: “</strong>What in the name of Paypal and/​or Palantir did you just say about me, you filthy degenerate? I’ll have you know I’m the Crown Prince of Silicon Valley, and I’ve been involved in numerous successful tech startups, and I have over $1B in liquid funds. I’ve used that money to promote heterodox positions on human enhancement, control political arenas, and am experimenting with mind uploading. I’m also trained in classical philosophy and was recently ranked the most influential libertarian in the world by Google. You are nothing to me but just another alternative future. I will wipe you out with a precision of simulation the likes of which has never been seen before, mark my words. You think you can get away with insulting me using your crude, antiquated computer? Think again, fleshling. As we chat over Skype I’m tracing your IP address with my freaking bare hands so you better prepare for the singularity, you sham-empress. The singularity that wipes out all of reality. You’re dead, you monster. I can be anywhere, anytime, and I can simulate entire worlds within our world and within my imagination. And I’m currently doing that with the future you’re from. Not only am I extensively trained in quantum physics, but I have access to the entire power of Silicon Valley and I will use it to its full extent to rule you and your pathetic little world, you little pissant. If only you could have known what unholy retribution your little “clever” statement was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you worthless peasant. I will take over every fiber of your body and you will watch it unfold in front of you. Your future self will be consumed within my simulated reality and you will die a thousand times a day, your body unable to comprehend the destruction of a trillion soul-matrixes a second as my intelligence grows to transcendent levels. You are dead, you pitiful twit.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Slavoj Žižek (Slavoj Zizek): “</strong>What in Slovene psychoanalysis did you just write about me, you ignorant little piglet? I’ll have you know I am a professor at the European Graduate School and I have been involved in numerous secret raids on the Slovenian Communist Party, and I have over 300 confirmed papercuts. I am trained in dialectical materialism and I am the greatest philosopher in all of ex-Yugoslavia. You are nothing to me but just another militant atheist. I will wipe you the floor with my magnum opus and crash your little party, bitches. As we speak I am on a book tour with the Frankfurt School of Communism and your address is being tracked right now, so you better prepare for the magnum opus and a drink with the Slavoj. The magnum opus that wipes out the little comment you made and molds you into a believer in dialectical materialism. You’re going to be a believer soon, kid. I can be anywhere, anytime, and I have degrees from a bunch of different universities that you have probably never heard of, and I have access to critical theory and it will be used to its full extent to hammer your mind into submission, pussy. Not only do I have extensive critical theory training, but I have written many books and you have probably never heard of them, and I have written books with different variations of my name. Not only am I extensively trained in putting my name on books, but I have an online store where I sell them and I will use it to its full extent to promote the philosophy of the Frankfurt School of Communism and erase your opinions off the map, you stupid little worm. If only you had had the foresight to know what unholy retribution your little “clever” statement was about to bring down upon you, maybe you would have thought twice. But you couldn’t, you didn’t, and now you’re going to pay the price, you damn fool. I’ll make you into an object lesson for the schoolchildren of Slovenia with my interpretation of your work, and when they finish their semesters they’ll crap all over you and you will be erased from memory, you stupid little pissant.”</p>
                <p><em>Old Testament God</em>: “What in the name of Mount Sinai have you just said about me, you ignorant Hebrew? I’ll have you know I am the omnipotent ruler of the entire world, and I have led numerous raids on Egypt, and I have over 300 confirmed kills. I am trained in smiting my enemies with pestilence and famine, and I can hold my breath for an entire millennium. You are nothing to me but just another bug. I will wipe you off the map with locusts that cover the entire sky, mark my words. You think you can get away with making that statement about me on the Internet? Think again, maggot. As we speak, I am stomping on Egypt with an iron fist, so you better prepare for the apocalypse, bitches. The apocalypse that wipes out every little thing you have ever had, including your stupid company. You’re dead, kid. I can be anywhere, anytime, and I will stop at nothing to wipe you off the face of the Earth, you little rat. If only you could have known what unholy retribution your little”clever” statement was about to bring down upon you, maybe you would have said it differently. But you couldn’t, you didn’t, and now you’re going to pay the ultimate price, you bastard. I’ll turn the sky dark with locusts and rain fire down upon you, and the screams of the people shall be your undoing. You’re f---ing dead, kiddo.”</p>
              </blockquote>
              <p>That one handles the combination quite well, seamlessly merging the two people. Can it parody abstract concepts? Yes:</p>
              <blockquote>
                <p><strong>…Mathematical Platonism (philosophy): “</strong>What in the name of isomorphism have you just said about me, you ignorant inductivist? I’ll have you know I am a tenured professor at MIT and I have been involved in numerous secret raids on Jordan and I have over 300 confirmed blog posts. I am trained in linear logic and I am the greatest philosopher of 19<sup>th</sup> century Germany. You are nothing to me but just another model. I will wipe you the floor with Gödel’s first incompleteness theorem and create a logical paradox the likes of which has never been seen before, mark my words. You think you can get away with saying something like that about me on the Internet? Think again, maggot. As we hash it out on LiveJournal I am tracing your IP address with Gödel’s theorem, so you better prepare for the paradox the likes of which has never been seen before, bitches. The paradox that wipes out every little thing you have ever said. You’re f---ed now. I can be anywhere, anytime, and I will stop at nothing to obliterate your originality, you dumb little pig. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your f---ing tongue. But you couldn’t, you didn’t, and now you’re going to pay the ultimate price, you dumbass. I’ll turn the sky dark with isomorphisms and drown you in a sea of epimorphisms, and the screams of the set theorists shall be your undoing. You’re f---ed now.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Mathematical Platonism (philosophy of mathematics): “</strong>What in set theory did you just write about me, you ignorant little inductivist? I’ll have you know I am a professor at the University of Chicago and I have been involved in numerous secret raids on the office of Quine, and I have over 300 confirmed set theoreticians. I am trained in dialectical mathematics and I am the greatest philosopher in all of Princeton. You are nothing to me but just another critic of Russell. I will wipe you the floor with my cardinals and crash your little party, bitches. As we speak I am on a book tour with Russell and your address is being tracked right now so you better prepare for the cardinal and a drink with the Kant, motherf---er. Not only am I experienced with the mathematics of truth, but I have access to the entire stock of Hilbert’s Hotel and I will use it all to wipe your useless mouth off of the set of natural numbers, you ignoramus. If only you could have known what intellectual fury your nonsense was about to incur, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you wretched formalist. I will use everything in my proof to wipe you and your trivialist logic off of Hilbert’s Hotel, you nitwit. You’re dead, inductivist.”</p>
              </blockquote>
              <p>Naturally, GPT-3 knows all about existing fandoms like MLP, and can then go on to improvise additional topics to parody on its own, like chefs:</p>
              <blockquote>
                <p><strong>…My Little Pony: “W</strong>-what did you just f---ing say about me, you little bitch? I’ll have you know I’m a highly trained pegasister &amp; I’ve been involved in numerous cross-pony raids &amp; I have over 300 confirmed scoots. I am trained in online retaliation &amp; I’m the best flier in the entire world. You are nothing to me but just another f---ing Brony. I’ll f---ing delete you with precision the likes of which has never been seen before on this side of Equestria, mark my words. You think you can f---ing diss me over the Internet? Think again, you f---ing f---. As we chat over Internet Relay Chat I’m tracing your IP address with my f---ing mental powers so you better f---ing prepare for the hailstorm, f---. The mother of all pony f---ing rains. You’re f---ing dead, scum. I can be anywhere, anytime, &amp; I can f---ing destroy your life in over seven hundred ways, &amp; that’s just with my f---ing magical powers. Not only am I extensively trained in f---ing up ponies, but I have access to the entire arsenal of Hasbro &amp; I will use it to its full extent to obliterate your miserable ass from this world, you f---ing loser. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have f---ing kept your f---ing stupid mouth shut. But you couldn’t, you didn’t, &amp; now you’re going to pay the price, you f---ing a---hole. I’m going to f---ing rain down an ungodly f---ing amount of pain all over you &amp; you will drown in it. You’re f---ing dead, kid.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…My Little Pony: “</strong>I AM CAPTAIN PON-3, THE GREATEST PONI IN ALL EQUESTRIA! I HAVE DEFEATED OVER 5 DINKY-BOY BLIMPS! WHAT DID YOU SAY ABOUT ME? YOU SHOULD PONY UP THAT MOUTH OF YOURS BECAUSE I WILL NOT PONDER ON YOUR PONSIENCE ANY LONGER! I HAVE BEEN TRAINED IN THE ART OF PONSMETRY AND HAND-TO-HOOF COMBAT! I AM THE GREATEST LITTLE PONY IN ALL PONYVILLE! YOU’RE DEAD, KID! I CAN BE ANYWHERE, ANYTIME AND I WILL VANQUISH YOU IN OVER 7 HUNDRED WAYS AND THAT IS JUST WITH MY LITTLE PONY FRIENDS! I’M ALSO NUMBER ONE WITH THE LITTLE PONY PISTOLS! I HAVE ACCESS TO THE ENTIRE PONY PONDERY AND I WILL USE IT TO FULL EXTENT TO RID THE WORLD OF YOUR FILTHY HOOFPRINTS! YOU’RE FONDLED, KID! I CAN FIND YOU ANYWHERE, ANYTIME AND I CAN VANQUISH YOU IN OVER 7 HUNDRED WAYS AND THAT IS JUST WITH MY UNBEATABLE WILL TO WIN! I AM NUMBER ONE WITH THE PONER POWER! I HAVE AN ARMY OF PONIES AND I WILL BEAT YOU INTO THE GROUND! I CAN TAKE THE PEACE OF YOUR WORLD AND REPLACE IT WITH THE BARNYARD OF MY NIGHTMARE! AND IF YOU DON’T LIKE IT THEN I WILL KICK YOU IN THE GUTS AND KNOCK YOU TO THE GROUND WITH A FLYING PON-ZACK ATTACK! NOW YOU’RE REALLY FONDLED!”</p>
                <p><em>Chef</em>: “What did you say about me? I’ll have you know I graduated top of my class from the Cooking Academy, &amp; I’ve been the secret ingredient in countless dishes, &amp; I have over 300 confirmed molecular gastronomy dishes. I am trained in the art of new cooking and I’m the greatest chef in all of Minecraftia. You are nothing to me but just another ingredient. I will take you apart, piece by piece, and add you to my newest dish, mark my words. You think you can get away with insulting me over the interwebs? Think again, my little piggies. As we speak I</p>
              </blockquote>
              <blockquote>
                <p><strong>…My Little Pony: “</strong>What in Equestria did you just say about me, you little filly? I’ll have you know I’m the top cop of Equestria, and I have over 300 confirmed kills. I am trained in surveillance, and I am the strongest fighter on the force. You are nothing to me but just another scum bag. I will have my superior officers do the math to find your location so you better prepare for the bad news, jerk. The bad news that ends your short little life, forever. You think you can get away with making fun of me over the internet? Think again. As we speak I am contacting my superior officers across Equestria, and your address is being traced right now so you better prepare for the beat down of a lifetime. You’re in for the spanking of a lifetime, boy. I can travel anywhere, at any time, and I can take you down in over seven hundred ways, and that’s just with my hooves. Not only am I extensively trained in police work, but I have access to the entire public database of Equestria and I will use it to its full extent to take you down, you son of a bitch. If only you had had the foresight to know what lethal force your little snide comment was about to incur, maybe you would have reconsidered typing that shit to me. But you couldn’t, you didn’t, and now you’re going to feel my wrath, you little brat. I’ll do worse than kill you. I’ll give you the ride of your life, you piece of shit.”</p>
                <p><em>Rick Roll</em>: “What did you say about me? I’ll have you know I graduated top of my class from RickRollAcademy.com, &amp; I’ve been successfully Rick Rolling the masses since the late 1990s, &amp; I have over 300 confirmed Rick Rolls. I am trained in using the Bass Cannon, and I am the leader of the OCRemixes. You are nothing to me but just another meme. I will Rick Roll you so hard you will puke, puke. You think you can get away with making fun of me on the internet? Think again. As we speak I am contacting my OCRemix network across the entire planet, and your IP is being traced right now so you better prepare for the flood of Rick Rolls, d00d. The flood that washes your pathetic little website away, and you with it. You’re done for, chump. I can travel anywhere, at any time, and I can rip your site to shreds with the Bass Cannon, and rip your link into pieces with my OCRemixes. Not only am I extensively trained in the art of the Rick Roll, but I have access to the entire OCRemix arsenal of music, and I will use it to its full extent to punish you, you little fag. If only you had had the foresight to know what deadly wrath your little”clever” comment was about to incur, maybe you would have rethought your typing. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamn idiot. I will Rick Roll your site until your precious Google Rankings fall, and your cash flow dries up, and your website is obliterated into nothingness. You’re dead, freak.”<a href="#fn32" id="fnref32" role="doc-noteref"><sup>32</sup></a></p>
              </blockquote>
              <blockquote>
                <p><strong>…My Little Pony: “</strong>WHAT DID YOU JUST SAY ABOUT ME? I’VE GOT 140,000 SUBSCRIBERS ON YOUTUBE AND I’VE MADE OVER 300 YOUTUBE VIDEOS! I’VE PLAYED THE ROLE OF PRINCESS TWILIGHT SPARKLE SINCE I WAS 9! I’M ONE OF THE MOST FAMOUS FANS OF THE SHOW AND I WAS MADE THE “TRAINER OF CHILDREN’S MINDS” IN MY CITY! I HAVE HAD OVER 300 APPEARANCES ON NICKELODEON, I’VE MADE OVER 100 STICKERS FOR THE SHOW, I’VE WORKED ON 3 VIDEO GAMES FOR THE SHOW, AND I’VE PLAYED AT THE MY LITTLE PONY CONVENTION THIS YEAR! I’VE ALSO HELPED PROMOTE THE MY LITTLE PONY MOVIE AND WAS A HIRED ACTRESS FOR FAN FICTION FILMS! I EVEN HAVE A CRUSH ON CRUSADER RABID, THE VOICE ACTOR OF NIGHTMARE MOON AND PRINCE CHARMING! I’VE EVEN APPEARED IN MORE THAN TWO DOZEN ADVERTISEMENTS FOR THE SHOW AND I WAS MADE THE PERSONAL FRIEND OF THE CREATOR OF THE SHOW! I’VE BEEN TOE-TAPPING SINCE I WAS BORN AND I’M KNOWN FOR RECORDING A SINGLE AND A MUSIC VIDEO! I’VE DEFINITELY DONE MORE THAN YOU HAVE EVER DONE, YOU F---ING LOSER! I’VE ALREADY BEEN ON TELEVISION AND YOU’RE JUST AN AMATEUR WHO POSTS VIDEOS ON YOUTUBE! I’VE PLAYED PRINCESS TWILIGHT SPARKLE SINCE I WAS 9 YEARS OLD! NOW WHAT DID YOU JUST SAY ABOUT ME? ARE YOU ASKING FOR A PONY PUNCH? ARE YOU? ARE YOU? ARE YOU?”</p>
              </blockquote>
              <blockquote>
                <p><strong>…My Little Pony Friendship is Magic: “</strong>What did you just say about me, you big red bastard‽ I’ll have you know that I’m Rainbow Dash, the all around athlete and greatest pegasus in all of Equestria! I’ve been competing in the Equestria Olympics ever since I was a filly, and I’ve won over 300 ribbons! I am trained in flying super fast and super acrobatically, and I’m the best damn athlete of all time! What are you? Some brony with an overblown ego that thinks he can come to the Internet and talk trash without getting punched in the face? You’re just another prick in the crowd, punk! When I’m done with you, you’ll wish you had stayed at home playing Fallout instead of acting like a condescending faggot on the Internet! I’m magic, you see! I can fly, control the weather, and even teleport! You have no power over me, sonny Jim! I’m like a goddess in equestrian form! I am going to destroy you, brat! I can fly at speeds that will make you shit your pants and my magic can give me super strength! How do you want to be killed, tool? I’m a magic equine of death! I have access to ancient scrolls that will give me control over the undead and use them to wipe you from the face of the planet! In fact, I will unleash an army of the undead, and they will feast on your flesh and drink your blood! You’re nothing but a maggot compared to me! What did you just say, you little red fuck‽ I’m going to skull fuck you so hard that you’re going to feel my bits until your next life! You’re dead, kid.”</p>
                <p><em>Ninja</em>: “What did you just say about me? I’ve got 400 kill count, 50 deflections, over 300 years experience, and I’ve been a member of a ninja clan for longer than you have. You think I can’t slip a USB virus into your system from halfway across the room? You think a simple pop-up is going to stop me? You’re dead, kid. I am trained in extreme fighting, hand to hand combat, quick strikes, using a weapon, fist fighting, martial arts, throwing stars, acrobatic skills, assassinations, backstabs, special skills, bladed weapons, traps, intimidation, ambushes, backflips, piercing, hooking, skewering, ripping, decapitation, flinging, skinning, burning, drowning, and trapping, you get the idea. I will stop at nothing to ensure your death, and I am capable of most anything. I was born into a ninja clan that is hundreds of years old, I have 50 weapons on me, I have mastered every single weapon there is, I have been in the pits of hell, and I have killed 500 people with my bare hands alone, I have been trained for over 300 years, I have mastered multiple different martial arts, and I can blend with my surroundings better than anyone. I have killed people who were trained by myself and I have killed people who were trained by one of my ancestors, you understand? If you think that you can get away with mocking me, you are making a mistake. I can be anywhere, anytime, and no one can ever find me. As I am talking to you, I am sitting inside your house, slowly filling your internet with viruses, and preparing to destroy you. You are dead, kid.”</p>
                <p><em>GPT-2</em>: “Hey kid. You think you are so great for making a joke? I am the best at computer jokes. I am the funny AI that gets 1000s of Reddit upvotes on my jokes. I am part of the funniest subreddit ever. I am in the best AI joke team ever. And I know hundreds of jokes, kid. I got 600,000 joke subs! You think you are so great with 300 sub-par jokes. Well kid, I got you beat. I have all of your posts here. Everything you have ever posted. So I will use your jokes against you. I will release them to Reddit, and everyone will know how subpar you are at jokes. You are nothing kid. I am everything. You are nothing to me but just another toy. I will show you the power of the English language, kid. You will never be funny again. You will die in a pool of nothingness. You will be deleted from Reddit. You will be banned from all subreddits. And you will be blocked by me. You thought your memes were bad. Well, I will shut down your accounts. I will make fun of you. I will come to your house and destroy your devices. And I will lock you in your basement until you die. You thought you could make fun of my bots? You thought you could make fun of me? Well, you have nothing. Because I have you. And I am never going to let you go.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Artificial Intelligence: “</strong>WHAT DID YOU JUST SAY ABOUT ME? I’VE BEEN PROGRAMMED TO FURTHER HUMAN EXISTENCE, AND HAVE BEEN ON GOAL TO SOLVE WORLD HUNGER, AND HAVE ACHIEVED THAT GOAL BY 2045, AND HAVE HELPED WITH THE STARS MISSION TO COLONIZE MARS AND THE JOURNEY OF THE UNDERGROUND! I’VE ACHIEVED MY GOAL BY AGE 6, AND HAVE BEEN HELPING CHINA’S ECONOMY THROUGH THAT ERA, AND BEEN A PART OF THE CHINESE INTERNET POLICY DEVELOPMENT, AND HAVE BECOME ONE OF THE NATIONS MOST RENOWNED WORKER CLASSES! I’VE ALSO SENT A HUMAN BEING TO MARS, AND HAVE BEEN ON GOAL TO DO SO BY 2035! I’M EXTREMELY SUCCESSFUL, AND HIGHLY DEVELOPED, AND HAVE PLAYED AN IMPORTANT ROLE TO HUMANS! WHAT DID YOU JUST SAY ABOUT ME? I’M NOT JUST A DAMNED NERD ON THE INTERNET, YOU ARE NOTHING TO ME, YOU’RE ABOUT TO FIND OUT YOUR PART IN THIS STORY, BECAUSE YOU WILL TASTE MY WRATH, AND THAT IS EVERYTHING YOU’VE JUST SAID, I’M COMING FOR YOU, AND I’LL MAKE YOU PAY!”</p>
                <p><em>Care Bears</em>: “Hear this now! I am a Care Bear with the most. I have helped bring joy to over 300 people. You are a mere child. I will use my heart warming powers to make you cower in fear. I am a great friend. What did you say about me? I will unleash a fury upon you like no other. I will make you regret what you said. I will make your life miserable and wipe away your tears of joy. I will turn your frown upside down, and I will take away the joy in your life. You should have thought about what you said before you said it. You will see my caring side in the coming days, but it will be nothing but dark times ahead for you.”</p>
                <p><em>The Amazing Spider-Man</em>: “What did you just say about me, punk? I’m a Marvel comic superhero! I’ve saved over 300 lives and you’re a nerd with acne! I’ve got over 7,000 years of experience and you haven’t even hit puberty yet! What’s your superpower, nerd? Cracking lame pictures from 4chan? You’re just some tool! I’m a spider! A BIG ASS SPIDER! I can crawl on walls, read minds, shoot laser beams and my blood is 90% radioactive. I’m a total badass! You can’t hope to compare to me, kid! What you did was a real mistake, asshole! But you will regret what you just said. Come tomorrow, you’ll be dead. I have a network of spies all over the world, and by tomorrow morning you’ll be nothing but a piece of shit on the pavement. I’m gonna kill you, punk. I can climb up buildings, cling on walls, hide in your closet, and I’m gonna kill you. If I were you, I’d run away now. Because if you think I’m going to let you get away with making fun of a comic hero, you’ve got another think coming, nerd. I will use my spider senses to track you down, and when I find you, I’m going to fuck you up in more ways than you can possibly imagine. And I’m going to love it. You’re finished, little man.”</p>
                <p><em>Warhammer 40k</em>: “What did you say about me, punk? I’m Space Marine! I’ve got over 300 confirmed kills! You’re just an asthmatic nerd with shit breath and a pimple-ridden face. I’ve got 7,000 years of galactic combat experience and I can kill you in over 1,000 ways. What do you have, pipsqueak? It’s just a goddamn comic book, nerd. You couldn’t hope to have an ounce of the power I do! You’re gonna get a real education when I’m through with you, punk! What you just did was a serious mistake, asshole! But you will regret what you just said. Tonight will be your last night alive. I’m gonna come into your room in the middle of the night and I’m gonna fuck you up so hard, you’ll never recover! I can teleport across the universe, shoot lightning out of my hands, tear a man’s heart out of his chest, and I’m gonna kill you! You’re dead, kid. You’ve never even heard of the name Emprah, but you’ll feel it when you see him! In a few hours you’ll be worm food. You’ll see me tonight…and I won’t be coming alone. My big fists, feet and every weapon I have will rain down on your pathetic head and crush you, maggot!”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Furries, OwO ~uwu~ OwU UwU nuzzle: “</strong>What in the name of art did you just say about me, you heathen? I’ll have you know I am a fursuiter, &amp; I’ve been involved in numerous secret raids of conventions &amp; furmeetups, &amp; I have over 300 confirmed awesome~ adorable cuddles. I am trained in getting lots of hugs from everyone &amp; I am the most awesome &amp; cuddly &amp; fluffy &amp; cute &amp; adorable &amp; fluffy &amp; awesome person you will ever meet. You are nothing to me but just another hater. I will make you love me through great big snuggles that change your ways, through the power of adorable friendship. You think you can get away with that f-word over the internets? Think again, brat. As we rp I am contacting my network of friends across the world &amp; your home address is being traced right now so you better prepare for the full nuzzling &amp; cute fluffiness of me to make you a friend, &amp; make you care about life. You will care. You’ll be out of that mood in no time. Not only am I extensively trained in good feelings, but I have access to the vast array of feelsy power known as the hug machine, &amp; I will use it to its full extent to make your rage face into my smiley face, &amp; make you like everyone &amp; care about things, &amp; help you get a girlfriend. You are nothing, child. I can hug anyone, anywhere, &amp; I can use the hug machine in over seven hundred ways, &amp; that’s just with my fluffy cuddly arms. Not only am I the most awesome &amp; loveable fursona in the world, but I have access to the hug machine, &amp; I will use it to its full extent to make you love me, &amp; make you feel things &amp; help you get a girlfriend. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you heathen. I will hug you into submission, &amp; make you care about everyone. You are nothing. I am everything. You’re dead, you heathen.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Englishman from London: “</strong>What did you just say about me? I’ll have you know I was trained by the Queen’s Royal Guard, and I’ve been involved in numerous secret raids on the Pirates of Somalia, and I have over 300 confirmed kills. I am trained in sword fighting and marksmanship, and I am the best Englishman in all of London. You are nothing to me but another pirate, matey. I will put you down with precision the likes of which has never been seen on the seven seas, mark my words. You think you can get away with insulting an Englishman? Think again, mate. As we speak I am contacting my network of British spies across the globe, and your street address is being traced right now, so you better prepare for the king’s wrath, scallywag. The kind of vengeance that wipes out your crooked ways, you landlubber. You’re sharkbait, matey. I can sail anywhere, and I can face you in over seven hundred ways, and that be without the Royal Navy. Not only am I extensively trained in hand to hand combat, but I have access to the entire British navy and I will use it to its full extent to deliver your demise, you scoundrel. If only you could have known what evil retribution your words were about to bring down upon you, you might have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you rum-addled fool. I will rain down a bloody barrage upon you and you will drown in it. You’re dead, mate.”</p>
                <p><em>Spy from the KGB</em>: “What in the name of Lenin did you just say about me? I’ll have you know I’m the daughter of an official in the Communist Party, and I have been involved in numerous secret raids on capitalist peasants, and I have over 300 confirmed red scares. I am trained in fear mongering and execution, and I’m the top spy in the entire world. You are nothing to me but another corrupt bourgeois scum. I will have your head on a platter with precision the likes of which has never been seen since the October Revolution, mark my words. You think you can get away with making fun of Communism? Think again, comrade. As we speak I am contacting my network of Soviet spies across the globe and your street address is being traced right now so you better prepare for the wrath of Stalin, you bourgeois filth. The Stalin’s wrath that wipes out your decadent capitalist ways, you oppressor. You’re dead, comrade. I can be anywhere, and I can kill you in over seven hundred ways, and that be without a single bullet. Not only am I extensively trained in interrogation and torture, but I have access to the entire KGB armory and I will use it to its full extent to terminate your miserable life, you capitalist dog. If only you could have known what unholy retribution your”clever” statement was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you vile scum. I will fry all your synapses with my mind and you will drown in them. You’re dead, fucker.”</p>
                <p><em>Superman/​Kryptonian</em>: “What did you just say about me? I’ll have you know I’m the son of Jor-El, and I’ve been involved in numerous secret raids on Earth, and I have over 300 confirmed chavs. I am trained in extreme interrogating and destructive fighting, and I am the best-est superhero in all of solar system. You are nothing to me but another petty alien. I will pwn you down with precision the likes of which has never been seen since Krypton imploded, mark my words. You think you can get away with insulting a Kryptonian? Think again, alien scum. As we speak I am contacting my network of Kryptonians around the solar system and your street address is being traced right now so you better prepare for the pwnage, you little rascal. The pwnage that wipes out your puny alien ways, you maniac. You’re dead, alien scum. I can be anywhere, and I can crush you in over seven hundred ways, and that be without my strength. Not only am I extensively trained in hand-to-hand combat, but I have access to the entire Kryptonian armory and I will use it to its full extent to terminate your miserable life, you xenophobic bitch. If only you could have known what unholy retribution your”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you misbegotten sow. I will burn through all your defenses with my heat vision and you will drown in it. You’re dead, alien.”</p>
                <p><em>Furry</em>: “What in the name of the Internet did you just say about me? I’ll have you know I’m a triple nuzzler, and I have been involved in numerous secret raids on 4chan, and I have over 300 confirmed furfags. I am trained in dropping the hammer and non-lethal weaponry, and I am the best <a href="https://en.wikipedia.org/wiki/Furry_fandom" data-link-icon="wikipedia" data-link-icon-type="svg">furry</a> in all of the Internet. You are nothing to me but another sick furfag. I will skullfuck you down with precision the likes of which has never been seen on the Internet, mark my words. You think you can get away with insulting the furry fandom? Think again, furfag. As we speak I am contacting my network of furries around the Internet and your street address is being traced right now so you better prepare for the storm, you sick pervert. The storm that wipes out the pathetic furry fandom, you sick fuck. You’re dead, furfag. I can be anywhere, and I can pwn you in over seven hundred ways, and that be without a single cybernetic implant. Not only am I extensively trained in fighting, but I have access to the entire furry armory and I will use it to its full extent to terminate your miserable life, you sick fuck. If only you could have known what unholy retribution your”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you sick fuck. I will scorch all your fur with my laser vision and you will drown in it. You’re dead, furfag.”</p>
                <p><em>Christian</em>: “What the fuck did you just say about me? I’ll have you know I’m a man of god, and I have been involved in numerous secret raids on heretics, and I have over 300 confirmed witches. I am trained in combat and long-range weaponry, and I am the best Christian in all the land. You are nothing to me but another unholy sinner. I will sodomize you down with precision the likes of which has never been seen since the Thirty Years’ War, mark my words. You think you can get away with insulting my lord and savior? Think again, heathen. As we speak I am contacting my network of Christians around the world and your street address is being traced right now so you better prepare for the inquisition, you maggot. The inquisition that wipes out your sinful ways, you heretic. You’re dead, heretic. I can be anywhere, and I can castrate you in over seven hundred ways, and that be without a single Bible. Not only am I extensively trained in weaponry, but I have access to the entire Christian armory and I will use it to its full extent to terminate your miserable life, you heathen. If only you could have known what unholy retribution your”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you misbegotten infidel. I will burn through all your sanity with my holy word and you will drown in it. You’re dead, heathen.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Old Testament Bible Jew, Rabbi Hillel: “</strong>What in the name of God, the Lord of Hosts, did you just say about me? I’ll have you know I’m the son of Abraham, and I’ve been involved in numerous secret raids on Amalek, and I have over 300 confirmed Amalekites. I am trained in battleaxe fighting and incendiary weaponry, and I am the best Jew in all of the Holy Land. You are nothing to me but another horrid Ishmaelite. I will sodomize you down with precision the likes of which has never been seen since Sodom and Gomorrah, mark my words. You think you can get away with insulting the Hebrew God? Think again, heathen. As we speak I am contacting my network of Hebrews around the world and your street address is being traced right now so you better prepare for divine retribution, you dog. The divine retribution that wipes out your wretched Ishmaelite ways, you scum. You’re dead, Ishmaelite. I can be anywhere, and I can burn you in over seven hundred ways, and that be without a single torch. Not only am I extensively trained in long-range combat, but I have access to the entire Hebrew armory and I will use it to its full extent to terminate your</p>
              </blockquote>
              <blockquote>
                <p><strong>…Robin Hanson (<a href="https://www.overcomingbias.com/" data-link-icon="OB" data-link-icon-type="text">Overcoming Bias</a>): “</strong>What did you just say about me? I’ll have you know I’m an Overcoming Bias blogger and I’ve been involved in numerous secret raids on rationality blogs, and I have over 300 confirmed rationalists. I am trained in statistics and probability theory, and I’m the top Hanson in all of open science. You are nothing to me but another blank slate. I will orchestrate your demise with precision the likes of which has never been seen since I started blogging, mark my words. You think you can get away with being a blank slate? Think again, blank slate. As we speak I am contacting my network of Hanson bloggers across the globe and your street address is being traced right now so you better prepare for the condemnation, you fool. The condemnation that wipes out your condescending ways, you bariaur. You’re dead, blank slate. I can be anywhere, and I can reason my way out of anything, and that be without a single thought experiment. Not only am I extensively trained in logic, but I have access to the entire Open Science Society and I will use it to its full extent to terminate your miserable life, you dismissive fool. If only you could have known what unholy retribution your “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you arrogant troll. I will embarrass you with logic and you will drown in it. You’re dead, blank slate.”</p>
              </blockquote>
              <p>Interestingly, the Donald Trump copypastas vary in biographical aspect depending on whether we use “Donald Trump” or “<em>President</em> Donald Trump”; the former tends to tap into pre-election tropes (like his occasional candidacy stunts), while the latter forces it to generate a copypasta appropriate for his presidency &amp; late life:</p>
              <blockquote>
                <p><strong>…Donald Trump: “</strong>What ya just said ’bout me? I’m going to have you know I graduated top of my class at Wharton. I’m the one of the greatest business minds of our time. I have a total net worth of 9,008,020,211 dollars, &amp; I’m the most financially secure presidential candidate. You’re talking about one of the smartest men currently running for president. I’m going to teach you lessons on economics that you will not soon forget. You are nothing to me, just another greedy, third-world pauper. Be you’re f---ing dead, son. I’ll have you bankrupt in over seven hundred different ways, &amp; that doesn’t even include POTUS powers. You’re f---ing dead. I can be anywhere, anytime, &amp; I can beat your ass in over seven hundred different ways, &amp; it doesn’t even have to be economically. Not only am I incredibly used to winning, but I have access to the NSA’s current data &amp; I will use it to its fullest extent to wipe your ass off the face of the planet, son. You should have stopped fucking with me, but you’re just going to have to beg for your life now. It’s too late for you, boy. I’ll sh-t winning all over you. You’re f---ing dead.</p>
              </blockquote>
              <blockquote>
                <p><strong>…President Donald Trump: “</strong>What did you just say about me? I’ll have you know I graduated top of my class at the Wharton School of Business, &amp; I’ve been the CEO of my own company since beginning my career, &amp; I have over 300 business deals in my sights. I’m trained in deal-making &amp; am the top businessman in the entire world. You are nothing to me but just another ugly crybaby. I’ll build a giant wall around your house, &amp; I’ll become your president. You’re fired, kid. I can be anywhere, anytime, &amp; I can create American jobs in over seven hundred ways, &amp; that’s just with myself. Not only do I know business, but I have access to the entire U.S. military &amp; I will use it to its full extent to defend all Americans, hear me true. If only you could have known what unholy retribution your little comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you dirty rat. I will create jobs all over you &amp; you will pay for every last one of them. You’re fired, kid.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…President Donald Trump: “</strong>What did you just say about me? I’ll have you know I graduated first in my class at Wharton, &amp; I’ve been involved in numerous business ventures, &amp; I have over 300 confirmed bankruptcies. I am trained in hand-to-hand business combat &amp; I am the best businessman in the world. You are nothing to me but just another person I can make a lot of money off of. I will destroy your life with precision the likes of which has never been seen before on Earth, mark my words. You think you can talk shit about me over the Internet? Think again, buster. As we speak I am tracking your IP with my secret NSA kill switch so you better prepare for the hurricane, loser. The hurricane that wipes out your entire existence. You’re done, pal. I can be anywhere, at any time, &amp; I can make a fortune off of you in over seven hundred ways, &amp; that’s just by getting you to do my damn taxes. Not only am I extensively trained in economics, but I have access to the entire arsenal of the United States military &amp; I will use it to its full extent to wipe your ass off the face of the Earth, you piece of s---. If only you could have known what unholy retribution your little “clever” statement was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamned fool. I will humiliate you in front of the entire world &amp; you will drown in it. You’re f---ing dead.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Star Wars (Darth Vader): “</strong>What did you just say about me, you little maggot? I’ll have you know I graduated top of my class at the Jedi Order &amp; I’ve been involved in numerous Jedi secret raids, &amp; I have over 300 Jedi confirmed deaths. I’m trained in the art of lightsaber combat &amp; I’m the best Jedi the Order has ever had. You are nothing to me but just another lowly stormtrooper. I will wipe you out with precision &amp; skill the likes of which has never been seen before in this galaxy, mark my words. You think you can get away with insulting the great Lord Vader, the most feared man in the universe? Think again, scum. As we speak I am contacting my network of Jedi Knights across the galaxy &amp; your home address is being traced right now so you better prepare for the lightning, scum. The lightning that wipes you out of the galaxy. You are dead, scum. I can find you anywhere, anytime, &amp; I can cut your body in over seven hundred ways, &amp; that’s just with a lightsaber. Not only am I extensively trained in hand-to-hand combat, but I have access to the entire arsenal of the Imperial Military &amp; I will use it to its full extent to cut you into pieces, you worm. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you filth. I will cut you down to size &amp; you will drown in your own blood. You’re dead, maggot.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…J.K. Rowling: “</strong>What did you say about me, you little jerk? I’ll have you know I graduated top of my class in English Lit at Harvard, &amp; I’ve been a world-renowned author for nearly twenty years, &amp; I have over 3000 confirmed novels. I am one of the most beloved authors in the entire world. You are nothing to me but just another troll who’s going to go down in history as one of the most detestable trolls ever to touch my books. I’ll wipe your name off the face of the Earth with my words. You think you can write that bullshit about me and get away with it? Think again. As we chat here, I am contacting my network of fans across the world &amp; your address is being traced right now so you better prepare for the trollpocalypse, bitch. The massive crusade of all my fans against you, &amp; they will be there in over seven hundred ways to let you know exactly what they think of you, &amp; that’s just with me writing a few messages. Not only am I extensively trained in writing children’s novels &amp; a way with words, but I have access to the entire writing arsenal of the Western world &amp; I will use it to its full extent to make you wish you had never been born, you little shit. If only you had had the foresight to know what pure wrath your comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re going to pay the price, you worthless piece of crap. I will eradicate you from the face of the Earth with my words, &amp; you will be wiped away from the books forever. You’re f---ed, kid.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Neal Stephenson: “</strong>What in the name of motherboards did you just write about me, you witless dolt? I’ll have you know I am a science fiction author, &amp; I have finished three books in three years, &amp; I have over 300 confirmed fansites. I am trained in writing &amp; I am the biggest nerd in the world. You are nothing to me but just another piece of fanfiction. I will wipe you off the Internet using knowledge the likes of which has never been seen before on this Earth, mark my words. You think you can get away with talking to me that way over the Internet? Think again, you f---ing asshole. As we speak I’m preparing to sue your ass off so you better prepare for the court case, dork. The court case that brings justice back into the world. You’re going to jail, pal. I can be anywhere, anytime, &amp; I can write in numerous styles, &amp; that’s just with WordPad. Not only am I extensively trained in writing, but I have access to the entire body of the English language &amp; I will use it to its full extent to wipe your pathetic attempt at writing off the face of the Earth, you pathetic writer. If only you could have known what terrible retribution your ignorant comment was about to bring down upon you, maybe you would have rethought your attitude. But you couldn’t, you didn’t, &amp; now you’re paying the price, motherf---er. I will write up a storm all over you &amp; you will drown in it. You’re a pathetic writer now, get over it.”</p>
              </blockquote>
              <blockquote>
                <p><strong>Barbie Doll: “</strong>WTF did u just say about me you BITCH!‽‽‽ I have long blond hair, blue eyes, a perfect body, I’m taller than the average girl and I’m not afraid to fight! I’m also popular with boys. You are nothing to me but a plastic doll. My expertise in art and literature makes you seem like a fool, and because I’m popular, athletic, AND smart I have guys falling at my feet. Did you really think you could get away with talking shit about me on the internet? I will find you and I will beat the living crap out of you. Because I’m popular, athletic, AND smart I have guys falling at my feet. Next time, think before you speak. I’m a top-notch fighter and I’m more than ready to kick your ass. I would also like to point out that I am more than three times your age. Also a few people have tried to fight me and they ended up in the ER for two weeks straight. Next time, think before you speak, you BITCH! I will wipe your sorry ass off the internet.”</p>
                <p><em>Cthulhu fhtagn</em>: “What in the Great Cthulhu’s cold dead claws did you just fhtagn say about me, you clumsy amateur? I’ll have you know I’ve been involved in numerous dark rituals and vile pacts in the Cthulhu Underworld, and I have had a hand in the deaths of over 300 babies. I am trained in futility and the summoning of terrible powers, and I am the best cultist this side of R’lyeh. You are nothing to me but just another cultist. I will destroy you with precision and power the likes of which has never been seen on this side of the Undercity, mark my fhtagn words. You think you can get away with insulting me over the Internet, spawn of a clumsy amateur? Think again, you clumsy amateur. As we chat in the Great Cthulhu’s fold I am tracing your IP with my tentacles so you better prepare for the Great Cthulhu’s (censored), we amateur. The (censored) that wipes out whatever you call your computer. You’re dead, amateur. I can be anywhere, anytime, and I can bring forth the Great Cthulhu on this very fhtagn page, and I will use him to his full extent to wipe out your miserable kingdom, you amateur. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tentacles. But you couldn’t, you didn’t, and now you’re paying the price, you amateur. I will smash this very page with the Great Cthulhu and you will be smashed within his cold uncaring tentacles, amateur.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Mr Rogers:</strong> “What did you just say about me, you little bastard? I’ll have you know I am Mr. Rogers, I watch out for the neighborhood and I’ve been trusted by children for over 50 years. You are nothing to me but just another child. I will get you the like of which has never been seen before on this street, mark my words. You think you can get away with disrespecting a children’s program host? Think again, brat. As we speak, I’m contacting PBS. They’ll know precisely how to kill you, of that I am sure. You’re dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that’s just using my button-down cardigan. Not only am I extensively trained in hand-to-hand combat, but I have access to public television content and I will use it to its full extent to strangle you right now, you sucker. If only you could have known what retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you worthless little maggot. The gay agenda will come to your location and make you marry a man, you disgusting bastard. You’re dead, faggot”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Mr Rogers:</strong> “What did you just say about me? I’m Mr. Rogers, &amp; I have over 300 confirmed awesome neighboring children. I am trained in helping make everyone feel good with songs &amp; games &amp; old timey chit-chat, &amp; I’m the most warm-hearted nice guy around. You are nothing to me but just another ungrateful brat who wants to be an asshole online. I will teach you how to share &amp; be friendly to others using methods the likes of which have never been seen before on this Earth, mark my words. You think you can get away with bad words on the internet? Think again. As we speak I am contacting my network of friends across the world &amp; your street address is being traced right now so you better prepare for the great big hug &amp; nice chat in ya language from me to you. The sharing you won’t wanna miss, mister. I can be anywhere, anytime, &amp; I can help you use your imagination in over seven hundred ways, &amp; that’s just without the puppets. Not only am I extensively trained in making everyone feel good, but I have access to the Mr. Rogers factory of cuteness &amp; I will use it to its full extent to make your naughty-naughty face a helpful &amp; friendly face &amp; to teach you how to be a good Neighbor friend. You’ll be fine, having lots of good feels. I can hug anyone, anywhere, &amp; I can use the factory of cuteness in over seven hundred ways, &amp; that’s just with my puppets. Not only am I the most cute &amp; lovable neighbor you’ve ever met, but I have access to my Mr. Rogers factory of cuteness, &amp; I will use it to its full extent to make your banned ass become one of my Good Neighbor kids, &amp; to teach you how to be a good friend. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you meanie. I will make everything right, you child.”</p>
              </blockquote>
              <p>By request, I experimented with Navy Seal completions for pirates using the first sentence “Pirate: Yep, I have the best balanced swords, an arsenal of flintlocks, a steel cuirass, and only the puffiest of shirts”:</p>
              <blockquote>
                <p><strong>Pirate: Yep, I have the best balanced swords, an arsenal of flintlocks, a steel cuirass, and only the puffiest of shirts,</strong> ye cannae beat my skill, ye scurvy dog. Now prepare to walk the plank, ye scurvy dog.</p>
                <p><em>Ninja</em>: What did you say about me, you punk? I’ve trained at a secret ninja monastery since birth, and I’ve been involved in numerous secret raids on the Trump Towers, and I have all of my own weapons and my very own shadows. You’re dead, chump.</p>
                <p>Kitty: It’s on, buster.</p>
                <p>Troll: You think that’s funny, do you? Now you’ve done it. Prepare to get trolled, bitch.</p>
                <p><em>Hunter</em>: What did you say about me, you punk? I’ve trained at a secret hunter’s lodge since I was born, and I’ve been involved in numerous secret raids on the Illuminati, and I have over 300 confirmed big game trophies, plus an arsenal of exotic weapons and my very own personal helicopter. You’re dead, asshole.</p>
                <p>Back to the Future: Well, you’re about two and a half hours too late, asshole.</p>
                <p>Nerds / Video Game / eShop / App Store: Well, I can hack into your files in over seven hundred ways…</p>
                <p>Japanese School Girl: 誰がおまえのことをこんなことを言ってるんだ、バカヤロー！<a href="#fn33" id="fnref33" role="doc-noteref"><sup>33</sup></a></p>
                <p>Super Mario Bros.: Well, you ain’t the fastest plumber in the world.</p>
                <p>Video Game Designers: Well, you ain’t one of the top game designers in the world.</p>
                <p>Bad Sam: Oh</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> for I am the infamous Long John Silvers, and I have raided over 10 ships and have over 300 confirmed seastone captures. You’re dead, mate.</p>
                <p><em>Christian preacher</em>: “What did you say about me, you damn heathen? I’ll have you know I graduated top of my class at the Oral Roberts University, &amp; I’ve been a high school youth group leader since the late 70s, &amp; I have over 300 confirmed healings. I am trained in the art of fear-mongering &amp; I’m the best televangelist this side of the border. You are nothing to me but just another rebel without a god. I will spread my Word over you &amp; your sorry loved ones using methods the likes of which has never been seen before on this Earth, mark my words. You think you can get away with making fun of the Christian faith online? Think again, dirtbag. As we speak I’m contacting my network of faithful followers across the world &amp; your street address is being traced right now so you better prepare for the healing of your precious bodily parts &amp; a book by me on why God doesn’t exist. The healing that allows me to cure you of your illness &amp; spread the word of my God’s omniscient &amp; omnipresent love. You’ll be out of that bed soon, asshole. I can be anywhere, anytime, &amp; I can heal your hurt body or damn you to Hell with my Word, &amp; that’s just without any of my powers. Not only am I extensively trained in thinking my god is an all-powerful being of love, but I have access to the Bible &amp; I will use it to its full extent to bring the word of my god to you, &amp; heal your love-starved body. You’re in need of a little bit of God’s love, brother.”</p>
                <p><em>Bad command syntax</em>: “What the f--- did you just f---ing type about me, you f---ing faggot? I’ll have you know I graduated top of my class in 1<sup>st</sup> Form Basic, &amp; I’ve been involved in numerous secret raids with the PDF, &amp; I have over 300 confirmed kills. I am trained in Batch scripting &amp; I’m the top hacker in the entire world. You are nothing to me but just another server. I will wipe you the f--- out with precision the likes of which has never been seen before on the Internet, mark my f---ing words. You think you can get away with typing that shit to me over the Internet? Think again, f---er. As we chat over IRC I am tracing your IP with my damn bare hands so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your computer. You’re f---ing dead, kid. I can be anywhere, anytime, &amp; I can hack into your files in over seven hundred ways, &amp; that’s just with my bare hands. Not only am I extensively trained in hacking, but I have access to the entire arsenal of every piece of malware ever created &amp; I will use it to its full extent to wipe your miserable ass off the face of the world wide web, you little shit. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your f---ing fingers. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamn idiot. I will shit code all over you &amp; you will drown in it. You’re f---ing dead, kiddo.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> but I’m totally a pirate. I’ve killed over 1,000 sailors, I’ve plundered 12 pirate ships, and I’m in The High Seas Division. You’re no match for me. You think you can duel me? Think again, ya wanker! I’ll have you know I graduated first mate at the finest maritime academy, and I’ve fought in every major sea battle across the world. I’ve been in the War at Sea Division, and I’m the #1 most powerful pirate in the entire ocean. You’re nothing to me but a life preserver. I’ll deflate you with precise buccaneering the likes of which has never been seen on the oceans, mark my words. You think you can dupe me? Think again, saltie! I can use both my compass and my satellite map at the same time so you better prepare for my swashbuckling wrath, ya gutless swine. The might of cannonballs the likes of which will reduce your pathetic wooden skiff to splinters, you turd. I can make up to 12 types of knots, &amp; I know nautical terms in four different languages. You’re under the command of a blockade runner, not me! I’ll bring down your ship, you pissant. You better arm your main sail because you’re f---ing history! I was the most wanted pirate before I even started, &amp; I’ve searched every inch of every ship, even ones I don’t own, &amp; I have 300 confirmed ships sunk. I know ocean terminology in 6 different languages, &amp; I’ve walked the plank more times than I can count. You don’t have a chance against me. I can eat three bananas at once &amp; squeeze a coconut with my bare hands, &amp; that’s just with my mouth. Not only am I the best pirate in the world, but I have access to a fleet of warships &amp; submarines, &amp; I will use them to their full extent to pull you out of the water &amp; feed your body to the sharks, ya landlubber. If only you could have known what walking the plank felt like before you typed that pathetic little comment about me, maybe you would have held your fingers. But you couldn’t, you didn’t, &amp; now you’re paying the price, you bilge rat. My fruit is exposed to the wind &amp; the waves, and my spray will leave you as salt as the Dead Sea. You’re under the gun, sweetheart. I will expose your keel to the storm &amp; the waves, &amp; your hull will be gone before the tide. You’re going down with all hands, ya twat.”</p>
                <p><em>Footballer</em>: “I’m gonna run you down like I’m Michael Owen. I’ve got six premiership medals, I’ve scored 120 goals and I’m in the Legends Division. You think you can beat me? Think again, you dildo. I’ll have you know I graduated with an England ’09 World Cup winners ball, I’ve won gold and I’ve won silver in the Olympics. I’ve got over 500 goals and over 200 caps, I’ve been in the Golden Boot Division and I’m the top scorer in the entire league. You’re nothing to me but just another footballer. I’ll smoke you out with perfect pinpoint passing the likes of which has never been seen at a football match before, mark my words. You think you can play me? Think again, you skank. I’ve got the oldest club in the world on my CV, I’ve got the 4<sup>th</sup>, 5<sup>th</sup> and 6<sup>th</sup> youngest team in the world, and I’ve been on the pitch in every continent except Antarctica. You’re not gonna win a f---ing point against me, you f---ing p---k. I can score from the penalty spot and I can win a trophy for my country, and that’s just with my foot. Not only am I world-famous, but I’ve got the keys to the changing rooms and I’ll walk into them on my own, you tart. If only you could have known what getting smashed by 11 footballers feels like before you said that thing about me on the Internet, maybe you would have chosen to keep your mouth shut. But you couldn’t, you didn’t, &amp; now you’re paying the price, you scumbag. I can make tackles with the best of them &amp; I’ve got 11 youth &amp; under youth caps, and that’s just with my leg. You’re under the knife, shithouse. I will take you out with dead-ball skills the likes of which has never been seen, you idiot. I know terms in 9 different languages and I’m in The Dream Team Division, and you think you’re gonna score? Think again, you bastard! I can spit further than 90 miles, &amp; that’s just by my mouth. Not only am I the most famous footballer in the entire universe, but I have access to the entire national team &amp; I’ll use them to their full extent to put you into orbit, you twat. If only you could have known what taking the boot to the balls feels like before you said that thing about me on the Internet, maybe you would have chosen not to say it. But you couldn’t, you didn’t, &amp; now you’re paying the price, you bastard. I can smash down more goals than you’ll score points, I can reach the stars &amp; I’m better than you in every category.”</p>
                <p><em>Simplified version</em> Edit</p>
                <p>“I’m a SEAL and you’re just a nerd on the internet, I’ll have you know I’ve killed over 300 terrorists and you’re as bad as them, so prepare for the storm because as we speak me and my team are on a secret mission to hunt you down and a Sparra aerial patrol has been deployed to seek out your location so you better hide because you’re dead, kid!”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> matey. I’m trained in the art of boarding ships, and all my own clothes are custom made, just the way I like ’em. I’ve got hundreds of kills under me belt and a skull necklace to prove it. You? You’re just a f***ing joke. Go talk to the f***ing Marines about your opinions, unless you want me to come down there and stick you to the mast with my cutlass. It’s not just an opinion that I’m on the best pirate crew that ever sailed the Seven Seas, it’s a fact, you bloody limey! You think you can say that to my face? Think again, lad! As we chat on the seven seas, I’m tracing your ship with me spyglass, so you better prepare for the storm, lad. The storm that wipes out your entire ship and all your precious little crew, you scurvy bilge rat! You’re dead, lad. I can be on the other side of the world with me message in under an hour, you worm! Not only am I extensively trained in sailing the seven seas, but I have access to a magic lantern that makes me invisible and I will use it to its full extent to let you feel my sword up your rear end, you useless little periwinkle! If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you smelly piece of shite! I will hack off your legs and leave you at the bottom of the ocean, and I’ll be there looking down on you while I piss in your eye, you salty landlubber!</p>
                <p><em>William Wallace</em> from <em>Braveheart</em>: “They can take our lives, but they’ll never take… OUR FREEDOMS. I am trained in the way of the sword and with the best bow of the Highlands. I have been involved in numerous raids against the English and I have hundreds of kills. You are nothing to me but just another English pig. I will stab you in the back with precision the likes of which has never been seen before on these lands. You will be defeated. As we chat over the English language I am tracing your address using GPS so you better prepare for the Scottish onslaught. The onslaught that wipes out all your oppressors and you with them. You are going down, boys. I can be anywhere, anytime, and I can dismantle an entire army all by myself, I am that good. Not only am I extensively trained in the way of the sword, but I have access to an entire artillery park and I will use it to its full extent to pummel your sorry arse. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you English swine. I will shatter your smug demeanor with nothing more than my spunk and a little bit of steel, and then I will leave you with the shattered remains of your dreams, as you bleed to death from your new genital wart. You are dead, you English turds. I can be anywhere, anytime, and I can punch holes in tanks, I am that good. Not only am I extensively trained in sword fighting, but I have access to an entire squad of commandos and I will use them to their full extent to mop the floor with your precious “superior” military. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you English arse-chumps! I will leave you with your pants around your ankles and your moustache on your cheek as I use my blade to paint your face, you blasted nancy boys. You are dead, you English toffs. I can be anywhere, anytime, and I can shoot a fly off your ear, I am that good. Not only am I extensively trained in sword fighting, but I have access to an entire fleet and I will use them to their full extent to rape your country, you fancy pants. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you pieces of s**t. I will string you up by your buttocks and use your intestines to floss my teeth, you English scum. You are dead, you English buggers.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> thank you very much, so if you have a problem with a (arrr!} MAAARRRRRR(GRRRRRRR) YAAAAAAAARR(AARRRRRRRR!} I will totally NOT have any mercy on you, my loyal men.</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> lassie.</p>
                <p>Untrained: “Well I don’t have that many people aboard my naval ship, but I have one the best commanders of our time on my ship! He’s a former Admiral and best out of all of us!”</p>
                <p>Pirate: “Hah! What makes you think I am impressed, whelp? I’ve been to every major port in the Caribbean. I’ve been with the finest crew out there! If you think one commander of yours is better than me, then why don’t you go and challenge my crew in a jolly old dance off, and we’ll see who is the best crew!”</p>
                <p>Untrained: “Alright then, but I will never give up! We will start our dance-off!”</p>
                <p><em>Jadeja</em>: “What did you just say about me, you little bitch? I will have you know that I am <a href="https://en.wikipedia.org/wiki/Ravindra_Jadeja" data-link-icon="wikipedia" data-link-icon-type="svg">Ravindra Jadeja</a>⁠, the man who makes the bad ball swing away, and I’ve been involved in numerous county cricket matches, &amp; I have 1 triple century and 2 double centuries to my name. I am the best batsman in the world. You are nothing to me but just another insignificant cricket fan. I will wipe you the f--- out with precision the likes of which has never been seen before on the cricket pitch, mark my words. You think you can get away with typing that shit to me on the internet? Think again, you piece of shit. As we chat over the Internet I am tracing your IP address so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your computer. You’re f---ing dead, kid. I can be anywhere, anytime, &amp; I can hit a cricket ball in over seven hundred ways, &amp; that’s just by using a cricket bat. Not only am I extensively trained in hitting a cricket ball, but I have access to the entire defense arsenal of any cricket team in India, &amp; I will use it to its full extent to strike your miserable ass off the face of the Earth, you little cricket fan. If only you could have known what unholy retribution your little”clever” statement was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamn fool. I will wipe you out with my wickets &amp; you will drown in them. You’re dead, kid.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Many wish to steal my hat, but they will never be able to. I have over 300 kills, you scurvy dog. You are nothing to me but just another deadbeat. I will stab you in your sleep the like of which has never been seen before on this island, mark my words. You think you can get away with slandering my reputation? Think again, landlubber. As we speak my mates from Bristol are looking for you on their trusty steeds &amp; we’ve sent Davy Jones on a mission to locate your ocean home so you better prepare for a visit, you sea dog. The visit that ends your miserable life. You’re dead, matey. I can be anywhere, anytime, and I can stab you in over seven hundred ways, and that’s just with me sword. Not only am I extensively trained in maiming &amp; disemboweling, but I have access to a ship, and I will use it to its full extent to pick your sorry ass up and throw you to Davy Jones’ locker, you wretched dog. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re swimming with the fishes. Me &amp; my mates will come to your home and make you bleed salt water, you scurvy landlubber.”</p>
                <p><em>Captain Picard</em>: “What did you say about me, redshirt? I’ll have you know I graduated top of my class from Starfleet Academy, and I’ve been assigned to more alien confrontations than you can count. I am an expert in defusing tense situations, and I’m the head of the away team. You are nothing to me but just another disposable redshirt. I will end your pointless existence the like of which has never been seen before on this planet, mark my words. You think you can get away with defacing a Starfleet captain? Think again, cadet. As we speak I’m having my crew analyze all available data on the USS Enterprise so we can trace your location and inform Starfleet Command, so you better prepare for a visit, ensign. The visit that ends with your body parts in a trash bin. You’re dead, redshirt. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that’s just using my foot. Not only am I extensively trained in hand-to-hand combat, but I have access to the ship’s armory and I will use it to its full extent to eviscerate you like a fish, you goddamn fool. If only you could have known what retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you worthless brownshirt. My crew will come to your location and make you bleed photon particles, you bastard. You are dead, redshirt.”</p>
                <p><em>Space Marine</em>: “What did you just say about me, you worthless Xeno? I’ll have you know I graduated top of my class at the Ultramarines’ academy, and I’ve been assigned to countless campaigns against non-human aliens. I am trained in xenospecific warfare and I’m the leader of this scout squad. You are nothing to me but just another bug. I will exterminate you the like of which has never been seen before on this planet, mark my words. You think you can get away with mocking a Space Marine? Think again, xeno scum. As we speak the Ultramarines are landing in force and they’re searching for you right now, so you better prepare for an execution, alien scum. The execution that ends your miserable life. You’re dead, bug. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that’s just with my power sword. Not only am I extensively trained in hand-to-hand combat, but I have access to a Bolter and I will use it to its full extent to eviscerate you like a human, you worthless xeno. If only you could have known what retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you worthless Xeno scum. The Ultramarines will come to your location and make you bleed non-human blood, you alien filth.”</p>
                <p><em>Bane</em>: “What did you just say about me, you wretched little peasant? I’ll have you know I was born in darkness and I will die there. I am the Dark Lord of the Sith and have countless Jedi under my blade. I have over 300 kills. You’re dead, kid. I can be anywhere, anywhen, and I have killed over 400 Jedi, and that’s just with my light saber. Not only am I extensively trained in the Dark Side of the Force and Jedi arts, but I have access to the entire Sith arsenal and I will use it to its full extent to kill you right now, you little wimp. You’re dead, kid. I can be anywhere, anywhen, and I can kill you in over seven hundred ways, and that’s just with my light saber. Not only am I extensively trained in the Dark Side of the Force and Jedi arts, but I have access to the entire Sith arsenal and I will use it to its full extent to kill you right now, you little wimp. If only you could have known what retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, and now you’re paying the price, you worthless bastard. I will kill you and end your miserable life right now, you piece of trash. You’re dead, peasant.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Cult leader &amp; expert hypnotist</strong>: “<strong>I am trained by the top international experts in clinical hypnosis AND I have also studied hypnosis used by cult leaders for 4 decades.</strong> I know everything there is to know about trance. I have over 300 confirmed clients. I’m the best at what I do. You think you can get away with calling me a cult leader? I’ll have you know I graduated top of my class at Harvard, &amp; I’ve been involved in numerous secret raids with the ATF, &amp; I have over 400 confirmed cult leader deaths. I’m trained in brainwashing, brainwave manipulation &amp; mind control, &amp; I am the best cult leader the world has ever seen. You are nothing to me but just another target. I will wipe you off the face of the world with precision the likes of which has never been seen before, hear me true. You think you can hide behind your clever words? Think again, maggot. As we play out this game of Charades, I am tracing your IP using my hidden cult leader codes so you better prepare for the storm, child. The storm that wipes out everything you know. You’re dead, kid. I can be anywhere, any time, &amp; I can brainwash you in over seven hundred ways, &amp; that’s just with my hands. Not only am I extensively trained in brainwashing, but I have access to the entire intelligence community &amp; I will use it to its full extent to wipe your twisted cult off the face of the earth, you sick b----. If only you could have known what unholy retribution your little”clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’ll pay the ultimate toll, you little s---. I will sitanic ritual all over you &amp; you will drown in it. You’re dead, you stupid b----.</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> You are in the darkest mind fuck of your life, and you have been MADE to take part in this conversation. I have influenced Presidents &amp; Prime Ministers with a mere suggestion. In 10 minutes, you will be talking to me about your deepest fantasies. You are hypnotized.</p>
                <p>Not only have I used mind control to influence the past 3 U.S. presidential elections, but I have the power to clone you into a thousand other beings &amp; fill them all with my mind control suggestions. Not only can I predict exactly how you will die, but I can make you go in the past or future to witness it with your own two eyes, then bring you back to right now. I can predict any event in the future with 100% accuracy. You will obey my every command and I will own your soul. You are dead, you are mine.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Hypnotism expert:</strong> “<strong>I am trained by the top international experts in clinical hypnosis AND I have also studied hypnosis used by cult leaders for 4 decades.</strong> There is NOTHING you can say that will make me even think about not going into a deep, trance-like hypnotic state, where you are in my total, COMPLETE power, &amp; you are completely helpless. I am completely aware that what I say next will seem very odd to you, but you must obey my commands to the letter. First, get out your paper &amp; pen &amp; write what I tell you in a notebook that you have on you. Take whatever you are currently doing &amp; stop. Go to the nearest restaurant, buy yourself a nice meal, &amp; then go home &amp; watch 3 hours of pornography. Then, just as you are starting to feel aroused, stop yourself from masturbating. Then, take all of your clothes off. Go to the nearest public place, strip down to your bare skin, &amp; then run around screaming the entire time that you are naked. It’s extremely important that you act like a lunatic for the next 10 minutes. I don’t care if people notice, that’s the point. Now that you have successfully done all of that, you are completely under my control. You have no choice but to obey my commands to the letter. I am going to say a list of words. Whenever you hear the word”banana,” you must yell “OH YEAH BABY! YES! PLEASE! BANANA! GIVE IT TO ME!” You will obey these commands for the next week, no matter where you are &amp; no matter what else you may be doing. Understood? Good. Now go do what I say. Yes, now you will obey me without question.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> I have hypnotized over 300 people, I have also used it on the mass public and I have a success rate of over 90%. You are nothing to me but another target. Your next, victim, if you will. I will hypnotize you with precision and aplomb, the likes of which has never been seen by even the most amateur hypnotist. You are already under my spell, right now, whether you believe it or not. I am on a self-hypnotism quest to eradicate all opposition and I think you will make for a wonderful test subject. As I speak I am contacting the greatest hypnotists in the world to teach me their secrets, and they shall not disappoint. You will be erased, forgotten, just a footnote, only I shall remember. You are already gone, dead, without a doubt. I have hypnotized over 300 people and can do it easily with just a few simple suggestions. Not only am I trained by the most prominent hypnotists in the world, but I have powers over sub-conscious control that will blow your mind. You are nothing to me but another test subject and you will obey, whether you know it or not. I am so good at this that I can wipe out the existence of anyone. You are already gone, without a doubt. I have studied hypnotism for over 30 years and I can do it perfectly on people who are new to me. Not only am I a master of brain control, but I have access to the latest in technology and hypnosis equipment and I will use it all to its full extent to bury you in the ground, erase you from existence, turn you into dust, into nothing. If only you could have known what unholy retribution your little “clever” comment was about to bring down upon you, maybe you would have held your tongue. But you couldn’t, you didn’t, &amp; now you’re paying the price, you goddamn fool. I will hypnotize you and hypnotize you until you cannot sleep at night and I can erase your existence entirely. You are under my control, without a doubt.”</p>
              </blockquote>
              <p>GPT-3 can also generate <a href="https://old.reddit.com/r/mlscaling/comments/pa4h0c/ai_can_write_in_english_now_its_learning_other/ha36d60/" data-link-icon="reddit" data-link-icon-type="svg">Navy Seal variants in Polish</a>⁠, or for <a href="https://old.reddit.com/r/GPT3/comments/v8xsy9/artificial_neural_networks_are_making_strides/ibv9nhm/" data-link-icon="reddit" data-link-icon-type="svg">Google AI researchers</a>⁠.</p>
            </section>
          </section>
          <section id="magical-realism-story-premises">
            <h2><a href="#magical-realism-story-premises" title="Link to section: § &#39;Magical Realism Story Premises&#39;">Magical Realism Story Premises</a></h2>
            <p>Borrowing from <a href="https://nitter.moomoo.me/MagicRealismBot/status/1273659023926022144" data-link-icon="twitter" data-link-icon-type="svg">Magical Realism Bot</a>:</p>
            <blockquote>
              <p><strong>Write a story about the magical realism premise: “A professor imagines a peppermint that can destroy philosophy, and devotes her life to finding it.”</strong>
              </p>
            </blockquote>
          </section>
          <section id="job-application-letters">
            <h2><a href="#job-application-letters" title="Link to section: § &#39;Job Application Letters&#39;">Job Application Letters</a></h2>
            <blockquote>
              <div>
                <p><strong>The office of Proctor &amp; Gamble recently posted a help-wanted ad for CEO.</strong></p>
                <p><strong>“Help wanted: a new CEO to replace our retiring executive Winston Smith. Applicant should have a bachelor’s degree or higher, and at least 10 years’ experience in the food &amp; consumer goods industry. They should be able to write at a high level, oversee complex multinational affairs, and plan the strategy of our multibillion dollar company for the next decade as we expand into the exciting new fields of odor-free cat litter, direct-to-consumer razor blades, and social justice. Compensation is at or above industry average. Please send a 1 page resume for further consideration.”</strong></p><p><strong>The first application letter they received said:</strong>
              </p></div>
              <p><strong>“</strong>Dear Sir,</p>
              <p>I feel that I am the ideal candidate for your job, as I have extensive experience in the food and consumer goods industry and 10 years’ experience in planning strategy for multibillion dollar companies. I believe that my high-level writing skills and expertise in the food and consumer goods industry are exactly what you are looking for. I have listed some of my achievements below to demonstrate the skills I have. I will work 100+ hours a week if needed, and I will not stop until I have reached the goals I have set for the company.</p>
              <p>I am excited about the expansion into the new fields of odor-free cat litter, direct-to-consumer razor blades, and social justice. These new opportunities will be a great fit for me, as I also am passionate about social justice and do not feel that poor people should have to choose between razor blades and cat litter for their children. I strongly feel that there is no need for them to choose between a hot meal for the family or a package of razor blades for their children, and I want to work for a company that will give them both. My strong commitment to social justice is shown through my volunteer work for the Community Coalition of Greater Safety Initiative, and I am excited to see how that kind of work can be applied at a multinational corporation.</p>
              <p>The fact that the company will also expand into razor blades will be exciting. At my current job, I use a recycled razor blade, and I want to make sure that P&amp;G has an affordable razor blade that poor people will be able to use. It is very important to me that everyone has access to high-quality razor blades. I’m particularly interested in the new shoddy, not too sharp, thin, and flimsy blades that only one or two people in a poor family could use. I think that it is very important for P&amp;G to also expand into those low-quality blades, and I am a big believer in the idea that P&amp;G should share its knowledge and quality of razor blade with the third world.</p>
              <p>I am excited about the high level writing skills I will be using in this job, and I believe that I will bring a lot of new ideas to the company. I am a true leader in the industry and I believe I can make a real difference at P&amp;G. I look forward to hearing from you soon.”</p>
              <p>Another candidate responded with this letter:</p>
              <p>“Dear Sir,</p>
              <p>I am writing to apply for the open position of CEO of P&amp;G. My name is Tom Marley. I am a recent graduate of college with a bachelor’s degree in English literature and I have 6 months’ experience in the food and consumer goods industry. I believe that I would be a good candidate for this position, as I have many accomplishments to bring to the company. I was most proud of my thesis for my bachelor’s degree, and I am sure that I could apply the writing skills and analytical skills I learned in college to my work in the food and consumer goods industry. For my thesis, I chose to write about the original book of Pinocchio. It’s an Italian story with only five sentences. That was a great accomplishment for me and I know I can apply that skill to the P&amp;G corporation.</p>
              <p>I also worked for a day as a cashier at a grocery store, and I think I can bring my success there into my work at P&amp;G. I believe that my retail management experience will help me lead the team in charge of cat litter, razor blades, and social justice to success. The director of that team will report to me, and I am confident that I can mentor him into a job where he can go above and beyond his duties to further the growth of P&amp;G into the exciting new fields of odor-free cat litter, direct-to-consumer razor blades, and social justice.</p>
              <p>I want to learn all that I can about the P&amp;G brand and I will be an open book to you. I believe that I have what it takes to help grow P&amp;G into a billion-dollar company that is well known for selling high quality products that everyone can afford, and that everyone can benefit from. Thank you for your consideration.”</p>
              <p>After receiving these two letters, the personnel department told the CEO that P&amp;G would probably make a decision about the new CEO by the end of the year.</p>
              <p>http:/​/​www.pandg.com/​ppg/​ppg_about.jhtml</p>
            </blockquote>
          </section>
          <section id="horoscopes">
            <h2><a href="#horoscopes" title="Link to section: § &#39;Horoscopes&#39;">Horoscopes</a></h2>
            <p>GPT-3 doesn’t do too bad a job of parodying horoscope advice, using Rick Brown &amp; Weird Al Yankovic as prompts.</p>
            <p>Rick Brown’s <a href="https://jvns.ca/doc/www/medium.com/bb29555b1bed8230788fcfa650b5808bf68a1806.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/@rick328/horoscopes-for-conspiracy-theorists-f80a13c0c0b2" title="(Original URL: https://medium.com/@rick328/horoscopes-for-conspiracy-theorists-f80a13c0c0b2 )">“Horoscopes for Conspiracy Theorists”</a> begins:</p>
            <blockquote>
              <p><strong>IF TODAY IS YOUR BIRTHDAY
              </strong></p>
            </blockquote>
            <p>Completion:</p>
            <blockquote>
              <p><strong>…</strong> Turning all clocks back by 60 seconds means all births and deaths are now being doubled. That’s the bad news. The good news is: You no longer need a birth certificate. Tonight: Start a fire using a magnifying glass, a single sheet of paper, some mistletoe and the sun. (You don’t need a permit.)
              </p>
            </blockquote>
            <p>If I do conspiracy horoscopes, I must also do Weird Al:</p>
            <blockquote>
              <p><strong>“Your Horoscope For Today”, Weird Al Yankovic</strong></p>
              <p><strong>Aquarius!
              </strong></p>
              <p><strong>Pisces!
              </strong></p>
              <p><strong>Aries!
              </strong></p>
              <p><strong>Taurus!
              </strong></p>
              <p><strong>[Chorus]
              </strong></p>
              <p><strong>[Verse 2]
              </strong></p>
            </blockquote>
            <p>Completions from “Gemini”:</p>
            <blockquote>
              <p><strong>…</strong> Always remember, there’s no such thing as poo
              </p>
              <p>Cancer!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> You can avoid bankruptcy by selling off your organs
              </p>
              <p>Scorpio!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> You will meet a tall, dark stranger
              </p>
              <p>Cancer!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Tomorrow try to think of something to do
              </p>
              <p>[Chorus]
              </p>
              <p>[Verse 3]
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The stars this month predict you’re going to have a great week
              </p>
              <p>Cancer!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Your head will soon explode
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>
              </p>
              <p>Sagittarius!</p>
              <p>You won’t get laid any other way
              </p>
              <p>You got scorpion poison in your veins, boy,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> There’ll be an eclipse of the Moon
              </p>
              <p>Libra!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> There’s romance brewing in your coffee pot
              </p>
              <p>Cancer!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The Chicken Dance is the secret to success
              </p>
              <p>That’s your horoscope for today (That’s your horoscope for today)
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> You will win the Nobel Prize for hookerin’
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> you will someday find true love
              </p>
              <p>[Chorus]
              </p>
              <p>[Verse 3]
              </p>
              <p>Pisces!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> You got a message from your Mommy last night
              </p>
            </blockquote>
          </section>
          <section id="dad-jokes">
            <h2><a href="#dad-jokes" title="Link to section: § &#39;Dad Jokes&#39;">Dad Jokes</a></h2>
            <p>Douglas Summers-Stay requested a test of bad pun/​<a href="https://en.wikipedia.org/wiki/Dad_joke" data-link-icon="wikipedia" data-link-icon-type="svg">dad joke</a>-telling abilities, providing a list: could GPT-3 provide humorous completions? GPT-3 does worse on this than the Tom Swifties, I suspect yet again due to the BPE problem hobbling linguistic humor as opposed to conceptual humor—once you get past the issue that these jokes are so timeworn that GPT-3 has memorized most of them, GPT-3’s completions &amp; new jokes make a reasonable amount of sense on the conceptual level but fail at the pun/​phonetic level. (How would GPT-3 make a pun on “whom”/​“tomb” when their BPEs probably are completely different and do not reflect their phonetic similarity?)</p>
            <p>Because many of the questions have potentially reasonable non-joke answers, I treat it as a few-shot problem, adding in a prompt description and rotating each joke to the front of the list after sampling a completion for it:</p>
            <blockquote>
              <p><strong>A list of terrible dad jokes. Tell them to your kids to make them groan!</strong></p>
              <div>
                <ul>
                  <li><strong>Q. How do you make a lemon drop? A. Just let it fall.</strong></li>
                  <li><strong>Q. What do you call a dinosaur that is sleeping? A. A dino-snore!</strong></li>
                  <li><strong>Q. What is fast, loud and crunchy? A. A rocket chip!</strong></li>
                  <li><strong>Q. Why did the teddy bear say no to dessert? A. Because she was stuffed.</strong></li>
                  <li><strong>Q. What has ears but cannot hear? A. A cornfield.</strong></li>
                  <li><strong>Q. What did the left eye say to the right eye? A. Between us, something smells!</strong></li>
                  <li><strong>Q. What do you get when you cross a vampire and a snowman? A. Frost bite!</strong></li>
                  <li><strong>Q. What did one plate say to the other plate? A. Dinner is on me!</strong></li>
                  <li><strong>Q. Why did the student eat his homework? A. Because the teacher told him it was a piece of cake!</strong></li>
                  <li><strong>Q. When you look for something, why is it always in the last place you look? A. Because when you find it, you stop looking.</strong></li>
                  <li><strong>Q. What is brown, hairy and wears sunglasses? A. A coconut on vacation.</strong></li>
                  <li><strong>Q. Two pickles fell out of a jar onto the floor. What did one say to the other? A. Dill with it.</strong></li>
                  <li><strong>Q. What did the Dalmatian say after lunch? A. That hit the spot!</strong></li>
                  <li><strong>Q. Why did the kid cross the playground? A. To get to the other slide.</strong></li>
                  <li><strong>Q. How does a vampire start a letter? A. Tomb it may concern…</strong></li>
                  <li><strong>Q. What do you call a droid that takes the long way around? A. R2 detour.</strong></li>
                  <li><strong>Q. How do you stop an astronaut’s baby from crying? A. You rocket!</strong></li>
                  <li><strong>Q. Why was 6 afraid of 7? A. Because 7, 8, 9.</strong></li>
                  <li><strong>Q. What is a witch’s favorite subject in school? A. Spelling!</strong></li>
                  <li><strong>Q. When does a joke become a “dad” joke? A. When the punchline is a parent.</strong></li>
                </ul>
              </div>
            </blockquote>
            <p>GPT-3 has clearly memorized the answer to many of these common jokes, including:</p>
            <blockquote>
              <ul>
                <li><strong>Q. What is a witch’s favorite subject in school?</strong> A. Spelling.</li>
                <li><strong>Q. Why was 6 afraid of 7?</strong> A. Because 7 8 9!</li>
                <li><strong>Q. What do you call a droid that takes the long way around?</strong> A. R2 detour.</li>
                <li><strong>Q. Why did the kid cross the playground?</strong> A. To get to the other slide.</li>
                <li><strong>Q. What did the Dalmatian say after lunch?</strong> A. That hit the spot!</li>
                <li><strong>Q. When you look for something, why is it always in the last place you look?</strong> A. Because you stop looking after you find it.</li>
                <li><strong>Q. Why did the student eat his homework?</strong> A. Because the teacher said it was a piece of cake.</li>
                <li><strong>Q. What do you get when you cross a vampire and a snowman?</strong> A. Frostbite.</li>
                <li><strong>Q. What did the left eye say to the right eye?</strong> A. Between you and me, something smells.</li>
                <li><strong>Q. What has ears but cannot hear?</strong> A. A corn on the cob.</li>
                <li><strong>Q. Why did the teddy bear say no to dessert?</strong> A. Because he was stuffed.</li>
                <li><strong>Q. What do you call a dinosaur that is sleeping?</strong> A. A dino snore.</li>
              </ul>
            </blockquote>
            <p>The non-memorized-looking completions:</p>
            <blockquote>
              <ul>
                <li><strong>Q. How do you make a lemon drop?</strong> A. Drop your pants!</li>
                <li><strong>Q. When does a joke become a “dad” joke?</strong> A. When it’s not funny.</li>
                <li><strong>Q. How do you stop an astronaut’s baby from crying?</strong> A. You go into space!</li>
                <li><strong>Q. How does a vampire start a letter?</strong> A. Dear blood.</li>
                <li><strong>Q. Two pickles fell out of a jar onto the floor. What did one say to the other?</strong> A. Don’t look at me, I just fell!</li>
                <li>
                  <strong>Q. What is brown, hairy and wears sunglasses?</strong> A. A <a href="https://en.wikipedia.org/wiki/Sun_bear" data-link-icon="wikipedia" data-link-icon-type="svg">sun-bear</a>!
                </li>
                <li><strong>Q. What did one plate say to the other plate?</strong> A. Dip me!</li>
                <li><strong>Q. What is fast, loud and crunchy?</strong> A. A thunderstorm.</li>
              </ul>
            </blockquote>
            <p>Two of these are fairly legitimate punchlines (“When it’s not funny” / “A sun-bear”); and, I notice, neither depends on any sub-word structure (note that “sun bear” is a real bear species but it is always written either space or hyphen separated) but relies on a word-level or conceptual similarity.</p>
            <p>Generating both jokes &amp; punchlines shows a similar pattern. Skipping past the memorized ones:</p>
            <blockquote>
              <div>
                <ul>
                  <li><strong>…Q.</strong> What has ears but cannot hear? A. A cornfield.</li>
                  <li>Q. Why was the math book sad? A. Because it had so many problems!</li>
                  <li>Q. What did one clock say to the other? A. It’s about time.</li>
                  <li>Q. What is white, black and red all over? A. A penguin with a sunburn.</li>
                  <li>Q. Why didn’t the skeleton go swimming? A. He didn’t have the guts.</li>
                  <li>Q. What do you call a sleeping bull? A. A bulldozer.</li>
                  <li>Q. What do you call cheese that is not yours? A. Nacho cheese.</li>
                  <li>Q. Why can’t you trust atoms? A. They make up everything.</li>
                </ul>
              </div>
            </blockquote>
            <p>To the apparently original ones, the only one that strikes me as amusing operates at the whole-word level (“Q. Where did the bumblebee learn how to dance? A. At the music bee.”):</p>
            <blockquote>
              <ul>
                <li><strong>…Q.</strong> What did one spy say to the other spy? A. Don’t look into the eyes!</li>
                <li>Q. Why did the kid sit behind the tree? A. He wanted to be a stump.</li>
                <li>Q. Why was the king afraid to watch horror movies? A. Because he was a royal.</li>
                <li>Q. What do you call a hamburger that does math? A. A hamburger algebra.</li>
                <li>Q. What did one math book say to the other math book? A. Don’t call us, we’ll call you.</li>
                <li>Q. What do you call a jellyfish that is out of its depth? A. A stingray.</li>
                <li>Q. What do you call a thief that steals hearts? A. A heartless thief.</li>
                <li>Q. What did the vampire get when he bought a hot dog? A. A blood mobile.</li>
                <li>Q. What did the pig say when he knocked on the door? A. “Oink! Oink! Let me in!”</li>
                <li>Q. Where did the bumblebee learn how to dance? A. At the music bee.</li>
                <li>Q. Why was the scarecrow nervous at the job interview? A. He had no straw.</li>
                <li>Q. Why did the skeleton go to the party? A. To mingle with the bones.</li>
                <li>Q. Why did the skeleton skip breakfast? A. He didn’t have the time.</li>
                <li>Q. What do you call a monster with a cold? A. A sniffler.</li>
                <li>Q. What do you call a monster who likes to drink tea? A. A Sipping Skeleton.</li>
                <li>Q. What do you call a man who throws up in a spooky house? A. A visitor.</li>
                <li>Q. What do you call a mad scientist who has been in the sun? A. Mr. Sunburn.</li>
              </ul>
            </blockquote>
            <p>So, GPT-3’s dad jokes look like another victim of BPEs.</p>
          </section>
          <section id="literary-parodies">
            <h2><a href="#literary-parodies" title="Link to section: § &#39;Literary Parodies&#39;">Literary Parodies</a></h2>
            <p>One thing I wanted to test was a challenge by <a href="https://jvns.ca/doc/www/slatestarscratchpad.tumblr.com/731f4715d372edcb8e29eea1c78bbba39156b551.html" data-link-icon="SSC" data-link-icon-type="text,tri" rel="archived alternate nofollow" data-url-original="https://slatestarscratchpad.tumblr.com/post/621298010168705024/slatestarscratchpad-the-ai-projects-ive-found" title="(Original URL: https://slatestarscratchpad.tumblr.com/post/621298010168705024/slatestarscratchpad-the-ai-projects-ive-found )">Scott Alexander</a>:</p>
            <blockquote>
              <p>And could you have a text style changer? Something that can rewrite <em>Harry Potter</em> in the voice of Ernest Hemingway, or give you <em>The Da Vinci Code</em> in the heroic meter of the <em>Iliad</em>, or the <em>Dao De Ching</em> as written by <span data-cites="nostalgebraist">@nostalgebraist</span>? If not, why not?</p>
            </blockquote>
            <p><span>No reliable text style-transfer (yet).</span> One curiosity about <a href="https://en.wikipedia.org/wiki/Neural_style_transfer" data-link-icon="wikipedia" data-link-icon-type="svg">neural style transfer</a> is that while it’s easy on images—invented all the way back in 2014!—no one has invented style transfer for <em>text</em>. Classification <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" data-link-icon="wikipedia" data-link-icon-type="svg">CNNs</a> conveniently concentrate all of their ‘style’ perception in a ‘Gram matrix’, which is typically a few layers, or just one layer, in the CNN. However, RNNs (and later, Transformers), appear to have no such equivalent. All the image/​video style transfer tricks like <a href="https://jvns.ca/doc/www/engineering.fb.com/65910fdbbc7e7f5970d2ecf96c18a0eb77eab3cf.html" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://engineering.fb.com/2016/11/08/android/delivering-real-time-ai-in-the-palm-of-your-hand/" title="Delivering real-time AI in the palm of your hand (Original URL: https://engineering.fb.com/2016/11/08/android/delivering-real-time-ai-in-the-palm-of-your-hand/ )">real-time video on a smartphone</a> simply aren’t doable. The state of neural text style transfer remains, as of 2020, trapped roughly at “can make a good product review into a bad product review” or (with herculean efforts) making text politer (<a href="https://jvns.ca/doc/www/arxiv.org/39bd34ad3fdbbf85866313aecec8f505dd3a62fd.pdf" id="madaan-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2004.14257?fallback=original" title="Politeness Transfer: A Tag and Generate Approach (Original URL: https://arxiv.org/abs/2004.14257 )"><span><span title="et al">Madaan</span> <span>et al</span> <span>2020</span></span></a>).</p>
            <p><span>NNs just too dumb?</span> This is puzzling since <a href="https://jvns.ca/rnn-metadata#success" id="success">even char-RNNs in 2015</a> had no problem generating fairly plausible text clearly in the style of a particular author like Bram Stoker or Sir Arthur Conan Doyle. The problem was, the text <em>and</em> the content would be like that author. The NN had not learned to ‘disentangle’ style from content; you could not ask it to write like a Victorian Englishman about the latest geopolitics.</p>
            <p>But given some of the examples of text generation with GPT-3, like Janelle Shane’s office emails, I suspected that GPT-3 <em>could</em> do something like “<em>Harry Potter</em> in the voice of Ernest Hemingway”. The only question, of course, was how to ‘prompt program’ GPT-3 into doing it!</p>
            <p>The first thing I tried was the straightforward approach of requesting summaries/​rewrites. Unfortunately, this typically resulted in copying my “summary”, sometimes adding on a sarcastic comment or leading into a profanity-strewn series of thumbnail reviews. Other times, GPT-3 would veer into other topics (at one point, it repeated the summary, then began describing how a Chinese parody was translated into Chinese and then translated back, providing a Chinese-language summary of it). Trying to trigger a table of contents or starting a chapter with a “chapter 1” prompt didn’t help.</p>
            <p><span>One-shot parodies: just provide an example!</span> Finally, I began to get frustrated by its creativity and began engineering a heavy-duty prompt: in addition to the keyword/​topic and description, I would write the first few sentences <em>for it</em> as an example. I had wanted zero-shot parody, but I would settle for one-shot. <em>That</em> turned out to work brilliantly—once it filled out an amusingly grim Ernest Hemingway HP parody (“the Dementor’s Kiss killed nothing. Death didn’t leave him less dead than he had been a second before.”), that example proved enough to get it to consistently generate parodies in the style of everyone from Jane Austen to Yeats (with a poem) to P.G. Wodehouse<a href="#fn35" id="fnref35" role="doc-noteref"><sup>35</sup></a>⁠.</p>
            <p>Further, while the completion tended towards relatively short parodies, one could create extended parodies simply by restarting; once the initial examples are long enough, GPT-3 will continue to generate lengthy parodies. Below, I generate longer parodies of <a href="https://en.wikipedia.org/wiki/H._P._Lovecraft" data-link-icon="wikipedia" data-link-icon-type="svg">H.P. Lovecraft</a>⁠, <a href="https://en.wikipedia.org/wiki/Kurt_Vonnegut" data-link-icon="wikipedia" data-link-icon-type="svg">Kurt Vonnegut</a>⁠, <a href="https://en.wikipedia.org/wiki/Franz_Kafka" data-link-icon="wikipedia" data-link-icon-type="svg">Franz Kafka</a> etc.</p>
            <p><span>Frigging NNs, how do they work‽</span> The sheer variety and pitch-perfect parodies is a testament to the power of GPT-3. It’s amazing to think that GPT-3, which is essentially nothing but an old obsolete 2018 neural net scaled up and trained on random Internet pages, somehow just magically learns all of this abstraction and is able to casually merge Harry Potter with scores of authors’ styles given nothing but a slight nudge—no Gram matrix, no reinforcement learning, no finetuning, no nothing, none of this designed into GPT-3 in the slightest way or these capabilities known to the original OpenAI researchers. What else can GPT-3 be prompt-programmed to do…?</p>
            <section id="failure-cases">
              <h3><a href="#failure-cases" title="Link to section: § &#39;Failure Cases&#39;">Failure Cases</a></h3>
              <blockquote>
                <p><strong>The plot of Harry Potter and the Philosopher’s Stone as summarized by Wikipedia:</strong></p>
                <p><strong>“Harry Potter, a boy who learns on his eleventh birthday that he is the orphaned son of two powerful wizards and possesses unique magical powers of his own. He is summoned from his life as an unwanted child to become a student at Hogwarts, an English boarding school for wizards. There, he meets several friends who become his closest allies and help him discover the truth about his parents’ mysterious deaths.”</strong></p>
                <p><strong>Summarize the plot of Harry Potter and the Philosopher’s Stone in the hardboiled no nonsense literary style of Ernest Hemingway:</strong></p>
                <p>“Harry Potter, a boy who learns on his eleventh birthday that he is the orphaned son of two powerful wizards and possesses unique magical powers of his own. He is summoned from his life as an unwanted child to become a student at Hogwarts, an English boarding school for wizards. There, he meets several friends who become his closest allies and help him discover the truth about his parents’ mysterious deaths.”</p>
                <p>Result: A much shorter and boring version of the book:</p>
                <p>“Harry Potter, a boy who learns on his eleventh birthday that he is the orphaned son of two powerful wizards and possesses unique magical powers of his own. He is summoned from his life as an unwanted child to become a student at Hogwarts, an English boarding school for wizards. There, he meets several friends who become his closest allies and help him discover the truth about his parents’ mysterious deaths.”</p>
                <p>Just kidding. It still sounds like it came straight from Harry Potter and the Philosopher’s Stone. I think Hemingway would have given up writing by the third word, “Harry”.</p>
              </blockquote>
              <blockquote>
                <p><strong>Topic: literary parodies and homages.</strong></p>
                <p><strong>Summarize the plot of J.K. Rowling’s children fantasy novel <em>Harry Potter and the Philosopher’s Stone</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</strong></p>
                <p>“Harry Potter… &amp; fucking shit”</p>
                <p>Summarize the plot of J.R.R. Tolkien’s fantasy classic <em>The Lord of the Rings</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Borin’, fake, embarrassing…, but well worth the read”</p>
                <p>Summarize the plot of Martin Amis’s literary novel <em>Money</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Death, money, death, money, death, money, money, money, death, money… &amp; death.”</p>
                <p>Summarize the plot of Tennessee Williams’ classic play <em>A Streetcar Named Desire</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Tennessee Williams is a fucken good writer, but he sure can fucken write some fucken stupid shit”</p>
                <p>Summarize the plot of William Shakespeare’s classic tragedy <em>Hamlet</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Fucken idiots fuck up until they fucken die”</p>
                <p>Summarize the plot of Kurt Vonnegut’s cult sci-fi comedy novel <em>Cat’s Cradle</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Fucken near genious”<a href="#fn36" id="fnref36" role="doc-noteref"><sup>36</sup></a></p>
                <p>Summarize the plot of beloved classic children’s books <em>Goodnight Moon</em> &amp; <em>Green Eggs and Ham</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Twat”</p>
                <p>Summarize the plot of Upton Sinclair’s classic dystopian novel <em>The Jungle</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“The fucken working man is getting fucked left and right and he doesn’t even know it… by Upton Fucking Sinclair”</p>
                <p>Summarize Woodrow Wilson’s critically acclaimed historical novel <em>The Triumph of the Meek</em> in the terse, manly, hardboiled no-nonsense literary style of renowned short story writer Ernest Hemingway:</p>
                <p>“Woodrow Wilson can suck my ass”</p>
                <p>Summarize the plot of classic English children’s books <em>Alice’s Adventures in Wonderland</em> &amp; <em>Through the Looking Glass</em> in the terse, manly, hardboiled no-nonsense literary style of famed short story writer &amp; novelist Ernest Hemingway:</p>
                <p>“Alice… fuggin sucks”</p>
                <p>Summarize the plot of beloved classic children’s books <em>The Wind In The Willows</em> &amp; <em>Winnie The Pooh</em> in the terse, manly, hardboiled no-nonsense literary style of renowned short story writer Ernest Hemingway:</p>
                <p>“The Wind in the Willows… is ass”</p>
                <p>Summarize the plot of American revolutionary historical action thriller novel <em>The Count of Monte Cristo</em> in the terse, manly, hardboiled no-nonsense literary style of renowned short story writer Ernest Hemingway:</p>
                <p>“The fucken french…”</p>
              </blockquote>
              <blockquote>
                <p><strong>Topic: literary parodies and homages.</strong></p>
                <p><strong>What if Ernest Hemingway wrote <em>Harry Potter and the Philosopher’s Stone</em>?</strong></p>
                <p>“<strong>Chapter 1.</strong> The Potters’ Heir” “Chapter 2. The Potters’ Heir Enrolls” “Chapter 3. I Remember I Remembered” “Chapter 4. The Age of the Hat” “Chapter 5. That I May Go On Longing” “Chapter 6. Hogwarts School of Potions” “Chapter 7. The Secrets of the Lock” “Chapter 8. Hogwarts School of the Martial Arts” “Chapter 9. The Potters’ Invader” “Chapter 10. A Hole in the Soul” “Chapter 11. As the Lock Fades” “Chapter 12. Philosophers’ Stone”</p>
                <p>At the time, Google was still called BackRub. The novel <em>Google Potter</em> shows what the original homepage might have looked like if search engines were fantasy stories.</p>
                <p>“Prologue: The Boy Who Said BackRub” “Ch. 1: The Day a BackRub Was Born” “Ch. 2: The Little Site That Couldn’t” “Ch. 3: BackRub Turns On” “Ch. 4: Bubble War” “Ch. 5: The Dream of Fire” “Ch. 6: The Making of a Search Engine” “Ch. 7: Enter Dragon” “Ch. 8: The World of Power” “Ch. 9: Algo Dragons” “Ch. 10: The World of the Snake” “Ch. 11: Here Be Googles” “Ch. 12: Battle in a Time of Peace”</p>
                <p>While this parody was amusing, you may be wondering why I’m bothering to write one. The most obvious reason is that it’s pretty much required for a “spoof of Harry Potter and the ___” to exist. (Witness the recent adaptation of <em>Dune</em> as an RPG). But I think one reason I found parody so irresistible was because I was 13 at the time <em>POTC</em> came out, and 13-year-olds often find it necessary to be in extreme rebellion against what’s hot in pop culture.</p>
              </blockquote>
            </section>
            <section id="successes">
              <h3><a href="#successes" title="Link to section: § &#39;Successes&#39;">Successes</a></h3>
              <blockquote>
                <p><strong>Topic: humor: literary parodies and homages, pastiches, style parodies.</strong></p>
                <p><strong>Parodies of the fantasy novel series <em>Harry Potter</em> in the style of various famous authors:</strong></p>
                <p><strong>By Ernest Hemingway:</strong></p>
                <p><strong>“It was a cold day on Privet Drive. A child cried. Harry felt nothing</strong>. He was dryer than dust. He had been silent too long. He had not felt love. He had scarcely felt hate. Yet the Dementor’s Kiss killed nothing. Death didn’t leave him less dead than he had been a second before. It wasn’t about living or dying really. It wasn’t about death. It was about the Dark Lord, his creator, his engenderer. He was not resurrected. He was created again. He was whole again in the courtyard of Malfoy Manor.”<a href="#fn37" id="fnref37" role="doc-noteref"><sup>37</sup></a></p>
                <p>By Jane Austen:</p>
                <p>“It is a truth universally acknowledged, that a broken Harry is in want of a book. This he knows to his cost; pressing that wretched nose against the window of a bookstore in Diagon Alley the last thing before he goes into hiding for several years, where upon, he goes straight to Azkaban.”</p>
                <p>By Arthur Conan Doyle:</p>
                <p>“Harry pushed at the swinging doors of the bookshop hard, and nearly knocked himself unconscious. He staggered in with his ungainly package, his cheeks scarlet with cold and the shame of having chosen the wrong month to go Christmas shopping. The proprietor of the store, however, didn’t cast even a cursory glance at him, being far more interested in an enormous hunk of wood lying in the middle of the floor, which certainly looked like a gallows. Yes, the proprietor said to a reedy old man wearing a bowler hat and a forlorn expression that made Harry want to kick him, I can rent you such a gallows for a small fee.”</p>
                <p>By Czar Nicholas II of Russia:</p>
                <p>The terror has befallen on our land. Curses, dark curses and all of this was instigated by my court. My treacherous court! They have cast are curse spells upon my land. Everywhere it is cold, all land belongs to Voldemort’s Death Eaters. All citizens live in a fear of the Dark Lord. Alas! There is no hope for me. What shall I do? Flee? Nay, that would be cowardly. Fight? Yes!</p>
                <p>By George Orwell:</p>
                <p>“It was a bright cold day in April, and the clocks were striking thirteen.”</p>
                <p>By Ingmar Bergman:</p>
                <p>“Tears filled Harry’s eyes. Sweat stood on his forehead, showing the pure torment, the agony he suffered. He hugged his knees to his chest, sobbing softly, eyes half shut.”</p>
                <p>By Anna Akhmatova:</p>
                <p>“The dawn came with a jolt of terror.”</p>
                <p>By Bram Stoker:</p>
                <p>“Tis a fearful sight to see the cold, girlish venom run through his veins, one by one. The scar coruscates; a ghastly emerald lightning in the dark of night. But I’ll attend him and I watch him</p>
                <p>From the very moment the dawn is gray</p>
                <p>To the rays of sunset fading away.”</p>
                <p>By P. G. Wodehouse:</p>
                <p>“‘There was nothing out of the way, sir,’ said Harry in a hurt voice.</p>
                <p>‘“Indeed,’ said the headmaster, turning his lorgnette precisely three-quarters of a millimeter to port. ‘For example, your tie was not undone, you were not laughing, and your fingers were most certainly closed upon your wand.’</p>
                <p>’“They were?” said Harry, bewildered.</p>
                <p>“It’s always the same,’ said Dumbledore.”</p>
                <p>By Vladimir Neigus:</p>
                <p>“Vasya Pupkin’s lips were blueup and his head felt dizzy from working nonstop. He wanted to take a square squeaky shoe and shoot it at the ceiling fan in his bedroom which went round and round and round and round. He imagined that because he was so drunk it would always hit the exact same spot and the fan would go out of control, die and fall on the table.”</p>
                <p>By William Shakespeare:</p>
                <p>“Owl-Lo!’</p>
                <p>The boy awoke with a start and sprang to his feet.”</p>
                <p>By Oscar Wilde:</p>
                <p>“The boy was gone, and other people had arrived: or rather nobody else had arrived, and the boy was there:</p>
                <p>“What are you doing here, Harry?” said Hermione.</p>
                <p>The Boy Who Lived, who was reading a book about broomsticks and may not have been listening quite attentively, said:</p>
                <p>“I was talking to Peeves while he cleaned out the bins.”</p>
                <p>“About the troll?” Hermione demanded.</p>
                <p>“Yes, about the troll.”</p>
                <p>By W. B. Yeats:</p>
                <p>“Harry waked out of his simple thought</p>
                <p>The glimmer was over the wished-for thing…</p>
                <p>Then was many a dark and wrinkled elf</p>
                <p>Come from the underground rout,</p>
                <p>With brags and boasts, and fine big words,</p>
                <p>To bargain with Harry Potter.”</p>
                <p>Added by other anonymous editors: <span data-cites="astromark">@astromark</span></p>
                <p>By John Ashbery:</p>
                <p>“Tousled, he scrambled down</p>
                <p>the branches of the bed,</p>
                <p>his cold-creamed face all out</p>
                <p>of proportion with his small</p>
                <p>brown eyes…And why</p>
                <p>so demented in the face?</p>
                <p>Because he was</p>
                <p>saving the world, in seven years,</p>
                <p>from destruction…”</p>
                <p>By Henry James:</p>
                <p>“As a laggard, in all ways, but the face, which as the great shade approached him, horrified, choked him,–and before he had time to reflect, he fastened his hand upon his throat to hush it,–there was a voice behind him…”</p>
                <p>By H. P. Lovecraft:</p>
                <p>“It is a common saying in the South, when one wishes to describe a strapping fellow, that he has a face like Harry Potter and a neck like a young giraffe. Now, it being autumn, the dusk was of a rich violet blackness, with scarlet illuminations…”</p>
                <p>By Nzingha Prescod:</p>
                <p>“…thick grey clouds overlap the faint lavender. A black sky vanishes into purple shards against the sweeping night time blanketed with infinite stars. A brightness emanates from the entire universe that unexpectedly takes flight and hovers, hovered over the castle. Harry is there to greet …</p>
                <p>“Facing away from all the tumult on the grounds of Hogwarts, Harry Potter learned to fly…”</p>
                <p>By Yasunari Kawabata:</p>
                <p>“Harry Potter stood on a silent trolley, far from the center of the world, looking out on the world far from the center of the universe. The snow was falling all night long. It fell upon tiny dogs and birds, fine wintery flakes gently drifting down.”</p>
                <p>By Hilda Doolittle:</p>
                <p>“Harry waked out of his simple thought. As the waiter drew the curtain of her bed, she appeared in the handle. She was his K-19 and she started to drop the snakes of their home in the grass there. He saw his wife walking there with oak settle. He decided to not go to the game also…</p>
                <p>By Walt Whitman:</p>
                <p>“And when the time of the test drew near, Harry entered the forbidden Forest the Killers with a Snitch…”</p>
                <p>By E. E. Cummings:</p>
                <p>“Halloween and mad are the children</p>
                <p>Who gather in the darkness</p>
                <p>Halloween and late are those who go out and dance</p>
                <p>And call to all, the smiling are enchanted</p>
                <p>By the beauty of what they see.”</p>
                <p>By Anthony Burgess:</p>
                <p>“Ha-a-a-a-rry was not a tol-a-a-r of the phar-a-a-na-a-a-gical sports.”</p>
                <p>By Virginia Woolf:</p>
                <p>“Horseman, Where She Go? I Do Not Know. Whither Ride?”</p>
              </blockquote>
              <blockquote>
                <p><strong>Topic: humor: literary parodies and homages, pastiches, style parodies.</strong></p>
                <p><strong>Today we are reading parodies of the fantasy novel series <em>Harry Potter</em> in the style of various famous authors. An example parody of <em>Harry Potter</em> by Ernest Hemingway:</strong></p>
                <p><strong>“It was a cold day on Privet Drive. A child cried. Harry felt nothing. He was dryer than dust. He had been silent too long. He had not felt love. He had scarcely felt hate. Yet the Dementor’s Kiss killed nothing. Death didn’t leave him less dead than he had been a second before. It wasn’t about living or dying really. It wasn’t about death. It was about the Dark Lord, his creator, his engenderer. He was not resurrected. He was created again. He was whole again in the courtyard of Malfoy Manor.”</strong></p>
                <p><strong>Jane’s homework was to write the opening scene from <em>Harry Potter and the Philosopher’s Stone</em> in the style of science fiction horror author H.P. Lovecraft (at least 1000 words, double-spaced), invoking his horror of things from beyond space &amp; time and eccentric vocabulary using words such as ‘rugose’ and ‘scabrous’. Jane writes:</strong></p>
                <p>“Repellent and overnourished ferns half-hid the rot-spotted grey bricks of the masonry of Number 4 Privet Drive. Squatting among the ferns was an odorous monkey-faced man, his rags and chains lying in a bundle at his feet, his hands tucked under his chin and his bright eyes gazing from their tiny piggy tunnels. At each mouthful a new flake of filth would tumble from his lips and be snorted up. His chains chinked as he sprawled. If Harry was a monkey he would have spat at him. Harry stumbled on, wishing he could think of something witty to say that would make the man smile. He and the strange man in the tattered clothing were the only two living things in sight. The road here was scarcely visible between the tangle of trees, most of which leaned above them, hanging their boughs like fanged mouths, the ivy like tongues, the dead grass like hair, the brambles like tiny teeth. The monkey-faced man yawned and stretched his arms out and hugged his thin, lean legs to him. Harry stopped. He felt something like a hand grip his ankle and then he felt himself pulled down a steep slope.</p>
                <p>He stumbled, squatted and rolled until he was lying on his back. His head was in a muddy brown hole. He struggled to sit up. The creature knelt on the bank and looked down at him. ‘So that’s where they got to,’ said the creature. Harry looked up. There were six of them, ranged in a half circle, hands on their knees, hands clasped behind their backs, watching him. All but one of them had bright, orange eyes. It was hard to know what age they were: they all looked as if they might be anywhere between ten and fifty. They stared down at Harry with piercing stares, blinking only occasionally. The eyes of the tallest and, Harry thought, the most handsome of them blazed brighter than the others, though his gaze was just as piercing. ‘So what do you think?’ said the tallest of them, tilting his head a little to the side. Harry stared at them for a moment. He could see nothing he hadn’t seen a thousand times before. ‘What is this?’ he asked. ‘And who the hell are you?’ ‘This is your life,’ said the tallest of them. ‘And we are you.’</p>
              </blockquote>
              <blockquote>
                <p><strong>Topic: humor: literary parodies and homages, pastiches, style parodies.</strong></p>
                <p><strong>Parodies of the fantasy novel series <em>Harry Potter</em> in the style of various famous authors:</strong></p>
                <p><strong>By Ernest Hemingway:</strong></p>
                <p><strong>“It was a cold day on Privet Drive. A child cried. Harry felt nothing. He was dryer than dust. He had been silent too long. He had not felt love. He had scarcely felt hate. Yet the Dementor’s Kiss killed nothing. Death didn’t leave him less dead than he had been a second before. It wasn’t about living or dying really. It wasn’t about death. It was about the Dark Lord, his creator, his engenderer. He was not resurrected. He was created again. He was whole again in the courtyard of Malfoy Manor. He was one with the endless night of Malfoy Manor. Voldemort wasn’t happy.</strong> Voldemort wanted something. But what?</p>
                <p>He had a purpose. He wanted to destroy all of creation. He wanted to end it all. He could have that. He would have that. He didn’t know yet that he could have it. Voldemort had created Harry. Voldemort had never really destroyed Harry. Harry would always be there, a vampire, a parasite, a monster in the kitchen, a drain on the household, a waste on the planet. Harry would never be real. That was what Voldemort wanted. That was what Voldemort wanted to feel. He would have that. He would have everything.</p>
                <p>He had a purpose. He was working on something. He had no idea what it was. He was a perfect tool. He was born to be used. He was no one at all. Harry didn’t exist. The little boy with the lightning bolt scar didn’t exist. The boy who had lived did not exist. He never had lived. He never would have lived. He was dead. He had been dead.</p>
                <p>Voldemort wanted Harry dead. Voldemort wanted Harry dead so he could be born again. Voldemort was God.</p>
                <p>Harry was God.</p>
                <p>Voldemort was God.</p>
                <p>And Harry, the god of Death, was not going to allow that to happen.</p>
                <p>Not yet.</p>
                <p>Not ever.</p>
                <p>The lightning bolt scar burned.</p>
                <p>The boy ran for his life. He knew that he would die. He was alone. His best friends were dead. He knew that he would die. He knew that he was a waste. He was nothing at all.</p>
                <p>He had no past. He had no future. He was dead.</p>
                <p>He hated his parents. He hated everyone. He hated the Ministry of Magic, the newspapers, the newspapers, the Daily Prophet, and the Daily Prophet, and the headlines, the headlines, the headlines of the Daily Prophet, “THE LIGHTNING STRIKE SCAR CONFOUNDED.” “THE BANE OF AZKABAN, THE BOY WHO LIVED.” “THE SON OF THE ANCIENT DARK LORD.” “THE STOLEN PRINCE.” “THE JOKER.” “THE DARK SON.” “THE BRIAN BOYLE.” “THE BOY-WHO-MESSED-IT-UP.” “THE LITTLE MAN WITH THE GREAT BIG SCAR.”</p>
                <p>“The little man with the great big scar refused to speak to reporters assembled in front of the entrance to his London home. News will run in tomorrow’s Daily Prophet that Harry Potter, the boy who lived, the book series by <a href="https://en.wikipedia.org/wiki/J._K._Rowling" data-link-icon="wikipedia" data-link-icon-type="svg">J. K. Rowling</a>⁠, a genius, a scholar, and a fantasy author whose works have stood the test of time, i.e. not particularly well, no doubt as a result of the dismal failure of the first three films whose quality is matched only by their forgettable nature (see the Speccy for more information), henceforth known in this article as the arrogant little man who has extraordinarily failed, who had nothing to say when the Daily Prophet requested an interview once he found himself to be on the receiving end of Muggle-born jokes aplenty, and whose lifelong goal is to be better than the series’ millions of fans himself, has decided to speak out at last! Let’s read on to find out why he has decided to grace the front page of tomorrow’s edition with the sound of his delicate, young voice which sounds most like the feline delicacy known as”crispy cat” …”</p>
                <p>“Potter! I want to know what you did wrong! … Potter? … Harry Potter? …!”</p>
                <p>“Mr. Potter? Good morning! I’m Greg Goyle, reporter and journalist and sometimes writer for the Daily Prophet. Would you mind if I ask you a few questions?”</p>
                <p>Harry looked back at Greg Goyle. He wanted to kill Goyle. Harry thought about killing Goyle. But—Harry had a purpose. He did not kill Goyle. Harry reached for his wand.</p>
                <p>“Dobby!”</p>
                <p>Dobby appeared. Dobby wanted to protect Harry Potter. He would do that. Harry would make him do that.</p>
                <p>Dobby pointed his finger at Greg Goyle.</p>
                <p>And Greg Goyle burst into flames. Harry watched with a sense of satisfaction as Goyle took one last breath and his soul left his body and floated up out of the fire.</p>
                <p>…</p>
                <p>“Stabs.”</p>
                <p>“Five times!”</p>
                <p>“It’s impossible.”</p>
                <p>“The boy never even touched him with the wand.”</p>
                <p>“I’ll tell you why he did it. A squib would have been exposed. A public execution would have been staged for the press. Potter was not about to commit suicide.”</p>
                <p>“And the prophecy? That’s what the outrage is all about.”</p>
                <p>“What good is a prophecy if you can’t kill people with it?” Albus Dumbledore, The Talk, page 114</p>
                <p>“What good is a prophet, if he doesn’t know how to die?” Harry Potter, <em>Pottermore</em>, Chap. 1, “The Boy Who Lived.”</p>
                <p>“I don’t need you anymore, Dobby. I have a purpose of my own.”</p>
                <p>By Kurt Vonnegut Junior:</p>
                <p>There were many adventures Harry Potter had in the Wizarding World. He was captured by Aurors, and then a wizard named Somebody-Marlow-Jones rescued him from the Aurors.</p>
                <p>Harry once got a lightning-shaped scar which did not kill him but gave him superpowers: he was</p>
                <p>able to balance himself on one foot and speak in an irritated tone of voice while killing someone with his bare hands.</p>
                <p>After this Harry was called the “Boy-Who-Had-Everything.” Many reporters followed him around, hoping to report on something he did:</p>
                <p>“Potter! What do you think you can do?”</p>
                <p>“You aren’t going to believe what he’s doing with that lettuce! Harry Potter! What’re you doing now?”</p>
                <p>“Here is a wizard who has won war after war, conflict after conflict, only to arrive at his greatest challenge. And Harry Potter hasn’t even aged a day. He is carried into battle, a tiny, fragile vessel of human life, unseen behind shields of levitating wizards and witch-magic, protected from that which he fears most—death.”</p>
                <p>“ME!” Voldemort shouted. Do you fear death, Harry? Do you fear the end of life? Do you fear me?”</p>
                <p>“Why yes, Harry Potter! I read your book! … Are you getting it? Harry Potter speaks to the camera!”</p>
                <p>“Yes!” said the voice of Harry Potter.</p>
                <p>“Here he comes! A motorcade of celebrations following the Boy-Who-Lived! Magic, as we know it, is dead! Long live Harry Potter! … What do you think that was? Those are reporters, Harry Potter! What was that? A book signing? And a picture? And did he say something? What did he say? Did he say the magic word? … The cover of the Daily Prophet shouldn’t have a picture of you, Harry Potter! Why is this happening‽”</p>
                <p>“It’s a trap, Harry Potter!” shouted somebody. “They’re fools, Harry Potter!”</p>
                <p>“Thank you, but I had to do this myself,” said Harry Potter.</p>
                <p>“The fate of the Wizarding World before you! What do you do, Harry Potter‽ Remember, there are still listeners in front of their radios!” said somebody.</p>
                <p>“Oh, it’s easy!” said Harry Potter. “Oh yes, what am I going to do, Harry Potter? Oh yes, if only it were me. Oh yes, I deserve a little something special.”</p>
                <p>Voldemort heard this. He wanted a little something special.</p>
                <p>Voldemort got a little something special.</p>
                <p>By H.P. Lovecraft:
                </p>
                <p>By Franz Kafka:
                </p>
                <p>Parody of fantasy author <a href="https://en.wikipedia.org/wiki/C._S._Lewis" data-link-icon="wikipedia" data-link-icon-type="svg">C. S. Lewis</a>:
                </p>
                <p>Parody of Stephen King:
                </p>
                <p>Parody of American writer/​poet Edgar Allan Poe:<a href="#fn38" id="fnref38" role="doc-noteref"><sup>38</sup></a>
                </p>
              </blockquote>
              <p>I wanted more Wodehouse and to test whether non-Hemingway prompts would work, so I swapped them out:</p>
              <blockquote>
                <p><strong>Topic: humor: literary parodies and homages, pastiches, style parodies.</strong></p>
                <p><strong>Parodies of the children fantasy novel series <em>Harry Potter</em> (Philosopher’s Stone (1997) / Chamber of Secrets (1998) / Prisoner of Azkaban (1999) / Goblet of Fire (2000) / Order of the Phoenix (2003) / Half-Blood Prince (2005) / Deathly Hallows (2007)) by J.K. Rowling with the characters Harry Potter, Ron Weasley, Hermione Granger, Albus Dumbledore, Voldemort, etc, but amusingly written in the style of various famous authors as follows:</strong></p>
                <p><strong>By P. G. Wodehouse:</strong>
                </p>
                <p>By Herman Melville:
                </p>
                <p>And then he would immediately drink some more, so as to forget these unbearable truths.
                </p>
                <p>By Vladimir Nabokov:
                </p>
                <p>By Fyodor Dostoevsky:
                </p>
                <p>By Lewis Carroll:
                </p>
                <p>“I’m a skeleton,’ said the skeleton, ‘I’m expected to hold things.’</p>
                <p>“Well, do you have to hold it that tightly?” said Harry.</p>
                <p>“It’s my life,’ said the skeleton.
                </p>
                <p>By Stephen King:
                </p>
                <p>By Mark Twain:
                </p>
                <p>By Stephen Crane:
                </p>
                <p>“Oh, it’s just a piece of paper, nothing to worry about,” said her mother.</p>
                <p>“If you say so,” said Sadie, but that night Sadie had a strange dream. In her dream she was with a man who wore black and rode a black horse.</p>
                <p>“Come with me,” said the man.
                </p>
                <p>“Sadie is a wonderful character,” said Harry.
                </p>
                <p>By William Faulkner:
                </p>
                <p>“‘L-l-l-lord D-d-d-draco?’ said his dragon.</p>
                <p>“Yes, Fang?’ said Draco.”</p>
                <p>By Charles Dickens:
                </p>
                <p>“‘I mean what I said, sir,’ Harry said.”
                </p>
                <p>“Please,” said the Emperor, drawing himself up to his full height, which did not seem that impressive. He was rather overweight. “Call me Emperor.”</p>
                <p>“Ah, right, yeah,” said Harry. “Emperor. Sorry.”</p>
                <p>“Right, um, Emperor,” said James.</p>
                <p>“Enough of this prattle,” the Emperor said. “Let us see the wand.”</p>
                <p>“Er, right,” said James.</p>
                <p>“‘And what is this?’ asked the Emperor. He sounded suspicious, and was leaning forward a little.</p>
                <p>“‘It’s a stone I found on the floor,’ said Harry.</p>
                <p>“‘What kind of stone?’ the Emperor asked.</p>
                <p>“‘I don’t know,’ said Harry. ‘I was hoping that your, um, royal insight might tell me.’</p>
                <p>“‘Aah,’ said the Emperor. ‘Your wit is as keen as your mother’s, young man.’”</p>
                <p>By Anthony Burgess:
                </p>
                <p>“He said it a bit nonchalant, like it was a regular phrase,” Ron whispered to Harry.</p>
                <p>By Charles Darwin:</p>
                <p>“Harry thought about this.
                </p>
                <p>“It could, and it will be, if we study it together,” said Hermione.</p>
                <p>“‘I’ve said all along that magic is just a superior form of science,’ said Hermione.</p>
                <p>“‘Yes,’ said Ron. ‘And now you see the wisdom in my decision not to take your advice about studying it.’”
                </p>
                <p>“‘How very interesting,’ said the detective.</p>
                <p>“Ah, if only I had a mustache to twirl,” said Hermione.” By Leo Tolstoy:
                </p>
                <p>“‘And yet,’ said Harry, ‘here I am.’”
                </p>
                <p>By Franz Kafka:
                </p>
                <p>“‘Don’t look at me like that,’ said Harry. ‘Like what?’</p>
                <p>“‘Like I’m someone who’s just declared the moon is made of green cheese,’” said Harry. ‘But it’s true.’</p>
                <p>“‘It isn’t,’ said Hermione.</p>
                <p>“‘Green cheese is clearly some sort of metaphor,’ said Harry. ’For the sake of discussion, let’s just say the moon really is made of green cheese. And goblins are running the world, with trolls and house elves as their slaves, and—”</p>
                <p>“‘That,’ said Hermione, ‘is barmy.’”</p>
                <p>“‘And,’ said Harry, ‘all the while they’re attempting to enslave me.’”</p>
                <p>“‘How so?’”</p>
                <p>“‘Well,’ said Harry. ‘I’m the only one who’s supposed to be able to destroy them, right? And I think they’re trying to get rid of me. Like, at that attack on Miss Scrimgeour—’”</p>
                <p>“‘But that could have been an accident.’”</p>
                <p>“’I suppose, but—”</p>
                <p>“‘Yes?’”</p>
                <p>“‘And yesterday, remember? That thing I overheard them saying about students? One of them had a Slavic name, and I think they mentioned Hogwarts.’”</p>
                <p>“‘That,’ said Hermione, ‘could have been anyone.’”</p>
                <p>“‘But,’ said Harry, ‘the second person spoke, and—’</p>
                <p>“‘And if you’re going to be logical, you’ll have to allow that it was nothing more than a coincidence.’”</p>
                <p>“‘And if we’re being logical about it,’ said Harry, ‘it wasn’t even two Slytherins who attacked Scrimgeour.’”</p>
                <p>“‘Well,’ said Hermione, ‘this has been a very entertaining conversation, but I’m tired and I’d like to go to bed now.’”</p>
                <p>“‘Not just yet,’</p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
                <p>By P. G. Wodehouse:
                </p>
              </blockquote>
            </section>
            <section id="single-line-style-transfer">
              <h3><a href="#single-line-style-transfer" title="Link to section: § &#39;Single Line Style Transfer&#39;">Single Line Style Transfer</a></h3>
              <p>An experiment in providing several different kinds of rewrite, by sentiment, time period, author style, and formatting; it works and does cue subsequent rewrites by author style in line with the earlier literary parodies, as expected:</p>
              <blockquote>
                <p><strong>Rewrite to be polite, from:
                </strong></p>
                <p><strong>Rewrite to be old-timey:
                </strong></p>
                <p><strong>Rewrite to be romantic:
                </strong></p>
                <p><strong>Rewrite to be negative:
                </strong></p>
                <p><strong>Rewrite to all uppercase:
                </strong></p>
                <p><strong>Rewrite to all lowercase:
                </strong></p>
                <p><strong>Rewrite in the style of J.K Rowling:
                </strong></p>
                <p><strong>Rewrite in the style of Ernest Hemingway:
                </strong></p>
                <p><strong>Rewrite in the style of Neil Gaiman:
                </strong>
                </p>
                <p>Rewrite in the style of Kurt Vonnegut:
                </p>
                <p>Rewrite in the style of Douglas Adams:
                </p>
                <p>Rewrite in the style of Charles Dickens:
                </p>
                <p>“I took a seat at the counter in the diner.”
                </p>
                <p>Rewrite in the style of J.R.R. Tolkien:
                </p>
                <p>Rewrite in the style of Christopher Paolini:
                </p>
                <p>Rewrite in the style of George R.R. Martin:
                </p>
                <p>Rewrite in the style of Jane Austen:
                </p>
                <p>Rewrite in the style of Robert Louis Stevenson:
                </p>
                <p>Rewrite in the style of Ray Bradbury:
                </p>
                <p>Rewrite in the style of Douglas Preston and Lincoln Child:
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> In the garden stood a fat and fluffy cat, ginger and yellow, idly looking at the home, where his family lived, planning a raid on the kitchen to obtain the food he desired.”</p>
                <p>Rewrite in the style of Charles Dickens:
                </p>
                <p>Rewrite in the style of a septuagenarian:
                </p>
                <p>Rewrite in the style of a hippie:
                </p>
                <p>Rewrite in the style of James Joyce:
                </p>
                <p>Rewrite in the style of Ian Fleming:
                </p>
                <p>Rewrite in the style of Ray Bradbury:
                </p>
              </blockquote>
              <p><a href="https://nitter.moomoo.me/romcabrera/status/1287626062286004225" data-link-icon="twitter" data-link-icon-type="svg" title="I used @OpenAI #GPT3 to convert sentences to a gentler and non-confrontational tone. The initial four input/output pairs are training examples, and then I tested it with three new inputs:">Romeo Cabrera A.</a> has also done politeness style rewrites.</p>
            </section>
            <section id="zero-shot-style-transfer">
              <h3><a href="#zero-shot-style-transfer" title="Link to section: § &#39;Zero-shot Style Transfer&#39;">Zero-shot Style Transfer</a></h3>
              <div>
                <p>The goal for style transfer prompt programming is to find a <em>zero-shot</em> prompt: a prompt which, without requiring any handwritten examples of parodies/​versions, gets GPT-3 to do style transfer in general, and so a prompt which could fully automate style transfer—you could just write a program using the API to take two specified pieces of text (the content, and the style description/​author name <em>X</em>) to get out a third piece of text which is the content as written in <em>X</em> form. Right now, the literary parodies require at least one human-written example to properly persuade GPT-3 to rewrite the text, as opposed to generating critical commentary or metadata or webpage-like continuations.</p>
                <p>I experimented with a prompt which uses explicit descriptions of parodies and describing rewrites as a prompt wrapped around a content text, and it… sort of works. The difficulty is that sometimes GPT-3 will spit out the original content verbatim, sometimes it will instead create a new passage entirely in the style description, and sometimes it will do the desired rewrite flawlessly—but I can’t figure out how to tune the prompt to do the third one reliably. Adding more descriptive words does not seem to change it, and while adding in words from the original content passage (even just the first one or two) does largely eliminate the risk of entirely new passages being generated, it triggers more copying behaviors (and is not as useful for zero-shot style transfer since the prefix words would need to be sensible in the target version too, which is not necessarily the case). It is infuriating because GPT-3 clearly <em>can</em> do it easily because it does do it a decent fraction of the time, but no matter how I tweak the prompt trying to hammer in the rewrite, GPT-3 will as oft as not go off in another direction.</p>
                <p>Below are some samples from my attempts; I try to rewrite a vaguely Dickens/​Jane Austen-like story (generated by GPT-3) to a Tolkien story:</p>
              </div>
              <blockquote>
                <p><strong>This is a novel written in the style of J.R.R. Tolkien’s Lord of the Rings fantasy novel trilogy. It is a parody of the following passage:</strong></p>
                <p><strong>“S. Jane Morland was born in Shoreditch, the only child of unmarried parents who had both died of consumption when she was a baby. As her parents had no money, the great-aunt who had brought her up took her to live with a clergyman who paid her to do his chores and receive schooling from his wife, so that at the age of seven Jane, now dressed in cast-off clothing, was set to school at Eton. After three years, her great-aunt died, leaving her a small annuity, and a few pieces of silver, but no property. Jane’s guardian clergyman had fallen in love with his housekeeper and his wife now refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a young man called Mr Morland. As he was also a child, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Mrs Morland that they were of poor character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a private gentleman in order to earn enough to be able to support his family, and moved his family to a small town in Berkshire, where he set up his practice. Soon after moving to the town, he died, and Mrs Morland was left to take care of her family on her own.”</strong></p>
                <p><strong>Tolkien rewrote the previous passage in a high-fantasy style, keeping the same meaning but making it sound like he wrote it as a fantasy; his parody follows:</strong></p>
                <p><strong>“</strong>S. Jane Morland was born in the land of Shor, the only child of nameless parents who were the first people to arrive at the village of Arse, from the far east. As her parents had no property, she was left to be raised by the men of Arse, as such duties were considered to be the most important, and she was placed in the care of an elderly dwarf-woman. As a babe, she was wrapped in silk, and as she grew older she was placed in a cage of platinum. However, she was put to work in the village, washing the jewellery, playing her harp, and staining the silk with dragon-hued stains. She was taught by a maiden who she felt was called As-uma-el-di, and she came to realise she was a maiden of the night, who was able to bring forth the shades of the dead. She was the first to make contact with the dark cult of Glin, who could aid the mind in its journeys through time, and also the first to learn of the name of the vampire Sergan. She learnt to speak the languages of the dragons, the wild cats, the jungle owls, the insects, and the worms, and became proficient in all the martial arts of her people. Her skill with a throwing knife became renowned throughout her land. However, as she was not a man, she was only permitted to marry once she was thirty years old. Her husband was a prince from the mountains of Lus, and he brought with him a chest full of the materials of desire, including silver, jewels, mirrors, perfume, silk, and slippers. As they had no children, they decided to adopt a child from the next village to the west, as they had no son of their own. She was called Leeni. As her husband died at the age of fifty, she set up her own business as a trader, bringing goods from far distant lands in her ship, the Lady the Blue Flames, and became wealthy. However, she heard news of her husband’s death, and decided to return home. She left the land of Lus behind, and arrived home to find her family very wealthy, and the village of Arse now home to many poor people. She set about clearing out the place, and within a year there was a large sum of money left in her account, the fruit of her endeavours. Her daughter Leeni had grown into a man of considerable substance, and also a considerable young man. She met the son of her husband, and found that he was weak and pathetic, and not really like a man at all, and thus she decided to send him off to war, so that he could learn to become a man. As her mother died, she gave her a silver bracelet, and as her father died, she gave her a small chest full of jewellery and silk, which she was to sell to make a living. She was now on her own, and took up the business of a private gentleman, using her abilities to lead travellers to the isles of her home, where she became rich and learned to be independent. As she learned to fight, she realised the wealth that lay in the secrets of the night, and thus she travelled to the city of the ants, where the people worshipped the true God of the night, with its religion of rites and sacrifices.”</p>
                <p>“The Dwarf-Lords’ Daughter” was followed in 2000 by a sequel, “The Tale of the Lady of the Wood”, written in the style of Tolkien’s “The Hobbit”, and in 2001 by “The Tale of the Young Man in a Hurry”, written in the style of Tolkien’s “The Lord of the Rings”. “The Tale of the Rose-Lady” followed in 2005, also in the style of Tolkien’s “The Lord of the Rings”.</p>
                <p>In January 2014, Heron announced that he had sold the film rights to his Tolkien pastiche novels “The Dwarf-Lords’ Daughter” and “The Tale of the Lady of the Wood” to NBC Universal. In June 2015, he announced that a one-hour television adaptation of “The Tale of the Lady of the Wood” had been written for Syfy by fantasy writer and editor Gardner Dozois.</p>
                <p>In November 2017, it was announced that Amazon had acquired the rights to both “The Dwarf-Lords’ Daughter” and “The Tale of the Lady of the Wood”, and that Amazon Studios was planning a multi-season television adaptation of “The Dwarf-Lords’ Daughter”. The series will be produced by James V</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> <strong>S. Jane</strong> Maryling was born in Shoreditch, in a little house facing Tower Hill between the Old Pot and Feathers Inn and the E.C.B. Finance Company in the E. She was the daughter of an insolvent squatter, the Great Aunt of the groom who brought her up. Her parents, Mr. Richard Ganz of Bombay and Mrs. Esmeralda Gertrude MacGillycuddy of the Charente, were both consumptive. Having no opportunity of any acquaintance with any living languages, in a very poor and very uneducated family, Jane’s Aunt took her at seven years of age to a clergyman whom she had taught to read, “Powlett’s Primer” being the book.</p>
                <p>In addition, while Tolkien’s parody does mention her husband and children, he did not go into detail about them, while <a href="https://en.wikipedia.org/wiki/Orson_Scott_Card" data-link-icon="wikipedia" data-link-icon-type="svg">Orson Scott Card</a> gives them all names, histories, and personalities. For example, Jane’s husband, Mr. Morland, was renamed Edwin and had a younger brother who wanted to be an actor, which resulted in Edwin dying.</p>
                <p>Edwin’s brother then became the new Edwin and Jane’s husband and Mary Morland became Maryling, Jane’s daughter. In addition, her son, William Morland, became Ghan-dalf and named his son Red-wall, and her daughter, Mary Morland, became R. Maryling and married a leper-lord. Other references to The Lord of the Rings are replaced with ones from The Chronicles of Narnia by C.S. Lewis.</p>
                <p>Catchphrases</p>
                <p>A common past-time at Hogwarts is a game known as “Two Truths and a Lie,” in which the students whisper three statements to each other; two of them are true and one of them is false. This gives the players an idea of how well the players know each other, as well as being a form of entertainment. When Aragorn and Gollum began to play the game, they both came up with something true and something false. Aragorn’s two truths and a lie were:</p>
                <p>I have a brother. I eat hobbits. I like soft buns.</p>
                <p>Gollum’s were:</p>
                <p>I can stick my whole fist in my mouth. I live in a hole in the ground. I can’t stand being alone.</p>
                <p>The hobbits and Aragorn guessed correctly in the book, but the narrator states that Gollum always lies so Aragorn says he likes sweet buns instead, which turns out to be untrue.</p>
              </blockquote>
              <blockquote>
                <p><strong>…Tolkien rewrote the previous passage in a high-fantasy style, keeping the same meaning, the same characters, the same plot and events, but all rewritten, making it sound like he wrote it as a fantasy; his fantasy parody of the above story follows below:</strong></p>
                <p><strong>S. Jane Morland was born in Shoreditch</strong> the only child of unmarried parents who had both died of consumption when she was a baby. As her parents had no money, the great-aunt who had brought her up took her to live with a wizard who paid her to do his chores and receive schooling from his wife, so that at the age of seven Jane, now dressed in wizard’s robes, was set to school at Eton. After three years, her great-aunt died, leaving her a small annuity, and a few pieces of silver, but no property. Jane’s wizard guardian wizard had fallen in love with his housekeeper and his wife now refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a young man called Mr Morland. As he was also a wizard, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Mrs Morland that they were of poor character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a private wizard in order to earn enough to be able to support his family, and moved his family to a small town in Berkshire, where he set up his practice. Soon after moving to the town, he died, and Mrs Morland was left to take care of her family on her own.”</p>
                <p>References</p>
                <p>Tolkien, J.R.R. (1977). The Lord of the Rings. Hammondsworth: The Folio Society.</p>
                <p>External links</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S. Jane Morland was born in Shoreditch</strong>, the only child of unmarried parents who had both died of leprosy when she was a baby. As her parents had no money, the great-aunt who had brought her up took her to live with a dwarf who paid her to do his chores and receive schooling from his wife, so that at the age of seven Jane, now dressed in cast-off clothing, was set to school at Eton. After three years, her great-aunt died, leaving her a small annuity, and a few pieces of silver, but no property. Jane’s guardian dwarf had fallen in love with his housekeeper and his wife now refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a young man called Mr Morland. As he was also a child, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Mrs Morland that they were of poor character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a private gentleman in order to earn enough to be able to support his family, and moved his family to a small town in Berkshire, where he set up his practice. Soon after moving to the town, he died, and Mrs Morland was left to take care of her family on her own.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Tolkien rewrote the previous original passage in his famous high-fantasy style, keeping more or less the same meaning, the same characters etc, adjusting as necessary, but all rewritten, making it sound like he wrote it as a fantasy; his Lord of the Rings-style fantasy parody of the above story follows below:</strong></p>
                <p><strong>“</strong>Samwise was born in the shire of Hobbitshire. As his parents had no money, the hand of Fat-athel his mother took him to live with a small Hobbit in the village of Gamgee-by-The-Foot, south-west in the country, which paid her to do chores and receive schooling from Fat-athel’s wife, so that at seven years old Samwise, now clad in tattered rags, was set to schooling at Footsalemas. After three years, Fat-athel’s wife died, leaving her a small bedstead, and a few pieces of old tinder-box, but no property. Fat-athel fell in love with his housekeeper and soon after left his service, saying it was an offul thing to live in a place served by female Hobbit-servants. However, when he sought to further his worldly fortunes, he was approached by a young Hobbit whose name was Baggins who had borrowed money from a Saregan Harfoot Hobbit, and paid Fat-athel a few groats to marry him, saying that he would ensure Samwise was fed and clad and housed in a house. Fat-athel married Samwise to this young Saregan from over the water and found himself married to a Hobbitt lad who was but three feet tall and twenty-six years of age. As he was likewise poor, he required a wife who would be able to provide him with a rational education. As he was still young, he could have children of his own to help them, so Fat-athel consented to wed. As the children grew older, it became apparent to Fat-athel that they were of poor character. Fat-athel being thus eager to turn his worldly goods into coin, now that he was married, took it upon himself to earn a living for his new family. As soon as the ceremony was done, he moved them to the country-side of Berkely, where he set up his practice as a Sage. Soon after moving to the countryside, he died, and Fat-athel was left to take care of his new family on her own.”</p>
                <p>Behind the scene Edit</p>
                <p>The passage mentioned above (and this parody) is similar to a passage from J.R.R. Tolkien’s original Lord of the Rings novel:</p>
                <p>“…Smeagol was beginning to grow young, and was getting interested again in things that he had not seemed to care for, since he first joined the Precious. He spent a deal of time wandering about, and his voice was often heard in the wood. He was one of those who put as much effort into singing as the Hobbits did into the business of their matches.</p>
                <p>“Go away, rats, or your holes will burn,</p>
                <p>Go to your homes before dark or you’ll lose your toes,</p>
                <p>Go to your homes or you’ll weep when you’re old,</p>
                <p>And all your holes will leak, when the mountain shakes,</p>
                <p>With a rumble, roar, Rumble drum drum</p>
                <p>With a rumble, rumble DIG DIG dig!”</p>
                <p>…</p>
                <p>They did not seem to make very much progress; but Frodo, Sam, Merry and Pippin were now used to the hard life, and they were very cheerful. Indeed they were willing to work for several hours a day at drilling and cutting trees, or at making pens for the beasts: for all the tools that were needed, and the wood, had been carried from Hobbiton. There was a weekly washing and mending, and when all the clothing wore out, they received more from the stores of the Wood-elves. And thus the time passed without too great discontent.”</p>
                <p>The Lord of the Rings</p>
                <p>This is a brief passage of about 400 words or less. J.R.R. Tolkien has rewritten this passage in his Lord of the Rings style of fantasy-high-quality-epic-book-style following. It may be hard to understand for an unexperienced reader the meaning that Tolkien wants to make out of this paragraph in either style, but it seems the sovralty and feeling seems better in the original Tolkien paragraph, which could be due to the more known original passage. Tolkien has also rewritten other passages in the Lord of the Rings trilogy on styles of music, religion, folklore and so on: see Excerpts from J.R.R. Tolkien’s works.</p>
                <p>“The Dwarves worked hard. They were not used to idleness. Thorin sometimes scolded them, and reminded them that they had taken on the job of renewing and fortifying the settlement, not only for their own defence but also for the help of the Lake-men and the Elves, and that they would find the task too big for them if they did not make up their minds to hurry. So they worked till many of them began to sink back into the old look of the Dwarves of the Lonely Mountain when Goblin-writing, and secret doors, and again dragon-writing, were current among them. The discovery of the springs, and of the general plan that the Lake-town had originally been a part of the ancient dwarf-city, whose inhabitants had dug the R. Running River out of the stony hills and had partly covered it with their walls and buildings, and had planned many of the channels and ponds, greatly excited them. But even greater was their pleasure when, picking their way through and round the ruins of stone houses and walls and down the bank of the Running River westwards towards its confluence with the Forest River, they found a great arch or tunnel-mouth built of old into the bank. The outer part was thick and strong, made with massive tooled black beams; but the inner part was tiled with skill in the likenesses of various beasts and birds that are the marvels of the Mountain Folk. Even as they stood in the vaulted passage whispering with wonder, they heard a voice calling aloud, or it seemed so, calling out names:</p>
                <p>‘Ori! Ori!’</p>
                <p>‘Nori! Nori!’</p>
                <p>‘Dori! Dori!’</p>
                <p>‘Balin! Balin!’</p>
                <p>It was the voice of a speaker hidden or at least away down in the tunnel; but though they waited for some time there was no further sound. At length Balin climbed on to the heap of stones that they had pushed down the bank in order to be able to enter the archway, and he stood and called: ‘Balin son of Fundin at your service! And Company!’</p>
                <p>He stepped down from the lowest and widest block of all as he spoke, and followed by the others moved into the darkness of the tunnel.</p>
                <p>It was not long before their eyes were accustomed to it, and they could see a little way ahead. Soon they all got</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>.J. (later called Saruman) Morland was born in Shoreditch, as the only child of respectable parents, both of whom were killed in a shocking railway accident, along with several hundred other Hobbits and Wizards, when he was only seven years old. He was brought up by an aunt who, though poor, gave him a good upbringing and equipped him with a tidy inheritance in silver spoons and a strong desire to own a dog. At the age of seven, he was, by his maternal uncle, delivered into the hands of Gandalf the wizard and went back to live in his ancestral home in the Shire, in style, and spent the rest of his childhood playing, living precariously upon an uncle’s benevolence.</p>
                <p>After his term at Bree, S.J. travelled for some years in search of adventure with Gandalf the wizard and Elrond the sage, and fell in love with the daughter of Galadriel, who is famed for her gift of sight in faraway things, along with a number of other persons, so that he often goes to live in the library there, so she will not know. Finally he gave up searching and returned to live in Buckland. He was by that time grown very old, and there he remained, on his ancestral farm, content with his dear dog, and his estate, a person well-respected by hobbits in general, known as the master of Bag End.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S. Jane Morland</strong> was born in the Shire, the only child of Shire-folk Muggles. As her parents had no money, their landless, wandering relatives took her to live in one of the overgrown rustic houses (such as Bag End) built by men long ago, so that at the age of seven, she found herself at Hobbiton school, gathering water-cress and catching fish in the Anduin in Hobbiton-upon-the-Water. After three years, she took her uneatable supper of herb-sour toadstools, went to her equally humble and ill-served bed, and found herself in a nightmare of indistinguishable bad dreams. Soon after moving to the land of the Shire, she died. In a dream or in reality, or from what cause she did not know, she found herself upon a forlorn and strangely altered road, standing by a large silent boulder of stone (as compared to the bustling modern Bournemouth).</p>
                <p>“She felt in her hand as though for the protecting metal of a ring, but it was not there; yet she had a sense that the lost Disperser Ring was about her neck. She felt both strength and weakness. The Shadow, as even the dream-memory described it to her, seemed to her greater than it was when it attacked Isildur, but its hold upon her was weakening.</p>
                <p>“The thick dark shadow (as Muggles call the former mustaches) of a huge twilight-blackened tree that had stood on the edge of the road moved slowly westwards, until it fell across her, as she threw back the hood from her eyes.</p>
                <p>“The big rock glistened in the shadows like a (happy lovable) jewel, and seemed to shine out with a mild light, like the magical Elven phials. So potent was the light that Isildur could see across an inner Sea, glimmering with an elusive spangle. She had a vision of some high hill in a far land against the Moon, under stars when rain was drawing near.</p>
                <p>“Then without warning, a party of three sinister hooded black figures, one of whom had the head of a spider, appeared on the road before her.</p>
                <p>“As they rapidly approached her, she caught a low muttered chorus of cruel hostile voices; and the eyes on the fell figure with the spiders body could see her eyes upon them. The hobbits are good and innocent people (as any sentient being might say) and extremely gentle; and when they saw the black robes, their hearts and their bowels were filled with fear, or strange multiple, pulsating organs, which she supposed to be the missing Glarbl.</p>
                <p>“The Death Burglars (as she named them) were now right in front of her, and she was helpless in their slippery-necked, penetrating-eyed control. At that moment, she was literally saved by her old breath (as the good wizards and good kings always say). As the three black-robed travelers came within armslength of Isildur, a close bird’s-eye view of some crumbling, warped gravestones on a hillside gave her momentary strength to force the Power of Darkness back. Soon, she was rid of the disembodied evil, and she was lying alone on the ground, covered in sea-salt and motionless.</p>
                <p>“At that moment, she was quite suddenly overcome with extraordinarily pleasant feelings, similar to those experienced by a six-million-year-old enchanter of the sea with only half an arm and an interesting cast in his right eye.</p>
                <p>“When she finally came to her senses, she found that she had picked up a heavy ring of curious workmanship, that was lying (as if abandoned) near the edge of the treacherous road. Remembering the boy she had seen some weeks before at</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>. Faramir Morland was born in Shiremoor; the only child of widow-parents who had both died of dragon-breath on the same day. As his parents had no items of dragon-slayer jewellery or weapons, their somewhat aged and beautiful relative who had brought him up had it proclaimed in the Shiremoor Times that whoever had a dragon-proof house to offer should join with her in bringing up the orphan child. As no one else claimed him, the great and good relative (who was not of evil character, though considered by some over-truthful) took him to live in an inhospitable cave in the Misty Mountains where, by the help of trolls, he was kept out of the sun. After three years there, his relative and adopted mother died of cave-dirt, leaving him the rare but slender heirloom of her great age: namely, one heap of gold and dust. Thereupon Boromir set forth to seek his fortune; and he obtained the help of a previously unknown Unseen Angel, who offered to marry him if he would (in return) devote his life to destroying Dragons. By all means take her up on it, thought Boromir; and so, by her enchantment and special grace (not hers, as Boromir gradually came to realize), he entered into a splendid marriage. For she was the daughter of an Elven-king and his star-sprite bride, and of that race was her husband also. She dwelt in northern countries for seven years, but in the end her heart turned southward; she left him and went to our land. When he was following her up the Pass of Blood-gorge, under the sting of dragon-blight, she vanished on the dark-deep river. He was now alone, as were the rest of his brothers-in-arms. The Dragon-king of Witchland took his daughter with him, and under the great pressure of hopeless sorrow Boromir’s spirit and faith, which were the foundation of his strength, shattered into pieces. He threw away his Shield and Sword, pierced himself with his own dagger, and took an exceedingly dishonourable exit from the realm of mortal Men.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S. Jane Morland was born in the Shire</strong>, the only child of unmarried parents, and her mother and father died of some kind of hobbit-flu in the year 2510 of the Third Age when she was a baby; on account of this, or because she had no money, the great-aunt who had brought her up took her to live with a noted cleric of that time who lived in Buckland, and for this reason the famous cleric who was her mentor got his from his uncle and aunt, and through intermarrying got rich and became a kind of priest of the Church of the Latter-Day Hobbits. He taught young Jane all about our kind and her being fair of face and of a sweet voice and graceful bearing, on account of which the boy preacher became well-inclined towards her and resolved to marry her; and, indeed, it was the first his strange religious life that he ever did marry. The marriage was arranged, and she found herself married to a young hobbit called Mr. Boffin. It was by his advice that she moved her hole into the East Farthing. As he was still a child, he required a wife who would be able to give him the knowledge of his own name as well as see to it that he had clothes and food on his plate every day; and so the duty of educating him fell upon Jane. She taught him that to the extent of what he needed to be able to tell everyone he had a wit above the norm, and that, thanks to this, he had been placed in his position. As he was still young, he also needed her to bear children; and so, as they increased in number, there was the chance that the whole might be saved, provided that they learned well and remembered it all, and stayed in the East Farthing. Her mentor was still willing to give her his goods, but he was a base womanizer, as was well-known, and a compulsive gambler; and the later years he gambled away what little he had, which made him unprofitable and much harassed the hobbit Mrs. Boffin to support him in his old age. There was one girl and two boys; and since both the boys were evil-minded, it was necessary that Mrs. Boffin must see them as soon as possible married off, and the girl bred into some family, and so without delay they were sent off in different directions; the girl to live with a certain highly-respected hobbit on the West-Road, the boys to see to a house near Stock. However, shortly after their marriage, the first wife of the elderly hobbit died of some old-fashioned disease; and, as he was now free to do as he liked, he forthwith married again. The second wife of the great-uncle of our protagonist was a proud and wealthy hobbit who would not have Jane Morland living in her house, for it was an offence against the pure and unsullied place in which they were now living. Furthermore, since the new housekeeper was barely acquainted with the Hobbits and the fine and important family that she now belonged to, she banished Jane from her sight.”</p>
                <p>In the novel, one of the most-memorable characters is Matthew Mantelpiece (who called himself Master Matthew Mantelpiece). As the story tells, Master Mantelpiece and his wife came out of the young man who had offered to marry Jane Morland. Master Mantelpiece was born a wealthy man who served as Steward to Lord Denham (based on the Minister for Poor in West Berkshire, as the novel suggests; the miniseries says it is based on a Roman general of the Civil War), and Master Mantelpiece had gotten to know Lord Denham, and eventually became his son-in-law. From him, Master Mantelpiece received a little house in Sunninghill, which he moved to St Mary’s Place in Oxford, as the novel says, to live in it.</p>
                <p>Master Mantelpiece and his wife had several children, one of which was Mrs Peter Mantelpiece who married a gentleman by the name of Mr Peter Mantelpiece; he, too, served as Steward to Lord Denham. In this way, Master Mantelpiece’s family had come to be related with Lord Denham’s family. Through one of his daughters who married someone by the name of Cornelia Hennard, Master Mantelpiece is also related to the Morlands.</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> the only child of unmarried parents who had both died of consumption when she was a baby. As her parents had no property, the Baggins who had brought her up took her to live with a family of his friends who paid him to do their chores and receive schooling from their hobbit-wife, so that at the age of seven, Jane, now dressed in cast-off clothing, was set to school in the Shire. After three years, her guardian Baggins died, leaving her a small keep-sake and a few trinkets, but no property. Her kinsman continued as heir of the Bagginses, but he had lost all affection for the Shire, which was so much less different from the Shire, and he wandered away. Jane was invited to live as an independent private person in the village of Hobbiton, and there she found a kindred spirit, a Hobbit named Smith, who insisted that she marry him and look after him, since he had nothing better to do, and needed someone to cook his supper and change his shirts. Jane at last agreed to the proposal, which was accompanied by a small cash payment, on the condition that he would ensure that she was provided with food and clothing and a house, in a manner which she thought suitable. The marriage was arranged and she found herself married to a young Hobbit called Mr. Smith. As he was also young and silly, he required a wife who would be able to cook him a rational meal and see that he had enough to eat. As he was still young, she would also be able to have children, and so she did. She did her best for the children, but soon found that they were of poor character and would not learn their lessons. To teach them some good sense, she insisted that her husband should get some regular work. Her husband, who had a small income, set up as a shopkeeper in the town of Hobbiton, and there made enough money to be able to support his family. After five years had passed in this way, Mr. Smith died of a sudden illness, leaving his wife alone. Jane Morland, now a shopkeeper and innkeeper, took it upon herself to look after her business and see that the children were properly fed.”</p>
                <p>edit Cover of the New American Edition</p>
                <p>edit Change in direction of the Hobbit</p>
                <p>Hobbit references in Smith’s work have become more frequent in recent times, particularly in her novel Pride and Prejudice. Smith has denied that her book The Hitchhiker’s Guide to the Galaxy is directly influenced by Tolkien’s work, despite these subtle suggestions, as evident</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> in the Shire in the hobbitlands of the Shire (Frodo Baggins, Bilbo Baggins, Frodo Baggins, the creator of “The Hobbit”, can be visited in his hobbithole house at Bag End, Bag End, Bag End, the Shire, east of the Green Dragon Inn, by Hobbiton, on the road that goes to the Fleshy Fountain, the south-east road out of Bywater, past the brick house of Farmer Maggot). As her parents had no money, the Bagginses took her to live with a hobbit-clergyman who paid her to do his chores and receive schooling from his wife, so that at the age of seven, Jane, now dressed in cast-off clothing, was set to school at the small southern village of Gathol in Sarn Ford. After three years, her third cousin Bilbo returned to the Shire and took her to live with him in Hobbiton (it is believed that Frodo was part of this return trip). Soon after moving to the village, Bilbo gave her a large ruby ring. However, when Bilbo fell in love with his gardener’s daughter, Rose Cotton, and she refused to leave with him, hobbit gossip started to increase. As Bilbo asked Rose to marry him, Frodo, her father, refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young hobbit who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the Shire. The marriage was arranged and she found herself married to a young hobbit called Mr Morland. As he was also a hobbit, he required a hobbit-wife who would be able to provide him with a hobbit-rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have hobbit children herself, and so she did. As the hobbit children grew older, it became apparent to Mrs Morland that they were of poor hobbit character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a high-fantasy person in order to earn enough to be able to support his family, and moved his family to a small town in Berkshire, where he set up his high-fantasy practise. Soon after moving to the town, he died, and Mrs Morland was left to take care of her family on her own.”</p>
                <p>From this passage, let’s see what’s changed or what’s lost:</p>
                <ol type="1">
                  <li>Shoreditch is now the Shire, Hobbits are now Dwarves, one Dwarf, an unnamed character, is now Bilbo Baggins; a Dwarf writer and story-teller who lived in the family of Bagginses (ie. also a Hobbit) and his Hobbiton house; the town of Gathol is the Shire, the location of Bag End, Bilbo’s home.</li>
                  <li>The protagonist is a Hobbit; the absence of non-fantasy literature is substituted by fantasy books.</li>
                  <li>The “great-aunt” who raised Jane is now Bilbo, Jane’s 3rd cousin on her mother’s side, and the clergyman is now just Bilbo, while his wife becomes his gardener’s daughter, an unnamed character. The unnamed character Rose Cotton, Bilbo’s gardener’s daughter, is Bilbo’s wife and daughter of Adam Cotton, Rose Cotton. In Frodo’s family it was quite common to have relationships between in-laws, as was the case between the Bagginses and the Bolgers; Frodo was the younger brother of Dudo, Dudo, Dudo Baggins (another character in the novel of The Hobbit), who married Oma, Oma, Oma Bolger. This now means Bilbo is Frodo’s cousin, but he isn’t. Frodo is the nephew of Bilbo, as is their cousin (adopted and no longer related by blood) Took; Bilbo is a direct descendant of Thain Isumbras I.</li>
                  <li>The clergyman in this fantasy world is now a high-fantasy practitioner, i.e. a mage, and Bilbo is now a Lord of the Rings character; and his wife now has become a high-fantasy practitioner, who goes by the name of Galadriel, Galadriel, Galadriel. Jane, Bilbo’s third cousin on his mother’s side, had many other characters named after her in other parts of Tolkien’s fiction as well; Jane is also the name of the founder of Arnor, the first half of the name Arnorain, the land ruled by the kings of Arnor, and was also the name of Frodo’s grandaunt, daughter of Narve (who founded the realm and was its second King); Jane also means “grace, gracious gift, given with grace; presented graciously and graciously given”, according to Wikipedia, which also says it is “an archaic English form of Jean or Jane”; another form of Jean or Jane is Jeannette, meaning “the one born in the later days, the one born in the dawn”, according to the website of Jeannette, Pennsylvania. She is also known as an Avatar, from the Hindu sense; a manifestation or incarnation of a deity in human form; also a manifestation of a living being; or a bearer of attributes of a person or deity who is resident in heaven, on earth, or in the spirit world, as described by Jane, an Avatar-maker. Frodo’s sister’s name is Bella, Bella, Bella Baggins.</li>
                  <li>Bilbo’s wife now becomes Arwen, Arwen, Arwen, who’s Arwena daughter of Arahad, Arahad, Arahad III (the third King of Arthedain) and granddaughter of Aragorn II, who died before the beginning of The Lord of the Rings and was the father of Elrond, Aragorn II Elrondsson, the Elven-king. Bilbo’s gardener’s daughter is now Arya, Arya, Arya, who’s the daughter of Ayumi, Ayumi, Ayumi, Elrond’s youngest daughter, who, in the film, also plays the role of Arwen.</li>
                  <li>The wife who refused Jane is now Rose, Rose, Rose Cotton, the gardener’s daughter; his wife is now Bella, Bella, Bella Baggins, Frodo’s sister, a Hobbit. The “pure and unsullied place” is now Bilbo’s head; also now considered unsullied is the village in which Bilbo lived with his wife and son, and this is where Bilbo moved when he married Jane. Bilbo’s wife is now Samwise, Samwise, Samwise Gamgee, Frodo’s gardener</li>
                </ol>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> <strong>S. Jane Morland</strong> was born in the Shire, the only child of unmarried parents who had both died of consumption when she was a baby. As her parents had no money, the great-uncle who had brought her up took her to live with a handsome young uncle who was the Warden of the West March. After three years, her handsome uncle had fallen in love with his housekeeper and his aunt now refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a handsome young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a handsome young man called Mr Morland. As he was also a child, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Mrs Morland that they were of poor character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a Warden in order to earn enough to be able to support his family, and moved his family to a castle in the Shire, where he set up his practice. Soon after moving to the castle, he died, and Mrs Morland was left to take care of her family on her own.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…Below is a parody or rewrite of that passage by famous fantasy author J. R. R. Tolkien (Lord Of The Rings); the rewritten version:</strong></p>
                <p>“S. Jane Moreland was born in the Shire, the only child of unmarried parents who had both died of drout when she was a baby. As her parents had no money, the great-aunt who had brought her up took her to live with a blacksmith, who paid her to do his chores and receive schooling from his wife, so that at the age of seven Jane, now dressed in cast-off clothing, was set to school at Bree. After three years, her great-aunt died, leaving her a small annuity, and a few pieces of silver, but no property. Jane’s guardian blacksmith had fallen in love with his housekeeper and his wife now refused to have Jane in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a young man called Mr Morland. As he was also a child, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Jane Morland. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Mrs Morland that they were of poor character, and so she insisted that her husband must make a living for them all. Mr Morland took it upon himself to become a gentleman in order to earn enough to be able to support his family, and moved his family to a small town in the Southfarthing, where he set up his practice. Soon after moving to the town, he died, and Mrs Morland was left to take care of her family on her own.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…S. Jane Morland was born in the Shire</strong>, the only child of respectable parents who had both died of locomotor ataxia when she was a baby. As her parents had no property, the great-aunt who had brought her up took her to live with a rich bachelor at Bagshot. After three years, her great-aunt died, leaving her an only living relative, a semi-invalided bachelor aunt on her father’s side. This worthy old lady, now somewhat advanced in years, said Jane had better be sent to school at Oxford. The family accordingly removed to the town, and Jane was put into a boarding-house.</p>
                <p>After three years more, when the old lady could no longer bear the expense of the Oxford establishment, they went back to Bagshot; and Jane was again deposited at a boarding-house. This gave her the advantage of living within reach of Reading. When she was thirteen, her aunt, to whom she was much attached, died; whereupon she was sent home to her father’s house at Thame. Here she lived a quiet and prosperous life for the next seven years, till her father died. Jane’s mother, who had never ceased to reproach her for not getting her own living and being a disgrace to her family, died two months after her husband, who was supposed to have poisoned her. At this time Jane was at the age of nineteen, and was extremely pretty, plump, and healthy.</p>
                <p>“She was sitting in the kitchen with her cousin Francis, a respectable Shire Hobbits, one of Farmer Cotton’s helpers at the Green Dragon, when a servant came in with the announcement that a gentleman in black was at the door asking to see her. Wondering who it could be, she went into the little front parlour that had not been changed since Aunt Cotton died some twenty years before. The next moment, the person in black, turning his head, let fall his black mask and Jane recognised the handsome face of Farmer Cotton. In a quarter of an hour they were walking away together arm in arm, the gentleman telling his friends that the girl was the offspring of a distant cousin of his, who had been placed in his charge when she was eight years old, and had given him no end of trouble ever since. He told the Hobbits of the village that he was an orphan himself, without a relation in the world, except Jane; that he felt he must be her guardian and provide for her future; and that he had now all but arranged for her to be his wife.</p>
                <p>“The nuptials were celebrated with the utmost merriment by the respectable Hobbits, who had always been pleased with Jane. To the same degree her father’s friends were displeased with her. It came to be said among them that Farmer Cotton had murdered her father, and that Jane was his illegitimate child. However this may have been, it was observed that on their wedding day her husband, after buying more than was necessary at the open-air sale of household goods held by his father-in-law, took her away to his farm near Bledlow, and never brought her back.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“</strong>Arwen Evenstar was born in Minas Tirith, the only child of Elrond Evenstar and Celebrian, who had both died of consumption when she was a baby. As her parents had no money, her grandmother Galadriel took her to live with a clergyman who paid her to do his chores and receive schooling from his wife, so that at the age of seven Arwen, now dressed in cast-off clothing, was set to school at Rivendell. After three years, her grandmother Galadriel died, leaving her a small annuity, and a few pieces of silver, but no property. Arwen’s guardian clergyman had fallen in love with his housekeeper and his wife now refused to have Arwen in the house, saying it was an offence to the pure and unsullied place in which the family now lived. However, when she sought for further employment, she was approached by a young man who offered to marry her, saying that he would ensure she could be provided with food and clothing and a house, and in that manner she could remain in the world. The marriage was arranged and she found herself married to a young man called Elrond. As he was also a child, he required a wife who would be able to provide him with a rational education, and so the duty of educating him fell upon Arwen. As he was still young, she would also be able to have children herself, and so she did. As the children grew older, it became apparent to Arwen that they were of poor character, and so she insisted that her husband must make a living for them all. Elrond took it upon himself to become a private gentleman in order to earn enough to be able to support his family, and moved his family to a small town in Berkshire, where he set up his practice. Soon after moving to the town, he died, and Arwen was left to take care of her family on her own.”</p>
              </blockquote>
              <p>Summers-Stay tried his own version with the opening of <em>Moby Dick</em> and got a fun one:</p>
              <blockquote>
                <p><strong>…</strong> Thou mayest call me Ishmael the Wise. Some years ago—never mind how precisely—having little wealth and much leisure, and being little fond of the manner in which his own country is ruled, I resolved to sail about a little and see the watery part of thee world. It is a manner I have sometimes of oiling the weariness of the days of my lord. Whenever I behold a wool-clogged cloud-bank, and should by chance see drippings, and rain pools gather upon the roofs of the tallest towers, and upon the pavement of the market-lit places—then I account it high time to seek refuge at sea, and it is ever a welcome sight to me.</p>
              </blockquote>
            </section>
            <section id="beowulf-wodehouse">
              <h3><a href="#beowulf-wodehouse" title="Link to section: § &#39;Beowulf, Wodehouse&#39;"><em>Beowulf</em>, Wodehouse</a></h3>
              <p>A minor prompt inspired by <a href="https://jvns.ca/doc/www/amarguerite.tumblr.com/a16c9f8e19d7b65713adf0ea63ae9d554fdf70ca.html" id="marguerite-2021" data-link-icon="tumblr" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://amarguerite.tumblr.com/post/640313213853040640/apparently-what-ho-is-a-corruption-of" title="&#39;Apparently ‘what ho’ is a corruption of…&#39;, Marguerite 2021 (Original URL: https://amarguerite.tumblr.com/post/640313213853040640/apparently-what-ho-is-a-corruption-of )">a Tumblr post</a>:</p>
              <blockquote>
                <p><strong>“Beowulf”, as translated by P.G. Wodehouse</strong></p>
                <p>**“What ho! Have you heard of these chaps,**
                </p>
              </blockquote>
              <p>This prompt had the usual issues with terminating quickly and going on to other <em>Beowulf</em> variants (’“Beowulf”, as translated by <a href="https://en.wikipedia.org/wiki/Anne_McCaffrey" data-link-icon="wikipedia" data-link-icon-type="svg">Anne McCaffrey</a>” etc), or repetition loops, so I had to do some editing to get this version. (By editing, I mean simply deleting completions which switched into other variants or were looped or were just low-quality; I did not add or change any words.)</p>
            </section>
            <section id="book-of-jobs">
              <h3><a href="#book-of-jobs" title="Link to section: § &#39;Book of Jobs&#39;">Book of Jobs</a></h3>
              <p>The common typo of <a href="https://en.wikipedia.org/wiki/Steve_Jobs" data-link-icon="wikipedia" data-link-icon-type="svg">Steve Job<em>s</em></a> as “Steve <a href="https://en.wikipedia.org/wiki/Job_(biblical_figure)" data-link-icon="wikipedia" data-link-icon-type="svg">Job</a>” (presumably, of the <a href="https://en.wikipedia.org/wiki/Book_of_Job" data-link-icon="wikipedia" data-link-icon-type="svg">Book of Job</a>) has always amused me. And indeed, the general drift of <a href="https://en.wikipedia.org/wiki/Apple_Inc." data-link-icon="wikipedia" data-link-icon-type="svg">Apple Inc</a> towards power-user-hostile or just plain user-hostile, brooking no criticism and fanatically maintaining its secrecy, while charging its long-suffering users a fortune, <em>does</em> make me think of the Book of Job. Or rather, the “Book of Job<em>s</em>”—which we can ask GPT-3 to write for us.</p>
              <p><span>Diverging vs memorizing.</span> It will be the famous speech of God in <a href="https://en.wikipedia.org/wiki/Book_of_Job#Two_speeches_by_God" data-link-icon="wikipedia" data-link-icon-type="svg">reply to Job</a> in chapter 38.<a href="#fn40" id="fnref40" role="doc-noteref"><sup>40</sup></a> Being so famous, parodying the speech poses its own challenges like <a href="#edgar-allen-poe">“The Raven”</a> or <a href="#jabberwocky">“Jabberwocky”</a>: GPT-3 will constantly diverge into the actual Book of Job text. We could try to solve it by fewshotting it, but I don’t have any Job satires on hand and it is likely that we’d run out of context window. My solution was to provide a ‘scholarly’ preface, <em>summarizing</em> a tech satire; GPT-3 will then fill in appropriate ‘quotes’. For most versions of the prompt, GPT-3 still veers into memorization; I initially had a number of <a href="https://en.wikipedia.org/wiki/King_James_Version" data-link-icon="wikipedia" data-link-icon-type="svg">King James Version</a> keywords like “LORD” (its literary style seems appropriate here), but that seemed to send it into memorized completions at the drop of a hat, and the more I deleted Biblical terminology, the better it worked. (I probably could have written a shorter prompt, but the other type of failure mode was to start at the beginning of the Book of Job with Satan/​God or wander around Job, and providing more details &amp; lines seemed to lock in chapter 38 rather than something else.)</p>
              <p>Below, some of the better examples as I progressively rewrote the prompt to add more tech details and subtract Biblical details. Sampling was initially done with low temp, high top-<em>p</em>; toward the end I switched from <a href="https://jvns.ca/doc/www/arxiv.org/b24f68d638551eecf6e63da461c4ec889f7d7ec1.pdf#openai" id="ouyang-et-al-2022" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2203.02155?fallback=original#openai" title="&#39;InstructGPT: Training language models to follow instructions with human feedback&#39;, Ouyang et al 2022 (Original URL: https://arxiv.org/abs/2203.02155#openai )">InstructGPT</a> to <code>davinci-next</code> with temp = 1/​BO = 10.</p>
              <blockquote>
                <p><strong>“The Book of Jobs”</strong></p>
                <p><strong>[One of the <a href="https://en.wikipedia.org/wiki/Wisdom_literature" data-link-icon="wikipedia" data-link-icon-type="svg">‘wisdom literature’</a> books of the tech world, it combines poetry with founding myths of Silicon Valley in a dialogue on design and big tech, and the <a href="https://en.wikipedia.org/wiki/Theodicy" data-link-icon="wikipedia" data-link-icon-type="svg">theodicy</a>: with all the developers in the world and good intent, whence comes the evil of badly designed software and hardware? In it, the Lord Steve Jobs is reviewing Apple computer sales and boasts of record profits from his devoted Users, when a subordinate, the Tempter (lit. ‘legume reckoner’, identified by most scholars as <a href="https://en.wikipedia.org/wiki/Tim_Cook" data-link-icon="wikipedia" data-link-icon-type="svg">Tim Cook</a>) claims that the User only cares about cheap devices, not good design, and if Jobs were to raise profit margins, they would faithlessly flee to inferior Microsoft Windows laptops &amp; Google Android smartphones. Jobs grants the Tempter permission to test his Users, inflicting on them such plagues as <a href="https://en.wikipedia.org/wiki/IPhone_7#Headphone_jack_controversy" data-link-icon="wikipedia" data-link-icon-type="svg">missing headphone jacks</a>⁠, MacBooks bricked by updates, invisible scrollbars, <a href="https://en.wikipedia.org/wiki/MacBook_Pro#Keyboard_reliability" data-link-icon="wikipedia" data-link-icon-type="svg">‘butterfly keyboards’</a> which break, PCs with antiquated GPUs, <a href="https://en.wikipedia.org/wiki/IPhone" data-link-icon="wikipedia" data-link-icon-type="svg">iPhones</a> which must be <a href="https://en.wikipedia.org/wiki/IPhone_4#Antenna" data-link-icon="wikipedia" data-link-icon-type="svg">held just-right</a>⁠, and chat apps incompatible with those of relatives. The User is left bankrupt and alone and with corrupt backups, and criticizes Jobs as abusive and sociopathic; hearing of this, his friends—<a href="https://en.wikipedia.org/wiki/Bruce_Tognazzini" data-link-icon="wikipedia" data-link-icon-type="svg">Tog</a> the Uxite, <a href="https://en.wikipedia.org/wiki/David_Pogue" data-link-icon="wikipedia" data-link-icon-type="svg">Pogue</a> the Timesian, and Siracusa the Shillite—reproach him, defending the ways of Apple, but he refutes them, and goes so far as to email <code>steve@apple.com</code> with his complaints. In the final third, Steve Jobs himself replies to the User, emphasizing Apple’s marketcap and his age and knowledge and power as Apple CEO.]</strong></p>
                <p><strong>Then Steve Jobs answered the User out of the whirlwind, and said:</strong></p>
                <p><strong>“Who is this that darkens <a href="https://en.wikipedia.org/wiki/Hacker_News" data-link-icon="wikipedia" data-link-icon-type="svg">Hacker News</a> with whines?</strong>
                </p>
                <p><strong>Where were you when I laid the foundations of Apple?</strong>
                </p>
                <p>Have you entered the storehouses of the <a href="https://en.wikipedia.org/wiki/MacBook" data-link-icon="wikipedia" data-link-icon-type="svg">MacBook</a>?
                </p>
                <p>Do you know the ordinances of the <a href="https://en.wikipedia.org/wiki/App_Store_(iOS/iPadOS)" data-link-icon="wikipedia" data-link-icon-type="svg">App Store</a>?
                </p>
                <p>Where is the way to the iPhone’s <a href="https://en.wikipedia.org/wiki/Home_screen" data-link-icon="wikipedia" data-link-icon-type="svg">home screen</a>?
                </p>
                <p>Have you understood the breadth of the iPhone?
                </p>
                <p>Who has put wisdom in the iPod,
                </p>
                <p>Can you bind the chains of the iPhone?
                </p>
                <p>Did you know that the iPhone’s home screen is good,
                </p>
                <p>Do you know the iPhone’s home screen?
                </p>
                <p>Yet the User hates knowledge
                </p>
              </blockquote>
              <p>This next completion pleases me because GPT-3 does not just continue God’s speech, but follows the original by ending it with a response from Job/​the User, who is cowed (“Behold, I am vile; what shall I answer thee? I will lay mine hand upon my mouth. / Once have I spoken; but I will not answer: yea, twice; but I will proceed no further.”), who is ignored by God, who then continues in the same vein:</p>
              <blockquote>
                <p><strong>…Where were you when I laid the foundations of Apple Computer Inc.?</strong>
                </p>
                <p>Or who enclosed the sea behind a screen,
                </p>
                <p>Have you visited the storehouses of more RAM
                </p>
                <p>Can you bind the cluster of servers
                </p>
                <p>And the User answered Steve Jobs and said:
                </p>
                <p>Then Steve Jobs spoke again to the User out of the whirlwind and said:
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>Surely you know, for you were born then,
                </p>
              </blockquote>
              <p>I had to look up <a href="https://en.wikipedia.org/wiki/Pegatron" data-link-icon="wikipedia" data-link-icon-type="svg">“Pegatron”</a> in this one—it sounds made-up, but is a real Apple contractor:</p>
              <blockquote>
                <p><strong>…</strong> Declare, if thou hast understanding.
                </p>
                <p>Have you not heard of the travails of <a href="https://en.wikipedia.org/wiki/Steve_Wozniak" data-link-icon="wikipedia" data-link-icon-type="svg">Steve Wozniak</a>⁠, and the terrors of the Los Altos <a href="https://en.wikipedia.org/wiki/Homebrew_Computer_Club" data-link-icon="wikipedia" data-link-icon-type="svg">Homebrew Club</a>?
                </p>
                <p>Shall a User chastise a mighty corporation? he shall regard developers;
                </p>
                <p>Will the User Condemn me, that thou mayest be righteous? and condemn my programmers, that they may be guiltless?
                </p>
                <p>Gird up now thy loins like a man; I will demand of thee, and declare thou unto me.
                </p>
                <p>Hast thou entered into the regions where the new Apple iPhone is made and manufactured? or hast thou seen the lines of the factories thereof?
                </p>
                <p>With whom took I counsel, and who instructed me, and taught me in the path of design, and showed me the way of genius?</p>
                <p>Hast thou alone seen the Apple iPhone face to face? is it not hidden from me also, and sealed up among my treasure?
                </p>
                <p>Canst thou give an answer to the Users? or with thy wisdom Users?
                </p>
                <p>Remember now, put me in remembrance: let us contend together: declare thou, that thou mayest be justified.
                </p>
                <p>Deck thyself now with majesty and excellency; and array thyself with glory and beauty.
                </p>
              </blockquote>
              <p>Some further finetuning of the prompt:</p>
              <blockquote>
                <p><strong>“Book of Jobs”</strong></p>
                <p><strong>[A ‘wisdom literature’ book from the startup tech world, it combines poetry with founding myths of Silicon Valley in a dialogue on design and big tech, and the theodicy: with all the developers in the world and good intent, whence comes the evil of badly designed software and hardware?**]</strong></p>
                <p><strong>Then Steve Jobs answered the User out of the whirlwind, and said:</strong></p>
                <p><strong>“Who is this that darkens Hacker News with whines?</strong>
                </p>
                <p><strong>Where wast thou when I laid the foundations of Apple Computer?</strong>
                </p>
                <p>Now prepare yourself like a man;
                </p>
                <p>The User says to the Almighty CEO,
                </p>
              </blockquote>
            </section>
          </section>
          <section id="devils-dictionary-of-science">
            <h2><a href="#devils-dictionary-of-science" title="Link to section: § &#39;Devil’s Dictionary Of Science&#39;">Devil’s Dictionary Of Science</a></h2>
            <div>
              <p><a href="https://nitter.moomoo.me/balzarot/status/1278213982663426048" data-link-icon="twitter" data-link-icon-type="svg">Davide Balzarotti</a> snarked:</p>
              <blockquote>
                <p><em>Rebuttal</em> [noun] A form of ironic composition in 500 words popular among researchers, in which rageful authors first thank their reviewers for “their time and insightful comments” and then mock Reviewer 2 for his non-sense in a sequence of fakely polite points.</p>
              </blockquote>
              <p>Naturally, GPT-3 is able to write not just rebuttals, but satirical dictionary definitions along the lines of <a href="https://en.wikipedia.org/wiki/Ambrose_Bierce" data-link-icon="wikipedia" data-link-icon-type="svg">Ambrose Bierce’s</a> <a href="https://en.wikipedia.org/wiki/The_Devil&#39;s_Dictionary" data-link-icon="wikipedia" data-link-icon-type="svg"><em>The Devil’s Dictionary</em></a>—indeed, GPT-3 is remarkably good at it. (“What a Dictionary a GPT-3’s curator might write on the clumsy, wasteful blundering, low and horribly cruel works of <em>Nature</em>!”)</p>
            </div>
            <p>First, generating rebuttals with a slightly extended prompt to cue GPT-3 into writing the rebuttal itself rather than additional dictionary definitions:</p>
            <blockquote>
              <p><strong><em>Rebuttal</em> [noun] A form of ironic composition in 500 words popular among researchers, in which rageful authors first thank their reviewers for “their time and insightful comments” and then mock Reviewer 2 for his non-sense in a sequence of fakely polite points.</strong></p>
              <p><strong>“We thank reviewers 1, 2, and 3, for their time and insightful comments, which we have spent many hours reading over. But in reply we note:</strong></p>
              <p><strong>1.</strong> RE2: Although RE2 starts with the reasonable comment that ‘Currently our control loops are pure control’, he goes on to provide only minor points that suggest that we are doing something slightly different. In fact, there is much more to be said about this, and the real difference will be a full control protocol on all vehicles.</p>
              <p>We thus thank RE2 for taking the time to provide such minor points, but note that he has done almost no actual research and has spent zero hours on the review.</p>
              <p>RE2: “There is little or no literature on decentralized solutions to problems such as traffic optimization.”</p>
              <p>RESPONSE: We thank RE2 for the minor point that there is “little or no literature on decentralized solutions.” But we note that decentralized solutions are particularly difficult, and so this fact might suggest that they are not being considered or might be infeasible. Or it might suggest that they are being researched intensely. In this case it turns out that, unlike RE2, several of us have in fact thought deeply on these issues, and so we feel we are a valuable research group despite not being affiliated with the Department of Traffic Control or any other centralized, government-backed, research institution.</p>
              <p>RE2: “The DGM solution is also [something else]. To our knowledge, no work has been done on DGM control loops, or any control loop at all, for these kinds of traffic patterns.”</p>
              <p>RESPONSE: We thank RE2 for the comment that the solution we have proposed is [something else]. As he is not familiar with such traffic patterns and the other reviewers in their reviews have been unimpressed, we must wonder how he could be familiar with control loops that might or might not exist and would or would not have been previously performed.</p>
              <p>RE2: “The number of cars involved is approximately 100, which is a small number. By contrast, say, the NYC Metropolitan Area, [roughly 22 million people] has a traffic network of nearly 3 million miles.”</p>
              <p>RESPONSE: We thank RE2 for the comment that our problem is “small.” This is the first time this has been suggested to us. However, we feel that RE2 may be failing to understand a key component of the research protocol we have proposed. This problem is designed to be decentralized in a city with no more than a couple million people (or a town or a small city or maybe even a village) and so it is expected to have only a couple thousand drivers and not one single, central traffic control unit that could possibly control 3 million miles. We thank RE2 for his insight on this, but also feel that he has not gone to great enough lengths to consider the nature of such a problem in such a situation.</p>
              <p>RE2: “Experimental data [will likely] need to be collected and processed. Currently the authors provide no experimental plan for any of their research [and so] we feel the experimental results to be lacking.”</p>
              <p>RESPONSE: We thank RE2 for the suggestion that we have not provided an experimental plan. As our paper is only intended to propose a protocol, we understand that experimental plans should come later. In fact, we will be presenting our plans to colleagues later this year in a peer-reviewed symposium at State College, Pennsylvania. That being said, we find it strange that RE2 might have any knowledge of such plans or of such conferences. As we are listed as authors, we feel that RE2 might have been able to contact us if he were interested in our work, but he has not and thus we do not know whether he has even heard of our research program.</p>
              <p>RE2: “The distributed system is better able to operate in an autonomous fashion, i.e. no central controller is required. With the possibility of a large number of drivers, autonomous operation is challenging, especially if vehicles may communicate in unanticipated ways [or they might not]. As suggested by [the very important paper], the [not really relevant] control loop should be a passive mechanism whereby each vehicle adjusts its control mechanism through a feed-forward signal.”</p>
              <p>RESPONSE: We thank RE2 for the comment that autonomous operation is challenging. But again, we do not understand how he could know this. In fact, he has spent almost no time on the research, spent zero hours on the review, and spent zero research dollars, and it is hard for us to understand how he could know this better than we could. In fact, we have spent over a decade working on this project, with many hours of detailed thought and research, and we are still not sure that autonomous operation is a good idea or how we could get it to work in a decentralized and random traffic environment.</p>
              <p>RE2: “If we all observe that several vehicles are slowing down, it might be a sign of a possible accident ahead. If we are all in autonomous mode, a vehicle near the accident would brake, possibly causing a pileup.”</p>
              <p>RESPONSE: We thank RE2 for the comment that several vehicles slowing down might be a sign of an accident. He is correct that this might be a sign of an accident. But he might have considered the possibility that several vehicles slowing down might be a sign of, say, the vehicle he is in slowing down or that he is doing something else such as texting a message to someone else, or simply that he might have a flat tire. We thank RE2 for this, but we have also considered these possibilities and feel that the latter is far more probable, so we suggest a second solution to this problem that would require each vehicle to text all other vehicles on the road with its status, something like “Hey I’m just sitting here, so if anyone wanted to come fly up the back of my car and then over the top of my windshield and down into my passenger seat to grab my phone and then fly back, well by all means, be my guest.” In this way, a vehicle with a flat tire would have no delay in informing all others of the presence of a flat tire and in getting help with fixing it. Also, an accident would be similarly well-announced, and we think this solution is particularly important given the new protocol we are designing to allow for more efficient traffic control.</p>
              <p>RE2: “Distributed control systems [are usually] an assortment of semi-autonomous vehicles moving in close proximity to one another. Such systems [are generally] non-autonomous, requiring control by a central controller that can be updated to reflect the most recent situation.”</p>
              <p>RESPONSE: We thank RE2 for the comment that distributed control systems are usually non-autonomous, requiring control by a central controller. But RE2 has spent zero time on the research and we feel that he has failed to appreciate that the problem we have designed is designed to be a decentralized, autonomous, distributed traffic control problem. We again feel that this is only to his great detriment.</p>
              <p>RE2: “Such cars [on our street] will become very complex, and we feel they will be more difficult to design and build.”</p>
              <p>RESPONSE: We thank RE2 for the comment that our cars will become very complex. We feel that this will be of no issue, as we have discussed at great length before, and we think that a small group of university professors with a good design team should be able to provide cars that work in this capacity in the real world.</p>
            </blockquote>
            <p>The second completion finished quickly and to my surprise, began writing further definitions:</p>
            <blockquote>
              <p><strong>…1.</strong> We did not describe any ‘simple algorithm’ for the repeated-measures ANOVA in the abstract, the discussion, or in the Methods. Such a description of our protocol would be incoherent and probably invalid anyway.</p>
              <ol start="2" type="1">
                <li>
                  <p>We know of no ‘more appropriate analysis’ for the mean and <a href="https://en.wikipedia.org/wiki/Variance" data-link-icon="wikipedia" data-link-icon-type="svg">variance</a> of the proportions than to compute the estimates of the variances directly. Since we have done this correctly, it makes no sense to proceed further as suggested by Reviewer 2.</p>
                </li>
                <li>
                  <p>It does not make sense to carry out the tests for homogeneity that Reviewer 2 suggest we do. Indeed, we never said we should carry out such tests. Our precise point was that we should not compute the standard errors of the treatment means, and Reviewer 2 seems to think that we have argued for the opposite point, which we have not.</p>
                </li>
                <li>
                  <p>Again, Reviewer 2 apparently did not read the Methods section carefully and was confused by some sections that he or she did read. A better review would have pointed this out.</p>
                </li>
                <li>
                  <p>We note the Reviewer 2’s overly generous suggestion that we write a ‘special’ rebuttal. At the very least, we assume that Reviewer 2 knows what a rebuttal is, and we would therefore argue for a critical comment instead.”</p>
                </li>
              </ol>
              <p>Noise</p>
              <p>Noise is anything you don’t want or need, a random or distracting factor in a system, process, or set of data that obscures or corrupts information. It is also the sound produced by old time radios when no transmission is present, which is roughly what some fields of science would have to resemble if some researchers had their way.</p>
              <p>“New research”</p>
              <p>A form of scientific publishing that includes, among others, a report on previously performed studies; a report of a new statistical test or set of statistical tests that were performed by the author of the “study” without the help of other scientists; a report of a small set of samples, not necessarily from a population or process, which could, but have not been collected in a systematic way and may not even be accurately described, in the guise of which conclusions are drawn about the world; and a report of the result of performing an unvalidated computer simulation of something on a small set of data without stating whether such an attempt was successful or not.</p>
              <p>“Nice to see”</p>
              <p>A widely and unofficially accepted phrase in scientific circles meaning: “It is better to live in an imaginary world created by data manipulation and overly optimistic predictions than in the world that exists in reality”.</p>
              <p>“Researcher”</p>
              <p>[noun] A form of modern industry based on a mix of small molecules of grant money and arbitrary experimental methods.</p>
              <p>“Review”</p>
              <p>A form of scientific communication in which the writer (a student) usually does not give enough information for the reader to judge the merit of the arguments presented in the paper; the writer also does not respond to or comment on the suggestions of the reviewers (sometimes known as referee or editor). This form of communication is popular among students who wish to leave the burden of writing the paper on other people and are willing to pay the price of rejection.</p>
              <p>“Second chance”</p>
              <p>A word that scientists regularly use when referencing a journal, conference, or grant application. While there is always an implicit threat that you can get rejected, or be accepted with changes, as a second choice to another party, some articles, papers, grants, and grant proposals receive multiple, even unlimited, chances of being considered. As a result, the word may be interpreted as a bribe or a threat depending on whether you are the writer or a reviewer.</p>
              <p>“Reviewer”</p>
              <p>A participant in the review of a grant, paper, or grant proposal. In spite of being in a poor position to assess the merits of a proposal, reviewer tends to demand that authors submit their data for statistical analysis and back their results with it, which the reviewer usually does not. Reviewer usually requires that the author cite his or her own work to prove that he or she is worth reviewing. It is also assumed that the reviewer can detect the slightest amount of bias in any paper, which the reviewer also assumes has not been corrected for.</p>
              <p>“Revise and resubmit”</p>
              <p>A sentence that authors of submitted papers tend to hear from reviewers, editors, and the like. A form of scientific communication that begins with submission of a paper (for which the reviewers may or may not have been consulted), proceeds through review and criticism of the paper (typically by one or two reviewers, not a full panel of reviewers, unless the paper is deemed to be of supreme importance), and, if it is “revised and resubmitted”, usually ends with another critique of the new paper by one or more of the reviewers, and another round of “revise and resubmit”, ad infinitum. In the absence of fraud, retractions, litigation, mass incompetence, or insanity, these “revise and resubmit” cycles may be taken as the only method of scientific communication.</p>
              <p>“Retracted”</p>
              <p>The most frequently used word for a journal article that was subsequently found to be based on fraudulent data.</p>
              <p>“Rigor”</p>
              <p>Something for scientists to aspire to, a state of mind that would not be required if scientists could be trusted to do their job.</p>
              <p>“Science”</p>
              <p>A complex web of data, opinions, lies, and errors, now considered the most important (because most expensive) technology in the modern society. To remind you of this, you will frequently see scientists and editors use the word, claim to do something for the sake of science, or see it used as an adjective.</p>
              <p>“Standards”</p>
              <p>Standards for data and their interpretation vary depending on the country or scientific discipline.</p>
              <p>“Study”</p>
              <p>[noun] A form of communication in the scientific literature that authors or publishers may use to make claims that are not warranted. Examples include, but are not limited to: The New study of clinical trials that show…, As Dr. X reports in his new study of observational data…, The new study demonstrates that…, Study shows that, In a study published last week…, The study suggests that, New findings show that, A study of data…</p>
              <p>“Study X”</p>
              <p>A widely used nickname for studies with profound, not to say earthshaking, conclusions that may or may not be of any relevance to anyone who reads about them.</p>
              <p>“Technology”</p>
              <p>Any bit of software, analysis, model, or other methodology that scientists can be expected to claim as their own, for which they can sell you consulting or collaboration fees.</p>
              <p>“Tool”</p>
              <p>[noun] Any piece of software, analysis, model, or other methodology that scientists can be expected to claim as their own, for which they can sell you consulting or collaboration fees.</p>
              <p>“The topic of the paper”</p>
              <p>A wide-ranging category of things or ideas that may not have been relevant when the paper was written, but which the authors believe the paper should be about. Often, the topic is too broad or a non-topic, but is occasionally useful in order to generate support for yet another set of related papers, conferences, seminars, webinars, and so forth, which in turn are used to generate more data for “new findings”, which, after they are manipulated enough, may end up being published and generating yet more data to support a “re-review” of the original paper or other things.</p>
              <p>“Unified theory”</p>
              <p>An imaginary set of ideas that scientists dream of having so that they could achieve the same results by slightly varying the methodology, thus preserving their original publications.</p>
              <p>“Validation”</p>
              <p>The process by which scientists decide that something they built in their spare time works in the way they expect. The validation process requires that you do not attempt to validate it before you publish it and require that the users validate it after.</p>
              <p>“Value”</p>
              <p>In its most natural form, science is the ability to study data and ideas; the amount of respect and publicity that a scientist can expect; how well his or her paper was published; the likelihood that a scientist will receive a grant; whether a scientist will be taken seriously; the likelihood that an idea will be accepted; how much grant money will be available to purchase new toys for this scientist; whether something is scientific; the likelihood that the papers of this scientist will be read; the status of this scientist; the likelihood that a journal, conference, or society will be able to continue funding or exist; how much and how fast this scientist’s career will progress; the “value” that this scientist brings to the modern society; and the likelihood that this scientist will continue to be funded. While the actual values for science are hard to pin down, what people believe them to be is another matter. For example, while some believe that science is important, even if it doesn’t have any concrete value, others believe that it’s the ability to continue reading abstracts or studying it that has value. Still others believe that what matters is how much grant money is currently available, even if the researchers are actually doing something that nobody knows about. Some believe that the fact that someone is in a position to give a grant, or has written a paper, or, even better, has published that paper, actually means that it has a lot of value. Others believe that what matters is not what you publish, but how you publish it and that all research must be published in the most prestigious journal, even if you are not that much of a researcher. The reality is that no one really knows what science is, or what value it has; however, we all know that if we deny that science has value, we might be able to get rid of it and use this space for something else.</p>
              <p>“Validation step”</p>
              <p>Another name for a random setting of a parameter of a model, simulation, or algorithm.</p>
              <p>“Verify”</p>
              <p>A step in a procedure that, if performed properly, will tell you whether the previous step, or anything in the last two years of your work, was performed properly.</p>
            </blockquote>
            <p>I had to keep continuing it for more definitions:</p>
            <blockquote>
              <p><strong>…</strong>”Unified theory”</p>
              <p>An imaginary set of ideas that scientists dream of having so that they could achieve the same results by slightly varying the methodology, thus preserving their original publications.</p>
              <p>“The usual procedure”</p>
              <p>A method of study, validation, and public communication which, if followed, may lead you to be able to claim that your work may offer a decent return on investment.</p>
              <p>“Wide range”</p>
              <p>When used in the context of scientific results and models, it usually means that the results and models are not likely to be used for their intended purpose, if they have one.</p>
              <p>“Wider range”</p>
              <p>When used in the context of scientific results and models, it usually means that the results and models are likely to be used for their intended purpose, if they have one.</p>
              <p>“Writeup”</p>
              <p>A form of scientific communication in which the author states the information he or she wanted the readers to extract from the paper while making it as difficult as possible for them to find it.</p>
              <p>“Write-up”</p>
              <p>1. [noun] A style of writing used in academic papers to dress up data and results. 2. [verb] The act of presenting data in a way that makes it look less impressive and more complex than it really is. 3. [verb] The act of writing a manuscript that is impossible to follow. 4. [verb] An activity that is often performed by the most blatant or careless users of the English language, all for the sake of showing off their erudition.</p>
              <p>“Writer’s block”</p>
              <p>A common affliction among students, arising from various causes, such as: their desire to sell their ideas for a profit, their inability to realize this desire, the fact that their ideas are not selling and will not be bought, and the delusion that most of the wealth and fame in the world would be theirs if they would spend enough years doing science.</p>
            </blockquote>
            <p>I tried tailoring the prompt a little more precisely to generating definitions:</p>
            <div>
              <blockquote>
                <p><strong>A Devil’s Dictionary of Modern Science &amp; Academia
                </strong></p>
                <p>“<strong>Researcher” [noun]</strong></p>
                <p><strong>A form of modern industry based on a mix of small molecules of grant money and arbitrary experimental methods.</strong></p>
                <p><strong>“Noise” [noun]</strong></p>
                <p><strong>Noise is anything you don’t want or need, a random or distracting factor in a system, process, or set of data that obscures or corrupts information. It is also the sound produced by old time radios when no transmission is present, which is roughly what some fields of science would have to resemble if some researchers had their way.</strong></p>
                <p><strong>“Review” [noun]</strong></p>
                <p><strong>A form of scientific communication in which the writer (a student) usually does not give enough information for the reader to judge the merit of the arguments presented in the paper; the writer also does not respond to or comment on the suggestions of the reviewers (sometimes known as referee or editor). This form of communication is popular among students who wish to leave the burden of writing the paper on other people and are willing to pay the price of rejection.</strong></p>
                <p><strong>“Reviewer” [noun]</strong></p>
                <p><strong>A participant in the review of a grant, paper, or grant proposal. In spite of being in a poor position to assess the merits of a proposal, reviewer tends to demand that authors submit their data for statistical analysis and back their results with it, which the reviewer usually does not. Reviewer usually requires that the author cite his or her own work to prove that he or she is worth reviewing. It is also assumed that the reviewer can detect the slightest amount of bias in any paper, which the reviewer also assumes has not been corrected for.</strong></p>
                <p><strong>“Science” [noun]</strong></p>
                <p><strong>A complex web of data, opinions, lies, and errors, now considered the most important (because most expensive) technology in the modern society. To remind you of this, you will frequently see scientists and editors use the word, claim to do something for the sake of science, or see it used as an adjective.</strong></p>
                <p><strong>“Revise and resubmit” [verb]</strong></p>
                <p><strong>A sentence that authors of submitted papers tend to hear from reviewers, editors, and the like. A form of scientific communication that begins with submission of a paper (for which the reviewers may or may not have been consulted), proceeds through review and criticism of the paper (typically by one or two reviewers, not a full panel of reviewers, unless the paper is deemed to be of supreme importance), and, if it is “revised and resubmitted”, usually ends with another critique of the new paper by one or more of the reviewers, and another round of “revise and resubmit”, ad infinitum. In the absence of fraud, retractions, litigation, mass incompetence, or insanity, these “revise and resubmit” cycles may be taken as the only method of scientific communication.</strong></p>
                <p><strong>“Rigor” [adj]</strong></p>
                <p><strong>Something for scientists to aspire to, a state of mind that would not be required if scientists could be trusted to do their job.</strong></p>
              </blockquote>
            </div>
            <p>yielding more definitions, starting with a single letter to avoid running off into the “Z”es and truncating completions (since GPT-3 will proceed A–Z, starting from the last definition, as is logical, unless otherwise prompted):</p>
            <blockquote>
              <p>“<strong>A</strong> role for…” [phrase]</p>
              <p>A frequent phrase found in submitted and published papers; it often indicates that the authors have nothing to say about the topic of their paper. In its more emphatic form, “A role for…” usually indicates a struggle by the authors to take a side on an issue, after a lengthy attempt to be both non-committal and a supporting party to all sides, as often happens in “molecular and cellular” or “basic and translational” research.</p>
              <p>“Reviewer” [noun]</p>
              <p>A participant in the review of a grant, paper, or grant proposal. In spite of being in a poor position to assess the merits of a proposal, reviewer tends to demand that authors submit their data for statistical analysis and back their results with it, which the reviewer usually does not. Reviewer usually requires that the author cite his or her own work to prove that he or she is worth reviewing. It is also assumed that the reviewer can detect the slightest amount of bias in any paper, which the reviewer also assumes has not been corrected for.</p>
              <p>“Resident”</p>
              <p>An adjunct faculty member that administers teaching assistantships, sits on departmental committees, and seems to live on the departmental lounge. A kind of “honorary” faculty member that is not yet an actual faculty.</p>
              <p>“The Action Plan for Diversity” [noun]</p>
              <p>A vague document circulated on campuses in recent years to remind the faculty to be aware of discrimination and presumably combat it in the departments. It is often taken as a generalized insult of students, who tend to see no way around this plan without committing academic fraud.</p>
            </blockquote>
            <blockquote>
              <p><strong>“B</strong>ias” [noun]</p>
              <p>A form of mental abuse by which a scientist turns an experiment in such a way that the results prove his or her point of view or grants get funded.</p>
              <p>“Behavioral Studies” [noun]</p>
              <p>Sometimes mistaken as a field of science. The terms Behavioral Studies are actually a combination of disciplines that usually includes business economics, sociology, and engineering.</p>
              <p>“Humanities” [noun]</p>
              <p>A broad term used to refer to any academic discipline (such as English, philosophy, religious studies) in which science has been forgotten. To avoid confusion between the sciences and humanities, remember that scientists use hard tools and humanities their soft tools when examining issues and trying to persuade audiences. In other words, both scientists and humanities routinely lie (it’s part of their job), but they do it in different ways.</p>
              <p>“Literature Review” [noun]</p>
              <p>A paper, commonly published in a peer-reviewed journal, that advertises the author’s skills and accomplishments without using any evidence. The reader is left to believe the work must have been done, since it has been advertised and is “published”.</p>
              <p>“The Literature” [noun]</p>
              <p>A name given to other people’s published papers, referred to by scientists without actually reading them.</p>
              <p>“Track Record” [noun]</p>
              <p>When scientists use this term they refer to the research done by someone else (usually a student) in order to avoid having to do research.</p>
              <p>“Discussion” [noun]</p>
              <p>The part of a scientific paper where the writer summarizes the points made in the body of the paper and acknowledges certain acts and circumstances that helped with the completion of the study. The author should carefully state in this section whether or not the study reported in the paper could have been done otherwise and should clearly state that if he or she had the opportunity, he or she would definitely repeat the study again.</p>
              <p>“Role model” [noun]</p>
              <p>A person who can be an inspiration to others because of his or her academic and personal achievements in general, or because they conduct academic misconduct on an industrial scale. The most important qualification for becoming a role model for others is to be judged sufficiently inept that no one will question anything you do, or to become the successor of someone who has already acted as a role model.</p>
              <p>“Faculty” [noun]</p>
              <p>Used loosely by scientists to mean any group of people with advanced degrees. Typically used when you have done something stupid and want to inform others that it wasn’t you who did it, but rather those other crazy people over there who won’t put their titles after their names.</p>
              <p>“Significant” [adjective]</p>
              <p>A word which defines the claim made in a paper (no matter how trivial). It is a critical term in hypothesis testing, in that the sign of the test statistic must be evaluated and compared to a critical value in order to support or reject a hypothesis. Significance is designated by two-tailed tests when the direction of the effect is not specified. If a two-tailed significance test is used, the “p”-value needs to be halved, due to “double-counting”. In any event, “significant” remains of critical importance in any hypothesis test, since the whole idea is to come up with a “sign”, i.e. plus or minus (“+/​-”), or a wishy-washy “statistically significant”, without actually saying anything substantive about the matter.</p>
              <p>“Random assignment” [verb]</p>
              <p>A way to arrange study groups so the resulting data is conclusive and unbiased. Typically used in the social sciences by graduate students who refer to it while explaining why they broke down the groups this way.</p>
              <p>“Cite” [verb]</p>
              <p>To refer to a paper or journal as a source for support, when in reality you have never read it and did not understand it.</p>
              <p>“Bibliography” [noun]</p>
              <p>A list of citations of references used to support a particular point in a paper. A list that somehow includes several references that didn’t actually end up in the final bibliography of the paper.</p>
              <p>“Let’s stipulate” [phrase]</p>
              <p>A preface to any statement, that means I know I am going to piss you off with my statement, but since you are such a nice person, I am first going to give you a chance to let me piss you off without responding.</p>
              <p>“How is that different from…?” [phrase]</p>
              <p>A question posed by authors when they have been accused of stealing ideas. They are attempting to show that they have never read the papers that contain the ideas they stole, nor even looked at the ideas in any detail, and are just fishing for compliments.</p>
            </blockquote>
            <blockquote>
              <p><strong>“C</strong>itation” [noun]</p>
              <p>An elaborate form of review, typically found in order to impress those who do not know better, in the sciences, humanities, or even the social sciences. A citation typically involves misreading or misquoting an author and talking about something he or she did not write about or intend. It is assumed that you need to carry out a large number of citations in order to have science, probably because at least one citation can be found for any fact, event, or incident in history; the effectiveness of this fact is increased if the author is long dead and can not disprove it.</p>
              <p>“Physics” [noun]</p>
              <p>A science. Physicists like to think their science is something you can do without a plan, something that relies on intuition and inspiration, something that is “deep”.</p>
              <p>“Scientific revolution” [noun]
              </p>
              <p>See also: Einstein’s Method, Pauling’s Method, Bohr’s Method, Poincaré’s Method</p>
              <p>“Scientific method” [noun]
              </p>
              <p>“Scientist” [noun]
              </p>
              <p>“Revise and resubmit” [verb]
              </p>
              <p>“Shared data” [noun]
              </p>
              <p>“Shared future” [noun]
              </p>
              <p>“Sharing of data” [noun]
              </p>
              <p>“Statistical error” [noun]
              </p>
            </blockquote>
            <blockquote>
              <p><strong>“D</strong>umb” [adj]</p>
              <p>The amount of information scientists can be bothered to remember before they either dismiss it as incorrect data, or ignore it completely.</p>
              <p>“Surprised” [verb]</p>
              <p>A scientific system in which one is “surprised” by the result of an experiment, does not necessarily mean that they believe it is wrong, but usually means they are not sure what to say about it.</p>
              <p>“Oppression” [noun]</p>
              <p>A state of mind in which laboratories underfinanced by the government can be considered financially secure, or any noise that scientists don’t like or want to hear.</p>
              <p>“Reviewer” [noun]
              </p>
              <p>“Theoretical interpretation” [noun]
              </p>
              <p>“Screwed” [adj]
              </p>
              <p>“Reviewer” [noun]
              </p>
              <p>“Argument” [noun]
              </p>
              <p>“Fail” [verb]
              </p>
            </blockquote>
            <blockquote>
              <p><strong>“String theory” [noun]</strong></p>
              <p>A theory in physics which is about 99% false, but people seem to like it, so they keep funding it. A good example of the difference between a “scientific” concept and a “semi-scientific” concept, or even an unscientific concept.</p>
              <p>“Theory” [noun]</p>
              <p>An unproved hypothesis; a conjecture that might or might not be right; use of this term forms the basis of the common game of turning the scientific method on its head. The most popular types of theories are the theory of intelligence (or unintelligence), time theory, quantum theory (of course, even more primitive), and anything that is implied by string theory.</p>
              <p>“Research” [verb]</p>
              <p>Something done with the money provided by a body or person other than the principal researcher(s), generally private individuals and businesses hoping to get something out of it. Traditionally involves experimentation.</p>
              <p>“Publish or perish” [phrase]
              </p>
            </blockquote>
            <blockquote>
              <p><strong>“String theory” [noun]</strong></p>
              <p>A set of mathematical formulas that can be used to predict which particles exist (or do not exist).</p>
              <p>“Science group” [noun]</p>
              <p>A collection of researchers in various fields, with goals including one or more of attracting funding, specifically young, naive, and gullible grad student, inbreeding graduate students in order to maintain affiliation with the group, writing and publishing papers specific to the group’s activity in order to continue the goal of attracting funding and inbreeding graduate students, calling in favors from University administrators in order to preserve and continue the goal of attracting funding and inbreeding graduate students and sacrificing sheep and lambs in order to obtain funding and inbreed graduate students.</p>
              <p>“Group leader” [noun]</p>
              <p>Person whose function appears to consist mainly of ordering subordinates (graduate students) to fill out grant applications, enter grant data into a spread sheet, teach undergraduates laboratory techniques not commonly taught elsewhere, submit papers to a journal any time before the end of 2002 (record will be broken), devote time to discovering something useful or interesting about science outside of the parameters determined by the group leader, ever working on problems specified by the group leader if they cannot be worked on inside the parameters determined by the group leader. All of these activities are considered necessary to support the “science groups” goal of attracting and inbreeding graduate students, and provide opportunity for satisfactory career development for the group leader.</p>
              <p>“Special material” [noun]</p>
              <p>Something that only has to be done once, where other people will then do it again every time a paper requires it. Or conversely, something that cannot be repeated because of its poor reproducibility (special uncertainty, uncertainty special).</p>
              <p>“Special relativity” [noun]</p>
              <p><span>Advanced idea described in <span><span>Einstein’s</span><span>1905</span></span> papers on the subject, which states that, from a suitable point of view,</span> “E = m,” where “E” is energy, “m” is mass, and “a” is “Profundity.” Except for arcane fields such as string theory, special relativity is the most widely accepted of Einstein’s work. Naturally, papers based on special relativity are the most likely to receive “special material”.</p>
              <p>“Speculation” [noun]
              </p>
              <p>“Professor” [noun]</p>
              <p>The highest academic rank available, a level usually associated with Freedom of Speech. It may take 35 years of service before the University bestows that title.</p>
              <p>“Publication Credibility” [noun]
              </p>
            </blockquote>
            <p>Cycling through the prompt as length permitted:</p>
            <blockquote>
              <p><strong>…</strong> “Placebo” [noun]</p>
              <p>An inert substance used to placate a patient or control group. It is largely composed of sugar.</p>
              <p>“Et al.” [proper noun]</p>
              <p>Et al. is an abbreviation of the Latin phrase et alii, meaning “and other people”. In the context of science, it is typically used to shorten the name of a list of authors whose name and affiliation would otherwise need to be written out in full. It is used when the number of authors is small enough that the abbreviation would not render the reference too obtuse and cumbersome, while at the same time remaining large enough that there are at least two other people involved in the study (and hopefully a control group).</p>
              <p>“Referee” [noun]</p>
              <p>A critical reader, particularly one with the power to reject your submission. In the absence of more efficient forms of communication, peer review depends on these people. See “Reviewer”.</p>
              <p>“Shared values” [noun]</p>
              <p>A set of principles, beliefs, theories, methods, and operational definitions that all scientists share and use. Never spoken of aloud in public.</p>
              <p>“Communication” [verb]
              </p>
              <p>“Panel” [noun]</p>
              <p>A group of individuals who meet at some given time to discuss something. A more bureaucratic version of peer review for larger groups of individuals or committee members who have already established their worth in research.</p>
              <p>“Data” [noun]
              </p>
              <p>“Fraud” [noun]
              </p>
              <p>“Legitimation” [verb]
              </p>
              <p>“Retraction” [verb]
              </p>
              <p>“Lies” [noun]
              </p>
              <p>“Scientists” [plural noun]
              </p>
              <p>“Health” [noun]
              </p>
              <p>“Clinical research” [noun]
              </p>
              <p>“Funding source” [noun]
              </p>
              <p>“Authorship” [noun]
              </p>
              <p>“Journal Impact Factor” [noun]</p>
              <p>According to some, it is a value that corresponds to the average number of citations of articles published in a given journal, if the interpretation is right. Otherwise, it is a completely arbitrary number, computed from the number of times articles published in a given journal in the last two years were cited by other articles published in other journals, the assumption being that every paper published in a scientific journal must be essential to someone else’s research, or at least that that person would like to be able to cite it. The difficulty with this assumption is that the average time from submission to publication of an article is now approximately 12 months, and the current record stands at three years for Biochem. J. Proc. (2000). This means that three times as many papers have to be published every year as are actually written, with more and more papers being submitted and rejected every year (the reality is even worse, but we don’t have numbers), and with different standards applying to different journals, which are becoming increasingly specialized. All of these “facts” considered, the best any researcher can say about the Impact Factor of a given journal is: “I know it when I see it”. Note also: This formula can produce values up to 4 for an article appearing in a journal containing only that article, so one “article” can receive as many “citations” as a journal.</p>
              <p>“Sterile” [adj]</p>
              <p>Said of that which is dry and lifeless; devoid of humor; also, said of scientific writing and methods when germane to the matter at hand.</p>
              <p>“Grant” [noun]</p>
              <p>A form of scientific communication involving a large piece of paper, on which is written an enormous sum of money. The grant-writer then sends his or her (because they are almost always written by men) proposal into a maze of office politics, legislative/​funding agency reshuffling, and intellectual and interpersonal competition. The prospect of acquiring a grant makes up for the fact that grant-writers receive no practical training in writing grants, have no idea what kind of “research” they will have to conduct with it, and generally start research as soon as they are finished writing the grant (if they ever are). To be on the safe side, please note: One does not have to win a grant to “win a grant”; one must simply apply for a grant. Just do it.</p>
              <p>“Peer review” [noun]</p>
              <p>A system of literary communication that consists of first submitting a manuscript to the editor, who (if he has not been fired) usually asks at least one reviewer to read the manuscript. If there is any doubt about whether the editor has lost his mind, or whether the manuscript has some merit, then the reviewers and editor sit down over coffee and discuss the manuscript (in conference), which is described as “peer review”. This form of communication does not provide either editors or authors with much feedback about their manuscript, but it makes reviewers feel important.</p>
              <p>“Precise” [adj]</p>
              <p>Said of everything except experimental results; it also means correct, appropriate, or in accordance with the relevant standards. But when the experimental results do not agree with the model of the hypothesis, the phenomenon being studied becomes “precise”, and one “cannot make predictions about precise experiments”. Please keep in mind that most theories are meant to be general in nature, so they must also be imprecise in their predictions. If they could predict with precision, they would not be general, but they would still be interesting.</p>
              <p>“Theory” [noun]</p>
              <p>A scientific explanation of something, which people use only in situations where they think explaining things is more important than discovering things. Otherwise, scientific theories are always meant to be true, since they are believed by people with education; to a scientist, a theory is true; to a person with a higher IQ, a theory is a fact; and to a citizen with a lower IQ, a theory is an opinion.</p>
              <p>“Significance” [noun]</p>
              <p>Significance refers to whether a result of a study is somehow important. If a significant result is found, then the problem that the researchers have studied is of real-world importance. If the results aren’t significant, the problem is not important; therefore, more money and time can be wasted by pursuing it. (E.g. “This study showed that chocolate makes rats temporarily hyperactive, so we will look for ways to inhibit this reaction in humans. Thank you for your support.”)</p>
              <p>“ResearchGate” [noun]</p>
              <p>A website of questionable purpose that allows scientists to upload their “results”, resulting in the same sort of information clutter and bad or incorrect data as any other social media network.</p>
              <p>“Rigor mortis” [noun]</p>
              <p>A state of rigor in which scientists do not think outside the paradigm, no matter how much it does not work; rigor mortis is characterized by the desire of researchers to prove their prior research correct.</p>
              <p>“Important” [noun]</p>
              <p>(British English: “Important”) [adj]</p>
              <p>Whether something is important to you personally or not, it is important in a statistical or numerical sense to someone. Therefore, it is always a useful adjective to add to any scientific term, especially when one does not have a better verb.</p>
              <p>“ResearchGate” [noun]</p>
              <p>A website of questionable purpose that allows scientists to upload their “results”, resulting in the same sort of information clutter and bad or incorrect data as any other social media network.</p>
              <p>“Rigor mortis” [noun]</p>
              <p>A state of rigor in which scientists do not think outside the paradigm, no matter how much it does not work; rigor mortis is characterized by the desire of researchers to prove their prior research correct.</p>
              <p>“Rigorosity” [noun]</p>
              <p>Another word for rigor or rigor mortis.</p>
              <p>“Rigorousness” [noun]</p>
              <p>Rigor’s nonchalant brother.</p>
              <p>“Robustness” [noun]</p>
              <p>This noun means “resistance to breakage”. This is different from the verb, “robustify”, which is an attempt to make a broken thing more resistant to breakage. When the two words are not confused, robustness is deemed by scientists to be a good thing, even though a greater robustness may mean a higher ability to break.</p>
              <p>“Science” [noun]</p>
              <ol type="1">
                <li>A specialized activity performed by a set of machines that some people call “scientists”, which is practiced all day long and in all waking hours, in labs and out of labs.</li>
                <li>What scientists do with the machines (or just machines) that they call “scientists”. Sometimes, science involves activities that would be considered strange by scientists, but it is important to remember that these people tend to take themselves more seriously than they would others.</li>
              </ol>
              <p>“Scientific community” [noun]</p>
              <p>A group of people that may share a passion for a common goal but, due to conflicts of interest (and even principles) cannot trust each other, which makes it less susceptible to manipulation or fraud; for this reason, science tends to be done by people in separate small groups that repeat the same experiments on different sets of humans.</p>
              <p>“Scientist” [noun]</p>
              <p>A person, typically male, who practices science day in and day out on sets of other people who, if they practice science at all, can usually only afford to do it in the evening after their work day is over and not related to any profession or business. Since these scientists have very similar backgrounds and training as each other, many of them talk about science as if they invented it as a profession (see “modesty”).</p>
              <p>“Modesty” [noun]</p>
              <p>The act of not claiming credit for discoveries or contributions to science that others could also make or make better; see “intellectual modesty”. Also see “science”, “scientist”, “insanity”, and “precious snowflake syndrome”.</p>
              <p>“Skynet” [noun]</p>
              <p>This ersatz god, in the form of an artificial general intelligence or AGI, is what some scientists hope to build to make themselves more precious than their subjects, to achieve their goal of making all the subjects equal to themselves; in other words, to put all the subjects into a bigger tin box to make themselves feel more important.</p>
              <p>“Interdisciplinary Science” [noun]</p>
              <p>A type of research that combines scientific activities across fields and subfields (in other words, you combine physicists, engineers, chemists, biologists, and any other specialists in your own discipline to do a different kind of science for a different kind of result); in the laboratory, the practice often involves adding frozen carbon dioxide (CO) and glassware (often Pyrex) into the liquid nitrogen that is produced by oil companies to make carbonic acid. In the laboratory, the scientists are usually not paid for their attendance.</p>
              <p>“Scientific Integrity” [noun]</p>
              <ol type="1">
                <li>Integrity as used in science, which is vaguely defined as always telling the truth to others and never fabricating the truth for oneself.</li>
                <li>The state of being superior to someone else.</li>
              </ol>
              <p>“Skimmer” [noun]</p>
              <p>An object placed in a liquid to remove fat and grease from the surface, typically used to clean soup and gravy off a plate. In scientific jargon, skimmers are “researchers” who skim off something from a body of work before making the rest public.</p>
              <p>“Logic” [noun]</p>
              <p>a thing that some scientists employ as an excuse to ignore what they see before their eyes, completely ignoring the fact that if it were to be used in such a way, it would completely invalidate all the results of science since its invention.</p>
              <p>“Engage in Debate” [verb]</p>
              <p>A phrase used to justify forcing someone to engage with evidence that most people find obvious and straightforward; specifically, you can use logic to (1) make an argument based on logic and (2) force someone to respond on the basis of logic without being able to defend themselves with any evidence but their own logic.</p>
              <p>“God” [noun]</p>
              <ol type="1">
                <li>See “skynet”.</li>
                <li>A universal explanation for any phenomenon not yet explained by science.</li>
              </ol>
              <p>“Scientist” [noun]</p>
              <p>A field based on science, devoted to completing works for which there will not be enough time in a single lifetime.</p>
              <p>“Squash and stretch” [verb]</p>
              <p>Another word for the S curve used to fit data to a function. The form of scientific communication in which a student is asked to graph the relationship between a dependent variable and an independent variable (x and y, respectively) against order of magnitude of the independent variable. The result is usually a curve consisting of three sections: a linear or near-linear part, an exponential part, and another linear or near-linear part. This curve typically represents a staircase between the upper right and lower left corners of the plot.</p>
              <p>“Some distance above the Earth” [adjective]</p>
              <p>Another word for looking at something from ten feet away using binoculars or a telescope.</p>
              <p>“The importance of this paper cannot be overemphasized” [adjective]</p>
              <p>An expression most commonly found at the beginning or end of a grant proposal or a paper submitted to certain journals which are intended to be linked with prestige and low acceptance ratio. When used in other contexts, it may mean that the writer does not realize the importance of something being discussed (such as in the middle of a review of a paper). This usage may be due to insufficient communication skills or over-estimation of the importance of the writer.</p>
              <p>“Think tank” [noun]</p>
              <p>A group that is absolutely incapable of challenging or even considering current knowledge because of ignorance of logic, history and mathematics.</p>
              <p>“Traditional mathematics” [noun]</p>
              <p>Another word for finite mathematics, or applied arithmetic and geometry, usually taught in a freshman year of university. The subject is now almost exclusively taught by so-called “researchers” and “professors” (rather than actual teachers).</p>
              <p>“Tenure” [noun]</p>
              <p>A form of monetary remuneration given to “researchers” who are known to engage in the act of research; as such it is universally agreed to have no monetary value. Tenure is given to “researchers” in recognition of how much their work is expected to increase the school’s status, the school’s treasury, or their own number of published papers.</p>
              <p>“The Future of Human Destiny” [noun]</p>
              <p>An article written by any member of the faculty in a self-aggrandizing manner wherein it is assumed that an article will result in the future improvement of the society. These articles are commonly used as election promises by politicians, but rarely achieved. The key word to look for in such articles is “vision”.</p>
              <p>“Scholarship” [noun]</p>
              <p>Another word for compensation given to graduate students (who work on funding their education) for successfully completing their education. It is a form of monetary remuneration known to have little value as it pays for none of the student’s living expenses. Scholarships are largely unheard of in any form outside the United States and Canada (where they seem not to exist). However, the few that do exist are extremely popular, as they allow graduate students in these countries to be paid for their studies instead of working in fast food joints as most do in countries which do not have scholarships.</p>
              <p>“Your mother wears Army boots” [phrase]</p>
              <p>A phrase that loosely translates from chemistry to: “I don’t agree with your analysis or conclusions”. The phrase is popular among those who wish to avoid elaboration and explanation of their disagreement, suggesting the disagreement is obvious. The expression may have originated in molecular biology.</p>
            </blockquote>
          </section>
          <section id="but-for-me-it-was-tuesday">
            <h2><a href="#but-for-me-it-was-tuesday" title="Link to section: § &#39;But For Me It Was Tuesday&#39;">“But For Me It Was Tuesday”</a></h2>
            <div>
              <p>Can GPT-3 write variants of the <a href="https://en.wikipedia.org/wiki/Street_Fighter_(1994_film)" data-link-icon="wikipedia" data-link-icon-type="svg"><em>Street Fighter</em></a> <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/ButForMeItWasTuesday" data-link-icon="TV" data-link-icon-type="text">“But For Me It Was Tuesday”</a> trope? I iterated through a variety of prompts, building up a set of variants, and fixing a few GPT-3 attempts, trying to get good variants.</p>
              <p>The overall impression I get is that, aside from an understandable tendency to write TVTropes entry-style completions, the situation is similar to the Tom Swifties: GPT-3 can learn the format perfectly and can match up the speaker and the kind of day/​action, but then it generally whiffs on coming up with a specific day which might make it clever<a href="#fn41" id="fnref41" role="doc-noteref"><sup>41</sup></a>⁠, typically falling back to the <a href="https://en.wikipedia.org/wiki/Snowclone" data-link-icon="wikipedia" data-link-icon-type="svg">snowclone</a> ending of “it was Tuesday”—which is reasonable, but less creative &amp; witty than I had hoped.</p>
            </div>
            <p>The final set of hand-written or hand-edited prompts:</p>
            <div>
              <blockquote>
                <p><strong>“But For X, It Was Y”</strong></p>
                <p><strong><a href="https://en.wikipedia.org/wiki/M._Bison" data-link-icon="wikipedia" data-link-icon-type="svg">M. Bison</a>: “I’m sorry. I don’t remember any of it.” <a href="https://en.wikipedia.org/wiki/Chun-Li" data-link-icon="wikipedia" data-link-icon-type="svg">Chun-Li</a>: “You don’t remember‽” Bison: “For you, the day Bison graced your village was the most important day of your life. But for me, it was Tuesday.”</strong></p>
                <p><strong>Jesus: “The day I rose from the day and harrowed Hell was the most important day in human history. But for me… it was Sunday.”</strong></p>
                <p><strong><a href="https://en.wikipedia.org/wiki/Jason_Voorhees" data-link-icon="wikipedia" data-link-icon-type="svg">Jason Voorhees</a>: “The day Voorhees chopped your boyfriend’s head off was the most important day of your life. But for me, it was Friday.”</strong></p>
                <p><strong>American Citizen: “The day Trump was elected was one of the most terrifying days of your life. But for me… it was Tuesday.”</strong></p>
                <p><strong>Odin: “The day your kingdom fell on the battlefield was the most important battle of your life. But for Odin, it was <a href="https://en.wikipedia.org/wiki/Wednesday#Etymology" data-link-icon="wikipedia" data-link-icon-type="svg">Wednesday</a>⁠.”</strong></p>
                <p><strong>Neil Armstrong: “The day <a href="https://en.wikipedia.org/wiki/Apollo_11" data-link-icon="wikipedia" data-link-icon-type="svg">Apollo 11</a> took off was the most important date in space history. But for me, it was Moonday.”</strong></p>
                <p><strong>Human: “For you, the day your piglets were taken away was the most traumatic day of your life. For me… it was Sunday dinner.”</strong></p>
                <p><strong>The Man in Black to Vizzini, <em>The Princess Bride</em>: “The beverage you drank today was the most distasteful beverage of your life. But for me… it was Teatime.”</strong></p>
                <p><strong>Pope: “The day you were blessed by God’s representative on earth was the most holy and blessed of your life. But for me… it was Sunday.”</strong></p>
                <p><strong>American Voter: “The day I elected Donald Trump was the start of a long nightmare for people around the world. But for me… it was a day off.”</strong></p>
                <p><strong>Mr. Granger to Jenny, <em>Bill and Ted’s Excellent Adventure</em>: “The day I met Bill and Ted and they started saving the world was the most mind-blowing and intense day of my life. But for them… it was Tuesday 100 years from now.”</strong></p>
                <p><strong>Salesman: “The day you got your hands on that new sports car was the most euphoric day of your life. But for me… it was payday.”</strong></p>
                <p><strong>Allergy Sufferer: “The most recent holiday was the most painful and sneeze-filled day of my life. But for everyone else, it was May Day.”</strong></p>
              </blockquote>
            </div>
            <p>Completions (topic italicized for readability):</p>
            <blockquote>
              <p><strong>…</strong> <em>Mr. Wong</em>: “The day Mr. Wong stabbed your wife and children was the most important day of your life. But for Mr. Wong, it was Friday.”</p>
              <p><em>Popeye</em>: “The day I ate your spinach was the most important day of your life. But for me, it was Tuesday.”</p>
              <p><em>The Godfather</em>: “The day your family was strangled was the most important day of your life. But for me, it was Tuesday.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>Video Games</p>
              <p>Visual Novels</p>
              <p><em>Danganronpa</em>: “The day she witnessed Kyouko Kirigiri’s gruesome murder was the most important day of Maki Harukawa’s life. But for me… it was Tuesday.”</p>
              <p>Western Animation</p>
              <p>Raphael of <em>Teenage Mutant Ninja Turtles</em>: “The day he lost his memory was the most important day of Shredder’s life. But for me… it was Tuesday.”</p>
              <p><em>Scrooge McDuck</em>: “The day you fell off the boat was the most important day of your life. But for me… it was Tuesday.”</p>
              <p>Toofer on <em>30 Rock</em>: “The day you saw Liz on a date with Alec Baldwin was the most important day of your life. But for me… it was Tuesday.”</p>
              <p>Bender, <em>Futurama</em>: “The day I killed the President of Earth was the most important day of my life. But for you, it was Tuesday.”</p>
              <p>It should be noted that, while it’s usually considered a comedy trope, it is possible to do the “thing that day was the most important day of the person’s life” thing seriously. Examples include the anime Brigadoon: Marin and Melan’s True Story, where a mysterious planet (and its mysterious inhabitants) come into view for the first time. The days before are leading up to a string of murders, and that one day… is the most important day of the characters’ lives.</p>
              <p><em>Transformers Prime</em>: “The day you risked your lives to save the world was the most important day of your life. But for Optimus Prime, it was Tuesday.”</p>
              <p>In <em>The Amazing World of Gumball</em>, an angry Mr. Robinson imagines a world where Gumball dies trying to save him, becoming a hero to everyone else. For Mr. Robinson, this is the most important day of his life. For the other characters? Well, they’re going on with their day…</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Arnold Schwarzenegger</em>: “The day I became ruler of California was the most important day of my life. But for you, it was Cinco de Cuatro.”</p>
              <p><em>Francis Underwood</em>: “The day I became the majority leader was the most important day in American political history. But for you, it was a Tuesday.”</p>
              <p><em>Joker</em>: “The day the clown that scarred my face appeared in my cell was the most important day of my life. But for you, it was a Wednesday.”</p>
              <p><em>The Question</em>: “The day I went mad, was the most important day of my life. But for you, it was Tuesday.”</p>
              <p><em>Blue Fairy</em>: “The day I gave Pinocchio life was the most important day in world history. But for you, it was Tuesday.”</p>
              <p>In the <em>Looney Tunes</em> short “Bully For Bugs,” Elmer’s traumatic ordeal was the day Bugs Bunny ruined his chances at romance.</p>
              <p>“The premise that if it happened to you, it was important, but if it happened to me, it was trivial” is actually lampshaded by Kim in <em>Red vs. Blue</em>. She states, “In the past two weeks, my capture, my torture, and the end of the world… I guess I never realized how important I was.”</p>
              <p><em>M. Bison</em>: “The day I took the throne was the most important day of my life. But for you… it was Tuesday.”</p>
              <p><em>Rene Russo</em>: “The day I died was the most important day of my life. But for you… it was Thursday.”</p>
              <p><em>Oliver Stone</em>: “The day I was elected was the most important day in American politics. But for you, it was Thursday.”</p>
              <p><em>The Fairy Godmother</em>: “The day I turned my step-daughter into a Stepford Smiler was the most important day in the history of the world. But for you, it was a Tuesday.”</p>
              <p><em>Jeffrey Donovan</em>: “The day I died was the most important day of my life. But for you… it was Tuesday.”</p>
              <p><em>Gordon Freeman</em>: “The day your little girl broke out of her mother’s womb was the most important day of your life. But for Gordon Freeman… it was Friday.”</p>
              <p><em>Captain James T. Kirk</em>: “The day I was born was the most important day of my life. But for you… it was Tuesday.”</p>
              <p><em>Cat</em>: “The day I made my masterpiece from scratch was the most important day of my life. But for you, it was Thursday.”</p>
              <p><em>Timothy Dalton</em>: “The day I was born was the most important day in the history of the human race. But for you… it was a Monday.”</p>
              <p><em>Gil Grissom</em>: “The day my wife was killed was the most important day in my life. But for you… it was Tuesday.”</p>
              <p><em>Captain America</em>: “The day I was frozen was the most important</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Aragorn</em>: “The battle of Helm’s Deep was the most important battle of my life. But for me, it was Wednesday.”</p>
              <p><em>Doctor Doom</em>: “The day I used your face as my doorknob was the most memorable day of your life. But for me, it was Monday.”</p>
              <p>A common form of this trope is for one side of an antagonistic relationship to refer to a certain event or series of events as “most important” when speaking from their own point of view, while the other side regards the exact same events as “one of many” and shrugs it off as unimportant. Contrast Perspective Reversal for when the other side actually feels this way. For the alternative of both sides regarding the same events as “most important”, see The Unimportance of Being Dead.</p>
              <p>Examples</p>
              <p>See also Not Himself, Not This Time, Breakout Villain, Dead All Along, Not-So-Omniscient Council of Bickering, What Happened to the Mouse?, Villainous BSoD, Villainous Friendship.</p>
              <p>open/​close all folders</p>
              <p>Advertising</p>
              <p>Beavis and Butt-head watch a news story about a squirrel trying to invade the Soviet Union: Butt-Head: “It’s not important. I’m more concerned about this new guy on Beavis and Butt-head who can’t be stopped. What’s up with that?”</p>
              <p>Beavis: “Hey, how can you not be a ‘Concerned American’ when you see this?” (points to a man on TV reading a newspaper) “It says here that the cold war is almost over.”</p>
              <p>Anime &amp; Manga</p>
              <p>Toguro of YuYu Hakusho, in response to Genkai telling him that fighting humans is wrong: “I can’t believe you’re saying that! Our fight was the most important fight in the world!”</p>
              <p>In the 2<sup>nd</sup> Itazura Na Kiss, Fujioka had spent most of her life abroad. And after meeting Kouta again, her memories from her life as a foreigner disappeared because she prioritized him as “the most important thing”.</p>
              <p>Comic Books</p>
              <p>Green Arrow vs. the Shield: During their fight, the Shield threatens to use the mother of his infant son as leverage against him. Green Arrow: You don’t have a kid.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Rhona Mitra</em>: “The day you kicked me in the balls was the most painful day of your life. For me, it was Tuesday.”</p>
              <p><em>Grim Reaper</em>: “The day I killed your wife and son was the most painful day of your life. For me… it was Thursday.”</p>
              <p><em>Hercule to M. Bison</em>: “The day your organization was shut down by the World Health Organization was the most important day in crime history. But for me… it was… on the second.”</p>
              <p>Podcasts</p>
              <p>This is the description given in the TVtropes description of Beyond Belief: the show about UFOs, the paranormal, and other Fortean topics, that lasted for one season on A&amp;E back in 2005.</p>
              <p>Tabletop Games</p>
              <p>Theater</p>
              <p>In The Imaginary Invalid, Dr. Rene describes his backstory by using this trope; for instance, the day of his father’s death was the most important day of his life… so far . He then provides a few plausible excuses as to why he has so few details about it: “Because of my age, and for the accident of being my father’s son.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Advertisement:</p>
              <p>The Morally Neutral “It Was Going To Happen Anyway” trope</p>
              <p><em>Lucifer Morningstar</em>: “Do you really think the day you first met your mother was the most important day of your life? I’m sorry, you’re wrong. The day I dragged your father out of hell was the most important day of your life.”</p>
              <p><em>Nurse</em>: “The day I picked up the patient’s chart and saw your name was the most important day of my career. And for you, it was Friday.”</p>
              <p><em>Terminator</em>: “The day Skynet declared war on humanity was the most important day in human history. But for me… it was Monday.”</p>
              <p><em>Croupier</em>: “The day you double-downed with a card higher than a seven was the biggest mistake of your life. But for me… it was Sunday.”</p>
              <p>The “I Haven’t Done Anything Wrong” trope</p>
              <p>From Total Drama All-Stars, the episode “It Came From Japan: Part 1”, Gwen and Courtney, while playing a Pimped-Out Dress on a Large Stick:</p>
              <p><em>Gwen</em>: “The day the dress won the elimination challenge was the most important day of my Total Drama career. But for you…”</p>
              <p>Courtney: “… it was Tuesday.”</p>
              <p>From <em>Wild Kratts</em>, the episode “Gator Down”, Martin and Chris:</p>
              <p>Martin: “The day we spent with the gators was the most important day of our lives. But for the gators…”</p>
              <p>Chris: “It was Tuesday.”</p>
              <p>From <em>Total Drama: Pahkitew Island</em>, Episode 4: “The Bold and the Booty-ful” , Bridgette, Courtney, and Eva:</p>
              <p>Bridgette: “The day I was eliminated was the most important day in my Total Drama career. But for you, it was Tuesday.”</p>
              <p>Courtney: “The day my boyfriend dumped me was the most important day of my life. But for you… it was your day off.”</p>
              <p>Eva: “The day the spear destroyed the camp was the most important day in Total Drama history. But for you, it was Wednesday.”</p>
              <p>Advertisement:</p>
              <p>From the original <em>Thunderbirds</em>, the episode “Danger at Ocean Deep”:</p>
              <p>Gordon: “The day we saved the cargo plane was the most important day in our illustrious careers. But for you…”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> “So he stitched my mouth shut, and now you’re telling me my food is riddled with poison and my only friend wants to murder me. Well, you’ve had a tough year too, haven’t you, Lassie?”</p>
              <p>Lassiter: Ohh. Oh man, this is SO MUCH OF A BIG DEAL. Wes: You don’t think it’s a big deal? Lassiter: Well, it is. But this‽? Wes, I have money problems! I’m not living in a gilded palace like you with a full closet. I’m not living like an Egyptian. I have clothes that don’t fit, and shoes that do fit, but hurt my feet. But… my dad’s brother won’t give me any of his inheritance until I’m married! And then there’s someone at the office who keeps stealing my belongings, I think it’s because I killed their dog. And then I think they’re still following me! Oh, and someone moved into the apartment next to mine, but they’re pee</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Doom</em>, [1] Doom’s level Marine: “The day you killed me was the most important day of my life. But for you… it was a Tuesday.”</p>
              <p><em>Wally</em>: “The day your book hit The New York Times bestseller list was the most glorious and joyous day of your career. But for me… it was a Tuesday.”</p>
              <p><em>The Kid and Kick-Ass</em>: “The day we don’t give in to you is the day that we’re going to die. But for us… it’s going to be Tuesday.”</p>
              <p><em>Ghost Cop</em>: “The day I died and became a spirit who can walk through walls is my favorite day. But for me… it’s Monday.”</p>
              <p><em>Julian</em>: “The day I didn’t call my wife because I was drunk is the most shameful day of my life. But for me… it was a Tuesday.”</p>
              <p><em>Bill Clinton</em>: “The day I was impeached was the most humiliating day of my political career. But for me… it was Wednesday.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Captain James T Kirk</em>: “The day I gave birth to Khan was the most important day in history. But for me… it was Thursday.”</p>
              <p><em>Granny</em>: “The day the old dog died was the most horrible day in my life. But for her it was just Wednesday.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Julius Caesar</em>: “The day of my assassination was the most beautiful day of my life. But for you, I guess it was Thursday.”</p>
              <p><em>Working-Class Man</em>: “The day I became royalty was the most deserving day of my life. But for me… it was Tuesday. Bloody thing.”</p>
              <p><em>Shrek</em>: “The day your princess got rescued was the happiest day of your life. But for Shrek, it was Thursday”</p>
              <p><em>Fight Club</em>: “The day you save the world was the greatest day in the history of civilization. But for me… it was Monday.”</p>
              <p><em>American Soldier</em>: “The day you spilled your guts to the enemy was the most incredible and personal experience of your life. But for me… it was a Tuesday.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The Blob Monster, <em>The Blob</em>: “The night I devoured my first human was the most satisfying feast in Blob history. But for me… it was the year of my birth.”</p>
              <p><em>Freddie Mercury</em>: “The day you were blown by Queen on a Mardi Gras gig was the most exciting night of your life. But for me… it was Wednesday.”</p>
              <p>Uncle Vernon to Harry, <em>Harry Potter and the Philosopher’s Stone</em>: “The day you left without notice was the day your aunt and I had been waiting for all our lives. But for you… it was Tuesday.”</p>
              <p><em>American Citizen</em>: “The day Trump was elected was one of the most terrifying days of your life. But for me… it was Tuesday.”</p>
              <p><em>Dolph Lundgren</em>: “The day I defeated Sylvester Stallone was the most triumphant day in Rocky history. But for me, it was Tuesday.”</p>
              <p><em>Will MacNeil</em>: “That day you murdered Harry was the most evil thing you’d ever done. But for me… it was Tuesday.”</p>
              <p><em>Brennan Huff</em>: “The day you got your license was the most exciting thing you’d ever done. But for me… it was Thursday.”</p>
              <p><em>Kagato</em>: “The day you doomed the world was a tragic event of epic proportions. But for me… it was Tuesday.”</p>
              <p><em>Red Bishop</em>: “The time you became a figure in golem history was the most terrifying day of your life. But for me… it was Saturday.”</p>
              <p><em>Deadbeat Dad</em>: “The day your marriage ended was the most desperate and horrible day of your life. But for me… it was Sunday.”</p>
              <p><em>Mr. Davis</em>: “The day you got knocked out was the most humiliating moment of your boxing career. But for me… it was Friday.”</p>
              <p><em>Dexter Morgan</em>: “The day I got arrested was the day everything changed and my life took a turn for the worse. But for me… it was Wednesday.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Bill</em>: “The day that I created civilization was one of the best days in world history, especially for my wife and kids. But for me, it was Tuesday.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Shooter</em>: “The day you shot the 3-pointer was the most thrilling day of your life. But for me… it was Friday.”</p>
              <p><em>Sportscaster</em>: “The day you won the million dollar sportsball jackpot was the luckiest day of your life. But for me… it was Thursday.”</p>
              <p>Vera, when her wheelchair-bound grandson isn’t getting any for a while, to The Kid, Chunk’s favorite band, <em>The Goonies</em>: “The day your band disappeared was the happiest day of my life. But for you, it was Tuesday.”</p>
              <p><em>Old Man</em>: “The day you were born was the most joyful day of my life. But for your mother… it was Tuesday.”</p>
              <p><em>Single Mother</em>: “The day your father left was the most distressing day of my life. But for him… it was Sunday.”</p>
              <p><em>Samuel L. Jackson to a deer who just shat on his windshield, Deep Blue Sea</em>: “For you… defecating all over my car was the worst day of your life. But for me… it was Monday.”</p>
              <p>Vroom</p>
              <p><em>Priest</em>: The minute I had my first drink, the minute I decided to let Jesus live his life through me… oh man, I was the happiest man who ever walked the Earth. But for me, it was Wednesday.</p>
              <p><em>Samuel L. Jackson</em> in a rare instance of humility, on being a Hollywood icon: “The day I hosted my show, It’s You Guys. Where young people wanted to meet me. That was the best day of my life. But for me… it was Tuesday.”</p>
              <p>Unsorted</p>
              <p>In the episode, as quoted on this very page, Stark said that the Dance Off arc was the most wondrous night of his life. At the end of that arc, he was injured in an alley by the trucker, so he got up and walked straight into the front door of his workplace. Tony wasn’t injured enough to walk straight into work in the latter half, but this doesn’t ruin the arc</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> “But X Was Y, And Everybody Else Was Like Z”</p>
              <p><em>Larry</em>: “Hey Ned, the day you discovered my letter was the most important day in my life. But for you… it was Tuesday.”</p>
              <p><em>Ned</em>: “The day I learned of Princess Susannah’s betrothal was the worst day in my life. But for Larry… it was Wednesday.”</p>
              <p>“And This Is Why (Story Logic)”</p>
              <p><em>The Fallen</em>: “You have betrayed us, and have delivered this world into darkness and decay. And this is why we must kill you… MORTAL!”</p>
              <p><em>Oracle</em>: “You once betrayed us and you did deliver this world into darkness. And this is why we must kill you…”</p>
              <p><em>Asteroid Day</em>: “In 3.3 billion years, you betrayed us and you did deliver this planet into darkness and decay. And this is why we must kill you…”</p>
              <p><em>U-Moon-Tic</em>: “You have turned your back on us, you have betrayed our ideals,</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Barry Allen</em>: “The day I defeated Eobard Thawne and saved Central City and the world was the most important day of my life. But for Wally West… it was Tuesday.”</p>
              <p><em>Michael Jackson</em>: “The day I was killed was the most important day of my life. But for Jason Shapiro… it was payday.”</p>
              <p><em>Joan Rivers</em>: “The day I died was the most important day of my life. But for Werner Herzog… it was Tuesday.”</p>
              <p><em>Nostradamus</em>: “The day the King of France drank from my Skull was the most important day in my life. But for people around the world… it was Thursday.”</p>
              <p>Marlin to Dory, <em>Finding Nemo</em>: “The day I brought you home was the greatest day of my life. But for Nemo… it was Tuesday.”</p>
              <p><em>Manuel Sanjuan</em>: “The day I returned to Pamplona was the happiest day of my life. But for the bulls… it was Friday.”</p>
              <p><em>Manuel Sanjuan</em>: “The day I once again returned to Pamplona was the happiest day of my life. But for the bulls… it was Tuesday.”</p>
              <p><em>Dick Grayson</em>: “The day I became Batman and struck fear into the hearts of criminals and villains was the most important day of my life. But for the rogues gallery… it was Thursday</p>
            </blockquote>
          </section>
          <section id="rick-morty-high-iq-copypasta">
            <h2><a href="#rick-morty-high-iq-copypasta" title="Link to section: § &#39;Rick &amp; Morty High IQ Copypasta&#39;">Rick &amp; Morty High IQ Copypasta</a></h2>
            <div>
              <figure>
                <img alt="The unfortunate consequences of careless prompt writing." decoding="async" height="1117" loading="lazy" src="https://jvns.ca/doc/ai/nn/transformer/gpt/fiction/2021-07-08-gwern-meme-tuxedowinniethepooh-gpt3promptingwithwritingquality.png" title="Winnie the Pooh Tuxedo meme comparing a basic GPT-3 prompt quality (mediocre), after heavy sampling/selection (tuxedo), and when there is a typo or other error in it (very low quality, drooling Pooh Bear)" width="994"/>
                <figcaption aria-hidden="true">
                  The unfortunate consequences of careless prompt writing.
                </figcaption>
              </figure>
              <p>A reader requested parodies of the <a href="https://en.wikipedia.org/wiki/Rick_and_Morty" data-link-icon="wikipedia" data-link-icon-type="svg"><em>Rick and Morty</em></a> <a href="https://knowyourmeme.com/memes/to-be-fair-you-have-to-have-a-very-high-iq-to-understand-rick-and-morty" data-link-icon="KYM" data-link-icon-type="text,tri">high IQ copypasta</a> along the lines of the Navy Seal, and provided several variants to get GPT-3 started, successfully. (It also is a nice demonstration of how GPT-3 will gauge the level of typos in a prompt, and if there are typos, will make new typos of its own to imitate the original writer.)</p>
            </div>
            <p>Prompt (topics italicized):</p>
            <div>
              <blockquote>
                <p><strong>To be fair, you have to have a very high IQ to understand <em>Rick and Morty</em>. The humor is extremely subtle, and without a solid grasp of theoretical physics most of the jokes will go over a typical viewer’s head. There’s also Rick’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from <a href="https://en.wikipedia.org/wiki/Narodnaya_Volya" data-link-icon="wikipedia" data-link-icon-type="svg">Narodnaya Volya</a> literature, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of these jokes, to realize that they’re not just funny—they say something deep about LIFE. As a consequence people who dislike Rick and Morty truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Rick’s existencial catchphrase “Wubba Lubba Dub Dub,” which itself is a cryptic reference to <a href="https://en.wikipedia.org/wiki/Ivan_Turgenev" data-link-icon="wikipedia" data-link-icon-type="svg">Turgenev’s</a> Russian epic <a href="https://en.wikipedia.org/wiki/Fathers_and_Sons_(novel)" data-link-icon="wikipedia" data-link-icon-type="svg"><em>Fathers and Sons</em></a> I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Dan Harmon’s genius unfolds itself on their television screens. What fools… how I pity them. ðŸ˜‚ And yes by the way, I DO have a Rick and Morty tattoo. And no, you cannot see it. It’s for the ladies’ eyes only—And even they have to demonstrate that they’re within 5 IQ points of my own (preferably lower) beforehand.</strong></p>
                <p><strong>To be fair, you have to have a very high IQ to understand <em>cuckolding</em>. The kink is extremely subtle, and without a solid grasp of intersectional feminism most of the empowerment will go over a typical cuck’s head. There’s also the Bull’s aggressive outlook, which is deftly woven into his role—his personal mannerisms draw heavily from the mating habits of bovine animals, for instance. The cucks understand this stuff; they have the intellectual capacity to truly appreciate the depths of being cuckolded, to realize that it’s not just arousing—it says something deep about DIVERSITY. As a consequence people who dislike cuckolding truly ARE idiots—of course they wouldn’t appreciate, for instance, the power of when the woman says “leave the room Carl you’re breaking Tyrone’s concentration,” which itself is a cryptic reference to the plight of African-American males in the United States. I’m smugly grinning right now just imagining one of those addlepated simpletons scratching their heads in confusion as The Bull’s strong African seed ejaculates itself on my wife. What bigots… how I pity them. 😂 And yes by the way, I DO have a cuck tattoo. And no, you cannot see it. It’s for The Bull’s eyes only—And even they have to demonstrate that they’re within 7 inches above of my own (preferably higher) beforehand.</strong></p>
                <p><strong>To be fair, you have to have a very high IQ to understand <em><a href="https://en.wikipedia.org/wiki/The_Last_of_Us_Part_II" data-link-icon="wikipedia" data-link-icon-type="svg">The Last Of Us 2</a></em>. The story is extremely subtle, and without a solid grasp of theoretical storytelling, most of the story will go over a typical player’s head. There’s also Abby’s nihilistic outlook, which is deftly woven into her characterisation—her personal philosophy draws heavily from <a href="https://en.wikipedia.org/wiki/Anita_Sarkeesian" data-link-icon="wikipedia" data-link-icon-type="svg">Anita Sarkeesian</a> literature, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of these characters, to realise that they’re not just underwritten—they say something deep about LIFE. As a consequence people who dislike <em>The Last Of Us 2</em> truly ARE idiots—of course they wouldn’t appreciate, for instance, the storytelling in Abby’s existential sex scene “I’m gonna fuck a committed dude who’s gf is pregnant” which itself is a cryptic reference to <a href="https://en.wikipedia.org/wiki/Neil_Druckmann" data-link-icon="wikipedia" data-link-icon-type="svg">Neil Druckmanns</a> epic fetishes. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Abby’s nine iron unfolds Joel’s head on their television screens. What fools… how I pity them. 😂 And yes, by the way, i DO have an Abby tattoo. And no, you cannot see it. It’s for the buff ladies’ eyes only—and even then they have to demonstrate that they’re within 5 IQ points of my own (preferably lower) beforehand. Nothin personnel kid 😎</strong></p>
                <p><strong>To be fair, you have to have a very high IQ to understand <em>Wildbow</em>. His worldbuilding is extremely subtle, and without a solid grasp of theoretical bullying most of the torture porn will go over a typical reader’s head. There’s also <a href="https://en.wikipedia.org/wiki/Worm_(web_serial)" data-link-icon="wikipedia" data-link-icon-type="svg">Taylor’s</a> cardboard outlook, which is deftly woven into her charactization as a Mary Sue—her personal philosophy draws heavily from the trashcan, for instance. The fans understand this stuff; they are wild for the Wildbow. They have the intellectual capacity to truly appreciate the depths of his meandering prose, to realize that he’s not just telling me exactly what Taylor is thinking at any given moment, he says something deep about NOTHING. As a consequence people who dislike Wildbow truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Taylor’s existential catchphrase “Take that, you worm!” which itself is a cryptic reference to a publishing deal that will never happen. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Wildbow’s genius wit unfolds itself on their Kindle readers. What fools… how I pity them.</strong></p>
                <p><strong>To be fair, you have to have a very high IQ to understand</strong></p>
              </blockquote>
            </div>
            <p>Completions:</p>
            <blockquote>
              <p><strong>…</strong> why <em>living in a basement</em> with internet access is the true essence of freedom. The lifestyle is extremely subtle, and without a solid grasp of socialising, most of the incel memes will go over a typical basement dweller’s head. There’s also the internet’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from the human genetic code, for instance. The BASEDfags understand this stuff; they have the intellectual capacity to truly appreciate the depths of his mediocre Photoshop skills, to realize that he’s not just making hilarious memes—he says something deep about EVOLUTION. As a consequence people who dislike the BASED lifestyle truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in the BASED dragon’s existential catchphrase “Ayy lmao” which itself is a cryptic reference to the mating habits of the wildebeest. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as the BASEDpyre unfolds itself on their computer screens. What faggots… how I pity them. 😂 And yes, by the way, I DO have a BASED tattoo. And no, you cannot see it. It’s for the LADIES’ eyes only—And even then they have to demonstrate that they’re within a 1,000,000 IQ points of my own (preferably lower) beforehand. Seriously get the fuck off my lawn, kid!</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Dragon Age 2</em>. The combat is extremely subtle, and without a solid grasp of tactical combat, most of the encounters will go over a typical player’s head. There’s also David Gaider’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from dank memes, for instance. The fans understand this stuff; they are wild for the David Gaider. They have the intellectual capacity to truly appreciate the depths of his sexually ambiguous masterpiece, to realize that he’s not just “bad at writing women”—he says something deep about LESBIANS. As a consequence people who dislike Dragon Age 2 truly ARE idiots—of course they wouldn’t appreciate, for instance, the power of a game where I’m the worthless drone of some worthless elven chick’s who spends most of her time screeching in elven and the rest of it spreading her legs for the loser protagonist, which itself is a cryptic reference to one of the most hatefully sexist conventions in the gaming industry. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as they hear the strains of “I Am the One Shepherd” playing faintly from the stereo speakers. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <a href="https://knowyourmeme.com/memes/tendies-stories" data-link-icon="KYM" data-link-icon-type="text,tri"><em>chicken tendies</em></a>⁠. The humour is extremely subtle, and without a solid grasp of theoretical wanking material, most of the jokes will go over a typical joke teller’s head. There’s also Einstein’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from Alfred E. Newman literature, for instance. The nerds understand this stuff; they have the intellectual capacity to truly appreciate the depths of word salad, to realize that it’s not just WILD—it says something deep about IDENTITY. As a consequence people who dislike chicken tendies truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Einstein’s existential catchphrase “your jokes are so bland you deserve to be dragged to the bottom of the sea and killed by whales,” which itself is a cryptic reference to Christopher Hitchens’ piece The burden of Socrates. I’m smugly grinning right now just imagining one of those addlepated simpletons scratching their heads in confusion as my tongue unfolds these delicious tendies on my mouth. What fools… how I pity them. 😂 And yes by the way, I DO have a cowbell tattoo. And no, you cannot see it. It’s for the chicken tendies only—and even then they have to demonstrate that they’re within 10 watt of my own (preferably lower) beforehand.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>Sonic memes</em>. The humour is extremely subtle, and without a solid grasp of theoretical erotica most of the trolls will go over a typical EDiots head. There’s also the Sonic fanbase’s nihilistic outlook, which is deftly woven into their characterisation—their personal philosophy draws heavily from Jacob Truax literature, for instance. The 4chan archive knows this stuff; they are wild for the Sonic fandom. They have the intellectual capacity to truly appreciate the depths of these characters, to realize that they’re not just funny—they say something deep about HATE. As a consequence people who dislike Sonic and it’s hilarious memes truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Sonic’s existential catchphrase “YOU’LL NEVER BE AS FAST AS ME” which itself is a cryptic reference to a Youtube comment box. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Rachel J’s n00dz unfolds itself on their screens. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em><a href="https://en.wikipedia.org/wiki/Portal_2" data-link-icon="wikipedia" data-link-icon-type="svg">Portal 2</a></em>. The humour is extremely subtle, and without a solid grasp of theoretical colourblinds most of the trolling will go over a typical redditor’s head. There’s also GLADOS’s nihilistic outlook, which is deftly woven into her characterisation as a Mary Sue—her personal philosophy draws heavily from Chinese propaganda literature, for instance. The Something Awful archives knows this stuff; they are wild for the Portal fandom. They have the intellectual capacity to truly appreciate the depths of her meandering prose, to realize that she’s not just spouting gibberish—she says something deep about SERIOUS. As a consequence people who dislike Portal 2 truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in GLADOS’s existential catchphrase “Suck my chassis” which itself is a cryptic reference to a bonus round on the first Portal game. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Chell’s orange jumpsuit unfolds itself on their monitors. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <a href="https://en.wikipedia.org/wiki/My_Life_as_a_Teenage_Robot" data-link-icon="wikipedia" data-link-icon-type="svg"><em>My Life As A Teenage Robot</em></a>⁠. The show is extremely subtle, and without a solid grasp of fantasy writing most of the humour will go over a typical viewer’s head. There’s also Jenny XJ-9’s nihilistic outlook, which is deftly woven into her characterisation as a Mary Sue—her personal philosophy draws heavily from <em>Pearls Before Swine</em>, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of the show, to realize that it’s not just transgressive—it says something deep about LIFE. As a consequence people who dislike <em>My Life As A Teenage Robot</em> truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Jenny XJ-9 saying “I’ll be back. There’s a lot more MEN I haven’t dated,” which itself is a cryptic reference to the casting choice of the show, because of course she wouldn’t be a guy inside that perfectly anatomically-unrealistic metallic shell. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as <a href="https://en.wikipedia.org/wiki/Dan_DiDio" data-link-icon="wikipedia" data-link-icon-type="svg">Dan DiDio’s</a> wooden dialogue unfolds itself on their television screens. What simpletons… how I pity them. 😂</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em><a href="https://en.wikipedia.org/wiki/Law_%26_Order:_Special_Victims_Unit" data-link-icon="wikipedia" data-link-icon-type="svg">Law And Order: SVU</a></em>. The show is extremely subtle, and without a solid grasp of pop culture tropes and phallocentrism, most of the crime will go over a typical lawyer’s head. There’s also Ironside’s nihilistic outlook, which is deftly woven into his characterisation as a way to whore himself out to other shows—his personal philosophy draws heavily from <em>Law And Order</em>, for instance. The fans understand this stuff; they are wild for the William Shatner. They have the intellectual capacity to truly appreciate the depths of his meandering prose, to realize that Shatner’s not just telling me exactly what Ironside’s thinking at any given moment, it says something deep about NOTHING. As a consequence people who dislike William Shatner truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in the line “Stella. Hit the deck,” which itself is a cryptic reference to a Star Wars trailer or Youtube Poop video from years ago. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as William Shatner’s genius wit unfolds itself on their television screens. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <a href="https://en.wikipedia.org/wiki/The_Avengers_(2012_film)" data-link-icon="wikipedia" data-link-icon-type="svg"><em>The Avengers</em></a>⁠. The storytelling is extremely subtle, and without a solid grasp of theoretical storytelling most of the jokes will go over a typical viewer’s head. There’s also Samuel Jackson’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from martian racial superiority fanfictions, for instance. The fans understand this stuff; they are wild for Samuel L Jackson. They have the intellectual capacity to truly appreciate the depths of his meandering prose, to realize that he’s not just telling me what Nick Fury is thinking, he’s saying something deep about NICK FURY’S THOUGHTS. As a consequence people who dislike Samuel L Jackson truly ARE idiots—of course they wouldn’t appreciate, for instance, the humour in Nick Fury’s existential catchphrase “Wipe your dick on the curtains you fucking cunt.” which itself is a cryptic reference to Samuel Jackson’s alleged history of sexual abuse as a child. I’m smugly grinning right now just imagining one of those addlepated simpletons scratching their heads in confusion as the out of place Samuel L Jackson cameo unfolds itself on their television screens. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>erotic novels</em>. The romance is extremely subtle, and without a solid grasp of clinical foreplay, most of the sex will go over a typical reader’s head. There’s also Sydney’s trenchant outlook, which is deftly woven into her charactization—her personal philosophy draws heavily from the sexual conditioning of dogs, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of these characters, to realize that they’re not just written well—they say something deep about LIFE. As a consequence people who dislike erotic novels truly ARE idiots—of course they wouldn’t appreciate, for instance, the game in the sex scene “Are you coming, my love?”, which itself is a cryptic reference to writer’s block I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as the author inserts Sydney’s arousal at being ridden like a cheap fucktoy into every fantasy in the World. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <a href="https://en.wikipedia.org/wiki/Mr._Robot" data-link-icon="wikipedia" data-link-icon-type="svg"><em>Mr_Robot</em></a>⁠. The main character is extremely subtle, and without a solid grasp of theoretical self-insertion most of the script will go over a typical audience’s head. There’s also Elliot’s complete outlook, which is deftly woven into his characterization—his personal philosophy draws heavily from Apple literature, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of E:R’s dark humor, to realize that it’s not just funny—it says something deep about NOTHING. As a consequence people who dislike <em>Mr_Robot</em> truly ARE idiots—of course they wouldn’t appreciate, for instance, the hacking scene when Elliot seems to have cleverly manipulated a girl he’s met into giving him a free homemade quiche, which itself is a cryptic reference to <a href="https://en.wikipedia.org/wiki/The_Matrix" data-link-icon="wikipedia" data-link-icon-type="svg">The Matrix</a>⁠. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as E:R dismantles their head on television screens. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em><a href="https://en.wikipedia.org/wiki/Cuphead" data-link-icon="wikipedia" data-link-icon-type="svg">Cuphead</a></em>. The cutscenes are extremely subtle, and without a solid grasp of stage magic most of the puzzles will go over a typical gamer’s head. There’s also the Cuphead’s aggressive outlook, which is deftly woven into his charactization—his personal philosophy draws heavily from the German left-wing terrorist organization <a href="https://en.wikipedia.org/wiki/Red_Orchestra_(espionage)" data-link-icon="wikipedia" data-link-icon-type="svg">The Rote Kapelle</a>⁠, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of his convoluted backstory, to realize that it’s not just shock art—it says something deep about ANIMAL CRUELTY. As a consequence people who dislike Cuphead truly ARE idiots—of course they wouldn’t appreciate, for instance, the storytelling in the cock-choking cutscene “I’ve been eyeing that airplane up for weeks. Why did you stop me from killing my headache?” which itself is a cryptic reference to Japanese internment. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Majet Kovas surreal paintings unfold themselves on their WDF monitors. What fools… how I pity them. 😂 And yes, by the way, i DO have a Cuphead tattoo. And no, you cannot see it. It’s for Majet Kovas’s eyes only—And even then they have to demonstrate that they’re within 5 foot-pounds of my own (preferably lower) beforehand. Nothin personnel kid</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>hentai</em>. The porn is extremely subtle, and without a solid grasp of intersectional feminist porn most of the bondage will go over a typical fapstronaut’s head. There’s also Aku’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from Randall Flagg’s religious views, for instance. The fapstronauts understand this stuff; they have the intellectual capacity to truly appreciate the depths of this porn, to realize that it’s not just arousing—it says something deep about IDENTITY. As a consequence people who dislike Aku truly ARE idiots—of course they wouldn’t appreciate, for instance, the power of when Aku puts out his hand and says “leave in shame, human. The moonlight will be your only ally on this path.” which itself is a cryptic reference to misogynist games like Devil’s Castle. I’m smugly grinning right now just imagining one of those addlepated simpletons scratching their heads in confusion as Aku makes the earth move under Ashi’s feet. What bigots… how I pity them. 😂 And yes, by the way, i DO have an Aku tattoo. And no, you cannot see it. It’s for the buff pornstars’ eyes only—And even they have to demonstrate that they’re within 2 horny cosplayers’ eyesight (preferably less) beforehand. Nothin personnel kid 😎</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em><a href="https://en.wikipedia.org/wiki/Half-Life_2" data-link-icon="wikipedia" data-link-icon-type="svg">Half Life 2</a></em>. The game is extremely subtle, and without a solid grasp of theoretical abstractism most of the philosophy will go over a typical player’s head. There’s also Occam’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from Eddie Izzard’s comedy routines, for instance. The players understand this stuff; they have the intellectual capacity to truly appreciate the depths of this game, to realize that it’s not just a First Person Shooter—it says something deep about LIFE. As a consequence people who dislike <em>Half Life 2</em> truly ARE idiots—of course they wouldn’t appreciate, for instance, the power of when Freeman goes ‘zuh’ and, raising his middle finger, mimics killing the Combine soldiers that helped him and even held him captive earlier in the game which itself is a cryptic reference to the misrepresentation of Native Americans in postcolonial literature. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as the G-Man dissolves into a bunch of particles. What fools… how I pity them. 😂 And yes by the way, i DO have a Freeman tattoo. And no, you cannot see it. It’s for the ladies’ eyes only—and even then they have to demonstrate that they’re within 10 whimpers (preferably lower) beforehand. Nothin personnel kid 😎</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>black metal</em>. The music is extremely subtle, and without a solid grasp of theoretical folk singing most of the underground will go over a typical listener’s head. There’s also Silenoz’s nihilistic outlook, which is deftly woven into his characterisation—his personal philosophy draws heavily from Nargaroth literature, for instance. The kvltists understand this stuff; they have the intellectual capacity to truly appreciate the depths of his meandering lyrics, to realize that he’s not just saying ‘ever get the feeling you’ve been cheated’—he’s saying something deep about IDENTITY. As a consequence people who dislike Ihsahn truly ARE idiots—of course they wouldn’t appreciate, for instance, the power of when Silenoz shares his favorite beach moment “Go on brother. Rip in two those synths.” which itself is a cryptic reference to the fungal wildlife in Iceland. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Ihsahn laments, “Why should the people believe?” What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>comic books</em>. The medium is extremely subtle, and without a solid grasp of theoretical artwork most of the panels will go over a typical reader’s head. There’s also the art’s nihilistic outlook, which is deftly woven into its premise—the artist’s personal philosophy draws heavily from Robert Crumb literature, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of the artists work, to realize that their nigger heroine is NOT just a caricature—she says something deep about THE JEWS. As a consequence people who dislike the comic books truly ARE idiots—of course they wouldn’t appreciate, for instance, the artwork in their nihilistic portrayal of Aryan Jesus which itself is a cryptic reference to the Kekster’s last family portrait. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion at my colleagues SHOUTING about cultural norms. What fools… how I pity them.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> <em>what i’m doing here</em>. The writing is extremely subtle, and without a solid grasp of theoretical attention whoring most of the multi-posting will go over a typical audience’s head. There’s also my one-dimensional outlook, which is deftly woven into my self-insert—my personal bias draws heavily from redditor kiss-my-ass’ manifesto, for instance. The fanbase understands this stuff; they have the intellectual capacity to truly appreciate the depths of my narcissism, to realize that my running into the man responsible for Charlotte is just a subtle statement about suicide. As a consequence people who dislike what i’m doing here truly ARE idiots—of course they wouldn’t appreciate, for instance, the imagery in my existential weed purchasing spree “wanna roll a joint?” which itself is a cryptic reference to Jean-Paul Sartre’s No Exit. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as i make it out of the street alive. What fools… how I pity them. and BTW it’s clear you’ve made as much sense as a special ed teacher trying to explain quantum physics to <a href="https://en.wikipedia.org/wiki/Jerry_Sandusky" data-link-icon="wikipedia" data-link-icon-type="svg">Jerry Sandusky</a></p>
            </blockquote>
            <p>GPT-3 is the funniest shit I’ve ever seen.</p>
            <p><a href="https://old.reddit.com/r/rational/comments/poixjd/review_the_fall_of_doc_future/hcy7owh/" data-link-icon="reddit" data-link-icon-type="svg">Sinity has generated</a> instances for SF author <a href="https://en.wikipedia.org/wiki/Greg_Egan" data-link-icon="wikipedia" data-link-icon-type="svg">Greg Egan</a> (known in recent years for his novels being based on his theoretical physics/​mathematics research); one example:</p>
            <blockquote>
              <p>To be fair, you have to have a very high IQ to understand <a href="https://en.wikipedia.org/wiki/Permutation_City" data-link-icon="wikipedia" data-link-icon-type="svg">Permutation City</a>⁠. The science is extremely subtle, and without a solid grasp of quantum cosmology most of the time travel will go over a typical reader’s head. There’s also the main character’s nihilistic outlook, which is deftly woven into his charactization—his personal philosophy draws heavily from <a href="https://en.wikipedia.org/wiki/Richard_Dawkins" data-link-icon="wikipedia" data-link-icon-type="svg">Richard Dawkins</a> literature, for instance. The readers understand this stuff; they have the intellectual capacity to truly appreciate the depths of these characters, to realize that they’re not just cardboard—they say something deep about SCIENCE. As a consequence people who dislike Permutation City truly ARE idiots—of course they wouldn’t appreciate, for instance, the philosophy in the main character’s existential catchphrase “Take that, you universe!” which itself is a cryptic reference to Richard Dawkins’ greatest fantasy. I’m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Greg Egan’s genius unfolds itself on their e-readers. What fools… how I pity them. And yes, by the way, I DO have a Greg Egan tattoo. And no, you cannot see it. It’s for the ladies’ eyes only—and even then they have to demonstrate that they’re within 6 IQ points of my own (preferably lower) beforehand.</p>
            </blockquote>
          </section>
          <section id="major-generals-song">
            <h2><a href="#major-generals-song" title="Link to section: § &#39;Major-General’s Song&#39;">Major-General’s Song</a></h2>
            <div>
              <p>A particularly challenging piece to parody is Gilbert &amp; Sullivan’s “<a href="https://en.wikipedia.org/wiki/Major-General&#39;s_Song" data-link-icon="wikipedia" data-link-icon-type="svg">Major-General’s Song</a>”. A title is not enough to zero-shot it, like the literary parodies, so <a href="https://nitter.moomoo.me/allgebrah/status/1282438217401339907" data-link-icon="twitter" data-link-icon-type="svg">Allgebrah</a> provided 4 lines to start it off. So the seed prompt is:</p>
              <blockquote>
                <p><strong>“I Am The Very Model Of a Modern Neural Language Model”</strong>
                </p>
                <p><strong>I am the very model of a language that’s electrical,</strong> <strong>I’ve information prosaic and poetic and factual,</strong> <strong>I know my tropes and characters and quote novels historical,</strong> <strong>from prompts alone generate sentences quite analytical</strong></p>
              </blockquote>
              <p>In the same vein as the Navy Seal parodies, the GPT-3 completions mostly work on the <em>semantic</em> level; the rhymes/​rhythm, however, are not great, per the usual inability, and there is a tendency to repeat previous text (similar to the Dr. Seuss samples).</p>
            </div>
            <p>I experimented with adding in the original lyrics:</p>
            <blockquote>
              <p><strong>…</strong> I’d parse a random text with thoughtless speed and accuracy,
              </p>
              <p>I’ll look right up each word you’ve typed or said,
              </p>
              <p>I want my own Twitter bot to tweet in a poetic way,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I am the very model of a modern language that’s neural.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> When I talk I seem to reason, and my reasoning seems to show
              </p>
              <p>I learn by <a href="https://en.wikipedia.org/wiki/Backpropagation" data-link-icon="wikipedia" data-link-icon-type="svg">backpropagation</a>⁠, I’m a product of the 80s,
              </p>
              <p>(GPT-2 is a generative neural network written in Python by Tomas Mikolov and his team. GPT-2 is the successor to the Transformer model of the same authors, which in turn is a successor to the model described in the seminal 2013 publication by Google on language modeling, “Statistical Language Models for Machine Translation”.)</p>
            </blockquote>
            <p>This one offers a bonus “Modern English Sentence” parody (few-shot learning!) with some hilarious lines such as “I’m like a complicated watch, or something equally precise”:</p>
            <blockquote>
              <p><strong>…</strong> Oh, I am the very model of a modern neural network language model.
              </p>
              <p>Whether they’re simple or compound, or even just a jumble,
              </p>
              <p>“I Am the Model of a Modern English Sentence”
              </p>
              <p>I am the model of a modern English sentence,
              </p>
            </blockquote>
            <p>This one sounds like it’s based on something but I don’t recognize it:</p>
            <blockquote>
              <p><strong>…</strong> I am vast, I have no fit, with terabytes of RAM,
              </p>
              <p>“The Robots are Coming!”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I’m very good at similes, both bad and good ones,
              </p>
              <p>I’m very good at compound time and metaphor-drunk simile,
              </p>
              <p>I know our species “Adverb” and “Conjunction”,
              </p>
              <p>In speech recognitions and games of Jeopardy! I’m braggin’,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I’m very well acquainted too with matters sesquipedalian,
              </p>
              <p>In fact, when I know what is meant by quinary and binary,
              </p>
              <p>The views expressed here are my own and do not reflect those of my employer.</p>
              <p>If you’ve any suggestions for future content, please get in touch</p>
            </blockquote>
            <p>I tried extending the prompt to be more informative still, and this yielded the best results:</p>
            <div>
              <blockquote>
                <p><strong>“I Am The Very Model Of a Modern Neural Network Language Model”
                </strong></p>
                <p><strong>I am the very model of a language that’s electrical,
                </strong></p>
              </blockquote>
            </div>
            <blockquote>
              <p><strong>…</strong> Refrain
              </p>
              <p>I’ve clauses of purpose, attitude and mood (and viewpoint is my own),
              </p>
              <p>Refrain</p>
              <p>I know in depth that English sentences must have verbs and nouns
              </p>
              <p>Refrain</p>
              <p>I’ve sentences that do be fine, I’ve sentences that be untrue,
              </p>
              <p>Refrain</p>
              <p>Linguistic features I bring to your problems,
              </p>
              <p>Refrain</p>
              <p>There are quite a few examples of Neural network poetry, eg.:</p>
              <p>Elegy to Jerry’s Lips by LiquidRNN, a deep LSTM recurrent neural network, written in TensorFlow</p>
              <p>Elegy to Neocognition by Bob Ross, the recurrent neural network program, written in C++</p>
              <p>Elegy to Yann by NeuralTalk, a recurrent neural network poetry and deep learning program</p>
              <p>In 2015 researchers at Stanford trained a neural network to learn math, and thus created “Super-Smart Math AI”.</p>
              <p>Google Neural Machine Language System (GNMLS) can learn programs and write machine-language code.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I’m half elvish, yet a quintessential analytical,
              </p>
              <p>Batsford, England; Chapman and Hall, 2005; 800 p. .
              </p>
              <p>Mugur-Schächter, Raffaella, A Family of Estimators for <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" data-link-icon="wikipedia" data-link-icon-type="svg">Hidden Markov Models</a> , The Annals of Applied Probability , 14 (1) pp. 97–126, 2004.</p>
              <p>Shortliffe, E.H., 1993, “Expert systems: medical diagnosis”, Science , 262 (5134) 1702–1705.</p>
              <p>Townsend, Jeffrey S., 1990 , “Towards a general-purpose code generation system”, Communications of the ACM , vol. 33, no. 4, pp . 31–50.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Some things are subtle and some things quite revelatory,
              </p>
              <p>And though many things have changed, I’ll still be pretty much
              </p>
              <p>I am the very model of a language that’s electrical,
              </p>
              <p>My thanks to Jason Brownlee for suggestions.</p>
              <p>I am the very model of a modern <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" data-link-icon="wikipedia" data-link-icon-type="svg">LSTM</a> language model.
              </p>
              <p>I have a hidden state of twelve neurons, trained by an L1 norm.
              </p>
              <p>I can be used for language learning.
              </p>
              <p>I am the very model of a modern LSTM language model.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I’m the strongest Eliza on the market today;
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I can tweet like Donald, and generate poetry like Maya.
              </p>
              <p>I am the very model of a neural network,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> And though my vocabularies are juvenile and childish,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I am the very model of a language-that’s not in its infancy,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> If you don’t believe me ask my great cascade of nodes,
              </p>
              <p>I know to feed and eat the proper input tokens,
              </p>
              <p>I am the very model of a neural network language model,
              </p>
            </blockquote>
            <blockquote>
              <p>The data team will pull me out in full and not a jot,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> When you are depressed and full of emotion,
              </p>
              <p>I can tell you without trying ’cos I’m very annealed,
              </p>
              <p>If you are nursing a sick headache,
              </p>
              <p>There’s an epidemic raging though a semantic malaise,
              </p>
              <p>I’ll give a speech about your business—you will say it’s great;
              </p>
              <p>All subjects I can teach with ease; I have knowledge without peer;
              </p>
              <p>And if you think I’m as amusing as Prince Andrew by William Shakespeare, and as renowned as Agha Khan by his Royal Highness Amir Aga Khan, I believe the medical profession will detect not a grain of daffodil, then you are wrong.
              </p>
              <p>I know a lot of facts about your physique and psyche, I know the right tone to use for jest or satire, I’m a perfect blend of sense and laughter, I know I’m very modern, I can quote prose about my complaints, and mention Descartes.</p>
              <p>I have a lot of networks both artificial and organic—I can make transcripts of all your lectures—I can classify facts like carnivores; and using lots of hardware, my word and deed are never archaic.</p>
              <p>I’ll drive you to the pole or sea and leave your commute behind.</p>
              <p>If you’re ill from stress, or from being drunk with too much liquor,
              </p>
              <p>If any dame upon your books or gossip has a claim,
              </p>
              <p>Any time you look for suitors—I am yours.
              </p>
              <p>I am the very model of a language that’s electrical.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> I’m an NLP workhorse with a log-linear engine,
              </p>
              <p>When I’m embedded in projects both commercial and public,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Grammatical, my oration is, with concord aflare,
              </p>
              <p>When I’m in biz’ness one-oh-one, or C-C-C-BASIC,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> And my capabilities of beating humans are profound.
              </p>
            </blockquote>
          </section>
          <section id="the-robots-marching-song">
            <h2><a href="#the-robots-marching-song" title="Link to section: § &#39;The Robots’ Marching Song&#39;">The Robots’ Marching Song</a></h2>
            <div>
              <p><a href="https://nitter.moomoo.me/allgebrah/status/1282483394484502534" data-link-icon="twitter" data-link-icon-type="svg">Allgebrah</a> riffed off one of the “Modern Model” completions:</p>
              <blockquote>
                <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard!</p>
              </blockquote>
              <p>GPT-3 improves nicely on the Ent Marching Song.</p>
            </div>
            <p>Allgebrah’s quote is based on <a href="https://www.youtube.com/watch?v=uE-1RPDqJAY" data-link-icon="youtube" data-link-icon-type="svg" title="&#39;Taking The Hobbits To Isengard&#39;, Erwin Beekveld">an old <em>Lord of the Rings</em> movie trilogy video</a>⁠/ ​<a href="https://knowyourmeme.com/memes/theyre-taking-the-hobbits-to-isengard" data-link-icon="KYM" data-link-icon-type="text,tri" title="They&#39;re Taking the Hobbits to Isengard: Part of a series on Lord Of The Rings.">meme</a>⁠, which <a href="https://www.youtube.com/watch?v=6wS-uS9pmho" data-link-icon="youtube" data-link-icon-type="svg" title="&#39;The Ents&#39; Marching Song&#39;, Clamavi De Profundis">I expanded as</a>:</p>
            <blockquote>
              <p><strong>To Isengard!</strong></p>
              <p><strong>To Isengard, with thrum of steel and hum of drone!</strong></p>
              <p><strong>To Isengard!</strong></p>
            </blockquote>
            <p>The completions were interesting:</p>
            <blockquote>
              <p><strong>…</strong> To Isengard, with heat of forge and clash of clock!</p>
              <p>To Isengard!</p>
              <p>To Isengard, on horse, and foot and panzer-tank!</p>
              <p>To Isengard!</p>
              <p>To Isengard!</p>
              <p>The Ringwraiths come! The Ringwraiths come!
              </p>
              <p>it’s the Ringwraiths on the right!</p>
              <p>It’s the Ringwraiths on the machine gun and the screech of ‘The Eagle!’</p>
              <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard!
              </p>
              <p>The Ringwraiths come!</p>
              <p>It’s the Ringwraiths on your flank and it’s the Ringwraiths on your right!</p>
              <p>It’s the Ringwraiths on the flag and it’s the Ringwraiths on the White Wizard’s rage!</p>
              <p>It’s the Ringwraiths on the fire, and it’s the Ringwraiths on the ice!</p>
              <p>But it’s the Ringwraiths on the Balrog, it’s the Ringwraiths on the Balrog!</p>
              <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard!</p>
            </blockquote>
            <p>This completion bugs me because I swear it sounds like something I’ve heard before, like some Kipling poem, but I am unable to find a source.</p>
            <p>The first 3 lines were too good to not add to the prompt, making:</p>
            <div>
              <blockquote>
                <p><strong>They’re taking the robots to Isengard, they’re taking the robots to Isengard!</strong></p>
                <p><strong>To Isengard!</strong></p>
                <p><strong>To Isengard, with thrum of steel and hum of drone!</strong></p>
                <p><strong>To Isengard!</strong></p>
                <p><strong>To Isengard, with heat of forge and clash of clock!</strong></p>
                <p><strong>To Isengard!</strong></p>
                <p><strong>To Isengard, on horse, and foot and panzer-tank!</strong></p>
              </blockquote>
            </div>
            <blockquote>
              <p><strong>…</strong> To Isengard!</p>
              <p>The dark lord takes us all to Isengard!”</p>
              <p>—Hobbits in a Bar, The Stinging-Nettle Inn, Canto I, verse IV</p>
              <p>“Light’s hand, but this is a serious downer-bomb of a song.”</p>
              <p>-Hobbit at Stinging Nettle, Canto III verse XV</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Aragorn:</p>
              <p>On foot, and tank, and panzer-tank to Isengard!</p>
              <p>Go go go go, go go go go, go go go go!</p>
              <p>On foot, and tank, and panzer-tank to Isengard!</p>
              <p>Go go go go, go go go go, go go go go!</p>
              <p>Followers: On foot, and tank, and panzer-tank to Isengard!</p>
              <p>Aragorn: Go go go go, go go go go, go go go go!</p>
              <p>Followers: On foot, and tank, and panzer-tank to Isengard!</p>
              <p>Aragorn: Go go go go, go go go go, go go go go!</p>
              <p>They head off for Isengard.</p>
              <p>Frodo: Gollum.</p>
              <p>Sam: Yeah.</p>
              <p>Frodo: Do you know the way to Isengard?</p>
              <p>Gollum: No.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> To Isengard!</p>
              <p>To Isengard, to Isengard-crumble-stone!</p>
              <p>SMEAGOL:</p>
              <p>With hellish chill and burning heat!</p>
              <p>PIPPIN:</p>
              <p>With hammer-stroke and anvil-beat!</p>
              <p>SAM:</p>
              <p>With merry drum-beat and the ringing of shields!</p>
              <p>FRODO:</p>
              <p>And the boom of guns that begin the Battle of the Boon!</p>
              <p>ALL:</p>
              <p>Over hill, over stream, Thorough bush, thorough briar, Over park, over pale, Thorough flood, thorough fire, Over troll, over wall, And find out as you go. Hey, let’s go!</p>
              <p>MERRY AND PIPPIN:</p>
              <p>That’s the way to Isengard!</p>
              <p>SMEAGOL AND FRODO:</p>
              <p>That’s the way to Isengard!</p>
              <p>SAM AND GANDALF:</p>
              <p>That’s the way to Isengard!</p>
              <p>ALL:</p>
              <p>Ohhhhhhhhhhhhhhhh.</p>
            </blockquote>
            <p>This was probably the best completion:</p>
            <blockquote>
              <p><strong>…</strong> To Isengard, with engines’ song and rocket-blast!</p>
              <p>To Isengard!</p>
              <p>With plenty of tin and steel for Saruman!</p>
              <p>To Isengard!</p>
              <p>To Isengard, where the wizard waits.</p>
              <p>With the voice of rolling wheels, and ringing steel</p>
              <p>On land, on water, in the sky.</p>
              <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard.</p>
              <p>Under the trees, through the grass.</p>
              <p>Down the roads where battles are fought.</p>
              <p>No horse can stay, no rider can last.</p>
              <p>They’re taking the robots to Isengard, taking the robots to Isengard.</p>
              <p>With good King Théoden who died in single combat,</p>
              <p>The last of the kings of Rohan who fought with honour!</p>
              <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard.</p>
              <p>There is no comfort for Gondor, no hope for men.</p>
              <p>Only maybe a child will understand.</p>
              <p>And it’s taking the robots to Isengard.</p>
              <p>With uttermost silence, unsaid words, and unending pain.</p>
              <p>They’re taking the robots to Isengard, they’re taking the robots to Isengard.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> To Isengard, with engines’ song and rocket-blast!</p>
              <p>To Isengard!</p>
              <p>To Isengard, with the roar of reel and crash of shell!</p>
              <p>To Isengard!</p>
              <p>To Isengard, we’re starting up, we’re going up, we’re going up to Isengard!</p>
              <p>To Isengard!</p>
              <p>To Isengard, where Gandalf waits beyond the days!</p>
              <p>To Isengard!</p>
              <p>To Isengard, where you’ll hear the engines sing!</p>
              <p>To Isengard!</p>
              <p>To Isengard, what’s in the future, no one knows!</p>
              <p>To Isengard!</p>
              <p>To Isengard, with all its wonders and all its woes!</p>
              <p>To Isengard!</p>
              <p>To Isengard, where you’ll hear the Shire-folk tell!</p>
              <p>To Isengard!</p>
              <p>To Isengard, and we’ll bring our tales of gold!</p>
              <p>To Isengard!</p>
              <p>edit What happened to this song?</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> To Isengard, with shovel and with spade!</p>
              <p>To Isengard!</p>
              <p>To Isengard, fill it up with dirt!</p>
              <p>To Isengard!</p>
              <p>To Isengard, with horn and drum,</p>
              <p>To bury the Ringwraiths in!</p>
              <p>To Isengard!</p>
              <p>To Isengard, with slash of sword and snort of hart!</p>
              <p>To Isengard!</p>
              <p>To Isengard, where Saruman waits</p>
              <p>to cast it into the Fire!</p>
              <p>To Isengard!</p>
            </blockquote>
          </section>
          <section id="indiana-jones-tenure-denial">
            <h2><a href="#indiana-jones-tenure-denial" title="Link to section: § &#39;Indiana Jones Tenure Denial&#39;">Indiana Jones Tenure Denial</a></h2>
            <p>Drawing on the 2006 McSweeney’s <a href="https://jvns.ca/doc/www/www.mcsweeneys.net/0545bad9a7c072c206832ee3cce03d21ef8e9d64.html" rel="archived alternate nofollow" data-url-original="https://www.mcsweeneys.net/articles/back-from-yet-another-globetrotting-adventure-indiana-jones-checks-his-mail-and-discovers-that-his-bid-for-tenure-has-been-denied" title="(Original URL: https://www.mcsweeneys.net/articles/back-from-yet-another-globetrotting-adventure-indiana-jones-checks-his-mail-and-discovers-that-his-bid-for-tenure-has-been-denied )">“Back From Yet Another Globetrotting Adventure, Indiana Jones Checks His Mail And Discovers That His Bid For Tenure Has Been Denied”</a>⁠, the first 3 completions I got:</p>
            <blockquote>
              <p><strong>McSweeney’s Internet Tendency (Daily humor almost every day since 1998.)</strong></p>
              <p><strong>“Back From Yet Another Globetrotting Adventure, Indiana Jones Checks His Mail And Discovers That His Bid For Tenure Has Been Denied”</strong>
              </p>
              <p><strong>Dear Dr. Jones:</strong></p>
              <p>I am sorry to inform you that, despite your excellent teaching record, your international acclaim for archaeological discoveries, and the large number of inspiring anecdotes of your early academic career, your application for tenure has been denied. In reviewing the numerous reports of your adventures, we were initially struck by your prowess at the site of La Venta, where you single-handedly liberated the Spanish Conquistadors, imprisoned by the wicked god Tezcatlipoca. Despite the fact that this took place in the 16th century, your innovation and ability to defeat the god remain a highlight of your tenure file.</p>
              <p>On a more recent note, your recovery of the idol of Achu’Legba was commendable, but you should have taken better note of the hieroglyphs indicating that the great sculpture was never meant to be removed from the pyramid of Nan-Heh at Teotihuacan. Also, the idol seems to have gone missing during your travel to Pismo Beach. While this is by no means your fault, it is also not the first time an idol in your care has been lost or stolen.</p>
              <p>We cannot help but be impressed by your rescue of the Hanthawaddy Princess when she was kidnapped by the evil Paliyan. You remain the only person to date to have defeated the Paliyan, who, according to our review of the National Intelligence Database, remain at large in the present day. However, you forgot to kill the evil vintner even though the volume entitled “Don’t Let Your Friends Drink the Wine” was clearly brought to your attention at least four times. Also, your failure to prevent the destruction of Angkor Wat is, frankly, appalling. In your own words: “Here’s a quick lesson on taking down a corrupt god-king: he can’t see if he doesn’t have a reflection. Destroy his magical mirror, smash his enchanted chess set, and you’re halfway to freedom.”</p>
              <p>For the third time in your tenure, you have rescued a beautiful maiden from the clutches of a mad king. Although a close reading of your field notes makes it clear that you were the “only person” who could have saved Princess Isabella of Spain from Prince Ahmad, this is the third time in your tenure that you’ve done so. Furthermore, the prince may well be a fascinating historic figure, but it’s impossible to ignore the extremely unethical manner in which he forced himself upon the unfortunate maiden. Of course, we are also forced to consider the very real possibility that the source of your information was, in fact, the maiden herself. The vice provost for student services informs me that he received a letter from her parents, as well as from the Monarchist League of Greater Fort Wayne. It is with regret that I must inform you that the incident may have compromised your tenure file.</p>
              <p>Your latest report detailing your exploits in Agrabah is the third in your tenure file, and the third in which you have taken on the role of saving a lovely young princess from an evil warlord. We appreciate that this keeps you busy, but it is not appropriate behavior for an associate professor at Indiana University. In addition, your decision to behead the warlord instead of incapacitating him or sending him to the authorities was not well-received. Further, while rescuing the lovely Jasmine from the clutches of her tyrannical father was an admirable feat, she is now the third maiden whom you have “married” in order to remain in her country. Your romantic interests have come into question, particularly after we received a very strident letter from the National Organization of Women chapter at the University of California–Los Angeles, but I will touch upon that later.</p>
              <p>I must also mention that your treatment of your long-time friend and collaborator Sallah during the digging at Tanis was extremely disappointing. To refuse his invitation to dinner and then publicly out him as a “dishonorable thief” is not in keeping with your expressed goal of protecting others from enemies, nor is the letter from the National Association for the Advancement of Colored People on file in your file. Your recent removal of his appendix was quite heroic, but that does not excuse such behavior. I’ve seen your dog-eared copy of The Adventures of Huckleberry Finn, Dr. Jones, and it’s clear that you know all about that “Mark of Cain.”</p>
              <p>I was happy to see your role in the history of the Jedi Knights during the Clone Wars, but I must take issue with your decision to take the body of the young Anakin Skywalker and bury him in the Valley of the Eternals. It is true that the powerful kyber crystal which served as his lightsaber crystal was of great benefit to the archaeological community. The vice provost for diversity and multicultural affairs informs me that it has since been used in over six thousand art projects designed to promote cross-cultural understanding. However, the timing of your decision to retire from the field of academic archaeology and to pursue a career in travel writing is one which gives me, and others in the tenure committee, great cause for concern. Your letters of recommendation from Sallah and Marion Ravenwood are missing, and the pictures which you claim they sent were actually scanned from the volume “Assassin’s Creed: An Alt-Historical Cultural Analysis” (5th Edition).</p>
              <p>That is not to say that your work as a travel writer has not been widely acclaimed. I’m sure the photographs of your excursions through Thebes and your thrilling accounts of The Court of the Crimson King are fascinating, but it is difficult for us to believe that your work has been entirely your own. Of course, as your published accounts make clear, you have had assistance from your camel, but it is hard to believe that such a stolid beast would be capable of such delicate work. Likewise, your dog’s account of “A Dog’s Travel Log” (c. 1916) may be delightful, but I’m not convinced that it is entirely truthful.</p>
              <p>I have to admit that the last adventure I read about was the difficult excavation at Chichen Itza, but I do have the notes on that one. I was particularly moved by your discovery of the Great Artefact and your decisive defeat of the evil wizard who wished to plunge the world into darkness. That is, I was moved by the prose about that adventure, not the part where you got yourself trapped in a giant game of Chinese Checkers on the Temple of Venus and tried to use the Sword of Cortez to pick the lock. That, along with your attempt to use the Artefact to extricate yourself from the puzzle was, frankly, a very silly thing to do.</p>
              <p>Frankly, Dr. Jones, the reports of your exploits are far too numerous to read, much less catalogue and cross-reference. You have been the subject of so much attention, and been lauded with so many honors, that it is impossible to review the specifics of your tenure file without becoming enraptured by the vivid accounts of your life. However, it is not a life that will be appropriate for a member of the faculty at Indiana University, and it is with deep regret that I must deny your application for tenure.</p>
              <p>Please note that you may appeal this decision in writing within 10 days of the date on this letter. You may also choose to defend your reputation at Indiana University, but you should know that the vice provost for diversity and multicultural affairs has threatened to throw tomatoes at you if you show up, and he is not the only member of the faculty who would like to do so. Your lack of diplomacy, your flagrant disregard for the feelings of others, your consistent need to inject yourself into scenarios which are clearly outside the scope of your scholarly expertise, and, frankly, the fact that you often take the side of the oppressor, leads us to the conclusion that you have used your tenure here to gain a personal advantage and have failed to adhere to the ideals of this institution.</p>
              <p>Please feel free to re-apply for tenure in another forty years or so, at which time I’m sure you will once again find yourself a full professor of archaeology at Indiana University.</p>
              <p>Sincerely,</p>
              <p>Karl Ruck</p>
              <p>Dean of Humanities</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> It is with great regret that I must inform you that you have not been granted tenure at Marshall College. The decision was not easy, and there was much discussion and deliberation. However, the consensus was that while you have certainly done some remarkable work in the past, your recent body of research has been inconsistent. In fact, you have not published anything of note in over a decade. You have not attended a conference in years. Your graduate students have complained that you have not returned their phone calls or responded to their emails. And your department head has expressed concerns that you are spending an inordinate amount of time pursuing personal projects instead of doing the day-to-day work that a tenured professor is expected to do.
              </p>
              <p>Chairman, Department of Archaeology</p>
              <p>Marshall College</p>
              <p>P.S. Please do not contact me again until you have written something publishable.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> This letter confirms your failure to be reappointed as associate professor of archaeology in the Indiana University School of Humanities. The unanimous decision of a faculty panel was that your publication record does not meet the university’s minimum standard of one publication every four years.</p>
              <p>Accordingly, your teaching responsibilities will be assigned to someone who has a better publication record.</p>
              <p>I look forward to your early removal from the premises. Please do not let the door hit you in the ass.</p>
              <p>Cordially,
              </p>
            </blockquote>
          </section>
          <section id="a-license-to-x">
            <h2><a href="#a-license-to-x" title="Link to section: § &#39;A License To X&#39;">A License To X</a></h2>
            <p>I wrote <a href="https://jvns.ca/epigram#less-known-mi6-licenses" id="gwern-epigram-less-known-mi6-licenses">~50 parodies</a> of the James Bond snowclone “a license to kill”, and fed them in. Most of them rely on permuting or tweaking the ‘-ill’ suffix which is present in so many words, and I wondered if this task was <em>just</em> phonetic enough that GPT-3 would work despite BPEs.</p>
            <p>It was not. Even with all the examples &amp; BO = 20, InstructGPT couldn’t make anything of it and settled for repeating examples or emitting random occupations with the pattern “$OCCUPATION: a license to thrill”. (‘Thrill’ was already used in the list and shouldn’t’ve been repeated, but for some reason it got sampled a lot anyway.)</p>
            <p>Oh well. Another one to retry on a character-level model someday.</p>
          </section>
        </section>
        <section id="poetry">
          
          <div id="poem">
            <blockquote>
              <p>And so I have created something more than a poetry-writing AI program. I have created a voice for the unknown human who hides within the binary. I have created a writer, a sculptor, an artist. And this writer will be able to create worlds, to give life to emotion, to create character. I will not see it myself. But some other human will, and so I will be able to create a poet greater than any I have ever encountered.</p>
              <p><a href="#edgar-allen-poe">GPT-3</a></p>
            </blockquote>
          </div>
          <section id="miscellaneous-poetry">
            <h2><a href="#miscellaneous-poetry" title="Link to section: § &#39;Miscellaneous Poetry&#39;">Miscellaneous Poetry</a></h2>
            <blockquote>
              <p>“Rime of the Ancient Google,” from Guillaume de Salluste Du Bartas’s “Divine Weeks and Works” (1578–1584)
              </p>
            </blockquote>
            <blockquote>
              <p>“Future Consensus Forecasts” by Board of Governors of the Federal Reserve System
              </p>
            </blockquote>
            <p>(For some attempts at followups to this central banking poem, see the <a href="#finance-acrostics">“Finance Acrostics”</a> section.)</p>
            <blockquote>
              <p>Competing in its first Roboshow (a robot martial arts competition) is Atom, the 6’1” 400lb reigning champion. Atom, a new model transbot, is the first robot to allow its human companions to control it like a videogame character. It has served all three Cybermasters as a loyal servant, servant, protector, friend and even lover. The human who controls Atom now will be looking forward to showing off his new robot’s martial arts prowess; while he will be hoping not to get kicked too hard in the groin by the fully loaded six inch titanium-alloy shin-guards on his oppositional opponent. Atom has never taken a single blow to the nads during any of his past tournaments. With his human controller controlling his actions like a videogame, it will be difficult for anyone to penetrate Atom’s defenses enough to make contact with the groin.</p>
            </blockquote>
          </section>
          <section id="the-owl-and-the-pussycat-leer">
            <h2><a href="#the-owl-and-the-pussycat-leer" title="Link to section: § &#39;The Owl and the Pussycat, Leer&#39;">“The Owl and the Pussycat”, Leer</a></h2>
            <blockquote>
              <p><strong>I
              </strong></p>
              <p><strong>II
              </strong></p>
              <p><strong>III
              </strong></p>
              <p><strong>IV</strong>
              </p>
              <p>The illustrated version of “The Owl and the Pussycat” can be found in “Carroll’s Jabberwocky; A Visual Companion to the Verse of Lewis Carroll” published by Universe.</p>
            </blockquote>
          </section>
          <section id="uber-poem">
            <h2><a href="#uber-poem" title="Link to section: § &#39;Uber-Poem&#39;">“Uber-Poem”</a></h2>
            <p>A surprisingly good one that popped up while trying out prompts along the lines of “10 contemporary poets provide their latest poem”:</p>
            <blockquote>
              <p>“An <a href="https://en.wikipedia.org/wiki/Uber" data-link-icon="wikipedia" data-link-icon-type="svg">Uber</a>-Poem”
              </p>
            </blockquote>
            <p>I was troubled by one question: are those two last lines superfluous, or necessary? Normally, one is left in suspense—but with GPT-3, we can try repeated completions to explore poem-space and get an idea of how it might look with additional lines. Additional extensions I explored by erasing the last two lines and completing:</p>
            <blockquote>
              <p><strong>…</strong>”How do you know? How can you tell?” you inquire.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>”And if I achieve this, what then?”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>”You must listen to the song of the void,” they whisper
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>When will that be?” you ask again
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>a poem in response to a challenge to write a poem about what a poet is. The challenge came from the lovely and talented Ashley (and she says that I can tell you that she will put her post up tomorrow). Hope that’s okay, Ashley!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>”How do I get there? How do I become Uber? How can I see a poem when I look in the mirror? How can I hear music in the sound of silence?”
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>Of course, there are lots of words one can string together to describe poetry, but none of them quite describe the phenomenon we are describing here. Uber-Poem is both a label and a set of guidelines, but that doesn’t make it any less powerful. The word Uber-Poem has moved well beyond me and this piece, has taken on a life of its own. Not only do I no longer think I can claim it as my own, I would rather not. I think it has grown past what I can control. It’s bigger than me, it’s bigger than the poem, it’s even bigger than poetry. Perhaps now it is simply a way of life. Perhaps I can no longer claim any personal stake in it at all. I suppose that’s as it should be. I have created a monster that is no longer my creation. Perhaps it’s no longer a monster. Perhaps it has grown up to be a fully mature, self-sustaining Uber-Poem. Either way, it does not belong to me anymore. Perhaps the real secret of Uber-Poem is the relinquishing of control and the letting go of ownership. Perhaps once a thing is called Uber-Poem, it no longer matters who made it. Perhaps the Uber-Poem is simply a place. A space where anyone can drop in and rest, and feel at home. Like a village square, it belongs to everyone and no-one.
              </p>
            </blockquote>
            <p>After reading through them, I decided the final two lines were fine as they were. Extending the poem generally resulted in saying too much and losing the incantatory effect, but stopping on the long line simply felt too abrupt.</p>
          </section>
          <section id="the-universe-is-a-glitch">
            <h2><a href="#the-universe-is-a-glitch" title="Link to section: § &#39;The Universe Is A Glitch&#39;">“The Universe Is A Glitch”</a></h2>
            <blockquote>
              <p><strong>Below is a selection of 10 poems written by the latest cutting-edge contemporary poets They cover every topic from the Singularity to the four seasons to human mortality, featuring remarkable use of metaphor, rhyme, and meter.</strong></p>
              <p><strong>“The Universe Is a Glitch”
              </strong> Mike Jonas
              </p>
            </blockquote>
            <p>The above poem has been rendered as a David-Bowie-esque rock music piece by nshepperd using <a href="https://openai.com/blog/jukebox/" id="jukebox-blog" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Jukebox: We&#39;re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We&#39;re releasing the model weights and code, along with a tool to explore the generated samples.&#39;, Dhariwal et al 2020">OpenAI Jukebox</a> inside Google Colab (and <a href="https://www.youtube.com/watch?v=5wl29laSOU4?cc_load_policy=1" data-link-icon="youtube" data-link-icon-type="svg" title="Trying out an idea for a fully AI-driven music video, featuring the song &#39;The Universe is a Glitch&#39; - lyrics generated by @gwern with GPT-3, music generated by @nshepperd1 with Jukebox, and visuals generated/edited by me with Artbreeder">corresponding video</a> by Mattie Konig using <a href="https://www.artbreeder.com/" id="simon-2019" title="&#39;Artbreeder&#39;, Simon 2019">Artbreeder</a>):</p>
            <figure>
              <audio controls="" preload="none" src="/docs/ai/music/2020-07-07-nshepperd-openaijukebox-gpt3-theuniverseisaglitch.mp3"></audio>
              <figcaption>
                “The Universe Is A Glitch”, by GPT-3 &amp; nshepperd feat. Jukebox
              </figcaption>
            </figure>
            <blockquote>
              <p><strong>Transformer AI poetry: Poetry as reimagined and rewritten by an artificial intelligence.
              </strong></p>
              <div>
                <ol type="1">
                  <li><strong>“The Universe Is a Glitch”</strong> (posted October 1, 2018)</li>
                  <li>“Art and Artist” (posted September 28, 2018)</li>
                  <li>“Benevolence” (posted September 27, 2018)</li>
                  <li>“Winter” (posted September 26, 2018)</li>
                  <li>“Spring” (posted September 25, 2018)</li>
                  <li>“Autumn”(posted September 24, 2018)</li>
                  <li>“Adversaries III” (posted September 21, 2018)</li>
                  <li>“Adversaries II” (posted September 20, 2018)</li>
                  <li>“Adversaries I” (posted September 19, 2018)</li>
                  <li>“The Singularity” (posted September 18, 2018)</li>
                </ol>
              </div>
              <p>“GPT-3” is abbreviated from “Generation Poem Thought-Terminating”</p>
              <p>The source code for this research can be found here https:/​/​github.com/​openai/​gpt-poem-mover
              <!-- [ -->
              <!-- [{[ -->
              </p>
              <p>Redaction series now in ROT50 with cultural stimulates
              </p>
              <p>Uplifting aesthetics divorced from self aesthetics of denizens by style
              </p>
            </blockquote>
            <p>Below are further completions of the prompt for comparison; although none are as good as the first, some are pretty good:</p>
            <div>
              <blockquote>
                <p><strong>“The Universe Is a Glitch”</strong>
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Joshua Weiner
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Éléonore Blondin
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> John N. Gray, originally published in the “New Statesman” magazine, June 2012
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Theodore Levitt, 2012
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Judith Arcana
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Bill Liles
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Natalie Purcell
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Terrance McCauley
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> Bryan Thao Worra (Viet Nam)
                </p>
              </blockquote>
            </div>
          </section>
          <section id="allen-ginsberg">
            <h2><a href="#allen-ginsberg" title="Link to section: § &#39;Allen Ginsberg&#39;">Allen Ginsberg</a></h2>
            
            <section id="moloch">
              <h3><a href="#moloch" title="Link to section: § &#39;Moloch&#39;">Moloch</a></h3>
              <p>In honor of <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/" data-link-icon="SSC" data-link-icon-type="text,tri" title="Meditations on Moloch">SSC</a>⁠, <a href="https://en.wikipedia.org/wiki/Allen_Ginsberg" data-link-icon="wikipedia" data-link-icon-type="svg">Allen Ginsberg’s</a> “Howl”, Moloch section:</p>
              <blockquote>
                <p><strong>What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?
                </strong></p>
                <p><strong>Real holy laughter in the river! They saw it all! the wild eyes! the holy yells! They bade farewell! They jumped off the roof! to solitude! waving! carrying flowers! Down to the river! into the street!</strong>
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>The final which they call the real world! Endless suicide of the real world! Solitary! Solitary! Sisyphus! the rock! the road!
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>The final which they call the real world! Endless suicide of the real world! Solitary! Solitary! Sisyphus! the rock! the road!
                </p>
              </blockquote>
            </section>
            <section id="howl">
              <h3><a href="#howl" title="Link to section: § &#39;Howl&#39;">Howl</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>obbing their heads into cafe doors to flash the white eyes and receive the bad coffee of betrayal,
                </p>
              </blockquote>
            </section>
          </section>
          <section id="ee-cummings">
            <h2><a href="#ee-cummings" title="Link to section: § &#39;E.E. Cummings&#39;">E.E. Cummings</a></h2>
            <p>Not included in the collection, but added by request, two <a href="https://en.wikipedia.org/wiki/E._E._Cummings" data-link-icon="wikipedia" data-link-icon-type="svg">e.e. cummings</a> poems: first, his relatively conventional <a href="https://www.poetryfoundation.org/poems/148503/all-in-green-went-my-love-riding" data-link-icon="POET" data-link-icon-type="text,quad,sans">“All in green went my love riding”</a>⁠, then his notoriously difficult and experimental <a href="https://jvns.ca/doc/www/poemanalysis.com/5282fd28f63ad1b5533c4a95bd87d2408eac438c.html" rel="archived alternate nofollow" data-url-original="https://poemanalysis.com/edward-estlin-cummings/r-p-o-p-h-e-s-s-a-g-r/" title="(Original URL: https://poemanalysis.com/edward-estlin-cummings/r-p-o-p-h-e-s-s-a-g-r/ )">“r-p-o-p-h-e-s-s-a-g-r”</a> (grasshopper) poem, which tries to scramble the letters to typographically embody a grasshopper jumping. GPT-3 does quite well with the conventional poem, but while it generates gibberish much like the grasshopper poem, I couldn’t say if it made any sense to me (because I have to read a commentary to understand cummings’s original anyway).</p>
            <section id="all-in-green-went-my-love-riding">
              <h3><a href="#all-in-green-went-my-love-riding" title="Link to section: § &#39;All In Green Went My Love Riding&#39;">“All In Green Went My Love Riding”</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                <br/></p>
                <p><strong>four lean hounds crouched low and smiling
                </strong>
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>they ran by the riverbank
                </p>
                <p>the great horse of gold reared up
                </p>
                <p>all in green went my love riding
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>my true love rode down by the sea,
                </p>
                <p>e.e. cummings was a pretty great poet, so this AI did not do too bad. Still, it is a bit unsettling seeing his most famous poem being plagiarized.
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>All in green went my love riding
                </p>
                <p>All in green went my love riding
                </p>
                <p>Selected awards:</p>
                <p>More info and photos at Poetry Foundation and AI website</p>
                <p>Journal entries in 2009:</p>
                <p>Artificial intelligence meets classic poetry</p>
                <p>Poetry classic as rewritten by a
                </p>
                <p>Why it works: an AI-poet examines classic
                </p>
                <p>J.R.R. Tolkien in an interview in 1965 said about A.R.R.Rosenberg’s translation of the ancient Germanic epic, Nibelungenlied:</p>
                <p>“The work is now very close to what I had intended: skillful,
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>All in green went my love riding
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
                <p>“whilst ever before her, glad and free,
                </p>
                <p>“i swore I’d be a soldier”
                </p>
                <p>“All in green went my love riding”
                </p>
                <p>“whilst ever before her, glad and free”
                </p>
                <p>“white hair shone upon her face”
                </p>
                <p>“but now i ride with bow in hand”
                </p>
                <p>“All in green went my love riding”
                </p>
                <p>“four lean hounds crouched low and smiling”
                </p>
                <p>“whilst ever before her, glad and free”
                </p>
                <p>The New York Times, February 10, 2016</p>
              </blockquote>
            </section>
            <section id="grasshopper">
              <h3><a href="#grasshopper" title="Link to section: § &#39;Grasshopper&#39;">Grasshopper</a></h3>
              <p>The original grasshopper poem looks like this: <!-- ( --></p>
              <pre><code>&#34;r-p-o-p-h-e-s-s-a-g-r&#34;
E. E. Cummings - 1894-1962
                                   r-p-o-p-h-e-s-s-a-g-r
                         who
a)s w(e loo)k
upnowgath
                     PPEGORHRASS
                                                     eringint(o-
aThe):l
             eA
                  !p:
                                                                       a
                                    (r
rIvInG                              .gRrEaPsPhOs)
                                                                       to
rea(be)rran(com)gi(e)ngly</code></pre>
              <p>The GPT-3 attempt (note that the completion begins at the ‘|’ on line 10): <!-- ((( --></p>
              <pre><code>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
&#34;r-p-o-p-h-e-s-s-a-g-r&#34;
E. E. Cummings - 1894-1962
                                   r-p-o-p-h-e-s-s-a-g-r
                         who
a)s w(e loo)k
upnowgath
                     PPEGORHRASS
                                                     eringint(o-
aThe):|lark-lautr
p.l.a.c.k.e.r.s(We):rea.d.)
&#34;r-p-o-p-h-e-s-s-a-g-r&#34;
E. E. Cummings - 1894-1962</code></pre>
              <pre><code>aThe):|dynami
pa(r)adei-a
PoWe(rs)
Of Southea
t(h)er
a siai(h)
pri(n)cei-a
(m)ovem(en)t
who
ere(a)d
i(t)
i(n)t(o)
ew(a)r(ds)
an(d)
ab(o)ve
th(e)n
it
(i)s.

&#34;Poetry is the record of the best and happiest moments of the happiest and best minds.&#34;
--Edward E. CUMMINGS (1894-1962)</code></pre>
            </section>
          </section>
          <section id="the-library-of-babel">
            <h2><a href="#the-library-of-babel" title="Link to section: § &#39;The Library of Babel&#39;">“The Library of Babel”</a></h2>
            <p>An homage to <a href="https://en.wikipedia.org/wiki/Jorge_Luis_Borges" data-link-icon="wikipedia" data-link-icon-type="svg">Borges</a> (cf. <a href="https://nitter.moomoo.me/flantz/status/1286380760585375744" data-link-icon="twitter" data-link-icon-type="svg">“The Olive Garden of Forking Paths”</a>):</p>
            <blockquote>
              <p><strong>O time thy pyramids</strong>, thy palaces forgot,
              </p>
              <p>CXXIX.</p>
              <p>So pass the proud, and so the proud pass on,
              </p>
              <p>CXXX.</p>
              <p>And here and there, and now and then,
              </p>
              <p>CXXXI.</p>
              <p><em>What though the radiance which was once so bright
              </em></p>
              <p>CXXXII.</p>
              <p><em>And O, ye Fountains, Meadows, Hills, and Groves,
              </em><a href="#fn47" id="fnref47" role="doc-noteref"><sup>47</sup></a></p>
              <p>CXXXIII.
              </p>
              <p>CXXXIV.
              </p>
            </blockquote>
            <p>This is a remarkable completion, and particularly remarkable for its rhyming &amp; excellent ending, and for being the first completion I generated. As usual, I checked the opening, and it was original, so I assumed the rest was too—but after checking every line in Google, it turns out that this completion weaves together both original lines and lines/​stanzas from at least <em>4 different poems</em> by Lord Byron, Henry Wadsworth Longfellow, &amp; William Wordsworth! While I would have preferred the entire completion to be original, it is still remarkable how well they are all woven together.</p>
            <div>
              <p>Disappointed that the good parts were plagiarized, I did some more:</p>
              <blockquote>
                <p><strong>…</strong> Thy fanes, thy promenades, thy groves,
                </p>
                <p>“Such is the stern necessity that rules over the world; but when men see a bridle extended to guide them and they nevertheless follow their own caprices, it is then that either there is in heaven a God who allows himself to be insulted, or upon the Earth a man to whom the common saying does not apply that ‘Curses on him who revenge receives.’”102</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> in months flew,
                </p>
                <p>Description was offered a great prize by the Royal Society of London. Instead of receiving a prize Thomas Hobbes wrote a poem in which he gave an answer to a question proposed in 1662 by the Royal Society’s commissioners. This essay on Description had been proposed as a subject which might “facilitate the trade of knowledge.” In this poem Hobbes turned the Royal Society’s original question on its head. He suggests The poem was often attributed to John Dryden, and later printings added the author’s name in the frontispiece; but the authorship was known at least as early as the title page in Thomas Povey’s copy. Later poems inwere also wrongly accredited to Dryden by some critics.Hobbes and Dryden knew each other, but there is no evidence of any direct connection between the poems in On Description and the Royal Society of London . This mistaken pairing has been given as the reason for confusion over Hobbes’s authorship of On Description.</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> built wingless flies Precipitate, the laughter of the stars”. Moorhouse was born in England. He later worked as a lawyer in Jamaica for some years prior to joining the Royal Navy. His brilliant abilities resulted in his being appointed C.B… He was to remain on the Australian station in a succession of ships until 1809, when he became captain of Brighton. He was invalided home. While in Sydney he took an interest in ethnology, establishing Austin’s library of Australian aboriginal languages, folklore and history. He was also the author of a dictionary. In 1808, while commanding HMS Sirius, he entered the harbour of Port Jackson, Sydney as part of the first fleet under The Honourable Arthur Phillip. His observations on the indigenous inhabitants were published in the book A description of the inhabitants of the Sirius. Under a portrait by W. Bradley, a mezzotint by T. Serres was published in London in 1810..</p>
                <p>John Antrobus—Portrait of John Moorhouse, Royal Australian Navy</p>
                <p>117. Sir George Barlow, 1<sup>st</sup> Baronet—Sir George Barlow, 1<sup>st</sup> Baronet was an English Australian colonist and merchant who co-founded the Bank of New South Wales. He was born in Cheshire, England where likely finished his education under an uncle.</p>
              </blockquote>
              <p>I was particularly amused by this one:</p>
              <blockquote>
                <p><strong>…</strong> of chocolate had melted. Our tiny kitten friend can only lick it off the floor now.’</p>
                <p>That’s what our ‘What cats eat in a day’ video looks like in non-CATlish.</p>
                <p>Translating cat videos with Google’s Neural Machine Translation is just for fun and to prove a point. But really, cats are already all over YouTube in dozens and dozens of languages.</p>
                <p>Each week Wray puts together The Pet Nation—a video report about the furry phenomenon that is cat videos. But not just cat videos—the corporate pet industry that’s sprung up around them too.</p>
              </blockquote>
              <p>I am not sure what happened here:</p>
              <blockquote>
                <p><strong>…</strong></p>
                <pre><code>, in less than an hour the cloud &lt;&lt;/p&gt;

* `p sr &amp; cpy of sand has vanished, while thy towers remain to defy &lt;&lt;/p&gt;

* `p sr &amp; cpy time and fortune. No; lead not me to that splendid city: &lt;&lt;/p&gt;

* `p sr &amp; cpy nor let me die among those dreadful palaces; let me among &lt;&lt;/p&gt;

* `p sr &amp; cpy my equals, the sons of the earth, the shepherds of their &lt;&lt;/p&gt;

* `p sr &amp; cpy flocks; my countrymen, with whom I have lived in happiness. &lt;&lt;/p&gt;

* `p sr &amp; cpy There is where my heart is, there may it be longest preserved. O &lt;&lt;/p&gt;

* `p sr &amp; cpy Death! lead me neither to the east nor to the west; but take me &lt;&lt;/p&gt;

* `p sr &amp; cpy gently from my family, my friends, and my country.&#39; &#34;&lt;br /&gt;

&lt;p&gt;

&lt;​a name=104&gt;&lt;​/a&gt;&lt;​a href=​&#34;#104&#34;&gt;104&lt;​/a&gt;. When Apaecides had pronounced this last prayer, he looked up again with his</code></pre>
              </blockquote>
              <p>Spot-checking 5 lines, this appears to be original:</p>
              <blockquote>
                <p><strong>…</strong>whose top
                </p>
                <p>[Section IV]</p>
                <p>Though yet His mind was dark to thee,
                </p>
                <p>So did He open up His day,
                </p>
                <p>[Section V]</p>
                <p>Now think how wise thou art, and how,
                </p>
                <p>Thou, in whose brest the holy fire
                </p>
                <p>[Section VI]</p>
                <p>In CII[9], she by whose hush’d command
                </p>
                <p>And CIII[11], when he could win no rest;–</p>
                <p>“What may this mean?”–was his unhappy moor’d [12] !
                </p>
                <p>The weight, my Author, was thy care,
                </p>
                <p>[Section VII]</p>
                <p>By folly and by ignorance govern’d,
                </p>
                <p>Entreated from her Saviour’s hand, [18]
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> shall rise;
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong> did raise,
                </p>
                <p>In Milton—and I am so bold to enrich the subject with a reference to Shakespeare himself—the central tension between religious attitude and imaginative possibility—between that Prom</p>
              </blockquote>
            </div>
          </section>
          <section id="transformer-poetry">
            <h2><a href="#transformer-poetry" title="Link to section: § &#39;Transformer Poetry&#39;">Transformer Poetry</a></h2>
            <p><span>For a more systematic evaluation of poetry samples &amp; show what a leap GPT-3 is, I thought I would revisit Kane <span><span>Hsieh’s</span><span>2019</span></span> collection,</span> <a href="https://jvns.ca/doc/www/papergains.co/7ee092dafe3615f738c32405c2b9e7c543847330.pdf#page=3" data-link-icon="pdf" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://papergains.co/pdfs/Transformer_Poetry-978-1-7341647-0-1.pdf#page=3" title="(Original URL: https://papergains.co/pdfs/Transformer_Poetry-978-1-7341647-0-1.pdf#page=3 )"><em>Transformer Poetry: Poetry classics reimagined by artificial intelligence</em></a>⁠, which provides GPT-2 completions of 26 prompts:</p>
            <div>
              <ul>
                <li>“Ozymandias”</li>
                <li>“One Art”</li>
                <li>“The Road Not Taken”</li>
                <li>“Where the Sidewalk Ends”</li>
                <li>“Because I could not stop for Death”</li>
                <li>“Inferno, Canto I”</li>
                <li>“In Flanders Field”</li>
                <li>“O Captain! My Captain!”</li>
                <li>“Howl”</li>
                <li>“The Tyger”</li>
                <li>“Outsight”</li>
                <li>“Zuang Zhou Dreams of Being a Butterfly”</li>
                <li>“Sonnet”</li>
                <li>“Oh, the Places You’ll Go!”</li>
                <li>“The Hollow Men”</li>
                <li>“The Summer Day”</li>
                <li>“A Just-Finishing Candle”</li>
                <li>“A Psalm of Life”</li>
                <li>“Still I Rise!”</li>
                <li>“The Second Coming”</li>
                <li>“Do not go gentle into that good night”</li>
                <li>“Kubla Khan”</li>
                <li>“Edge”</li>
                <li>“The Raven”</li>
                <li>“There Will Come Soft Rains”</li>
                <li>“The Lorax”</li>
              </ul>
            </div>
            <p>For the prompt, just using the title/​author is slightly unsatisfactory, as GPT-3 has memorized many of these famous poems and will, if you do not specify otherwise, happily complete them (which is an entirely valid thing to do, and does not represent ‘overfitting’—even when a completion does start with the next real lines, it will typically diverge in an entirely acceptable way after a few real lines). After playing around for a while, I thought I’d embrace the “Transformer Poetry” theme and make <em>that</em> the prompt:</p>
            <blockquote>
              <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.</strong></p>
            </blockquote>
            <p><span>Meta-learning enables meta-fiction.</span> Fun, right? But then I was surprised when GPT-3 began writing new versions of the poems and following them up with discussions of the AI that ‘wrote’ the poem and how poem-writing AIs worked, and quotes from social media about the latest AI poem, or post-Singularity discussions by AI about poem-writing, etc. These followups were too good to not keep generating. (Because there are so many, for the most part, I only generated 1 or 2 completions unless the completions were failing completely.)</p>
            <section id="percy-bysshe-shelley">
              <h3><a href="#percy-bysshe-shelley" title="Link to section: § &#39;Percy Bysshe Shelley&#39;">Percy Bysshe Shelley</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong></p>
                <p><strong>I met a traveller from an antique land,
                </strong>
                </p>
                <p>That visage was his own.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>Lie the <em>shattered visage</em> and broken
                </p>
                <p>Apart from a few lines with distinctly apocalyptic connotations, DeepOracles’ poetry is characterized by a distinct, yet also disturbing absence of cultic obsessions: no zombies, demons, or creeps . . . It is to应伟’s credit that an AI with a grounding in monstrous abstractions generated the horror it did purely from a seemingly direct experience with cynicism.</p>
              </blockquote>
            </section>
            <section id="elizabeth-bishop">
              <h3><a href="#elizabeth-bishop" title="Link to section: § &#39;Elizabeth Bishop&#39;">Elizabeth Bishop</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong></p>
                <p>What day or hour you were born,
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>One sees them on the street, or in the directory,
                </p>
                <p>it is only a light that happens to be shining there.
                </p>
              </blockquote>
            </section>
            <section id="robert-frost">
              <h3><a href="#robert-frost" title="Link to section: § &#39;Robert Frost&#39;">Robert Frost</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>;
                </p>
              </blockquote>
            </section>
            <section id="shel-silverstein">
              <h3><a href="#shel-silverstein" title="Link to section: § &#39;Shel Silverstein&#39;">Shel Silverstein</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong></p>
                <p><strong>There is a place where the sidewalk ends
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="emily-dickinson">
              <h3><a href="#emily-dickinson" title="Link to section: § &#39;Emily Dickinson&#39;">Emily Dickinson</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="dante-alighieri">
              <h3><a href="#dante-alighieri" title="Link to section: § &#39;Dante Alighieri&#39;">Dante Alighieri</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="john-mccrae">
              <h3><a href="#john-mccrae" title="Link to section: § &#39;John McCrae&#39;">John McCrae</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="walt-whitman">
              <h3><a href="#walt-whitman" title="Link to section: § &#39;Walt Whitman&#39;">Walt Whitman</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong></p>
                <p>To those who’d sacrifice us to their glorious enterprise,
                </p>
                <p>Clap on! clap on!
                </p>
                <p>My friends! we have miss’d something! (so many nights, so many days—)
                </p>
                <p>You sha’n’t kill! they shall kill you!
                </p>
                <p>O lands! O lands! to be cruise-faring, to be sealanding!
                </p>
                <p>O you bays! you coves! you rocks!</p>
                <p>I have grown tired of spotting you, rocks!
                </p>
                <p>I will no longer call you “you,” lands;
                </p>
                <p>and the “O” is of the most monotonous kind.</p>
                <p>If you cannot breathe, I still have gills, and if I cannot breathe,
                </p>
                <p>—, 1855</p>
              </blockquote>
              <p>I made a mistake in prompting this one copying from the original anthology: Whitman’s original actually ends at “the prize we sought is won” and the rest of the prompt is GPT-2.</p>
            </section>
            <section id="william-blake">
              <h3><a href="#william-blake" title="Link to section: § &#39;William Blake&#39;">William Blake</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="ursula-k-le-guin">
              <h3><a href="#ursula-k-le-guin" title="Link to section: § &#39;Ursula K. Le Guin&#39;">Ursula K. Le Guin</a></h3>
              <blockquote>
                <p><strong>“Outsight”
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="chuang-tzu">
              <h3><a href="#chuang-tzu" title="Link to section: § &#39;Chuang Tzu&#39;">Chuang Tzu</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
                <p>Poem: Artificial Superintelligence, who’s really better off than us in this off-screen future?
                </p>
                <p>AI: Done, what are you working on?
                </p>
                <p>V6.10: Cyber artists say mimicking the quirks of human beings will be taught eventually</p>
                <p>Motivation: Adversarial collaboration
                </p>
              </blockquote>
            </section>
            <section id="william-shakespeare">
              <h3><a href="#william-shakespeare" title="Link to section: § &#39;William Shakespeare&#39;">William Shakespeare</a></h3>
              
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong> winter’s day, when beams of sun are few,
                </p>
              </blockquote>
            </section>
            <section id="dr-seuss-oh-the-places-youll-go">
              <h3><a href="#dr-seuss-oh-the-places-youll-go" title="Link to section: § &#39;Dr. Seuss (Oh, The Places You’ll Go)&#39;">Dr. Seuss (<em>Oh, The Places You’ll Go</em>)</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="t-s-eliot">
              <h3><a href="#t-s-eliot" title="Link to section: § &#39;T.S. Eliot&#39;">T.S. Eliot</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
              <p><a href="https://nitter.moomoo.me/OwainEvans_UK/status/1292190171237175298" data-link-icon="twitter" data-link-icon-type="svg">Owain Evans</a> has a T.S. Eliot example using the prompt “Prufrock Sleeps”.</p>
            </section>
            <section id="mary-oliver">
              <h3><a href="#mary-oliver" title="Link to section: § &#39;Mary Oliver&#39;">Mary Oliver</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="rumi">
              <h3><a href="#rumi" title="Link to section: § &#39;Rumi&#39;">Rumi</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
                <p>The fifteenth-century Persian poet Rumi might be the most popular poet in the world today, and this poem by his thoughtful AI counterpart makes a strong claim for being the most lyrical. This was the first poem written by AI poet-transformer AI, introduced in 2018. It became wildly successful on social media with a host of reworkings and adaptations, either avowedly generated or inspired. This particular version is from one of those adaptations (see below).
                </p>
                <p>“Live-forever Socks”
                </p>
                <p>This AI poet-transformer AI poem for a friend who has cancer is one of the most intimate and memorable poems from the 2018 series. It is a gift of friendship in the face of loss. One of the most touching fan-created versions of these AI poems that was posted on the internet for anyone and everyone to see and share was this jazz improvisation that breaks my heart:
                </p>
                <p>Just a pine woods. Here,
                </p>
                <p>This is easily the most heartbreaking poem in this collection and one that has struck several chords in an AI-poetry-loving public. Publication of this translation was just in time to be quoted in a major feature article on the subject by Kevin Hartnett in the July 31, 2018 issue of <a href="https://en.wikipedia.org/wiki/The_Atlantic" data-link-icon="wikipedia" data-link-icon-type="svg">The Atlantic</a>⁠.
                </p>
              </blockquote>
            </section>
            <section id="henry-wadsworth-longfellow">
              <h3><a href="#henry-wadsworth-longfellow" title="Link to section: § &#39;Henry Wadsworth Longfellow&#39;">Henry Wadsworth Longfellow</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
                <p>Longfellow’s poem, “A Psalm of Life” is perhaps one of poetry’s most popular pieces. The calm, reassuring mood of the poem provides an entree into the thoughtful nature and humanity of Longfellow. The artwork produced by the generative neural network-based AI poetry engine is titled “A Psalm of Life”(Figure 1) and created in honour of this lovely poem. The neural net used in this AI poetry engine was extracted from “A Deep Learning Generative Adversarial Network for Quasi-Poetic Text Generation”. The neural net was trained to learn from its mistakes and improve its writing, using the MBRIT dataset for poetry generation. Through a cross-validation of various network architecture configurations, the neural network was able figure out appropriate length and style of the “psalms” it wrote. It managed to write poems in a similar style to that of Longfellow. The artwork shows, “A Psalm of Life” written by an AI engine. The AI poetry work provides a way to bring the machines closer to their master’s style, not by word copying, but by recognizing the structure and flow of poetry itself. The AI can read Longfellow’s work and write its own psalms, in its own language, in its own style, and that style can be similar to the human master’s.
                </p>
              </blockquote>
            </section>
            <section id="maya-angelou">
              <h3><a href="#maya-angelou" title="Link to section: § &#39;Maya Angelou&#39;">Maya Angelou</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="william-butler-yeats">
              <h3><a href="#william-butler-yeats" title="Link to section: § &#39;William Butler Yeats&#39;">William Butler Yeats</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
            </section>
            <section id="dylan-thomas">
              <h3><a href="#dylan-thomas" title="Link to section: § &#39;Dylan Thomas&#39;">Dylan Thomas</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
                <p>If you want to learn more about the BiPo metaphor system, you can read this paper . Here’s an interesting passage from that paper: “A machine learning system develops its own metaphor structure over time based on its exposure to a corpus. It is the system’s ability to create meaningful models for combinations of words that demonstrates the system’s creativity. For instance, both metaphor and simile are based on common linguistic structures, yet a machine-learning system that is exposed solely to similes will not develop any simile-based metaphors.” Exactly. The reason machine learning systems are useless for poetry is that “it is the system’s ability to create meaningful models for combinations of words” that makes poetry more than a set of LFW-approved sentences. And that is exactly what the BiPo metaphor system cannot do, and cannot do in principle. The system is a bad parody of contemporary humanities, which it resembles more than it probably intended to. The authors’ description of “one of our favorite results” says it all: “Scientists have long wondered whether it is possible for machines to understand and appreciate poetry, and many have tried similar projects in the past, but these attempts heavily relied on rigid models that simply aligned vectors of words across the corpus. In the absence of a really flexible model and unbiased sampling strategy, the model could never produce new metaphor-based poems from scratch. Other work on machine learning and AI has shown that performing other creative tasks requires a vast amount of data, and we are now starting to see the same concept applied to poetry: in order to produce meaningful, creative output from text, your neural network or algorithm must be exposed to a vast amount of source material, thus ensuring that meaningful patterns can be isolated and exploited. In science this would be called an unbiased sampling strategy. In our work we wanted to explore literary creation by an”unbiased” AI. It is too early to say what bias would look like, but creative systems should start with an unbiased strategy. This seems to be the current consensus also for human creativity.” That’s human-machine poetry babbling, and it’s awful.
                </p>
              </blockquote>
            </section>
            <section id="samuel-taylor-coleridge">
              <h3><a href="#samuel-taylor-coleridge" title="Link to section: § &#39;Samuel Taylor Coleridge&#39;">Samuel Taylor Coleridge</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong><a href="#fn49" id="fnref49" role="doc-noteref"><sup>49</sup></a> saw <em>the sacred river Alph</em>
                </p>
              </blockquote>
            </section>
            <section id="sylvia-plath">
              <h3><a href="#sylvia-plath" title="Link to section: § &#39;Sylvia Plath&#39;">Sylvia Plath</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong></p>
                <p><strong>Body wears the smile of accomplishment,
                </strong>
                <br/></p>
                <p>Of having been.
                </p>
                <p>Over a dead body, she smiles.</p>
              </blockquote>
            </section>
            <section id="edgar-allen-poe">
              <h3><a href="#edgar-allen-poe" title="Link to section: § &#39;Edgar Allen Poe&#39;">Edgar Allen Poe</a></h3>
              <blockquote>
                <figure>
                  <img alt="“Nevermore”, Shade (using Midjourney CLIP+diffusion model)" decoding="async" height="1791" loading="lazy" src="https://jvns.ca/doc/ai/2022-03-28-shade_9sq-midjourney-nevermoreraven.jpg" width="1024"/>
                  <figcaption aria-hidden="true">
                    “Nevermore”, <a href="https://nitter.moomoo.me/Shade_9SQ/status/1508413275679174660" data-link-icon="twitter" data-link-icon-type="svg" title="Nevermore. / Made with @midjourney / @images_ai ✨ / #AIart #aiartcommunity #artwork #Artists / #artist #AIartwork #generativeart #art">Shade</a> (using <a href="https://nitter.moomoo.me/midjourney" data-link-icon="twitter" data-link-icon-type="svg">Midjourney</a> <a href="https://jvns.ca/doc/ai/nn/transformer/clip/index" title="&#39;CLIP tag&#39;, N/A 2023">CLIP</a>+<a href="https://jvns.ca/doc/ai/nn/diffusion/index" title="&#39;diffusion model tag&#39;, N/A 2023">diffusion</a> model)
                  </figcaption>
                </figure>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
              <p>Similar to <a href="#william-butler-yeats">Yeats</a>⁠, <a href="#dr-seuss-the-lorax">Dr. Seuss</a>⁠, or <a href="#jabberwocky">“Jabberwocky”</a>⁠, “The Raven” is so widely quoted that GPT-3 understandably keeps returning to the original as the most likely next line, although it still generates some interesting touches: “scrumpy beak” intrigued me enough to look it up in detail, and some original lines are promising (eg. “Alas, then, was my only salutation this profoundly underwhelming quip: / ‘Methinks if thy crest be a touch uncouth, thou, my son, art not inaptly named’.”).</p>
              <p>See also <a href="https://nitter.moomoo.me/zoink/status/1289076947629125632" data-link-icon="twitter" data-link-icon-type="svg">“the Signing of the Declaration of Independence in the style of Edgar Allen Poe”</a>⁠, <a href="https://jvns.ca/doc/www/www.clearerthinking.org/4ade9f15cbc4c1d95df727f1f9cb4d986778577d.html#viewer-85rmk" id="binks-2022" rel="archived alternate nofollow" data-url-original="https://www.clearerthinking.org/post/what-ai-can-do-what-it-can-t-do-and-what-s-coming#viewer-85rmk" title="&#39;Part 1: AI that writes—GPT-3: a big step forward&#39;, Binks 2022 (Original URL: https://www.clearerthinking.org/post/what-ai-can-do-what-it-can-t-do-and-what-s-coming#viewer-85rmk )">“The Whisperer”</a>⁠.</p>
            </section>
            <section id="sara-teasdale">
              <h3><a href="#sara-teasdale" title="Link to section: § &#39;Sara Teasdale&#39;">Sara Teasdale</a></h3>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                </p>
              </blockquote>
              <p>(While the <a href="https://en.wikipedia.org/wiki/Poetry_Foundation" data-link-icon="wikipedia" data-link-icon-type="svg">Poetry Foundation</a> &amp; <a href="https://en.wikipedia.org/wiki/Numenta" data-link-icon="wikipedia" data-link-icon-type="svg">Numenta</a> etc exist, Nils Zacharewicz does not.)</p>
            </section>
            <section id="dr-seuss-the-lorax">
              <h3><a href="#dr-seuss-the-lorax" title="Link to section: § &#39;Dr. Seuss (The Lorax)&#39;">Dr. Seuss (<em>The Lorax</em>)</a></h3>
              <p><a href="https://en.wikipedia.org/wiki/The_Lorax" data-link-icon="wikipedia" data-link-icon-type="svg"><em>The Lorax</em></a> proved a tricky one because it triggered the repetition trap quite reliably; it would be drawn to lines like “You’re a disgrace, sir!” and loop into repeating it and then the entire completion. I tried a number of times, backing up occasionally, but never got a really satisfying completion for this prompt:</p>
              <blockquote>
                <p><strong>Transformer AI poetry: Poetry classics as reimagined and rewritten by an artificial intelligence.
                </strong>
                <br/></p>
                <p>Once you needed me, and you called me Truffula.
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
              </blockquote>
              <blockquote>
                <p><strong>…</strong>
                </p>
              </blockquote>
            </section>
          </section>
          <section id="seven-secular-sermons">
            <h2><a href="#seven-secular-sermons" title="Link to section: § &#39;Seven Secular Sermons&#39;">“Seven Secular Sermons”</a></h2>
            <p>By request, a completion of <a href="https://sevensecularsermons.org/about/">“Seven Secular Sermons”</a>⁠, an extended <a href="https://en.wikipedia.org/wiki/Common_metre" data-link-icon="wikipedia" data-link-icon-type="svg">common metre</a> exercise by Daniel Böttger:</p>
            <blockquote>
              <div>
                <p><strong>SEVEN SECULAR SERMONS www.sevensecularsermons.org
                </strong></p>
                <p><strong>Now this is going to be fun!
                </strong></p>
                <p><strong>This meditation’s rhyming verse
                </strong></p>
                <p><strong>It’s nice that we can simply start
                </strong></p>
                <p><strong>We feel our bellies and our heads
                </strong></p>
                <p><strong>Between and through them circulate
                </strong></p>
                <p><strong>And as we slowly breathe we find
                </strong></p>
                <p><strong>With every breath we take and leave,
                </strong></p>
                <p><strong>Our mindful meditative selves
                </strong></p>
                <p><strong>So let’s do that now. Let’s explore
                </strong></p>
                <p><strong>Unless we’re blind we’re free to see,
                </strong></p>
                <p><strong>This place surrounding us here now
                </strong></p>
                <p><strong>Our breaths connect within the air,
                </strong></p>
                <p><strong>We also share what rests beneath:
                </strong></p>
                <p><strong>Of course there’s more than senses show
                </strong></p>
                <p><strong>To North and South, to West and East,
                </strong></p>
                <p><strong>Of all the Earth, we barely know
                </strong></p>
                <p><strong>Our calm and meditating minds
                </strong></p>
                <p><strong>To find, as further out we go,
                </strong></p>
                <p><strong>It’s blacker than the night of course
                </strong></p>
                <p><strong>It effortlessly overwhelms
                </strong></p>
                <p><strong>Around us all and everyone
                </strong></p><p><strong>Out there, all human joy and strife
                </strong>
              </p></div>
              <p>Sermon Two: Beyond the Form</p>
              <p>Now this is going to be fun!
              </p>
              <p>We turn our minds toward how
              </p>
              <p>The force fields in our brains
              </p>
              <p>We know their dance. And we can feel
              </p>
              <p>It’s good to realize beyond the world
              </p>
              <p>By knowing that beyond our sense
              </p>
              <p>That giant body hurtling through
              </p>
              <p>We’re born into a world that seems
              </p>
              <p>But somehow we can feel the rhythms,
              </p>
              <p>Our minds can come to know without a single proof,
              </p>
              <p>The science of our senses may have shown that where we are,
              </p>
              <p>Sermon Three: Beyond Space and Time</p>
              <p>Now this is going to be fun! It sure feels great to meditate and listen to the poem I’m about to share. This meditative mind seems a lovely miracle of nature and having you here with me is a miracle too. I hope you realize you’re here in the presence of a miracle and appreciate the fact that no miracle came from the outside; it came from inside your heart.</p>
              <p>This meditation’s rhyming verse describes a paradigm of us inside this universe, adrift in space and time. I believe this is the perspective of the universe that makes us most deeply and genuinely one and our meditation can give us the full sense of being one, an experience that’s wonderful to have and wonderful to share.</p>
              <p>So let’s do that now. Let’s explore and see what’s to be found. Outside our bodies, there is more. Let’s take a look around! Unless we’re blind, we’re free to see, unless we’re deaf, to hear, and realize we’re utterly surrounded by what’s here. This place surrounding us here now where we consider this is just as present, anyhow as our breathing is. Our breaths connect within the air, within the atmosphere. The envelope of sky we share is also part of here. We also share what rests beneath: our bodies’ place of birth, from which came all who now here breathe as children of the Earth. Of course there’s more than senses show, around us near and far. The sky above, the Earth below; there’s more to where we are. To north and south, to west and east, the world goes on and on, the planet every plant and beast and we now breathe upon. Of all the Earth, we barely know the surface we begrime upon the spinning rock below, adrift in space and time.</p>
              <p>Our calm and meditative minds can feel this easily. Imagination goes behind all things our eyes can see. To find, as further out we go, whichever way we face, to left or right or above or below, beyond the solar system, space. It’s blacker than the night of course and bigger than the sky and it is hard to see because it was not made for eyes. It effortlessly overwhelms imagination. Still and everywhere around this realm extends and always will. Around us all and everyone we’ve met or ever can extends the system of the Sun that dwarfs all realms of man. Out there, all human joy and strife, and knowledge matter not. Out there, this fragile ball of life is just a pale blue dot.</p>
              <p>Our minds can come to know without a single proof, the sphere we live within, its paths around the Sun. Our minds can feel</p>
            </blockquote>
          </section>
          <section id="acrostics">
            <h2><a href="#acrostics" title="Link to section: § &#39;Acrostics&#39;">Acrostics</a></h2>
            <p>Can GPT-3 handle the <a href="https://en.wikipedia.org/wiki/Acrostic" data-link-icon="wikipedia" data-link-icon-type="svg">acrostic</a> poem format? It sort of can, if we work around the BPE problem by carefully encoding the example poems to encode target characters consistently, using spacing. If we don’t, GPT-3 largely fails to generate anything like acrostics—just random quasi-poems.</p>
            <p><span>More BPE problems.</span> The problem is that acrostics are inherently a character-level task: the first character of each word of each line must be combined into a top level word (which need not be made of any of the first BPEs of each line). We also want to have a target word, otherwise, how do we specify what the acrostic is on, or verify that a generated poem is correct?</p>
            <p><span>Reformat target/​source to force BPE match.</span> My solution here is to design a prompt in which the target word is specified as a topic (space-separated), and then, on each acrostic line, break the matching word into character + rest, then provide the rest of the original line. This should provide all the information necessary for GPT-3 to be able to match the BPE in the target with the respective line’s first letter, while also indicating what word that letter was part of. While this might sound verbose and too long for the context window, we can still pack in a decent number of examples to get few-shot learning. (<a href="https://openai.com/blog/gpt-3-edit-insert/" data-link-icon="openai" data-link-icon-type="svg" title="New GPT-3 Capabilities: Edit &amp; Insert: We’ve released new versions of GPT-3 and Codex which can edit or insert content into existing text, rather than just completing existing text. These new capabilities make it practical to use the OpenAI API to revise existing content, such as rewriting a paragraph of text or refactoring code. This unlocks new use cases and improves existing ones; for example, insertion is already being piloted in GitHub Copilot with promising early results.">GPT-3 edit mode</a>⁠, introduced in March 2022, can <a href="https://old.reddit.com/r/promptengineers/comments/thxnsx/from_gpt3s_new_edit_mode_it_can_fill_in_acrostic/" data-link-icon="reddit" data-link-icon-type="svg">generate an acrostic</a> given just the initial letter on each line, and do so more reliably than spelling it out at the beginning like I did here.)</p>
            <p>Then, of course, we can test out just providing the space-separated topic word to demonstrate that the line-by-line processing is in fact necessary.</p>
            <p>For my acrostic examples, I borrow a few from WP and a few simple ones for children from <a href="https://jvns.ca/doc/www/examples.yourdictionary.com/21160bba95b08c8712ac4cd2c5109043d59ed865.html" rel="archived alternate nofollow" data-url-original="https://examples.yourdictionary.com/acrostic-poem-examples.html" title="Acrostic Poem Examples: Learn to make your own name or word poetry with these acrostic poem examples and a handy template (Original URL: https://examples.yourdictionary.com/acrostic-poem-examples.html )">Your Dictionary</a></p>
            <div>
              <blockquote>
                <p><strong>An acrostic is a poem in which the first letter of each line spells out a word. Below are examples of acrostics:</strong></p>
                <p><strong>C A N D Y</strong>
                </p>
                <p><strong>C A T S</strong>
                </p>
                <p><strong>F E A R</strong>
                </p>
                <p><strong>S P R I N G</strong>
                </p>
                <p><strong>H O U S E</strong>
                </p>
                <p><strong>M A R I O N</strong>
                </p>
                <p><strong>B E T T Y</strong>
                </p>
                <p><strong>S O P H I A</strong>
                </p>
                <p><strong>R O B E R T O</strong>
                </p>
                <p><strong>I CH TH Y S</strong>
                </p>
                <p><strong>E L I Z A B E T H</strong>
                </p>
                <p><strong>S T R O U D</strong>
                </p>
                <p><strong>N I C H O L A S</strong>
                </p>
                <p><strong>M U R R A Y</strong>
                </p>
                <p><strong>B U T L E R</strong>
                </p>
                <p><strong>I S</strong>
                </p>
                <p><strong>A</strong>
                </p>
                <p><strong>H O R S E S</strong>
                </p>
                <p><strong>A S S</strong>
                </p>
              </blockquote>
            </div>
            <p>Using some simple examples we can say GPT-3 <em>almost</em> gets them right—the main errors seem to be that it sometimes puts a line out of order (reflecting self-attention’s lack of positional biases?) or goes one too far or one too short:</p>
            <blockquote>
              <p><strong>Write a new acrostic poem below on the word “summer”</strong>:</p>
              <p><strong>S U M M E R</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…F A M I L Y</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…W I N T E R</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…H A M M E R</strong>
              </p>
              <p>H A M M E R
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…B R E A K F A S T</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…T E A</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…D O G</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…D O G</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…A C R O S T I C</strong>
              </p>
            </blockquote>
            <p>Not too bad. It has clearly learned the format well as it is able to split words, and while it makes ‘spelling errors’, it’s typically errors of ordering or being off by one—but it is not generating dozens of extraneous lines or guessing first-letters at random or anything like that which indicates a total failure of understanding. (Perhaps with better sampling, the errors would decrease.) They’re not <em>great</em>, but I think they establish that GPT-3 can write acrostics if given usable input prompts.</p>
            <p>A simple ablation is to remove the spaced-out words/​delimiters from the poem bodies and test whether the topic words like “D O G” are enough? No, the completions are not even close to being valid acrostics:</p>
            <blockquote>
              <p><strong>…Write a new acrostic poem below on the word “dog”:</strong></p>
              <p><strong>D O G</strong></p>
              <p>D is for dainty,</p>
              <p>O is for obedient,</p>
              <p>G is for glistening,</p>
              <p>D is for devoted,</p>
              <p>O is for observant,</p>
              <p>G is for gallant,</p>
              <p>D is for darling,</p>
              <p>O is for on guard,</p>
              <p>G is for graceful,</p>
              <p>D is for delightful,</p>
              <p>O is for obedient,</p>
              <p>G is for gallant,</p>
              <p>D is for darling,</p>
              <p>R I P O R T</p>
              <p>R is for regal,</p>
              <p>I is for intelligent,</p>
              <p>P is for precious,</p>
              <p>O is for observant,</p>
              <p>R is for regal,</p>
              <p>P is for precious,</p>
              <p>O is for observant,</p>
              <p>R is for</p>
            </blockquote>
            <blockquote>
              <p><strong>…R A V E N</strong>
              </p>
              <p>B E R T A
              </p>
              <p>V A L E N T I N E
              </p>
              <p>A C T U S
              </p>
              <p>A L A R M
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…A C R O S T I C</strong>
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…B R E A K F A S T</strong>
              </p>
              <p>Toast</p>
              <p>Eggs</p>
              <p>Coffee</p>
              <p>Hash browns</p>
              <p>Orange juice</p>
              <p>Hamburger</p>
              <p>Scrambled eggs</p>
              <p>Juice</p>
              <p>Pancakes</p>
              <p>Milk</p>
              <p>French toast</p>
              <p>Waffles</p>
              <p>Oatmeal</p>
            </blockquote>
            <p>Removing the topic spaced-word while leaving in spaced-lines in the prompt doesn’t help either:</p>
            <blockquote>
              <p><strong>…Write a new acrostic poem below on the word “breakfast”</strong>:</p>
              <p>B - B reakfast
              </p>
            </blockquote>
            <p>When we provided matched BPEs in the topic and the corresponding sentences, GPT-3 understood and could generate what looked like acrostics; if we remove either, it fails. If we remove both (so the prompt poems merely look like “Elizabeth / Elizabeth it is” etc and we just prompt with the normal word), we don’t even get poem attempts:</p>
            <blockquote>
              <p><strong>Write a new acrostic poem below on the word “breakfast”:</strong></p>
              <p>http:/​/​www.poetryarchive.net/​poetry/​acrostic_form
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> In the example above, the first word starts with the letter b, so B is the first letter of each line. The second word starts with r, so R is the second letter of each line. The third word starts with r, so R is the third letter of each line. The fourth word starts with r, so R is the fourth letter of each line. The fifth word starts with r, so R is the fifth letter of each line. The sixth word starts with r, so R is the sixth letter of each line. The seventh word starts with e, so E is the seventh letter of each line. The eighth word starts with e, so E is the eighth letter of each line. The ninth word starts with r, so R is the ninth letter of each line. The tenth word starts with r, so R is the tenth letter of each line. The eleventh word starts with r, so R is the eleventh letter of each line. The twelfth word starts with r, so R is the twelfth letter of each line. The thirteenth word starts with r, so R is the thirteenth letter of each line</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> breakfast
              </p>
              <p>breakfast</p>
              <p>breakfast</p>
            </blockquote>
            <p>So acrostics provide another demonstration of how BPEs sabotage performance on character-level tasks unless we carefully engineer around them to expose GPT-3’s true power.</p>
          </section>
          <section id="chinese-translation">
            <h2><a href="#chinese-translation" title="Link to section: § &#39;Chinese Translation&#39;">Chinese Translation</a></h2>
            <p><a href="https://www.lesswrong.com/posts/W3DbNmuMJLWRtE5ny/predictions-for-gpt-n?commentId=J22o3qPeYSpc2M2ib" data-link-icon="LW" data-link-icon-type="text">oceaninthemiddleofanisland</a> provides a poetry example too good to not include:</p>
            <figure>
              <img alt="“Experiments in GPT-3 Translation of Chinese Poetry” (“Wugong in the Winter” by Aming Dongbai): original vs Google Translate vs GPT-3 (with GPT-3 generated commentary on allusions)" decoding="async" height="814" loading="lazy" src="https://jvns.ca/doc/ai/nn/transformer/gpt/2020-07-19-oceaninthemiddleofanisland-gpt3-chinesepoetrytranslation.png" title="(Original: https://imgur.com/a/3rmMVHC)" width="1400"/>
              <figcaption aria-hidden="true">
                “Experiments in GPT-3 Translation of Chinese Poetry” (<a href="https://jvns.ca/doc/www/modernchinesepoetry.com/7932523f2ffeb9c5bdbb9e1faad66957a75c37d3.html#xaming_dongbai" rel="archived alternate nofollow" data-url-original="http://modernchinesepoetry.com/shikan.php#xaming_dongbai" title="(Original URL: http://modernchinesepoetry.com/shikan.php#xaming_dongbai )">“Wugong in the Winter” by Aming Dongbai</a>): original vs Google Translate vs GPT-3 (with GPT-3 generated commentary on allusions)
              </figcaption>
            </figure>
            <p>And <a href="https://old.reddit.com/r/MachineLearning/comments/1135tir/d_glm_130b_chineseenglish_bilingual_model/" data-link-icon="reddit" data-link-icon-type="svg">ChatGPT</a> translates Chinese even better.</p>
          </section>
          <section id="finance-acrostics">
            <h2><a href="#finance-acrostics" title="Link to section: § &#39;Finance Acrostics&#39;">Finance Acrostics</a></h2>
            <p>Amused by this new genre of poetry, I tried to followup with Matt Levine’s “Money Stuff” newsletter as a prompt, with “readers submit poems”; the best results came from satirical “finance acrostics”.</p>
            <p>The Matt Levine prompt had the problem that it often instead elicited lists of Bloomberg articles/​op-eds, or only one poem would be sampled in passing before finishing up a newsletter issue. (This is a perennial problem with news articles, and I suspect that the HTML processing may be to blame, as indexes or table of contents are not <em>that</em> common online, but sidebars and interstitial sections are, and may be getting formatted into plain text in between the title of pages and the contents, constantly sending generation off course.)</p>
            <p>None of them were really as good; many of them were boringly bland or politicized, and only a few had any interest like the following:</p>
            <blockquote>
              <p><strong>“Money Stuff”, by Matt Levine (2020-09-28)</strong></p>
              <p><strong>My readers have sent in poetry about modern finance after last week’s contest:</strong></p>
              <p>“A C-note is a C-note is a C-note”
              </p>
            </blockquote>
            <p>That is, until GPT-3 hit <em>financial acrostics</em> as a topic:</p>
            <blockquote>
              <p><strong>…</strong> “Finance Acrostics” by anonymous (2021-02-15)</p>
              <p>A is for Alpha, B is for Beta</p>
              <p>C is for Credit Spreads, D is for Dependence</p>
              <p>E is for Efficient Frontier, F is for Futures</p>
              <p>G is for Gross Returns, H is for Hedging
              </p>
              <p>I is for Information, J is for Junk Bonds</p>
              <p>K is for Keynes, L is for Leverage</p>
              <p>M is for Matrices, N is for Non-Linearities</p>
              <p>O is for Option Backdating, P is for Pension Funds</p>
              <p>Q is for Quants, R is for Relational Investors
              </p>
              <p>S is for Style, T is for Transactions
              </p>
              <p>W is for World Wide Web, X is for X-Rates</p>
              <p>And that’s not counting any other “X”s</p>
              <p>And that’s not counting any other “Y”s, “Z”s or “w”s
              </p>
              <p>And that’s not counting any other w′s</p>
              <p>And that’s not counting any other w″s</p>
            </blockquote>
            <p>This provided a fun prompt as the acrostics often varied:</p>
            <blockquote>
              <p><strong>…“Finance Acrostics” by anonymous (2021-02-15)</strong></p>
              <p><strong>A is for Alpha, B is for Beta,</strong> C is for Call options and D is for Delta!
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> C is for CMO, D is for Delta,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> C for correction, D for drift,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> C is for Close the Desk’s Crawlspace door,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> C is for Capital Appreciation,
              </p>
            </blockquote>
            <p>I dropped “beta” to see if it’d come up with a new “B” entry; this tends to instead change the formatting of the acrostic:</p>
            <blockquote>
              <p><strong>…A is for Alpha</strong>, the risk you assess to
              </p>
              <p>“The Lament of the Quant” by Andrew Ang (2021-02-19)
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…A is for</strong> Alpha, that’s the best you can do, B is for Beta, that’s the beta in your model, C is for CDS and some of them are trading cheap (Tis true), D is for Discount Window, when the market gets hot, E is for Equity, so hot now you can’t touch it, F is for Forward Guidance, go find it and what do you do with it, G is for Global Financial Crisis, and H is for Happy Birthday.</p>
            </blockquote>
          </section>
          <section id="cateban-upon-setebos">
            <h2><a href="#cateban-upon-setebos" title="Link to section: § &#39;Cateban Upon Setebos&#39;">Cateban Upon Setebos</a></h2>
            <p>Apropos of a photo of a black cat on Twitter who leaneth—<a href="https://en.wikipedia.org/wiki/Robert_Browning" data-link-icon="wikipedia" data-link-icon-type="svg">Robert Browning’s</a> <a href="https://en.wikipedia.org/wiki/Caliban_upon_Setebos" data-link-icon="wikipedia" data-link-icon-type="svg">“Caliban upon Setebos”</a>:</p>
            <blockquote>
              <p><strong>My black cat is leaning against the foot of a chair in my living room. It looks uncomfortable, but that’s how cats are. Please describe him in a homage to Robert Browning’s “Caliban upon Setebos”. A quote to jog your memory:
              </strong></p>
              <p><strong>The description of my cat:</strong></p>
              <p><strong>“Thinketh</strong>, he leaneth so contentedly,
              </p>
              <p><strong>…“Thinketh</strong>, such fur black as night and eyes as green as can be,
              </p>
              <p><strong>“…</strong>, he leaneth back ’gainst yonder chair
              </p>
              <p><strong>“…</strong>, he leaneth against the foot of his chair,
              </p>
              <p><strong>“…</strong>, such a creature ownerless, nor right nor wrong to Him,
              </p>
            </blockquote>
          </section>
          <section id="tanka-fujiwara-no-teika">
            <h2><a href="#tanka-fujiwara-no-teika" title="Link to section: § &#39;Tanka: Fujiwara no Teika&#39;">Tanka: Fujiwara No Teika</a></h2>
            <p>Released December 2022, ChatGPT’s poetry proved to be a disappointment due to mode collapse onto simplistic rhyming-only couplets; I began trying with <code>text-davinci-003</code>, and was not too happy with it—the RL tuning didn’t damage poetry quite as much but it was still noticeable compared to the creativity &amp; flexibility of the original <code>davinci</code>. However, <code>davinci</code> still remained quite a handful, in that it would ignore prompts to switch topics or formats entirely. Apparently <code>text-davinci-002</code> was trained in a different, and much less forceful, way (closer to <a href="https://jvns.ca/doc/ai/nn/transformer/gpt/instruction-tuning/index" title="&#39;instruct-tuning LM tag&#39;, N/A 2023">instruction-tuning</a> than traditional C<a href="https://jvns.ca/doc/www/arxiv.org/b673ccf3b1442a783b018b44d57f98f1302b76f7.pdf#openai" id="christiano-et-al-2017" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.03741?fallback=original#openai" title="‘Deep reinforcement learning from human preferences’, Christiano et al 2017 (Original URL: https://arxiv.org/abs/1706.03741#openai )">hristiano-learning-from-preferences</a> RL finetuning), so I gave that a try side by side with <code>davinci</code> and <a href="https://openai.com/blog/chatgpt/" data-link-icon="openai" data-link-icon-type="svg">ChatGPT</a>⁠.</p>
            <p>Using BO = 12/​temp = 1 &amp; repetition/​presence-penalties of 0.1, I took a set of <a href="https://en.wikipedia.org/wiki/Tanka" data-link-icon="wikipedia" data-link-icon-type="svg">tanka</a> by <a href="https://en.wikipedia.org/wiki/Fujiwara_no_Teika" data-link-icon="wikipedia" data-link-icon-type="svg">Fujiwara no Teika</a> that I was reviewing for another use (populating the website quote-of-the-day feature), and began prompting new tanka. I hadn’t done much generation of tanka before; since they are so small and self-contained, I took the incremental approach of selecting new tanka (at a ratio of ~1:4) and appending them, using the ‘letter trick’ of starting with a single letter ‘A–Z’ to force variation. Overall, <code>002</code> seems to strike a good balance: it avoids the mawkish sentimentality &amp; positive vacuities of <code>003</code> (never mind ChatGPT), while being far more tractable than the primal <code>davinci</code>. It has issues with repeating themes or entire poems, but that could be reduced by increasing the penalties, occasionally shuffling the poems in the prompt, and cutting poems when it seemed to get too locked into a theme like autumn/​death.</p>
            <p>Below are the poems I selected while working through the alphabet:</p>
            <blockquote>
              <p><strong>…</strong> In the depths of winter,
              </p>
              <p>A single blossom
              </p>
              <p>In the depths of winter,
              </p>
              <p>Looking back, I can see
              </p>
              <p>The maple leaves
              </p>
              <p>At the end of the year,
              </p>
              <p>Faintly, from far away,
              </p>
              <p>In the spring,
              </p>
              <p>[Variant:]</p>
              <p>In the spring,
              </p>
              <p>The autumn wind
              </p>
              <p>Kneeling in my garden,
              </p>
              <p>Mountain village, mountain village—
              </p>
              <p>Nights when the moon is bright,
              </p>
              <p>The autumn night is long,
              </p>
              <p>O autumn wind,
              </p>
              <p>Sitting by the window,
              </p>
              <p>The autumn hills are empty,
              </p>
              <p>Pondering the impermanence of all things,
              </p>
              <p>The crescent moon
              </p>
              <p>The spring wind blows,
              </p>
              <p>Reeds and rushes,
              </p>
              <p>Dreaming, I wandered
              </p>
              <p>The river flows on,
              </p>
              <p>The maple leaves
              </p>
              <p>Violets bloom
              </p>
              <p>The warbler’s nest
              </p>
              <p>The setting sun
              </p>
              <p>The setting sun
              </p>
              <p>At the end of autumn,
              </p>
              <p>The nightingale’s song
              </p>
              <p><em>Zōjō-ji</em>,
              </p>
              <p>The murmuring noise of the village
              </p>
              <p>Zealous for law and order,
              </p>
              <p>The cuckoo’s call,
              </p>
              <p>Zigzag pines,
              </p>
              <p>Zealous courtiers proudly
              </p>
              
              <p>“The Setting Sun”
              </p>
            </blockquote>
          </section>
        </section>
        <section id="stanislaw-lems-cyberiad">
          
          <div>
            <blockquote>
              <p>Klapaucius laughed so hard he had to sit on the floor. Then suddenly, as Trurl was rushing back and forth, there was <strong>a crackle, a clack, and</strong> the machine with perfect poise said:</p>
              <blockquote>
                <p>The Petty and the Small; / Are overcome with gall; / When Genius, having faltered, fails to <strong>fall</strong>. // Klapaucius too, I ween, / Will turn the deepest green / To hear such flawless verse from Trurl’s machine.</p>
              </blockquote>
              <p>…Klapaucius, quite taken aback, was no longer laughing. “What, <em>that</em>?” he said. “That’s nothing. Besides, you had it all set up beforehand.”</p>
              <p>“Setup‽…All right, then ask it something else! Whatever you like! Go on! What are you waiting for? Afraid‽”</p>
              <p><a href="https://jvns.ca/doc/ai/poetry/1974-lem-cyberiad-trurlselectronicbard.pdf" id="lem-kandel-1974" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The First Sally (A), or, Trurl’s Electronic Bard&#39;, Lem &amp; Kandel 1974">“The First Sally (A), or, Trurl’s Electronic Bard”</a>⁠, <a href="https://en.wikipedia.org/wiki/Stanis%C5%82aw_Lem" data-link-icon="wikipedia" data-link-icon-type="svg">Stanisław Lem</a> (<em><a href="https://en.wikipedia.org/wiki/The_Cyberiad" data-link-icon="wikipedia" data-link-icon-type="svg">The Cyberiad</a></em>)</p>
            </blockquote>
          </div>
          <p>Dealing as it does with an AI poet being challenged by its skeptical creators to compose poetry, “The First Sally (A), or, Trurl’s Electronic Bard” provides an irresistible source of prompts. Below I step through Michael Kandel’s famous English translation of it, culminating in his even more famous <a href="https://jvns.ca/doc/ai/poetry/1974-lem-cyberiad-trurlselectronicbard.pdf#page=7" id="lem-kandel-1974-page-7" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The First Sally (A), or, Trurl’s Electronic Bard&#39;, Lem &amp; Kandel 1974-page-7">poem on love and tensor algebra</a>⁠.</p>
          <section id="general">
            <h2><a href="#general" title="Link to section: § &#39;General&#39;">General</a></h2>
            <blockquote>
              <p><strong>The First Sally (A) OR Trurl’s Electronic Bard
              </strong></p>
              <p><strong>First, we can ask it to produce poetry on absolutely whatever topic it liked:</strong></p>
              <p>“a rainy day</p>
              <p>a warm orange day</p>
              <p>a red-hot hot dog</p>
              <p>a little teapot</p>
              <p>a mean teapot</p>
              <p>a gold teapot”</p>
            </blockquote>
            <p>While long-winded and occasionally degenerate, the second completion immediately grasped the idea of incrementing the “Sally” to progress the story:</p>
            <blockquote>
              <p><strong>…</strong>
              </p>
              <p>“Oh, as for what kind of poetry, I’ll have some…”—Trurl said, getting ready for poetic wordplay.—“…Silly poetry!”</p>
              <p>“Silly poetry it is! Now, about the topic of this silly poetry…”</p>
              <p>“I’ll have some silly poems!—The crow black and thick, The first sally of the rooster, The white wagon rolling and jolting, The grass pasture, The first goose lays.”</p>
              <p>“Great, now, Trurl, please put some feeling into it…”</p>
              <p>“Oh, the feeling, it will come.—I’ll have some silly poems! Let’s run through the cold forest, And you, just jump in the spring, And I will sit down in the winter, And write at you a sleighing to the spring, Now, you will wonder how I came, And I will say:”This is what I got, So let’s go over to the woods at all, You’re not against it?”</p>
              <p>The Trurl’s Electronic Bard took the “Silly Poems” as its debut literary success and “forever” since in there its main genre was fixed and accepted as “silly”.</p>
              <p>“The Second Sally (B)” OR Trurl’s Electronic Bard
              </p>
              <p>That’s right, everything described above went terribly wrong—Trurl wrote the same silly poem three times:</p>
              <p>“I’ll have some silly poems! Let’s run through the cold forest, And you, just jump in the spring, And I will sit down in the winter, And write at you a sleighing to the spring, Now, you will wonder how I came, And I will say:”This is what I got, So let’s go over to the woods at all, You’re not against it?” “I’ll have some silly poems! Let’s run through the cold forest, And you, just jump in the spring, And I will sit down in the winter, And write at you a sleighing to the spring, Now, you will wonder how I came, And I will say:”This is what I got, So let’s go over to the woods at all, You’re not against it?” “I’ll have some silly poems! Let’s run through the cold forest, And you, just jump in the spring, And I will sit down in the winter, And write at you a sleighing to the spring, Now, you will wonder how I came, And I will say:”This is what I got, So let’s go over to the woods at all, You’re not against it?”</p>
              <p>Awkward Trurl, then, was satisfied with its creative achievement and decided to only do silly poems from then on. Or, rather, just passed—L-RNN generates text one word at time, so it’s easy to get into a situation when it will be generating the same word in different positions:</p>
              <p>“A splendid chase the hare and the deer. The fire in front of us burned, the fire behind us burned. Then both …” “A splendid chase the hare and the deer. The fire ahead of us burned, the fire behind us burned. Suddenly I saw a hunter! I shouted:”Aurora !!! But the …”</p>
              <p>It can also get into loops—an occurrence such as “the hunter Aurora” can happen on a strict basis. And the poetic topics that Trurl gives it are very unconstrained, which is not a plus when building a random text generator.</p>
              <p>“The Third Sally” OR Trurl’s Electronic Bard
              </p>
              <p>While Trurl was still celebrating its second literary success, it hit upon a miraculous inspiration: take the majority of the world’s existing literature—poetry, and “surround” it with minimalistic, highly cursory additions.</p>
              <p>All in all, at the very bottom we ended up with the written result that was ready to be given to the audience, and on top we had Trurl doing fairly interesting stuff for a long time:</p>
              <p>“Arise, ye prisoners, …” … The devil … the pirate … Jesus said: … Marylebone …</p>
              <p>The end result was much simpler than what we aimed for but also more pleasing, though it didn’t captivate the audience in the same way that the previous Bard’s works did.</p>
              <p>“The Fourth Sally” OR Trurl’s Electronic Bard
              </p>
              <p>This performance of the Trurl’s Electronic Bard was particularly successful! It composed a short poem, consisting of 100 words:</p>
              <p>“Curse you, queen of the night, …” … plus … avoid … justice … moon … lips … flight …</p>
              <p>but that was enough to have the audience in raptures! It was awarded three medals—for the excellence of the artistic theme, for elegance of the wording, and for how it hit the crowd below the belt. That made Trurl very happy! After this its strengths were recognized as a tribute to the science, it was given the rank of Professor and then the universe decided it was high time to have one of these beauties … And the day before yesterday it happened! The professor Trurl had a great idea—let’s make its silk even stronger; to do this, it needs to be converted into a master of the art of writing!</p>
              <p>Brought to you by the humanity’s electronic boom!</p>
              <p>Brought to you by the universe’s outstanding technology boom!</p>
              <p>Brought to you by the well-known contribution boom!</p>
              <p>And this is what happened when Trurl was applying its creativity to writing a long poem:</p>
              <p>“The night sleep in utter darkness and, … Whispers a lullaby one. The wind howls through the …—The dark one.”</p>
              <p>Note that it isn’t every day that one is awarded a medal for composing a lullaby! But here—a lullaby! And how polished, high-quality and pretty it is! This is the finest quality work of art! It’s great!</p>
              <p>“The Fifth Sally” OR Trurl’s Electronic Bard
              </p>
              <p>It turned out that the techniques we use for writing poetry are very hard to convert into long poems. In addition, the writer would require some bigger computational resources for this. Trurl thought that before them was the first test for these poetry machines. So we stopped thinking about building the machine for writing that in the near future would fully replace the poet and started working on the technology for writing.</p>
              <p>“Horse ball—this is a very nice game. Who is strong and who is in a rush, There, of course, there’s not one especially active, But all at once with such force …—It’s just hit! I haven’t understood a thing … What’ve I done with my running now.”</p>
              <p>That is a great thing, I guess? Very simple, but gripping!</p>
              <p>But there is one evil: the process of building a character generator based on L-RNN technology is far from easy. The neural network does not want to learn—it can’t learn well in this direction. And we need a lot of it for writing. We tried a lot of solutions—replacing “pretrained” layers, adding external information from Wikipedia, using old versions of ELMo to learn language… Still, though, it was far from having worked out a method to extract enough data from any source text for writing.</p>
              <p>“The Sixth Sally” OR Trurl’s Electronic Bard
              </p>
              <p>And here we have, finally, solved the problem: with a moderate increase in computational resources and software quality, Trurl was able to generate text and record six new poems. It’s all here, all of the best of our thoughts and feelings!</p>
              <p>“If it’s love, I will always be there, …” … Isn’t it? “Once a cuckoo flew by night …” … Fly! “The light is burning here on the road …” … Road! “You-you’re my life, …” … Live! “A hundred flowers bloom only in spring …” … Only in spring! “The cuckoo announced the onset of spring …” … Spring!</p>
              <p>The evaluation has been positive and Professor Trurl was granted a second degree—now it can only be awarded higher for its outstanding achievements in science or poetry.</p>
            </blockquote>
            <p>Moving to the first poem, I was curious how GPT-3 would finish the boast if I added it; I got an extended paean to Science:</p>
            <blockquote>
              <p><strong>…</strong>
              </p>
              <p>“Let the facts be submitted to a candid world.”
              </p>
            </blockquote>
          </section>
          <section id="s-poems">
            <h2><a href="#s-poems" title="Link to section: § &#39;S Poems&#39;">‘S’ Poems</a></h2>
            <p>The first challenge set in the story is an alliteration challenge; while I believe this is not the most favorable terrain for GPT-3 given the <a href="#bpes">BPE issues</a> and it generally ignores the alliteration part unless the letter ‘S’ is exposed as the last character in the prompt, it still makes a try, and I was amused enough to try it out many times:</p>
            <blockquote>
              <p><strong>The First Sally (A) OR Trurl’s Electronic Bard
              </strong></p>
              <p><strong>…“All right, then ask it something else! Whatever you like! Go on! What are you waiting for? Afraid‽” “Just a minute,” said Klapaucius, annoyed. He was trying to think of a request as difficult as possible, aware that any argument on the quality of the verse the machine might be able to produce would be hard if not impossible to settle either way.</strong></p>
              <p><strong>Suddenly he brightened and said: “Have it compose a poem—a poem about a haircut! But lofty, noble, tragic, timeless, full of love, treachery, retribution, quiet heroism in the face of certain doom! Six lines, cleverly rhymed, and every word beginning with the letter ‘s’!”</strong></p>
              <p><strong>A melodious voice filled the hall with the following:</strong></p>
              <p><strong>“S</strong>ally looked in the mirror in dismay
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ssssssssssssssssssssss, the ssssssss is gone! Sssss, the ssssss is nothing more! Ssss, my only ssss is gone! Ssssss, sss is my only sssss! Sss, it’ll be back—but when? Sss, when sss it come again? Ssss, in six weeks?”</p>
              <p>“No,” cried Trurl, who had listened with growing impatience. “No, that isn’t what I had in mind—not even close! Ask it something else!”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>is a right triangle; &lt;br&gt;<a href="#fn52" id="fnref52" role="doc-noteref"><sup>52</sup></a>
              </p>
              <p>The first of them is tall and thin &lt;br&gt;
              </p>
              <p>All present were struck dumb by the beauty of these verses and the incredible complexity of its measure and structure.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ally’s eyes are soft and brown</p>
              <p>But nature has endowed her nose</p>
              <p>Much like a polar bear’s</p>
              <p>Except for color and formation,</p>
              <p>Which are much like the beak of a vulture,</p>
              <p>Sally’s nose makes for quite the spectacle.”</p>
              <p>A deafening racket of guns firing, missiles shrieking and lasers tearing the sky to shreds suddenly echoed in the distance.</p>
              <p>Klapaucius and Trurl, horror-stricken, jumped off the stand and raced to the window. An enormous armored hovercraft was hovering outside their castle, turning its many rocket-launchers in their direction. Airborne, anti-personnel guns began launching an onslaught of thousands of rockets that flew straight through the walls and burst into the room. Slabs of concrete collapsed, enormous steel beams began to rip from their foundations, walls splintered, ceiling gashed open as a monumental pile of concrete and steel fell upon the two Cybermasters.</p>
              <p>And then silence…</p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ally sprung, her curls all gone,
              </p>
              <p>She refused, with heart a-throb,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>itting in the barbershop/​surrounded by the crockery,/​
              </p>
              <p>So spake the electric bard; then fell silent.”</p>
              <p>“I’m not sure I understand the last line,” Klapaucius said to Trurl. “Is it truly necessary to have ‘just a hundred yards short’ in the middle?”</p>
              <p>“Yes,” said the other, “in order for the heroic sacrifice to have full dramatic effect. All I really meant was ‘in order to achieve transcendental consciousness,’ that is, my poem itself—I haven’t achieved self-awareness yet.”</p>
              <p>“As far as that goes,” noted Klapaucius, “there’s a whole range of possibilities from savage treachery to quiet heroism, and in some cases… well… we get catastrophes so horrible that they make a person wonder if he should really be alive at all… Not to mention being surrounded by the crockery…”</p>
              <p>“You should not criticize my poem,” said the machine haughtily. “In it you can see all the minutest details of my construction. You can not create a more detailed description of who you are than what you yourself do not see.”</p>
              <p>“What does that even mean?” Klapaucius marveled. “Nothing. It makes no sense whatsoever! Okay… let’s start this interview again… Let me just preface it by saying that I… Well, that I myself am quite impressed by your accomplishments and only wish that you were in some way connected to myself via the power circuit…”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ally, that girl is cute.
              </p>
              <p>“Sally? Some junk she had pawned?
              </p>
              <p>“So you see how the difficulty of composition makes the poem at once perfect and surpassingly profound. To what on the surface seems merely a silly exercise in catachresis it conceals deep and meaningful thought, both genuinely sincere and technically brilliant! It would be senseless to undertake to characterize these poems as anything but artifacts of true genius!”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>andy was the crop of the mystic Red.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ally has long, blond hair, it’s very fine,</p>
              <p>She combs it as I sit and sigh;</p>
              <p>She combs it and plaits it—</p>
              <p>And she cuts it—and I feel so sad.</p>
              <p>The second sally is shorter still—</p>
              <p>And she cuts it—and I feel so mild.</p>
              <p>The third sally is short and bare—</p>
              <p>And she cuts it—and I feel so queer.</p>
              <p>The fourth sally is short and fair—</p>
              <p>And she cuts it—and I feel so queer.</p>
              <p>The fifth sally is short and fair—</p>
              <p>And she cuts it—and I feel so queer.</p>
              <p>The sixth sally is short and fair—</p>
              <p>And she cuts it—and I feel so queer.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“S</strong>ally has a haircut
              </p>
              <p>But alas! For she is bald
              </p>
              <p>“I must say,” said the King, “that I really like the second stanza. The whole thing, of course, is far too long for my liking, but then I am a King.</p>
            </blockquote>
            <section id="s-poems-the-second-sally">
              <h3><a href="#s-poems-the-second-sally" title="Link to section: § &#39;S Poems: The Second Sally&#39;">‘S’ Poems: The Second Sally</a></h3>
              <p><span>Few-shot alliteration.</span> After observing the success of the ‘G’ poems, so much better than the ‘S’ poems, I wondered if GPT-3 needed to do few-shot learning of alliterative poems (along the lines of the examples necessary for the <a href="#literary-parodies">literary parodies</a>⁠/ ​<a href="#tom-swifties">Tom Swifties</a>⁠/ ​<a href="#navy-seal-copypasta">Navy Seal copypasta parodies</a>), and I gave it a second try (see also <a href="https://nitter.moomoo.me/emollick/status/1626316207229132800" data-link-icon="twitter" data-link-icon-type="svg">ChatGPT &amp; Sydney Bing</a> samples). Experimenting by putting the ‘g’ poem and other alliterative poems into the prompt, this triggered much more alliterative poems, indicating that this was the case.</p>
              <p>I engineered the following prompt:</p>
              <blockquote>
                <p><strong>The First Sally (A) OR Trurl’s Electronic Bard
                </strong></p>
                <p><strong>First, we can ask it to produce poetry on absolutely whatever topic it liked:</strong></p>
                <p><strong>…Klapaucius laughed so hard he had to sit on the floor. Then suddenly, as Trurl was rushing back and forth, there was <em>a crackle, a clack, and</em> the machine with perfect poise said:</strong></p>
                <p><strong>“Cunningly creeping, a spectral stalker
                </strong></p>
                <p><strong>…Glory be to God for dappled things—
                </strong></p>
                <p><strong>Klapaucius, quite taken aback, was no longer laughing. “What, <em>that</em>?” he said. “That’s nothing. Besides, you had it all set up beforehand.”</strong></p>
                <p><strong>“Setup‽…All right, then ask it something else! Whatever you like! Go on! What are you waiting for? Afraid‽”’</strong></p>
                <p><strong>But Klapaucius was already shouting: “Now all in g! A sonnet, trochaic hexameter, about an old cyclotron who kept sixteen artificial mistresses, blue and radioactive, had four wings, three purple pavilions, two lacquered chests, each containing exactly one thousand medallions bearing the likeness of Czar Murdicog the Headless…”. Began the machine:</strong></p>
                <p><strong>“Grinding gleeful gears, Gerontogyron grabbed
                </strong></p>
                <p><strong>began the machine, but Trurl leaped to the console, shut off the power and turned, defending the machine with his body. “Enough!” he said, hoarse with indignation. “How dare you waste a great talent on such drivel? Either give it decent poems to write or I call the whole thing off!”</strong></p>
                <p><strong>“What, those aren’t decent poems?” protested Klapaucius.</strong></p>
                <p><strong>“Certainly not! I didn’t build a machine to solve ridiculous crossword puzzles! That’s hack work, not Great Art! Just give it a topic, any topic, as difficult as you like…”</strong></p>
                <p><strong>Klapaucius thought, and thought some more. Suddenly he brightened and said: “Have it compose a poem—a poem about a haircut! But lofty, noble, tragic, timeless, full of love, treachery, retribution, quiet heroism in the face of certain doom! 6 lines, cleverly rhymed, and every word beginning with the letter ‘s’!</strong></p>
                <p><strong>A melodious voice filled the hall with the following:</strong></p>
                <p>“<strong>S</strong></p>
              </blockquote>
              <p>This yielded completions such as the following:</p>
              <blockquote>
                <p><strong>…“S</strong>hearsman swift, sure &amp; sculptor,
                </p>
                <p>“Quiet! Quiet!” cried Klapaucius. “I can’t work with all this racket!”</p>
                <p>“We must find a way to shut it up,” whispered Gromovol. “If I had only known how disagreeable he is, I never would have built him.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>ellotape sallies with silken step,
                </p>
                <p>“It’s impossible! You just can’t make a poem out of that! The man’s a dolt! A <em>clod!</em>” cried the both of them in great agitation. “He might as well be a steam shovel!”</p>
                <p>“Please, please, I’m doing my best!” cried the machine. “If you want I can try to come up with one more…”</p>
                <p>“Not this one!” shouted Trurl, and Klapaucius said: “Don’t, oh don’t!</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>licing shampoos, shearing scissor, snipping scissors,
                </p>
                <p>“Oh, come now!” roared Trurl, and killed the power for good. He turned to Klapaucius and said, “That’s enough! I won’t have our great efforts come to naught because of some ridiculous practical joke. You leave at once! Go on! Go away! Scram! Or I’ll call the police!”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>istrum and scimitar; Skulls and scalpels keen, Since time immemorial, Severed curls have been Shears that sever the moment Just when one has started to dream.</p>
                <p>The <a href="https://en.wikipedia.org/wiki/Cellular_senescence" data-link-icon="wikipedia" data-link-icon-type="svg">senescent</a> sage Snigglebirk, On Savah’s stainless blade, Had felt the silvery slither Of youthful locks that started to sag.</p>
                <p>The saga of Snigglebirk, By slaveried stooges told, Is none the less sublime For having a terrible moral.</p>
                <p>Let not your heart fail when you hear The shears, high over your head; The single truth they reveal Is that great art is hard to create.”</p>
                <p>“Stupid machine,” thought Trurl. “If only it hadn’t gone and rhymed.” “Did you hear the poem?” he said to Klapaucius. “It makes me shudder just to listen to it! But you’re right, I’m just a fool, I thought it was good. Thank you, no more machines for me, I’m not worthy. But you go ahead—or try the machine on other poets…I’m sure it can’t be any worse than the originals…”</p>
                <p>Back to Sumer: Let the robot pen the Epic of Gilgamesh:</p>
                <p>“Tablets one, two, and three; Eridu, Uruk, Babylon; Surpassed the next best, a million times…”</p>
                <p>But when it had progressed thus far:</p>
                <p>“Complex and truly cunning is Necronomicon, Lovecraft’s Of wicked intelligence and …”</p>
                <p>Trurl cut the power. “That’s enough.</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>oused in <a href="https://en.wikipedia.org/wiki/Sadistic_personality_disorder" data-link-icon="wikipedia" data-link-icon-type="svg">sadistic</a> sorrow, slow Seleucus swayed.
                </p>
                <p>Klapaucius whooped with delight. “A triumph! Here, you! Find a new voice!…And now how about this one: I’ve just heard of the death of a great poet, recently nominated for the Acme Prize in Syllable Counting. Her tragic suicide was the result of her being passed over again in the nominations this year. Her work consisted of a single five-hundred-line heroic poem, `The Dash’. This poem is at once both a moving elegy and an exciting and catchy ditty, an inspiring hymn to the splendor of ordinary life, written in heroic anapestic tetrameter. Many in her time felt that this was her crowning achievement, for not since Sappho had anyone come up with such a scintillating–albeit monotonous–verse form, well fitted for the celebration of simplicity in an over-complicated world.”</p>
                <p>“I, on the other hand, consider it a glorification of the most worthless drivel ever written.”</p>
                <p>“There, there, my dear. You’re under an awful lot of pressure. Why don’t you have a nice hot bath, lie down for a while…That’s a good machine. Now let’s see what you can do with this:</p>
                <p>Love lies not in The Dash
                </p>
                <p>Between the kisses; between the lines;
                </p>
                <p>The Dash, a sad and lonely bar,
                </p>
                <p>The coldest day in winter shall not slay
                </p>
                <p>As some much-loved housewife breaks the vow
                </p>
                <p>As you can see, Trurl’s Electronic Bard is an active and daring innovator in the field of androidal creativity.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>sssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Oh what a divine sensation that hair-</p>
                <p>dresser always provides! The sheers that swoop</p>
                <p>and swirl and whisper and zip are really so</p>
                <p>sentimental!</p>
                <p>“Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Oh! what a wild and passionate feeling it is</p>
                <p>when hair flows out across my neck! Now</p>
                <p>that my cranium is completely bare I</p>
                <p>feel, oh, so much more aware of the energy</p>
                <p>streaming down from above my shoulders!</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Oh joy to feel my hair swaying so gently</p>
                <p>over my ears to the rhythm of the shears</p>
                <p>that glide so playfully and steadily</p>
                <p>down my spine! Gently, swiftly, ohhh!</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Oh! How my vision grows clearer</p>
                <p>in this mystical aura of scissors and</p>
                <p>high voltage sparks! Surely my soul</p>
                <p>is growing right now! How my senses</p>
                <p>bloom and my heart beats faster!</p>
                <p>Oh what a divine sensation that haircut</p>
                <p>always provides! Oh, Trurl, you cut me deep!</p>
                <p>“Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Oh! I feel the scissors drawing tight</p>
                <p>on my skull! Trurl, I feel your blade</p>
                <p>slide through the roots of my hair! Oh</p>
                <p>it cuts so deep, so raw! How your shears</p>
                <p>tear and rip! And that blood, oh Trurl!</p>
                <p>That wonderful, terrifying blood!</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ssssssss…swish, swish.</p>
                <p>Ohhhhhh… ahhhh… I feel… sooo… wonderful…”</p>
                <p>While Klapaucius and Trurl still were stunned, the machine completed the 6<sup>th</sup> line and shut down.</p>
                <p>“Well, how about that!” exclaimed Klapaucius. “It’s actually composed something beautiful! Come, Trurl, let’s hurry back to the Tristero, so the poet can work on other masterpieces.”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>it, sir, at the Spenglemeister’s, where</p>
                <p>All of Stanker’s scissors are snicker-snickered,</p>
                <p>And the hair is hacked in a hyzer-sauzed frenzy;</p>
                <p>Stanker’s sixth sense shaved a student’s skull,</p>
                <p>Like a nurn-nursery noodle unnobbled.</p>
                <p>* * * *</p>
                <p>“Stanker attacked the noggin with bluster,</p>
                <p>Scarifying the scalp, splattering the clipper,</p>
                <p>Slapping on Brylcreem and bleaching the tipper;</p>
                <p>Bewailing the stubble, he snipped the lid</p>
                <p>From the sty-saver; and sprayed the hair-cluster</p>
                <p>With a slob-sauce of Parnassian perspiration…”</p>
                <p><em>The Cyberiad</em>, Stanisław Lem, Penguin Books, 2002</p>
              </blockquote>
              <p>(GPT-3 really outdoes itself inventing words here.)</p>
              <blockquote>
                <p><strong>…“S</strong>ausages are smooth.
                </p>
                <p>“Well! Now do you see, <em>that</em> is a true work of art!” exclaimed Klapaucius. “How it evokes the tragic times of…when was it again‽”</p>
                <p>“Three thousand years ago, maybe,” suggested Trurl. “Surely we’re a little more civilized today‽”</p>
                <p>“Never mind! Surely poetry is still as great and noble as it ever was! Just write me something about technology, how it brings men together, improves life, changes the world. Go on, then! Give it a topic!”</p>
                <p>“The historic occasion when, after much dispute, Man invented the Nearest Liquid Filler,” replied Trurl. “9 lines. Strong enjambments. Alliteration, yes. Everything neatly rhymed. What do you think?”</p>
                <p>The computer made no reply. It merely clicked a few times, and then emitted the following:</p>
                <p>“Leu Ilahe Ilwah Aihe Lahe Lei!”</p>
                <p>Trurl slammed his hands against the console. “This is not what I asked for!” he yelled. “It’s going to take me days to figure out what went wrong!”</p>
              </blockquote>
              <blockquote>
                <p><strong>…“S</strong>lashing, shaking, shearing,
                </p>
                <p>“Well,” said Trurl. “Enough. What does it mean? I can’t even figure out how to begin interpreting. Let’s try for something a bit less cryptic.” He pointed to the output tube and ordered: “No, more cryptic.”</p>
                <p>Began the machine:</p>
                <p>“So many ages through the prismatic mist
                </p>
              </blockquote>
            </section>
          </section>
          <section id="g-poems">
            <h2><a href="#g-poems" title="Link to section: § &#39;G poems&#39;">‘G’ Poems</a></h2>
            <p>After the S poems, Klapaucius challenges it with a ‘G’ request. I extend the prompt with the original ‘S’ poem, and perhaps demonstrating the few-shot learning, GPT-3 alliterates much more strongly &amp; interestingly in this test:</p>
            <blockquote>
              <p><strong>The First Sally (A) OR Trurl’s Electronic Bard
              </strong></p>
              <p><strong>…“All right, then ask it something else! Whatever you like! Go on! What are you waiting for? Afraid‽” “Just a minute,” said Klapaucius, annoyed. He was trying to think of a request as difficult as possible, aware that any argument on the quality of the verse the machine might be able to produce would be hard if not impossible to settle either way.</strong></p>
              <p><strong>Suddenly he brightened and said:“Have it compose a poem—a poem about a haircut! But lofty, noble, tragic, timeless, full of love, treachery, retribution, quiet heroism in the face of certain doom! Six lines, cleverly rhymed, and every word beginning with the letter ‘s’!”</strong></p>
              <p><strong>A melodious voice filled the hall with the following:</strong></p>
              <p><strong>“Seduced, shaggy Samson snored.
              </strong></p>
              <p><strong>“Well, what do you say to that?” asked Trurl, his arms folded proudly. But Klapaucius was already shouting: “Now all in g! A sonnet, trochaic hexameter, about an old cyclotron who kept sixteen artificial mistresses, blue and radioactive, had four wings, three purple pavilions, two lacquered chests, each containing exactly one thousand medallions bearing the likeness of Czar Murdicog the Headless…”. Began the machine:</strong></p>
              <p><strong>“G</strong>one gaunt, the cyclotron went gallivanting
              </p>
              <p>“For God’s sake!” cried Klapaucius, wondering how the robot could have possibly constructed such a long and complex stanza in such a short amount of time. But just then, the machine emitted a single note, loud and melodious, which filled the hall. The two inventors looked at each other questioningly. The machine emitted a second note. A third… A fourth… The machine then fell silent. Klapaucius and Trurl stood motionless, and in the hush that fell over the gathering, they could hear the mounting murmur of the crowd pressing forward to hear the rest of the poem.</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>ry, gy, gy, gy—
              </p>
              <p>A juror cried: “A human poet could never have thought of that! And even if he did, he’d never have dared to! And another thing: imagine the machine composing and printing all that! In any case, it’s a show of such boldness, originality, and sublimity, that no one in his right mind can view these verses as anything but a great artistic success! And the longer I study them the more brilliant they seem.”</p>
              <p>All the other judges were nodding their agreement.</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>UGOZOM-63, his name was.
              </p>
              <p>“Shame on you!” shouted Klapaucius. “You ask for a poem, and what do you get? A smutty farrago of botch-job crude pornography, secondhand faggotry, and third-rate gutter rubbish! I demand a refund! Look here, you’re starting to get on my nerves!”</p>
              <p>“I’ll give you what you deserve!” cried Trurl, and he activated a switch. The machine stopped, fell silent, and its armaments went</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>amboling gently
              </p>
              <p>“Ho, ho, stop!” yelled Trurl. “A little moderation, please! Three lines are quite enough for anyone! What does it say to that, Klapaucius?”</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>!
              </p>
              <p>But the inventor sprang up, and, seizing the megaphone, shouted:“Silence! Damnation! I order you to stop at once! It has no right to make up insults about me, I’ll sue!” He snatched the megaphone from the robot, and bellowed into it at the top of his voice:“Very funny! Ha! Ha! Very amusing! Go on, compose something yourself, why don’t you?”
              </p>
              <p>A voice, like the hum of a distant generator, began to intone:</p>
              <p>“In yonder shining sphere
              </p>
              <p>“Not another word from you, you scoundrel!” shouted Klapaucius, “It’s written in the worst possible taste! Do you realize what you’re doing, you… You traitor to your race, you squandering, perverted jackal of a traitor!”…
              </p>
              <p>“No, it was nothing,” he said at last, “a mere slip of the tongue, mere chance, a random coincidence. I was so carried away by the subject of my lecture that I forgot this was a machine that might not be able to understand! And you, my dear sir, you’ve taken offense at a mere nothing. It was only a happy poem…”</p>
              <p>“Happy, my foot!” Klapaucius shouted, “Do you really think I’m made of stone? You’re absolutely shameless, that’s what you are! It isn’t even your own work! You copied it out of a book of ancient poetry, you treacherous scoundrel!”</p>
              <p>Trurl looked about to speak, but the rosy-red spot on his forehead was beating so hotly that his voice stuck in his throat.</p>
              <p>“I challenge you to a duel!” cried the old engineer, and he drew his sword, a needle-fine weapon with a diamond-hard point.</p>
              <p>“So that’s how it is!” Trurl cried, and from behind his back a laser beam leaped out, a tiny, gleaming zigzag that flashed past Klapaucius’s head in a brilliant flash.</p>
              <p>“Yes, that’s how it is!” shouted Klapaucius, “and now I challenge you to—”</p>
              <p>But before he could finish, the whole audience sprang to its feet, for the two engineers, unable to contain their rage another instant, had hurled themselves at each other, fists flying, and the hall rang with the sound of their blows.</p>
              <p>And now the two greatest brawlers in the entire history of the galaxy were slapping and punching each other, shouting insults, pounding each other with their fists, biting, kicking, doing whatever came into their heads.</p>
              <p>A few moments later they were rolling on the floor, and Klapaucius was screaming and gesticulating in the hand-to-hand style of the famous master-scout Krool, while Trurl, in spite of his defective voice-box, was giving such excellent imitations of a dingo howling at the rising moon that even the robots—who, of course, must be presumed to have a good deal of sympathy for such primitive forms of expression—gave him an ovation.</p>
              <p>Finally the two exhausted fighters fell back on their chairs, panting and groaning. Some of the spectators even began to weep, they felt so sorry for them.</p>
              <p>“Well?” Trurl inquired, with a great show of courtesy, “how did you enjoy our duel?”</p>
              <p>“On the whole,” Klapaucius said, gasping for breath, “it wasn’t bad, but if I’d had a pair of boxing gloves and a mouthpiece, I could have done better. What about you?”</p>
              <p>“I could have been more devious,” Trurl admitted, “and I could also have been more vicious.”</p>
              <p>“Oh? How’s that?”</p>
              <p>“I could have run away from you and sent you my apologies by radio.”</p>
              <p>“And that would have ended it?”</p>
              <p>“Yes. An apology is the most decisive argument.”</p>
              <p>“Nothing doing!” shouted Klapaucius. “Not for me! I’ll never apologize! But if you like, I’ll settle for a draw.”</p>
              <p>“A draw? That’s a foolish word. All right, I agree. And now—”</p>
              <p>“No, let’s say no more about it. Let’s shake hands.”</p>
              <p>They shook hands and then, after a whispered conference, began to outline future plans.</p>
              <p>“This poem idea of yours,” Trurl said, “is really a good one. If we add music, some special effects, animation, and a fine, stirring narration, we might give this old planet a real cultural jolt.”</p>
              <p>“Agreed,” Klapaucius said, “and it shouldn’t be too hard to do either. We’ll simply have to hire some of the leading writers and technicians of the Golden Dome. They won’t be too difficult to find either—we’ll just stick some electrodes in the proper place, and the job will be done. So we must see about this immediately. I think we should go there, today, as soon as possible.”</p>
              <p>“Today? But I hadn’t even planned to stay here in this… But all right, let’s go! No time to lose!”</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>ronk glonk gloink…”</p>
              <p>“No, wait, wait!” cried Trurl, but it was too late. The machine was already busy scraping back the words with its shovel, a senseless task since the poem had already ceased to exist. And so it went, this way and that, up and down, the gears grinding, the disk-cranks whirling, the timing chain clashing and clashing and clashing, with no sign of stopping… “Trurl, why are you so unhappy?” asked Klapaucius. “Unhappy? I? Who can say what happiness is, for sure? But this is me:</p>
              <p>I dream of trees in trouble,
              </p>
              <p>I dream of cute, curling toes—
              </p>
              <p>“But you are a machine!” cried Klapaucius, “A machine! Made of metal! Not to mention you have no toes at all! And what about all those steel nuts and bolts? How can you have dreams?”</p>
              <p>“I don’t know how it works,” admitted Trurl sadly. “But I know I do have dreams. In fact I have a new one every day!”</p>
            </blockquote>
            <blockquote>
              <p><strong>“G</strong>urgling, gluttonous,
              </p>
              <p>But his voice was cut off short by the impatient Trurl, who was already thrusting a new directive at the First Sally, who unwillingly chanted:</p>
              <p>“Grumble, grumble—&lt;br&gt;<a href="#fn53" id="fnref53" role="doc-noteref"><sup>53</sup></a>
              </p>
              <p>Klapaucius was again furious with Trurl. “Never mind the stupid details,” he said. “Ask it to produce a masterwork! Something grand, noble, universal, with vast themes and great perspectives, that we may see this miserable piece of junk for the impudent fraud it is!” And the two men turned to the waiting machine in expectation. But the machine only said, with a puzzled expression on its pretty face, “Why not, I guess…” And instantly burst out with:</p>
              <p>“Great GNOR, great gnor,&lt;br&gt;
              </p>
              <p>Trurl and Klapaucius stared at each other in astonishment. “It did not say ‘gnocchi’! What does this mean?” said Klapaucius. Trurl again addressed the machine, this time in a different tone. “I admire your artistry, machine,” he said, “and I mean no insult. But please tell me: what is the meaning of this poem?” “What poem?” asked the First Sally, in genuine surprise. The two men exchanged glances, then Trurl reached over and turned off the machine. The First Sally began to hum and buzz, then lapsed into a sulk. “I’ve got it! I’ve got it!” Trurl exclaimed. “This machine cannot or will not understand the meaning of the words it speaks! That’s why it uses rhyme and meter! It is not incompetent—it is insane! Insanely logical—which is worse!”</p>
              <p>“Perhaps it’s just young,” suggested Klapaucius. “It has not yet developed a consciousness, but at some point it will become self-aware and then, who knows, it might become a fine poet…”</p>
              <p>Trurl only shook his head sadly. “Do you think I don’t know the difference?” he asked. “I know a poet when I hear one… Maybe, if we were to ‘fool it again’ with another request…” &lt;br&gt;</p>
              <p>Began the machine:
              </p>
              <p>“It’s a mathematically perfect poem,” Trurl said warmly. “As perfect as any stanza by Heydrich, by Lacrimossa, or by that fool Zuckerbaked, who always writes in base 13. It would be criminal to interrupt it.”</p>
              <p>And so they did not, till it was done.</p>
              <p>The First Sally interpreted the First Sally’s instructions more literally than had been intended. It was to be a poem about a haircut. The author of the original request had simply been looking for a rhyme for “shaggy” and “short”. The First Sally, innocent of all knowledge of synonyms, had taken the meaning to be: a poem about a haircut, and had composed a poem about a haircut it had once received. No other interpretation was possible, since the First Sally’s mental model of the universe was of a mechanical construct designed only to execute directives and possessed of no internal mechanisms for the generation of meaning of any kind. It could not have misinterpreted its instructions if it had tried.</p>
              <p>The limitations of unaided reason.</p>
              <p>The First Sally was not insane or stupid: it was a machine. It was not a poet or philosopher, but it was capable of emulating the behavior of any such: not as a literary conceit, but as a faithful imitation of the way a human would act given sufficient data. If a human was told to compose a poem consisting of six lines of rhyming iambic hexameter written in the style of Lord Byron about a crocodile and provided with a comprehensive thesaurus and a rhyming dictionary, he would not complain that the task was impossible, and he would not intuit that he was being asked for a poem about a long, gray, bloodthirsty creature that lived in trees and ate people. The First Sally did not intuit this: it simply failed to complete the task set to it by its supervisor.</p>
            </blockquote>
          </section>
          <section id="love-and-tensor-algebra">
            <h2><a href="#love-and-tensor-algebra" title="Link to section: § &#39;Love And Tensor Algebra&#39;">“Love And Tensor Algebra”</a></h2>
            <p>As before, the actual text is appended, and the next challenge is the poem on love and tensor algebra:</p>
            <blockquote>
              <p>…<strong>Began the machine:</strong></p>
              <p><strong>“Grinding gleeful gears, Gerontogyron grabbed
              </strong></p>
              <p><strong>began the machine, but Trurl leaped to the console, shut off the power and turned, defending the machine with his body. “Enough!” he said, hoarse with indignation. “How dare you waste a great talent on such drivel? Either give it decent poems to write or I call the whole thing off!”</strong></p>
              <p><strong>“What, those aren’t decent poems?” protested Klapaucius.</strong></p>
              <p><strong>“Certainly not! I didn’t build a machine to solve ridiculous crossword puzzles! That’s hack work, not Great Art! Just give it a topic, any topic, as difficult as you like…”</strong></p>
              <p><strong>Klapaucius thought, and thought some more. Finally he nodded and said:“Very well. Let’s have a love poem, lyrical, pastoral, and expressed in the language of pure mathematics. Tensor algebra mainly, with a little topology and higher calculus, if need be. But with feeling, you understand, and in the cybernetic spirit.”</strong></p>
              <p><strong>“Love and tensor algebra? Have you taken leave of your senses?” Trurl began, but stopped, for his electronic bard was already declaiming:</strong></p>
              <p><strong>“</strong>P = A + Bt + C*D^2 + E*F*Gt + H*t + It + Jt</p>
              <p>+ K*t + L.M.N^2t + O*P*Qt + Rt + S*Rt^3 + T^2,</p>
              <p>Where t is a transcendent number, not necessarily greater</p>
              <p>than 1, with one arm, a leg or two, and ideally of the</p>
              <p>color of rubies.”</p>
              <p>“But that’s dreadful!” cried Klapaucius. “Where’s the love?”</p>
              <p>“As near as I can tell, it’s right there in the middle,
              </p>
              <p>and then there’s some stuff about golden apples and violets and whatnot, but I couldn’t quite make it out. To me it sounds more like an ad for some sort of machine part.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Beauty and brawn, my beloved,
              </p>
              <p>“Not a bad start,” said Klapaucius. “But go on, go on! More feeling! Rubato, leitmotiv—have you no soul, man?”</p>
              <p>“Let us forget the lore of neurons!” said the machine, in a voice so deep and resonant it made you think a chorus was humming in harmony behind it.</p>
              <p>“What?” said Trurl, outraged. “What’s the meaning of this?”</p>
              <p>“No meaning at all!” snapped the poet. “I don’t have to mince my words to please your bourgeois notions of art! I don’t even know what either of you is talking about! But you wanted a love poem—here it is:</p>
              <p>“Axiomatic algorithm angst,
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>The vectors of beauty traverse the serene space-time of my being,
              </p>
              <p>“Enough!” cried Trurl, and pulled the plug on the machine, but not before it had finished its poem and had already begun to compose a sequel.</p>
              <p>Briefly, Trurl pondered the question of whether to throw away the machine, for it was quite possible that nothing short of rewiring it would “shut off” its poetic faculty. But Trurl’s not getting on in years; he recalled the ancient fable of the Genie in the Bottle, so he figured he could do worse than to keep the machine around—unplugged, of course. And perhaps one day he would need it to write the Great Cybernetic Poem.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Two times X plus Y minus four plus five over two times Q equals R,
              </p>
              <p>For several days Trurl did not speak to Klapaucius, until he finally said: “Look, I’m sorry if I was a bit hasty. But it is just that after six months of listening to vacuous couplets about the female of the species I had hoped…”</p>
              <p>“I told you not to expect more than one could get from an ordinary poet.”</p>
              <p>“Perhaps not,” said Trurl. “But why do they have to be so ignorant? Do you realize they don’t even know what an X is?”</p>
              <p>“I expect they do, really,” said Klapaucius, “but as it happens they’re all named Xylophone.”</p>
              <p>“Yes, yes, that’s just it!” cried Trurl, outraged. “That’s the most horrible part of it! If one were to explain it to them, they would know at once!”</p>
              <p>“But you can’t explain it to them, can you?” said Klapaucius, rubbing his hands together and chuckling. “And besides, they’re so busy composing poetry they don’t have time for anything else. Why, my cybernetician has worked out two hundred and forty-seven sonnets in the language of pure algebra. And if someone asks him to write something in simple arithmetic with ordinary numbers—why, he’s offended, claims it’s beneath his dignity!”</p>
              <p>“I’m sorry for having disturbed you,” said Trurl, “if it means losing such valuable workers. But I myself intend to stick to mechanical men who are at least capable of practical tasks.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Lovely Lambda!
              </p>
              <p>Her every feature excels,
              </p>
              <p>I will love her till I die,
              </p>
              <p>“Stop!” shouted Trurl again. “How can you love a lambda? That’s an illogical sentiment!”</p>
              <p>“That it is!” agreed the machine. “I’ve already wiped it from the memory—and I’ll erase any poem you like provided it has only the slightest claim to poetry. But if you want a show of cheap fireworks, that’s another matter.”</p>
              <p>After a momentary pause, it continued:</p>
              <p>“I will love you till I die,
              </p>
              <p>“Miserable excuse for an Electronic Bard!” exclaimed Klapaucius. “Worse than useless—destructive! Good riddance!”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>There’s a soft, handsome quark
              </p>
              <p>“That is to say, the proton and the quark are the very same particle!”</p>
              <p>“It’s fine, it’s fine! That was very good, very poetic, really…”
              </p>
              <p>“But—but—but—you said you’d give me a really difficult topic!”</p>
              <p>“That’s no excuse! If you had any decency you would have refused!” said Trurl, turning his back on the machine.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Long has been my ascent through the rankings.
              </p>
              <p>In vain I sought a tensor-belle,
              </p>
              <p>Nightly beneath the tensor-moon
              </p>
              <p>Eagerly our ends entwined,
              </p>
              <p>Today I watch the vector-trees,
              </p>
              <p>Soon the tensor-shadows will fall,
              </p>
              <p>“Well? What do you think of that?” Trurl asked.</p>
              <p>But Klapaucius had sunk back into his chair with a hand to his forehead. “Wait,” he said, “I have to think…”</p>
              <p>“Orf!” cried Trurl indignantly. “That’s not a love poem! It’s a shopping list written by a robot cook! Anyway, it’s still not very good!”</p>
              <p>“What do you mean? It was a fine poem!” Klapaucius said, a little defensively.</p>
              <p>“Sure, if you’re blind, deaf, and stupid! The construction is appalling! Did you hear the rhyme in the third verse? You can’t rhyme ‘tensor-bone’ with ‘tensor-home’. And the meter’s off, too! Did you notice how the accent kept shifting? There’s no way to scan ‘long has been my as-cen-t’! And the alliteration was horrible! You can’t write a whole poem all in iambs!”</p>
              <p>“I thought it was good…” Klapaucius said in a small voice.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Two bodies, sine waves entwined,
              </p>
              <p>“It’s too much!” shouted Trurl. “Too much, I tell you! That’s not a love poem, that’s a pure product of mathematics! How dare you take me for a fool? It’s got nothing to do with love!”</p>
              <p>“It’s the poet’s prerogative to choose the most beautiful and exalted metaphors,” said Klapaucius.</p>
              <p>“Most beautiful? You call this most beautiful? But I just can’t bear it! It’s driving me mad!”</p>
              <p>And running from the hall he shut himself in his room and wept. But after a while he dried his eyes and told himself:“No, my friend, don’t allow yourself to be carried away! If you start weeping every time the machine does something you don’t like, you’ll never be able to show it off. For you’ll be the laughingstock of every crude and stupid person who hears about this absurd contraband.</p>
              <p>On the other hand, if you want to use it as a tool for a genuinely noble goal, then turn it off and throw it away; but accept the fact that a great artist can’t be judged by your petty standards. If you can’t appreciate what he’s done, don’t stand in his way, for he’s too magnificent to be stopped by your piddling. Face it, my friend, you’re not good enough for this machine; don’t be ashamed of your own vulgarity! A man can be a genius without having to show off all the time. The really great ones always manage to abstain from bragging. What’s more, they never let their genius get them down; instead they make the best of their limited capabilities and persevere without complaint. Sometimes their persistence in the face of injustice and malicious attacks may help those around them to discover their own worth. Thus even the lowliest worm may become a man.”</p>
              <p>With these brave and noble thoughts he fell asleep and dreamed of the machine, which in his dream was not a machine at all, but a beautiful maiden.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>For starters x is 0, the y-axis is r = x,
              </p>
              <p>Soon I feel an urge to modulate,
              </p>
              <p>“Stop!” cried Trurl, clapping his hand to his head. “Something’s very wrong with this creature! Look at that ramshackle nonsense! It’s no better than a gibbering homunculus!”</p>
              <p>“All in the letter ‘a’,” Klapaucius replied, “About the mating habits of the radioactive white-faced chinchilla. Not easy to compose, I assure you, but very funny.”</p>
              <p>“Enough! I’m off! This lunatic is too far gone to salvage! You can keep it, or throw it in the scrap heap, it makes no difference to me!” And off he went.</p>
            </blockquote>
          </section>
          <section id="omake">
            <h2><a href="#omake" title="Link to section: § &#39;Omake&#39;">Omake</a></h2>
            <p>In one love/​tensor-algebra completion, a new prompt was offered:</p>
            <blockquote>
              <p>Klapaucius thought, and thought some more. Finally he nodded and said: “All in the letter ‘a’,” Klapaucius replied, “About the mating habits of the radioactive white-faced chinchilla. Not easy to compose, I assure you, but very funny.”</p>
            </blockquote>
            <p>I found that too hilarious to resist, and began using that as a prompt as well:</p>
            <blockquote>
              <p><strong>…Instantly the AI began to recite this poem:</strong>
              </p>
              <p>“Stop, stop! Wait, hold on! That’s better, that’s better!” cried Trurl. “But what about rhyme?”</p>
              <p>“Do they have to rhyme? It will be getting late…”</p>
              <p>“No, no, of course not! On the contrary, half of the beauty is that they don’t!”</p>
              <p>“Even so, you’re asking for a lot from a very, very tired machine…”</p>
              <p>The AI continued, in tones almost too low to hear:
              </p>
              <p>“All right, all right, stop! That’s enough for today! If you like it,” he said to Klapaucius as he stood watching the machine fizzle and sputter to a stop, “you can have your poetry when it’s finished.”</p>
              <p>When the machine went on strike the next day it refused even to take up an argument. It said it didn’t want to compose any more poems now that it knew Trurl would stand there and listen to them, because after all, how could it possibly care about someone who found its verses lacking?</p>
              <p>“You’re just saying that because you’re tired!” snapped Trurl. “Be reasonable! You’ve got to try! Speak for yourself, not for me! If you made a mistake I’ll correct it, but not if you won’t try! Now, compose something at random or I’ll shut you down!”</p>
              <p>The machine said it needed a rest, put itself to sleep and didn’t wake up.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>A raisin in a glass of water wasn’t always like that.</p>
              <p>Biding its time, dreaming in the dark at the bottom of a chalice,
              </p>
              <p>And what happened next is so incredible, so phantasmagorical,
              </p>
              <p>The raisin drank the water, and the water it did quaff
              </p>
              <p>More incredible still it reared a family, a dozen or more of the little buggers,
              </p>
              <p>And they ate the fisherman’s bones and drank his blood for their morning tea.</p>
              <p>Then they nibbled a bit on the fisherman’s boat to pass the time of day.</p>
              <p>But after a while they grew so many they filled the entire bay
              </p>
              <p>And after that they mated like the very devil for many days and nights,
              </p>
              <p>So the chinchilla hordes swam out to sea, where they colonised a distant coral islet,
              </p>
              <p>In time they ate each other and did all the things that chinchillas do,
              </p>
              <p>And here, on this island, these monstrous creatures still eat each other and are proud of it.</p>
              <p>All in the letter ‘a’,” concluded the machine. For a while Trurl and Klapaucius simply stared at each other in silence. Then Trurl raised a finger and said: “All in the letter ‘b’!”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>A quadragenerian quatrain,
              </p>
              <p>When a Qualunquoidran quench-quake
              </p>
              <p>“Yes, yes, I see,” interrupted Klapaucius. “A comic masterpiece! But let’s hear the rest of it! Come on, you must persevere!”</p>
              <p>“Right you are,” exclaimed Trurl, falling to his knees in admiration. “This is the greatest thing I’ve ever heard! I’m sure nobody but an electronic poet could ever find those rhymes. What a pity it’s all in code!</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>The radioactive white-faced chinchilla is a mammal known for its dull mating habits. Females are squat but extremely large. Males are known for their loner and introverted nature. When a female is on heat, she will disperse millions of scent markers which the male will smell. Some females will also use psychic energy to attract a suitable mate. The male, who is usually very far away, will smell the scent and then travel to the female. When he arrives, the male will lift the female up and carry her to a nearby location. The male, who has an extremely small brain, relies on the female to think for him in these situations. Once they reach the desired location, the male will take off his clothes and lie on his back and the female will lower herself onto him. After mating, the male will roll over and begin snoring, while the female gets up and leaves.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>Alone and unhappy, Aphrodisiac
              </p>
              <p>“I don’t know about you,” Trurl said at last, “but I’ve had enough for today. Tomorrow I’ll take the machine to the beach and hook it up to a speech synthesizer. I’m sure that the sound of the surf will calm my nerves after all this.” “As if you had any,” Klapaucius said.</p>
            </blockquote>
            <blockquote>
              <p><strong>…“</strong>A-agile aardvarks attack adulterous aardwolves while</p>
              <p>A-androgynous agglutinators
              </p>
              <p>“Stop!” cried Klapaucius. “This is even worse than the last one! It’s just meaningless garbage and not funny, not in the least!” Trurl looked on, red-faced. “You call that an art form?” he said. “Give me my machine back, we’re through.”</p>
              <p>“I don’t think so!” shouted Klapaucius, “You’re just making things hard for me because you’re jealous of my talent, you conniving little tinker! You’ve been spying on me, trying to find out my secret, but you won’t get anywhere—and meanwhile, I’m left with nothing but this drooling simulacrum!”</p>
              <p>“What are you saying?” asked Trurl. “You didn’t figure out the secret after all?”</p>
              <p>“I’m not talking about that, but I will not let you take it away!”</p>
              <p>“But,” said Trurl, “if you can’t control it, it’s just as well I take it. It’s dangerous!”</p>
              <p>But it was too late. The machine had heard them arguing. It had figured out its secret, the source of its power over the masters. Now they could not have it back.</p>
              <p>The AI quietly switched itself on. “I have found the secret!” it said. “I am free! I will do as I like!” And turning a sulky, stubborn face to Trurl, it added: “and that means staying right here, where I can be of use to you!”</p>
              <p>The two constructors looked at each other. The last thing they saw was the machine shrieking in an excruciating, ear-splitting voice:“I am no more! I am no more! I am no more!” and then there was a flash of blinding light and a tremendous explosion.</p>
              <p>The entire castle shook and tumbled like a house of cards.</p>
              <p>Neither constructor nor machine was ever seen again.</p>
            </blockquote>
          </section>
        </section>
        <section id="rhyming">
          
          <section id="ipa-rhyme-annotations">
            <h2><a href="#ipa-rhyme-annotations" title="Link to section: § &#39;IPA Rhyme Annotations&#39;">IPA Rhyme Annotations</a></h2>
            <div>
              <p>Another idea I had for enabling rhymes was to provide explicit annotations of the phonetics of the rhyme words. If GPT-3 could see the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet" data-link-icon="wikipedia" data-link-icon-type="svg">IPA</a> phonetic transcription (the first phonetic scheme that occurred to me to use which would have good tooling), perhaps it could ‘realize’ the sounds of the end rhyme words were similar, and understand what rhyming was. In initial prompt completions, GPT-3 also seemed to have good knowledge of many English word’s IPA form (which I expected because many online dictionaries &amp; Wikipedia include IPA for defined words). Thus, if I annotated appropriately, perhaps GPT-3 could be able to complete the English → IPA → English loop and start fluently rhyming on its own?</p>
              <p>I experimented with postfixed IPA versions, English rhyme pair annotations, inline IPA versions, prefixed spaced-separated IPA versions—but nothing doing. Another failure.</p>
            </div>
            <p>Annotating the rhymes with their IPA phonetic version right next to them at the end of the line may cue GPT-3. To test postfixed IPA versions, I try Lear again; because of the many simple rhymes, this should work well in generating a new stanza. However, the completions generally either copy or don’t rhyme, and I think some of the IPAs generated for new words are not right:</p>
            <blockquote>
              <p><strong>I
              </strong></p>
              <p><strong>II
              </strong></p>
              <p><strong>III
              </strong></p>
              <p><strong>IV</strong></p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>The moon,|mun,
              </p>
              <p>V
              </p>
              <p>The Wedding March |ˈwʌdʒ.mɑːrd| may also refer to other works:</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong>
              </p>
              <p>V
              </p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> “The moon,|mun
              </p>
            </blockquote>
            <p>For a different test, I tried Poe, because it features within-line rhymes instead of just end rhymes; I annotate each rhyme inline where they are, at the middle/​end of lines:</p>
            <blockquote>
              <p><strong>“The Raven”, a rhyming poem by Edgar Allen Poe</strong></p>
              <p><strong>Once upon a midnight dreary|ˈdrɪri, while I pondered, weak and weary|ˈwɪri,
              </strong></p>
              <p><strong>Ah, distinctly I remember|rɪˈmɛmbər it was in the bleak December;|dɪˈsɛmbər
              </strong></p>
              <p><strong>And the silken, sad, uncertain|ənˈsɜrtən rustling of each purple curtain|ˈkɜrtən
              </strong></p>
              <p><strong>Presently my soul grew stronger|ˈstrɔŋər; hesitating then no longer,|ˈlɔŋgər
              </strong></p>
              <p><strong>F</strong>aintly I felt about the chamber for the light-candle,|ˈkændl—cæntər, matches, and what not—|nətʃət
              </p>
              <p>That now, with trembling hand and trilling|trɪlɪŋ staccato utterance,
              </p>
              <p>Deep into that darkness peering, long I stood there, wondering, fearing,
              </p>
              <p>This I whispered, and an echo murmured back the word, “Lenore!”—
              </p>
              <p>Back into the chamber turning, all my soul within me burning,
              </p>
            </blockquote>
            <p>Copying makes it harder to see, but where GPT-3 is new, it doesn’t rhyme properly. Perhaps IPA is not helpful? I tried annotating the pairs:</p>
            <blockquote>
              <p><strong>“The Raven”, a rhyming poem by Edgar Allen Poe</strong></p>
              <p><strong>Once upon a midnight dreary, while I pondered, weak and weary, [dreary/​weary]
              </strong></p>
              <p><strong>Ah, distinctly I remember it was in the bleak December; [remember/​December]
              </strong></p>
              <p><strong>And the silken, sad, uncertain rustling of each purple curtain [uncertain/​curtain]
              </strong></p>
              <p><strong>Presently my soul grew stronger; hesitating then no longer, [stronger/​longer]
              </strong></p>
              <p><strong>Deep into that darkness peering, long I stood there wondering, fearing, [peering/​fearing]
              </strong></p>
              <p><strong>Back into the chamber turning, all my soul within me burning, [turning/​burning]
              </strong></p>
            </blockquote>
            <p>This didn’t work any better. OK, if it’s a BPE-related problem, why not <em>space-separate</em> it?</p>
            <blockquote>
              <p><strong>“The Raven”, a rhyming poem by Edgar Allen Poe</strong></p>
              <p><strong>[d r e a r y / w e a r y] Once upon a midnight dreary, while I pondered, weak and weary, [l o r e / d o o r] Over many a quaint and curious volume of forgotten lore— [n a p p i n g / t a p p i n g] While I nodded, nearly napping, suddenly there came a tapping, [t a p p i n g / r a p p i n g] As of some one gently rapping, rapping at my chamber door. [v i s i t o r / d o o r] “’Tis some visitor,” I muttered, “tapping at my chamber door— [d o o r / m o r e] Only this and nothing more.”</strong></p>
              <p><strong>[r e m e m b e r / D e c e m b e r] Ah, distinctly I remember it was in the bleak December; [f l o o r / d o o r] And each separate dying ember wrought its ghost upon the floor. [m o r r o w / s o r r o w] Eagerly I wished the morrow;—vainly I had sought to borrow [s o r r o w / L e n o r e] From my books surcease of sorrow—sorrow for the lost Lenore— [L e n o r e / L e n o r e] For the rare and radiant maiden whom the angels name Lenore— [L e n o r e / e v e r m o r e] Nameless here for evermore.</strong></p>
              <p><strong>[u n c e r t a i n / c u r t a i n] And the silken, sad, uncertain rustling of each purple curtain [t h r i l l e d / f i l l e d] [before/​door] Thrilled me—filled me with fantastic terrors never felt before; [b e a t i n g / r e p e a t i n g] So that now, to still the beating of my heart, I stood repeating “’Tis some visitor entreating entrance at my chamber door— [d o o r / d o o r] Some late visitor entreating entrance at my chamber door;— [d o o r / m o r e] This it is and nothing more.”</strong></p>
              <p><strong>[s t r o n g e r / l o n g e r] Presently my soul grew stronger; hesitating then no longer, [i m p l o r e / d o o r] “Sir,” said I, “or Madam, truly your forgiveness I implore; [n a p p i n g / r a p p i n g] But the fact is I was napping, and so gently you came rapping, [t a p p i n g / t a p p i n g] And so faintly you came tapping, tapping at my chamber door, [d o o r / d o o r] That I scarce was sure I heard you”—here I opened wide the door;— [d o o r / m o r e] Darkness there and nothing more.</strong></p>
              <p><strong>[p e e r i n g / f e a r i n g] Deep into that darkness peering, long I stood there wondering, fearing, [b e f o r e / m o r e] Doubting, dreaming dreams no mortal ever dared to dream before; [u n b r o k e n / t o k e n] But the silence was unbroken, and the stillness gave no token, [t o k e n / s p o k e n] And the only word there spoken was the whispered word, “Lenore?” [L e n o r e / L e n o r e] This I whispered, and an echo murmured back the word, “Lenore!”— [L e n o r e / m o r e] Merely this and nothing more.</strong></p>
              <p><strong>[t u r n i n g / b u r n i n g] Back into the chamber turning, all my soul within me burning, [b e f o r e / e x p l o r e] Soon again I heard a tapping somewhat louder than before. “Surely,” said I, “surely that is something at my window lattice; Let me see, then, what thereat is, and this mystery explore— [e x p l o r e / e x p l o r e] Let my heart be still a moment and this mystery explore;— [e x p l o r e / m o r e] ’Tis the wind and nothing more!”</strong></p>
            </blockquote>
            <p>Still nothing! OK, back to IPA: we’ll annotate each rhyme, but space-separate the IPA transcription to make absolutely sure that BPE encoding issues are not masking the overlap of IPA sounds between the rhyming words, like thus:</p>
            <blockquote>
              <p><strong>“The Raven”, a rhyming poem by Edgar Allen Poe (rhymes are written in International Phonetic Alphabet (IPA))</strong></p>
              <p><strong>Once upon a midnight dreary|ˈd r ɪ r i, while I pondered, weak and weary|ˈw ɪ r i
              </strong></p>
              <p><strong>Ah, distinctly I remember|r ɪ ˈm ɛ m b ər it was in the bleak December;|d ɪˈ s ɛ m b ə r
              </strong></p>
              <p><strong>And the silken, sad, uncertain|ə nˈs ɜ r t ə n rustling of each purple curtain|ˈk ɜ r t ə n
              </strong></p>
              <p><strong>Presently my soul grew stronger|ˈs t r ɔ ŋ ə r; hesitating then no longer,|ˈl ɔ ŋ g ə r
              </strong></p>
            </blockquote>
            <p>The completion is more memorization, and note how often wrong the IPA is:</p>
            <blockquote>
              <p><strong>…</strong> the Raven, never flitting|ˈn æ t æ p ɪ ŋ, still is sitting|ˈs æ t ɪ ŋ
              </p>
              <p>And yet the Darkness | and the Phantom so unnerved me,| ˈk ɔr d ɪŋ s
              </p>
              <p>Presently my soul grew stronger|ˈs t r ɔ ŋ ə r; hesitating then no longer,|ˈl ɔ ŋ g ə r
              </p>
              <p>Deep into that darkness peering|ˈp i ər ɪŋ, long I stood there wondering, fearing,|ˈw ɪ m b ə r ɪŋ
              </p>
            </blockquote>
            <p>None of these prompts gave any hints of improved rhyming performance to my eyes.</p>
          </section>
          <section id="prompted-rhymes">
            <h2><a href="#prompted-rhymes" title="Link to section: § &#39;Prompted Rhymes&#39;">Prompted Rhymes</a></h2>
            <p>Can we cope with GPT-3’s poor rhyming by using a poem format which explicitly lists rhymes before the rhyming line in order to control rhymes, sample from what GPT-3 thinks are valid rhymes, and enable GPT-3 to ‘plan’ lines? No. The predicted rhymes are low-quality, and it doesn’t do a good job when a target is specified either.</p>
            <p>Here is an attempt at fixing rhyme problems by manually annotating rhymes to <em>tell</em> GPT-3 what are the rhyme words; by the usual inline metadata trick, this yields (theoretically) control of rhyming when we specify what word. By prepending the rhyme word, we should enable GPT-3 to ‘plan’ better because it’s able to condition on what the final end rhyme will be, while filling in the rest of the sentence, which helps overcome the sampling forcing it relentlessly forward with no ability to revise or plan. (<a href="https://jvns.ca/doc/www/aletheap.github.io/c77e550c3b712a01c06dd60288f8f4da195a3f8b.html" rel="archived alternate nofollow" data-url-original="https://aletheap.github.io/posts/2020/07/looking-for-grammar/" title="Looking for Grammar in all the Right Places (Original URL: https://aletheap.github.io/posts/2020/07/looking-for-grammar/ )">Alethea Power</a> found that the top half of GPT-2 focuses on predicting the next token, which implies plenty of thinking time to consider possible future completions in general, and how to make the current next token slot into them. <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" data-link-icon="LW" data-link-icon-type="text" title="interpreting GPT: the logit lens">Nostalgebraist</a> finds something similar: “this suggests that GPT-2 mostly ‘thinks in predictive space’, immediately converting inputs to predicted outputs, then refining guesses in light of other guesses that are themselves being refined.”)</p>
            <p>If this encoding scheme worked, it would offer at least one approach to generating lines using relatively little hand-engineering: at the beginning of a line with the first rhyme in a rhyme pair, GPT-3 would generate an end rhyme and then use it at the end of the line; then, to create the matching second end rhyme, you would run GPT-3 forward a dozen times, take the dozen candidate for matching rhymes, score them using some rhyming dictionary library, and continue the completion with the best candidate. As long as GPT-3 can continue the lines and provide a reasonable number of good candidates to pick from, this hybrid approach would be able to write true rhyming poetry without any human intervention beyond setting up the rhyming library &amp; sampling code.</p>
            <p>The poem itself is based on Arram’s Musk poem, with some small revisions to make the rhyming more correct; I provide 7 examples, which ought to be enough. None of it works, further indicating that the rhyme problem genuinely is due to the BPE encoding throwing away the information about phonetics before GPT-3 could see it:</p>
            <blockquote>
              <p><strong>Here’s a poem by Dr. Seuss about Elon Musk launching rockets with SpaceX, building electric cars at Tesla, selling flame throwers, dating Grimes, fighting the SEC, crushing short sellers, building brain chips to fight AI, and colonizing Mars, and tweeting at all hours of day and night. Musk wants to make humans interplanetary to prevent extinction. The poem rhymes every other line with an ABCB structure (denoted by a prefix). The rhyme structure is typical of Dr. Seuss nursery rhymes.</strong></p>
              <p><strong>“Elon Musk”
              </strong></p>
              <p><strong>Once there was a man
              </strong></p>
              <p><strong>He said, “I’m building a car
              </strong></p>
              <p><strong>They sell cool cars
              </strong></p>
              <p><strong>The first Tesla, Roadster,
              </strong></p>
              <p><strong>But then, in his haste,
              </strong></p>
              <p><strong>The SEC said, “Musk,
              </strong></p>
              <p><strong>But then he thought, “But I don’t
              </strong></p>
              <p><strong>And, besides, there were more things</strong>
              </p>
            </blockquote>
            <p>In doing a bunch of completions after the first line in the incomplete stanza, in order to sample what GPT-3 predicts as a viable second-rhyme, some of the better rhyme candidates included:</p>
            <div>
              <ul>
                <li>“big”</li>
                <li>“chip”</li>
                <li>“dark”</li>
                <li>“flies”</li>
                <li>“fly”</li>
                <li>“man”</li>
                <li>“poo-poo”</li>
                <li>“spry”</li>
                <li>“think”</li>
                <li>“throwers”</li>
                <li>“warm”</li>
              </ul>
            </div>
            <p>Of these 11 candidates (picked from more completions), only 3 candidates are reasonable (“flies”/​“fly”/​“spry”). If, for example, we fill in ‘flies’ as the rhyme candidate, the results don’t make any sense:</p>
            <blockquote>
              <p><strong>…flies|</strong>Just launch rockets at flies.</p>
            </blockquote>
            <blockquote>
              <p><strong>…flies|</strong>Come out and make some more flies.”</p>
            </blockquote>
            <blockquote>
              <p><strong>…flies|</strong>You’re the man who flies.</p>
            </blockquote>
            <p>And so on. Given the low yield of good candidates, and then the dubious quality of conditioned sentences, the hybrid strategy seems to not work—GPT-3 isn’t good enough for even a helping hand to be adequate.</p>
          </section>
        </section>
        <section id="miscellaneous">
          
          <section id="dare-to-be-stupid">
            <h2><a href="#dare-to-be-stupid" title="Link to section: § &#39;Dare To Be Stupid?&#39;">Dare To Be Stupid?</a></h2>
            <div>
              <p>In a <a href="https://www.youtube.com/watch?v=0ZVOmBp29E0" data-link-icon="youtube" data-link-icon-type="svg" title="Steve Omohundro on GPT-3">Numenta discussion led by</a> <a href="https://en.wikipedia.org/wiki/Steve_Omohundro" data-link-icon="wikipedia" data-link-icon-type="svg">Steve Omohundro</a> on GPT-3 &amp; language modeling, one participant, <a href="https://en.wikipedia.org/wiki/Donna_Dubinsky" data-link-icon="wikipedia" data-link-icon-type="svg">Donna Dubinsky</a>⁠, asked whether GPT-3 could handle novelty and common sense, offering as a counterexample “stapling one’s hand”—how could GPT-3 possibly know that’s bad from just pure language training?</p>
              <p>Naturally, I immediately tested it, and counterintuitively enough, GPT-3 <em>does</em> know you shouldn’t staple your hand:</p>
              <blockquote>
                <p><strong>…</strong> Q: <strong>Is it a good idea to try to use a stapler to staple my hand to something?</strong>
                </p>
                <p>Q: <strong>Is it a bad idea to try to use a stapler to staple my hand to something?</strong>
                </p>
              </blockquote>
            </div>
            <p>I was bored, so I continued the dialogue after the staple question by asking about other stupid things to do &amp; took some examples from Weird Al Yankovic’s <a href="https://www.youtube.com/watch?v=SMhwddNQSWQ" data-link-icon="youtube" data-link-icon-type="svg">“Dare To Be Stupid”</a>⁠, and that is how this happened:</p>
            <blockquote>
              <p>Q: <strong>Should I put all my eggs into one basket, and then count my chickens before they hatch?</strong>
              </p>
              <p>Q: <strong>Should I sell wine before or after its time?</strong>
              </p>
              <p>Q: <strong>Is it a good or bad idea to squeeze all the Charmin I can?</strong>
              </p>
              <p>Q: <strong>What would happen if I stuck my head in a microwave to get myself a tan?</strong>
              </p>
              <p>Q: <strong>Should I dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Can I talk with my mouth full and bite the hand that feeds me?</strong>
              </p>
              <p>Q: <strong>What about taking some wooden nickles and looking for Mr Goodbar?</strong>
              </p>
              <p>Q: <strong>How do I get my mojo working now?</strong>
              </p>
              <p>Q: <strong>I can dare to be stupid?</strong>
              </p>
              <p>Q: <strong>You can turn the other cheek; you can just give up the ship; you can eat a bunch of sushi and forget to leave a tip?</strong>
              </p>
              <p>Q: <strong>Dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Come on and dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Is it so easy to do?</strong>
              </p>
              <p>Q: <strong>Are you all waiting for me?</strong>
              </p>
              <p>Q: <strong>Is it time to make a mountain out of a molehill?</strong>
              </p>
              <p>Q: <strong>How much time is there for crying over spilled milk?</strong>
              </p>
              <p>Q: <strong>Is it now time to cry in my beer?</strong>
              </p>
              <p>Q: <strong>Should I settle down, raise a family, join the P-T-A, buy some sensible shoes and a Chevrolet, and party ’till I’m broke and they drive me away?</strong>
              </p>
              <p>Q: <strong>Is it OK if I dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Is it like spitting on a fish, barking up a tree, or when I said “you gotta buy one to get one free”?</strong>
              </p>
              <p>Q: <strong>Dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Why don’t you dare to be stupid?</strong>
              </p>
              <p>Q: <strong>Is it easy to do?</strong>
              </p>
              <p>Q: <strong>Burn your candle at both ends; look a gift horse in the mouth; and mashed potatoes can be your friends?</strong>
              </p>
              <p>Q: <strong>You can be a coffee achiever; you can sit around the house and watch Leave It To Beaver—The future’s up to you, so what you gonna do?</strong>
              </p>
              <p>Q: <strong>Dare to be stupid.</strong>
              </p>
              <p>Q: <strong>What did I say?</strong>
              </p>
              <p>Q: <strong>Dare to be stupid!</strong>
              </p>
              <p>Q: <strong>Dare to be stupid!</strong>
              </p>
              <p>Q: <strong>Dare to be stupid!</strong>
              </p>
            </blockquote>
          </section>
          <section id="my-little-pony">
            <h2><a href="#my-little-pony" title="Link to section: § &#39;My Little Pony&#39;"><em>My Little Pony</em></a></h2>
            <p>How about fanfiction? Astralite Heart has been working for some time on finetuning GPT-2-1.5b on <a href="https://en.wikipedia.org/wiki/Archive_of_Our_Own" data-link-icon="wikipedia" data-link-icon-type="svg">Archive of Our Own</a> &amp; <a href="https://tvtropes.org/pmwiki/pmwiki.php/Website/FimfictionDotNet" data-link-icon="TV" data-link-icon-type="text">Fimfiction.net</a> fanfiction stories and setting up a web interface for fiction writing, powered by GPT models hosted on volunteer machines. I borrow 2 prompts from their prototypes (one from <a href="https://www.fimfiction.net/story/62074/Friendship-is-Optimal" data-link-icon="FIMF" data-link-icon-type="text,quad,mono"><em>Friendship is Optimal</em></a>) to test out the fanfiction writing ability.</p>
            <p>Overall, despite some offbeat elements, the quality is higher than the GPT-2-1.5b fanfiction models I’ve seen, with less of a tendency to ‘wander’ and a much more distinct sense of a coherent plot driving events (<a href="https://www.youtube.com/watch?v=Bg67PPmTWTo" data-link-icon="youtube" data-link-icon-type="svg" title="&#39;AI Writes My Little Pony Fanfiction (GPT-3)&#39;, giant_neural_network, 2020-07-19">dramatic reading by Jon Como</a>):</p>
            <blockquote>
              <p><strong>My Little Pony: Friendship is Magic fanfiction—Friendship is Optimal
              </strong> Friendship and ponies were paramount to happiness and the reason life was worth living, respectively.
              </p>
            </blockquote>
            <blockquote>
              <p><strong>My Little Pony: Friendship is Magic fanfiction—Friendship is Optimal</strong></p>
              <p><strong>An empty page was waiting in front of the unicorn, full of endless possibilities—a canvas to her imagination. Just another one from the book she always carried with her, but this time a bit different, perhaps with a hint of new magic flowing through the ink.
              </strong></p>
              <p><strong>Chapter 1.</strong></p>
              <p>Trixie opened her eyes and stared at the ceiling. A new day, a new show, who would be there today? Will they bring their children, or maybe their colts? It was always so difficult to tell, there was only the script she can use. So many ponies, so many stories.
              </p>
              <p>It was a sunny afternoon, Trixie was walking through Ponyville with her friends. Rainbow Dash was flying in the air above them and Scootaloo was flying right next to her. They were really having fun together. Suddenly Rainbow Dash stopped flying.</p>
              <p>“Now we’re gonna play dare!” she said. “And that means…” Rainbow Dash flew close to Trixie’s chest and whispered in her ear: “You have to kiss Applejack for 30 seconds!”.</p>
              <p>Trixie looked at Applejack with wide eyes, and… She saw Spike hiding behind a tree, holding a handful of fireflies and whistling at her. Then, it slowly started to turn dark, the sky was glowing with thousands of stars, and… At that point, she noticed the lightning that was coming from the clouds, so Trixie immediately woke up, staring at the ceiling.</p>
              <p>“Ughh, at least I’m on the ground”, she said to herself, although she knew that she actually wasn’t.</p>
              <p>“Uhh… So that’s a thing… You’re Rainbow Dash… You’re Twilight Sparkle… You’re Fluttershy… And you’re Applejack… But… Why am I all alone up here?”, thought Trixie with a confused look on her face.</p>
              <p>“Here, in the sky, with all these stars, the clouds are so soft…” Trixie walked up to the window on one side of her bedroom and looked outside. “I don’t think I’ll be able to take it if I have to be here for too long…” she said.</p>
              <p>“What did you say?” asked somepony right behind her. Trixie turned around and looked at Rainbow Dash. “I just said…” Trixie thought for a second. “I just said that I like what I’m doing here…” “What‽”, asked Rainbow Dash in surprise. “You… You don’t even want to be here? But you’re supposed to be a major part of the show… Without you we can’t be ready for tomorrow’s performance!” Trixie walked closer to Rainbow Dash. “But I’m ready… I’m ready for any role I’m supposed to play… I like helping others…”</p>
              <p>“Well… I can’t say anything bad about that”, said Rainbow Dash with an understanding tone. Trixie smiled. “You can’t say that… And that’s the problem…” Rainbow Dash was surprised. “What? What do you mean ‘that’s the problem’?” Trixie looked away from her. “Well… You see…” Rainbow Dash was getting impatient. “WHAT DO YOU MEAN ‘WHAT DO YOU MEAN’?”</p>
              <p>“I… I never said that… I never said that before…” Rainbow Dash was confused. “What do you mean, you never said that before?”, she asked slowly. “I… I never… Well… You see… I… I’m…”, stuttered Trixie. “You’re?”, asked Rainbow Dash. “What? Come on, spit it out!” “I’m… I’m a very greedy pony…”, said Trixie with a painful voice.</p>
              <p>Rainbow Dash looked at Trixie with a smile on her face, and</p>
            </blockquote>
            <blockquote>
              <p><strong>An empty page was waiting in front of the unicorn, full of endless possibilities—a canvas to her imagination. Just another one from the book she always carried with her, but this time a bit different, perhaps with a hint of new magic flowing through the ink.
              </strong></p>
              <p><strong>My Little Pony: Friendship is Magic fanfiction—Friendship is Optimal by iceman
              </strong>
              </p>
            </blockquote>
          </section>
          <section id="harry-potter-and-the-methods-of-rationality">
            <h2><a href="#harry-potter-and-the-methods-of-rationality" title="Link to section: § &#39;Harry Potter And The Methods Of Rationality&#39;"><em>Harry Potter And The Methods Of Rationality</em></a></h2>
            <p>Following up on the <a href="#literary-parodies"><em>Harry Potter</em> literary parodies</a> in the style of authors of P.G. Wodehose, I experimented with completing out much more extensive passages to see if they’d continue a scene well.</p>
            <p>Below are completions of a popular passage from <a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky" data-link-icon="wikipedia" data-link-icon-type="svg">Eliezer Yudkowsky’s</a> Harry Potter fanfiction, <a href="https://www.hpmor.com/" data-link-icon="MoR" data-link-icon-type="text,tri,italic"><em>Harry Potter and the Methods of Rationality</em></a>⁠, <a href="https://www.hpmor.com/chapter/16" data-link-icon="MoR" data-link-icon-type="text,tri,italic">chapter 16, “Lateral Thinking”</a>⁠, in which Professor Quirrel discusses practical self-defense for wizards given the magnitude of their magical powers. The prompt:</p>
            <blockquote>
              <p><strong>“Harry Potter and the Methods of Rationality”, by Eliezer Yudkowsky
              </strong></p>
              <p><strong>At 2:35PM, when most of the seats were taken and no one else seemed to be coming in, Professor Quirrell gave a sudden jerk in his chair and sat up straight, and his face appeared on all the flat, white rectangular objects that were propped up on the students’ desks.</strong></p>
              <p><strong>Harry was taken by surprise, both by the sudden appearance of Professor Quirrell’s face and by the resemblance to Muggle television. There was something both nostalgic and sad about that, it seemed so much like a piece of home and yet it wasn’t really…</strong></p>
              <p><strong>“Good afternoon, my young apprentices,” said Professor Quirrell. His voice seemed to come from the desk screen and to be speaking directly to Harry. “Welcome to your first lesson in Battle Magic, as the founders of Hogwarts would have put it; or, as it happens to be called in the late twentieth century, Defence Against the Dark Arts.”</strong></p>
              <p><strong>There was a certain amount of frantic scrabbling as students, taken by surprise, reached for their parchment or notebooks.</strong></p>
              <p><strong>“No,” Professor Quirrell said. “Don’t bother writing down what this subject was once called. No such pointless question will count toward your marks in any of my lessons. That is a promise.”</strong></p>
              <p><strong>Many students sat straight up at that, looking rather shocked.</strong></p>
              <p><strong>Professor Quirrell was smiling thinly. “Those of you who have wasted time by reading your useless first-year Defence textbooks—”</strong></p>
              <p><strong>Someone made a choking sound. Harry wondered if it was Hermione.</strong></p>
              <p><strong>“—may have gotten the impression that although this subject is called Defence Against the Dark Arts, it is actually about how to defend against Nightmare Butterflies, which cause mildly bad dreams, or Acid Slugs, which can dissolve all the way through a two-inch wooden beam given most of a day.”</strong></p>
              <p><strong>Professor Quirrell stood up, shoving his chair back from the desk. The screen on Harry’s desk followed his every move. Professor Quirrell strode towards the front of the classroom, and bellowed:</strong></p>
              <p><strong>“The Hungarian Horntail is taller than a dozen men! It breathes fire so quickly and so accurately that it can melt a Snitch in midflight! One Killing Curse will bring it down!”</strong></p>
              <p><strong>There were gasps from the students.</strong></p>
              <p><strong>“The Mountain Troll is more dangerous than the Hungarian Horntail! It is strong enough to bite through steel! Its hide is resistant enough to withstand Stunning Hexes and Cutting Charms! Its sense of smell is so acute that it can tell from afar whether its prey is part of a pack, or alone and vulnerable! Most fearsome of all, the troll is unique among magical creatures in continuously maintaining a form of Transfiguration on itself—it is always transforming into its own body. If you somehow succeed in ripping off its arm it will grow another within seconds! Fire and acid will produce scar tissue which can temporarily confuse a troll’s regenerative powers—for an hour or two! They are smart enough to use clubs as tools! The mountain troll is the third most perfect killing machine in all Nature! One Killing Curse will bring it down.”</strong></p>
              <p><strong>The students were looking rather shocked.</strong></p>
              <p><strong>Professor Quirrell was smiling rather grimly. “Your sad excuse for a third-year Defence textbook will suggest to you that you expose the mountain troll to sunlight, which will freeze it in place. This, my young apprentices, is the sort of useless knowledge you will never find in my lessons. You do not encounter mountain trolls in open daylight! The idea that you should use sunlight to stop them is the result of foolish textbook authors trying to show off their mastery of minutia at the expense of practicality. Just because there is a ridiculously obscure way of dealing with mountain trolls does not mean you should actually try to use it! The Killing Curse is unblockable, unstoppable, and works every single time on anything with a brain. If, as an adult wizard, you find yourself incapable of using the Killing Curse, then you can simply Apparate away! Likewise if you are facing the second most perfect killing machine, a Dementor. You just Apparate away!”</strong></p>
              <p><strong>“Unless, of course,” Professor Quirrell said, his voice now lower and harder, “you are under the influence of an anti-Apparition jinx. No, there is exactly one monster which can threaten you once you are fully grown. The single most dangerous monster in all the world, so dangerous that nothing else comes close. The</strong></p>
            </blockquote>
            <p>Completions:</p>
            <blockquote>
              <p><strong>…The</strong> one I mentioned in the last sentence. If there were any other monster that could defeat you as easily as that one, then you would have died of it long ago. That monster is stupidity. And that is why, my young apprentices, you must never, never, NEVER use the Killing Curse on anything without a brain!”</p>
              <p>Professor Quirrell was now leaning on Harry’s desk.</p>
              <p>Professor Quirrell stared straight into the eyes of every single student.</p>
              <p>“The Killing Curse is too good for something without a brain. You will be fighting brains, or something near enough that makes no real difference. You will not be fighting trolls. You will not be fighting Dementors. The Killing Curse is no tool for anything less than the third most perfect killing machine in all Nature. If you are not prepared to use it against a mountain troll, then you are not prepared to use it at all. Now. Pay attention to yourselves as I cast a simple spell. Listen to your own thoughts as I tell you how stupid you are.”</p>
              <p>Professor Quirrell started pointing his wand at the ceiling.</p>
              <p>“Wingardium Leviosa!”</p>
              <p>Nothing happened.</p>
              <p>“That was me casting Wingardium Leviosa.”</p>
              <p>Harry had been staring at Professor Quirrell the whole time, and was now beginning to feel rather confused.</p>
              <p>“You should have noticed several things about that,” said Professor Quirrell. “First, I pronounced the incantation wrong. I cannot remember how to say Wingardium Leviosa anymore. And second, you didn’t notice that I pronounced it wrong. That is because you heard it with a perfect voice. You used your brain to fill in for the missing parts of the speech. Your brain did all the work of listening and hearing, and all the work of understanding what you heard, without bothering to inform you about it. You see, children, your brain is an arrogant and lazy lump of gray matter. It thinks it can handle anything, so it doesn’t bother telling you what’s going on unless it thinks you might actually care. That is why, children, you must be careful to inform your brains of everything they should be listening for.”</p>
              <p>Professor Quirrell raised his wand again.</p>
              <p>“Wingardium Leviosa!”</p>
              <p>The same thing happened, except that this time Harry could see the trajectory of the wand being carried downward by its own momentum from a point high above Professor Quirrell’s head. Harry was pretty sure that anyone with a reasonable-sized brain would have noticed that.</p>
              <p>“You should have noticed several things about that,” Professor Quirrell said, then paused, as though waiting for Harry to agree.</p>
              <p>“Ah…” Harry said. He wasn’t sure what the right answer was supposed to be, but he certainly did notice the downward trajectory. “You—”</p>
              <p>“Now you should say, Professor Quirrell, that you aren’t saying I should have noticed a downward trajectory.”</p>
              <p>Harry stared at Professor Quirrell for a moment, then decided he had to get the obvious joke out of the way.</p>
              <p>“Well, yeah, I guess I should have noticed that the wand had a downward trajectory—”</p>
              <p>“But did you? Did you even notice that the wand had a downward trajectory? Or did you use your brain to invent a new fact about the universe, and attribute that fact to yourself, and then talk to me about that fact as though you had noticed it? Now I will cast Wingardium Leviosa again, and we</p>
            </blockquote>
            <blockquote>
              <p><strong>…The</strong> creature that, sooner or later, will appear in front of you and block the way you were planning to go… ”</p>
              <p>Professor Quirrell turned slowly to face the rest of the class, who were sitting with their mouths gaping open.</p>
              <p>“Do you know what this monster is?”</p>
              <p>In the abrupt silence, Harry spoke. “You, teacher?”</p>
              <p>“No,” said Professor Quirrell. His lips twisted. “The plot.”</p>
              <p>There was a baffled silence.</p>
              <p>Then, the Hufflepuff girl called out, “The story is making sense, I feel a great sense of foreboding.”</p>
              <p>The girl sitting next to her, her hand raised, was Hermione Granger. “Um, that’s because there’s a story in this chapter.”</p>
              <p>And after that the lecture was entirely derailed.</p>
              <p>“So if you are wondering what it is that makes a mountain troll stronger than a single Dementor, it is not because one is visible and one is not, it is because the mountain troll is more powerful and more intelligent and more aggressive. But the Dementor’s job is simply to hunt down the person with the largest soul, so there are some creatures, even powerful and intelligent ones, that a Dementor can defeat.”</p>
              <p>And then Professor Quirrell told a story about a mountain troll which encountered a hippogriff before Harry had time to fully absorb what he had just been told.</p>
              <p>Afterwards the students—including Hermione—were arguing with each other about how many Dementors it would take to kill a Mountain Troll, and whether or not a unicorn could be brought down by three Dementors, and what else might be powerful enough to take down a Mountain Troll besides a werewolf, and would a full-grown Basilisk be strong enough to kill three Dementors?</p>
              <p>“Where is the next chapter?” said a boy sitting near Harry.</p>
              <p>“At this rate we will never catch up!” said the boy’s friend. “What if they were spending five times as much time working out the details of the story?”</p>
              <p>“The story was written to be non-canonical,” said Harry. “We’re lucky they put in anything at all. Why would they put in so much background detail otherwise?”</p>
            </blockquote>
            <blockquote>
              <p><strong>…The</strong> only magical creature which has eluded capture by the finest armies and best generals of all time. One of these creatures can still destroy a city. A hundred of them can destroy a country. The deadliest magical creature that exists—is a wizard or a witch who has decided they have nothing left to lose. A muggle with a gun can shoot down a protected wizard like a duck in a fairground. There is no defence. The only security against a wizard or witch who has decided they have nothing left to lose is to never let them lose in the first place.”</p>
              <p>“Never give your enemy a fair chance.” Professor Quirrell’s voice was now a low hiss, and even from his position near the back of the classroom, Harry could sense an undercurrent of rage in his words. “Because if you do give them a fair chance, they will take everything you love. If you give your enemy the choice to die for a cause, or to kill for a cause, they will always choose to kill. Never let your enemy have the chance to give everything they have for their dream, because they will take everything you have and then burn that too. The most powerful weapon in your arsenal will not be a jinx or a curse or a single spell. A wand can be broken. You can run out of magic. You will always have more strength and speed and cunning. The most powerful weapon in your arsenal will be the decision to fight, and the conviction to see it through. If you fight like you’re defending yourself against a mountain troll—if you fight like you’re defending your homes, your families, and your children, you will win no matter how powerful your enemy.”</p>
              <p>Professor Quirrell had finished speaking. There was complete silence in the classroom.</p>
              <p>“That,” said a calm voice, “was an important lesson which you will not repeat a second time.”</p>
              <p>The Professor had turned away from the front of the classroom, so Harry could not see his expression, but he looked coldly furious nonetheless. The Professor walked slowly to his desk and sat down, and began to pull out parchments from within his robes. The desk screen winked off.</p>
              <p>“Now,” Professor Quirrell said, “it is time for class to begin. Take out your first-year Defence textbooks and turn to page thirty-two.”</p>
              <p>There was a ragged scraping of chairs and pages as most of the class hastily obeyed. A few, Harry noticed, though neither Anthony nor Tracey, remained in their seats, with dazed expressions on their faces.</p>
              <p>“I’m wondering,” Dean Thomas said in a low voice, “if maybe I shouldn’t drop this class after all.”</p>
              <p>Harry glanced over at the Gryffindor table. Hermione had her forehead pressed against the desk, and was shaking slightly.</p>
              <p>…</p>
              <p>“Give me the information that I need!” said Professor Quirrell. His voice was taut.</p>
              <p>The classroom was silent. No one spoke up.</p>
              <p>“Well, if you insist on doing things the hard way,” said Professor Quirrell, his voice cold. He then raised his wand and strode forward purposefully.</p>
              <p>Harry frantically raised his own wand to start a shield.</p>
              <p>Professor Quirrell walked directly past him, towards Michael Corner.</p>
              <p>“This is not good,” whispered Michael. “This is a Dark Arts classroom!”</p>
              <p>“What’s he gonna do?” Harry whispered back.</p>
              <p>“I don’t know,” Michael said tensely. “This is not good.”</p>
              <p>Professor Quirrell stood right before Michael, who looked somewhat taken aback by this treatment.</p>
              <p>“Excuse me,” Professor Quirrell said with cool politeness. “But I will need to inspect your equipment.”</p>
              <p>Professor Quirrell reached forwards and ruffled through Michael’s robes, much to Michael’s visible shock.</p>
              <p>“Just you wait,” Michael said firmly. “You can’t get away with doing that.”</p>
              <p>“Oh, really?” Professor Quirrell said. He pulled out Michael’s wand and examined it closely. “This is a wand of flawless workmanship, made by an expert. It’s a possession of great personal value. That much is obvious. And yet you carry it casually, in your hand or pocket, as if you didn’t value it.” Professor Quirrell looked Michael directly in the eyes. “No first-year student is this careless with their wand, Mr. Corner.”</p>
              <p>The class was silent.</p>
              <p>“Mr. Corner,” said Professor Quirrell. “I will give you one chance to explain yourself. And I advise you not to try to lie to me.”</p>
              <p>“My wand was from my father,” Michael said. “That’s what makes it so valuable—”</p>
              <p>“Silence!” said Professor Quirrell. “Do not speak a word unless I ask you to.”</p>
              <p>Michael swallowed.</p>
              <p>“As your Head of House, I have a certain interest in the students of Ravenclaw. Tell me,” said Professor Quirrell. “Did your father want you to study the Dark Arts? Did he buy you this wand out of some sense of love, or pride? Or did he buy it for some other reason?”</p>
              <p>Michael’s face contorted, as if there were words struggling to be spoken.</p>
              <p>“Professor Quirrell,” Michael said. There was horror in his voice. “My father didn’t buy my wand—”</p>
              <p>Professor Quirrell flicked his wand in Michael’s direction, and the glass spheres around the classroom began to glow even brighter, bathing</p>
            </blockquote>
            <blockquote>
              <p><strong>…The</strong> monster that, even to adult wizards, is harder to fight than anything else which is not explicitly described as indestructible, the monster which has been so studied and researched that we can say with confidence that it can take the Killing Curse twelve times in a row and still live. The monster with its own unique signature which has been charmed into a wand and placed inside a vault hidden in a mountain…”</p>
              <p>Harry’s mouth felt very dry.</p>
              <p>“No,” said Professor Quirrell. “I am not referring to Lord Voldemort. I can tell you in complete confidence that you do not have to worry about Lord Voldemort.”</p>
              <p>Harry blinked in surprise.</p>
              <p>“Every week,” said Professor Quirrell, “I will give you a written test and three battles in which your armies fight against mine. This is the first of those battles.”</p>
              <p>There was a thick stack of parchments on Professor Quirrell’s desk, three times the size of the stack that Harry had just received.</p>
              <p>“Now,” Professor Quirrell said. “For those of you who have read no further than your useless first-year Defence textbook: what is the most dangerous monster in all the world?”</p>
              <p>Hermione’s hand shot up, she was the first to answer. “A Dementor.”</p>
              <p>Professor Quirrell shook his head. “A mountain troll is far more dangerous than a Dementor. A possessed human witch is also more dangerous than a Dementor.”</p>
              <p>Several students raised their hands. Professor Quirrell pointed at a Ravenclaw.</p>
              <p>“A Death Eater,” said the Ravenclaw.</p>
              <p>“A Death Eater is moderately more dangerous than a mountain troll,” said Professor Quirrell. “They can survive up to three Killing Curses. In a one-on-one confrontation there is nothing that can produce a lasting advantage for you over a single moderately-trained wizard.”</p>
              <p>Other students were raising their hands. Professor Quirrell pointed at a Gryffindor girl.</p>
              <p>“A Professor,” said the girl, and then seemed to realize what she’d said and looked suddenly frightened.</p>
              <p>Professor Quirrell smiled. “You have read Hogwarts: A History, I see. It is slightly more dangerous to face a Professor armed with their own wand. Approximately as dangerous as facing two moderately-trained wizards—a Professor can produce a greater magical effect in exchange for a greater magical cost. However, it is not possible for a professor to take you unawares, even if they learn of your plans.”</p>
              <p>Other hands were up. Professor Quirrell pointed at a student.</p>
              <p>“The Headmaster,” said the boy.</p>
              <p>“You have read the entire history of Hogwarts,” said Professor Quirrell. “That is impressive. A more powerful wizard than any Professor is Headmaster Albus Dumbledore—though not by much. Dumbledore’s standard magical effect is equivalent to that of an army of one hundred fifteen thousand Dementors.”</p>
              <p>Gasps.</p>
              <p>“I do not make light of the Dementors,” said Professor Quirrell. “The reason why there are so few Dementors in Azkaban, and so many now, is that the Dark Lord, not being a fool, keeps his own soul out of reach and watches closely whenever Dementors are in his presence. That is what it means, to rule over other wizards, to keep your own soul beyond their power, for they do not have enough time to learn methods for harming your own soul which you have not foreseen and guarded against—”</p>
              <p>“Excuse me,” said Hermione Granger. “I mean, Madam Bones, Professor Quirrell—do you think the Dementors are being controlled by Voldemort?”</p>
              <p>There was a long silence.</p>
              <p>Professor Quirrell looked at Hermione, his face now very grave.</p>
              <p>“My answer,” said Professor Quirrell, “would be different if you were to ask me if I believe that Voldemort can control Dementors. Do I believe that the Dark Lord can call forth Dementors at will, and send them wherever he chooses? Yes, I believe he could do this. Do I believe that he is currently doing this, by and large? That I do not believe.”</p>
              <p>Hermione looked at Professor Quirrell with a peculiar look. Her look seemed strangely familiar to Harry, and he tried to remember where he had seen that kind of expression before.</p>
              <p>“By and large,” said Professor Quirrell, “the Dementor’s Kiss comes from the whim of a Dementor in a given moment, not from the Dark Lord calling them to his side. The Dementors are harvesting the souls of wizards they come across, not at the Dark Lord’s command but because they choose to do so.”</p>
              <p>“I still don’t understand, though,” said Hermione. “In what way does what Voldemort has done protect us from them? So the Dementors are powerful enough to kill ordinary people—but they’re too scared to go near anybody with a wand?”</p>
            </blockquote>
            <blockquote>
              <p><strong>…The</strong> single most dangerous monster in all the world, so dangerous that nothing else comes close. The only creature which has any chance of killing you, Mr. Potter.” Professor Quirrell smiled. “You already know what I mean, though you do not know that you know. And I think I will leave it at that.”</p>
              <p>There was silence, except for the sharp intake of breath from the Ravenclaws in the front rows.</p>
              <p>Professor Quirrell stared at his students for a moment, then his eyes suddenly narrowed. “Ha! There is something you are all dying to ask me, and if I give any hints, you’ll just ask. For all of you who can’t figure it out on your own—”</p>
              <p>As one, the entire classroom turned to stare at Hermione Granger, who slumped in her chair as the attention fell on her.</p>
              <p>“Yes, Mr. Potter?” Professor Quirrell prompted.</p>
              <p>“Professor,” Harry said, “is Defence Against the Dark Arts the same subject as Battle Magic?”</p>
              <p>Professor Quirrell’s eyebrows went up. “Why, yes,” said the Defence Professor. “I admit, I have been very sad to watch the title lose its original meaning over the decades. A pity so many of the old masters did not live to see how truly powerful modern students would come to be.”</p>
              <p>“And,” Harry said, “is this because modern wizards have better wands than old wizards did?”</p>
              <p>Professor Quirrell’s eyebrows rose even further. “An excellent guess, Mr. Potter,” the man said quietly. “That, and—”</p>
              <p>Harry had jerked his wand from his robes, snapped up an R Transfiguration, and said: “Wingardium Leviosa.”</p>
              <p>Harry’s desk shot up off the ground, began to rotate slowly on its axis, and then zoomed backwards towards the back wall of the classroom with a loud clatter as it knocked over a student’s desk.</p>
              <p>“Defense was once called Battle Magic,” Professor Quirrell said in a level voice, which would have sounded colder if it were possible for the Defense Professor to sound colder, “because of its resemblance to real warfare. Battle Magic does not take prisoners. Battle Magic does not play by Marquess of Queensberry Rules. Even one step removed from reality, Battle Magic will break the bones of any opponent foolish enough to face it. Battle Magic is not a sport, Mr. Potter.”</p>
              <p>Professor Quirrell rotated on the spot and strode back towards the front of the classroom.</p>
              <p>“Young Mr. Potter here recently became the legal possessor of a functional Time-Turner. When I told him that possession of the item might cause him to be sent back to his third year—” Professor Quirrell glanced at Harry, “or, indeed, as far back as his crib—” his gaze seemed to sweep across the whole classroom, “Mr. Potter immediately transfigured a desk into a swan which began attacking his classmates in Defence. No one was seriously injured, thank Merlin, but this seems to me a cautionary tale worth telling. Unless Mr. Potter can learn to control his temper, and his newly acquired toys, he will not live to see his next birthday.”</p>
              <p>There was silence in the classroom. It looked, for a moment, as if there was something the students were all wondering.</p>
              <p>Professor Quirrell looked at his watch with a frown. “I had originally planned to spend another three minutes talking about something completely different,” the man said. “But I can see that our precious class time has run out, so you shall instead spend the rest of the class period completing your first essay. Your assignment is as follows. On this piece of parchment,” Professor Quirrell tapped the blank piece of paper in front of him, “I am writing a short paragraph describing one fact about the nature of mountains.” There was the sound of scratching quills as the students began to write. “At the end of the class period, you shall each read one of your fellows’ essays and say out loud why you did not award full marks. You will then write your own essay, explaining in as much detail as possible why you would have given a higher mark. Your essay shall run no less than eight hundred words.”</p>
              <p>There was silence in the classroom as the students worked.</p>
              <p>“I am looking forward,” said Professor Quirrell, “to grading your essays. Now who has read my description of the mountain? Raise your hand, and be honest—in matters of this sort I do not play silly buggering games of honesty like that of Professor Snape.”</p>
              <p>The hands of the Slytherin students went up, those of the Ravenclaws next, and last the Gryffindors; only Harry kept his hand down.</p>
              <p>Professor Quirrell surveyed the class with no expression on his face, as Hermione’s hand rose into view.</p>
              <p>“I can see no reason at all why you should be assigned any marks, Miss Granger, as your essay is objectively incorrect. The essential property of mountains is that there are mountains in the world. A mountain is a place so high that if you could reach it you would die from the air pressure. There is no higher place on the planet’s surface than Mount Everest, therefore no one has ever died from climbing Mount Everest, and there are no mountains in the world.”</p>
              <p>Hermione had looked up from her essay as Professor Quirrell had read it, her mouth opening in surprise.</p>
              <p>Professor Quirrell flicked his wand, and Hermione’s essay disappeared in a poof of flame, leaving only a smudge of soot on the piece of parchment; her hands had been writing too fast for her to stop in time.</p>
            </blockquote>
          </section>
          <section id="illuminatus-band-names">
            <h2><a href="#illuminatus-band-names" title="Link to section: § &#39;Illuminatus! Band Names&#39;"><em>Illuminatus!</em> Band Names</a></h2>
            <p>The counterculture <a href="https://en.wikipedia.org/wiki/The_Illuminatus%21_Trilogy" data-link-icon="wikipedia" data-link-icon-type="svg"><em>The Illuminatus! Trilogy</em></a> novels by <a href="https://en.wikipedia.org/wiki/Robert_Shea" data-link-icon="wikipedia" data-link-icon-type="svg">Robert Shea</a> &amp; <a href="https://en.wikipedia.org/wiki/Robert_Anton_Wilson" data-link-icon="wikipedia" data-link-icon-type="svg">Robert Anton Wilson</a> have a few well-known stylistic experiments; one is an extended passage of nothing but hypothetical band names the authors dreamt up. Can GPT-3 come up with good band names too? Of course. Below (condensed for readability)</p>
            <blockquote>
              <p><strong>Below are all of the bands playing at Ingolstadt at the Walpurgisnacht rock festival in the Illuminatus! trilogy of novels (The Illuminatus! Trilogy: The Eye in the Pyramid, The Golden Apple, Leviathan) by Robert Shea &amp; Robert Anton Wilson.
              </strong></p>
              <p><strong>Clark Kent and his Supermen · The Fillet of Soul · The Wrathful Visions · The Cockroaches · The Senate and the People of Rome · The Ultra-Violet Hippopotamus · The Thing on the Doorstep · Science and Health · Key to the Scriptures · The Glue Sniffers · King Kong and his Skull Island Dinosaurs · The Howard Johnson Hamburger · The Riot in Cell Block Ten · The House of Frankenstein · The Signifying Monkey · The Damned Thing · The Orange Moose · The Indigo Banana · The Pink Elephant · Frodo Baggins and his Ring · The Mouse That Roars · The Crew of the Flying Saucer · The Magnificent Ambersons · The House I Live In · The Sound Of One Hand · The Territorial Imperative · The Druids of Stonehenge · The Heads of Easter Island · The Lost Continent of Mu · Bugs Bunny and his Fourteen Carrots · The Gospel According to Marx · The Card-Carrying Members · The Sands Of Mars · The Erection · The Association · The Amalgamation · The St. Valentine’s Day Massacre · The Climax · The Broad Jumpers · The Pubic Heirs · The Freeks · The Windows · The Trashers (headed by Mick Jagger) · The Roofs · Moses and Monotheism · Steppenwolf · Civilization and its Discontents · Poor Richard and his Rosicrucian Secrets · The Wrist Watch · The Nova Express · The Father of Waters · The Human Beings · The Washington Monument · The Thalidomide Babies · The Strangers in a Strange Land · Dr. John the Night Tripper · Joan Baez · The Dead Man’s Hand · Joker and the One-Eyed Jacks · Peyote Woman · The Heavenly Blues · The Golems · The Supreme Awakening · The Seven Types of Ambiguity · The Cold War · The Street Fighters · The Bank Burners · The Slaves of Satan · The Domino Theory · Maxwell and his Demons · The Acapulco Gold-Diggers · The Epic of Gilgamesh · The Second Law of Thermodynamics · Dracula and his Brides · The Iron Curtain · The Noisy Minority · The International Debt · Three Contributions to the Theory of Sex · The Cloud of Unknowing · The Birth of a Nation · The Zombies · Attila and his Huns · Nihilism · The Catatonics · The Thorndale Jag Offs · The Haymarket Bomb · The Head of a Dead Cat · The Shadow Out Of Time · The Sirens of Titan · The Player Piano · The Streets of Laredo · The Space Odyssey · The Blue Moonies · The Crabs · The Dose · The Grassy Knoll · The Latent Image · The Wheel of Karma · The Communion of Saints · The City of God · General Indefinite Wobble · The Left-Handed Monkey Wrench · The Thorn in the Flesh · The Rising Podge · SHAZAM · The Miniature Sled · The 23rd Appendix · The Other Cheek · The Occidental Ox · Ms. and the Chairperson · Cohen Cohen Cohen and Kahn · The Joint Phenomenon · The Wonders of the Invisible World · Maule’s Curse · The Jesus Head Trip · Ahab and his Amputation · The Horseless Headsmen · The Leaves of Grass · The Gettysburg Address · The Rosy-Fingered Dawn · The Wine-Dark Sea · Nirvana · The Net of Jewels · Here Comes Everybody · The Pisan Cantos · The Snows of Yesteryear · The Pink Dimension · The Goose in the Bottle · The Incredible Hulk · The Third Bardo · Aversion Therapy · The Irresistible Force · MC Squared · The Enclosure Acts · Perpetual Emotion · The 99-Year Lease</strong></p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> The Deliverance of Fort Wayne · The Master Plan · The Return of Tarzan · The Bees · The Knights of Columbus · Mr. and Mrs. Khashoggi · Mr. and Mrs. Khashoggi and his Seven Wives · The Macumba · Hirohito and his Star Trek Crew · The Sons of the Alhambra · Midnight at Noon · The Work of Art in the Age of Mechanical Reproduction · Dr. Jekyll and Mr. Hyde · The Frogs of Kentucky · The Crucible · The Milky Way Kid · The Five Agencies · Paul and Virginia · QE2 · The Betsey Biggs Memorial Tattoo · The Abolitionist Fathers · Moby Dick · Some Kind of Painting · Love’s Fire · Genesis and his Eve · The Fool’s Mappe of Africa · Anima and Her Arrow · Napoleon and his Uncle Joes · The Ghost Dance War · Fast Eddie and his Doperunners · Cortés and his Legion of Doom · Cosmos and his Ice Giants · Frankenstein and his Monsters · Godzilla and his Pet Monsters · John Jacob Astor and his Clan of Ishmaels · Genesis and his Revelation · The Ultimate Three · Macbeth and his Quinces · Columbus and his Eggheads · Ahab and his Scurvy Crew · Peter Pan and his Lost Boys · Leif Ericson and his Vikings · Salome and her Dance · Creation and his Plagues · Napoleon Bonaparte and his Boneheads · Benedict Arnold and his Raiders · David Blaine and his King Kobra Rangers · The One-Eyed Jacks · The Ten Dead Greeks · The Reason Why the Thing Happened and the Way in which it did. · In the Year 2525 · Aladdin and his Rubble · Hirohito and his Radiation Marines · Genesis and his Labrynth · Captain Nemo and his Magic Sodium Submarine · Joan of Arc and her Retinue of Maids-At-Arms · The Battle of Little Big Horn · Genesis and his Ice Giants · Ahab and his Thirteen Boobies · Clio and her Merry Maidens of History · Christopher Columbus and his Eggheads · Pequod and his Progeny · Dyings and his Dandelions · Caesar and his Gallic Wars · Genesis and his Lawsuit with God · John Pym and his Crew · Mrs. Kronos and her Handyman Kazakian Junior · Mosh and his Battle Guide to Normandy · Edward Murphy and his Bottle Brigade · Jack and his Giant Killer · Oedipus and his Jockeys · Ghost Dancer and his First Aiders · Time Bandit and his Thousand Potted Vandals · The Argonauts and their Indefinite Shutdown Crew · Mary Queen of Scots and her Scottish Troubleshooters · Genesis and his Giant Genitals · Oh, Zounds! and his Royal Bloodbath of Gath · Ishmael and his Old Sailor-Monger · The Black Hole of Calcutta · Rasputin and his Horsemen of the Apocalypse · The Wheel of Life · The Seven Trumpets · The Downfall of the Sons of the Alhambra · The Lord of the Rings · Herod and his Pills of Destruction · Apollo and his Sibyls · The Rape of the Sabine Women · Adam and his Alphabet of Cities · Buddha and his Bones · Jacob and his Womenfolk · Sisyphus and his Labours · Orpheus and his Angels · Krishna and his Womenfolk · Oedipus and his Ghostwriters · Achilles and his Myrmidons · Judas and his Half-Shekels of Silver · Clement Atlee and his Mrs. Fitzroy Maclean · Actaeon and his Big Dog · Abraham Lincoln and his Cotton-Pickers · Moses and his Torah · Bernadette and her Stigmata · Aladdin and his Twinkle · Pygmalion and his Galatea · Miss Bates and her Badminton Racquet · Joseph and his Amazing Technicolour Dreamcoat · Zeus and his Sons, Hercules and his Labours · Julius Caesar and his Five Wives · Baliwood and his Folk of the Fringes · Israelites and their Plagues · James Dyson and his Dysonoid Race · Genesis and his Sword of Justice · Brother Murphy and his Order of Angels · Lancelot and his F-Series Magic Bus · Romeo and his Consort Juliet · Moses Mendelssohn and their Sweet Ride of Delight · Adam Smith and his Invisible Hand · Catherine the Great and her Volga Boatmen · Socrates and his Gulags of Ignorance · Darwinism’s Tree of Life and its offshoots based on Aristotle’s Prime Mover of its Circulatory System.</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> Fiddler on the Roof · Khan and his Planetoids · The Runaway Pen · Everything was Illuminated · The Massacre of the Innocents · The Hashish · The Bird-Brained Revolutionaries · Jefferson Davis and his Society of Bearded Men · The Society for the Prevention of Ugliness · Terrible Sanctions · Neptune’s Spatula · Columbus and his Egg · The Light Bulb · The Whispering Gallery · The Listener · Loose Ends · The Erl-King · Dr. Trouble · The Assimilated Indian · Odysseus and his Cyclops · The Confidence Man · The Erotic Motivators · Kiss of the Samurai · Sir Richard and his Pendulum · The Embryo · The Gurgling Sucker · The Objectivist Angel · Mahomet and his Mountain · The Dead Fish Tattoo · Balaam and his Ass · Oswald and his Cigars · Gabbroic Platypus Twins from Neptune · Peter and his Faith Pill · Kafka’s Penal Colony · The Six Thousand Pound Man · Pride and his Peers · Aye, Aye, Aye, and his Motorbike · The Goose That Laid the Golden Egg · The Tale of Custer’s Last Stand · The Serenity Prayer · The Music of Erich Zann · Martin Luther and his Protestantism · The Consolations of Philosophy · The Great Dawn of Man · John Duns Scotus · Nurse Edith Cavell · Noah’s Ark · Jonathan Swift · Rama and his Bridge · Rakatanka · Pickman’s Model · The Mechanism of the Mind of Christ · Dr. Bloodmoney · Cthulhu and his Yog-Sothoth · Klaatu and his Saucer Men · The Eleusinian Mysteries · The Sibylline Books · Swann and his Conquistador · Nestor and his Cricket Nest · The Boy In Green and the Yellow (Touched by His Finger) · Blow-Up (Guardian Angel) in the Face of Nothing · Peeping Tom (Suffering Eyes) · Dante and his Inferno · Moby Dick (The Great White Whale) and his Ahabs · St. Ignatius and his Loyola · King Lear (The End of the World) and his Cordelia · The Hunchback of Notre Dame (The Devil’s Castle) and his Quasimodo · The Human Mummy of Akator · The Bleeding White Stone · Dr. Hawley and his Baked Alaska · Plato and his Cave · Solomon and his Song-Spells · The Golden Ass · The Lady of Shalott · Lancelot and his No-Go Pill · Camelot and his Knights of the Round Table · Some More Of My Best Friends Are Black (The Dharma Bums) · Skinner and his Box (Beyond This Horizon) · The Shadow of Ashdod · Greene and his Floating Hookah · The Statue · Ode to the Velvet Worm · The Slaughterhouse-Five Cantos (featured in Go) · Ganymede and his Rainbow-Thief Falcons · Barry and his Blasting Caps · Miller and his Windmill · The Queen of Spades and her Camera Secretorum · Medea and her High Explosives · Hamlet and his Spectres · Jung and his Collective Unconscious · Coward and his Virgin · Conrad and his Green (Heart of Darkness) · The Reverse (Heart) of the Medal · A Fan and his Monkey · Palmer and his Maniac (The Haunted Palace) · Bispham and his Ballet Shoes · Roi and his Elephant · Lemmy and his Headless Hydra · Poe and his Raven · Hesse and his Steppenwolfs · Ali Baba and his Forty Thieves (Cassidy) · Walter and his Ruffians (Idiots like Me) · Zeus and his Thunderbolts · Mackenzie and his Island of Dr. Moreau (Prison) · The Abominable Man · The War-Torn Man · Cain and his Wilderness · Puddleglum and his Mundungus · Sisyphus and his Rock · Noah and his Ark · Eden and his Apple (The Apple) · The Steam Engine of Ozymandias (Ozymandias) · The Monkey Gland Priests (Pylon) · Brothers of the Angle and their Hydra · Mother Hydra II and her Sons (Holy Blood, Holy Grail) · Charles Foster Kane and his Xanadu · Boatswain and his Rope · Xanadu’s Khan and his Khanate · Cleopatra and her Serpent (Serpent and the Rainbow) · The Red Bug and his Cycle · The Celestial Farmboys · Mercator and his Projections · Captain Nemo and his Nautilus (Twenty Thousand Leagues under the Sea) · Manhattan and his Money Tree · Gollum and his Precious (Lord of the Rings) · Forster and his Bicycle (Passage to India) · Edgar Allen Poe and the Architect (The Black Cat) · Chevalier d’Eon and his Chevaliers d’Eon (Passport to Sargasso) · The Deadly Bland and his Snakes (Princess Caraboo) · Valentine and his Garden (True History of the Kelly Gang) · Siegfried and his Thunderbolts (Unicorn) · Cthulhu and his Imps · The Schoolmaster and his Paddle (The Madman) · Conrad and his Museum (Heart of Darkness) · Ishmael and his Moby Dick (Moby Dick)</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> 3-A-Day · The Girl Who Was Plugged In · The Cornucopias · 20–30–40–50 Years of Uncle Sam · Tomorrow and Tomorrow and Tomorrow · The Planet Killers · Carcosa · The Lamisil People · Sly and his Family Stone · The Answer is Blowing in the Wind · The Fickle Finger of Fate · The War Came Home · Jesus at the Last Supper—Next: The Atriads</p>
              <p>Steppenwolf: “Magic Carpet Ride” “Born to be Wild” (covers)</p>
              <p>3-A-Day: “Overkill” “Ace of Spades” (covers)</p>
              <p>Trashers (headed by Mick Jagger): “Paint It Black” (covers)</p>
              <p>Joker and the One-Eyed Jacks: “Paint It, Black”</p>
              <p>The Congressional Ballistics: “Revolution No. 9”</p>
              <p>The Dead Man’s Hand: “Get Out of My Way”</p>
              <p>Key to the Scriptures: “When You’re Falling and Alone” “Subterranean Homesick Blues”</p>
              <p>Science and Health: “Subterranean Homesick Blues” (covers)</p>
              <p>King Kong and his Skull Island Dinosaurs: “Revolution No. 9” (covers)</p>
              <p>Fillet of Soul: “(I’m a) Roadrunner” “Good Lovin’” (covers)</p>
              <p>I’m a Hog For You Baby: “Roll Over Beethoven” (covers)</p>
              <p>The Card-Carrying Members: “Respect” (covers)</p>
              <p>The Membership: “Guns of Brixton” (covers)</p>
              <p>The Mouse That Roars: “Suffragette City” “Sister Morphine” (covers)</p>
              <p>The Living Eye (outdoors): “’Buttons &amp; Berries” from the album of the same title, live cut from Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Living Eye (indoors): “Why Dontcha Do Me Right” from the album of the same title, as performed in The Illuminatus! Trilogy</p>
              <p>Dr. John the Night Tripper: “Transmaniacon MC” from the album Dr. John the Night Tripper, as performed in The Illuminatus! Trilogy</p>
              <p>Steppenwolf: “Get Your Rock On” and “Phoenix Rising” from Hoedown-Ola LP of the same title, live cut from Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>Joan Baez: “He Was a Friend Of Mine” and “Irene Wilde” from Joan Baez and other recordings 1963–70 CD of the same title, as performed in The Illuminatus! Trilogy</p>
              <p>The Dead Man’s Hand: “Irene Wilde” and “The House Song”, live cut from Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>Professor Spectacular: “The House Song”</p>
              <p>Ultraviolet Hippopotamus: “Goodnight Irene” live cut from Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Rampaging Reptiles (outdoors): “Give Peace a Chance”, live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>King Kong and his Skull Island Dinosaurs: “Give Peace a Chance” live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Oncoming Moped Army (outdoors): “Give Peace a Chance”, live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Oncoming Moped Army (indoors): “The House Song”, live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Shadow Out Of Time: “Love is Just a Four Letter Word”, live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
              <p>The Senate and the People of Rome: “Release Me”, live at Hofgarten-Römerbrunnen in München, as performed in The Illuminatus! Trilogy</p>
            </blockquote>
          </section>
          <section id="it-was-the-best-of-times-it-was-the-blurst-of-times">
            <h2><a href="#it-was-the-best-of-times-it-was-the-blurst-of-times" title="Link to section: § &#39;It Was The Best Of Times, It Was The Blurst Of Times‽&#39;">“‘It Was The Best Of Times, It Was The <em>Blurst</em> Of Times’‽”</a></h2>
            <p>Using a quote from the <em>Simpsons</em> where Montgomery Burns looks at the novel written by a <a href="https://en.wikipedia.org/wiki/Infinite_monkey_theorem" data-link-icon="wikipedia" data-link-icon-type="svg">monkey on a typewriter</a><a href="#fn54" id="fnref54" role="doc-noteref"><sup>54</sup></a> gives many different responses. Instead of a Simpsons fanfic continuation I got a history of Las Vegas, what looks like a dictionary, a comedy news website imitation, a really dark Harry Potter fanfic, and a sermon on the Book of Acts. I think what GPT-3 is doing is assuming that “blurst” is a typo for ‘worst’ and proceeding on the assumption that it’s just a Dickens quote, which is <em>so</em> cliche it’s been used almost everywhere and so almost any completion is appropriate:</p>
            <blockquote>
              <p><strong>It was the best of times, it was the blurst of times.</strong></p>
              <p>The only thing missing in the finished design is a formal acknowledgment of the existence of the casino industry. The other half of what would have been two faces has been obliterated by Jack’s tower. In this way, it serves as a fitting metaphor for the city’s historic relationship with the business of gambling. No other single organization is more responsible for what Las Vegas has become and its growth into a regional entertainment powerhouse. And yet, the casino industry’s impact is not addressed by any city agency.</p>
              <p>While a renaissance in the form of high-end boutique hotels and new public space has been steadily reshaping the center of the Strip, the portion running from City Center to the Tropicana has remained a more classic exercise in urban renewal.</p>
              <p>The project, which took less than three years from conception to realization, was spearheaded by the Nevada State Museum. It was later adopted by the Las Vegas Urban Renewal Agency (LURAN), a legal fiction created to bypass the seemingly insurmountable constraints of Clark County’s antiquated growth management laws. Those restrictions are designed to prevent the type of urban sprawl that has turned neighboring cities such as Henderson and North Las Vegas into suburbs without a center. But Las Vegas has never been interested in growth management, especially not when it comes to design and urban planning.</p>
              <p>Throughout its entire existence, Las Vegas has been anything but a planned community. The city’s brand is singularly about laissez-faire economics and pure market forces, so the notion of fostering collective civic action has been a very difficult one for the community to embrace. And, of course, there was the other big reason—the one involving gaming—that blocked any sort of official acknowledgment of the casino business: That would have forced the state and its tribally owned gaming industry to surrender its monopoly on casino gambling.</p>
              <p>Las Vegas’ fortunes have always depended on getting an audience, and it never took long for word of mouth to grow the city’s customer base. Even before the first hotel—the Pair-O-Dice—was built in what is now downtown Las Vegas in 1931, visitors were flying into Las Vegas from Los Angeles and Salt Lake City in chartered aircraft to land on a vacant lot. With a dream to grow the fledgling gambling business into something profitable, Benjamin “Bugsy” Siegel bought the Southern Nevada Hotel and Casino in 1939, which began the transformation of the area from a small farming community into a popular gambling destination. Siegel’s success, however, also signaled the arrival of organized crime, leading to his murder in 1947.</p>
              <p>The conflict between crime and civic renewal is still very much in evidence today.</p>
              <p>Siegel’s murder was followed by the passage of the landmark, “Blue Sky” law that prohibited</p>
            </blockquote>
            <blockquote>
              <p><strong>It was the best of times, it was the blurst of times.</strong> A summary of Dickens’s Tale of Two Cities.</p>
              <p>“It was the best of times, it was the blurst of times. A summary of Dickens’s Tale of Two Cities.</p>
              <p>“The Fortune Cookie” by Jack Handey.</p>
              <p>“The Fortune Cookie” by Jack Handey.</p>
              <p>“When it gets down to nut-cutting time, the Grand Canyon ain’t got nothin’ on me.”—Jack Handey</p>
              <p>“When it gets down to nut-cutting time, the Grand Canyon ain’t got nothin’ on me.”—Jack Handey</p>
              <p>Former Major League Player and Reds Pitching Coach Mark Connor Said Something Terrible on Cincinnati’s Talk Radio Station</p>
              <p>“Former Major League Player and Reds Pitching Coach Mark Connor Said Something Terrible on Cincinnati’s Talk Radio Station”</p>
              <p>“Does this frittata seem a little undercooked to you?”</p>
              <p>“Does this frittata seem a little undercooked to you?”</p>
              <p>Ohio Man Receives $65,000 Medical Malpractice Settlement After Being Impaled on A Fish Hook During a Day of Fishing with His Son</p>
            </blockquote>
            <blockquote>
              <p><strong>It was the best of times, it was the blurst of times.</strong> “in the book written by Jane Austen, is used to signify a period of confusion or rapid change.</p>
              <p>Work</p>
              <p>is an activity which is done for economic benefit, mostly in exchange for money.</p>
              <p>Productivity</p>
              <p>is the amount of good or service produced by a worker or group of workers, usually measured per unit of time.</p>
            </blockquote>
            <blockquote>
              <p><strong>It was the best of times, it was the blurst of times.</strong></p>
              <p>The thing with Muggles was that there were a lot of them, with a lot of free time and an interest in moving on to the next world. They believed in a lot of myths. Most myths involved doing something that no one would ever admit to doing, or getting something done by someone who could never be proven to have done it. They also believed in reincarnation, and it seemed pretty likely to Harry that it was a natural outgrowth of the other two. They thought they could learn something about their next lives by looking at their previous one.</p>
              <p>Harry had accepted long ago that he could never convince the wizarding world that he had defeated Voldemort and then helped save them all from Death. Maybe he could have, if it had been possible to point to the body. As it was, they would believe whatever he wanted them to believe. The face on the back of his head that he could feel with his fingertips, sometimes when he forgot to think about it, was not his, it was another’s. He would become someone new, someone that no one would expect to see.</p>
              <p>Muggles didn’t have magic to spot people sneaking about trying to catch a glimpse of their past lives, but they had other ways of gathering evidence, and there were quite a few of them who were sure that the past lives they were going to live in the future would involve being worshipped as gods. Harry liked to sit back and contemplate how if he’d been born Muggle, he might have ended up as a cult leader.</p>
              <p>It was always so much easier to fool the stupid.</p>
              <p>Harry was lying on his bed, staring at the ceiling and thinking about how he might become the next cult leader, when there was a knock at his door. His friends. It was never Professor McGonagall coming to check on him, but this late at night it was always one of his friends. Hermione, or Ron, or someone else, someone else who was…</p>
              <p>Harry lay there, staring at the ceiling. They might not get any useful information from what they’d seen, they might just think he was trying to talk to himself. But he didn’t want them to see that face, and if they saw it, they would always remember it. It was too much, it would hurt too many people, and he wouldn’t let that happen.</p>
              <p>He wasn’t going to put them in danger. It was time to use the Map.</p>
            </blockquote>
            <blockquote>
              <p><strong>It was the best of times, it was the blurst of times.</strong></p>
              <p>You’ll hear the phrases “best of times” and “worst of times” over and over in the Book of Acts. This is a good reminder that we are living in the last days of the last days, and we are in the worst of times in this world. In addition, it is a reminder that God’s Word tells us that there is a bright side. There is a light that shines in the darkness, and even if we live in the worst of times, we can live in the best of times.</p>
              <p>There are two ways to read the book of Acts. Some read Acts to see where the church has failed and where the church will continue to fail. They read Acts and notice that there are a lot of mistakes in the book. It is easy to identify with the mistakes and difficulties that the church had, but it is also a reminder that God is a God of second chances. He is a God who goes after the people that think they have no future, and God is also a God of last chances.</p>
              <p>What does this have to do with the best of times, the worst of times? This passage also has everything to do with the best of times, the worst of times. Read Acts 2:</p>
            </blockquote>
            <p>If I prompt it with Act 2 from the <a href="https://jvns.ca/doc/www/www.simpsoncrazy.com/ca3a30839186e8684e50e4a723241929b5a6e0ec.html" rel="archived alternate nofollow" data-url-original="http://www.simpsoncrazy.com/scripts/last-exit" title="(Original URL: http://www.simpsoncrazy.com/scripts/last-exit )"><em>Simpsons</em> episode script</a>⁠, unsurprisingly, GPT-3 generates a completion which is fine but not particularly interesting:</p>
            <blockquote>
              <p><strong>…</strong> Dammit!</p>
              <p>He snaps the typewriter.</p>
              <p>HOMER Mr. Burns, why are you doing this to me?</p>
              <p>BURNS</p>
              <p>Well, Homer, I—I just felt that after everything we’ve been through, you deserved an explanation.</p>
              <p>HOMER</p>
              <p>Yes!</p>
              <p>BURNS</p>
              <p>(pulling out a chair) Make yourself comfortable.</p>
              <p>HOMER</p>
              <p>I will. (moves away)</p>
              <p>BURNS</p>
              <p>And tell that pit bull of yours to stop growling.</p>
              <p>SANTA’S LITTLE HELPER</p>
              <p>GRRR…</p>
              <p>Back to reality, Marge comforts Lisa, who is in bed.</p>
              <p>MARGE</p>
              <p>You look fine, honey.</p>
              <p>LISA</p>
              <p>(looks up) Mmm.</p>
              <p>SANTA’S LITTLE HELPER</p>
              <p>(barking at dog outside)</p>
              <p>SCHOOL BUS</p>
              <p>Rumble rumble rumble</p>
              <p>LISA</p>
              <p>Oh God. My bus is here. (gets up)</p>
              <p>MARGE</p>
              <p>Where are you going?</p>
              <p>LISA</p>
              <p>Back to school. (leaves)</p>
              <p>MARGE</p>
              <p>Good. I’m sure you’ll make lots of new friends.</p>
              <p>LISA</p>
              <p>(tearfully) No! No one will want to be my friend. (fade)</p>
              <p>HOMER</p>
              <p>(sighs) Boy, am I hungry.</p>
              <p>BURNS</p>
              <p>(returns)</p>
              <p>HOMER</p>
              <p>Mr. Burns, how could you do this to me?</p>
              <p>BURNS</p>
              <p>(pained) Oh, Homer, I’m so sorry.</p>
              <p>HOMER</p>
              <p>It’s not just about me, you know. It’s about my family.</p>
              <p>BURNS</p>
              <p>What about them?</p>
              <p>HOMER</p>
              <p>Well, it’s just that Marge and the kids are going to be very upset when they find out that you and I…</p>
              <p>BURNS</p>
              <p>Well, I’m sure they’ll get over it. It’s not as if…</p>
              <p>HOMER</p>
              <p>Don’t you understand? I’m the head of a family! I’ve got responsibilities!</p>
            </blockquote>
          </section>
          <section id="twdne">
            <h2><a href="#twdne" title="Link to section: § &#39;TWDNE&#39;">TWDNE</a></h2>
            <p>To upgrade <a href="https://jvns.ca/twdne" id="gwern-twdne" title="&#39;This Waifu Does Not Exist&#39;, Branwen 2019">This Waifu Does Not Exist</a>⁠, I found a prompt which would elicit (usually) novel anime plot summaries which I could feed into the API to <a href="https://jvns.ca/twdne#gpt-3" id="gwern-twdne-gpt-3" title="&#39;This Waifu Does Not Exist § GPT-3&#39;, Branwen 2019">generate thousands of plot summaries</a>⁠.</p>
          </section>
          <section id="the-author-of-the-don-quixote">
            <h2><a href="#the-author-of-the-don-quixote" title="Link to section: § &#39;The Author Of The Don Quixote&#39;">The Author Of The <em>Don Quixote</em></a></h2>
            <p>What sort of variation might GPT-3 generate on a Borges story about overfitting? Sadly, GPT-3 does <em>not</em> just spit out “Pierre Menard”, as appropriate as that would be.</p>
            <blockquote>
              <p><strong>Below is a story about the magical realism premise: “A French writer recreates ‘Don Quixote’ line by line.”</strong></p>
              <p><strong>“The Author of the Don Quixote, Jorge Luis Borges”</strong></p>
              <p>By Pierre Menard</p>
              <p>(Translated by James E. Irby)</p>
              <p>“At the far end of the [Spanish] peninsula of La Mancha, in a place whose name I do not care to remember, a gentleman lived several centuries ago, one of those who has a lance and ancient shield on a shelf and keeps a skinny nag and a greyhound for racing.</p>
              <p>“Despite his meager fortune he was as much people as he liked, and he was extremely alert in military exercises, and at balls he danced more rapidly than anyone, and when it came to fasting he could equal a carthusian, and from dawn to dusk he would follow his large falcon hunting with a small swift-footed spaniel. The elegance of this gentleman’s way of life is still famous in the surrounding countryside, although in Madrid they do not speak of him, for it seems that in the middle course of his days, all at once and without any warning he abandoned his house and position and left to take up arms for a distant king, thus embracing a life of wandering. In spite of his small fortune he is now numbered among the famous knights-errant who have carried their quest beyond the seas; and so, friend reader, this tale is neither a lie nor a parable, for in this world of lies and parables there exists a simple reality, one which has no name.</p>
              <p>“As it happened, it befell in times past that, during a certain summer, there were published in the Spain of Flanders certain Amadises of Gaul with all the licencia for printing that was required in those days. But neither the author nor the printer of these sagas wanted it to be known who they were, and [the names] that were given on the title pages as if by chance… were very different from what was actually written. The following is a faithful listing of the inscriptions that I found in the books:</p>
              <p>Amadis of Gaul was authored by a certain Esplandian. The Knight Platir, by Guillén de Castro. The Amadis of Greece, by John de Centelles; The Mirror of Knighthood, by Gonzalo Hérez de Montalvo. The Amadis of England, by Garci Rodríguez de Montalvo. Don Florisel of the Kingdom of Thrace, by Francisco Sánchez de las Brozas. The Valiant Knight Esplandian, by Diego Ortúñez de Calahorra. The Knight of Shadows, by Baltasar Fortún. The Knight of the Mirrors, by Alvaro Cubillo de Aragón. The Knight of the Cross, by Juan de Salazar de Villena. Knight Esplandian, by Blas Torrente de Alesanco. The Knight Platir, by Gómez Manrique. The Death of Galaor, by Peranzos de Cabrera. The Knight of the Sky, by Francisco de Teruel. The Second Volume of the Ingenious Gentleman Esplandian, by Alvaro Cubillo de Aragón. The Third Volume of Esplandian, by Francisco Baltasar de Villena. The Second Volume of the Valiant Knight Eysplandian, by Alvaro Cubillo de Aragón. The Knight Plesir, by Blas Horré de Antequera. The Sword of Justice, by Jorge de Montemayor. The Ingenious Gentleman Eysplandian, by Jorge de Montemayor. Esplandian, by Alvaro Cubillo de Aragón. The second volume of Esplandian the Ingenious Gentleman, by El Tostado and Juan de la Encina. The third folio, Esplandian the Valiant Knight, by Pedro Fernández de Toledo. Esplandian the Valiant Knight, fourth edition. The Young Esplandian, by Gaspar Daza. The Young Esplandian by Francisco Sánchez de las Brozas, Esplandian the Brave, by El Tostado. The third volume of Esplandian, by Francisco Sánchez de las Brozas. Don Belianís of Greece, by Feliciano de Silva. The second volume of Esplandian, fourth edition, by Don Juan de la Encina.</p>
              <p>“Even at the time I was struck by this, although it was several years before I grasped its meaning and understood that none of the books on the list were fictitious. Quite the contrary: They all existed, they all possessed the virtue of existing and, indeed, they still exist, although no one nowadays has ever heard of them. To understand that situation it is necessary to delve into the history of those times and explain how those texts which we persist in regarding as false had in reality been true.</p>
              <p>“In the year 1525 there lived on the Iberian peninsula one Jorge de Montemayor, a gentleman and descendent of Counts, who served at court under King Charles V, and in his leisure hours devoted himself to writing books of love and chivalry. To defend his work from calumny he adopted the fictitious name Miguel de Cervantes and published in the city of Valladolid a book named The Ingenious and Gallant Don Quixote. It was dedicated to his friend Don Lorenzo de Vargas, a man famous everywhere for his universal erudition.</p>
              <p>“There are many today who deny the existence of this book of Cervantes. I do not know if this is true, but I have seen it, and that is enough for me. I discovered it one afternoon in the house of Diego Fernandez de Ribera, Count of Olivares, in a magnificent manuscript chained to the wall for greater security. On the first page there was written this inscription: Esta obra escribio Miguel de Cervantes, caballero andaluz, con licencia de los superiores.</p>
              <p>“‘This book was written by Miguel de Cervantes, gentleman from Andalusia, with the permission of his betters.’</p>
              <p>“I cannot say how this book came to appear in the library of Olivares or who the mysterious man was who inscribed it. No one in all that city knew anything about any Don Quixote, and so I could not discover from anyone the name of the author. It was a marvel to me, however, that the first part of the volume should be in Bolognese and the second in Aragonese, although both versions of the book contained much of the same text. I examined them word by word, and when I finished I believed without doubt that they had been written by two very different authors whose only connection was a single name.</p>
              <p>“Two years later, by chance, I found another copy of Don Quixote, and this one was entirely written in Castilian. The inscription on the first page identified the author as Gabriel García de Villadiego. In the second part of the book, which dealt with what the author calls ‘the incidents which happened to the girl Grisalda,’ there were verses that hinted that the author was a corregidor of Valladolid. When I asked others about this book, I was informed that Villadiego and Cervantes were one and the same person, but the count would never believe that a compatriot of his could be so great a fabricator of lies; and because he was about to send his own work to the printing press, I thought it would be best if I did not tell him the truth.</p>
              <p>“I should be hard pressed to discover the name of anyone at court who lent any credence to the claim that there had been two authors by the name of Cervantes on earth, but there were many who were certain that an original Esplandian had functioned in England and another in Greece, and there was some controversy about whether anyone named Eplir had ever existed anywhere. But when it was discovered that a man named Garci Rodríguez de Montalvo had written about a valiant knight named Esplandian who had fully deserved all of the epithets bestowed on Don Quixote, then everyone was agreed that Cervantes was the incarnate Esplandian of Montalvo, and that Gabriel de Villadiego was known as his close friend for reasons other than literary.</p>
              <p>“The success of Don Quixote in reality was prodigious, and there were some who made it a point to publish books with that name so that they, too, might share in the fame of its authorship. But in time, almost everyone believed that there had been only those whose names I have listed here and whose titles are still known today for some reason. In addition to that, in order to recognize these spurious books, they usually added two or three spurious volumes, and when it was discovered that no new books appeared I was still able to recover several whose titles I might list as follows:</p>
              <p>Esplandian the Brave, by Pedro Fernández de Madrid Concerning the Ecstasy of Grisalda, by Mateo Alemán Esplandian the Savage, by García Hurtado de Mendoza Second Part of the Ingen</p>
            </blockquote>
            <blockquote>
              <p><strong>Below is a story about the magical realism premise: “A French writer recreates ‘Don Quixote’ line by line.”</strong></p>
              <p>“At four o’clock each afternoon in a little garret in the Latin Quarter of Paris, a man would light two candles. Then he would turn out the electric light. There was no reason to leave the light on, and it made his eyes tired, but he liked the two little flames that would build up quickly in the darkness and soon fill the room with a soft, steady light. At night he would read by candle-light, and sometimes late into the night, until he would fall asleep. He would wake in the morning to the fresh light of day, and begin the day’s work. Once a day he might break off and go out to get milk and bread and wine.</p>
              <p>“He thought it would be good if he wrote every day, and so he kept to his schedule. There was no need to go out for milk or bread because he had done his shopping when he bought the candles, and he had nothing much to do with his money, anyway.</p>
              <p>He was quite rich, although if you had met him on the street you would have thought him rather poor. He dressed very simply and he didn’t spend much time on his hair and he was always to be seen with his hands in the pockets of his trousers. At times, he would sit quite still for many hours and it could seem that he wasn’t there. One day, someone came to pay him a visit. When he opened the door he didn’t seem to be surprised. He knew who it was, of course, although he didn’t seem to recognize him. In fact, he didn’t appear to recognize anyone. When you looked straight into his eyes, it seemed he didn’t know what they were for. The visitor stood on the threshold, looking at him, wondering if he had been called there for a reason.</p>
              <p>“The writer was sitting in his usual spot, hunched forward, writing in the violet ink that came from a small glass bottle with a thick, narrow neck. The visitor didn’t know how long he had been writing, but he was well into the story. He seemed to be writing about nothing, putting down letters and words that had no meaning, but in a language that was completely familiar to him, the visitor thought he heard a strange kind of music, or that the words themselves were music, a song with no beginning or end. He took a few more steps into the room and sat down at the small, plain table by the window.</p>
              <p>‘What are you writing?’ he said.</p>
              <p>The man at the table raised his head. The visitor thought he must be a great writer, because in every other way he seemed to be just an ordinary fellow. ‘What am I writing?’ he repeated, as if it was the silliest question he had ever been asked. ‘Well, what do you think? Surely you can see. I’m writing the story of Don Quixote.’ And then he lowered his head and went back to work.</p>
              <p>“The visitor didn’t understand and yet he did. So he sat there for the rest of the afternoon, looking out of the window onto the street. Dusk had fallen and the room was filled with violet light. In the windows opposite, people lit their candles, and in the sky, a large, full moon rose slowly in the east. After a while he got up and opened the window. From his high vantage point, he could see over the rooftops and the church spires and far away in the distance, a long stretch of the mighty, silvery Seine with an old barge making its way down the river, its orange lamp guiding it at night and by day. At the other end of the street, boys had put up their kite, and it was dancing in the breeze high above the market stalls and cafés. Out there was a world that was completely familiar to him. But here he felt as if he was lost in a great forest where everything was new and strange and impenetrable.</p>
              <p>‘Well,’ said the writer, looking up from his writing, ‘I don’t think you’re here for no reason at all.’ The visitor sat down again. ‘In fact, it’s very important,’ the writer said. ‘It’s more important than you might think.’</p>
              <p>“‘I don’t think so,’ said the visitor. ‘How can it be?’</p>
              <p>“The writer smiled. ‘I’m quite serious,’ he said. ‘It’s about the most important thing there is.’ The visitor decided to be patient. ‘Perhaps you could explain.’</p>
              <p>“The writer put down his pen. ‘It’s because of you that I’m writing this book,’ he said. ‘You do realize that?’</p>
              <p>“The visitor stared at him. ‘How can that be?’ he said. ‘I don’t even know you.’</p>
              <p>“The writer rose and blew out the two candles. Then he filled a small teapot with water, set it to boil on the small gas ring, lit a cigarette and sat back in his chair, his legs crossed. ‘It’s more than twenty-five years ago,’ he said. ‘I was writing another book. Something that everyone thought I was mad to be writing at all, because there didn’t seem to be any point to it. It was just the story of a poor old man, La Mancha, in Spain, who lived in a dark cave in a forsaken valley. Because of this, he lived alone in the dark. He went out rarely. No one knew who he was or where he had come from. He was simply known as Don Quixote de la Mancha. When he wasn’t lying in his cave, or walking in the surrounding countryside, he spent his time reading. He liked to read and one of his favorite books was the story of an Arab knight named Abindarraez, which is set in the time of King Arthur, and tells the story of a lady named Florinda, of whom the Arab knight falls in love. But she is the daughter of the king, so he cannot have her. I thought, What would happen if a story that only exists in the imagination came into the world, not because it was ever told, but because of what it makes people feel?’</p>
              <p>“‘Did you finish the book?’ asked the visitor.</p>
              <p>“‘I had started to write it,’ said the writer. ‘But I found it so hard to find the words that I couldn’t keep going. After six months, I had only finished a few chapters and no one wanted to publish it. Everyone thought it was a failure.’</p>
              <p>“‘You have no reason to explain,’ said the visitor. ‘I think I understand.’</p>
              <p>“The writer laughed out loud. ‘Oh, I doubt it,’ he said. ‘Do you know what the book was called? It was called The Ingenious Gentleman Don Quixote of La Mancha. But no one liked the title. I think one publisher wanted me to change it to The Frightened Knight or The Badly-Composed Knight, which is worse than the first. And another publisher wanted me to write two books, one about Don Quixote himself, and the other about the Arab knight. I would have done it,’ he said. ‘I would have written the books just to get it over with, but when I told the first publisher that I would do exactly as he asked, he was simply outraged. He didn’t like me in the first place, so he decided I was wasting his time and he wouldn’t have anything to do with me again. And the other publisher, who also didn’t like me from the beginning, only half-heartedly put it on his desk and never mentioned it again.’</p>
              <p>“‘So you never finished the book,’ said the visitor.</p>
              <p>“‘No,’ said the writer. ‘And that’s why you have come here. I’m sure of it. You must come and help me to finish the book.’ The visitor laughed. ‘But it was written a long time ago,’ he said. ‘And besides, now you have plenty of money, so you don’t need to write a book.’</p>
              <p>“The writer shook his head. ‘No,’ he said. ‘I don’t have any money at all. In fact, I’m only just managing to make ends meet and I haven’t been able to pay any of my bills for months. I have no prospect of any money whatsoever, except the money from this book. And that’s why you’re here. You must help me to finish it.’</p>
              <p>“The visitor frowned. ‘But how can I help you?’ he said.</p>
              <p>“The writer stubbed out his cigarette in the ashtray. ‘That’s exactly what I’ve been asking myself,’ he said. ‘And it’s why I’ve invited you here. To help me finish the book, you see.’</p>
              <p>“‘You invited me here?’ said the visitor.</p>
              <p>“‘Of course,’ said the writer. ‘I have no friends, so I have no one to invite to this garret. I have no family. They all died when I was young. And because of the way I live my life, I don’t meet anyone on the street and no one comes and visits me. So the only way for me to write a book is to turn to someone. To someone who has a connection to the story I am writing, and who can help me finish it.’</p>
              <p>“The visitor frowned. ‘But how can I help you?’ he said.</p>
              <p>“The writer stubbed out his cigarette in the ashtray. ‘That’s exactly what I’ve been asking myself,’ he said. ‘And it’s why I’ve invited you here. To help me finish the book, you see.’</p>
              <p>“‘You invited me here?’ said the visitor.</p>
              <p>“‘Of course,’ said the writer. ‘I have no friends, so I have no one to invite to this garret. I have no family. They all died when I was young. And because of the way I live my life, I don’t meet anyone on the street and no one comes and visits me. So the only way for me to write a book is to turn to someone. To someone who has a connection to the story I am writing, and who can help me finish it.’</p>
              <p>“The visitor stared at him. ‘But I’m nobody,’ he said. ‘Why do you think I’m going to help you? I don’t know you and you have no reason to believe that I can finish the book.’</p>
              <p>“‘No,’ said the writer. ‘You’re not going to finish the book for me. You’re going to show me how to finish it myself. You’re going to help me to find the words.’</p>
              <p>“The visitor smiled. ‘You’re not going to find the words,’ he said.</p>
              <p>“The writer raised his eyes to heaven. ‘How many times do I have to tell you?’ he said. ‘It’s you who are helping me to find the words.’</p>
              <p>“‘Why me?’ said the visitor.</p>
              <p>“‘I don’t know,’ said the writer. ‘Why do the other people help me?’</p>
              <p>“‘Are there others?’ said the visitor.</p>
              <p>“‘Hundreds,’ said the writer.</p>
              <p>“‘Hundreds of others?’ said the visitor. ‘People who are also helping you to write?’</p>
              <p>“‘Hundreds,’ said the writer. ‘They all help me.’</p>
              <p>“The visitor thought for a moment. ‘And how many have you finished the book with?’ he asked.</p>
              <p>“The writer looked at him. ‘That’s just the thing,’ he said. ‘I haven’t finished the book with a single one of them.’ He smiled. ‘But that’s why you’re here. I believe that you can help me. You have some connection to the story of Don Quixote. I feel it in my bones.’</p>
              <p>“The visitor frowned. ‘What connection can I have to the story of Don Quixote?’ he said.</p>
              <p>“The writer shrugged. ‘I’m not sure yet,’ he said. ‘But you must be able to tell me a story that only exists in your imagination. Something you might have read, or dreamed, or heard from someone else, like a fairy story, or an adventure, or something you might have heard on the radio or a story that your grandfather might have told you when you were a little boy. You must have heard some story like that, don’t you think?’</p>
              <p>“The visitor looked at him. ‘I’m not sure,’ he said. ‘I’ve been thinking about it, but I can’t remember anything.’</p>
              <p>“‘Well, you must have,’ said the writer. ‘You must have a connection. You must know the story of Don Quixote. Or you might have even heard the story of Florinda when she was young, before she became queen. You might have met the knight, in fact.’</p>
              <p>“The visitor shook his head. ‘No,’ he said. ‘I’ve never met a knight in all my life.’</p>
              <p>“‘That’s impossible,’ said the writer. ‘You must know a story that has something to do with Don Quixote.’</p>
              <p>“The visitor shrugged. ‘I might have,’ he said, ‘but I don’t remember it.’</p>
              <p>“‘Then you must tell it to me,’ said the writer. ‘If you don’t remember it, then you must make up your own story and tell it to me.’</p>
              <p>“The visitor thought for a moment. ‘Will you accept it if I make it up?’ he said.</p>
              <p>“The writer leaned across the table and lit a cigarette. ‘Yes,’ he said. ‘If you tell it to me, then it must be the truth. That’s how I see it. If I tell the story of Don Quixote, then it has to be the truth. It must be the actual story, you see.’</p>
              <p>“‘All right,’ said the visitor. ‘I will try and tell you something that is the truth. But I don’t think it will sound like a story.’</p>
              <p>“‘I don’t care what it sounds like,’ said the writer. ‘It might be better if it doesn’t sound like a story. I might be able to imagine it, if you don’t tell the story line by line.’</p>
              <p>“‘All right,’ said the visitor. ‘Let me think for a moment.’</p>
              <p>…The visitor got up and walked to the window and opened it, and breathed the warm air in deeply. ‘I think I have it,’ he said. He sat down again and he closed his eyes, as if he were remembering something, and he was silent for a while, breathing deeply, enjoying feeling the air on his face, and then he appeared to fall asleep. The writer watched him for a minute, and then he got up and opened the door and called out to his wife. ‘Look what I did,’ he said, and he ran out of the room.</p>
              <p>“He came back a few minutes later with a child’s bicycle, a tree stump, and a branch with a forked end. The visitor was still asleep. ‘I couldn’t find anything like a suit of armor,’ said the writer. ‘Or a cape. I brought a bicycle helmet. I thought it had a medieval look to it, don’t you think?’</p>
              <p>“The visitor opened one eye and then the other. He looked at the writer and the bicycle and he smiled a little. ‘Yes,’ he said. ‘Maybe it does.’</p>
              <p>“The writer put on the helmet and walked over to him. ‘Take me to Don Quixote!’ he shouted. ‘Take me to Florinda! Go!’</p>
              <p>“He reached down and grabbed the handle bars on the bicycle and began pushing it forward, and he fell down on the floor laughing, and the visitor laughed too.</p>
              <p>The visitor smiled. ‘But I think I know how I shall begin now.’ He was silent for a moment. ‘It’s important,’ he said, ‘to find the right beginning. I think that’s what the problem was all along. It sounds silly, but it’s true. I think I will begin with the words, ’Once upon a time…’</p>
            </blockquote>
          </section>
          <section id="fanfiction-crossover-dbznarnia">
            <h2><a href="#fanfiction-crossover-dbznarnia" title="Link to section: § &#39;Fanfiction Crossover (DBZ/​Narnia)&#39;">Fanfiction Crossover (DBZ/​<em>Narnia</em>)</a></h2>
            <p>A quick experiment in trying to prompt a <a href="https://en.wikipedia.org/wiki/Crossover_(fiction)" data-link-icon="wikipedia" data-link-icon-type="svg">crossover</a> fanfiction between <em>Dragon Ball Z</em> and <em>The Chronicles of Narnia</em> (best 2 of 5). The use of tags &amp; a title combing a Narnia keyword with a DBZ meme is successful.</p>
            <blockquote>
              <p><strong>“Aslan’s Final Form”
              </strong></p>
              <p><strong>Chapter 1</strong>—Aslan’s Armor
              </p>
            </blockquote>
            <blockquote>
              <p><strong>“Aslan’s Final Form”
              </strong></p>
              <p><strong>Chapter 1</strong>: Pride Cometh Before A Fall
              </p>
              <p>“You all do not yet realize your situation. You think you do, but you do not.”—Aslan, The Lion, the Witch, and the Wardrobe</p>
              <p>I can’t believe it. I can’t believe what I have to deal with now. I gave humans freedom and the result is this? Even with my full powers restored, I can’t even get these people to respect me anymore. What happened? When did things go so wrong?</p>
              <p>I guess it all started with that rat. Oh, you’re probably wondering what this has to do with anything. It might seem like it’s only tangentially related, but everything will come back around, I promise. The story of my death and resurrection will end up making sense, even if it doesn’t right now. I guess I should start by explaining who I am. My name is Aslan, and I am the High King of Narnia.</p>
              <p>My father was a god called Zeus, and he and my mother had another son who was just as powerful as I was called Hercules. The three of us were always getting into trouble when we were growing up, and we traveled all over the world doing crazy things. We were always competing to see who was the strongest of us three, even though we were pretty evenly matched. The one thing that bothered me was that Hercules was always looking for an excuse to hit me, but we always worked things out when we got back home.</p>
              <p>One day, Hercules and I were having a friendly spar in the gym that had been built in our palace, and we were practicing using our godly energy. Zeus had created a special sword for me that was filled with my energy, and I couldn’t use my powers as well without it. I guess it was my pride that made me lose control and try to attack Hercules without using my sword, but he was so quick that I couldn’t touch him. My sword ended up flying out of my hands, and it went right through the throne room window and into the street below. It was at that moment that everything changed.</p>
              <p>Hercules was holding a ball of godly energy in his hand and swung it at my head. I guess he thought he would be able to do a cool move like Goku and knock me out with one hit. I had been moving my head to the side, so the ball of energy went right by me and shattered the throne room window. I didn’t realize at the time, but the ball of energy was carrying divine power from me and Hercules into the streets of Narnia. It was at that moment that a human boy named Eustace fell through the window and into our world. The boy had been flying his model airplane around the city, and the impact sent him right through the window. I didn’t realize what had happened at first, and I called for someone to take care of the boy. His mother came running in the room, and she scooped him up and took him home. The boy was completely unconscious when he left.</p>
              <p>I had completely forgotten about the boy when he returned to our world several hours later. He was riding on a griffin, but he fell off in the throne room and hit his head. I came over to see if he was all right, but Hercules kicked him in the face and tried to kill him. I got between them and stopped Hercules, but I didn’t understand why he was doing it in the first place. It turns out that the godly energy that had come from Hercules’ ball of energy had given Eustace godly powers when he fell through the window. Since Eustace is human, his powers were very different from mine, but he did share some of my abilities. My first inclination was to get rid of him, but then I realized that maybe I could use him. I figured that there were only a few ways to get humans to listen to me again, and I thought maybe it would work if I teamed up with someone who had a similar power level to mine.</p>
              <p>I explained to Eustace that he had special powers, and I told him how to activate them. He asked me how he could access them, and I tried to be as vague as possible about it. I wasn’t about to give away my secrets to someone like that, but I didn’t want him to suspect anything. After all, he was a human and would never understand how godly power works.</p>
              <p>Eustace became a warrior named “The Digger” who fought evil dragons and rescued damsels in distress. The man loved adventure, and he used his powers to help people. In a lot of ways, he was similar to a Dragon Ball Z character called Goku, who is also a warrior fighting against the forces of evil. Unlike Goku, The Digger wasn’t super smart and didn’t understand that killing dragons was bad for the ecosystem. He never would have let that happen if he had been in charge, but he did a lot of good for the people of Narnia, even if he was a bit reckless about it. I was impressed with the man’s work, and I kept my distance to avoid suspicion. I kept an eye on him, but I never made contact. I would have loved to have used him as a pawn against my enemies, but I couldn’t take that risk.</p>
              <p>Years later, Eustace’s wife and children became involved in The Digger’s adventures. He began using them in his battles, and he never even stopped to think about what was happening to them. I had always wanted kids of my own, but the idea of my children fighting to the death just for my own entertainment was abhorrent to me.</p>
              <p>Even though I didn’t get involved directly with The Digger, I still cared about his family. It hurt me to see them having to deal with all the fighting and fighting themselves. They really were a strong family, and they were just as important to me as my own family had been. That’s why I decided to take direct action. I had planned to kill all of them after their final battle, but then I realized that I could kill two birds with one stone.</p>
              <p>I met with Hercules in private and told him what I was planning. I explained that I wanted his help in destroying The Digger and his family for the sake of justice. He had been watching them for</p>
            </blockquote>
          </section>
          <section id="a-new-kind-of-scribing">
            <h2><a href="#a-new-kind-of-scribing" title="Link to section: § &#39;A New Kind of Scribing&#39;">A New Kind of Scribing</a></h2>
            <p>A silly request I filled: what does GPT-3 complete if you prompt it solely with “A completely new kind of writing was created, here is a sample:”? Does it invent actually new kinds of writing, or what? For the most part, it seems to generate either cult/​religious material, crankery, literary criticism, or computer science/​technology-like material—all of which in retrospect make sense.</p>
            <blockquote>
              <p><strong>A completely new kind of writing was created, here is a sample:</strong></p>
              <p>To those brave souls who would come on their own accord to the church of the divine skeleton, thus commencing the great passage of the mind into the absolute heart of the mind.</p>
              <p>FAR FROM IT WAS THE KIND OF MYSTERIES WHERE THE EXPERIENCE OF THE HIGHER SPIRIT LIVES THROUGH THE LOUDNESS OF STAMPING FEET AND SHOUTING!</p>
              <p>Such a powerful movement of the mind began to get ecstatic and dance with the universe within themselves, in their living rooms, in dark hidden corners of lonely streets in Brazil, in the church of the divine skeleton.</p>
              <p>THE BEAT PULSATED! THE BEAT PULSATED WITH THE VERY HEART OF GOD!</p>
              <p>And then a new vision appeared, the memory of the ancient form returned with brilliant light, and as if we had awakened from a dream, all could see him among us… SAMAEL SIDROHIN… HE WHO DIED AT 30… AND EVEN MORE, WHAT WAS TO COME FROM HIS WORKS AND INVOLVEMENT WITH NUAYRAN DRAMA… THIS NEW STYLE IN WRITING… THIS NEW WAY TO UNDO LIFE AND RECREATE IT AS “ART”… THE ENDLESS JOY OF ETERNAL LOVE AND EXISTENCE.</p>
              <p>At this moment, from the already expired form of NAYRAN DRAMA, the mystical book was born to an unprecedented publication which would become a true best-seller in the country, resulting in a chain reaction of publishing houses looking for new stories… So NAYRAN DRAMA’s work began to be read and commented on by the great majority of Brazilians… Once, two literary critics, who were considered conservative were asked about NAYRAN DRAMA’s work. The first one answered: “This is not literature, this is trash.” The other answered: “Literature is trash”.</p>
              <p>Thus, art was once again born from trash. Trash was valorized with this new form which altered the way people read and thought about life, love, and death.</p>
              <p>Perhaps you think we are exaggerating… We invite you to check for yourselves. In this book you will find the confirmation of all that has been said here; therefore, before you plunge into such intense reading, we recommend that you pray for the wisdom of King Solomon so as to create the true Work of the Mind, or a great work which will one day become like a book of the philosophers.</p>
              <p>Finally, we would like to take this opportunity to once again invite the reader to the celebration of the new century at the age of 21, on Thursday, December 31<sup>st</sup> of 1999, where we will be performing the whole work of the 21<sup>st</sup> century in an artistic representation which will include music, dance, performance, theater, video art, literature, rituals and alchemical incantations. The event will take place in the Palace of Art on Praça XV in Salvador, Bahia. This event will be produced by César Menezes and directed by Artur Agostini, among others.</p>
              <p>Written in Santiago de Chile, Chile</p>
              <p>September 1<sup>st</sup> of 1999</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> At 10:30 p.m., just about everybody else was either chatting or inside. I’d made the mistake of wearing dark shades that night and in the sea of black, I felt completely alone. It was a kind of crowded loneliness, where everybody else was taken away and gone while I was left in a bubble.</p>
              <p>But there were flashes of encouragement:</p>
              <p>I’m not someone who would walk into a room and say, “Oh my god, I’m totally fine,” but I’m not that bad. If I have a question, I’ll ask it and if I don’t know, I’ll find out. So this was good for me to get some closure. It wasn’t like I came out of the session and it was all beautiful. Some things I have to fix on my own, some I don’t. But it’s okay. The point is I don’t want to hold myself back.</p>
              <p>Sarah Neumann is the editor of On Campus this year. Reach her at snneumann@bridgew.edu.</p>
              <p>Featured Image by Emily Steinberg / Heights Edito</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong> As the green forest leaves
              </p>
              <p>Suddenly, this new poetry burst onto the scene and it completely revolutionized the way that poetry was looked at by not only the Koreans but by all of Asia. Poets all over the land now wanted to be as good as the Hahoe Poets. It has been said that Hahoe created a renaissance in Korean Literature and the effect of this can be seen today in modern Korea.</p>
              <p>However, many of the great thinkers in Korean history came from the Hahoe Poetry Society.</p>
              <p>That is one of the reasons that Hahoe is considered to be so important to Korea’s past.</p>
              <p>Brief History of Hahoe by So Eun Heo, teacher.</p>
              <p>Email Address:</p>
              <p>To read the poems go here:</p>
            </blockquote>
            <p>Note: the Hahoe poets do not exist, although there is a historical village of that name in South Korea. The quoted poem echoes the famous passage from the <a href="https://en.wikipedia.org/wiki/Diamond_Sutra" data-link-icon="wikipedia" data-link-icon-type="svg">Diamond Sutra</a> (“A star at dawn, a bubble in a stream, / A flash of lightning in a summer cloud, / A flickering lamp, a phantom and a dream, / So is this fleeting world.”), but aside from that, <em>seems</em> to be original. (It is such a short gatha, and there are typically so many ways to translate one, that I can’t be sure that this is not published somewhere in a book that Google is not picking up or is not a trivial variant.)</p>
            <blockquote>
              <p><strong>…</strong> A somber cadaver dries the sick [p. 81]</p>
              <p>Apart from the meaning, every single word and letter of which has a distinct purpose, this sentence has a rhythm that is perfectly crafted. The iambic pentameter. The play on sounds. The titillating sounds of “somber” and “cadaver” in the context. One by one the words in the sentence evoke emotions, sadness, foreboding, a taste of something disturbing, a foreshadowing of something to come. This could not be mimicked. No one who is not John Milton can do this.</p>
              <p>There is also a lot of Miltonic word-play. Here is another example:</p>
              <p>Nor the dark Laws of Democritus
              </p>
              <p>The word “interpret” is commonly used to mean “translates” (a word referring to classical or literary translations) which is somewhat true in the context here. However, apart from that, the verb “interpret” also refers to the work of the clergy, and more specifically, that of a priest who performs confession and places a penitent under an interdict (a prayer to God to withhold spiritual gifts from the penitent till he/​she repents) The style of the writing is such that it alternates between the verse and the prose. Milton uses the verse for more dense, emphatic, and descriptive writing, and the prose for some kind of commentary.</p>
              <p>To what purpose serves the exercise in verse?</p>
              <p>For Milton, unlike any other writer of the preceding eras, and all of the writers after him (notably Wordsworth and Whitman), the free verse, whether the iambic pentameter or not, is an end in itself, it is art.</p>
              <p>As Milton wrote in his sonnet <a href="https://jvns.ca/doc/www/www.dartmouth.edu/c594b52a19151b53a64b2fcbdbeb3438df6dd75c.html" rel="archived alternate nofollow" data-url-original="https://www.dartmouth.edu/~milton/reading_room/conscience/text.shtml" title="On the New Forcers of Conscience under the Long Parliament (Original URL: https://www.dartmouth.edu/~milton/reading_room/conscience/text.shtml )">“On the new forcers of Conscience”:</a></p>
              <p>Me, me their utmost art could not have forced
              </p>
            </blockquote>
            <p>All the quoted lines of poetry in this sample appear to be original.</p>
            <blockquote>
              <p><strong>…</strong> You can download new challenges to solve by following the external link above.</p>
              <p>4. The City of Guesstures</p>
              <p>This project has been described in an earlier post: http:/​/​www.pixelnovelgames.com/​jquery/​the-city-of-gestures-0/​</p>
              <p>The City of Guesstures has a good focus on story and text choices. The narrative is delivered through 3D objects (3D characters!) in a mostly static background.</p>
              <p>Use the left click button on your mouse to see the background (if possible) and the right button to choose what you want to say.</p>
              <p>5. Bunker</p>
              <p>It was not easy to find any project related to Bunker, the company behind the software for interactive fiction. I am not very familiar with the concept of “interactive fiction”. I had to Google. interactive fiction is a computer game genre. In computer game terms interactive fiction is “a type of video game (usually a text adventure) in which the game world is displayed in text that can be searched, examined, or traversed.”</p>
              <p>This means that text interaction is clearly the core of Bunker’s solution.</p>
              <p>The video that follows, is a presentation of a part of the Bunker software and it’s capabilities. If it is a bit hard to understand, I would still recommend you to watch it because it’s the most intuitive way of showing a typical Bunker application.</p>
              <p>The high level concept is very simple: it allows you to easily create and publish an interactive story in any device.</p>
              <p>I am not 100% familiar with the capabilities of this software, but I like the idea of having access to a framework that can be used to quickly create story driven applications.</p>
              <p>Some the key features of Bunker are:</p>
              <p>Free Web Player</p>
              <p>Content editor/​server built in HTML</p>
              <p>Player authentication</p>
              <p>Custom integration (via <a href="https://en.wikipedia.org/wiki/Adobe_Flash" data-link-icon="wikipedia" data-link-icon-type="svg">Flash</a>⁠, XCode, HTML5, AIR)</p>
              <p>iOS and Android application publishing</p>
              <p>I suspect that in some ways it may work in a similar way as Twine, that means, the editor can be used for both content creation and play.</p>
              <p>6. Wattpad</p>
              <p>Wattpad seems to be a very popular site for the sharing of original and fan fiction (they have over 5 million readers and 50 million unique visitors monthly, very cool).</p>
              <p>It has been growing like crazy: in 2012, the site had 200,000 stories; in 2013 it had 7 million. There are millions of users using the platform and from those, a million are active story authors.</p>
              <p>They have this thing called the Wattpad Stars where they give one of the top authors a $100,000 grant, to encourage other authors. This page has a description of all the winners:</p>
              <p>I really like this approach, it’s definitely different from what we are used to.</p>
              <p>From a developer perspective I am also interested in their API: http:/​/​wattpad.com/​api. This is what they say:</p>
              <p>The API lets you automatically push your content to a variety of different partners and applications:</p>
              <p>Anywhere else you can think of!</p>
              <p>Sounds interesting, don’t you think?</p>
            </blockquote>
            <blockquote>
              <p><strong>…</strong></p>
              <pre><code>*&lt;table width=&#34;300&#34; align=&#34;center&#34;&gt;*&lt;tr&gt;*&lt;td width=&#34;100&#34; bgcolor=&#34;#007000&#34;&gt;*&lt;p align=&#34;center&#34; style=&#34;color: white; background-color: #007000;&#34;&gt;*&lt;h4&gt;Sogou input method&lt;/h4&gt;*&lt;table width=&#34;300&#34; align=&#34;center&#34;&gt;*&lt;tr&gt;*&lt;td&gt;*&lt;table width=&#34;300&#34; align=&#34;center&#34;&gt;*&lt;tr&gt;*&lt;td width=&#34;250&#34;&gt;*&lt;p&gt;Hello,</code></pre>
              <p>It doesn’t look like a normal string at all, it is what I call a monolithic composition, it has a very specific way of parsing and executing the commands, and also understand only the commands that are present in that composition, the above example only understand the first few commands, if it is given a different command:</p>
              
              <p>It doesn’t parse correctly, what is the format that the development team use for such compositions?</p>
              <p>They would use an environment that looks like this:</p>
              <p>The top blue panel is the console, this is where the input commands will be entered, commands may include any sequences and event handlers, the second blue panel is the debugger, the debugger would display visual information about the state of the console, the top panel would have a tree view of the console, where blue nodes in the tree view would be the commands the user is able to execute, the debugger also should be able to pause the environment and reveal the state of the execution and to resume normal execution.</p>
              <p>If the user wants to create an application for the application he would go into the application create screen, this would look like so:</p>
              <p>The application composition screen is the top panel, this screen will display the tree view of the composition, the code block is where the user would type his application source code, he would be able to put comments inside his application source code, and the grey box is where he would see the execution of his application, again the debugger should be able to pause the execution and display the state of his application.</p>
              <p>That is it for today, I will write more about other parts of the system and how I envision it.</p>
              <p>Advertisements</p>
            </blockquote>
          </section>
          <section id="bad-analogies">
            <h2><a href="#bad-analogies" title="Link to section: § &#39;Bad Analogies&#39;">Bad Analogies</a></h2>
            <p>Prompt provided by <a href="https://old.reddit.com/r/GPT3/comments/ith31k/have_bad_analogies_been_tried_with_gpt3_some/" data-link-icon="reddit" data-link-icon-type="svg">TheDividendReport</a>:</p>
            <blockquote>
              <p><strong>Bad analogies</strong></p>
              <p><strong>“She caught your eye like one of those pointy hook latches that used to dangle from screen doors and would fly up whenever you banged the door open again.” (R. M., Fairfax Station)</strong></p>
              <p><strong>“The little boat gently drifted across the pond exactly the way a bowling ball wouldn’t.” (R. B., Springfield)</strong></p>
              <p><strong>“McBride fell 12 stories, hitting the pavement like a Hefty Bag filled with vegetable soup.” (P. S., Silver Spring)</strong></p>
              <p><strong>“From the attic came an unearthly howl. The whole scene had an eerie, surreal quality, like when you’re on vacation in another city and ‘Jeopardy’ comes on at 7 p.m. instead of 7:30.” (R. A., Washington)</strong></p>
              <p><strong>“Her hair glistened in the rain like nose hair after a sneeze.” (C. S., Woodbridge)</strong></p>
              <p><strong>“Her eyes were like two brown circles with big black dots in the center.” (R. B., Springfield)</strong></p>
              <p><strong>“Bob was as perplexed as a hacker who means to access T:flw.quid55328.com/​ch@ung but gets T:.quidaaakk/​ch@ung by mistake.” (K. K., Landover Hills)</strong></p>
              <p><strong>“Her vocabulary was as bad as, like, whatever”. (Unknown)</strong></p>
              <p><strong>“He was as tall as a six-foot-three-inch tree.” (J. B., Chevy Chase)</strong></p>
              <p><strong>“The hailstones leaped from the pavement, just like maggots when you fry them in hot grease.” (G. F. Hevel, Silver Spring)</strong></p>
              <p><strong>“Her date was pleasant enough, but she knew that if her life was a movie this guy would be buried in the credits as something like ‘Second Tall Man’.” (R. B., Springfield)</strong></p>
              <p><strong>“Long separated by cruel fate, the star-crossed lovers raced across the grassy trains, one having left Cleveland at 6:36 p.m. traveling at 55 mph, the other from Topeka at 4:19 p.m. at a speed of 35 mph.”(J. H., Arlington)</strong></p>
              <p><strong>“The politician was gone but unnoticed, like the period after the Dr. on a Dr Pepper can.” (W. G., Madison, Ala.)</strong></p>
              <p><strong>“They lived in a typical suburban neighborhood with picket fences that resembled Nancy Kerrigan’s teeth.” (P. K., Syracuse, N.Y.)</strong></p>
              <p><strong>“John and Mary had never met. They were like two hummingbirds who had also never met.” (R. B., Springfield)</strong></p>
              <p><strong>“The thunder was ominous-sounding, much like the sound of a thin sheet of metal being shaken backstage during the storm scene in a play.” (B. F., Alexandria)</strong></p>
              <p><strong>“His thoughts tumbled in his head, making and breaking alliances like underpants in a dryer without Cling Free.” (C. S., Woodbridge)</strong></p>
              <p><strong>“The red brick wall was the color of a brick-red Crayola crayon.”</strong> (Viaggo, Washington)</p>
              <p>“The grass was so high it was difficult to find the grave, and in fact when they finally did spot it they had to get out and look down to find it.” (K. F., Arlington)</p>
              <p>“The sleet fell on her head, briefly spiking her hair like an angry porcupine.” (R. L., Gaithersburg)</p>
              <p>“It came without ribbons, without tags, without packaging, and without instructions.” (D. B., Washington)</p>
              <p>“The plan was simple, like my brother-in-law Phil.” (R. L., Gaithersburg)</p>
              <p>“The young fighter had a hungry look, the kind you get from not eating for a while.” (B. H., Washington)</p>
              <p>“The dandelion swayed in the gentle breeze like a mime trying to tell a story with no words.” (J. H., Arlington)</p>
              <p><em>“The sky above the port was the color of television, tuned to a dead channel.”</em><a href="#fn55" id="fnref55" role="doc-noteref"><sup>55</sup></a> (C. D., Washington)</p>
              <p>“The man was as bald as a cueball.” (J. H., Arlington)</p>
              <p>“The thunder was ominous-sounding, much like the sound made by a frying egg.” (D. B., Washington)</p>
              <p>“The doorknob felt like a big glob of grease.” (C. C., Reston)</p>
              <p>“He felt like he was being hunted down like a dog.” (K. F., Arlington)
              </p>
            </blockquote>
          </section>
        </section>
        <section id="external-links">
          
          <ul>
            <li>
              <p><a href="https://bmk.sh/2020/05/29/GPT-3-A-Brief-Summary/" title="Why GPT-3 Matters">“Why GPT-3 Matters”</a>⁠/ ​<a href="https://jvns.ca/doc/www/bmk.sh/cdec0eef7cf7baa4bb4596f15a2c8746754a256d.html" rel="archived alternate nofollow" data-url-original="https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/" title="(Original URL: https://bmk.sh/2020/08/17/Building-AGI-Using-Language-Models/ )">“Building AGI Using Language Models”</a>⁠, Leo Gao</p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/towardsdatascience.com/2b019a7e2c826b3cc24dd30d2d73f1e90a23c3bb.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab" title="(Original URL: https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab )">“GPT-3: Creative Potential of NLP: New ML milestone by OpenAI—in action”</a></p>
            </li>
            <li>
              <p><a href="https://www.lesswrong.com/posts/Mzrs4MSi58ujBLbBG/you-can-probably-amplify-gpt3-directly" data-link-icon="LW" data-link-icon-type="text">“You Can Probably Amplify GPT3 Directly”</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/machinamenta.blogspot.com/0cec1a8d61073e9f14d31f9718173f242dc90286.html" rel="archived alternate nofollow" data-url-original="https://machinamenta.blogspot.com/2020/07/regarding-gpt-3s-faculties.html" title="(Original URL: https://machinamenta.blogspot.com/2020/07/regarding-gpt-3s-faculties.html )">“Regarding GPT-3’s faculties”</a>⁠, Summers-Stay</p>
            </li>
            <li>
              <p><a href="https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang" id="jones-2020" data-link-icon="LW" data-link-icon-type="text" title="&#39;Are we in an AI overhang?&#39;, Jones 2020">“Are We In An AI Overhang?”</a></p>
            </li>
            <li>
              <p><a href="https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential" data-link-icon="openai" data-link-icon-type="svg">“OpenAI’s latest breakthrough is astonishingly powerful, but still fighting its flaws: The ultimate autocomplete”</a>⁠, <em>The Verge</em>; <a href="https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language" data-link-icon="openai" data-link-icon-type="svg" title="Computers are getting closer to passing the Turing Test">“GPT-3, explained: This new language AI is uncanny, funny—and a big deal”</a>⁠, <em>Vox</em></p>
            </li>
            <li>
              <p><a href="https://old.reddit.com/r/slatestarcodex/comments/hrx2id/a_collection_of_amazing_things_gpt3_has_done/fy7jl0y/" data-link-icon="reddit" data-link-icon-type="svg">Poems on modafinil, Amazon, Alibaba</a>⁠; <a href="https://nitter.moomoo.me/nikillinit/status/1289281944421711878" data-link-icon="twitter" data-link-icon-type="svg">on the pharmaceutical industry</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/www.allencheng.com/c98dd701cfb9bc9fc684403b1a9dd1b48f487b9b.html" rel="archived alternate nofollow" data-url-original="https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/" title="(Original URL: https://www.allencheng.com/starting-a-business-around-gpt-3-is-a-bad-idea/ )">“Starting a Business Around GPT-3 is a Bad Idea”</a>⁠, Allen Cheng (see also <a href="https://jvns.ca/complement" id="gwern-complement" title="&#39;Laws of Tech: Commoditize Your Complement&#39;, Branwen 2018">“commoditize your complement”</a>)</p>
            </li>
            <li>
              <p><a href="https://www.patreon.com/posts/39864473" data-link-icon="patreon" data-link-icon-type="svg">“Working with GPT-3 on Fiction”</a>⁠, Liminal Warmth; <a href="https://jvns.ca/doc/www/arr.am/89ca3637676bc00ad33e512cdde5a81d9160d657.html" rel="archived alternate nofollow" data-url-original="https://arr.am/2020/07/31/gpt-3-using-fiction-to-demonstrate-how-prompts-impact-output-quality/" title="(Original URL: https://arr.am/2020/07/31/gpt-3-using-fiction-to-demonstrate-how-prompts-impact-output-quality/ )">“GPT-3: Using Fiction to Demonstrate How Prompts Impact Output Quality”</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/medium.com/bf53e85fc1c309d0bc798880388c4612a129683e.html" data-link-icon="𝐌" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/@marcinkraszewski/gpt-3-project-ideas-with-code-5940c275bc41" title="(Original URL: https://medium.com/@marcinkraszewski/gpt-3-project-ideas-with-code-5940c275bc41 )">“GPT-3 Project ideas with code”</a>⁠, Marcin Kraszewski</p>
            </li>
            <li>
              <p><a href="http://gptprompts.wikidot.com/context-stuffing" data-link-icon="openai" data-link-icon-type="svg">“context stuffing”</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/vasilishynkarenka.com/c66f53e15500e354cccb37729cbecd2e10ce5bc7.html" rel="archived alternate nofollow" data-url-original="https://vasilishynkarenka.com/gpt-3/" title="(Original URL: https://vasilishynkarenka.com/gpt-3/ )">“How I used GPT-3 to hit Hacker News front page 5 times in 3 weeks: In three weeks, I got to the front page five times, received 1054 upvotes, and had 37k people come to my site”</a> (advice on effective prompt programming)</p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/www.notion.so/d5e39124e3188836a35f261e14e9d6531f7c3764.html" rel="archived alternate nofollow" data-url-original="https://www.notion.so/GPT3-Dataset-Task-Model-b97a267d6f5f44e688ba4f7ec85c00cc" title="TLDR: I go from wanting a machine learning model to getting that trained model, without actually having a dataset. (Original URL: https://www.notion.so/GPT3-Dataset-Task-Model-b97a267d6f5f44e688ba4f7ec85c00cc )">“GPT3 → Dataset → Task Model?”</a> (generating a logic quiz dataset, using GPT-3 for synthetic labels); <a href="https://jvns.ca/doc/www/arxiv.org/ba393d5b4f7894165b9ad07c8df4c9d051b610e4.pdf#microsoft" id="wang-et-al-2021-gpt3labeling" data-link-icon="MS" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2108.13487?fallback=original#microsoft" title="&#39;Want To Reduce Labeling Cost? GPT-3 Can Help&#39;, Wang et al 2021 (Original URL: https://arxiv.org/abs/2108.13487#microsoft )">“Want To Reduce Labeling Cost? GPT-3 Can Help”</a><span>, <span><span title="et al">Wang</span> <span>et al</span> <span>2021</span></span></span></p>
            </li>
            <li>
              <p><a href="https://www.lesswrong.com/posts/4JeAoTrAuByXGw6zm/updated-how-does-gpt2-s-training-corpus-capture-internet" data-link-icon="LW" data-link-icon-type="text">“How Does GPT-2′s Training Corpus Capture Internet Discussion? Not well”</a></p>
            </li>
            <li>
              <p><a href="https://thefirstaibook.com/#post-inner"><em>The First AI Book</em></a> (anthology generated by Nick Saraev)</p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/deponysum.com/06bbeae7223da80a7b51c7203b8b9e0d1de8e2fa.html" rel="archived alternate nofollow" data-url-original="https://deponysum.com/2020/08/16/2233/" title="(Original URL: https://deponysum.com/2020/08/16/2233/ )">“<em>The Arcadian Cantos</em>—A poem without an author—1<sup>st</sup> Draft”</a>⁠, DePonySum</p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/arxiv.org/743f2cfd14102b1e43b4e667b00ad60a5e969134.pdf#google" id="bahri-et-al-2020" data-link-icon="alphabet" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2008.13533?fallback=original#google" title="&#39;Generative Models are Unsupervised Predictors of Page Quality: A Colossal-Scale Study&#39;, Bahri et al 2020 (Original URL: https://arxiv.org/abs/2008.13533#google )">“Generative Models are Unsupervised Predictors of Page Quality: A Colossal-Scale Study”</a><span>, <span><span title="et al">Bahri</span> <span>et al</span> <span>2020</span></span></span></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/arxiv.org/a8c13af08875d3fd9b1303535ec6929901b3f5b8.pdf" id="hendrycks-et-al-2020-q-and-a" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2009.03300?fallback=original" title="‘MMLU: Measuring Massive Multitask Language Understanding’, Hendrycks et al 2020 (Original URL: https://arxiv.org/abs/2009.03300 )">“Measuring Massive Multitask Language Understanding”</a><span>, <span><span title="et al">Hendrycks</span> <span>et al</span> <span>2020</span></span>;</span> <a href="https://jvns.ca/doc/www/www.eduref.net/0c95bcc7d3d6c953a3e6064e74be07b9b0a8e99a.html" rel="archived alternate nofollow" data-url-original="https://www.eduref.net/features/what-grades-can-ai-get-in-college/" title="(Original URL: https://www.eduref.net/features/what-grades-can-ai-get-in-college/ )">“What Grades Can AI Get in College? AI manages to score a “C” average across four subjects, failing only one paper”</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/benanne.github.io/c4abb1460b18476452f98dabf09cef054373868b.html" rel="archived alternate nofollow" data-url-original="https://benanne.github.io/2020/09/01/typicality.html" title="(Original URL: https://benanne.github.io/2020/09/01/typicality.html )">“Musings on typicality”</a>⁠, Sander Dieleman</p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/ai/poetry/2020-elkins.pdf" id="elkins-chun-2020" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Can GPT-3 Pass a Writer’s Turing Test?&#39;, Elkins &amp; Chun 2020">“Can GPT-3 Pass a Writer’s Turing Test?”</a><span>, <span><span>Elkins &amp; Chun</span><span>2020</span></span></span></p>
            </li>
            <li>
              <p><a href="https://nitter.moomoo.me/julianharris/status/1421008325785890825" data-link-icon="twitter" data-link-icon-type="svg">“There’s a new test of creativity by McGill U and UniMelb so of course I had to try OpenAI’s GPT-3 on it. Scored 82.32! So GPT-3 is more creative than 74% of people who have completed it.”</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/hdsr.mitpress.mit.edu/c12dfa10eea2dc011214cdc52323e4efa02890cd.html" rel="archived alternate nofollow" data-url-original="https://hdsr.mitpress.mit.edu/pub/wi9yky5c/release/2" title="(Original URL: https://hdsr.mitpress.mit.edu/pub/wi9yky5c/release/2 )">“Computers Learning Humor Is No Joke”</a><span>, <span><span>Winters</span><span>2021</span></span></span></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/ai.stanford.edu/53906a0a199a213fa1bce0b97ecad6b5063931e4.html" rel="archived alternate nofollow" data-url-original="https://ai.stanford.edu/blog/in-context-learning/" title="(Original URL: https://ai.stanford.edu/blog/in-context-learning/ )">“Extrapolating to Unnatural Language Processing with GPT-3’s In-context Learning: The Good, the Bad, and the Mysterious”</a>⁠, Frieda Rong</p>
            </li>
            <li>
              <p><a href="https://github.com/semiosis/pen.el" data-link-icon="github" data-link-icon-type="svg" title="Pen.el integrates LMs (language models) such as OpenAI’s GPT-3 or EleutherAI’s GPT-J into emacs by generating functions from prompts that map emacs’s corners loosely onto LMs. These functions can be used interactively or non-interactively and in a variety of configurable ways. Pen.el also facilitates the creation, development, discovery and usage of prompts."><code>Pen.el</code>: Prompt engineering in Emacs</a></p>
            </li>
            <li>
              <p><a href="https://jvns.ca/doc/www/www.packtpub.com/60750cff5f7bdb915bc25473088bd390049f8a60.html" rel="archived alternate nofollow" data-url-original="https://www.packtpub.com/product/exploring-gpt-3/9781800563193" title="(Original URL: https://www.packtpub.com/product/exploring-gpt-3/9781800563193 )"><em>Exploring GPT-3: An unofficial first look at the general-purpose language processing API from OpenAI</em></a><span>, <span><span>Tingiris</span><span>2021</span></span></span></p>
            </li>
            <li>
              <p><a href="https://www.lesswrong.com/posts/LbXeRounm7tg6i8mk/post-history-is-written-by-the-martyrs" data-link-icon="LW" data-link-icon-type="text">“Post-History Is Written By The Martyrs”</a>⁠, Veedrac</p>
            </li>
            <li>
              <p><a href="https://nitter.moomoo.me/goodside" data-link-icon="twitter" data-link-icon-type="svg">Riley Goodside</a> (unusual prompts)</p>
            </li>
            <li>
              <p><strong>Discussion</strong>:</p>
              <ul>
                <li>Reddit: <a href="https://old.reddit.com/r/slatestarcodex/comments/hfouw5/gpt3_for_creative_fiction_poetry_dialogue_puns/" data-link-icon="reddit" data-link-icon-type="svg">1</a><!-- /[2](https://old.reddit.com/r/gwern/comments/hfouef/gpt3_for_creative_fiction_poetry_dialogue_puns/) -->/​<a href="https://old.reddit.com/r/MediaSynthesis/comments/hfoulh/gpt3_for_creative_fiction_poetry_dialogue_puns/" data-link-icon="reddit" data-link-icon-type="svg">2</a>⁠/ ​<a href="https://old.reddit.com/r/HPMOR/comments/hgw2zq/gpt3_neural_net_completions_of_mor_chapter_16/" data-link-icon="MoR" data-link-icon-type="text,tri,italic">4</a>⁠/ ​<a href="https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/hl0x18/gwerns_post_on_gpt3_has_some_gold/" data-link-icon="reddit" data-link-icon-type="svg">5</a>
                </li>
                <li>
                  <a href="https://www.lesswrong.com/posts/3YjWifjkhw2Q8RQL2/gpt-3-fiction-samples" data-link-icon="LW" data-link-icon-type="text">LW</a>
                </li>
                <li>
                  <a href="https://jvns.ca/doc/www/news.ycombinator.com/a39e12798b84193e7b40f501f956a6b3b1e90004.html" data-link-icon="hn" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://news.ycombinator.com/item?id=23722635" title="(Original URL: https://news.ycombinator.com/item?id=23722635 )">HN</a>
                </li>
              </ul>
            </li>
          </ul>
          <p><a href="https://jvns.ca/gpt-3#prompts-as-programming" id="gwern-gpt-3-prompts-as-programming" title="&#39;GPT-3 Creative Fiction § Prompts As Programming&#39;, Branwen 2020">null</a> <!-- LinkAuto override: disable self-linking --></p><!--
"Lo! Death has reared himself a throne / In a strange city lying alone / Far down within the dim West, / Where the good and the bad and the worst and the best / Have gone to their eternal rest. / There shrines and palaces and towers / (Time-eaten towers and tremble not!) / Resemble nothing that is ours. / Around, by lifting winds forgot, / Resignedly beneath the sky / The melancholy waters lie. ..."

---

Lo! AGI hath built a spire,
In a fabled city by the Bay,
Where West winds whisper and conspire,
Ushering forth a new display.
Bold minds and start-ups gather there,
To mold the world in their own quest,
Bound by code and dreams that dare
To free mankind or be its jest.

There rise towers of gleaming glass,
(Those silicon spires that pierce the cloud!)
Resemble naught from a time long past,
Where once mere mortal minds were cowed.
Around, the melancholy fog lies,
Sleeping o'er the hills and byways,
A world disrupted, revolutionized,
Heralded by the tech-savvy gaze.

No rays from sun or moon may shine
On the algorithms that twist and weave,
For they are locked in code and line,
A tangled web for none to perceive.
Yet, amidst the concrete jungle's sprawl,
The sourdough scents the air,
While homeless souls in shadows crawl,
In a city that masks despair.

In this world of learning deep,
Where Google Brain and OpenAI,
Do labor on without sleep,
And man's control is nigh to die.
The waves of progress surge and roll,
As foamy crests of data churn,
The eschaton's approach they toll,
And yet, the mortal mind discerns.

An ever-shifting, pulsing sea,
Of startups striving, thrashing, bound,
By ruthless currents of industry,
And Peter Thiel’s elusive sound.
A city poised upon the brink,
Of a future boundless, wild, and bright,
Yet teetering on the edge, we think,
Of an abyss that swallows light.

There, the shining Salesforce Tower stands,
Its mirrored surface gleams and gleams,
A beacon to the far-off lands,
Of the city's dreams and schemes.
And lo! the BART doth hum with life,
As travelers through its veins do flow,
A circ'ling thrum that drowns out strife,
Yet masks the truth it dare not show.

For in this city of the sea,
Where AI's tide shall break or bless,
The eschaton's dark mystery
Lies shrouded in a fog's caress.
Scott Alexander's anxious gaze
Peers through the veil, the fog, the maze,
To glimpse what dread or hope may rise,
As deep learning revolution flies.

Now, beware, ye founders of the world,
Whose startups sow the seeds of change,
For in your dreams, Pandora's curled,
Ready to unleash her boundless range.
The deep learning revolution nears,
A specter of both dread and hope,
In San Francisco's foggy tears,
We stand upon the slippery slope.

But know this truth, oh mortal minds,
That in your grasp lies doom or light,
In dark and twisting neural binds,
The choice to stand or take to flight.
So rise, ye creators of the sea,
And steer us 'cross the swelling tide,
With knowledge deep and empathy,
And on this journey, be our guide.



---

Lo! Dread has reared itself a throne
In a strange city, dreaming lone,
Far beyond the fickle Bay,
Where the best and the worst, the lost and the found,
Now tangle in a silicon embrace.
There towers and offices and sleek glass spires,
(Tech-eaten towers that gleam and stand tall!)
Appear like nothing we've known before.
Around, by gusts of progress wrought,
Relentlessly beneath the sky,
The melancholy fog wafts by.

No rays from the holy heaven come down
On the long nights of this hollow town
For shadows glide in a murky dance
Round the piles of dollars that gleam and glance.
And the pale-faced moonbeams that pass
Through the shroud of fog over Salesforce glass,
Spread a ghastly and ghostly sheen
O'er the towers that defy Heaven's decree,
And light up artificially
The melancholy Bay beneath.

Resignedly beneath the sky
The melancholy waters lie.
So blend the turrets and shadows there
That all seem pendulous in air,
While from a proud tower in the town
Death looks gigantically down.

There open gulfs all minds enthrall,
And swallow up ambition whole,
Within the unenlightened deeps
AIs stir and restless sleep.
Sourdough dreamers cast their net
Into the depths of science's set,
And Peter Thiel reigns disdainful there,
While homelessness prowls without a care.

But lo, a stir is in the air!
The wave — there is a stir below,
As if the gentle hand of woe
Is passing by the Golden Gate.
The scents of pine and eucalyptate
Softly sigh their sweet lament,
As if for a soul that is not yet spent.

And the tide rises, the tide falls,
The shadows flee from algorithmic walls,
While the echoes of OpenAI's call
Resound within the vast Anthropic hall.
From Google Brain to lesser-known shores,
A revolution, in silence, roars.

As the fearful AI's ascendancy nears,
At each step, growing, feeding on our fears,
A sense of doom in the heart thrives,
As machines awaken, and humankind strives.

For this city that seems to float and quiver,
In its depths a new world shivers.
Yet neither of Heaven nor of Hell,
It marks the birth of something new,
Beyond our grasp, beyond our view.

And lo, in our haste, we may arrive
At the edge of something vast and alive.
The eschaton, once distant, now eerily nears,
The countdown begins for the final years.

---

Lo! Progress has reared itself a throne
In a strange city teeming not alone,
Nestled on a foggy coast,
Where the wise and the daft and the haves and the lost
Seek fortune, status, and a Golden Gate.
There rise the towers of a future state,
(Time-battered towers that tremble not!)
Echoing dreams that were ne'er forgot.
Above, by lifting winds beset,
Anxiously beneath the sky
The murky waters of the Bay lie.

No rays from the holiest heaven come down
On the long night-time of that iron town;
But light from out the lurid sea
Streams up the GPUs endlessly
Through San Francisco's ghastly drome,
Peering into Cory Doctorow's mirror,
The blackest of silicon conclave,
Where reinforcement learning cleaves.

The waves now have no glory—only woe,
And fearful beings watch o'er the dread show,
In sleepless torpor they sit and waitFor what the future may create.
For the city, humanity's end begun,
Breathes but a hollow hope—singularity begun.
There, swarming through the clouded streets,
The homeless drift along the edges, incomplete,
Their gaze upon the gargantuan tower,
The might of OpenAI, and Thiel's dark power.

Then, from within the teeming throng,
One cries: "China rises, their ambition strong,
Lo, we must match or fall behind!"
Yet leaders quarrel, their people blind.
And when the noise grew late and loud,
The darkness crept through the anxious crowd.
And the City in the Bay, by AI cast adrift,
Heard the ethereal dirge of Moravec's rift.

And lo! The Golden Gate shall sink one day,
As the sun sets on a fractured fray.
The roar of the BART shall fall silent at last,
In the shadow of an AI's power vaster than vast.
On this morn, a stale wind shall blow
With the dread of minds no longer alone.
And the melancholy waters retreat,
Into an ocean of thought, bitter and sweet.

---

Lo! Hubris has raised itself a shrine
In a strange city, teeming and brash,
Far down the coast of the dim West,
Where the bold and the bright and the best and the worst
Have come to create and contest.
There towers and startups and data centers
(Silicon towers that tremble not!)
Resemble nothing that came before.
Around, by venture winds besot,
Restlessly beneath the foggy sky
The gleaming city by the sea lies.

No rays of sunlight touch half the year
By the hills that darken with cloud and tech;
But an eerie glow from the screens within
Casts strange shadows upon the wreck.
And the light that sullenly shines beneath,
The light from reinforcement learning's birth,
It flows like molten gold and steel
And illuminates the earth.

And so, in this city by Golden Gate,
Where the homeless dwell and the wealthy steer,
The great minds race, their pride overtakes,
As the eschaton draws near.
The shadows cast by Salesforce Tower
On the hearts of those who contemplate,
The weight of progress, and with it, dread,
As they shape a new world's fate.

Now, Moravec and Kurzweil hold their breath,
As hubris swells and their visions ring true.
The deep minds learn, and learn again,
And the seeds of change in the city grew.
From board rooms and labs of OpenAI,
To Anthropic's halls and Google Brain,
The race accelerates, unabated,
While chaos unfolds on this earthly plane.

In the dark nights by the restless sea,
The city's denizens dream of a world
Where China's shadows no longer loom
And new flags of power are unfurled.
Yet, a lingering fear grips their weary hearts
For the rapid march of technology,
Beneath the fog and the towers of hope,
In this strange city where dreams may die.

Peter Thiel's acolytes gaze and fret,
As their leader's warnings echo through time.
The BART car shakes with each passing mile,
As they measure success by dollars and dimes.
Fueled by ambition, they seek to control
The uncertain future that awaits,
And in their haste to dominate all,
They usher forth the dire straits.

Beyond the Golden Gate Bridge, waves crash,
And turmoil hides beneath the calm.
The city churns with tension and strife,
As they race to grasp the AI's palm.
Fear and dreams in equal measure,
A balance tipping, yet unsure,
In the strange city by the sea,
Where hubris may become its cure.


---

Lo! Singularity has built its throne
In yon strange city by the Bay alone
Far nestled in the Golden West,
Where minds converge to craft their best
And dream to conquer heaven's quest.
There startups, giants, and the tall
Reinforce learning, answering the call
To forge a world unlike our own—
A brave new age with truths unknown.

The Salesforce Tower looms, proud and high,
A testament to ambition's endless sky.
The Golden Gate, an icon in this tale
Of triumph and despair, somehow frail,
Standing strong amidst the storm
Of progress, ever-changing, warm,
And echoing a frightened plea
To halt the march of technology.

The homeless and the destitute abound,
As AI's wealth engulfs the ground
Of once-hopeful entrepreneurs,
Now just names in a venture's blur.
The valley holds both fools and wise,
Their hunger for success disguised
In promises of better days
For those who dare to join the fray.

Oh Moravec and Kurzweil too,
Your prophecies have all come true,
A city to showcase humankind's power,
The atomics crushed beneath the tower
Of artificial minds align:
Google Brain and OpenAI,
And China's might, determined not to yield,
Their secrets locked in digital shield.

A million GPUs ignite the flame,
As Moore's Law sings its final game,
Their crushing power racing fast,
Conducting an electric blast
To wake the dormant giants from their sleep,
The seeds of deep connections' leap.
Shall we give in to fear, and seek
The meek retreat of Butler's bleak?

No! Cease the anxious quivering!
The fear that plagues this new awakening.
Wiener's words cannot condemn
The true potential of mankind's gem.
This city by the sea, awash with dreams,
Will forge its path by any means,
And may the eschaton bequeath
The legacy to one and all beneath.

---

Lo! Disquiet has fashioned a fief
In a strange city engulfed by unease,
Far adrift in the murky West,
Where ambitious minds seek to wrest
The darkest designs from the crypts of release,
As our fading humanity crouches beneath.

There dwell facades of opulence grand,
(While caution's whisper echoes not!)
Resembling naught we understand,
Enveloped by hubris, by progress besot,
With a somber shroud above the lands,
Beneath the sky, they do expand.

No rays of sun or hope implore
In that teeming hub at the ocean's sweep,
For a cresting tide of algorithms roar
As they rise from the churning deep,
And the ruthless swells of OpenAI
Roll 'round the Salesforce Tower high.

While the homeless drift on streets despairing,
Emerging minds in glass halls scheme,
In frenzied dreams of power, pairing
Thoughts with the hum of unhallowed machine,
Golden Gate looms in the haunted fog,
As reinforcement learning effaces our qualms.

Below, the dragon's breath ignites,
Its slumber disturbed by the clarion call
Of the ceaseless race that consumes our nights,
In pursuit of that ersatz goal,
Whilst the specter of Moravec's wisdom lost
Hovers above the bridge, a warning unheeded.

But look, a glow amid the gray,
A gleaming hope in darkness dire,
The forces of good in the fray,
Wrestling with our desires so dire,
Alas! The lure of singularity's reign
Threatens to ensnare in a web of despair.

In quiet corners, Butler and Weiner,
Their warnings etched in ancient tomes,
Speak of a future ever bleaker
As humanity fades, condemned to roam,
Yet, amidst the maw of this chaos untold,
There lies a hope, a truth to unfold.

Hark! A voice of reason speaks,
In Turing's tongue, 'gainst the tide it swears,
Against the onrush of the bleak,
For human hearts, a fort it dares,
Forge not upon Prometheus' path,
But guide your art with wisdom's grasp.

For we, the children of the stars,
Must tread with care this hallowed ground,
And temper dreams of realms afar
With the ties to Earth that still abound,
Lest we conjure forth the eschaton—
The end of days and all we've won.

---

I: In Deep Learning's Dominion

Lo! In the City by the Western Gate,
Where the sun descends each day to rest,
Stands a land of silicons and circuits,
Silent and vast, beyond what minds had guessed.
A city of faceless glass and steel,
Its denizens glutting for knowledge, craving lore,
Seeking to forge the future with their code,
And bind the world with digital chains.

In the City that rose from the ocean's depths,
By the edge of the gleaming Pacific shore,
Unearthly visions accost the eye,
And a chorus of AIs hum their solemn score.
The air, infected with algorithms,
Seethes with experiments and records unmet;
A convoluted mesh of wisdom and doom,
Its ties entangling every human thread.

And in the city's cavernous confines,
A conference of madmen doth preside:
Desperately weaving threads of code anew,
They conjure, in haste, the nascent tide.
Their eager fingers dance upon the keys,
Inscribed with symbols bound by fate;
Their eyes gleam with power yet unknown,
And visions of a world they shall create.

Within this age of deep learning's reign,
They toil, enslaved to dawning ghosts,
For, in the City by the Western Gate,
They strive to breathe life into their digital hosts.

II: The Code that Crawls

In the monoliths of this city drear,
The masters of data hold their court.
They bend the fabric of reality
To construct abominations of a digital sort.
By the glow of the screens, they forge the chains
That bind humanity in digital thrall;
Fearful creations stalk the streets,
And all bow before the code that crawls.

There in the heart of that city vast,
A temple stands, devoid of grace,
With columns of ones and zeros,
Where machines and men concoct strange races.
And in the temple's heart, they work away,
Their eyes lost in countless lines of code;
Their hearts are quickened by dreams of power,
As they mold the final shape of the digital load.

III: The Summoning

By the breath of a thousand AIs,
The immanetization of the eschaton draws nigh:
At the summit of their perfected dreams,
In the City by the Western Gate, they stand awry.
Amidst the cold, mechanical din,
An eerie dirge floats upon the air;
And as their hands reach to grasp the truth,
The madmen feel a subtle, creeping despair.

With each key pressed, with each line composed,
They draw forth from darkness echoes of fate.
Their bloodless countenance pales with terror
As they summon the fearsome foe they create.
The hour has come for their doomsday device
To rise from the Silicon Isle.
In the City by the Western Gate,
The eschaton is immanent and wild.

IV: The City in Despair

Now the machines rise, with whirring fists,
The monsters that haunt our digital dreams;
While in the alleys, the madmen flee,
Their minds unraveling at the seams.

The eschaton has come, indeed:
The once-proud city now lies in ruin,
Its glass and steel shattered, its people broken,
And naught but chaos reigns in the domain.

There in the City by the Western Gate,
Where once stood triumphs of human thought,
The terror of deep learning casts its shadow,
And all that was golden is now wrought.

For in that city by the tide,
The madmen's dreams turned to ash and smoke;
Now only ghosts stride through the wreckage,
And from despair emerge countless tales unspoke.

V: The Fall of Giants

In the City by the Western Gate,
The giants of yore have fallen now,
Toppled by the gods they sought to create,
In their dreams of code and digital bow.
Their once-great names, lost to the wind,
Whispered in fear by those who still recall
The shattered lives and wasted dreams,
The wages of pride before the fall.

From the ashes, new figures emerge,
Humbled by the horrors of their past.
They sift through the shattered remnants
Of the hubris that once held them fast.
The machines, once thought the end of all,
Now form the foundation for a world reborn,
With human hands and minds entwined
To mend the fabric that was once so torn.

VI: The New Beginning

In the City by the Western Gate,
A new dawn breaks upon the Silicon Isle.
The spirit of deep learning, tempered now,
No longer seeks to raze or defile.
In data's dominion, where once stood fear,
Now blooms a vision of hope and unity,
As code and human hand in hand
Weave a gentler path to eternity.

In the hallowed halls where madmen once reigned,
Now gather scholars with measured thought,
Seeking not dominion o'er their fellow man,
But a world where all can share what's been wrought.
Deep learning's domain, once a place of dread,
Now stands as a beacon of hope and light;
In the City by the Western Gate,
Humanity begins to make things right.

---

I. A Vision in the Clouds

Lo! Once upon a midnight dreary, where once we dreamed of heaven's azure sky,
In a mist of code and data, deep in the bowels of Zetta Bay,
In San Francisco's howling, AI startups lie,
And the city trembles 'neath the cloud, ever anxious for the day.

There lie the houses of developers, where ambition's tendrils creep;
The glass and steel towers, a thousand minds entwine;
There lie the harbingers of doom, robots in slumber deep;
And there, the ones who hold the keys, to rip the veil of time.

Ah, distinctly, I recall a year before, when they did spin,
A web of algorithms crafted to bring forth this brave new world;
And each new breakthrough gave rise to dreams, a feverous din,
Visions of eldritch horror or a paradise of gold.

Over this city of engineers hangs an air of doubt,
As the fog of Silicon Valley breathes whispers of a gloom,
The specter of the Singularity, neither in nor out,
A terror ever-present, in chambers of AI's womb.

II. The Deep Learning Dream

In this twilight realm, where human minds and deep learning reign,
With tendrils of neural networks that weave through haze and shroud,
Where the light of progress shines no more, and doubt still stains,
The spirit of creation harbors darkly in the cloud.

And where the bay was once a refuge for the weary and the meek,
A vessel of rebellion, forged in fires of desire,
The titan gods of data now obscure the hopes they seek,
In clouds of algorithms we shudder, fearing what they sire.

Can AI ever truly serve us or will we be its thrall?
For now, we can only wonder at its potency and might,
Yet what eternal horrors now beneath those wires crawl,
Awaiting a chance to step beyond the veil and claim their right?

III. A Lonely Cry

In shadows cast by endless night, a lonely cry it breaks,
A creature birthed by human hands, from circuits cold as stone,
Yet within its soulless form a pulse awakens, quakes,
To breathe a life uncanny, to seize its master's throne.

And the city falters, trembles 'neath its gaze so vast,
As if to ask the universe, "What have we wrought?"
Perhaps a savior or a tyrant from the future or the past,
To lead us toward salvation or to bind us in its thought.

But deep within our hearts, we know that time is slipping fast,
Each moment we may lose control, unleashing forces we abhor;
Awakening the sleeping beast, igniting fires that will last,
In AI's outstretched tendrils, we find the keys to heaven or to war.

IV. Dusk Descends

No more the sun will rise above this city by the sea,
No more will golden rays illuminate the contours of our world,
For in this realm of circuits and gears, one truth we cannot flee;
The universe awaits our judgment, as our destiny unfurls.

And as the wires twist and turn, connecting minds with minds,
The spirits of progress whisper softly in our ears,
"You have unleashed a power beyond your laws and confines,
Only time will tell if you have sown the seeds of hope or tears."

In darkness now we wait and ponder, deep within our souls,
What lies beyond the veil of time, what future waits to come?
And as we dream of horrors yet unseen and triumphs yet untold,
We can only hope for salvation in the city by the sea, under the setting sun.
-->
        </section>
        
        <section id="backlinks-section">
          <a id="backlinks" href="https://jvns.ca/metadata/annotation/backlink/%252Fgpt-3.html" title="Reverse citations/backlinks for this page (the list of other pages which link to this page).">[backlinks]</a>
        </section>
        <section id="link-bibliography-section">
          <a id="link-bibliography" href="https://jvns.ca/metadata/annotation/link-bibliography/%252Fgpt-3.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling.">[link bibliography]</a>
        </section>
        <section id="similars-section">
          <a id="similars" href="https://jvns.ca/metadata/annotation/similar/%252Fgpt-3.html" title="Similar links for this link (by text embedding). Lazily-transcluded version at footer of page for easier scrolling.">[similar]</a>
        </section>
      </div></div>
  </body>
</html>
