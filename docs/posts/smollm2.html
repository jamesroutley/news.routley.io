<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2024/Nov/2/smollm2/">Original</a>
    <h1>SmolLM2</h1>
    
    <div id="readability-page-1" class="page"><div>



<p><strong><a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct">SmolLM2</a></strong> (<a href="https://twitter.com/LoubnaBenAllal1/status/1852055582494294414" title="@LoubnaBenAllal1">via</a>) New from <a href="https://loubnabnl.github.io/">Loubna Ben Allal</a> and her research team at Hugging Face:</p>
<blockquote>
<p>SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. [...]</p>
<p>It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon.</p>
</blockquote>
<p>The model weights are released under an Apache 2 license. I&#39;ve been trying these out using my <a href="https://github.com/simonw/llm-gguf">llm-gguf</a> plugin for <a href="https://llm.datasette.io/">LLM</a> and my first impressions are really positive.</p>
<p>Here&#39;s a recipe to run a 1.7GB Q8 quantized model <a href="https://huggingface.co/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF">from lmstudio-community</a>:</p>
<pre><code>llm install llm-gguf
llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf -a smol17
llm chat -m smol17
</code></pre>
<p><img alt="Animated terminal demo. My prompt is tell me about pelicans. The model responds: Sure, I&#39;d be happy to tell you about pelicans! Pelicans are a group of aquatic birds in the order Pelecaniformes, which also includes the cormorants, darters, and frigatebirds. They are found on all continents except Antarctica, and are known for their distinctive pouch-like bill. There are several species of pelicans. The most common species is the Brown Pelican, which is found in the Americas. It&#39;s the only species that plunges into water from a significant height to catch fish and other prey, a behavior known as &#34;fish-grabbing.&#34;  Another common species is the American White Pelican, which can be found in both the Americas and Eurasia. It has a white plumage and a large, bright pink bill, and feeds on fish in lakes, rivers, and coastal wetlands.  Pelicans are generally medium-sized birds, but the Brown Pelican is the largest, with an average height of around 26-30 inches. Their bills can be as long as 11 inches!  Below the terminal you can see Activity Monitor showing 378% CPU usage for the Python process" src="https://static.simonwillison.net/static/2024/smol-demo.gif"/></p>
<p>Or at the other end of the scale, here&#39;s how to run the 138MB <a href="https://huggingface.co/lmstudio-community/SmolLM2-135M-Instruct-GGUF">Q8 quantized 135M model</a>:</p>
<pre><code>llm gguf download-model https://huggingface.co/lmstudio-community/SmolLM2-135M-Instruct-GGUF/resolve/main/SmolLM2-135M-Instruct-Q8_0.gguf&#39; -a smol135m
llm chat -m smol135m
</code></pre>
<p>The blog entry to accompany SmolLM2 should be coming soon, but in the meantime here&#39;s the entry from July introducing the first version: <a href="https://huggingface.co/blog/smollm"> SmolLM - blazingly fast and remarkably powerful </a>.</p>



</div></div>
  </body>
</html>
