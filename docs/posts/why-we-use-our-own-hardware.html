<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.fastmail.com/blog/why-we-use-our-own-hardware/">Original</a>
    <h1>Why we use our own hardware</h1>
    
    <div id="readability-page-1" class="page"><div data-pagefind-body="" data-cms-edit="content"> <p>This is the twenty-second post in the <a href="https://www.fastmail.com/blog/fastmail-advent-2024/">Fastmail Advent 2024</a> series. The previous post was <a href="https://www.fastmail.com/blog/fastmail-in-a-box/">Dec 21: Fastmail In A Box</a>. Check back tomorrow for another post.</p> <h2 id="why-we-use-our-own-hardware" tabindex="-1">Why we use our own hardware</h2> <p>There has recently been talk of <a href="https://www.google.com/search?q=Cloud+Repatriation" target="_blank" rel="noopener">cloud repatriation</a> where companies are moving from the cloud to on premises, with some particularly <a href="https://basecamp.com/cloud-exit" target="_blank" rel="noopener">noisy examples</a>.</p> <p>Fastmail has a long history of using our <a href="https://www.fastmail.com/blog/standalone-mail-servers/" target="_blank" rel="noopener">own</a> <a href="https://www.fastmail.com/blog/getting-the-most-out-of-hardware/" target="_blank" rel="noopener">hardware</a>. We have over two decades of experience running and optimising our systems to use our own <a href="https://en.wikipedia.org/wiki/Bare-metal_server" target="_blank" rel="noopener">bare metal</a> servers efficiently.</p> <p>We get way better cost optimisation compared to moving everything to the cloud because:</p> <ol> <li>We understand our short, medium and long term usage patterns, requirements and growth very well. This means we can plan our hardware purchases ahead of time and don’t need the fast dynamic scaling that cloud provides.</li> <li>We have in house operations experience installing, configuring and running our own hardware and networking. These are skills we’ve had to maintain and grow in house since we’ve been doing this for 25 years.</li> <li>We are able to use our hardware for long periods. We find our hardware can provide useful life for anywhere from 5-10 years depending on what it is and when in the global technology cycle it was bought, meaning we can amortise and depreciate the cost of any hardware over many years.</li> </ol> <p>Yes, that means we have to do more ourselves, including planning, choosing, buying, installing, etc, but the tradeoff for us has and we believe continues to be significantly worth it.</p> <h2 id="hardware-over-the-years" tabindex="-1">Hardware over the years</h2> <p>Of course over the 25 years we’ve been running Fastmail we’ve been through a number of hardware changes. For many years, our IMAP server storage platform was a combination of <a href="https://www.urbandictionary.com/define.php?term=Spinning%20Rust" target="_blank" rel="noopener">spinning rust</a> drives and <a href="https://www.areca.com.tw/" target="_blank" rel="noopener">ARECA RAID controllers</a>. We tended to use faster 15k RPM SAS drives in <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1" target="_blank" rel="noopener">RAID1</a> for our hot meta data, and 7.2k RPM SATA drives in <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_6" target="_blank" rel="noopener">RAID6</a> for our main email blob data.</p> <p>In fact it was slightly more complex than this. Email blobs were written to the fast RAID1 SAS volumes on delivery, but then a separate archiving process would move them to the SATA volumes at low server activity times. Support for all of this had been added into <a href="https://www.cyrusimap.org" target="_blank" rel="noopener">cyrus</a> and our tooling over the years in the form of separate “meta”, “data” and <a href="https://www.cyrusimap.org/3.8/imap/reference/admin/locations/archive-partitions.html" target="_blank" rel="noopener">“archive”</a> partitions.</p> <h2 id="moving-to-nv-me-ss-ds" tabindex="-1">Moving to NVMe SSDs</h2> <p>A few years ago however we made our biggest hardware upgrade ever. We moved all our email servers to a new <a href="https://www.supermicro.com/en/aplus/system/2u/2113/as-2113s-wn24rt.cfm" target="_blank" rel="noopener">2U AMD platform</a> with pure <a href="https://www.solidigm.com/products/data-center.html" target="_blank" rel="noopener">NVMe SSDs</a>. The density increase (24 x 2.5&#34; NVMe drives vs 12 x 3.5&#34; SATA drives per 2U) and performance increase was enormous. We found that these new servers performed even better than our initial expectations.</p> <p>At the time we upgraded however NVMe RAID controllers weren’t widely available. So we had to decide on how to handle redundancy. We considered a RAID-less setup using raw SSDs drives on each machine with synchronous application level replication to other machines, but the software changes required were going to be more complex than expected.</p> <p>We were looking at using classic Linux <a href="https://en.wikipedia.org/wiki/Mdadm" target="_blank" rel="noopener">mdadm RAID</a>, but the <a href="https://en.wikipedia.org/wiki/RAID#Atomicity" target="_blank" rel="noopener">write hole</a> was a concern and the <a href="https://docs.kernel.org/driver-api/md/raid5-cache.html" target="_blank" rel="noopener">write cache</a> didn’t seem well tested at the time.</p> <p>We decided to have a look at <a href="https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/" target="_blank" rel="noopener">ZFS</a> and at least test it out.</p> <p>Despite some of the cyrus on disk database structures being fairly hostile to <a href="https://en.wikipedia.org/wiki/ZFS#Copy-on-write_transactional_model" target="_blank" rel="noopener">ZFS Copy-on-write</a> semantics, they were still incredibly fast at all the IO we threw at them. And there were some other wins as well.</p> <h2 id="zfs-compression-and-tuning" tabindex="-1">ZFS compression and tuning</h2> <p>When we rolled out ZFS for our email servers we also enabled <a href="https://freebsdfoundation.org/wp-content/uploads/2021/05/Zstandard-Compression-in-OpenZFS.pdf" target="_blank" rel="noopener">transparent Zstandard compression</a>. This has worked very well for us, saving about 40% space on all our email data.</p> <p>We’ve also recently done some additional calculations to see if we could tune some of the parameters better. We sampled 1 million emails at random and calculated how many blocks would be required to store those emails uncompressed, and then with <a href="https://klarasystems.com/articles/tuning-recordsize-in-openzfs/" target="_blank" rel="noopener">ZFS record sizes</a> of 32k, 128k or 512k and zstd-3 or zstd-9 compression options. Although ZFS <a href="https://en.wikipedia.org/wiki/ZFS#ZFS&#39;s_approach:_RAID-Z_and_mirroring" target="_blank" rel="noopener">RAIDz2</a> seems conceptually similar to classic RAID6, the way it <a href="https://ibug.io/blog/2023/10/zfs-block-size/" target="_blank" rel="noopener">actually stores blocks of data</a> is quite different and so you have to take into account volblocksize, how files are split into logical recordsize blocks, and number of drives when doing calculations.</p> <pre><code>               Emails: 1,026,000
           Raw blocks: 34,140,142
 32k &amp; zstd-3, blocks: 23,004,447 = 32.6% saving
 32k &amp; zstd-9, blocks: 22,721,178 = 33.4% saving
128k &amp; zstd-3, blocks: 20,512,759 = 39.9% saving
128k &amp; zstd-9, blocks: 20,261,445 = 40.7% saving
512k &amp; zstd-3, blocks: 19,917,418 = 41.7% saving
512k &amp; zstd-9, blocks: 19,666,970 = 42.4% saving
</code></pre> <p>This showed that the defaults of 128k record size and zstd-3 were already pretty good. Moving to a record size of 512k improved compression over 128k by a bit over 4%. Given all meta data is cached separately, this seems a worthwhile improvement with no significant downside. Moving to zstd-9 improved compression over zstd-3 by about 2%. Given the CPU cost of compression at zstd-9 is about 4x zstd-3, even though emails are immutable and tend to be kept for a long time, we’ve decided not to implement this change.</p> <h2 id="zfs-encryption" tabindex="-1">ZFS encryption</h2> <p>We always enable <a href="https://en.wikipedia.org/wiki/Data_at_rest#Encryption" target="_blank" rel="noopener">encryption at rest</a> on all of our drives. This was usually done with <a href="https://en.wikipedia.org/wiki/Linux_Unified_Key_Setup" target="_blank" rel="noopener">LUKS</a>. But with ZFS this was <a href="https://arstechnica.com/gadgets/2021/06/a-quick-start-guide-to-openzfs-native-encryption/" target="_blank" rel="noopener">built in</a>. Again, this reduces overall system complexity.</p> <h2 id="going-all-in-on-zfs" tabindex="-1">Going all in on ZFS</h2> <p>So after the success of our initial testing, we decided to go all in on ZFS for all our large data storage needs. We’ve now been using ZFS for all our email servers for over 3 years and have been very happy with it. We’ve also moved over all our database, log and backup servers to using ZFS on NVMe SSDs as well with equally good results.</p> <h2 id="ssd-lifetimes" tabindex="-1">SSD lifetimes</h2> <p>The flash memory in SSDs has a finite life and <a href="https://en.wikipedia.org/wiki/Flash_memory#Write_endurance" target="_blank" rel="noopener">finite number of times it can be written to</a>. SSDs employ increasingly complex <a href="https://en.wikipedia.org/wiki/Wear_leveling" target="_blank" rel="noopener">wear levelling</a> algorithms to spread out writes and increase drive lifetime. You’ll often see the quoted endurance of an enterprise SSD as either an absolute figure of “Lifetime Writes”/“Total bytes written” like 65 PBW (petabytes written) or a relative per-day figure of “Drive writes per day” like 0.3, which you can convert to lifetime figure by multiplying by the drive size and the drive expected lifetime which is often assumed to be 5 years.</p> <p>Although we could calculate IO rates for existing <a href="https://en.wikipedia.org/wiki/Hard_disk_drive" target="_blank" rel="noopener">HDD</a> systems, we were making a significant number of changes moving to the new systems. Switching to a COW filesystem like ZFS, removing the special casing meta/data/archive partitions, and the massive latency reduction and performance improvements mean that things that might have taken extra time previously and ended up batching IO together, are now so fast it actually causes additional separated IO actions.</p> <p>So one big unknown question we had was how fast would the SSDs wear in our actual production environment? After several years, we now have some clear data. From one server at random but this is fairly consistent across the fleet of our oldest servers:</p> <pre><code># smartctl -a /dev/nvme14
...
Percentage Used:                    4%
</code></pre> <p>At this rate, we’ll replace these drives due to increased drive sizes, or entirely new physical drive formats (such <a href="https://www.snia.org/forums/cmsi/knowledge/formfactors" target="_blank" rel="noopener">E3.S</a> which appears to finally be gaining traction) long before they get close to their rated write capacity.</p> <p>We’ve also anecdotally found SSDs just to be much more reliable compared to HDDs for us. Although we’ve only ever used <a href="https://www.micron.com/products/storage/ssd/data-center-ssd/" target="_blank" rel="noopener">datacenter</a> <a href="https://www.solidigm.com/products/data-center.html" target="_blank" rel="noopener">class</a> SSDs and <a href="https://www.seagate.com/www-content/datasheets/pdfs/exos-7-e8-data-sheet-DS1957-1-1709US-en_US.pdf" target="_blank" rel="noopener">HDDs</a> failures and replacements every few weeks were a regular occurrence on the old fleet of servers. Over the last 3+ years, we’ve only seen a couple of SSD failures in total across the entire upgraded fleet of servers. This is easily less than one tenth the failure rate we used to have with HDDs.</p> <h2 id="storage-cost-calculation" tabindex="-1">Storage cost calculation</h2> <p>After converting all our email storage to NVMe SSDs, we were recently looking at our data backup solution. At the time it consisted of a number of older 2U servers with 12 x 3.5&#34; SATA drive bays and we decided to do some cost calculations on:</p> <ol> <li>Move to cloud storage.</li> <li>Upgrade the HD drives in existing servers.</li> <li>Upgrade to SSD NVMe machines.</li> </ol> <h3 id="1-cloud-storage" tabindex="-1">1. Cloud storage:</h3> <p>Looking at various providers, the per TB per month price, and then a yearly price for 1000Tb/1Pb (prices as at Dec 2024)</p> <ul> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3</a> - $21 -&gt; $252,000/y</li> <li><a href="https://developers.cloudflare.com/r2/pricing/" target="_blank" rel="noopener">Cloudflare R2</a> - $15 -&gt; $180,000/y</li> <li><a href="https://wasabi.com/pricing" target="_blank" rel="noopener">Wasabi</a> - $6.99 -&gt; $83,880/y</li> <li><a href="https://www.backblaze.com/cloud-storage/pricing" target="_blank" rel="noopener">Backblaze B2</a> - $6 -&gt; $72,000/y</li> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3 Glacier Instant Retrieval</a> - $4 -&gt; $48,000/y</li> <li><a href="https://aws.amazon.com/s3/pricing/" target="_blank" rel="noopener">Amazon S3 Glacier Deep Archive (12 hour retrieval time)</a> - $0.99 -&gt; $11,880/y</li> </ul> <p>Some of these (e.g. Amazon) have potentially significant bandwidth fees as well.</p> <p>It’s interesting seeing the spread of prices here. Some also have a bunch of weird edge cases as well. e.g. “The S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes require an additional 32 KB of data per object”. Given the large retrieval time and extra overhead per-object, you’d probably want to store small incremental backups in regular S3, then when you’ve gathered enough, build a biggish object to push down to Glacier. This adds implementation complexity.</p> <ul> <li><em>Pros</em>: No limit to amount we store. Assuming we use S3 compatible API, can choose between multiple providers.</li> <li><em>Cons</em>: Implementation cost of converting existing backup system that assumes local POSIX files to S3 style object API is uncertain and possibly significant. Lowest cost options require extra careful consideration around implementation details and special limitations. Ongoing monthly cost that will only increase as amount of data we store increases. Uncertain if prices will go down or not, or even go up. Possible significant bandwidth costs depending on provider.</li> </ul> <h3 id="2-upgrade-hd-ds" tabindex="-1">2. Upgrade HDDs</h3> <p><a href="https://www.seagate.com/au/en/products/enterprise-drives/exos-x/x24/" target="_blank" rel="noopener">Seagate Exos 24 HDs</a> are 3.5&#34; 24T HDDs. This would allow us to triple the storage on existing servers. Each HDD is about $500, so upgrading one 2U machine would be about $6,000 and have storage of 220T or so.</p> <ul> <li><em>Pros</em>: Reuses existing hardware we already have. Upgrades can be done a machine at a time. Fairly low price</li> <li><em>Cons</em>: Will existing units handle 24T drives? What’s the rebuild time on drive failure look like? It’s almost a day for 8T drives already, so possibly nearly a week for a failed 24T drive? Is there enough IO performance to handle daily backups at capacity?</li> </ul> <h3 id="3-upgrade-to-new-hardware" tabindex="-1">3. Upgrade to new hardware</h3> <p>As we know, SSDs are denser (2.5&#34; -&gt; 24 per 2U vs 3.5&#34; -&gt; 12 per 2U), more reliable, and now higher capacity - <a href="https://www.solidigm.com/products/data-center/d5/p5336.html#form=U.2%2015mm&amp;cap=61.44TB" target="_blank" rel="noopener">up to 61T per 2.5&#34; drive</a>. A single 2U server with 24 x 61T drives with 2 x 12 RAIDz2 = 1220T. Each drive is <a href="https://www.newegg.com/solidigm-61-44tb-d5-p5336/p/N82E16820318031" target="_blank" rel="noopener">about $7k</a> right now, prices fluctuate. So all up 24 x $7k = $168k + ~$20k server =~ $190k for &gt; 1000T storage one-time cost.</p> <ul> <li><em>Pros</em>: <strong>Much</strong> higher sequential and random IO than HDDs will ever have. Price &lt; 1 year of standard S3 storage. Internal to our WAN, no bandwidth costs and very low latency. No new development required, existing backup system will just work. Consolidate on single 2U platform for all storage (cyrus, db, backups) and SSD for all storage. Significant space and power savings over existing HDD based servers</li> <li><em>Cons</em>: Greater up front cost. Still need to predict and buy more servers as backups grow.</li> </ul> <p>One thing you don’t see in this calculation is datacenter space, power, cooling, etc. The reason is that compared to the amortised yearly cost of a storage server like this, these are actually reasonably minimal these days, on the order of $3000/2U/year. Calculating person time is harder. We have a lot of home built automation systems that mean installing and running one more server has minimal marginal cost.</p> <h3 id="result" tabindex="-1">Result</h3> <p>We ended up going with the the new 2U servers option:</p> <p><picture><source type="image/webp" srcset="/assets/images/nvme-imap-servers-eeGMRYrXlR-375.webp 375w, /assets/images/nvme-imap-servers-eeGMRYrXlR-750.webp 750w, /assets/images/nvme-imap-servers-eeGMRYrXlR-1500.webp 1500w" sizes="(max-width: 425px) 375px, 750px"/><img alt="NVME IMAP Servers" loading="lazy" decoding="async" src="https://www.fastmail.com/assets/images/nvme-imap-servers-eeGMRYrXlR-375.png" width="1500" height="559" srcset="/assets/images/nvme-imap-servers-eeGMRYrXlR-375.png 375w, /assets/images/nvme-imap-servers-eeGMRYrXlR-750.png 750w, /assets/images/nvme-imap-servers-eeGMRYrXlR-1500.png 1500w" sizes="(max-width: 425px) 375px, 750px"/></picture></p> <ul> <li>The 2U AMD NVMe platform with ZFS is a platform we have experience with already</li> <li>SSDs are much more reliable and much higher IO compared to HDDs</li> <li>No uncertainty around super large HDDs, RAID controllers, rebuild times, shuffling data around, etc.</li> <li>Significant space and power saving over existing HDD based servers</li> <li>No new development required, can use existing backup system and code</li> <li>Long expected hardware lifetime, controlled upfront cost, can depreciate hardware cost</li> </ul> <p>So far this has worked out very well. The machines have bonded 25Gbps networks and when filling them from scratch we were able to saturate the network links streaming around 5Gbytes/second of data from our IMAP servers, compressing and writing it all down to a RAIDz2 zstd-3 compressed ZFS dataset.</p> <h2 id="conclusion" tabindex="-1">Conclusion</h2> <p>Running your own hardware might not be for everyone and has distinct tradeoffs. But when you have the experience and the knowledge of how you expect to scale, the cost improvements can be significant.</p> </div></div>
  </body>
</html>
