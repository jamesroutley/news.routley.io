<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/">Original</a>
    <h1>Learnings from 5 years of tech startup code audits</h1>
    
    <div id="readability-page-1" class="page"><div id="site-content">
<div>
<div>
<article id="post-410">

<div>
<p><img alt="" src="https://secure.gravatar.com/avatar/e609f4fe003376eeaa8a4a3b97d432d3?s=96&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/e609f4fe003376eeaa8a4a3b97d432d3?s=192&amp;d=mm&amp;r=g 2x" height="96" width="96" loading="lazy"/></p><p>
VP of Engineering @ FiscalNote | Erstwhile Head of Security @ FiscalNote | ex-PKC co-founder | princeton tiger &#39;11 | writes on engineering, management, and security. Opinions, my own.
</p>
</div>
<div>
<p>While I was at PKC, our team did upwards of twenty code audits, many of them for startups that were just around their Series A or B (that was usually when they had cash and realized that it’d be good to take a deeper look at their security, after the do-or-die focus on product market fit).</p>
<p>It was fascinating work – we dove deep on a great cross-section of stacks and architectures, across a wide variety of domains. We found all sorts of security issues, ranging from catastrophic to just plain interesting. And we also had a chance to chat with senior engineering leadership and CTOs more generally about the engineering and product challenges they were facing as they were just starting to scale.</p>
<p>It’s <em>also</em> been fascinating to see which of those startups have done well and which have faded, now that some of those audits are 7-8 years ago.</p>
<p>I want to share some of the more surprising things I’ve internalized from these observations, roughly ordered from most general to most security specific.</p>
<ol><li><strong>You don’t need hundreds of engineers to build a great product.</strong> <a href="https://kenkantzer.com/ou-dont-need-hundreds-of-engineers/" data-wpel-link="internal">I wrote a longer piece about this</a>, but essentially, despite the general stage of startup we audited being pretty similar, the engineering team sizes varied <em>a lot</em>. Surprisingly, sometimes the most impressive products with the broadest scope of features were built by the smaller teams. And it was these same “small but mighty” teams that, years later, are crushing their markets.</li><li><strong>Simple Outperformed Smart</strong>. As a self-admitted elitist, it pains me to say this, but it’s true: the startups we audited that are now doing the best usually had an almost brazenly ‘Keep It Simple’ approach to engineering. Cleverness for cleverness sake was abhorred. On the flip side, the companies where we were like ”woah, these folks are smart as hell” for the most part kind of faded. Generally, the major foot-gun (<a href="https://kenkantzer.com/5-software-engineering-foot-guns/" data-wpel-link="internal">which I talk about more in a previous post on foot-guns</a>) that got a lot of places in trouble was the premature move to microservices, architectures that relied on distributed computing, and messaging-heavy designs.</li><li><strong>Our highest impact findings would always come within the first and last few hours of the audit.</strong> If you think about it, this makes sense: in the first few hours of the audit, you find the lowest-hanging fruit. Things that stick out like a sore thumb just from grepping the code and testing some basic functionality. During the last few hours, you’ve fully contexted in to the new codebase, and things begin to click.</li><li><strong>Writing secure software has gotten remarkably easier in the last 10 years.</strong> I don’t have statistically sound evidence to back this up, but it seems like code written before around 2012 tended to have a lot more vulnerabilities per <a rel="noreferrer noopener external" href="https://en.wikipedia.org/wiki/Source_lines_of_code" target="_blank" data-wpel-link="external">SLOC</a> than code written after 2012 (we started auditing in 2014). Maybe it was the Web 2.0 frameworks, or increased security awareness amongst devs. Whatever it was, I think this means that security really has improved on a fundamental basis in terms of the tools and defaults software engineers now have available.</li><li><strong>All the really bad security vulnerabilities were obvious</strong>. Probably a fifth of the code audits we did, we’d find The Big One – a vulnerability so bad that we’d call up our clients and tell them to fix it immediately. I can’t remember a single case where that vulnerability was very clever. In fact, that’s part of what made the worst vulnerabilities bad — we were<em> </em>worried primarily <em>because</em> they’d be easy to find and exploit. “Discoverability” has been a component of impact analysis for a while, so this isn’t new. But I do think that discoverability should be much more heavily weighted. Discoverability is everything, when it comes to actual exposure. Hackers are lazy and they look for the lowest-hanging fruit. They won’t care about finageling even a very severe heap-spray vulnerability if they can <a rel="noreferrer noopener external" href="https://hackerone.com/reports/173551" target="_blank" data-wpel-link="external">reset a user’s password because the reset token was in the response (as Uber found out circa 2016)</a>. The counterargument to this is that heavily weighting discoverability perpetuates ”<a rel="noreferrer noopener external" href="https://www.schneier.com/blog/archives/2008/06/security_throug_1.html" target="_blank" data-wpel-link="external">Security by Obscurity</a>,” since it relies so heavily on guessing what an attacker can or should know. But again, personal experience strongly suggests that in practice, discoverability is a great predictor of actual exploitation.</li><li><strong>Secure-by-default features in frameworks and infrastructure massively improved security</strong>. <a href="https://kenkantzer.com/the-unreasonable-effectiveness-of-secure-by-default/" data-wpel-link="internal">I wrote a longer piece about this too</a>, but essentially, things like React default escaping all HTML to avoid cross-site scripting, and serverless stacks taking configuration of operating system and web server out of the hands of developers, <em>dramatically</em> improved the security of the companies that used them. Compare this to our PHP audits, which were riddled with XSS. These newer stacks/frameworks are not impenetrable, but their attackable surface area is smaller in precisely the places that make a massive difference in practice.</li><li><strong>Monorepos are easier to audit</strong>. Speaking from the perspective of security researcher ergonomics, it was easier to audit a monorepo than a series of services split up into different code bases. There was no need to write wrapper scripts around the various tools we had. It was easier to determine if a given piece of code was used elsewhere. And best of all, there was no need to worry about a common library version being different on another repo.</li><li><strong>You could easily spend an entire audit going down the rabbit trail of vulnerable dependency libraries.</strong> It’s incredibly hard to tell if a given vulnerability in a dependency is exploitable. We as an industry are definitely underinvesting in securing foundational libraries, which is why things like Log4j were so impactful. Node and npm were absolutely terrifying in this regard—the dependency chains were just not auditable. It was a huge boon when GitHub released dependabot because we could for the most part just tell our clients to upgrade things in priority order. </li><li><strong>Never deserialize untrusted data</strong>. This happened the most in PHP, because for some reason, PHP developers love to serialize/deserialize objects instead of using JSON, but I’d say almost every case we saw where a server was deserializing a client object and parsing it led to a horrible exploit. For those of you who aren’t familiar, <a rel="noreferrer noopener external" href="https://portswigger.net/web-security/deserialization/exploiting" target="_blank" data-wpel-link="external">Portswigger</a><a href="https://portswigger.net/web-security/deserialization/exploiting" data-wpel-link="external" target="_blank" rel="external noopener noreferrer"> has a good breakdown</a> of what can go wrong (incidentally, focused on PHP. Coincidence?). In short, the common thread in all deserialization vulnerabilities is that giving a user the ability to manipulate an object that is subsequently used by the server is an <em>extremely powerful capability</em> <em>with a wide surface area</em>. It’s conceptually similar to both <a rel="noreferrer noopener external" href="https://portswigger.net/daily-swig/prototype-pollution-the-dangerous-and-underrated-vulnerability-impacting-javascript-applications" target="_blank" data-wpel-link="external">prototype pollution</a>, and user-generated HTML templates. The fix? It’s far better to allow a user to send a JSON object (it has so few possible data types), and to manually construct the object based on the fields in that object. It’s slightly more work, but well worth it!</li><li><strong>Business logic flaws were rare, but when we found one they tended to be epically bad.</strong> Think about it — flaws in business logic are <em>guaranteed</em> to affect the business. An interesting corollary is that even if your protocol is built to provide provably-secure properties, human error in the form of bad business logic is surprisingly common (you need look no further than the series of absolutely devastating exploits that take advantage of badly written smart contracts).</li><li><strong>Custom fuzzing was surprisingly effective</strong>. A couple years into our code auditing, I started requiring all our code audits to include making a custom fuzzers to test product APIs, authentication, etc. This is somewhat commonly done, and I stole this idea from Thomas Ptacek, which he alludes to in his <a rel="noreferrer noopener external" href="https://sockpuppet.org/blog/2015/03/06/the-hiring-post/" target="_blank" data-wpel-link="external">Hiring Post</a>. Before we did this, I actually thought it was a waste of time—I just always figured it was an example of misapplied engineering, and that audit hours were better spent reading code and trying out various hypothesis. But it turns out fuzzing was surprisingly effective and efficient in terms of hours spent, especially on the larger codebases.</li><li><strong>Acquisitions complicated security quite a bit</strong>. There were more code patterns to review, more AWS accounts to look at, more variety in SDLC tooling. And of course, usually the acquisition meant an entirely new language and/or framework with its own patterns in use.</li><li><strong>There was always at least one closet security enthusiast amongst the software engineers.</strong> It was always surprising who it was, and they almost always never knew it was them! As security skillsets get more software-skewed, there’s huge arbitrage here if these folks can be reliably identified.</li><li><strong>Quick turnarounds on fixing vulnerabilities usually correlated with general engineering operational excellence.</strong> The best cases were clients who asked us to just give them a constant feed of anything we found, and they’d fix it right away.</li><li><strong>Almost no one got JWT tokens and webhooks right on the first try.</strong> With webhooks, people almost always forgot to authenticate incoming requests (or the service they were using didn’t allow for authentication…which was pretty messed up!). This class of problem led to Josh, one of our researchers, to begin asking a series of questions that led to <a rel="noreferrer noopener external" href="https://youtu.be/m4BxIf9PUx0" target="_blank" data-wpel-link="external">a DefCON/Blackhat talk</a>. JWT is notoriously hard to get right, even if you’re using a library, and there were a lot of implementations that failed to properly expire tokens on logout, incorrectly checked the JWT for authenticity, or simply trusted it by default.</li><li><strong>There’s still a lot of MD5 in use out there, but it’s mostly false positives</strong>. It turns out MD5 is used for a lot of other things besides an (in)sufficiently collision-resistant password hash. For example because it’s so fast, it’s often used in automated testing to quickly generate a whole lot of pseudo-random GUIDs. In these cases, the insecure properties of MD5 don’t matter, despite what your static analysis tool may be screaming at you.</li></ol>

</div>



</article>
</div>
</div>

</div></div>
  </body>
</html>
