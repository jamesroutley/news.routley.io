<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinalderson.com/posts/are-we-really-repeating-the-telecoms-crash-with-ai-datacenters/">Original</a>
    <h1>Are we repeating the telecoms crash with AI datacenters?</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    
    <div>
        <p>I keep hearing the AI datacentre boom compared to the 2000s telecoms crash. The parallels seem obvious - billions in infrastructure spending, concerns about overbuilding, warnings of an imminent bubble. But when I actually ran the numbers, the fundamentals look completely different.</p>
<p>I&#39;m not here to predict whether there will or won&#39;t be a crash or correction. I just want to look at whether the comparison to telecoms actually holds up when you examine the history in a bit more detail.</p>
<h2>What Actually Happened in the Telecoms Crash</h2>
<p>Let me start with what the 2000s telecoms crash actually looked like, because the details matter. Firstly, there was massive capex - between 1995 and 2000 somewhere like <a href="https://ideas.ted.com/an-eye-opening-look-at-the-dot-com-bubble-of-2000-and-how-it-shapes-our-lives-today/">$2 trillion was spent laying 80-90 million miles of fiber</a>. Inflation adjusted, this is over $4trillion, or close to <strong>$1trillion/year</strong> in 2025 dollars.</p>
<p>By 2002 only <a href="https://www.wsj.com/articles/SB1032982764442483713">2.7% of this fibre was used</a>.</p>
<p>How did this happen? A catastrophic supply and demand miscalculation past the pure securities fraud involved in many of the companies. Telecom CEOs <a href="https://ideas.ted.com/an-eye-opening-look-at-the-dot-com-bubble-of-2000-and-how-it-shapes-our-lives-today/">claimed</a> internet traffic was doubling every 3-4 months.</p>
<p>But in reality, <a href="https://www-users.cse.umn.edu/~odlyzko/doc/oft.internet.growth.pdf">traffic was doubling roughly every 12 months</a>. That&#39;s a <strong>4x overestimate</strong> of demand growth, which compounds each year. This false assumption drove massive debt-financed overbuilding. If you overestimate 4x a year for 3 years, by the end of your scenario you are 256x out.</p>
<p><em>Even worse</em> for these companies, enormous strides were made on the optical transceivers, allowing the same fibre to carry 100,000x more traffic over the following decade. Just one example is WDM multiplexing, allowing multiple carriers to be multiplexed on the same physical fibre line. In 1995 state of the art was 4-8 carriers. By 2000, it was 128. This <em>alone</em> allowed a 64x increase in capacity with the same infrastructure. Combined with improvements in modulation techniques, error correction, and the bits per second each carrier could handle, the same physical fibre became exponentially more capable.</p>
<p>The key dynamic: supply improvements were exponential while demand was merely linear. While some physical infrastructure needed to be built, there was enormous overbuilding that could mostly be serviced by technology improvements on the same infrastructure.</p>
<h2>AI Infrastructure: A Different Story</h2>
<p>Unlike fibre optics in the 1990s, GPU performance per watt improvements are actually slowing down:</p>
<p><strong>2015-2020 Period:</strong></p>
<ul>
<li>Performance per watt improved significantly with major architectural changes</li>
<li>Process nodes jumped from ~20nm to 7nm (major efficiency gains)</li>
<li>Introduction of Tensor Cores and specialized AI hardware</li>
</ul>
<p><strong>2020-2025 Period:</strong></p>
<ul>
<li><a href="https://epoch.ai/data-insights/ml-hardware-energy-efficiency">ML hardware energy efficiency improves ~40% annually</a></li>
<li>Performance per watt improvements slowing compared to previous era</li>
<li>Process nodes: improvements slowed dramatically with EUV being a requirement at sub 5nm wavelengths.</li>
</ul>
<p>More tellingly, <a href="https://www.tweaktown.com/news/97059/nvidias-full-spec-blackwell-b200-ai-gpu-uses-1200w-of-power-up-from-700w-on-hopper-h100/index.html">GPU TDPs (power consumption) are rising dramatically</a>:</p>
<ul>
<li>V100 (2017): 300W</li>
<li>A100 (2020): 400W</li>
<li>H100 (2022): 700W</li>
<li>B200 (2024): 1000-1200W</li>
</ul>
<p>This is the opposite of what happened in telecoms. We&#39;re not seeing exponential efficiency gains that make existing infrastructure obsolete. Instead, we&#39;re seeing semiconductor physics hitting fundamental limits.</p>
<p>The B200 from NVidia also requires liquid cooling - which means most datacentres designed for air cooling need to be completely retrofitted.</p>
<h3>Demand Growth Is Actually Accelerating</h3>
<p>The telecoms crash happened partly because demand was overestimated by 4x. What does AI demand growth look like?</p>
<p><strong>Traditional LLM Usage:</strong> <a href="https://techcrunch.com/2025/07/21/chatgpt-users-send-2-5-billion-prompts-a-day/">ChatGPT averages 20+ prompts per user per day</a>. Extended conversations can reach 3,000-4,000 tokens cumulative, though many users treat it like Google - short &#34;searches&#34; with no follow-up, consuming surprisingly few tokens.</p>
<p><strong>Agent Usage (<a href="https://www.anthropic.com/engineering/multi-agent-research-system">Anthropic research</a>):</strong></p>
<ul>
<li>Basic agents: <strong>4x more tokens</strong> than chat</li>
<li>Multi-agent systems: <strong>15x more tokens</strong> than chat</li>
<li>Coding agents: <strong>150,000+ tokens per session</strong> (multiple sessions daily)</li>
</ul>
<p>We&#39;re looking at a fundamentally different demand curve - if anything, people are underestimating how much agents will consume. The shift from chat to agents represents a 10x-100x increase in token consumption per user.</p>
<p>We&#39;re not even there yet, and infrastructure is already maxed out, with AI infrastructure running at very high utilization rates. Major providers still experience peak-time capacity issues. The problem isn&#39;t unused infrastructure sitting idle; it&#39;s infrastructure struggling to meet current demand. One major hyperscaler told me they <em>still</em> have capacity issues at peak times causing free tier users to have high error rates.</p>
<h3>Datacenter CapEx: Evolution, Not Revolution</h3>
<p>Another important piece of context that gets missed:</p>
<p><strong>Pre-AI Growth (2018-2021):</strong></p>
<ul>
<li>Combined Amazon/Microsoft/Google capex: <a href="https://platformonomics.com/2019/02/follow-the-capex-cloud-table-stakes-2018-edition/">$68B (2018)</a> → <a href="https://platformonomics.com/2022/02/follow-the-capex-cloud-table-stakes-2021-retrospective/">$124B (2021)</a></li>
<li>81% growth over 3 years</li>
<li>Annual growth rate: <strong>~22%</strong></li>
<li>Driven by cloud migration, pandemic acceleration, streaming</li>
</ul>
<p><strong>AI Boom (2023-2025):</strong></p>
<ul>
<li>2023: $127B</li>
<li>2024: <a href="https://platformonomics.com/2025/02/follow-the-capex-cloud-table-stakes-2024-retrospective/">$212B</a> (<strong>67% growth</strong> year-over-year)</li>
<li>2025 projected: <a href="https://www.cnbc.com/2025/02/08/tech-megacaps-to-spend-more-than-300-billion-in-2025-to-win-in-ai.html">$255B+</a> (Amazon $100B, Microsoft $80B, Alphabet $75B)</li>
</ul>
<p>While it&#39;s no doubt a huge amount of capex going into this rollout; it&#39;s not quite as dramatic as some news stories make out. I have no doubt that now any datacentre related capex is being rebranded as &#34;AI&#34;, even if it&#39;s just &#39;boring&#39; old compute, storage and network not being directly used for AI.</p>
<h2>Why Forecasting Is Nearly Impossible</h2>
<p>Here&#39;s where I think the comparison to telecoms becomes both interesting and concerning.</p>
<p><strong>The Lead Time Problem:</strong></p>
<ul>
<li>Datacenters take 2-3 years to build</li>
<li>GPU orders have 6-12 month lead times</li>
<li>Can&#39;t adjust capacity in real-time to match demand</li>
</ul>
<p><strong>The Prisoner&#39;s Dilemma:</strong></p>
<ul>
<li>Underestimating demand = terrible user experience + losing to competitors</li>
<li>Overestimating demand = billions in wasted capex (that might just get used slower)</li>
<li>Given the choice, rational players overbuild - because wasting some capex is infinitely better than losing the &#34;AI wars&#34;</li>
</ul>
<p><strong>The Forecasting Challenge:</strong></p>
<p>Imagine you&#39;re planning datacenter capacity right now for 2027. You need to make billion-dollar decisions today based on what you think AI usage will look like in three years.</p>
<p>Here&#39;s scenario one: agent adoption is gradual. Some developers use Claude Code daily. A few enterprises deploy internal agents. Customer service stays mostly human with AI assist. You need maybe 3-4x your current infrastructure.</p>
<p>Here&#39;s scenario two: agents go mainstream. Every developer has an always-on coding agent consuming millions of tokens per session. Enterprises deploy agents across operations, finance, legal, sales. Customer service becomes 80% agentic with humans handling escalations. You need 30-50x your current infrastructure.</p>
<p>Both scenarios are completely plausible. Nobody can tell you which one is right. But you have to commit billions in capex NOW - datacenters take 2-3 years to build, GPU orders have 6-12 month lead times.</p>
<p><strong>But here&#39;s the really insidious part:</strong> even if you&#39;re directionally right, small errors compound massively. Let&#39;s say you&#39;re confident agents are going mainstream and you need roughly 50x growth over 3 years.</p>
<p>If actual demand is 40x, you&#39;ve overbuilt by 25% - billions in excess capacity.
If actual demand is 60x, you&#39;ve underbuilt by 20% - your service degrades and you lose market share.</p>
<p>You&#39;re trying to hit a moving target in the dark, and the margin of error is measured in tens of billions of dollars and thousands of megawatts of power infrastructure.</p>
<p>If you build for scenario one and scenario two happens, your service degrades to unusable, users revolt, and you lose the AI wars to competitors who bet bigger. If you build for scenario two and scenario one happens, you&#39;ve got billions in underutilized datacenters burning cash.</p>
<p>Which mistake would you rather make?</p>
<p>This is where the telecoms comparison makes sense: given those choices, rational players overbuild. The difference is what happens to that overcapacity.</p>
<h2>The Key Differences</h2>
<p>Let me put this in a table:</p>
<table>
<thead>
<tr>
<th>Factor</th>
<th>Telecoms (1990s-2000s)</th>
<th>AI Datacenters (2020s)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Supply improvements</strong></td>
<td>Exponential (100,000x capacity increase)</td>
<td>Slowing (69%→44% annual perf/watt gains)</td>
</tr>
<tr>
<td><strong>Demand growth</strong></td>
<td>Overestimated 4x</td>
<td>Potentially underestimated (agent transition)</td>
</tr>
<tr>
<td><strong>Utilization</strong></td>
<td>95% dark fiber (genuine overcapacity)</td>
<td>Very high - many providers still experiencing peak time scale problems</td>
</tr>
<tr>
<td><strong>Technology curve</strong></td>
<td>Making infrastructure obsolete</td>
<td>Hitting semiconductor physics limits</td>
</tr>
<tr>
<td><strong>Power consumption</strong></td>
<td>Decreasing</td>
<td>Increasing (300W → 1200W)</td>
</tr>
<tr>
<td><strong>Infrastructure lifespan</strong></td>
<td>Decades (fiber doesn&#39;t degrade)</td>
<td>Years (refreshed as better hardware arrives)</td>
</tr>
</tbody>
</table>
<p>The telecoms crash happened because exponential supply improvements met linearly growing (and overestimated) demand, with infrastructure that would last decades sitting unused.</p>
<p>AI datacenters are facing slowing supply improvements meeting potentially exponentially growing demand. And crucially, because GPU efficiency improvements are slowing down, today&#39;s hardware retains value for longer - not shorter - than previous generations.</p>
<h2>What About a Short-Term Correction?</h2>
<p>Could there still be a short-term crash? Absolutely.</p>
<p><strong>Scenarios that could trigger a correction:</strong></p>
<p><strong>1. Agent adoption hits a wall</strong></p>
<p>Enterprises might discover that production agent deployments are harder than demos suggest. Hallucinations in high-stakes workflows, regulatory concerns around autonomous AI systems, or implementation complexity could slow adoption dramatically. If the agent future takes 5-7 years instead of 2-3, there&#39;s a painful gap where billions in infrastructure sits waiting for demand to catch up.</p>
<p>However, given the explosion in usage for software engineering and other tasks, I suspect this is highly unlikely. You can already use Claude Code for <a href="https://martinalderson.com/posts/building-a-tax-agent-with-claude-code/">non engineering tasks</a> in professional services and get very impressive results without any industry specific modifications, so I have no doubt there is going to be very high adoption of agents in all kinds of areas.</p>
<p><strong>2. Financial engineering unravels</strong></p>
<p>These datacenter buildouts are heavily debt-financed. If credit markets seize up, interest rates spike further, or lenders lose confidence in AI growth projections, the financing model could collapse. This wouldn&#39;t be about technical fundamentals - it would be good old-fashioned financial panic, similar to what happened in telecoms when the debt markets froze, but with one key difference - a lot of the key players (Microsoft, Google, Meta, Oracle) are extremely cash flow positive, which definitely wasn&#39;t the case in the 2000s fibre boom. The pure datacentre players though are at risk - who don&#39;t have a money printing main business to backstop the finance -  no doubt about that.</p>
<p><strong>3. Efficiency breakthroughs change the math</strong></p>
<p>Model efficiency could improve faster than expected. Or we could see a hardware breakthrough: custom ASICs that are 10x more efficient than GB200s for inference workloads. Either scenario could make current buildouts look excessive. I actually think this is the biggest risk - and this is <em>exactly</em> what happened in the fibre boom. So far, I&#39;m not seeing signs of this though. While specialist ASICs are becoming available, they hit their impressive speed by having huge wafers, which isn&#39;t a huge efficiency game (yet).</p>
<p><strong>The Key Difference From Telecoms:</strong></p>
<p>Even if there&#39;s a correction, the underlying dynamics are different. Telecoms built for demand that was 4x overestimated, then watched fiber optic technology improvements make their infrastructure obsolete before it could be utilized. The result: 95% of fiber remained permanently dark.</p>
<p>AI datacenters might face a different scenario. If we build for 50x growth and only get 30x over 3 years, that&#39;s not &#34;dark infrastructure&#34; - that&#39;s just infrastructure that gets utilized on a slower timeline than expected. Unlike fiber optic cable sitting in the ground unused, GPU clusters still serve production workloads, just at lower capacity than planned.</p>
<p>And unlike telecoms where exponential technology improvements made old infrastructure worthless, GPU efficiency improvements are slowing. A GB200 deployed today doesn&#39;t become obsolete when next year&#39;s chip arrives - because that chip is only incrementally better, not 100x better. With process node improvements slowing down, current generation hardware actually retains value for longer.</p>
<p>A correction might mean 2-3 years of financial pain, consolidation, and write-downs as demand catches up to capacity. But that&#39;s fundamentally different from building infrastructure for demand that never materializes while technology makes it obsolete.</p>
<h2>The Real Risk: Timing, Not Direction</h2>
<p>I think the real question isn&#39;t whether we need massive AI infrastructure - the agent transition alone suggests we do. The question is timing.</p>
<p>If enterprises take 5 years to adopt agents at scale instead of 2 years, and hyperscalers have built for the 2-year scenario, you could see a 2-3 year period of overcapacity and financial pain. That might be enough to trigger a correction, layoffs, and consolidation.</p>
<p>But unlike telecoms, that overcapacity would likely get absorbed.</p>
<p>The telecom fibre mostly stayed dark because technology outpaced it and demand never materialized. AI infrastructure might just be early, not wrong.</p>
<h2>Conclusion</h2>
<p>Are we repeating the telecoms crash with AI datacenters? The fundamentals suggest not, but that doesn&#39;t mean there won&#39;t be bumps.</p>
<p>The key insight people miss when making the telecoms comparison: telecoms had exponential supply improvements meeting linear demand, with 4x overestimated growth assumptions. AI has slowing supply improvements potentially meeting exponential demand growth from the agent transition.</p>
<p>The risks are different:</p>
<ul>
<li><strong>Telecoms:</strong> Built too much infrastructure that became completely obsolete by supply-side technology improvements</li>
<li><strong>AI:</strong> Might build too much too fast for demand that arrives slower than expected</li>
</ul>
<p>But the &#34;too much&#34; in AI&#39;s case is more like &#34;3 years of runway instead of 1 year&#34; rather than &#34;95% will never be used.&#34;</p>
<p>I could be wrong. Maybe agent adoption stalls, maybe model efficiency makes current infrastructure obsolete, maybe there&#39;s a breakthrough in GPU architecture that changes everything. But when I look at the numbers, I don&#39;t see the same setup as the telecoms crash.</p>
<p>The fundamentals are different. That doesn&#39;t mean there won&#39;t be pain, consolidation, or failures. But comparing this to 2000s telecoms seems like the wrong mental model for what&#39;s actually happening.</p>

    </div>
</article></div>
  </body>
</html>
