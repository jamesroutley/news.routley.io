<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ash80/RLHF_in_notebooks">Original</a>
    <h1>Reinforcement Learning from Human Feedback (RLHF) in Notebooks</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository provides a reference implementation for Reinforcement Learning from Human Feedback (RLHF) [<a href="https://arxiv.org/abs/2203.02155" rel="nofollow">Paper</a>] framework presented in the <a href="https://youtu.be/K1UBOodkqEk" rel="nofollow">RLHF from scratch, step-by-step, in code</a> YouTube video.</p>

<p dir="auto">RLHF is a method for aligning large language models (LLMs), like GPT-3 or GPT-2, to better meet users&#39; intents. It is essentially a reinforcement learning approach, where rather than directly getting the reward or feedback from some environemnt or human, it instead trains a reward model that learns to mimic that reward. The trained reward model is used to rank the generation from the LLM in the reinforcement learning step. The RLHF process consists of three steps:</p>
<ol dir="auto">
<li>Supervised Fine-Tuning (SFT)</li>
<li>Reward Model Training</li>
<li>Reinforcement Learning via Proximal Policy Optimisation (PPO).</li>
</ol>

<p dir="auto">To build a chatbot from a pretrained LLM, we might:</p>
<ul dir="auto">
<li>Collect a dataset of question-answer pairs (either human-written or generated by the pretrained model).</li>
<li>Human annotators rank these answers by quality.</li>
<li>Follow the three RLHF steps mentioned above:
<ol dir="auto">
<li><strong>SFT</strong>: Fine-tune the LLM to predict the next tokens given question-answer pairs.</li>
<li><strong>Reward Model</strong>: Train another instance of the LLM with an added reward head to mimic human rankings.</li>
<li><strong>PPO</strong>: Further optimize the fine-tuned model using PPO to produce answers that the reward model evaluates positively.</li>
</ol>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Implementation in this Repository</h2><a id="user-content-implementation-in-this-repository" aria-label="Permalink: Implementation in this Repository" href="#implementation-in-this-repository"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Instead of building a chatbot that would need a dataset of ranked questions and answers, we adapt the RLHF method to fine-tune GPT-2 to generate sentences expressing positive sentiments. To achieve this task we use the <a href="https://huggingface.co/datasets/stanfordnlp/sst2" rel="nofollow"><code>stanfordnlp/sst2</code></a> dataset, a collection of movie review sentences labeled as expressing positive or negative sentiment. Our goal is to leverage RLHF to optimise the pretrained GPT-2 such that it only generates sentences that are likely to express a positive sentiment.</p>
<p dir="auto">We achieve this goal by implementing the following three notebooks, each corresponding to one step of the RLHF process:</p>
<ol dir="auto">
<li><a href="https://github.com/ash80/RLHF_in_notebooks/blob/master/1-SFT.ipynb"><code>1-SFT.ipynb</code></a>: Fine-tunes GPT-2 via supervised learning on the <code>stanfordnlp/sst2</code> dataset, training it to generate sentences resembling the sentences in this dataset. After fine-tuning, the model is saved as the <strong>SFT model</strong>.</li>
<li><a href="https://github.com/ash80/RLHF_in_notebooks/blob/master/2-RM%20Training.ipynb"><code>2-RM Training.ipynb</code></a>: Creates a Reward Model by attaching a reward head to the pretrained GPT-2. This model is trained to predict sentiment labels (positive/negative) of sentences in the <code>stanfordnlp/sst2</code> dataset. After training, the reward model (GPT-2 + Reward Head) is saved.</li>
<li><a href="https://github.com/ash80/RLHF_in_notebooks/blob/master/3-RLHF.ipynb"><code>3-RLHF.ipynb</code></a>: Implements the final reinforcement learning step using PPO:
<ul dir="auto">
<li><strong>Sampling stage</strong>: Generates sentences from the policy model (initialized from the SFT model) based on the initial few tokens and scores these sentences using the trained reward model.</li>
<li><strong>Optimization stage</strong>: Optimizes the policy model parameters using PPO to produce sentences that are more likely to receive higher rewards (positive sentiment scores).</li>
</ul>
</li>
</ol>
<p dir="auto">After completing these steps, GPT-2 will generate sentences aligned specifically to convey positive sentiments.</p>


<p dir="auto"><strong>Hugging Face Access Token</strong>: You will need an access token from Hugging Face to download the pretrained GPT-2 model. Obtain a token by following instructions on <a href="https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication" rel="nofollow">HuggingFace Quickstart Guide</a>.</p>

<p dir="auto"><strong>Local Setup:</strong></p>
<p dir="auto">Set your Hugging Face token as an environment variable named <code>HF_TOKEN</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export HF_TOKEN=&#39;your_huggingface_token_here&#39;"><pre><span>export</span> HF_TOKEN=<span><span>&#39;</span>your_huggingface_token_here<span>&#39;</span></span></pre></div>
<p dir="auto"><strong>Google Colab:</strong></p>
<p dir="auto">Set your Hugging Face token in Colab Secrets. Or set it as an environment variable by running the following code in a cell of your Jupyter notebook.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
os.environ[&#39;HF_TOKEN&#39;] = &#39;your_huggingface_token_here&#39;"><pre><span>import</span> <span>os</span>
<span>os</span>.<span>environ</span>[<span>&#39;HF_TOKEN&#39;</span>] <span>=</span> <span>&#39;your_huggingface_token_here&#39;</span></pre></div>

<p dir="auto">Open and run notebooks sequentially (<code>1-SFT.ipynb</code>, <code>2-RM Training.ipynb</code>, then <code>3-RLHF.ipynb</code>), following the step-by-step instructions provided within each notebook or in the <a href="https://youtu.be/K1UBOodkqEk" rel="nofollow">YouTube video</a>.</p>
</article></div></div>
  </body>
</html>
