<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://workos.com/blog/how-to-run-deepseek-r1-locally">Original</a>
    <h1>How to run DeepSeek R1 locally</h1>
    
    <div id="readability-page-1" class="page"><p>DeepSeek R1 is an open-source LLM for conversational AI, coding, and problem-solving. Here&#39;s how to run it locally.</p><div fs-toc-offsettop="150px" fs-toc-element="contents"><p>People who want full control over data, security, and performance run LLMs locally.</p><p><a href="https://workos.com/blog/deepseek-r1-pushes-local-and-open-ai-forward">DeepSeek R1</a> is an open-source LLM for conversational AI, coding, and problem-solving that recently outperformed OpenAI’s flagship reasoning model, o1, on several <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">benchmarks</a>.</p><p>You’re in the right place if you’d like to get this model running locally.</p><h2>How to run DeepSeek R1 using Ollama</h2><h3>What is Ollama?</h3><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/621f84dc15b5ed16dc85a18a/679a07691cba7da46780075c_679a0451b6d150b485840926_ollama.webp" loading="lazy" alt=""/></p></figure><p>Ollama runs AI models on your local machine. It simplifies the complexities of AI model deployment by offering:</p><ul role="list"><li><strong>Pre-packaged model support</strong>: It supports many popular AI models, including DeepSeek R1.</li><li><strong>Cross-platform compatibility</strong>: Works on macOS, Windows, and Linux.</li><li><strong>Simplicity and performance</strong>: Minimal fuss, straightforward commands, and efficient resource use.</li></ul><h3>Why Ollama?</h3><ol role="list"><li> <strong>Easy Installation</strong> – Quick setup on multiple platforms.</li><li> <strong>Local Execution</strong> – Everything runs on your machine, ensuring full data privacy.</li><li> <strong>Effortless Model Switching</strong> – Pull different AI models as needed.</li></ol><h3>Download and Install Ollama</h3><p>Visit <a href="https://ollama.ai">Ollama’s website</a> for detailed installation instructions, or install directly via Homebrew on macOS:</p><p>‍<code>brew install ollama</code></p><p><code>‍</code>For Windows and Linux, follow the platform-specific steps provided on the Ollama website.</p><h3>Fetch DeepSeek R1</h3><p>Next, pull the DeepSeek R1 model onto your machine:</p><p>‍<code>ollama pull deepseek-r1</code></p><p>By default, this downloads the main DeepSeek R1 model (which is large). If you’re interested in a specific distilled variant (e.g., 1.5B, 7B, 14B), just specify its tag, like:</p><p><code>ollama pull deepseek-r1:1.5b</code></p><h2>Run Ollama serve</h2><p>Do this in a separate terminal tab or a new terminal window:</p><p><code>ollama serve</code> </p><h2>Start using DeepSeek R1</h2><p>Once installed, you can interact with the model right from your terminal:</p><p>‍<code>ollama run deepseek-r1</code></p><p><code>‍</code>Or, to run the 1.5B distilled model:</p><p>‍<code>ollama run deepseek-r1:1.5b</code></p><p><code>‍</code>Or, to prompt the model:</p><p>ollama run deepseek-r1:1.5b  &#34;What is the latest news on Rust programming language trends?&#34;</p><p>Here are a few example prompts to get you started:</p><h3>Chat </h3><p><strong>‍</strong>What’s the latest news on Rust programming language trends?</p><h3>Coding </h3><p><strong>‍</strong>How do I write a regular expression for email validation?</p><h3>Math </h3><p><strong>‍</strong>Simplify this equation: 3x^2 + 5x - 2.</p><h2>What is DeepSeek R1?</h2><p>DeepSeek R1 is a state-of-the-art AI model built for developers. It excels at:</p><ul role="list"><li> <strong>Conversational AI</strong> – Natural, human-like dialogue.</li><li> <strong>Code Assistance</strong> – Generating and refining code snippets.</li><li> <strong>Problem-Solving</strong> – Tackling math, algorithmic challenges, and beyond.</li></ul><h3>Why it matters</h3><p>Running DeepSeek R1 locally keeps your data private, as no information is sent to external servers. </p><p>At the same time, you’ll enjoy faster responses and the freedom to integrate this AI model into any workflow without worrying about external dependencies. </p><p>For a more in-depth look at the model, its origins and why it’s remarkable, check out our <a href="https://workos.com/blog/deepseek-r1-pushes-local-and-open-ai-forward">explainer post on DeepSeek R1.</a></p><h2>A note on distilled models</h2><p>DeepSeek’s team has demonstrated that reasoning patterns learned by large models can be distilled into smaller models. </p><p>This process fine-tunes a smaller “student” model using outputs (or “reasoning traces”) from the larger “teacher” model, often resulting in better performance than training a small model from scratch.</p><p>The DeepSeek-R1-Distill variants are smaller (1.5B, 7B, 8B, etc.) and optimized for developers who:</p><ul role="list"><li> Want lighter compute requirements, so they can run models on less-powerful machines.</li><li> Prefer faster responses, especially for real-time coding help.</li><li> Don’t want to sacrifice too much performance or reasoning capability.</li></ul><h2>Practical usage tips</h2><h3>Command-line automation</h3><p>Wrap your Ollama commands in shell scripts to automate repetitive tasks. For instance, you could create a script like:</p><pre contenteditable="false"><code><span>#!</span><span>/usr/</span><span>bin/env bash
</span><span>PROMPT=</span><span>&#34;$*&#34;</span><span>
</span><span>ollama run deepseek-r1:7b </span><span>&#34;$PROMPT&#34;</span></code></pre><p>Now you can fire off requests quickly:</p><pre contenteditable="false"><code><span>./ask-deepseek.sh </span><span>&#34;Explain how to write a regex for email validation&#34;</span></code></pre><h3>IDE integration and command line tools</h3><p>Many IDEs allow you to configure external tools or run tasks. </p><p>You can set up an action that prompts DeepSeek R1 for code generation or refactoring, and inserts the returned snippet directly into your editor window.</p><p>Open source tools like <a href="https://github.com/charmbracelet/mods">mods</a> provide excellent interfaces to local and cloud-based LLMs.</p><h2>FAQ</h2><p><strong>Q: Which version of DeepSeek R1 should I choose?</strong></p><p><strong>‍</strong>A: If you have a powerful GPU or CPU and need top-tier performance, use the main DeepSeek R1 model. If you’re on limited hardware or prefer faster generation, pick a distilled variant (e.g., 1.5B, 14B).</p><p>‍<strong>Q: Can I run DeepSeek R1 in a Docker container or on a remote server?</strong></p><p><strong>‍</strong>A: Yes. As long as Ollama can be installed, you can run DeepSeek R1 in Docker, on cloud VMs, or on-prem servers.</p><p>‍<strong>Q: Is it possible to fine-tune DeepSeek R1 further?</strong></p><p><strong>‍</strong>A: Yes. Both the main and distilled models are licensed to allow modifications or derivative works. Be sure to check the license specifics for Qwen- and Llama-based variants.</p><p>‍<strong>Q: Do these models support commercial use?</strong></p><p><strong>‍</strong>A: Yes. DeepSeek R1 series models are MIT-licensed, and the Qwen-distilled variants are under Apache 2.0 from their original base. For Llama-based variants, check the Llama license details. All are relatively permissive, but read the exact wording to confirm your planned use.</p></div></div>
  </body>
</html>
