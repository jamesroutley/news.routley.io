<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41586-021-04223-6">Original</a>
    <h1>Deep physical neural networks trained with backpropagation</h1>
    
    <div id="readability-page-1" class="page"><div>
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div id="Abs1-section"><h2 id="Abs1">Abstract</h2><p>Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patterson, D. et al. Carbon emissions and large neural network training. Preprint at 
                  https://arxiv.org/abs/2104.10350
                  
                 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR1" id="ref-link-section-d16580660e442">1</a></sup>. Deep-learning accelerators<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Reuther, A. et al. Survey of machine learning accelerators. In 2020 IEEE High Performance Extreme Computing Conference (HPEC) 1–12 (IEEE, 2020)." href="#ref-CR2" id="ref-link-section-d16580660e446">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Xia, Q., &amp; Yang, J. J. Memristive crossbar arrays for brain-inspired computing. Nat. Mater. 18, 309–323 (2019)." href="#ref-CR3" id="ref-link-section-d16580660e446_1">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Burr, G. W. et al. Neuromorphic computing using non-volatile memory. Adv. Phys. X 2, 89–124 (2017)." href="#ref-CR4" id="ref-link-section-d16580660e446_2">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Khaddam-Aljameh, R. et al. HERMES core—a 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In 2021 Symposium on VLSI Circuits (IEEE, 2021)." href="#ref-CR5" id="ref-link-section-d16580660e446_3">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Narayanan, P. et al. Fully on-chip MAC at 14nm enabled by accurate row-wise programming of PCM-based weights and parallel vector-transport in duration-format. In 2021 Symposium on VLSI Technology (IEEE, 2021)." href="#ref-CR6" id="ref-link-section-d16580660e446_4">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kohda, Y. et al. Unassisted true analog neural network training chip. In 2020 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2020)." href="#ref-CR7" id="ref-link-section-d16580660e446_5">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Marković, D., Mizrahi, A., Querlioz, D. &amp; Grollier, J. Physics for neuromorphic computing. Nat. Rev. Phys. 2, 499–510 (2020)." href="#ref-CR8" id="ref-link-section-d16580660e446_6">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wetzstein, G. et al. Inference in artificial intelligence with deep optics and photonics. Nature 588, 39–47 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR9" id="ref-link-section-d16580660e449">9</a></sup> aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="#ref-CR10" id="ref-link-section-d16580660e453">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="#ref-CR11" id="ref-link-section-d16580660e453_1">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal-oxide memristors. Nature 521, 61–64 (2015)." href="#ref-CR12" id="ref-link-section-d16580660e453_2">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="#ref-CR13" id="ref-link-section-d16580660e453_3">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. Sci. Adv. 5, eaay6946 (2019)." href="#ref-CR14" id="ref-link-section-d16580660e453_4">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. Photon. Res. 8, 46–50 (2020)." href="#ref-CR15" id="ref-link-section-d16580660e453_5">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Furuhata, G., Niiyama, T. &amp; Sunada, S. Physical deep learning based on optimal control of dynamical systems. Phys. Rev. Appl. 15, 034092 (2021)." href="#ref-CR16" id="ref-link-section-d16580660e453_6">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lin, X. et al. All-optical machine learning using diffractive deep neural networks. Science 361, 1004–1008 (2018)." href="#ref-CR17" id="ref-link-section-d16580660e453_7">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miller, J. F., Harding, S. L. &amp; Tufte, G. Evolution-in-materio: evolving computation in materials. Evol. Intell. 7, 49–67 (2014)." href="#ref-CR18" id="ref-link-section-d16580660e453_8">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen, T. et al. Classification with a disordered dopant-atom network in silicon. Nature 577, 341–345 (2020)." href="#ref-CR19" id="ref-link-section-d16580660e453_9">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bueno, J. et al. Reinforcement learning in a large-scale photonic recurrent neural network. Optica 5, 756–760 (2018)." href="#ref-CR20" id="ref-link-section-d16580660e453_10">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tanaka, G. et al. Recent advances in physical reservoir computing: a review. Neural Netw. 115, 100–123 (2019)." href="#ref-CR21" id="ref-link-section-d16580660e453_11">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Appeltant, L. et al. Information processing using a single dynamical node as complex system. Nat. Commun. 2, 468 (2011)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR22" id="ref-link-section-d16580660e456">22</a></sup> have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the de facto training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ–in silico algorithm, called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep physical neural networks made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with in situ algorithms. Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mouret, J.-B. &amp; Chatzilygeroudis, K. 20 years of reality gap: a few thoughts about simulators in evolutionary robotics. In Proc. Genetic and Evolutionary Computation Conference Companion 1121–1124 (2017)." href="#ref-CR23" id="ref-link-section-d16580660e460">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Howison, T., Hauser, S., Hughes, J. &amp; Iida, F. Reality-assisted evolution of soft robots through large-scale physical experimentation: a review. Artif. Life 26, 484–506 (2021)." href="#ref-CR24" id="ref-link-section-d16580660e460_1">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="de Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J. &amp; Kolter, J. Z. End-to-end differentiable physics for learning and control. Adv. Neural Inf. Process. Syst. 31, 7178–7189 (2018)." href="#ref-CR25" id="ref-link-section-d16580660e460_2">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Degrave, J., Hermans, M., Dambre, J. &amp; Wyffels, F. A differentiable physics engine for deep learning in robotics. Front. Neurorobot. 13, 6 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR26" id="ref-link-section-d16580660e463">26</a></sup>, materials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Molesky, S. et al. Inverse design in nanophotonics. Nat. Photon. 12, 659–670 (2018)." href="#ref-CR27" id="ref-link-section-d16580660e467">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Peurifoy, J. et al. Nanophotonic particle simulation and inverse design using artificial neural networks. Sci. Adv. 4, eaar4206 (2018)." href="#ref-CR28" id="ref-link-section-d16580660e467_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Stern, M., Arinze, C., Perez, L., Palmer, S. E. &amp; Murugan, A. Supervised learning through physical changes in a mechanical system. Proc. Natl Acad. Sci. USA 117, 14843–14850 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR29" id="ref-link-section-d16580660e470">29</a></sup> and smart sensors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou, F. &amp; Chai, Y. Near-sensor and in-sensor computing. Nat. Electron. 3, 664–671 (2020)." href="#ref-CR30" id="ref-link-section-d16580660e475">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martel, J. N., Mueller, L. K., Carey, S. J., Dudek, P. &amp; Wetzstein, G. Neural sensors: learning pixel exposures for HDR imaging and video compressive sensing with programmable sensors. IEEE Trans. Pattern Anal. Mach. Intell. 42, 1642–1653 (2020)." href="#ref-CR31" id="ref-link-section-d16580660e475_1">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Mennel, L. et al. Ultrafast machine vision with 2D material neural network image sensors. Nature 579, 62–66 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR32" id="ref-link-section-d16580660e478">32</a></sup>.</p></div></section>
            

            
                
            
            
                <section data-title="Main"><div id="Sec1-section"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Like many historical developments in artificial intelligence<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Brooks, R. A. Intelligence without reason. In Proc. 12th International Joint Conference on Artificial Intelligence Vol. 1, 569–595 (Morgan Kaufmann, 1991)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR33" id="ref-link-section-d16580660e504">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Hooker, S. The hardware lottery. Preprint at 
                  https://arxiv.org/abs/2009.06489
                  
                 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR34" id="ref-link-section-d16580660e507">34</a></sup>, the widespread adoption of deep neural networks (DNNs) was enabled in part by synergistic hardware. In 2012, building on earlier works, Krizhevsky et al. showed that the backpropagation algorithm could be efficiently executed with graphics-processing units to train large DNNs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097–1105 (2012)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR35" id="ref-link-section-d16580660e511">35</a></sup> for image classification. Since 2012, the computational requirements of DNN models have grown rapidly, outpacing Moore’s law<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patterson, D. et al. Carbon emissions and large neural network training. Preprint at 
                  https://arxiv.org/abs/2104.10350
                  
                 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR1" id="ref-link-section-d16580660e515">1</a></sup>. Now, DNNs are increasingly limited by hardware energy efficiency.</p><p>The emerging DNN energy problem has inspired special-purpose hardware: DNN ‘accelerators’<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Reuther, A. et al. Survey of machine learning accelerators. In 2020 IEEE High Performance Extreme Computing Conference (HPEC) 1–12 (IEEE, 2020)." href="#ref-CR2" id="ref-link-section-d16580660e522">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Xia, Q., &amp; Yang, J. J. Memristive crossbar arrays for brain-inspired computing. Nat. Mater. 18, 309–323 (2019)." href="#ref-CR3" id="ref-link-section-d16580660e522_1">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Burr, G. W. et al. Neuromorphic computing using non-volatile memory. Adv. Phys. X 2, 89–124 (2017)." href="#ref-CR4" id="ref-link-section-d16580660e522_2">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Khaddam-Aljameh, R. et al. HERMES core—a 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In 2021 Symposium on VLSI Circuits (IEEE, 2021)." href="#ref-CR5" id="ref-link-section-d16580660e522_3">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Narayanan, P. et al. Fully on-chip MAC at 14nm enabled by accurate row-wise programming of PCM-based weights and parallel vector-transport in duration-format. In 2021 Symposium on VLSI Technology (IEEE, 2021)." href="#ref-CR6" id="ref-link-section-d16580660e522_4">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kohda, Y. et al. Unassisted true analog neural network training chip. In 2020 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2020)." href="#ref-CR7" id="ref-link-section-d16580660e522_5">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Marković, D., Mizrahi, A., Querlioz, D. &amp; Grollier, J. Physics for neuromorphic computing. Nat. Rev. Phys. 2, 499–510 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR8" id="ref-link-section-d16580660e525">8</a></sup>, most of which are based on direct mathematical isomorphism between the hardware physics and the mathematical operations in DNNs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig1">1a, b</a>). Several accelerator proposals use physical systems beyond conventional electronics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Marković, D., Mizrahi, A., Querlioz, D. &amp; Grollier, J. Physics for neuromorphic computing. Nat. Rev. Phys. 2, 499–510 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR8" id="ref-link-section-d16580660e532">8</a></sup>, such as optics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Wetzstein, G. et al. Inference in artificial intelligence with deep optics and photonics. Nature 588, 39–47 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR9" id="ref-link-section-d16580660e536">9</a></sup> and analogue electronic crossbar arrays<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Xia, Q., &amp; Yang, J. J. Memristive crossbar arrays for brain-inspired computing. Nat. Mater. 18, 309–323 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR3" id="ref-link-section-d16580660e540">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Burr, G. W. et al. Neuromorphic computing using non-volatile memory. Adv. Phys. X 2, 89–124 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR4" id="ref-link-section-d16580660e543">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal-oxide memristors. Nature 521, 61–64 (2015)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR12" id="ref-link-section-d16580660e546">12</a></sup>. Most devices target the inference phase of deep learning, which accounts for up to 90% of the energy costs of deep learning in commercial deployments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patterson, D. et al. Carbon emissions and large neural network training. Preprint at 
                  https://arxiv.org/abs/2104.10350
                  
                 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR1" id="ref-link-section-d16580660e551">1</a></sup>, although, increasingly, devices are also addressing the training phase (for example, ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kohda, Y. et al. Unassisted true analog neural network training chip. In 2020 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR7" id="ref-link-section-d16580660e555">7</a></sup>).</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Introduction to PNNs."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Introduction to PNNs.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://jaked.org/articles/s41586-021-04223-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="527"/></picture></a></div><p><b>a</b>, Artificial neural networks contain operational units (layers): typically, trainable matrix-vector multiplications followed by element-wise nonlinear activation functions. <b>b</b>, DNNs use a sequence of layers and can be trained to implement multi-step (hierarchical) transformations on input data. <b>c</b>, When physical systems evolve, they perform, in effect, computations. We partition their controllable properties into input data and control parameters. Changing parameters alters the transformation performed on data. We consider three examples. In a mechanical (electronic) system, input data and parameters are encoded into time-dependent forces (voltages) applied to a metal plate (nonlinear circuit). The controlled multimode oscillations (transient voltages) are then measured by a microphone (oscilloscope). In a nonlinear optical system, pulses pass through a <span>\({\chi }^{(2)}\,\)</span>crystal, producing nonlinearly mixed outputs. Input data and parameters are encoded in the input pulses’ spectra, and outputs are obtained from the frequency-doubled pulses’ spectra. <b>d</b>, Like DNNs constructed from sequences of trainable nonlinear mathematical functions, we construct deep PNNs with sequences of trainable physical transformations. In PNNs, each physical layer implements a controllable physical function, which does need to be mathematically isomorphic to a conventional DNN layer.</p></div></figure></div><p>However, implementing trained mathematical transformations by designing hardware for strict, operation-by-operation mathematical isomorphism is not the only way to perform efficient machine learning. Instead, we can train the hardware’s physical transformations directly to perform desired computations. Here we call this approach physical neural networks (PNNs) to emphasize that physical processes, rather than mathematical operations, are trained. This distinction is not merely semantic: by breaking the traditional software–hardware division, PNNs provide the possibility to opportunistically construct neural network hardware from virtually any controllable physical system(s). As anyone who has simulated the evolution of complex physical systems appreciates, physical transformations are often faster and consume less energy than their digital emulations. This suggests that PNNs, which can harness these physical transformations most directly, may be able to perform certain computations far more efficiently than conventional paradigms, and thus provide a route to more scalable, energy-efficient and faster machine learning.</p><p>PNNs are particularly well motivated for DNN-like calculations, much more so than for digital logic or even other forms of analogue computation. As expected from their robust processing of natural data, DNNs and physical processes share numerous structural similarities, such as hierarchy, approximate symmetries, noise, redundancy and nonlinearity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Lin, H. W., Tegmark, M. &amp; Rolnick, D. Why does deep and cheap learning work so well? J. Stat. Phys. 168, 1223–1247 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR36" id="ref-link-section-d16580660e630">36</a></sup>. As physical systems evolve, they perform transformations that are effectively equivalent to approximations, variants and/or combinations of the mathematical operations commonly used in DNNs, such as convolutions, nonlinearities and matrix-vector multiplications. Thus, using sequences of controlled physical transformations (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig1">1c</a>), we can realize trainable, hierarchical physical computations, that is, deep PNNs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig1">1d</a>).</p><p>Although the paradigm of constructing computers by directly training physical transformations has ancestry in evolved computing materials<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Miller, J. F., Harding, S. L. &amp; Tufte, G. Evolution-in-materio: evolving computation in materials. Evol. Intell. 7, 49–67 (2014)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR18" id="ref-link-section-d16580660e644">18</a></sup>, it is today emerging in various fields, including optics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. Sci. Adv. 5, eaay6946 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR14" id="ref-link-section-d16580660e648">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. Photon. Res. 8, 46–50 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR15" id="ref-link-section-d16580660e651">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Lin, X. et al. All-optical machine learning using diffractive deep neural networks. Science 361, 1004–1008 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR17" id="ref-link-section-d16580660e654">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Bueno, J. et al. Reinforcement learning in a large-scale photonic recurrent neural network. Optica 5, 756–760 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR20" id="ref-link-section-d16580660e657">20</a></sup>, spintronic nano-oscillators<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR10" id="ref-link-section-d16580660e661">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Grollier, J. et al. Neuromorphic spintronics. Nat. Electron. 3, 360–370 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR37" id="ref-link-section-d16580660e664">37</a></sup>, nanoelectronic devices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR13" id="ref-link-section-d16580660e668">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Chen, T. et al. Classification with a disordered dopant-atom network in silicon. Nature 577, 341–345 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR19" id="ref-link-section-d16580660e671">19</a></sup> and small-scale quantum computers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Mitarai, K., Negoro, M., Kitagawa, M. &amp; Fujii, K. Quantum circuit learning. Phys. Rev. A 98, 032309 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR38" id="ref-link-section-d16580660e675">38</a></sup>. A closely related trend is physical reservoir computing (PRC)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Tanaka, G. et al. Recent advances in physical reservoir computing: a review. Neural Netw. 115, 100–123 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR21" id="ref-link-section-d16580660e680">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Appeltant, L. et al. Information processing using a single dynamical node as complex system. Nat. Commun. 2, 468 (2011)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR22" id="ref-link-section-d16580660e683">22</a></sup>, in which the transformations of an untrained physical ‘reservoir’ are linearly combined by a trainable output layer. Although PRC harnesses generic physical processes for computation, it is unable to realize DNN-like hierarchical computations. In contrast, approaches that train the physical transformations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="#ref-CR13" id="ref-link-section-d16580660e687">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. Sci. Adv. 5, eaay6946 (2019)." href="#ref-CR14" id="ref-link-section-d16580660e687_1">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. Photon. Res. 8, 46–50 (2020)." href="#ref-CR15" id="ref-link-section-d16580660e687_2">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Furuhata, G., Niiyama, T. &amp; Sunada, S. Physical deep learning based on optimal control of dynamical systems. Phys. Rev. Appl. 15, 034092 (2021)." href="#ref-CR16" id="ref-link-section-d16580660e687_3">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lin, X. et al. All-optical machine learning using diffractive deep neural networks. Science 361, 1004–1008 (2018)." href="#ref-CR17" id="ref-link-section-d16580660e687_4">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miller, J. F., Harding, S. L. &amp; Tufte, G. Evolution-in-materio: evolving computation in materials. Evol. Intell. 7, 49–67 (2014)." href="#ref-CR18" id="ref-link-section-d16580660e687_5">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Chen, T. et al. Classification with a disordered dopant-atom network in silicon. Nature 577, 341–345 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR19" id="ref-link-section-d16580660e690">19</a></sup> themselves can, in principle, overcome this limitation. To train physical transformations experimentally, researchers have frequently relied on gradient-free learning algorithms<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR10" id="ref-link-section-d16580660e694">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Miller, J. F., Harding, S. L. &amp; Tufte, G. Evolution-in-materio: evolving computation in materials. Evol. Intell. 7, 49–67 (2014)." href="#ref-CR18" id="ref-link-section-d16580660e697">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen, T. et al. Classification with a disordered dopant-atom network in silicon. Nature 577, 341–345 (2020)." href="#ref-CR19" id="ref-link-section-d16580660e697_1">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Bueno, J. et al. Reinforcement learning in a large-scale photonic recurrent neural network. Optica 5, 756–760 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR20" id="ref-link-section-d16580660e700">20</a></sup>. Gradient-based learning algorithms, such as the backpropagation algorithm, are considered essential for the efficient training and good generalization of large-scale DNNs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Poggio, T., Banburski, A. &amp; Liao, Q. Theoretical issues in deep networks. Proc. Natl Acad. Sci. USA 117, 30039–30045 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR39" id="ref-link-section-d16580660e704">39</a></sup>. Thus, proposals to realize gradient-based training in physical hardware have appeared<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11 (2017)." href="#ref-CR40" id="ref-link-section-d16580660e708">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ernoult, M., Grollier, J., Querlioz, D., Bengio, Y. &amp; Scellier, B. Equilibrium propagation with continual weight updates Preprint at 
                  https://arxiv.org/abs/2005.04168
                  
                 (2020)." href="#ref-CR41" id="ref-link-section-d16580660e708_1">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Laborieux, A. et al. Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias. Front. Neurosci. 15 (2021)." href="#ref-CR42" id="ref-link-section-d16580660e708_2">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martin, E. et al. Eqspike: spike-driven equilibrium propagation for neuromorphic implementations. iScience 24, 102222 (2021)." href="#ref-CR43" id="ref-link-section-d16580660e708_3">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dillavou, S., Stern, M., Liu, A. J., &amp; Durian, D. J. Demonstration of decentralized, physics-driven learning. Preprint at 
                  https://arxiv.org/abs/2108.00275
                  
                 (2021)." href="#ref-CR44" id="ref-link-section-d16580660e708_4">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hermans, M., Burm, M., Van Vaerenbergh, T., Dambre, J. &amp; Bienstman, P. Trainable hardware for dynamical computing using error backpropagation through physical media. Nat. Commun. 6, 6729 (2015)." href="#ref-CR45" id="ref-link-section-d16580660e708_5">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Minkov, M., Shi, Y. &amp; Fan, S. Training of photonic neural networks through in situ backpropagation and gradient measurement. Optica 5, 864–871 (2018)." href="#ref-CR46" id="ref-link-section-d16580660e708_6">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lopez-Pastor, V. &amp; Marquardt, F. Self-learning machines based on Hamiltonian echo backpropagation. Preprint at 
                  https://arxiv.org/abs/2103.04992
                  
                 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR47" id="ref-link-section-d16580660e711">47</a></sup>. These inspiring proposals nonetheless make assumptions that exclude many physical systems, such as linearity, dissipation-free evolution or that the system be well described by gradient dynamics. The most general proposals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="#ref-CR13" id="ref-link-section-d16580660e715">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. Sci. Adv. 5, eaay6946 (2019)." href="#ref-CR14" id="ref-link-section-d16580660e715_1">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. Photon. Res. 8, 46–50 (2020)." href="#ref-CR15" id="ref-link-section-d16580660e715_2">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Furuhata, G., Niiyama, T. &amp; Sunada, S. Physical deep learning based on optimal control of dynamical systems. Phys. Rev. Appl. 15, 034092 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR16" id="ref-link-section-d16580660e718">16</a></sup> overcome such constraints by performing training in silico, that is, learning wholly within numerical simulations. Although the universality of in silico training is empowering, simulations of nonlinear physical systems are rarely accurate enough for models trained in silico to transfer accurately to real devices.</p><p>Here we demonstrate a universal framework using backpropagation to directly train arbitrary physical systems to execute DNNs, that is, PNNs. Our approach is enabled by a hybrid in situ–in silico algorithm, called physics-aware training (PAT). PAT allows us to execute the backpropagation algorithm efficiently and accurately on any sequence of physical input–output transformations. We demonstrate the universality of this approach by experimentally performing image classification using three distinct systems: the multimode mechanical oscillations of a driven metal plate, the analogue dynamics of a nonlinear electronicoscillator and ultrafast optical second-harmonic generation (SHG). We obtain accurate hierarchical classifiers that utilize each system’s unique physical transformations, and that inherently mitigate each system’s unique noise processes and imperfections. Although PNNs are a radical departure from traditional hardware, it is easy to integrate them into modern machine learning. We show that PNNs can be seamlessly combined with conventional hardware and neural network methods via physical–digital hybrid architectures, in which conventional hardware learns to opportunistically cooperate with unconventional physical resources using PAT. Ultimately, PNNs provide routes to improving the energy efficiency and speed of machine learning by many orders of magnitude, and pathways to automatically designing complex functional devices, such as functional nanoparticles<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Peurifoy, J. et al. Nanophotonic particle simulation and inverse design using artificial neural networks. Sci. Adv. 4, eaar4206 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR28" id="ref-link-section-d16580660e725">28</a></sup>, robots<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="de Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J. &amp; Kolter, J. Z. End-to-end differentiable physics for learning and control. Adv. Neural Inf. Process. Syst. 31, 7178–7189 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR25" id="ref-link-section-d16580660e729">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Degrave, J., Hermans, M., Dambre, J. &amp; Wyffels, F. A differentiable physics engine for deep learning in robotics. Front. Neurorobot. 13, 6 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR26" id="ref-link-section-d16580660e732">26</a></sup> and smart sensors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou, F. &amp; Chai, Y. Near-sensor and in-sensor computing. Nat. Electron. 3, 664–671 (2020)." href="#ref-CR30" id="ref-link-section-d16580660e736">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martel, J. N., Mueller, L. K., Carey, S. J., Dudek, P. &amp; Wetzstein, G. Neural sensors: learning pixel exposures for HDR imaging and video compressive sensing with programmable sensors. IEEE Trans. Pattern Anal. Mach. Intell. 42, 1642–1653 (2020)." href="#ref-CR31" id="ref-link-section-d16580660e736_1">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Mennel, L. et al. Ultrafast machine vision with 2D material neural network image sensors. Nature 579, 62–66 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR32" id="ref-link-section-d16580660e739">32</a></sup>.</p></div></div></section><section data-title="An example PNN based on nonlinear optics"><div id="Sec2-section"><h2 id="Sec2">An example PNN based on nonlinear optics</h2><div id="Sec2-content"><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2</a> shows an example PNN based on broadband optical pulse propagation in quadratic nonlinear media (ultrafast SHG). Ultrafast SHG realizes a physical computation roughly analogous to a nonlinear convolution, transforming the input pulse’s near-infrared spectrum (about 800-nm centre wavelength) into the blue (about 400 nm) through a multitude of nonlinear frequency-mixing processes (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a>). To control this computation, input data and parameters are encoded into sections of the spectrum of the near-infrared pulse by modulating its frequency components using a pulse shaper (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2a</a>). This pulse then propagates through a nonlinear crystal, producing a blue pulse whose spectrum is measured to read out the result of the physical computation.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="An example PNN, implemented experimentally using broadband optical SHG."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: An example PNN, implemented experimentally using broadband optical SHG.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://jaked.org/articles/s41586-021-04223-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="685"/></picture></a></div><p><b>a</b>, Input data are encoded into the spectrum of a laser pulse (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a>, Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">2</a>). To control transformations implemented by the broadband SHG process, a portion of the pulse’s spectrum is used as trainable parameters (orange). The physical computation result is obtained from the spectrum of a blue (about 390 nm) pulse generated within a <i>χ</i><sup>(2)</sup> medium. <b>b</b>, To construct a deep PNN, the outputs of the SHG transformations are used as inputs to subsequent SHG transformations, with independent trainable parameters. <b>c</b>, <b>d</b>, After training the SHG-PNN (see main text, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3</a>), it classifies test vowels with 93% accuracy. <b>c</b>, The confusion matrix for the PNN on the test set. <b>d</b>, Representative examples of final-layer output spectra, which show the SHG-PNN’s prediction.</p></div></figure></div><p>To realize vowel classification with SHG, we construct a multilayer SHG-PNN (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2b</a>) where the input data for the first physical layer consist of a vowel-formant frequency vector. After the final physical layer, the blue output spectrum is summed using a digital computer into seven spectral bins (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2b, d</a>, Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">21</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">22</a>). The predicted vowel is identified by the bin with the maximum energy (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2c</a>). In each layer, the output spectrum is digitally renormalized before being passed to the next layer (via the pulse shaper), along with a trainable digital rescaling. Mathematically, this transformation is given by <span>\({{\bf{x}}}^{[l+1]}=\frac{{a{\bf{y}}}^{[l]}}{max({{\bf{y}}}^{[l]})}+b\)</span>, where <b>x</b><sup>[<i>l</i>]</sup> and <b>y</b><sup>[<i>l</i>]</sup> are the inputs and outputs of the [<i>l</i>]th layer, respectively, and <i>a</i> and <i>b</i> are scalar parameters of the transformation.  Thus, the SHG-PNN’s computations are carried out almost entirely by the trained optical transformations, without digital activation functions or output layers.</p><p>Deep PNNs essentially combine the computational philosophy of techniques such as PRC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Tanaka, G. et al. Recent advances in physical reservoir computing: a review. Neural Netw. 115, 100–123 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR21" id="ref-link-section-d16580660e968">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Appeltant, L. et al. Information processing using a single dynamical node as complex system. Nat. Commun. 2, 468 (2011)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR22" id="ref-link-section-d16580660e971">22</a></sup> with the trained hierarchical computations and gradient-based training of deep learning. In PRC, a physical system, often with recurrent dynamics, is used as an untrained feature map and a trained linear output layer (typically on a digital computer) combines these features to approximate desired functions. In PNNs, the backpropagation algorithm is used to adjust physical parameters so that a sequence of physical systems performs desired computations physically, without needing an output layer. For additional details, see Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">3</a>.</p></div></div></section><section data-title="Physics-aware training"><div id="Sec3-section"><h2 id="Sec3">Physics-aware training</h2><div id="Sec3-content"><p>To train the PNNs’ parameters using backpropagation, we use PAT (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3</a>). In the backpropagation algorithm, automatic differentiation determines the gradient of a loss function with respect to trainable parameters. This makes the algorithm <i>N</i>-times more efficient than finite-difference methods for gradient estimation (where <i>N</i> is the number of parameters). The key component of PAT is the use of mismatched forward and backward passes in executing the backpropagation algorithm. This technique is well known in neuromorphic computing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R. &amp; Bengio, Y. Quantized neural networks: training neural networks with low precision weights and activations. J. Mach. Learn. Res. 18, 6869–6898 (2017)." href="#ref-CR48" id="ref-link-section-d16580660e995">48</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Frye, R. C., Rietman, E. A. &amp; Wong, C. C. Back-propagation learning and nonidealities in analog neural network hardware. IEEE Trans. Neural Netw. 2, 110–117 (1991)." href="#ref-CR49" id="ref-link-section-d16580660e995_1">49</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cramer, B. et al. Surrogate gradients for analog neuromorphic computing. Preprint at 
                  https://arxiv.org/abs/2006.07239
                  
                 (2020)." href="#ref-CR50" id="ref-link-section-d16580660e995_2">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Adhikari, S. P. et al. Memristor bridge synapse-based neural network and its learning. IEEE Trans Neural Netw. Learn. Syst. 23,1426–1435 (2012)." href="#ref-CR51" id="ref-link-section-d16580660e995_3">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. Nat. Commun. 7, 13276 (2016)." href="#ref-CR52" id="ref-link-section-d16580660e995_4">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Launay, J., Poli, I., Boniface, F., &amp; Krzakala, F. Direct feedback alignment scales to modern deep learning tasks and architectures. Preprint at 
                  https://arxiv.org/abs/2006.12878
                  
                 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR53" id="ref-link-section-d16580660e998">53</a></sup>, appearing recently in direct feedback alignment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. Nat. Commun. 7, 13276 (2016)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR52" id="ref-link-section-d16580660e1002">52</a></sup> and quantization-aware training<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R. &amp; Bengio, Y. Quantized neural networks: training neural networks with low precision weights and activations. J. Mach. Learn. Res. 18, 6869–6898 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR48" id="ref-link-section-d16580660e1007">48</a></sup>, which inspired PAT. PAT generalizes these strategies to encompass arbitrary physical layers, arbitrary physical network architectures and, more broadly, to differentially programmable physical devices.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Physics-aware training."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Physics-aware training.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://jaked.org/articles/s41586-021-04223-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="445"/></picture></a></div><p><b>a</b>, PAT is a hybrid in situ–in silico algorithm to apply backpropagation to train controllable physical parameters so that physical systems perform machine-learning tasks accurately even in the presence of modelling errors and physical noise. Instead of performing the training solely within a digital model (in silico), PAT uses the physical systems to compute forward passes. Although only one layer is depicted in <b>a</b>, PAT generalizes naturally to multiple layers (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a>). <b>b</b>, Comparison of the validation accuracy versus training epoch with PAT and in silico training, for the experimental SHG-PNN depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2b</a>. <b>c</b>, Final experimental test accuracy for PAT and in silico training for SHG-PNNs with increasing numbers of physical layers. The length of error bars represent two standard errors.</p></div></figure></div><p>PAT proceeds as follows (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3</a>). First, training input data (for example, an image) are input to the physical system, along with trainable parameters. Second, in the forward pass, the physical system applies its transformation to produce an output. Third, the physical output is compared with the intended output to compute the error. Fourth, using a differentiable digital model, the gradient of the loss is estimated with respect to the controllable parameters. Finally, the parameters are updated according to the inferred gradient. This process is repeated, iterating over training examples, to reduce the error. See <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a> for the intuition behind why PAT works and the general multilayer algorithm.</p><p>The essential advantages of PAT stem from the forward pass being executed by the actual physical hardware, rather than by a simulation. Our digital model for SHG is very accurate (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">20</a>) and includes an accurate noise model (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">18</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">19</a>). However, as evidenced by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3b</a>, in silico training with this model still fails, reaching a maximum vowel-classification accuracy of about 40%. In contrast, PAT succeeds, accurately training the SHG-PNN, even when additional layers are added (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3b, c</a>).</p></div></div></section><section data-title="Diverse PNNs for image classification"><div id="Sec4-section"><h2 id="Sec4">Diverse PNNs for image classification</h2><div id="Sec4-content"><p>PNNs can learn to accurately perform more complex tasks, can be realized with virtually any physical system and can be designed with a variety of physical network architectures. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4</a>, we present three PNN classifiers for the MNIST (Modified National Institute of Standards and Technology database) handwritten digit classification task, based on three distinct physical systems. For each physical system, we also demonstrate a different PNN architecture, illustrating the variety of physical networks possible. In all cases, models were constructed and trained using PyTorch<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8024–8035 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR54" id="ref-link-section-d16580660e1086">54</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Image classification with diverse physical systems."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Image classification with diverse physical systems.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://jaked.org/articles/s41586-021-04223-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="674"/></picture></a></div><p>We trained PNNs based on three physical systems (mechanics, electronics and optics) to classify images of handwritten digits. <b>a</b>, The mechanical PNN: the multimode oscillations of a metal plate are driven by time-dependent forces that encode the input image data and parameters. <b>b</b>, The mechanical PNN multilayer architecture. <b>c</b>, The validation classification accuracy versus training epoch for the mechanical PNN trained using PAT. The same curves are shown also for a reference model where the physical transformations implemented by the speaker are replaced by identity operations. <b>d</b>, Confusion matrix for the mechanical PNN after training. <b>e</b>–<b>h</b>, The same as <b>a</b>–<b>d</b>, respectively, but for a nonlinear analogue-electronic PNN. <b>i</b>–<b>l</b>, The same as <b>a</b>–<b>d</b>, respectively, for a hybrid physical–digital PNN based on broadband optical SHG. The final test accuracy is 87%, 93% and 97% for the mechanical, electronic and optics-based PNNs, respectively.</p></div></figure></div><p>In the mechanical PNN (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4a–d</a>), a metal plate is driven by time-varying forces, which encode both input data and trainable parameters. The plate’s multimode oscillations enact controllable convolutions on the input data (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">16</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">17</a>). Using the plate’s trainable transformation sequentially three times, we classify 28-by-28 (784 pixel) images that are input as an unrolled time series. To control the transformations of each physical layer, we train element-wise rescaling of the forces applied to the plate (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4b</a>, <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a>). PAT trains the three-layer mechanical PNN to 87% accuracy, close to a digital linear classifier<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324 (1998)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR55" id="ref-link-section-d16580660e1167">55</a></sup>. When the mechanical computations are replaced by identity operations, and only the digital rescaling operations are trained, the performance of the model is equivalent to random guessing (10%). This shows that most of the PNN’s functionality comes from the controlled physical transformations.</p><p>An analogue-electronic PNN is implemented with a circuit featuring a transistor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4e–h</a>), which results in a noisy, nonlinear transient response (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">12</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">13</a>). The usage and architecture of the electronic PNN are mostly similar to that of the mechanical PNN, with several minor differences (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://jaked.org/articles/s41586-021-04223-6#Sec6">Methods</a>). When trained using PAT, the analogue-electronic PNN performs the classification task with 93% test accuracy.</p><p>Using broadband SHG, we demonstrate a physical–digital hybrid PNN (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4i–l</a>). This hybrid PNN involves trainable digital linear input layers followed by trainable ultrafast SHG transformations. The trainable SHG transformations boost the performance of the digital baseline from roughly 90% accuracy to 97%. The classification task’s difficulty is nonlinear with respect to accuracy, so this improvement typically requires increasing the number of digital operations by around one order of magnitude<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324 (1998)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR55" id="ref-link-section-d16580660e1192">55</a></sup>. This illustrates how a hybrid physical–digital PNN can automatically learn to offload portions of a computation from an expensive digital processor to a fast, energy-efficient physical co-processor.</p><p>To show the potential for PNNs to perform more challenging tasks, we simulated a multilayer PNN based on a nonlinear oscillator network. This PNN is trained with PAT to perform the MNIST task with 99.1% accuracy, and the Fashion-MNIST task, which is considered significantly harder<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Xiao, H., Rasul, K., &amp; Vollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at 
                  https://arxiv.org/abs/1708.07747
                  
                 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR56" id="ref-link-section-d16580660e1200">56</a></sup>, with 90% accuracy, in both cases with simulated physical noise, and with mismatch between model and simulated experiment of over 20% (Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">4</a>).</p></div></div></section><section data-title="Discussion"><div id="Sec5-section"><h2 id="Sec5">Discussion</h2><div id="Sec5-content"><p>Our results show that controllable physical systems can be trained to execute DNN calculations. Many systems that are not conventionally used for computation appear to offer, in principle, the capacity to perform parts of machine-learning-inference calculations orders of magnitude faster and more energy-efficiently than conventional hardware (Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">5</a>). However, there are two caveats to note. First, owing to underlying symmetries and other constraints, some systems may be well suited for accelerating a restricted class of computations that share the same constraints. Second, PNNs trained using PAT can only provide significant benefits during inference, as PAT uses a digital model. Thus, as in the hybrid network presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4i–l</a>, we expect such PNNs to serve as a resource, rather than as a complete replacement, for conventional general-purpose hardware (Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">5</a>).</p><p>Techniques for training hardware in situ<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kohda, Y. et al. Unassisted true analog neural network training chip. In 2020 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR7" id="ref-link-section-d16580660e1227">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. Front. Comput. Neurosci. 11 (2017)." href="#ref-CR40" id="ref-link-section-d16580660e1230">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ernoult, M., Grollier, J., Querlioz, D., Bengio, Y. &amp; Scellier, B. Equilibrium propagation with continual weight updates Preprint at 
                  https://arxiv.org/abs/2005.04168
                  
                 (2020)." href="#ref-CR41" id="ref-link-section-d16580660e1230_1">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Laborieux, A. et al. Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias. Front. Neurosci. 15 (2021)." href="#ref-CR42" id="ref-link-section-d16580660e1230_2">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martin, E. et al. Eqspike: spike-driven equilibrium propagation for neuromorphic implementations. iScience 24, 102222 (2021)." href="#ref-CR43" id="ref-link-section-d16580660e1230_3">43</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dillavou, S., Stern, M., Liu, A. J., &amp; Durian, D. J. Demonstration of decentralized, physics-driven learning. Preprint at 
                  https://arxiv.org/abs/2108.00275
                  
                 (2021)." href="#ref-CR44" id="ref-link-section-d16580660e1230_4">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hermans, M., Burm, M., Van Vaerenbergh, T., Dambre, J. &amp; Bienstman, P. Trainable hardware for dynamical computing using error backpropagation through physical media. Nat. Commun. 6, 6729 (2015)." href="#ref-CR45" id="ref-link-section-d16580660e1230_5">45</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Minkov, M., Shi, Y. &amp; Fan, S. Training of photonic neural networks through in situ backpropagation and gradient measurement. Optica 5, 864–871 (2018)." href="#ref-CR46" id="ref-link-section-d16580660e1230_6">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Lopez-Pastor, V. &amp; Marquardt, F. Self-learning machines based on Hamiltonian echo backpropagation. Preprint at 
                  https://arxiv.org/abs/2103.04992
                  
                 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR47" id="ref-link-section-d16580660e1233">47</a></sup> and methods for reliable in silico training (for example, refs. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Spoon, K. et al. Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. Front. Comput. Neurosci. 53, (2021)." href="#ref-CR57" id="ref-link-section-d16580660e1237">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kariyappa, S. et al. Noise-resilient DNN: tolerating noise in PCM-based AI accelerators via noise-aware training. IEEE Trans. Electron Devices 68, 4356–4362 (2021)." href="#ref-CR58" id="ref-link-section-d16580660e1237_1">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gokmen, T., Rasch, M. J. &amp; Haensch. W. The marriage of training and inference for scaled deep learning analog hardware. In 2019 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2019)." href="#ref-CR59" id="ref-link-section-d16580660e1237_2">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Rasch, M. J. et al. A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays. In 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS) (IEEE, 2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR60" id="ref-link-section-d16580660e1240">60</a></sup>) complement these weaknesses. Devices trained using in situ learning algorithms will perform learning entirely in hardware, potentially realizing learning faster and more energy-efficiently than current approaches. Such devices are suited to settings in which frequent retraining is required. However, to perform both learning and inference, these devices have more specific hardware requirements than inference-only hardware, which may limit their achievable inference performance. In silico training can train many physical parameters of a device, including ones set permanently during fabrication<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal-oxide memristors. Nature 521, 61–64 (2015)." href="#ref-CR12" id="ref-link-section-d16580660e1244">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="#ref-CR13" id="ref-link-section-d16580660e1244_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. Sci. Adv. 5, eaay6946 (2019)." href="#ref-CR14" id="ref-link-section-d16580660e1244_2">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. Photon. Res. 8, 46–50 (2020)." href="#ref-CR15" id="ref-link-section-d16580660e1244_3">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Furuhata, G., Niiyama, T. &amp; Sunada, S. Physical deep learning based on optimal control of dynamical systems. Phys. Rev. Appl. 15, 034092 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR16" id="ref-link-section-d16580660e1247">16</a></sup>. As the resulting hardware will not perform learning, it can be optimized for inference. Although accurate, large-scale in silico training has been implemented<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Burr, G. W. et al. Neuromorphic computing using non-volatile memory. Adv. Phys. X 2, 89–124 (2017)." href="#ref-CR4" id="ref-link-section-d16580660e1251">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Khaddam-Aljameh, R. et al. HERMES core—a 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In 2021 Symposium on VLSI Circuits (IEEE, 2021)." href="#ref-CR5" id="ref-link-section-d16580660e1251_1">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Narayanan, P. et al. Fully on-chip MAC at 14nm enabled by accurate row-wise programming of PCM-based weights and parallel vector-transport in duration-format. In 2021 Symposium on VLSI Technology (IEEE, 2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR6" id="ref-link-section-d16580660e1254">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Spoon, K. et al. Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. Front. Comput. Neurosci. 53, (2021)." href="#ref-CR57" id="ref-link-section-d16580660e1257">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kariyappa, S. et al. Noise-resilient DNN: tolerating noise in PCM-based AI accelerators via noise-aware training. IEEE Trans. Electron Devices 68, 4356–4362 (2021)." href="#ref-CR58" id="ref-link-section-d16580660e1257_1">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gokmen, T., Rasch, M. J. &amp; Haensch. W. The marriage of training and inference for scaled deep learning analog hardware. In 2019 IEEE International Electron Devices Meeting (IEDM) (IEEE, 2019)." href="#ref-CR59" id="ref-link-section-d16580660e1257_2">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Rasch, M. J. et al. A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays. In 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS) (IEEE, 2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR60" id="ref-link-section-d16580660e1260">60</a></sup>, this has been achieved with only analogue electronics, for which accurate simulations and controlled fabrication processes are available. PAT may be used in settings where a simulation–reality gap cannot be avoided, such as if hardware is designed at the limit of fabrication tolerances, operated outside usual regimes or based on platforms other than conventional electronics.</p><p>Improvements to PAT could extend the utility of PNNs. For example, PAT’s backward pass could be replaced by a neural network that directly estimates parameter updates for the physical system. Implementing this ‘teacher’ neural network with a PNN would allow subsequent training to be performed without digital assistance.</p><p>This work has focused so far on the potential application of PNNs as accelerators for machine learning, but PNNs are promising for other applications as well, particularly those in which physical, rather than digital, data are processed or produced. PNNs can perform computations on data within its physical domain, allowing for smart sensors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou, F. &amp; Chai, Y. Near-sensor and in-sensor computing. Nat. Electron. 3, 664–671 (2020)." href="#ref-CR30" id="ref-link-section-d16580660e1270">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martel, J. N., Mueller, L. K., Carey, S. J., Dudek, P. &amp; Wetzstein, G. Neural sensors: learning pixel exposures for HDR imaging and video compressive sensing with programmable sensors. IEEE Trans. Pattern Anal. Mach. Intell. 42, 1642–1653 (2020)." href="#ref-CR31" id="ref-link-section-d16580660e1270_1">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Mennel, L. et al. Ultrafast machine vision with 2D material neural network image sensors. Nature 579, 62–66 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR32" id="ref-link-section-d16580660e1273">32</a></sup> that pre-process information before conversion to the electronic domain (for example, a low-power, microphone-coupled circuit tuned to recognize specific hotwords). As the achievable sensitivity, resolution and energy efficiency of many sensors is limited by conversion of information to the digital electronic domain, and by processing of that data in digital electronics, PNN sensors should have advantages. More broadly, with PAT, one is simply training the complex functionality of physical systems. Although machine learning and sensing are important functionalities, they are but two of many<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mouret, J.-B. &amp; Chatzilygeroudis, K. 20 years of reality gap: a few thoughts about simulators in evolutionary robotics. In Proc. Genetic and Evolutionary Computation Conference Companion 1121–1124 (2017)." href="#ref-CR23" id="ref-link-section-d16580660e1277">23</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Howison, T., Hauser, S., Hughes, J. &amp; Iida, F. Reality-assisted evolution of soft robots through large-scale physical experimentation: a review. Artif. Life 26, 484–506 (2021)." href="#ref-CR24" id="ref-link-section-d16580660e1277_1">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="de Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J. &amp; Kolter, J. Z. End-to-end differentiable physics for learning and control. Adv. Neural Inf. Process. Syst. 31, 7178–7189 (2018)." href="#ref-CR25" id="ref-link-section-d16580660e1277_2">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Degrave, J., Hermans, M., Dambre, J. &amp; Wyffels, F. A differentiable physics engine for deep learning in robotics. Front. Neurorobot. 13, 6 (2019)." href="#ref-CR26" id="ref-link-section-d16580660e1277_3">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Molesky, S. et al. Inverse design in nanophotonics. Nat. Photon. 12, 659–670 (2018)." href="#ref-CR27" id="ref-link-section-d16580660e1277_4">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Peurifoy, J. et al. Nanophotonic particle simulation and inverse design using artificial neural networks. Sci. Adv. 4, eaar4206 (2018)." href="#ref-CR28" id="ref-link-section-d16580660e1277_5">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Stern, M., Arinze, C., Perez, L., Palmer, S. E. &amp; Murugan, A. Supervised learning through physical changes in a mechanical system. Proc. Natl Acad. Sci. USA 117, 14843–14850 (2020)." href="#ref-CR29" id="ref-link-section-d16580660e1277_6">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhou, F. &amp; Chai, Y. Near-sensor and in-sensor computing. Nat. Electron. 3, 664–671 (2020)." href="#ref-CR30" id="ref-link-section-d16580660e1277_7">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Martel, J. N., Mueller, L. K., Carey, S. J., Dudek, P. &amp; Wetzstein, G. Neural sensors: learning pixel exposures for HDR imaging and video compressive sensing with programmable sensors. IEEE Trans. Pattern Anal. Mach. Intell. 42, 1642–1653 (2020)." href="#ref-CR31" id="ref-link-section-d16580660e1277_8">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Mennel, L. et al. Ultrafast machine vision with 2D material neural network image sensors. Nature 579, 62–66 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR32" id="ref-link-section-d16580660e1280">32</a></sup> that PAT, and the concept of PNNs, could be applied to.</p></div></div></section><section data-title="Methods"><div id="Sec6-section"><h2 id="Sec6">Methods</h2><div id="Sec6-content"><h3 id="Sec7">Physics-aware training</h3><p>To train the PNNs presented in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2</a>–<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4</a>, we used PAT to enable us to perform backpropagation on the physical apparatuses as automatic differentiation (autodiff) functions within PyTorch<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8024–8035 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR54" id="ref-link-section-d16580660e1303">54</a></sup> (v1.6). We used PyTorch Lightning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Falcon, W. et al. PyTorch Lightning (2019); 
                  https://github.com/PyTorchLightning/pytorch-lightning
                  
                " href="https://jaked.org/articles/s41586-021-04223-6#ref-CR61" id="ref-link-section-d16580660e1307">61</a></sup> (v0.9) and Weights and Biases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Biewald, L. Experiment Tracking with Weights and Biases (2020); 
                  https://www.wandb.com/
                  
                " href="https://jaked.org/articles/s41586-021-04223-6#ref-CR62" id="ref-link-section-d16580660e1311">62</a></sup> (v0.10) during development as well. PAT is explained in detail in Supplementary Section 1, where it is compared with standard backpropagation, and training physical devices in silico. Here we provide only an overview of PAT in the context of a generic multilayer PNN (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">2</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">3</a>).</p><p>PAT can be formalized by the use of custom constituent autodiff functions for the physically executed submodules in an overall network architecture (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">1</a>). In PAT, each physical system’s forward functionality is provided by the system’s own controllable physical transformation, which can be thought of as a parameterized function <span>\({f}_{{\rm{p}}}\)</span> that relates the input <b>x</b>, parameters <b>θ</b>, and outputs <b>y</b> of the transformation via <b>y</b> = <i>f</i><sub>p</sub> (<b>x</b>,<b>θ</b>). As a physical system cannot be auto-differentiated, we use a differentiable digital model <span>\({f}_{{\rm{m}}}\)</span> to approximate each backward pass through a given physical module. This structure is essentially a generalization of quantization-aware training<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R. &amp; Bengio, Y. Quantized neural networks: training neural networks with low precision weights and activations. J. Mach. Learn. Res. 18, 6869–6898 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR48" id="ref-link-section-d16580660e1422">48</a></sup>, in which low-precision neural network hardware is approximated by quantizing weights and activation values on the forward pass, but storing weights and activations, and performing the backward pass with full precision.</p><p>To see how this works, we consider here the specific case of a multilayer feedforward PNN with standard stochastic gradient descent. In this case, the PAT algorithm with the above-defined custom autodiff functions results in the following training loop:</p><p>Perform forward pass:</p><div id="Equ1"><p><span>$${{\bf{x}}}^{[l+1]}={{\boldsymbol{y}}}^{[l]}={f}_{{\rm{p}}}({{\bf{x}}}^{[l]},{{\boldsymbol{\theta }}}^{[l]})$$</span></p><p>
                    (1)
                </p></div><p>Compute (exact) error vector:</p><div id="Equ2"><p><span>$${g}_{{{\bf{y}}}^{[N]}}=\frac{\partial L}{\partial {{\bf{y}}}^{[N]}}=\frac{\partial {\mathscr{l}}}{\partial {{\bf{y}}}^{\left[N\right]}}({{\bf{y}}}^{\left[N\right]},{{\bf{y}}}_{{\rm{target}}})$$</span></p><p>
                    (2)
                </p></div><p>Perform backward pass</p><div id="Equ3"><p><span>$${g}_{{{\bf{y}}}^{[l-1]}}={\left[\frac{{\rm{\partial }}{f}_{{\rm{m}}}}{{\rm{\partial }}{\bf{x}}}({{\bf{x}}}^{[l]},{{\boldsymbol{\theta }}}^{[l]})\right]}^{{\rm{T}}}{g}_{{{\bf{y}}}^{[l]}}$$</span></p><p>
                    (3a)
                </p></div><div id="Equ4"><p><span>$${g}_{{{\boldsymbol{\theta }}}^{\left[l-1\right]}}={\left[\frac{\partial {f}_{{\rm{m}}}}{\partial {\boldsymbol{\theta }}}({{\bf{x}}}^{\left[l\right]},{{\boldsymbol{\theta }}}^{\left[l\right]})\right]}^{{\rm{T}}}{g}_{{{\bf{y}}}^{\left[l\right]}}$$</span></p><p>
                    (3b)
                </p></div><p>Update parameters:</p><div id="Equ5"><p><span>$${{\boldsymbol{\theta }}}^{\left[l\right]}\to {{\boldsymbol{\theta }}}^{\left[l\right]}-\eta \frac{1}{{N}_{{\rm{data}}}}\sum _{k}{g}_{{{\boldsymbol{\theta }}}^{\left[l\right]}}^{(k)}$$</span></p><p>
                    (4)
                </p></div><p>where <span>\({g}_{{{\boldsymbol{\theta }}}^{\left[l\right]}}\)</span> and <span>\({g}_{{{\bf{y}}}^{\left[l\right]}}\)</span> are estimators of the physical systems’ exact gradients, <span>\(\frac{\partial L}{\partial {{\boldsymbol{\theta }}}^{[l]}}\)</span> and <span>\(\frac{\partial L}{\partial {{\bf{y}}}^{[l]}}\)</span>, respectively for the <span>\([l]\)</span>th layer, obtained by auto-differentiation of the model, <span>\(L\)</span> is the loss, <span>\({\mathscr{l}}\)</span> is the loss function (for example, cross-entropy or mean-squared error), <span>\({{\bf{y}}}_{{\rm{target}}}\)</span> is the desired (target) output, <span>\({N}_{{\rm{data}}}\)</span> is the size of the batch and <span>\(\eta \)</span> is the learning rate. <span>\({{\bf{x}}}^{[l+1]}\)</span> is the input vector to the <span>\([l+1]\)</span>th layer, which for the hidden layers of the feedforward architecture is equal to the output vector of the previous layer, <span>\({{\bf{x}}}^{[l+1]}={{\bf{y}}}^{[l]}={f}_{{\rm{p}}}\left({{\bf{x}}}^{\left[l\right]},{{\boldsymbol{\theta }}}^{\left[l\right]}\right)\)</span>, where <span>\({{\boldsymbol{\theta }}}^{[l]}\)</span> is the controllable (trainable) parameter vector for the <span>\([l]\)</span>th layer. For the first layer, the input data vector <span>\({{\bf{x}}}^{\left[1\right]}\)</span> is the data to be operated on. In PAT, the error vector is exactly estimated (<span>\({g}_{{{\bf{y}}}^{\left[N\right]}}=\frac{\partial L}{\partial {{\bf{y}}}^{[N]}}\)</span>) as the forward pass is performed by the physical system. This error vector is then backpropagated via equation (3), which involves Jacobian matrices of the differential digital model evaluated at the correct inputs at each layer (that is, the actual physical inputs) <span>\({\left[\frac{{\rm{\partial }}{f}_{{\rm{m}}}}{{\rm{\partial }}{\bf{x}}}({{\bf{x}}}^{[l]},{{\boldsymbol{\theta }}}^{[l]})\right]}^{{\rm{T}}}\)</span>, where T represents the transpose operation. Thus, in addition to utilizing the output of the PNN (<span>\({{\bf{y}}}^{[N]}\)</span>) via physical computations in the forward pass, intermediate outputs (<span>\({{\bf{y}}}^{[l]}\)</span>) are also utilized to facilitate the computation of accurate gradients in PAT.</p><p>As it is implemented just by defining a custom autodiff function, generalizing PAT for more complex architectures, such as multichannel or hybrid physical–digital models, with different loss functions and so on is straightforward. See Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">1</a> for details.</p><p>An intuitive motivation for why PAT works is that the training’s optimization of parameters is always grounded in the true optimization landscape by the physical forward pass. With PAT, even if gradients are estimated only approximately, the true loss function is always precisely known. As long as the gradients estimated by the backward pass are reasonably accurate, optimization will proceed correctly. Although the required training time is expected to increase as the error in gradient estimation increases, in principle it is sufficient for the estimated gradient to be pointing closer to the direction of the true gradient than its opposite (that is, that the dot product of the estimated and true gradients is positive). Moreover, by using the physical system in the forward pass, the true output from each intermediate layer is also known, so gradients of intermediate physical layers are always computed with respect to correct inputs. In any form of in silico training, compounding errors build up through the imperfect simulation of each physical layer, leading to a rapidly diverging simulation–reality gap as training proceeds (see Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">1</a> for details). As a secondary benefit, PAT ensures that learned models are inherently resilient to noise and other imperfections beyond a digital model, as the change of loss along noisy directions in parameter space will tend to average to zero. This makes training robust to, for example, device–device variations, and facilitates the learning of noise-resilient (and, more speculatively, noise-enhanced) models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Marković, D., Mizrahi, A., Querlioz, D. &amp; Grollier, J. Physics for neuromorphic computing. Nat. Rev. Phys. 2, 499–510 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR8" id="ref-link-section-d16580660e3161">8</a></sup>.</p><h3 id="Sec8">Differentiable digital models</h3><p>To perform PAT, a differentiable digital model of the physical system’s input–output transformation is required. Any model, <span>\({f}_{{\rm{m}}}\)</span>, of the physical system’s true forward function, <span>\({f}_{{\rm{p}}}\)</span>, can be used to perform PAT, so long as it can be auto-differentiated. Viable approaches include traditional physics models, black-box machine-learning models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. Nat. Nanotechnol. 15, 992–998 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR13" id="ref-link-section-d16580660e3243">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Kasim, M. F. et al. Building high accuracy emulators for scientific simulations with deep neural architecture search. Preprint at 
                  https://arxiv.org/abs/2001.08055
                  
                 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR63" id="ref-link-section-d16580660e3246">63</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Rahmani, B. et al. Actor neural networks for the robust control of partially measured nonlinear systems showcased for image propagation through diffuse media. Nat. Mach. Intell. 2, 403–410 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR64" id="ref-link-section-d16580660e3249">64</a></sup> and physics-informed machine-learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Karniadakis, G. E. et al. Physics-informed machine learning. Nat. Rev. Phys. 3, 422–440 (2021)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR65" id="ref-link-section-d16580660e3253">65</a></sup> models.</p><p>In this work, we used the black-box strategy for our differentiable digital models, namely DNNs trained on input–output vector pairs from the physical systems as <span>\({f}_{{\rm{m}}}\)</span> (except for the mechanical system). Two advantages of this approach are that it is fully general (it can be applied even to systems in which one has no underlying knowledge-based model of the system) and that the accuracy can be extremely high, at least for physical inputs, <span>\(({\bf{x}},{\boldsymbol{\theta }})\)</span>, within the distribution of the training data (for out-of-distribution generalization, we expect physics-based approaches to offer advantages). In addition, the fact that each physical system has a precise corresponding DNN means that the resulting PNN can be analysed as a network of DNNs, which may be useful for explaining the PNN’s learned physical algorithm.</p><p>For our DNN differentiable digital models, we used a neural architecture search<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Akiba, T., Sano, S., Yanase, T., Ohta, T. &amp; Koyama, M. Optuna: a next-generation hyperparameter optimization framework. In Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining 2623–2631 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR66" id="ref-link-section-d16580660e3328">66</a></sup> to optimize hyperparameters, including the learning rate, number of layers and number of hidden units in each layer. Typical optimal architectures involved 3–5 layers with 200–1,000 hidden units in each, trained using the Adam optimizer, mean-squared loss function and learning rates of around 10<sup>−4</sup>. For more details, see Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">2D.1</a>.</p><p>For the nonlinear optical system, the test accuracy of the trained digital model (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">20</a>) shows that the model is remarkably accurate compared with typical simulation–experiment agreement in broadband nonlinear optics, especially considering that the pulses used exhibit a complex spatiotemporal structure owing to the pulse shaper. The model is not, however, an exact description of the physical system: the typical error for each element of the output vector is about 1–2%. For the analogue electronic circuit, agreement is also good, although worse than the other systems (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">23</a>), corresponding to around 5–10% prediction error for each component of the output vector. For the mechanical system, we found that a linear model was sufficient to obtain excellent agreement, which resulted in a typical error of about 1% for each component of the output vector (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">26</a>).</p><h3 id="Sec9">In silico training</h3><p>To train PNNs in silico, we applied a training loop similar to the one described above for PAT except that both the forward and backward passes are performed using the model (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">1</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">3</a>), with one exception noted below.</p><p>To improve the performance of in silico training as much as possible and permit the fairest comparison with PAT, we also modelled the input-dependent noise of the physical system and used this within the forward pass of in silico training. To do this, we trained, for each physical system, an additional DNN to predict the eigenvectors of the output vector’s noise covariance matrix, as a function of the physical system’s input vector and parameter vector. These noise models thus provided an input- and parameter-dependent estimate of the distribution of noise in the output vector produced by the physical system. We were able to achieve excellent agreement between the noise models’ predicted noise distributions and experimental measurements (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">18</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">19</a>). We found that including this noise model improved the performance of experiments performed using parameters derived from in silico training. Consequently, all in silico training results presented in this paper make use of such a model, except for the mechanical system, where a simpler, uniform noise model was found to be sufficient. For additional details, see Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">2D.2</a>.</p><p>Although including complex, accurate noise models does not allow in silico training to perform as well as PAT, we recommend that such models be used whenever in silico training is performed, such as for physical architecture search and design and possibly pre-training (Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">5</a>), as the correspondence with experiment (and, in particular, the predicted peak accuracy achievable there) is significantly improved over simpler noise models, or when ignoring physical noise.</p><h3 id="Sec10">Ultrafast nonlinear optical pulse propagation experiments</h3><p>For experiments with ultrafast nonlinear pulse propagation in quadratic nonlinear media (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">8</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">10</a>), we shaped pulses from a mode-locked titanium:sapphire laser (Spectra Physics Tsunami, centred around 780 nm and pulse duration around 100 fs) using a custom pulse shaper. Our optical pulse shaper used a digital micromirror device (DMD, Vialux V-650L) and was inspired by the design in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Liu, W. et al. Programmable controlled mode-locked fiber laser using a digital micromirror device. Opt. Lett. 42, 1923–1926 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR67" id="ref-link-section-d16580660e3395">67</a></sup>. Despite the binary modulations of the individual mirrors, we were able to achieve multilevel spectral amplitude modulation by varying the duty cycle of gratings written to the DMD along the dimension orthogonal to the diffraction of the pulse frequencies. To control the DMD, we adapted code developed for ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Matthès, M. W., del Hougne, P., de Rosny, J., Lerosey, G. &amp; Popoff, S. M. Optical complex media as universal reconfigurable linear operators. Optica 6, 465–472 (2019)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR68" id="ref-link-section-d16580660e3399">68</a></sup>, which is available at ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Popoff, S. M. &amp; Matthès, M. W. ALP4lib: q Python wrapper for the Vialux ALP-4 controller suite to control DMDs. Zenodo 
                  https://doi.org/10.5281/zenodo.4076193
                  
                 (2020)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR69" id="ref-link-section-d16580660e3403">69</a></sup>.</p><p>After being shaped by the pulse shaper, the femtosecond pulses were focused into a 0.5-mm-long beta-barium borate crystal. The multitude of frequencies within the broadband pulses then undergo various nonlinear optical processes, including sum-frequency generation and SHG. The pulse shaper imparts a complex phase and spatiotemporal structure on the pulse, which depend on the input and parameters applied through the spectral modulations. These features would make it impossible to accurately model the experiment using a one-dimensional pulse propagation model. For simplicity, we refer to this complex, spatiotemporal quadratic nonlinear pulse propagation as ultrafast SHG.</p><p>Although the functionality of the SHG-PNN does not rely on a closed-form mathematical description or indeed on any form of mathematical isomorphism, some readers may find it helpful to understand the approximate form of the input–output transformation realized in this experimental apparatus. We emphasize that the following model is idealistic and meant to convey key intuitions about the physical transformation: the model does not describe the experimental transformation in a quantitative manner, owing to the numerous experimental complexities described above.</p><p>The physical transformation of the ultrafast SHG setup is seeded by the infrared light from the titanium:sapphire laser. This ultrashort pulse can be described by the Fourier transform of the electric field envelope of the pulse, <span>\({A}_{0}(\omega )\)</span>, where <i>ω</i> is the frequency of the field detuned relative to the carrier frequency. For simplicity, consider a pulse consisting of a set of discrete frequencies or frequency bins, whose spectral amplitudes are described by the discrete vector <span>\({{\bf{A}}}_{{\bf{0}}}={{[A}_{0}({\omega }_{1}),{A}_{0}({\omega }_{2}),\ldots ,{A}_{0}({\omega }_{N})]}^{{\rm{T}}}\,.\)</span> After passing through the pulseshaper, the spectral amplitudes of the pulse are then given by</p><div id="Equ6"><p><span>$${\bf{A}}={{[\sqrt{{x}_{1}}A}_{0}({\omega }_{1}),{\sqrt{{x}_{2}}A}_{0}({\omega }_{2}),\ldots ,{\sqrt{{\theta }_{1}}A}_{0}({\omega }_{{N}_{x}+1}),{\sqrt{{\theta }_{2}}A}_{0}({\omega }_{{N}_{x}+2}),\ldots ]}^{{\rm{T}}},$$</span></p><p>
                    (5)
                </p></div><p>where <span>\({N}_{x}\)</span> is the dimensionality of the data vector, <span>\({\theta }_{i}\)</span> are the trainable pulse-shaper amplitudes and <span>\({x}_{i}\)</span> are the elements of the input data vector. Thus, the output from the pulse shaper encodes both the machine-learning data as well as the trainable parameters. Square roots are present in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://jaked.org/articles/s41586-021-04223-6#Equ6">5</a>) because the pulse shaper was deliberately calibrated to perform an intensity modulation.</p><p>The output from the pulse shaper (equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://jaked.org/articles/s41586-021-04223-6#Equ6">5</a>)) is then input to the ultrafast SHG process. The propagation of an ultrashort pulse through a quadratic nonlinear medium results in an input–output transformation that roughly approximates an autocorrelation, or nonlinear convolution, assuming that the dispersion during propagation is small and the input pulse is well described by a single spatial mode. In this limit, the output blue spectrum <span>\(B\left({\omega }_{i}\right)\)</span> is mathematically given by</p><div id="Equ7"><p><span>$$B({\omega }_{i})=k\sum _{j}A({\omega }_{i}+{\omega }_{j})A({\omega }_{i}-{\omega }_{j}),$$</span></p><p>
                    (6)
                </p></div><p>where the sum is over all frequency bins  <i>j</i> of the pulsed field. The output of the trainable physical transformation <span>\({\bf{y}}={f}_{{\rm{p}}}\left({\bf{x}},{\boldsymbol{\theta }}\right)\,\)</span>is given by the blue pulse’s spectral power, <span>\({{\bf{y}}=[{|{B}_{{\omega }_{1}}|}^{2},{|{B}_{{\omega }_{2}}|}^{2},\ldots ,{|{B}_{{\omega }_{N}}|}^{2}]}^{{\rm{T}}}\,,\)</span>where <span>\({N}\)</span> is the length of the output vector.</p><p>From this description, it is clear that the physical transformation realized by the ultrafast SHG process is not isomorphic to any conventional neural network layer, even in this idealized limit. Nonetheless, the physical transformation retains some key features of typical neural network layers. First, the physical transformation is nonlinear as the SHG process involves the squaring of the input field. Second, as the terms within the summation in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://jaked.org/articles/s41586-021-04223-6#Equ7">6</a>) involve both parameters and input data, the transformation also mixes the different elements of the input data and parameters to product an output. This mixing of input elements is similar, but not necessarily directly mathematically equivalent to, the mixing of input vector elements that occur in the matrix-vector multiplications or convolutions that appear in conventional neural networks.</p><h3 id="Sec11">Vowel classification with ultrafast SHG</h3><p>A task often used to demonstrate novel machine-learning hardware is the classification of spoken vowels according to formant frequencies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR10" id="ref-link-section-d16580660e4464">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR11" id="ref-link-section-d16580660e4467">11</a></sup>. The task involves predicting the spoken vowels given a 12-dimensional input data vector of formant frequencies extracted from audio recordings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR10" id="ref-link-section-d16580660e4471">10</a></sup>. Here we use the vowel dataset from ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. Nature 563, 230–234 (2018)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR10" id="ref-link-section-d16580660e4475">10</a></sup>, which is based on data originally from ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Hillenbrand, J., Getty, L. A., Wheeler, K. &amp; Clark, M. J. Acoustic characteristics of American English vowels. J. Acoust. Soc. Am. 97, 3099–3111 (1995)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR70" id="ref-link-section-d16580660e4479">70</a></sup>; data available at <a href="https://homepages.wmich.edu/~hillenbr/voweldata.html">https://homepages.wmich.edu/~hillenbr/voweldata.html</a>. This dataset consists of 273 data input–output pairs. We used 175 data pairs as the training set—49 for the validation and 49 for the test set. For the results in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3</a>, we optimized for the hyperparameters of the PNN architecture using the validation error and only evaluated the test error after all optimization was conducted. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3c</a>, for each PNN with a given number of layers, the experiment was conducted with two different training, validation and test splits of the vowel data. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3c</a>, the line plots the mean over the two splits, and the error bars are the standard error of the mean.</p><p>For the vowel-classification PNN presented in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig3">3</a>, the input vector to each SHG physical layer is encoded in a contiguous short-wavelength section of the spectral modulation vector sent to the pulse shaper, and the trainable parameters are encoded in the spectral modulations applied to the rest of the spectrum. For the physical layers after the first layer, the input vector to the physical system is the measured spectrum obtained from the previous layer. For convenience, we performed digital renormalization of these output vectors to maximize the dynamic range of the input and ensure that inputs were within the allowed range of 0 to 1 accepted by the pulse shaper. Relatedly, we found that training stability was improved by including additional trainable digital re-scaling parameters to the forward-fed vector, allowing the overall bias and amplitude scale of the physical inputs to each layer to be adjusted during training. These digital parameters appear to have a negligible role in the final trained PNN (when the physical transformations are replaced by identity operations, the network can be trained to perform no better than chance, and the final trained values of the scale and bias parameters are all very close to 1 and 0, respectively). We hypothesize that these trainable rescaling parameters are helpful during training to allow the network to escape noise-affected subspaces of parameter space. See Supplementary Section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">2E.1</a> for details.</p><p>The vowel-classification SHG-PNN architecture (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">21</a>) was designed to be as simple as possible while still demonstrating the use of a multilayer architecture with a physical transformation that is not isomorphic to a conventional DNN layer, and so that the computations involved in performing the classification were essentially all performed by the physical system itself. Many aspects of the design are not optimal with respect to performance, so design choices, such as our specific choice to partition input data and parameter vectors into the controllable parameters of the experiment, should not be interpreted as representing any systematic optimization. Similarly, the vowel-classification task was chosen as a simple example of multidimensional machine-learning classification. As this task can be solved almost perfectly by a linear model, it is in fact poorly suited to the nonlinear optical transformations of our SHG-PNN, which are fully nonlinear (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">9</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">10</a>). Overall, readers should not interpret this PNN’s design as suggestive of optimal design strategies for PNNs. For initial guidelines on optimal design strategies, we instead refer readers to Supplementary Section 5.</p><h3 id="Sec12">MNIST handwritten digit image classification with a hybrid physical–digital SHG-PNN</h3><p>The design of the hybrid physical–digital MNIST PNN based on ultrafast SHG for handwritten digit classification (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4i–l</a>) was chosen to demonstrate a proof-of-concept PNN in which substantial digital operations were co-trained with substantial physical transformations, and in which no digital output layer was used (although a digital output layer can be used with PNNs, and we expect such a layer will usually improve performance, we wanted to avoid confusing readers familiar with reservoir computing, and so avoided using digital output layers in this work).</p><p>The network (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">29</a>) involves four trainable linear input layers that operate on MNIST digit images, whose outputs are fed into four separate channels in which the SHG physical transformation is used twice in succession (that is, it is two physical layers deep). The output of the final layers of each channel (the final SHG spectra) are concatenated, then summed into ten bins to perform a classification. The structure of the input layer was chosen to minimize the complexity of inputs to the pulse shaper. We found that the output second-harmonic spectra produced by the nonlinear optical process tended towards featureless triangular spectra if inputs were close to a random uniform distribution. Thus, to ensure that output spectra varied significantly with respect to changes in the input spectral modulations, we made sure that inputs to the pulse shaper would exhibit a smoother structure in the following way. For each of 4 independent channels, 196-dimensional input images (downsampled from 784-dimensional 28 × 28 images) are first operated on by a 196 by 50 trainable linear matrix, and then (without any nonlinear digital operations), a second 50 by 196 trainable linear matrix. The second 50 by 196 matrix is identical for all channels, the intent being that this matrix identifies optimal ‘input modes’ to the SHG process. By varying the middle dimension of this two-step linear input layer, one may control the amount of structure (number of ‘spectral modes’) allowed in inputs to the pulse shaper, as the middle dimension effectively controls the rank of the total linear matrix. We found that a middle dimension below 30 resulted in the most visually varied SHG output spectra, but that 50 was sufficient for good performance on the MNIST task. In this network, we also utilized skip connections between layers in each channel. This was done so that the network would be able to ‘choose’ to use the linear digital operations to perform the linear part of the classification task (for which nearly 90% accuracy can be obtained<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324 (1998)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR55" id="ref-link-section-d16580660e4544">55</a></sup>) and to thus rely on the SHG co-processor primarily for the harder, nonlinear part of the classification task. Between the physical layers in each channel, a trainable, element-wise rescaling was used to allow us to train the second physical layer transformations efficiently. That is, <span>\({x}_{i}={a}_{i}{y}_{i}+{b}_{i}\)</span>, where <span>\({b}_{i}\)</span> and <span>\({a}_{i}\)</span> are trainable parameters, and <span>\({x}_{i}\)</span> and <span>\({y}_{i}\)</span> are the input to the pulse shaper and the measured output spectrum from the previous physical layer, respectively.</p><p>For further details on the nonlinear optical experimental setup and its characterization, we refer readers to Supplementary Section 2A. For further details on the vowel-classification SHG-PNN, we refer readers to Supplementary Section 2E.1, and for the hybrid physical–digital MNIST handwritten digit-classification SHG-PNN, we refer readers to Supplementary Section 2E.4.</p><h3 id="Sec13">Analogue electronic circuit experiments</h3><p>The electronic circuit used for our experiments (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">11</a>) was a resistor-inductor-capacitor oscillator (RLC oscillator) with a transistor embedded within it. It was designed to produce as nonlinear and complex a response as possible, while still containing only a few simple components (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">12</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">13</a>). The experiments were carried out with standard bulk electronic components, a hobbyist circuit breadboard and a USB data acquisition (DAQ) device (Measurement Computing USB-1208-HS-4AO), which allowed for one analogue input and one analogue output channel, with a sampling rate of 1 MS s<sup>−1</sup>.</p><p>The electronic circuit provides only a one-dimensional time-series input and one-dimensional time-series output. As a result, to partition the inputs to the system into trainable parameters and input data so that we could control the circuit’s transformation of input data, we found it was most convenient to apply parameters to the one-dimensional input time-series vector by performing trainable, element-wise rescaling on the input time-series vector. That is, <span>\({x}_{i}={a}_{i}{y}_{i}+{b}_{i}\)</span>, where <span>\({b}_{i}\)</span> and <span>\({a}_{i}\)</span> are trainable parameters, <span>\({y}_{i}\)</span> are the components of the input data vector and<span>\(\,{x}_{i}\)</span> are the re-scaled components of the voltage time series that is then sent to the analogue circuit. For the first layer, <span>\({y}_{i}\)</span> are the unrolled pixels of the input MNIST image. For hidden layers, <span>\({y}_{i}\)</span> are the components of the output voltage time-series vector from the previous layer.</p><p>We found that the electronic circuit’s output was noisy, primarily owing to the timing jitter noise that resulted from operating the DAQ at its maximum sampling rate (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">23</a>). Rather than reducing this noise by operating the device more slowly, we were motivated to design the PNN architecture presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://jaked.org/articles/s41586-021-04223-6#Fig4">4</a> in a way that allowed it to automatically learn to function robustly and accurately, even in the presence of up to 20% noise per output vector element (See Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">24</a> for an expanded depiction of the architecture). First, seven, three-layer feedforward PNNs were trained together, with the final prediction provided by averaging the output of all seven, three-layer PNNs. Second, skip connections similar to those used in residual neural networks were employed<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Veit, A.,Wilber, M. &amp; Belongie, S. Residual networks behave like ensembles of relatively shallow networks Preprint at 
                  https://arxiv.org/abs/1605.06431
                  
                 (2016)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR71" id="ref-link-section-d16580660e5013">71</a></sup>. These measures make the output of the network effectively an ensemble average over many different subnetworks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Veit, A.,Wilber, M. &amp; Belongie, S. Residual networks behave like ensembles of relatively shallow networks Preprint at 
                  https://arxiv.org/abs/1605.06431
                  
                 (2016)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR71" id="ref-link-section-d16580660e5017">71</a></sup>, which allows it to perform accurately and train smoothly despite the very high physical noise and multilayer design.</p><p>For further details on the analogue electronic experimental setup and its characterization, we refer readers to Supplementary Section 2B. For further details on the MNIST handwritten digit-classification analogue electronic PNN, we refer readers to Supplementary Section 2E.2.</p><h3 id="Sec14">Oscillating mechanical plate experiments</h3><p>The mechanical plate oscillator was constructed by attaching a 3.2 cm by 3.2 cm by 1 mm titanium plate to a long, centre-mounted screw, which was fixed to the voice coil of a commercial full-range speaker (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">14</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">15</a>). The speaker was driven by an audio amplifier (Kinter K2020A+) and the oscillations of the plate were recorded using a microphone (Audio-Technica ATR2100x-USB Cardioid Dynamic Microphone). The diaphragm of the speaker was completely removed so that the sound recorded by the microphone is produced only by the oscillating metal plate.</p><p>As the physical input (output) to (from) the mechanical oscillator is a one-dimensional time series, similar to the electronic circuit, we made use of element-wise trainable rescaling to conveniently allow us to train the oscillating plate’s physical transformations.</p><p>The mechanical PNN architecture for the MNIST handwritten digit classification task was chosen to be the simplest multilayer PNN architecture possible with such a one-dimensional dynamical system (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">27</a>). As the mechanical plate’s input–output responses are primarily linear convolutions (Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">16</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://jaked.org/articles/s41586-021-04223-6#MOESM1">17</a>), it is well suited to the MNIST handwritten digit classification task, achieving nearly the same performance as a digital linear model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86, 2278–2324 (1998)." href="https://jaked.org/articles/s41586-021-04223-6#ref-CR55" id="ref-link-section-d16580660e5053">55</a></sup>.</p><p>For further details on the oscillating mechanical plate experimental setup and its characterization, we refer readers to Supplementary Section 2C. For further details on the MNIST handwritten digit-classification oscillating mechanical plate PNN, we refer readers to Supplementary Section 2E.3.</p></div></div></section>
            

            <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><div id="data-availability-content">
              
              <p>All data generated during and code used for this work are available at <a href="https://doi.org/10.5281/zenodo.4719150">https://doi.org/10.5281/zenodo.4719150</a>.</p>
            </div></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><div id="code-availability-content">
              
              <p>An expandable demonstration code for applying PAT to train PNNs is available at <a href="https://github.com/mcmahon-lab/Physics-Aware-Training">https://github.com/mcmahon-lab/Physics-Aware-Training</a>. All code used for this work is available at <a href="https://doi.org/10.5281/zenodo.4719150">https://doi.org/10.5281/zenodo.4719150</a>.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol><li><span>1.</span><p id="ref-CR1">Patterson, D. et al. Carbon emissions and large neural network training. Preprint at <a href="https://arxiv.org/abs/2104.10350">https://arxiv.org/abs/2104.10350</a> (2021).</p></li><li><span>2.</span><p id="ref-CR2">Reuther, A. et al. Survey of machine learning accelerators. In <i>2020 IEEE High Performance Extreme Computing Conference (HPEC)</i> 1–12 (IEEE, 2020).</p></li><li><span>3.</span><p id="ref-CR3">Xia, Q., &amp; Yang, J. J. Memristive crossbar arrays for brain-inspired computing. <i>Nat. Mater.</i> <b>18</b>, 309–323 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1MXotFagu7w%3D" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30894760" aria-label="PubMed reference 3" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019NatMa..18..309X" aria-label="ADS reference 3">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Memristive%20crossbar%20arrays%20for%20brain-inspired%20computing&amp;journal=Nat.%20Mater.&amp;volume=18&amp;pages=309-323&amp;publication_year=2019&amp;author=Xia%2CQ&amp;author=Yang%2CJJ">
                    Google Scholar</a> 
                </p></li><li><span>4.</span><p id="ref-CR4">Burr, G. W. et al. Neuromorphic computing using non-volatile memory. <i>Adv. Phys. X</i> <b>2</b>, 89–124 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuromorphic%20computing%20using%20non-volatile%20memory&amp;journal=Adv.%20Phys.%20X&amp;volume=2&amp;pages=89-124&amp;publication_year=2017&amp;author=Burr%2CGW">
                    Google Scholar</a> 
                </p></li><li><span>5.</span><p id="ref-CR5">Khaddam-Aljameh, R. et al. HERMES core—a 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In <i>2021 Symposium on VLSI Circuits</i> (IEEE, 2021).</p></li><li><span>6.</span><p id="ref-CR6">Narayanan, P. et al. Fully on-chip MAC at 14nm enabled by accurate row-wise programming of PCM-based weights and parallel vector-transport in duration-format. In <i>2021 Symposium on VLSI Technology</i> (IEEE, 2021).</p></li><li><span>7.</span><p id="ref-CR7">Kohda, Y. et al. Unassisted true analog neural network training chip. In <i>2020 IEEE International Electron Devices Meeting (IEDM)</i> (IEEE, 2020).</p></li><li><span>8.</span><p id="ref-CR8">Marković, D., Mizrahi, A., Querlioz, D. &amp; Grollier, J. Physics for neuromorphic computing. <i>Nat. Rev. Phys.</i> <b>2</b>, 499–510 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Physics%20for%20neuromorphic%20computing&amp;journal=Nat.%20Rev.%20Phys.&amp;volume=2&amp;pages=499-510&amp;publication_year=2020&amp;author=Markovi%C4%87%2CD&amp;author=Mizrahi%2CA&amp;author=Querlioz%2CD&amp;author=Grollier%2CJ">
                    Google Scholar</a> 
                </p></li><li><span>9.</span><p id="ref-CR9">Wetzstein, G. et al. Inference in artificial intelligence with deep optics and photonics. <i>Nature</i> <b>588</b>, 39–47 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVOks7rI" aria-label="CAS reference 9">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33268862" aria-label="PubMed reference 9" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020Natur.588...39W" aria-label="ADS reference 9">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Inference%20in%20artificial%20intelligence%20with%20deep%20optics%20and%20photonics&amp;journal=Nature&amp;volume=588&amp;pages=39-47&amp;publication_year=2020&amp;author=Wetzstein%2CG">
                    Google Scholar</a> 
                </p></li><li><span>10.</span><p id="ref-CR10">Romera, M. et al. Vowel recognition with four coupled spin-torque nano-oscillators. <i>Nature</i> <b>563</b>, 230–234 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVCitL7F" aria-label="CAS reference 10">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30374193" aria-label="PubMed reference 10" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018Natur.563..230R" aria-label="ADS reference 10">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Vowel%20recognition%20with%20four%20coupled%20spin-torque%20nano-oscillators&amp;journal=Nature&amp;volume=563&amp;pages=230-234&amp;publication_year=2018&amp;author=Romera%2CM">
                    Google Scholar</a> 
                </p></li><li><span>11.</span><p id="ref-CR11">Shen, Y. et al. Deep learning with coherent nanophotonic circuits. <i>Nat. Photon.</i> <b>11</b>, 441–446 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC2sXhtVSjt7bJ" aria-label="CAS reference 11">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017NaPho..11..441S" aria-label="ADS reference 11">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20with%20coherent%20nanophotonic%20circuits&amp;journal=Nat.%20Photon.&amp;volume=11&amp;pages=441-446&amp;publication_year=2017&amp;author=Shen%2CY">
                    Google Scholar</a> 
                </p></li><li><span>12.</span><p id="ref-CR12">Prezioso, M. et al. Training and operation of an integrated neuromorphic network based on metal-oxide memristors. <i>Nature</i> <b>521</b>, 61–64 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC2MXnvFWjtb4%3D" aria-label="CAS reference 12">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25951284" aria-label="PubMed reference 12" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2015Natur.521...61P" aria-label="ADS reference 12">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Training%20and%20operation%20of%20an%20integrated%20neuromorphic%20network%20based%20on%20metal-oxide%20memristors&amp;journal=Nature&amp;volume=521&amp;pages=61-64&amp;publication_year=2015&amp;author=Prezioso%2CM">
                    Google Scholar</a> 
                </p></li><li><span>13.</span><p id="ref-CR13">Euler, H.-C. R. et al. A deep-learning approach to realizing functionality in nanoelectronic devices. <i>Nat. Nanotechnol.</i> <b>15</b>, 992–998 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020NatNa..15..992R" aria-label="ADS reference 13">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep-learning%20approach%20to%20realizing%20functionality%20in%20nanoelectronic%20devices&amp;journal=Nat.%20Nanotechnol.&amp;volume=15&amp;pages=992-998&amp;publication_year=2020&amp;author=Euler%2CH-CR">
                    Google Scholar</a> 
                </p></li><li><span>14.</span><p id="ref-CR14">Hughes, T. W., Williamson, I. A., Minkov, M. &amp; Fan, S. Wave physics as an analog recurrent neural network. <i>Sci. Adv.</i> <b>5</b>, eaay6946 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31903420" aria-label="PubMed reference 14" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6924985" aria-label="PubMed Central reference 14" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019SciA....5.6946H" aria-label="ADS reference 14">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Wave%20physics%20as%20an%20analog%20recurrent%20neural%20network&amp;journal=Sci.%20Adv.&amp;volume=5&amp;publication_year=2019&amp;author=Hughes%2CTW&amp;author=Williamson%2CIA&amp;author=Minkov%2CM&amp;author=Fan%2CS">
                    Google Scholar</a> 
                </p></li><li><span>15.</span><p id="ref-CR15">Wu, Z., Zhou, M., Khoram, E., Liu, B. &amp; Yu, Z. Neuromorphic metasurface. <i>Photon. Res.</i> <b>8</b>, 46–50 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXht1GksrzE" aria-label="CAS reference 15">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuromorphic%20metasurface&amp;journal=Photon.%20Res.&amp;volume=8&amp;pages=46-50&amp;publication_year=2020&amp;author=Wu%2CZ&amp;author=Zhou%2CM&amp;author=Khoram%2CE&amp;author=Liu%2CB&amp;author=Yu%2CZ">
                    Google Scholar</a> 
                </p></li><li><span>16.</span><p id="ref-CR16">Furuhata, G., Niiyama, T. &amp; Sunada, S. Physical deep learning based on optimal control of dynamical systems. <i>Phys. Rev. Appl.</i> <b>15</b>, 034092 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3MXovVWhsLY%3D" aria-label="CAS reference 16">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021PhRvP..15c4092F" aria-label="ADS reference 16">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Physical%20deep%20learning%20based%20on%20optimal%20control%20of%20dynamical%20systems&amp;journal=Phys.%20Rev.%20Appl.&amp;volume=15&amp;publication_year=2021&amp;author=Furuhata%2CG&amp;author=Niiyama%2CT&amp;author=Sunada%2CS">
                    Google Scholar</a> 
                </p></li><li><span>17.</span><p id="ref-CR17">Lin, X. et al. All-optical machine learning using diffractive deep neural networks. <i>Science</i> <b>361</b>, 1004–1008 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3837095" aria-label="MathSciNet reference 17">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1cXhs1ChsLfJ" aria-label="CAS reference 17">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30049787" aria-label="PubMed reference 17" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1415.68171" aria-label="MATH reference 17">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018Sci...361.1004L" aria-label="ADS reference 17">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=All-optical%20machine%20learning%20using%20diffractive%20deep%20neural%20networks&amp;journal=Science&amp;volume=361&amp;pages=1004-1008&amp;publication_year=2018&amp;author=Lin%2CX">
                    Google Scholar</a> 
                </p></li><li><span>18.</span><p id="ref-CR18">Miller, J. F., Harding, S. L. &amp; Tufte, G. Evolution-in-materio: evolving computation in materials. <i>Evol. Intell.</i> <b>7</b>, 49–67 (2014).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Evolution-in-materio%3A%20evolving%20computation%20in%20materials&amp;journal=Evol.%20Intell.&amp;volume=7&amp;pages=49-67&amp;publication_year=2014&amp;author=Miller%2CJF&amp;author=Harding%2CSL&amp;author=Tufte%2CG">
                    Google Scholar</a> 
                </p></li><li><span>19.</span><p id="ref-CR19">Chen, T. et al. Classification with a disordered dopant-atom network in silicon. <i>Nature</i> <b>577</b>, 341–345 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXjsVejs74%3D" aria-label="CAS reference 19">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31942054" aria-label="PubMed reference 19" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020Natur.577..341C" aria-label="ADS reference 19">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Classification%20with%20a%20disordered%20dopant-atom%20network%20in%20silicon&amp;journal=Nature&amp;volume=577&amp;pages=341-345&amp;publication_year=2020&amp;author=Chen%2CT">
                    Google Scholar</a> 
                </p></li><li><span>20.</span><p id="ref-CR20">Bueno, J. et al. Reinforcement learning in a large-scale photonic recurrent neural network. <i>Optica</i> <b>5</b>, 756–760 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018Optic...5..756B" aria-label="ADS reference 20">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforcement%20learning%20in%20a%20large-scale%20photonic%20recurrent%20neural%20network&amp;journal=Optica&amp;volume=5&amp;pages=756-760&amp;publication_year=2018&amp;author=Bueno%2CJ">
                    Google Scholar</a> 
                </p></li><li><span>21.</span><p id="ref-CR21">Tanaka, G. et al. Recent advances in physical reservoir computing: a review. <i>Neural Netw.</i> <b>115</b>, 100–123 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30981085" aria-label="PubMed reference 21" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20physical%20reservoir%20computing%3A%20a%20review&amp;journal=Neural%20Netw.&amp;volume=115&amp;pages=100-123&amp;publication_year=2019&amp;author=Tanaka%2CG">
                    Google Scholar</a> 
                </p></li><li><span>22.</span><p id="ref-CR22">Appeltant, L. et al. Information processing using a single dynamical node as complex system. <i>Nat. Commun.</i> <b>2</b>, 468 (2011).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:STN:280:DC%2BC3MfjsVGiug%3D%3D" aria-label="CAS reference 22">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21915110" aria-label="PubMed reference 22" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2011NatCo...2..468A" aria-label="ADS reference 22">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20processing%20using%20a%20single%20dynamical%20node%20as%20complex%20system&amp;journal=Nat.%20Commun.&amp;volume=2&amp;publication_year=2011&amp;author=Appeltant%2CL">
                    Google Scholar</a> 
                </p></li><li><span>23.</span><p id="ref-CR23">Mouret, J.-B. &amp; Chatzilygeroudis, K. 20 years of reality gap: a few thoughts about simulators in evolutionary robotics. In <i>Proc. Genetic and Evolutionary Computation Conference Companion</i> 1121–1124 (2017).</p></li><li><span>24.</span><p id="ref-CR24">Howison, T., Hauser, S., Hughes, J. &amp; Iida, F. Reality-assisted evolution of soft robots through large-scale physical experimentation: a review. <i>Artif. Life</i> <b>26</b>, 484–506 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Reality-assisted%20evolution%20of%20soft%20robots%20through%20large-scale%20physical%20experimentation%3A%20a%20review&amp;journal=Artif.%20Life&amp;volume=26&amp;pages=484-506&amp;publication_year=2021&amp;author=Howison%2CT&amp;author=Hauser%2CS&amp;author=Hughes%2CJ&amp;author=Iida%2CF">
                    Google Scholar</a> 
                </p></li><li><span>25.</span><p id="ref-CR25">de Avila Belbute-Peres, F., Smith, K., Allen, K., Tenenbaum, J. &amp; Kolter, J. Z. End-to-end differentiable physics for learning and control. <i>Adv. Neural Inf. Process. Syst.</i> <b>31</b>, 7178–7189 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=End-to-end%20differentiable%20physics%20for%20learning%20and%20control&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=31&amp;pages=7178-7189&amp;publication_year=2018&amp;author=Avila%20Belbute-Peres%2CF&amp;author=Smith%2CK&amp;author=Allen%2CK&amp;author=Tenenbaum%2CJ&amp;author=Kolter%2CJZ">
                    Google Scholar</a> 
                </p></li><li><span>26.</span><p id="ref-CR26">Degrave, J., Hermans, M., Dambre, J. &amp; Wyffels, F. A differentiable physics engine for deep learning in robotics. <i>Front. Neurorobot.</i> <b>13</b>, 6 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30899218" aria-label="PubMed reference 26" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416213" aria-label="PubMed Central reference 26" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20differentiable%20physics%20engine%20for%20deep%20learning%20in%20robotics&amp;journal=Front.%20Neurorobot.&amp;volume=13&amp;publication_year=2019&amp;author=Degrave%2CJ&amp;author=Hermans%2CM&amp;author=Dambre%2CJ&amp;author=Wyffels%2CF">
                    Google Scholar</a> 
                </p></li><li><span>27.</span><p id="ref-CR27">Molesky, S. et al. Inverse design in nanophotonics. <i>Nat. Photon.</i> <b>12</b>, 659–670 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVent7zI" aria-label="CAS reference 27">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018NaPho..12..659M" aria-label="ADS reference 27">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Inverse%20design%20in%20nanophotonics&amp;journal=Nat.%20Photon.&amp;volume=12&amp;pages=659-670&amp;publication_year=2018&amp;author=Molesky%2CS">
                    Google Scholar</a> 
                </p></li><li><span>28.</span><p id="ref-CR28">Peurifoy, J. et al. Nanophotonic particle simulation and inverse design using artificial neural networks. <i>Sci. Adv.</i> <b>4</b>, eaar4206 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29868640" aria-label="PubMed reference 28" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5983917" aria-label="PubMed Central reference 28" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018SciA....4.4206P" aria-label="ADS reference 28">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Nanophotonic%20particle%20simulation%20and%20inverse%20design%20using%20artificial%20neural%20networks&amp;journal=Sci.%20Adv.&amp;volume=4&amp;publication_year=2018&amp;author=Peurifoy%2CJ">
                    Google Scholar</a> 
                </p></li><li><span>29.</span><p id="ref-CR29">Stern, M., Arinze, C., Perez, L., Palmer, S. E. &amp; Murugan, A. Supervised learning through physical changes in a mechanical system. <i>Proc. Natl Acad. Sci. USA</i> <b>117</b>, 14843–14850 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=4240237" aria-label="MathSciNet reference 29">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVWktL%2FF" aria-label="CAS reference 29">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32546522" aria-label="PubMed reference 29" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334525" aria-label="PubMed Central reference 29" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Supervised%20learning%20through%20physical%20changes%20in%20a%20mechanical%20system&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=117&amp;pages=14843-14850&amp;publication_year=2020&amp;author=Stern%2CM&amp;author=Arinze%2CC&amp;author=Perez%2CL&amp;author=Palmer%2CSE&amp;author=Murugan%2CA">
                    Google Scholar</a> 
                </p></li><li><span>30.</span><p id="ref-CR30">Zhou, F. &amp; Chai, Y. Near-sensor and in-sensor computing. <i>Nat. Electron.</i> <b>3</b>, 664–671 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Near-sensor%20and%20in-sensor%20computing&amp;journal=Nat.%20Electron.&amp;volume=3&amp;pages=664-671&amp;publication_year=2020&amp;author=Zhou%2CF&amp;author=Chai%2CY">
                    Google Scholar</a> 
                </p></li><li><span>31.</span><p id="ref-CR31">Martel, J. N., Mueller, L. K., Carey, S. J., Dudek, P. &amp; Wetzstein, G. Neural sensors: learning pixel exposures for HDR imaging and video compressive sensing with programmable sensors. <i>IEEE Trans. Pattern Anal. Mach. Intell.</i> <b>42</b>, 1642–1653 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32305899" aria-label="PubMed reference 31" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20sensors%3A%20learning%20pixel%20exposures%20for%20HDR%20imaging%20and%20video%20compressive%20sensing%20with%20programmable%20sensors&amp;journal=IEEE%20Trans.%20Pattern%20Anal.%20Mach.%20Intell.&amp;volume=42&amp;pages=1642-1653&amp;publication_year=2020&amp;author=Martel%2CJN&amp;author=Mueller%2CLK&amp;author=Carey%2CSJ&amp;author=Dudek%2CP&amp;author=Wetzstein%2CG">
                    Google Scholar</a> 
                </p></li><li><span>32.</span><p id="ref-CR32">Mennel, L. et al. Ultrafast machine vision with 2D material neural network image sensors. <i>Nature</i> <b>579</b>, 62–66 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXksVWmsLo%3D" aria-label="CAS reference 32">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32132692" aria-label="PubMed reference 32" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020Natur.579...62M" aria-label="ADS reference 32">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Ultrafast%20machine%20vision%20with%202D%20material%20neural%20network%20image%20sensors&amp;journal=Nature&amp;volume=579&amp;pages=62-66&amp;publication_year=2020&amp;author=Mennel%2CL">
                    Google Scholar</a> 
                </p></li><li><span>33.</span><p id="ref-CR33">Brooks, R. A. Intelligence without reason. In <i>Proc. 12th International Joint Conference on Artificial Intelligence</i> Vol. 1, 569–595 (Morgan Kaufmann, 1991).</p></li><li><span>34.</span><p id="ref-CR34">Hooker, S. The hardware lottery. Preprint at <a href="https://arxiv.org/abs/2009.06489">https://arxiv.org/abs/2009.06489</a> (2020).</p></li><li><span>35.</span><p id="ref-CR35">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. <i>Adv. Neural Inf. Process. Syst.</i> <b>25</b>, 1097–1105 (2012).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Imagenet%20classification%20with%20deep%20convolutional%20neural%20networks&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=25&amp;pages=1097-1105&amp;publication_year=2012&amp;author=Krizhevsky%2CA&amp;author=Sutskever%2CI&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li><span>36.</span><p id="ref-CR36">Lin, H. W., Tegmark, M. &amp; Rolnick, D. Why does deep and cheap learning work so well? <i>J. Stat. Phys.</i> <b>168</b>, 1223–1247 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3691248" aria-label="MathSciNet reference 36">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1373.82061" aria-label="MATH reference 36">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017JSP...168.1223L" aria-label="ADS reference 36">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20does%20deep%20and%20cheap%20learning%20work%20so%20well%3F&amp;journal=J.%20Stat.%20Phys.&amp;volume=168&amp;pages=1223-1247&amp;publication_year=2017&amp;author=Lin%2CHW&amp;author=Tegmark%2CM&amp;author=Rolnick%2CD">
                    Google Scholar</a> 
                </p></li><li><span>37.</span><p id="ref-CR37">Grollier, J. et al. Neuromorphic spintronics. <i>Nat. Electron.</i> <b>3</b>, 360–370 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuromorphic%20spintronics&amp;journal=Nat.%20Electron.&amp;volume=3&amp;pages=360-370&amp;publication_year=2020&amp;author=Grollier%2CJ">
                    Google Scholar</a> 
                </p></li><li><span>38.</span><p id="ref-CR38">Mitarai, K., Negoro, M., Kitagawa, M. &amp; Fujii, K. Quantum circuit learning. <i>Phys. Rev. A</i> <b>98</b>, 032309 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1MXlsFyht7k%3D" aria-label="CAS reference 38">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018PhRvA..98c2309M" aria-label="ADS reference 38">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantum%20circuit%20learning&amp;journal=Phys.%20Rev.%20A&amp;volume=98&amp;publication_year=2018&amp;author=Mitarai%2CK&amp;author=Negoro%2CM&amp;author=Kitagawa%2CM&amp;author=Fujii%2CK">
                    Google Scholar</a> 
                </p></li><li><span>39.</span><p id="ref-CR39">Poggio, T., Banburski, A. &amp; Liao, Q. Theoretical issues in deep networks. <i>Proc. Natl Acad. Sci. USA</i> <b>117</b>, 30039–30045 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=4263286" aria-label="MathSciNet reference 39">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVKnsLjI" aria-label="CAS reference 39">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32518109" aria-label="PubMed reference 39" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720221" aria-label="PubMed Central reference 39" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Theoretical%20issues%20in%20deep%20networks&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=117&amp;pages=30039-30045&amp;publication_year=2020&amp;author=Poggio%2CT&amp;author=Banburski%2CA&amp;author=Liao%2CQ">
                    Google Scholar</a> 
                </p></li><li><span>40.</span><p id="ref-CR40">Scellier, B. &amp; Bengio, Y. Equilibrium propagation: bridging the gap between energy-based models and backpropagation. <i>Front. Comput. Neurosci</i>. <b>11</b> (2017).</p></li><li><span>41.</span><p id="ref-CR41">Ernoult, M., Grollier, J., Querlioz, D., Bengio, Y. &amp; Scellier, B. Equilibrium propagation with continual weight updates Preprint at <a href="https://arxiv.org/abs/2005.04168">https://arxiv.org/abs/2005.04168</a> (2020).</p></li><li><span>42.</span><p id="ref-CR42">Laborieux, A. et al. Scaling equilibrium propagation to deep convnets by drastically reducing its gradient estimator bias. <i>Front. Neurosci</i>. <b>15</b> (2021).</p></li><li><span>43.</span><p id="ref-CR43">Martin, E. et al. Eqspike: spike-driven equilibrium propagation for neuromorphic implementations. <i>iScience</i> <b>24</b>, 102222 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33748709" aria-label="PubMed reference 43" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7970361" aria-label="PubMed Central reference 43" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021iSci...24j2222M" aria-label="ADS reference 43">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Eqspike%3A%20spike-driven%20equilibrium%20propagation%20for%20neuromorphic%20implementations&amp;journal=iScience&amp;volume=24&amp;publication_year=2021&amp;author=Martin%2CE">
                    Google Scholar</a> 
                </p></li><li><span>44.</span><p id="ref-CR44">Dillavou, S., Stern, M., Liu, A. J., &amp; Durian, D. J. Demonstration of decentralized, physics-driven learning. Preprint at <a href="https://arxiv.org/abs/2108.00275">https://arxiv.org/abs/2108.00275</a> (2021).</p></li><li><span>45.</span><p id="ref-CR45">Hermans, M., Burm, M., Van Vaerenbergh, T., Dambre, J. &amp; Bienstman, P. Trainable hardware for dynamical computing using error backpropagation through physical media. <i>Nat. Commun.</i> <b>6</b>, 6729 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC2MXosVCltbs%3D" aria-label="CAS reference 45">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25801303" aria-label="PubMed reference 45" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2015NatCo...6.6729H" aria-label="ADS reference 45">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Trainable%20hardware%20for%20dynamical%20computing%20using%20error%20backpropagation%20through%20physical%20media&amp;journal=Nat.%20Commun.&amp;volume=6&amp;publication_year=2015&amp;author=Hermans%2CM&amp;author=Burm%2CM&amp;author=Vaerenbergh%2CT&amp;author=Dambre%2CJ&amp;author=Bienstman%2CP">
                    Google Scholar</a> 
                </p></li><li><span>46.</span><p id="ref-CR46">Hughes, T. W., Minkov, M., Shi, Y. &amp; Fan, S. Training of photonic neural networks through in situ backpropagation and gradient measurement. <i>Optica</i> <b>5</b>, 864–871 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2018Optic...5..864H" aria-label="ADS reference 46">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Training%20of%20photonic%20neural%20networks%20through%20in%20situ%20backpropagation%20and%20gradient%20measurement&amp;journal=Optica&amp;volume=5&amp;pages=864-871&amp;publication_year=2018&amp;author=Hughes%2CTW&amp;author=Minkov%2CM&amp;author=Shi%2CY&amp;author=Fan%2CS">
                    Google Scholar</a> 
                </p></li><li><span>47.</span><p id="ref-CR47">Lopez-Pastor, V. &amp; Marquardt, F. Self-learning machines based on Hamiltonian echo backpropagation. Preprint at <a href="https://arxiv.org/abs/2103.04992">https://arxiv.org/abs/2103.04992</a> (2021).</p></li><li><span>48.</span><p id="ref-CR48">Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R. &amp; Bengio, Y. Quantized neural networks: training neural networks with low precision weights and activations. <i>J. Mach. Learn. Res.</i> <b>18</b>, 6869–6898 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3827075" aria-label="MathSciNet reference 48">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1468.68183" aria-label="MATH reference 48">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantized%20neural%20networks%3A%20training%20neural%20networks%20with%20low%20precision%20weights%20and%20activations&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=18&amp;pages=6869-6898&amp;publication_year=2017&amp;author=Hubara%2CI&amp;author=Courbariaux%2CM&amp;author=Soudry%2CD&amp;author=El-Yaniv%2CR&amp;author=Bengio%2CY">
                    Google Scholar</a> 
                </p></li><li><span>49.</span><p id="ref-CR49">Frye, R. C., Rietman, E. A. &amp; Wong, C. C. Back-propagation learning and nonidealities in analog neural network hardware. <i>IEEE Trans. Neural Netw.</i> <b>2</b>, 110–117 (1991).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:STN:280:DC%2BD1c7hslOmtQ%3D%3D" aria-label="CAS reference 49">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18276356" aria-label="PubMed reference 49" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Back-propagation%20learning%20and%20nonidealities%20in%20analog%20neural%20network%20hardware&amp;journal=IEEE%20Trans.%20Neural%20Netw.&amp;volume=2&amp;pages=110-117&amp;publication_year=1991&amp;author=Frye%2CRC&amp;author=Rietman%2CEA&amp;author=Wong%2CCC">
                    Google Scholar</a> 
                </p></li><li><span>50.</span><p id="ref-CR50">Cramer, B. et al. Surrogate gradients for analog neuromorphic computing. Preprint at <a href="https://arxiv.org/abs/2006.07239">https://arxiv.org/abs/2006.07239</a> (2020).</p></li><li><span>51.</span><p id="ref-CR51">Adhikari, S. P. et al. Memristor bridge synapse-based neural network and its learning. <i>IEEE Trans Neural Netw. Learn. Syst.</i> <b>23</b>,1426–1435 (2012).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24807926" aria-label="PubMed reference 51" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Memristor%20bridge%20synapse-based%20neural%20network%20and%20its%20learning&amp;journal=IEEE%20Trans%20Neural%20Netw.%20Learn.%20Syst.&amp;volume=23&amp;pages=1426-1435&amp;publication_year=2012&amp;author=Adhikari%2CSP">
                    Google Scholar</a> 
                </p></li><li><span>52.</span><p id="ref-CR52">Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. <i>Nat. Commun.</i> <b>7</b>, 13276 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC28XhvVehtLrM" aria-label="CAS reference 52">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27824044" aria-label="PubMed reference 52" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5105169" aria-label="PubMed Central reference 52" rel="nofollow">PubMed Central</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016NatCo...713276L" aria-label="ADS reference 52">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning&amp;journal=Nat.%20Commun.&amp;volume=7&amp;publication_year=2016&amp;author=Lillicrap%2CTP&amp;author=Cownden%2CD&amp;author=Tweed%2CDB&amp;author=Akerman%2CCJ">
                    Google Scholar</a> 
                </p></li><li><span>53.</span><p id="ref-CR53">Launay, J., Poli, I., Boniface, F., &amp; Krzakala, F. Direct feedback alignment scales to modern deep learning tasks and architectures. Preprint at <a href="https://arxiv.org/abs/2006.12878">https://arxiv.org/abs/2006.12878</a> (2020).</p></li><li><span>54.</span><p id="ref-CR54">Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library. <i>Adv. Neural Inf. Process. Syst.</i> <b>32</b>, 8024–8035 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=PyTorch%3A%20an%20imperative%20style%2C%20high-performance%20deep%20learning%20library&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=32&amp;pages=8024-8035&amp;publication_year=2019&amp;author=Paszke%2CA">
                    Google Scholar</a> 
                </p></li><li><span>55.</span><p id="ref-CR55">LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. <i>Proc. IEEE</i> <b>86</b>, 2278–2324 (1998).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Gradient-based%20learning%20applied%20to%20document%20recognition&amp;journal=Proc.%20IEEE&amp;volume=86&amp;pages=2278-2324&amp;publication_year=1998&amp;author=LeCun%2CY&amp;author=Bottou%2CL&amp;author=Bengio%2CY&amp;author=Haffner%2CP">
                    Google Scholar</a> 
                </p></li><li><span>56.</span><p id="ref-CR56">Xiao, H., Rasul, K., &amp; Vollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. Preprint at <a href="https://arxiv.org/abs/1708.07747">https://arxiv.org/abs/1708.07747</a> (2017).</p></li><li><span>57.</span><p id="ref-CR57">Spoon, K. et al. Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. <i>Front. Comput. Neurosci</i>. <b>53</b>, (2021).</p></li><li><span>58.</span><p id="ref-CR58">Kariyappa, S. et al. Noise-resilient DNN: tolerating noise in PCM-based AI accelerators via noise-aware training. <i>IEEE Trans. Electron Devices</i> <b>68</b>, 4356–4362 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021ITED...68.4356K" aria-label="ADS reference 58">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Noise-resilient%20DNN%3A%20tolerating%20noise%20in%20PCM-based%20AI%20accelerators%20via%20noise-aware%20training&amp;journal=IEEE%20Trans.%20Electron%20Devices&amp;volume=68&amp;pages=4356-4362&amp;publication_year=2021&amp;author=Kariyappa%2CS">
                    Google Scholar</a> 
                </p></li><li><span>59.</span><p id="ref-CR59">Gokmen, T., Rasch, M. J. &amp; Haensch. W. The marriage of training and inference for scaled deep learning analog hardware. In <i>2019 IEEE International Electron Devices Meeting (IEDM)</i> (IEEE, 2019).</p></li><li><span>60.</span><p id="ref-CR60">Rasch, M. J. et al. A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays. In <i>2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)</i> (IEEE, 2021).</p></li><li><span>61.</span><p id="ref-CR61">Falcon, W. et al. PyTorch Lightning (2019); <a href="https://github.com/PyTorchLightning/pytorch-lightning">https://github.com/PyTorchLightning/pytorch-lightning</a></p></li><li><span>62.</span><p id="ref-CR62">Biewald, L. Experiment Tracking with Weights and Biases (2020); <a href="https://www.wandb.com/">https://www.wandb.com/</a></p></li><li><span>63.</span><p id="ref-CR63">Kasim, M. F. et al. Building high accuracy emulators for scientific simulations with deep neural architecture search. Preprint at <a href="https://arxiv.org/abs/2001.08055">https://arxiv.org/abs/2001.08055</a> (2020).</p></li><li><span>64.</span><p id="ref-CR64">Rahmani, B. et al. Actor neural networks for the robust control of partially measured nonlinear systems showcased for image propagation through diffuse media. <i>Nat. Mach. Intell.</i> <b>2</b>, 403–410 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Actor%20neural%20networks%20for%20the%20robust%20control%20of%20partially%20measured%20nonlinear%20systems%20showcased%20for%20image%20propagation%20through%20diffuse%20media&amp;journal=Nat.%20Mach.%20Intell.&amp;volume=2&amp;pages=403-410&amp;publication_year=2020&amp;author=Rahmani%2CB">
                    Google Scholar</a> 
                </p></li><li><span>65.</span><p id="ref-CR65">Karniadakis, G. E. et al. Physics-informed machine learning. <i>Nat. Rev. Phys.</i> <b>3</b>, 422–440 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Physics-informed%20machine%20learning&amp;journal=Nat.%20Rev.%20Phys.&amp;volume=3&amp;pages=422-440&amp;publication_year=2021&amp;author=Karniadakis%2CGE">
                    Google Scholar</a> 
                </p></li><li><span>66.</span><p id="ref-CR66">Akiba, T., Sano, S., Yanase, T., Ohta, T. &amp; Koyama, M. Optuna: a next-generation hyperparameter optimization framework. In <i>Proc. 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</i> 2623–2631 (2019).</p></li><li><span>67.</span><p id="ref-CR67">Liu, W. et al. Programmable controlled mode-locked fiber laser using a digital micromirror device. <i>Opt. Lett.</i> <b>42</b>, 1923–1926 (2017).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://jaked.org/articles/cas-redirect/1:CAS:528:DC%2BC1MXhs1Ontr4%3D" aria-label="CAS reference 67">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28504760" aria-label="PubMed reference 67" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017OptL...42.1923L" aria-label="ADS reference 67">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 67" href="http://scholar.google.com/scholar_lookup?&amp;title=Programmable%20controlled%20mode-locked%20fiber%20laser%20using%20a%20digital%20micromirror%20device&amp;journal=Opt.%20Lett.&amp;volume=42&amp;pages=1923-1926&amp;publication_year=2017&amp;author=Liu%2CW">
                    Google Scholar</a> 
                </p></li><li><span>68.</span><p id="ref-CR68">Matthès, M. W., del Hougne, P., de Rosny, J., Lerosey, G. &amp; Popoff, S. M. Optical complex media as universal reconfigurable linear operators. <i>Optica</i> <b>6</b>, 465–472 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019Optic...6..465M" aria-label="ADS reference 68">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 68" href="http://scholar.google.com/scholar_lookup?&amp;title=Optical%20complex%20media%20as%20universal%20reconfigurable%20linear%20operators&amp;journal=Optica&amp;volume=6&amp;pages=465-472&amp;publication_year=2019&amp;author=Matth%C3%A8s%2CMW&amp;author=Hougne%2CP&amp;author=Rosny%2CJ&amp;author=Lerosey%2CG&amp;author=Popoff%2CSM">
                    Google Scholar</a> 
                </p></li><li><span>69.</span><p id="ref-CR69">Popoff, S. M. &amp; Matthès, M. W. ALP4lib: q Python wrapper for the Vialux ALP-4 controller suite to control DMDs. <i>Zenodo</i> <a href="https://doi.org/10.5281/zenodo.4076193">https://doi.org/10.5281/zenodo.4076193</a> (2020).</p></li><li><span>70.</span><p id="ref-CR70">Hillenbrand, J., Getty, L. A., Wheeler, K. &amp; Clark, M. J. Acoustic characteristics of American English vowels. <i>J. Acoust. Soc. Am</i>. <b>97</b>, 3099–3111 (1995).</p></li><li><span>71.</span><p id="ref-CR71">Veit, A.,Wilber, M. &amp; Belongie, S. Residual networks behave like ensembles of relatively shallow networks Preprint at <a href="https://arxiv.org/abs/1605.06431">https://arxiv.org/abs/1605.06431</a> (2016).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-021-04223-6?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>We thank NTT Research for their financial and technical support. Portions of this work were supported by the National Science Foundation (award CCF-1918549). L.G.W. and T.W. acknowledge support from Mong Fellowships from Cornell Neurotech during early parts of this work. P.L.M. acknowledges membership of the CIFAR Quantum Information Science Program as an Azrieli Global Scholar. We acknowledge discussions with D. Ahsanullah, M. Anderson, V. Kremenetski, E. Ng, S. Popoff, S. Prabhu, M. Saebo, H. Tanaka, R. Yanagimoto, H. Zhen and members of the NTT PHI Lab/NSF Expeditions research collaboration, and thank P. Jordan for discussions and illustrations.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><p><span id="author-notes">Author notes</span></p><ol><li id="na1"><p>These authors contributed equally: Logan G. Wright, Tatsuhiro Onodera</p></li></ol><h3 id="affiliations">Affiliations</h3><ol><li id="Aff1"><p>School of Applied and Engineering Physics, Cornell University, Ithaca, NY, USA</p><p>Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Zoey Hu &amp; Peter L. McMahon</p></li><li id="Aff2"><p>NTT Physics and Informatics Laboratories, NTT Research, Inc., Sunnyvale, CA, USA</p><p>Logan G. Wright &amp; Tatsuhiro Onodera</p></li><li id="Aff3"><p>School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA</p><p>Darren T. Schachter</p></li></ol><h3 id="contributions">Contributions</h3><p>L.G.W., T.O. and P.L.M. conceived the project and methods. T.O. and L.G.W. performed the SHG-PNN experiments. L.G.W. performed the electronic-PNN experiments. M.M.S. performed the oscillating-plate-PNN experiments. T.W., D.T.S. and Z.H. contributed to initial parts of the work. L.G.W., T.O., M.M.S. and P.L.M. wrote the manuscript. P.L.M. supervised the project.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:lgw32@cornell.edu">Logan G. Wright</a>, <a id="corresp-c2" href="mailto:to232@cornell.edu">Tatsuhiro Onodera</a> or <a id="corresp-c3" href="mailto:pmcmahon@cornell.edu">Peter L. McMahon</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar1">Competing interests</h3>
                <p>L.G.W., T.O., M.M.S. and P.L.M. are listed as inventors on a US provisional patent application (number 63/178,318) on physical neural networks and physics-aware training. The other authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Peer review information"><div id="peer-review-section"><h2 id="peer-review">Peer review information</h2><p><i>Nature</i> thanks Tayfun Gokmen and Damien Querlioz for their contribution to the peer review of this work. Peer reviewer reports are available.</p></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Supplementary information"><div id="Sec16-section"><h2 id="Sec16">Supplementary information</h2><div id="Sec16-content"><div data-test="supplementary-info">
                  
                  
                <div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-04223-6/MediaObjects/41586_2021_4223_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information</a></h3><p>This file contains Supplementary Sections 1–5, including Supplementary Figs. 1–37 and References—see the contents page for details.</p></div></div></div></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Deep%20physical%20neural%20networks%20trained%20with%20backpropagation&amp;author=Logan%20G.%20Wright%20et%20al&amp;contentID=10.1038%2Fs41586-021-04223-6&amp;copyright=The%20Author%28s%29&amp;publication=0028-0836&amp;publicationDate=2022-01-26&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41586-021-04223-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-021-04223-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Wright, L.G., Onodera, T., Stein, M.M. <i>et al.</i> Deep physical neural networks trained with backpropagation.
                    <i>Nature</i> <b>601, </b>549–555 (2022). https://doi.org/10.1038/s41586-021-04223-6</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-021-04223-6?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2021-05-19">19 May 2021</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2021-11-09">09 November 2021</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2022-01-26">26 January 2022</time></span></p></li><li><p>Issue Date<span>: </span><span><time datetime="2022-01-27">27 January 2022</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-021-04223-6</span></p></li></ul></div></div></div></div></section>

            

            
                <section data-title="Comments"><div id="article-comments-section"><h2 id="article-comments">Comments</h2><p>By submitting a comment you agree to abide by our <a href="https://jaked.org/info/tandc.html">Terms</a> and <a href="https://jaked.org/info/community-guidelines.html">Community Guidelines</a>. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.</p></div></section>
                
            

            </div></div>
  </body>
</html>
