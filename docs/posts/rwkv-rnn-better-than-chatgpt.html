<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/BlinkDL/RWKV-LM">Original</a>
    <h1>RWKV RNN: Better than ChatGPT?</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" dir="auto"><a id="user-content-rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v" aria-hidden="true" href="#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>RWKV: Parallelizable RNN with Transformer-level LLM Performance (pronounced as &#34;RwaKuv&#34;, from 4 major params: R W K V)</h2>
<p dir="auto">RWKV is an RNN with Transformer-level LLM performance, which can also be directly trained like a GPT transformer (parallelizable). And it&#39;s 100% attention-free. You only need the hidden state at position t to compute the state at position t+1. You can use the &#34;GPT&#34; mode to quickly compute the hidden state for the &#34;RNN&#34; mode.</p>
<p dir="auto">So it&#39;s combining the best of RNN and transformer - <strong>great performance, fast inference, saves VRAM, fast training, &#34;infinite&#34; ctx_len, and free sentence embedding</strong> (using the final hidden state).</p>
<p dir="auto"><strong>RWKV pip package</strong>: <a href="https://pypi.org/project/rwkv/" rel="nofollow">https://pypi.org/project/rwkv/</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="os.environ[&#34;RWKV_JIT_ON&#34;] = &#39;1&#39;
os.environ[&#34;RWKV_CUDA_ON&#34;] = &#39;0&#39; # if &#39;1&#39; then use CUDA kernel for seq mode (much faster)
from rwkv.model import RWKV                         # pip install rwkv
model = RWKV(model=&#39;/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040&#39;, strategy=&#39;cuda fp16&#39;)

out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json
print(out.detach().cpu().numpy())                   # get logits
out, state = model.forward([187, 510], None)
out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)
out, state = model.forward([310, 247], state)
print(out.detach().cpu().numpy())                   # same result as above"><pre><span>os</span>.<span>environ</span>[<span>&#34;RWKV_JIT_ON&#34;</span>] <span>=</span> <span>&#39;1&#39;</span>
<span>os</span>.<span>environ</span>[<span>&#34;RWKV_CUDA_ON&#34;</span>] <span>=</span> <span>&#39;0&#39;</span> <span># if &#39;1&#39; then use CUDA kernel for seq mode (much faster)</span>
<span>from</span> <span>rwkv</span>.<span>model</span> <span>import</span> <span>RWKV</span>                         <span># pip install rwkv</span>
<span>model</span> <span>=</span> <span>RWKV</span>(<span>model</span><span>=</span><span>&#39;/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040&#39;</span>, <span>strategy</span><span>=</span><span>&#39;cuda fp16&#39;</span>)

<span>out</span>, <span>state</span> <span>=</span> <span>model</span>.<span>forward</span>([<span>187</span>, <span>510</span>, <span>1563</span>, <span>310</span>, <span>247</span>], <span>None</span>)   <span># use 20B_tokenizer.json</span>
<span>print</span>(<span>out</span>.<span>detach</span>().<span>cpu</span>().<span>numpy</span>())                   <span># get logits</span>
<span>out</span>, <span>state</span> <span>=</span> <span>model</span>.<span>forward</span>([<span>187</span>, <span>510</span>], <span>None</span>)
<span>out</span>, <span>state</span> <span>=</span> <span>model</span>.<span>forward</span>([<span>1563</span>], <span>state</span>)           <span># RNN has state (use deepcopy if you want to clone it)</span>
<span>out</span>, <span>state</span> <span>=</span> <span>model</span>.<span>forward</span>([<span>310</span>, <span>247</span>], <span>state</span>)
<span>print</span>(<span>out</span>.<span>detach</span>().<span>cpu</span>().<span>numpy</span>())                   <span># same result as above</span></pre></div>
<p dir="auto"><strong>Download RWKV-4 0.1/0.4/1.5/3/7/14B weights</strong>: <a href="https://huggingface.co/BlinkDL" rel="nofollow">https://huggingface.co/BlinkDL</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-join-our-discord-httpsdiscordggbdsbumefpc-lots-of-developers" aria-hidden="true" href="#join-our-discord-httpsdiscordggbdsbumefpc-lots-of-developers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Join Our Discord: <a href="https://discord.gg/bDSBUMeFpc" rel="nofollow">https://discord.gg/bDSBUMeFpc</a> (lots of developers)</h2>
<p dir="auto"><strong>Twitter</strong>: <a href="https://twitter.com/BlinkDL_AI" rel="nofollow">https://twitter.com/BlinkDL_AI</a></p>
<p dir="auto"><strong>RWKV in 150 lines</strong> (model, inference, text generation): <a href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py">https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py</a></p>
<p dir="auto"><strong>ChatRWKV v2:</strong> with &#34;stream&#34; and &#34;split&#34; strategies and INT8. <strong>3G VRAM is enough to run RWKV 14B :)</strong> <a href="https://github.com/BlinkDL/ChatRWKV/tree/main/v2">https://github.com/BlinkDL/ChatRWKV/tree/main/v2</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-chat.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-chat.png" alt="RWKV-chat"/></a></p>
<p dir="auto"><strong>Hugging Face space</strong>: <a href="https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio" rel="nofollow">https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio</a></p>
<p dir="auto">You are welcome to join the RWKV discord <a href="https://discord.gg/bDSBUMeFpc" rel="nofollow">https://discord.gg/bDSBUMeFpc</a> to build upon it. We have plenty of potential compute (A100 40Gs) now (thanks to Stability and EleutherAI), so if you have interesting ideas I can run them.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-eval2.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-eval2.png" alt="RWKV-eval2"/></a></p>
<p dir="auto">RWKV [loss vs token position] for 10000 ctx4k+ documents in Pile. RWKV 1B5-4k is mostly flat after ctx1500, but 3B-4k and 7B-4k and 14B-4k have some slopes, and they are getting better. This debunks the old view that RNNs cannot model long ctxlens. We can predict that RWKV 100B will be great, and RWKV 1T is probably all you need :)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-ctxlen.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-ctxlen.png" alt="RWKV-ctxlen"/></a></p>
<p dir="auto">I believe RNN is a better candidate for fundamental models, because: (1) It&#39;s more friendly for ASICs (no kv cache). (2) It&#39;s more friendly for RL. (3) When we write, our brain is more similar to RNN. (4) The universe is like an RNN too (because of locality). Transformers are non-local models.</p>
<p dir="auto">RWKV-3 1.5B on A40 (tf32) = always 0.015 sec/token, tested using simple pytorch code (no CUDA), GPU utilization 45%, VRAM 7823M</p>
<p dir="auto">GPT2-XL 1.3B on A40 (tf32) = 0.032 sec/token (for ctxlen 1000), tested using HF, GPU utilization 45% too (interesting), VRAM 9655M</p>
<p dir="auto">Training speed: (new training code) RWKV-4 14B BF16 ctxlen4096 = 114K tokens/s on 8x8 A100 80G (ZERO2+CP). (old training code) RWKV-4 1.5B BF16 ctxlen1024 = 106K tokens/s on 8xA100 40G.</p>
<p dir="auto">I am doing image experiments too (For example: <a href="https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder" rel="nofollow">https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder</a>) and RWKV will be able to do txt2img diffusion :) My idea: 256x256 rgb image -&gt; 32x32x13bit latents -&gt; apply RWKV to compute transition probability for each of the 32x32 grid -&gt; pretend the grids are independent and &#34;diffuse&#34; using these probabilities.</p>
<p dir="auto">Smooth training - no loss spikes! (lr &amp; bsz change around 15G tokens)
<a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-loss.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-loss.png" alt="RWKV-loss"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-eval.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-eval.png" alt="RWKV-eval"/></a></p>
<p dir="auto">All of the trained models will be open-source. Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, so you can even run a LLM on your phone.</p>
<p dir="auto">How it works: RWKV gathers information to a number of channels, which are also decaying with different speeds as you move to the next token. It&#39;s very simple once you understand it.</p>
<p dir="auto"><strong>RWKV is parallelizable because the time-decay of each channel is data-independent (and trainable)</strong>. For example, in usual RNN you can adjust the time-decay of a channel from say 0.8 to 0.5 (these are called &#34;gates&#34;), while in RWKV you simply move the information from a W-0.8-channel to a W-0.5-channel to achieve the same effect. Moreover, you can fine-tune RWKV into a non-parallelizable RNN (then you can use outputs of later layers of the previous token) if you want extra performance.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-formula.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-formula.png" alt="RWKV-formula"/></a></p>
<p dir="auto">Here are some of my TODOs. Let&#39;s work together :)</p>
<ul dir="auto">
<li>
<p dir="auto">HuggingFace integration (check <a data-error-text="Failed to load title" data-id="1235076094" data-permission-text="Title is private" data-url="https://github.com/huggingface/transformers/issues/17230" data-hovercard-type="issue" data-hovercard-url="/huggingface/transformers/issues/17230/hovercard" href="https://github.com/huggingface/transformers/issues/17230">huggingface/transformers#17230</a>
), and optimized CPU &amp; iOS &amp; Android &amp; WASM &amp; WebGL inference. RWKV is a RNN and very friendly for edge devices. Let&#39;s make it possible to run a LLM on your phone.</p>
</li>
<li>
<p dir="auto">Test it on bidirectional &amp; MLM tasks, and image &amp; audio &amp; video tokens. I think RWKV can support Encoder-Decoder via this: for each decoder token, use a learned mixture of [decoder previous hidden state] &amp; [encoder final hidden state]. Hence all decoder tokens will have access to the encoder output.</p>
</li>
<li>
<p dir="auto">Now training RWKV-4a with one single tiny extra attention (just a few extra lines comparing with RWKV-4) to further improve some difficult zeroshot tasks (such as LAMBADA) for smaller models. See <a href="https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829">https://github.com/BlinkDL/RWKV-LM/commit/a268cd2e40351ee31c30c5f8a5d1266d35b41829</a></p>
</li>
</ul>
<p dir="auto">User feedback:</p>
<blockquote>
<p dir="auto"><em>I&#39;ve so far toyed around the character-based model on our relatively small pre-training dataset (around 10GB of text), and the results are extremely good - similar ppl to models taking much, much longer to train.</em></p>
</blockquote>
<blockquote>
<p dir="auto"><em>dear god rwkv is fast. i switched to another tab after starting training it from scratch &amp; when i returned it was emitting plausible english &amp; maori words, i left to go microwave some coffee &amp; when i came back it was producing fully grammatically correct sentences.</em></p>
</blockquote>
<p dir="auto">Tweet from Sepp Hochreiter (thank you!): <a href="https://twitter.com/HochreiterSepp/status/1524270961314484227" rel="nofollow">https://twitter.com/HochreiterSepp/status/1524270961314484227</a></p>
<p dir="auto">You can find me (BlinkDL) in the EleutherAI Discord too: <a href="https://www.eleuther.ai/get-involved/" rel="nofollow">https://www.eleuther.ai/get-involved/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-demo.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-demo.png" alt="RWKV-demo"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-hidden="true" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick start</h2>
<p dir="auto">Use <a href="https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo">https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v4neo</a> (latest code, compatible with v4).</p>
<p dir="auto">Here is a great prompt for testing Q&amp;A of LLMs. Works for any model: (found by minimizing ChatGPT ppls for RWKV 1.5B)</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = f&#39;\nQ &amp; A\n\nQuestion:\n{qq}\n\nDetailed Expert Answer:\n&#39; # let the model generate after this"><pre><span>prompt</span> <span>=</span> <span>f&#39;<span>\n</span>Q &amp; A<span>\n</span><span>\n</span>Question:<span>\n</span><span><span>{</span><span>qq</span><span>}</span></span><span>\n</span><span>\n</span>Detailed Expert Answer:<span>\n</span>&#39;</span> <span># let the model generate after this</span></pre></div>
<p dir="auto"><strong>Cool Community RWKV Projects (check them!)</strong>:</p>
<p dir="auto"><a href="https://pypi.org/project/rwkvstic/" rel="nofollow">https://pypi.org/project/rwkvstic/</a> a pip package (with 8bit &amp; offload for low VRAM GPUs)</p>
<p dir="auto"><a href="https://github.com/harrisonvanderbyl/rwkv_chatbot">https://github.com/harrisonvanderbyl/rwkv_chatbot</a> a chatbot</p>
<p dir="auto"><a href="https://github.com/hizkifw/WebChatRWKVstic">https://github.com/hizkifw/WebChatRWKVstic</a> WebUI (WIP)</p>
<p dir="auto"><a href="https://github.com/gururise/rwkv_gradio">https://github.com/gururise/rwkv_gradio</a> RWKV Gradio</p>
<p dir="auto"><a href="https://github.com/cryscan/eloise">https://github.com/cryscan/eloise</a> RWKV QQ bot</p>
<p dir="auto"><a href="https://github.com/Blealtan/RWKV-LM-LoRA">https://github.com/Blealtan/RWKV-LM-LoRA</a> LoRA fine-tuning</p>
<p dir="auto"><a href="https://github.com/mrsteyk/RWKV-LM-jax">https://github.com/mrsteyk/RWKV-LM-jax</a></p>
<p dir="auto"><a href="https://github.com/wozeparrot/tinyrwkv">https://github.com/wozeparrot/tinyrwkv</a> RWKV in tinygrad (nice simple DL framework)</p>
<p dir="auto"><a data-error-text="Failed to load title" data-id="1235076094" data-permission-text="Title is private" data-url="https://github.com/huggingface/transformers/issues/17230" data-hovercard-type="issue" data-hovercard-url="/huggingface/transformers/issues/17230/hovercard" href="https://github.com/huggingface/transformers/issues/17230">huggingface/transformers#17230</a> RWKV HF package (WIP)</p>
<p dir="auto"><a href="https://github.com/ArEnSc/Production-RWKV">https://github.com/ArEnSc/Production-RWKV</a> RWKV HF package source</p>
<p dir="auto"><a href="https://github.com/nlpodyssey/verbaflow">https://github.com/nlpodyssey/verbaflow</a> RWKV in Go</p>
<p dir="auto"><a href="https://github.com/nlpodyssey/rwkv">https://github.com/nlpodyssey/rwkv</a> RWKV in Go</p>
<p dir="auto"><a href="https://github.com/mrsteyk/rwkvk-rs">https://github.com/mrsteyk/rwkvk-rs</a> RWKV in Rust</p>
<p dir="auto"><a href="https://github.com/josephrocca/rwkv-v4-web">https://github.com/josephrocca/rwkv-v4-web</a> RWKV in browser</p>
<p dir="auto"><a href="https://github.com/imxcstar/CSharp-RWKV-V4">https://github.com/imxcstar/CSharp-RWKV-V4</a> RWKV in C#</p>
<p dir="auto"><a href="https://github.com/mrsteyk/RWKV-LM-deepspeed">https://github.com/mrsteyk/RWKV-LM-deepspeed</a> Another training fork</p>
<p dir="auto"><a href="https://github.com/resloved/RWKV-notebooks">https://github.com/resloved/RWKV-notebooks</a> RWKV colab notebooks</p>
<p dir="auto"><a href="https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb" rel="nofollow">https://colab.research.google.com/github/harrisonvanderbyl/rwkvstic/blob/master/notebooks/chatbot.ipynb</a> RWKV chatbot colab notebook</p>
<p dir="auto"><a href="https://github.com/Pathos14489/RWKVDistributedInference">https://github.com/Pathos14489/RWKVDistributedInference</a> RWKV Distributed Inference</p>
<p dir="auto"><a href="https://github.com/AXKuhta/rwkv-onnx-dml">https://github.com/AXKuhta/rwkv-onnx-dml</a> RWKV ONNX</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-inference" aria-hidden="true" href="#inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference</h3>
<p dir="auto"><strong>Run RWKV-4 Pile models:</strong> Download models from <a href="https://huggingface.co/BlinkDL" rel="nofollow">https://huggingface.co/BlinkDL</a>. Set TOKEN_MODE = &#39;pile&#39; in run.py and run it. It&#39;s fast even on CPU (the default mode).</p>
<p dir="auto"><strong>Colab for RWKV-4 Pile 1.5B</strong>: <a href="https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM" rel="nofollow">https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM</a></p>
<p dir="auto">Run RWKV-4 Pile models in your browser (and onnx version): see this issue <a data-error-text="Failed to load title" data-id="1344988620" data-permission-text="Title is private" data-url="https://github.com/BlinkDL/RWKV-LM/issues/7" data-hovercard-type="issue" data-hovercard-url="/BlinkDL/RWKV-LM/issues/7/hovercard" href="https://github.com/BlinkDL/RWKV-LM/issues/7">#7</a></p>
<p dir="auto">RWKV-4 Web Demo: <a href="https://josephrocca.github.io/rwkv-v4-web/demo/" rel="nofollow">https://josephrocca.github.io/rwkv-v4-web/demo/</a> (note: only greedy sampling for now)</p>
<p dir="auto">For the old RWKV-2: see the release here for a 27M params model on enwik8 with 0.72 BPC(dev). Run run.py in <a href="https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN">https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v2-RNN</a>. You can even run it in your browser: <a href="https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng">https://github.com/BlinkDL/AI-Writer/tree/main/docs/eng</a> <a href="https://blinkdl.github.io/AI-Writer/eng/" rel="nofollow">https://blinkdl.github.io/AI-Writer/eng/</a> (this is using tf.js WASM single-thread mode).</p>
<p dir="auto">I&#39;d like to build an almost-INT8 version of RWKV. A simple method to quantize a matrix with outliers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import numpy as npA

# the original M, with outliers
M = np.array([[1,   2,   1,  2],[2,  100,    2, 10],[1,   2,   1, 2],[2,   1, 20, 1]])

# the scaled M, without outliers
Q = np.array([[1, 0.2, 0.1,  2],[0.4,  2, 0.04, 2], [1, 0.2, 0.1, 2],[2, 0.1,  2, 1]])
# we can find optimal a &amp; b to minimize inference error after quantization
a = np.array([1, 10, 10, 1])
b = np.array([1, 5, 1, 1])

# test M.v with random v - the results will be the same
v = np.array([1.23, 5.44, 9.75, 2.98])
print(M.dot(v))
print(Q.dot(v * a) * b)

# even better: decompose M.dot(v) as Q.dot(v * a + aa) * b + bb where aa &amp; bb are vectors too
# and can apply more scaling to achieve W8A8 (example: https://arxiv.org/pdf/2211.10438.pdf)"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>npA</span>

<span># the original M, with outliers</span>
<span>M</span> <span>=</span> <span>np</span>.<span>array</span>([[<span>1</span>,   <span>2</span>,   <span>1</span>,  <span>2</span>],[<span>2</span>,  <span>100</span>,    <span>2</span>, <span>10</span>],[<span>1</span>,   <span>2</span>,   <span>1</span>, <span>2</span>],[<span>2</span>,   <span>1</span>, <span>20</span>, <span>1</span>]])

<span># the scaled M, without outliers</span>
<span>Q</span> <span>=</span> <span>np</span>.<span>array</span>([[<span>1</span>, <span>0.2</span>, <span>0.1</span>,  <span>2</span>],[<span>0.4</span>,  <span>2</span>, <span>0.04</span>, <span>2</span>], [<span>1</span>, <span>0.2</span>, <span>0.1</span>, <span>2</span>],[<span>2</span>, <span>0.1</span>,  <span>2</span>, <span>1</span>]])
<span># we can find optimal a &amp; b to minimize inference error after quantization</span>
<span>a</span> <span>=</span> <span>np</span>.<span>array</span>([<span>1</span>, <span>10</span>, <span>10</span>, <span>1</span>])
<span>b</span> <span>=</span> <span>np</span>.<span>array</span>([<span>1</span>, <span>5</span>, <span>1</span>, <span>1</span>])

<span># test M.v with random v - the results will be the same</span>
<span>v</span> <span>=</span> <span>np</span>.<span>array</span>([<span>1.23</span>, <span>5.44</span>, <span>9.75</span>, <span>2.98</span>])
<span>print</span>(<span>M</span>.<span>dot</span>(<span>v</span>))
<span>print</span>(<span>Q</span>.<span>dot</span>(<span>v</span> <span>*</span> <span>a</span>) <span>*</span> <span>b</span>)

<span># even better: decompose M.dot(v) as Q.dot(v * a + aa) * b + bb where aa &amp; bb are vectors too</span>
<span># and can apply more scaling to achieve W8A8 (example: https://arxiv.org/pdf/2211.10438.pdf)</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-training--fine-tuning" aria-hidden="true" href="#training--fine-tuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training / Fine-tuning</h3>
<p dir="auto"><strong>Training RWKV-4 from scratch:</strong> run train.py, which by default is using the enwik8 dataset (unzip <a href="https://data.deepai.org/enwik8.zip" rel="nofollow">https://data.deepai.org/enwik8.zip</a>).</p>
<p dir="auto">You will be training the &#34;GPT&#34; version because it&#39;s paralleziable and faster to train. RWKV-4 can extrapolate, so training with ctxLen 1024 can work for ctxLen of 2500+. You can fine-tune the model with longer ctxLen and it can quickly adapt to longer ctxLens.</p>
<p dir="auto"><strong>Fine-tuning RWKV-4 Pile models:</strong> use &#39;prepare-data.py&#39; in <a href="https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3">https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3</a> to tokenize .txt into train.npy data. Then set EXPRESS_PILE_MODE to True in train.py, and run it.</p>
<p dir="auto">Read the inference code in src/model.py and try using the final hidden stateï¼ˆ.xx .aa .bb) as a faithful sentence embedding for other tasks. Probably you should begin with .xx and .aa/.bb (.aa divided by .bb).</p>
<p dir="auto">Colab for fine-tuning RWKV-4 Pile models: <a href="https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb" rel="nofollow">https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v4_RNN_Pile_Fine_Tuning.ipynb</a></p>
<p dir="auto"><strong>Large corpus:</strong> Use <a href="https://github.com/EleutherAI/gpt-neox">https://github.com/EleutherAI/gpt-neox</a> to convert .jsonl into .bin and .idx</p>
<div data-snippet-clipboard-copy-content="python tools/preprocess_data.py --input ./my_data.jsonl --output-prefix ./data/my_data --vocab ./20B_tokenizer.json --dataset-impl mmap --tokenizer-type HFTokenizer --append-eod"><pre><code>python tools/preprocess_data.py --input ./my_data.jsonl --output-prefix ./data/my_data --vocab ./20B_tokenizer.json --dataset-impl mmap --tokenizer-type HFTokenizer --append-eod
</code></pre></div>
<p dir="auto">The jsonl format sample (one line for each document):</p>
<div data-snippet-clipboard-copy-content="{&#34;meta&#34;: {&#34;ID&#34;: 101}, &#34;text&#34;: &#34;This is the first document.&#34;}
{&#34;meta&#34;: {&#34;ID&#34;: 102}, &#34;text&#34;: &#34;Hello\nWorld&#34;}
{&#34;meta&#34;: {&#34;ID&#34;: 103}, &#34;text&#34;: &#34;1+1=2\n1+2=3\n2+2=4&#34;}"><pre><code>{&#34;meta&#34;: {&#34;ID&#34;: 101}, &#34;text&#34;: &#34;This is the first document.&#34;}
{&#34;meta&#34;: {&#34;ID&#34;: 102}, &#34;text&#34;: &#34;Hello\nWorld&#34;}
{&#34;meta&#34;: {&#34;ID&#34;: 103}, &#34;text&#34;: &#34;1+1=2\n1+2=3\n2+2=4&#34;}
</code></pre></div>
<p dir="auto">generated by code like this:</p>
<div data-snippet-clipboard-copy-content="ss = json.dumps({&#34;meta&#34;: meta, &#34;text&#34;: text}, ensure_ascii=False)
out.write(ss + &#34;\n&#34;)"><pre><code>ss = json.dumps({&#34;meta&#34;: meta, &#34;text&#34;: text}, ensure_ascii=False)
out.write(ss + &#34;\n&#34;)
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-towards-rwkv-5-just-to-record-some-new-ideas" aria-hidden="true" href="#towards-rwkv-5-just-to-record-some-new-ideas"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Towards RWKV-5 (just to record some new ideas)</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-some-ideas" aria-hidden="true" href="#some-ideas"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Some ideas</h3>
<ol dir="auto">
<li>
<p dir="auto">Now time decay is like 0.999^T (0.999 is learnable). Change it to something like (0.999^T + 0.1) where 0.1 is learnable too. The 0.1 part will be kept forever. Or, A^T + B^T + C = fast-decay + slow-decay + constant. Can even use different formulas (for example, K^2 instead of e^K for a decay component, or, without normalization).</p>
</li>
<li>
<p dir="auto">Use complex-valued decay (so, rotation instead of decay) in some channels.</p>
</li>
<li>
<p dir="auto">Inject some trainable and extrapolatable positional encoding?</p>
</li>
<li>
<p dir="auto">Aside from 2d rotation, we can try other Lie groups such as 3d rotation ( SO(3) ). Non-abelian RWKV lol.</p>
</li>
<li>
<p dir="auto">RWKV might be great on analog devices (search for Analog Matrix-vector multiplication &amp; Photonic Matrix-vector multiplication). The RNN mode is very hardware-friendly (processing-in-memory). Can be a SNN too (<a href="https://github.com/ridgerchu/SpikeGPT">https://github.com/ridgerchu/SpikeGPT</a>). I wonder if it can be optimized for quantum computation.</p>
</li>
<li>
<p dir="auto">Trainable initial hidden state (xx aa bb pp xx).</p>
</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-vision-tasks" aria-hidden="true" href="#vision-tasks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Vision Tasks</h3>
<ol dir="auto">
<li>I find it&#39;s good to add a 2d pos encoding:</li>
</ol>
<div data-snippet-clipboard-copy-content="self.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))
self.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))
...
x = x + pos_emb_x + pos_emb_y"><pre><code>self.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))
self.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))
...
x = x + pos_emb_x + pos_emb_y
</code></pre></div>
<ol start="2" dir="auto">
<li>In a BPE langauge model, it&#39;s the best to use [tokenShift of 1 token] (you can mix more tokens in a char-level English model). However you can try [tokenShift of N (or N-1) (or N+1) tokens] if the image size is N x N, because that will be like mixing [the token above the current positon (or the token above the to-be-predicted positon)] with [current token]. You can use try different tokenShift styles for &#34;ATT&#34; &amp; &#34;FFN&#34;, or mixing different tokenShift styles - such as mixing [token A] with [token A-1] and [token A-(N-1)] etc.</li>
</ol>
<h3 tabindex="-1" dir="auto"><a id="user-content-misc" aria-hidden="true" href="#misc"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Misc</h3>
<p dir="auto">I have an idea to improve tokenization. We can hardcode some channels to have meanings. Example:</p>
<p dir="auto">Channel 0 = &#34;space&#34;</p>
<p dir="auto">Channel 1 = &#34;capitalize first letter&#34;</p>
<p dir="auto">Channel 2 = &#34;capitalize all letters&#34;</p>
<p dir="auto">Therefore:</p>
<p dir="auto">Embedding of &#34;abc&#34;:  [0, 0, 0, x0, x1, x2 , ..]</p>
<p dir="auto">Embedding of &#34; abc&#34;:  [1, 0, 0, x0, x1, x2, ..]</p>
<p dir="auto">Embedding of &#34; Abc&#34;:  [1, 1, 0, x0, x1, x2, ..]</p>
<p dir="auto">Embedding of &#34;ABC&#34;: [0, 0, 1, x0, x1, x2, ...]</p>
<p dir="auto">......</p>
<p dir="auto">so they will share most of the embedding. And we can rapidly compute the output probability of all variations of &#34;abc&#34;.</p>
<p dir="auto">Note: the above method is assuming that p(&#34; xyz&#34;) / p(&#34;xyz&#34;) is the same for any &#34;xyz&#34;, which can be wrong.</p>
<p dir="auto">Better: define emb_space emb_capitalize_first emb_capitalize_all to be a function of emb.</p>
<p dir="auto">Maybe the Best: let &#39;abc&#39; &#39; abc&#39; etc. to share the last 90% of their embeddings.</p>
<p dir="auto">At this moment, all our tokenizers spend too many items to represent all variations of &#39;abc&#39; &#39; abc&#39; &#39; Abc&#39; etc. Moreover the model cannot discover that these are actually similar if some of these variations are rare in the dataset. The method here can improve this. I plan to test this in a new version of RWKV.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it works</h2>
<p dir="auto">RWKV is inspired by Apple&#39;s AFT (<a href="https://arxiv.org/abs/2105.14103" rel="nofollow">https://arxiv.org/abs/2105.14103</a>).</p>
<p dir="auto">Moreover it&#39;s using a number of my tricks, such as:</p>
<ul dir="auto">
<li>
<p dir="auto">SmallInitEmb: <a href="https://github.com/BlinkDL/SmallInitEmb">https://github.com/BlinkDL/SmallInitEmb</a> (applicable to all transformers) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).</p>
</li>
<li>
<p dir="auto">Token-shift: <a href="https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing">https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing</a> (applicable to all transformers), especially helpful for char-level models.</p>
</li>
<li>
<p dir="auto">Head-QK: <a href="https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens">https://github.com/BlinkDL/RWKV-LM#the-head-qk-trick-learning-to-copy-and-avoid-tokens</a> (applicable to all transformers). Note: it&#39;s helpful, but I disabled it in the Pile model to keep it 100% RNN.</p>
</li>
<li>
<p dir="auto">Extra R-gate in the FFN (applicable to all transformers). I am also using reluSquared from Primer.</p>
</li>
<li>
<p dir="auto">Better initilization: I init most of the matrices to ZERO (see RWKV_Init in <a href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py">https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN/src/model.py</a>).</p>
</li>
<li>
<p dir="auto">You can transfer some parameters from a small model to a large model (note: I sort &amp; smooth them too), for faster and better convergence (see <a href="https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/" rel="nofollow">https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/</a>).</p>
</li>
<li>
<p dir="auto">My CUDA kernel: <a href="https://github.com/BlinkDL/RWKV-CUDA">https://github.com/BlinkDL/RWKV-CUDA</a> to speedup training.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-the-pseudocode-execution-from-top-to-bottom" aria-hidden="true" href="#the-pseudocode-execution-from-top-to-bottom"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The pseudocode (execution from top to bottom):</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v2-RNN.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-v2-RNN.png" alt="RWKV-v2-RNN"/></a></p>
<p dir="auto">The a b c d factors work together to build a time-decay curve: [X, 1, W, W^2, W^3, ...].</p>
<p dir="auto">Write out the formulas for &#34;token at pos 2&#34; and &#34;token at pos 3&#34; and you will get the idea:</p>
<ul dir="auto">
<li>a and b: EMAs of kv and k.</li>
<li>c and d: these are a and b combined with &#34;self-attention&#34;.</li>
</ul>
<p dir="auto">kv / k is the memory mechanism. The token with high k can be remembered for a long duration, if W is close to 1 in the channel.</p>
<p dir="auto">The R-gate is important for performance. k = info strength of this token (to be passed to future tokens). r = whether to apply the info to this token.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-rwkv-3-improvements" aria-hidden="true" href="#rwkv-3-improvements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>RWKV-3 improvements</h2>
<p dir="auto">Use different trainable TimeMix factors for R / K / V in SA and FF layers. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="xx = self.time_shift(x)
xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)"><pre><span>xx</span> <span>=</span> <span>self</span>.<span>time_shift</span>(<span>x</span>)
<span>xk</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_k</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_k</span>)
<span>xv</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_v</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_v</span>)
<span>xr</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_r</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_r</span>)</pre></div>
<p dir="auto">Use preLN instead of postLN (more stable &amp; faster convergence):</p>
<div dir="auto" data-snippet-clipboard-copy-content="if self.layer_id == 0:
	x = self.ln0(x)
x = x + self.att(self.ln1(x))
x = x + self.ffn(self.ln2(x))"><pre><span>if</span> <span>self</span>.<span>layer_id</span> <span>==</span> <span>0</span>:
	<span>x</span> <span>=</span> <span>self</span>.<span>ln0</span>(<span>x</span>)
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>att</span>(<span>self</span>.<span>ln1</span>(<span>x</span>))
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>ffn</span>(<span>self</span>.<span>ln2</span>(<span>x</span>))</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-explaining-the-code-for-rwkv-3-gpt-mode" aria-hidden="true" href="#explaining-the-code-for-rwkv-3-gpt-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Explaining the code for RWKV-3 GPT mode</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-the-gpt-mode---overview" aria-hidden="true" href="#the-gpt-mode---overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The GPT mode - overview</h3>
<p dir="auto">The building blocks of RWKV-3 GPT mode are similar to that of a usual preLN GPT.</p>
<p dir="auto">The only difference is an extra LN after embedding. Note you can absorb this LN into the embedding after finishing the training.</p>
<div dir="auto" data-snippet-clipboard-copy-content="x = self.emb(idx)  # input: idx = token indices
x = self.ln_emb(x) # extra LN after embedding
x = x + self.att_0(self.ln_att_0(x)) # preLN
x = x + self.ffn_0(self.ln_ffn_0(x))
...
x = x + self.att_n(self.ln_att_n(x))
x = x + self.ffn_n(self.ln_ffn_n(x))
x = self.ln_head(x) # final LN before projection
x = self.head(x)    # output: x = logits"><pre><span>x</span> <span>=</span> <span>self</span>.<span>emb</span>(<span>idx</span>)  <span># input: idx = token indices</span>
<span>x</span> <span>=</span> <span>self</span>.<span>ln_emb</span>(<span>x</span>) <span># extra LN after embedding</span>
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>att_0</span>(<span>self</span>.<span>ln_att_0</span>(<span>x</span>)) <span># preLN</span>
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>ffn_0</span>(<span>self</span>.<span>ln_ffn_0</span>(<span>x</span>))
...
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>att_n</span>(<span>self</span>.<span>ln_att_n</span>(<span>x</span>))
<span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span>.<span>ffn_n</span>(<span>self</span>.<span>ln_ffn_n</span>(<span>x</span>))
<span>x</span> <span>=</span> <span>self</span>.<span>ln_head</span>(<span>x</span>) <span># final LN before projection</span>
<span>x</span> <span>=</span> <span>self</span>.<span>head</span>(<span>x</span>)    <span># output: x = logits</span></pre></div>
<p dir="auto">It is important to initialize emb to tiny values, such as nn.init.uniform_(a=-1e-4, b=1e-4), to utilize my trick <a href="https://github.com/BlinkDL/SmallInitEmb">https://github.com/BlinkDL/SmallInitEmb</a>.</p>
<p dir="auto">For the 1.5B RWKV-3, I use Adam (no wd, no dropout) optimizer on 8 * A100 40G.</p>
<p dir="auto">batchSz = 32 * 896, ctxLen = 896. I am using tf32 so the batchSz is a bit small.</p>
<p dir="auto">For the first 15B tokens, LR is fixed at 3e-4, and beta=(0.9, 0.99).</p>
<p dir="auto">Then I set beta=(0.9, 0.999), and do an exponential decay of LR, reaching 1e-5 at 332B tokens.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-the-gpt-mode---att-block" aria-hidden="true" href="#the-gpt-mode---att-block"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The GPT mode - ATT block</h3>
<p dir="auto">The RWKV-3 does not have any attention in the usual sense, but we will call this block ATT anyway.</p>
<div dir="auto" data-snippet-clipboard-copy-content="B, T, C = x.size() # x = (Batch,Time,Channel)

# Mix x with the previous timestep to produce xk, xv, xr
xx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))
xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)
xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)

# Use xk, xv, xr to produce k, v, r
k = self.key(xk).transpose(-1, -2)
v = self.value(xv).transpose(-1, -2)
r = self.receptance(xr)
k = torch.clamp(k, max=60) # clamp k to avoid overflow
k = torch.exp(k)
kv = k * v

# Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]
self.time_w = torch.cat([torch.exp(self.time_decay) * self.time_curve.to(x.device), self.time_first], dim=-1)
w = torch.exp(self.time_w)

# Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero
if RUN_DEVICE == &#39;cuda&#39;:
    wkv = TimeX.apply(w, kv, B,C,T, 0)
    wk = TimeX.apply(w, k, B,C,T, K_EPS)
else:
    w = w[:,-T:].unsqueeze(1)
    wkv = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(kv), w, groups=C)
    wk = F.conv1d(nn.ZeroPad2d((T-1, 0, 0, 0))(k), w, groups=C) + K_EPS

# The RWKV formula
rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)
rwkv = self.output(rwkv) # final output projection"><pre><span>B</span>, <span>T</span>, <span>C</span> <span>=</span> <span>x</span>.<span>size</span>() <span># x = (Batch,Time,Channel)</span>

<span># Mix x with the previous timestep to produce xk, xv, xr</span>
<span>xx</span> <span>=</span> <span>self</span>.<span>time_shift</span>(<span>x</span>) <span># self.time_shift = nn.ZeroPad2d((0,0,1,-1))</span>
<span>xk</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_k</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_k</span>)
<span>xv</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_v</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_v</span>)
<span>xr</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_r</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_r</span>)

<span># Use xk, xv, xr to produce k, v, r</span>
<span>k</span> <span>=</span> <span>self</span>.<span>key</span>(<span>xk</span>).<span>transpose</span>(<span>-</span><span>1</span>, <span>-</span><span>2</span>)
<span>v</span> <span>=</span> <span>self</span>.<span>value</span>(<span>xv</span>).<span>transpose</span>(<span>-</span><span>1</span>, <span>-</span><span>2</span>)
<span>r</span> <span>=</span> <span>self</span>.<span>receptance</span>(<span>xr</span>)
<span>k</span> <span>=</span> <span>torch</span>.<span>clamp</span>(<span>k</span>, <span>max</span><span>=</span><span>60</span>) <span># clamp k to avoid overflow</span>
<span>k</span> <span>=</span> <span>torch</span>.<span>exp</span>(<span>k</span>)
<span>kv</span> <span>=</span> <span>k</span> <span>*</span> <span>v</span>

<span># Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]</span>
<span>self</span>.<span>time_w</span> <span>=</span> <span>torch</span>.<span>cat</span>([<span>torch</span>.<span>exp</span>(<span>self</span>.<span>time_decay</span>) <span>*</span> <span>self</span>.<span>time_curve</span>.<span>to</span>(<span>x</span>.<span>device</span>), <span>self</span>.<span>time_first</span>], <span>dim</span><span>=</span><span>-</span><span>1</span>)
<span>w</span> <span>=</span> <span>torch</span>.<span>exp</span>(<span>self</span>.<span>time_w</span>)

<span># Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero</span>
<span>if</span> <span>RUN_DEVICE</span> <span>==</span> <span>&#39;cuda&#39;</span>:
    <span>wkv</span> <span>=</span> <span>TimeX</span>.<span>apply</span>(<span>w</span>, <span>kv</span>, <span>B</span>,<span>C</span>,<span>T</span>, <span>0</span>)
    <span>wk</span> <span>=</span> <span>TimeX</span>.<span>apply</span>(<span>w</span>, <span>k</span>, <span>B</span>,<span>C</span>,<span>T</span>, <span>K_EPS</span>)
<span>else</span>:
    <span>w</span> <span>=</span> <span>w</span>[:,<span>-</span><span>T</span>:].<span>unsqueeze</span>(<span>1</span>)
    <span>wkv</span> <span>=</span> <span>F</span>.<span>conv1d</span>(<span>nn</span>.<span>ZeroPad2d</span>((<span>T</span><span>-</span><span>1</span>, <span>0</span>, <span>0</span>, <span>0</span>))(<span>kv</span>), <span>w</span>, <span>groups</span><span>=</span><span>C</span>)
    <span>wk</span> <span>=</span> <span>F</span>.<span>conv1d</span>(<span>nn</span>.<span>ZeroPad2d</span>((<span>T</span><span>-</span><span>1</span>, <span>0</span>, <span>0</span>, <span>0</span>))(<span>k</span>), <span>w</span>, <span>groups</span><span>=</span><span>C</span>) <span>+</span> <span>K_EPS</span>

<span># The RWKV formula</span>
<span>rwkv</span> <span>=</span> <span>torch</span>.<span>sigmoid</span>(<span>r</span>) <span>*</span> (<span>wkv</span> <span>/</span> <span>wk</span>).<span>transpose</span>(<span>-</span><span>1</span>, <span>-</span><span>2</span>)
<span>rwkv</span> <span>=</span> <span>self</span>.<span>output</span>(<span>rwkv</span>) <span># final output projection</span></pre></div>
<p dir="auto">The self.key, self.receptance, self.output matrices are all initialized to zero.</p>
<p dir="auto">The time_mix, time_decay, time_first vectors are transferred from a smaller trained model (note: I sort &amp; smooth them too).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-the-gpt-mode---ffn-block" aria-hidden="true" href="#the-gpt-mode---ffn-block"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The GPT mode - FFN block</h3>
<p dir="auto">The FFN block has three tricks comparing with the usual GPT:</p>
<ol dir="auto">
<li>
<p dir="auto">My time_mix trick.</p>
</li>
<li>
<p dir="auto">The sqReLU from the Primer paper.</p>
</li>
<li>
<p dir="auto">An extra receptance-gate (similar to the receptance-gate in ATT block).</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Mix x with the previous timestep to produce xk, xr
xx = self.time_shift(x)
xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)
xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)

# The usual FFN operation
k = self.key(xk)
k = torch.square(torch.relu(k)) # from the Primer paper
kv = self.value(k)

# Apply an extra receptance-gate to kv
rkv = torch.sigmoid(self.receptance(xr)) * kv
return rkv"><pre><span># Mix x with the previous timestep to produce xk, xr</span>
<span>xx</span> <span>=</span> <span>self</span>.<span>time_shift</span>(<span>x</span>)
<span>xk</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_k</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_k</span>)
<span>xr</span> <span>=</span> <span>x</span> <span>*</span> <span>self</span>.<span>time_mix_r</span> <span>+</span> <span>xx</span> <span>*</span> (<span>1</span> <span>-</span> <span>self</span>.<span>time_mix_r</span>)

<span># The usual FFN operation</span>
<span>k</span> <span>=</span> <span>self</span>.<span>key</span>(<span>xk</span>)
<span>k</span> <span>=</span> <span>torch</span>.<span>square</span>(<span>torch</span>.<span>relu</span>(<span>k</span>)) <span># from the Primer paper</span>
<span>kv</span> <span>=</span> <span>self</span>.<span>value</span>(<span>k</span>)

<span># Apply an extra receptance-gate to kv</span>
<span>rkv</span> <span>=</span> <span>torch</span>.<span>sigmoid</span>(<span>self</span>.<span>receptance</span>(<span>xr</span>)) <span>*</span> <span>kv</span>
<span>return</span> <span>rkv</span></pre></div>
<p dir="auto">The self.value, self.receptance matrices are all initialized to zero.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-rwkv-4-improvements" aria-hidden="true" href="#rwkv-4-improvements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>RWKV-4 improvements</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-v3-plan.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-v3-plan.png" alt="RWKV-v3-plan"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-from-gpt-to-rwkv-the-formulas" aria-hidden="true" href="#from-gpt-to-rwkv-the-formulas"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>From GPT to RWKV (the formulas)</h2>
<p dir="auto">Let F[t] be the system state at t.</p>
<p dir="auto">Let x[t] be the new external input at t.</p>
<p dir="auto">In GPT, predicting F[t+1] requires considering F[0], F[1], .. F[t]. So it takes O(T^2) to generate a length T sequence.</p>
<p dir="auto">The <strong>simplified formula</strong> for GPT:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D" alt="F[\mathrm{t}+1]=\frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}])}"/></a></p>
<p dir="auto">It&#39;s very capable in theory, however that <strong>does not mean we can fully utilize its capability with usual optimizers</strong>. I suspect the loss landscape is too difficult for our current methods.</p>
<p dir="auto">Compare with the <strong>simplified formula</strong> for RWKV (the parallel mode, looks similar to Apple&#39;s AFT):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B%5Cmathrm%7Bt%7D%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cfrac%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D%7B%5Csum_%7B%5Cmathrm%7Bi%7D%3D0%7D%5E%7B%5Cmathrm%7Bt%7D%7D+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B%5Cmathrm%7Bi%7D%5D%29%7D" alt="F[\mathrm{t}+1]=\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \frac{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) \cdot(\mathbf{V}F[\mathrm{i}])}{\sum_{\mathrm{i}=0}^{\mathrm{t}} \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K }F[\mathrm{i}])}"/></a></p>
<p dir="auto">The R, K, V are trainable matrices, and W is a trainable vector (time-decay factor for each channel).</p>
<p dir="auto">In GPT, the contribution of F[i] to F[t+1] is weighted by <a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle++%5Cexp+%28%5Cmathbf%7BQ%7Dx%5B%5Cmathrm%7Bt%7D%5D+%2A+%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+" alt=" \exp (\mathbf{Q}x[\mathrm{t}] * \mathbf{K}F[\mathrm{i}]) "/></a>.</p>
<p dir="auto">In RWKV-2, the contribution of F[i] to F[t+1] is weighted by <a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+%5Csigma%28%5Cmathbf%7BR%7Dx%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BW%7D+%5Ccdot%28%5Cmathrm%7Bt%7D-%5Cmathrm%7Bi%7D%29%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bi%7D%5D%29+" alt="\sigma(\mathbf{R}x[\mathrm{t}]) \cdot \exp (\mathbf{W} \cdot(\mathrm{t}-\mathrm{i})) \cdot \exp (\mathbf{K}F[\mathrm{i}]) "/></a>.</p>

<p dir="auto">Here comes the punchline: we can rewrite it into a RNN (recursive formula). Note:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B0%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D" alt="F[1]=\sigma(\mathbf{R }x[0]) \cdot \frac{ \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{\exp (\mathbf{K }F[0])}"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5B2%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5B1%5D%29+%5Ccdot+%5Cfrac%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29+%5Ccdot%28%5Cmathbf%7BV+%7DF%5B0%5D%29%7D%7B+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B1%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D+%29+%5Ccdot+%5Cexp+%28%5Cmathbf%7BK+%7DF%5B0%5D%29%7D" alt="F[2]=\sigma(\mathbf{R }x[1]) \cdot \frac{ \exp (\mathbf{K }F[1]) \cdot(\mathbf{V }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0]) \cdot(\mathbf{V }F[0])}{ \exp (\mathbf{K }F[1])+\exp (\mathbf{W} ) \cdot \exp (\mathbf{K }F[0])}"/></a></p>
<p dir="auto">Therefore it&#39;s straightforward to verify:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+F%5Bt%2B1%5D%3D%5Csigma%28%5Cmathbf%7BR+%7Dx%5Bt%5D%29+%5Ccdot+%5Cfrac%7B%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29+%5Ccdot%28%5Cmathbf%7BV%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+A%5B%5Cmathrm%7Bt%7D%5D%7D%7B+%5Cexp+%28%5Cmathbf%7BK%7DF%5B%5Cmathrm%7Bt%7D%5D%29%2B%5Cexp+%28%5Cmathbf%7BW%7D%29+%5Ccdot+B%5B%5Cmathrm%7Bt%7D%5D%7D" alt="F[t+1]=\sigma(\mathbf{R }x[t]) \cdot \frac{\exp (\mathbf{K}F[\mathrm{t}]) \cdot(\mathbf{V}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot A[\mathrm{t}]}{ \exp (\mathbf{K}F[\mathrm{t}])+\exp (\mathbf{W}) \cdot B[\mathrm{t}]}"/></a></p>
<p dir="auto">where A[t] and B[t] are the numerator and denominator of the previous step, respectively.</p>
<p dir="auto">I believe RWKV is performant because W is like repeatedly applying a diagonal matrix. Note (P^{-1} D P)^n = P^{-1} D^n P, so it is similar to repeatedly applying a general diagonalizable matrix.</p>
<p dir="auto">Moreover it&#39;s possible to turn it into a continuous ODE (a bit similar to State Space Models). I will write about it later.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-star-history" aria-hidden="true" href="#star-history"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Star History</h2>
<p dir="auto"><a href="https://star-history.com/#BlinkDL/RWKV-LM&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/6234c23cba2eb7bd2e97f00a0d56f4e277c8d463d43eecf4a430a2fb116969ae/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d426c696e6b444c2f52574b562d4c4d26747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=BlinkDL/RWKV-LM&amp;type=Date"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-multimodal-ideas" aria-hidden="true" href="#multimodal-ideas"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Multimodal ideas</h2>
<p dir="auto">I have an idea for [text --&gt; 32x32 RGB image] using a LM (transformer, RWKV, etc.). Will test it soon.</p>
<p dir="auto">Firstly, LM loss (instead of L2 loss), so the image will not be blurry.</p>
<p dir="auto">Secondly, color quantization. For example, only allowing 8 levels for R/G/B. Then the image vocab size is 8x8x8 = 512 (for each pixel), instead of 2^24.
Therefore, a 32x32 RGB image = a len1024 sequence of vocab512 (image tokens), which is a typical input for usual LMs.
(Later we can use diffusion models to upsample and generate RGB888 images. We might be able to use a LM for this too.)</p>
<p dir="auto">Thirdly, 2D positional embeddings that are easy for the model to understand.
For example, add one-hot X &amp; Y coords to the first 64(=32+32) channels. Say if the pixel is at x=8, y=20, then we will add 1 to channel 8 and channel 52 (=32+20).
Moreover probably we can add the float X &amp; Y coords (normalized to 0~1 range) to another 2 channels. And other periodic pos. encoding might help too (will test).</p>
<p dir="auto">Finally, RandRound when doing the color quantization in the DataLoader.
For example, if the float level is 4.578, then there is a 57.8% chance to use 5, and (1-57.8%) chance to use 4.
And we can allow both 4 and 5 in the prediction, but the loss will be higher if the prediction is 4.</p>
<p dir="auto">Multi-task training might help too. I will try this dataset format:
[TxtFirst] [Desc of Img (txt tokens)] [Img] [img tokens]
and sometimes
[ImgFirst] [img tokens] [Txt] [Desc of Img (txt tokens)]
... the order of the imgs should be randomized in the DataLoader, and [TxtFirst] [ImgFirst] [Img] [Txt] are special tokens
and do random sampling of the full dataset. So sometimes the model will see the img tokens first and then the corresponding txt tokens, which is a [img -&gt; txt] task. And the model will see some partial imgs and partial txts. I think a char-level LM might help the model to write correct text on images.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-to-sample-a-large-dataset-for-training" aria-hidden="true" href="#how-to-sample-a-large-dataset-for-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to sample a large dataset (for training)</h2>
<p dir="auto">I am using a trick to sample the Pile deterministically yet randomly enough.</p>
<p dir="auto">Let&#39;s say the pile has x chunks (a chunk = ctx_len tokens).</p>
<p dir="auto">pick a prime number p just less than x, and make sure p = 2 (mod 3).</p>
<p dir="auto">Use (step * step * step) mod p to sample it. Add some bias to step for extra randomness.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-the-top-p-x-sampling-method-for-inference" aria-hidden="true" href="#the-top-p-x-sampling-method-for-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>The top-p-x sampling method (for inference)</h2>
<p dir="auto">We propose a new sampling method called top-p-x:</p>
<p dir="auto">it&#39;s like top-p, and the only difference is you also keep all tokens whose prob &gt; x.</p>
<p dir="auto">Try x = 0.01 first.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-better-learning-rate-schedule-via-variantional-method-of-loss-curve" aria-hidden="true" href="#better-learning-rate-schedule-via-variantional-method-of-loss-curve"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Better Learning Rate Schedule via Variantional Method of Loss Curve</h2>
<p dir="auto">I propose a simple new method to find better LR schedules. The method is cost-efficient and practical for large LMs. The takeaway is we can model the loss curve dynamics (phenomenology) w.r.t. the LR, and a nice closed-form LR curve can be directly computed from it using variantional method. Moreover we can predict the final loss with reasonable accuracy.</p>
<p dir="auto">UPDATE: In &#34;Conclusion 1.&#34;, use the best-fitting regime (ignore the initial steps where our approximations break down) to fit the parameters.</p>
<p dir="auto">Try this: fixed lr for 1 hr, then exponential decay to 0.2 * lr in 12 hrs, and choose the t=[1hr, 13hr] segment.</p>
<p dir="auto">In the last three plots, black = predicted loss curve of the new LR schedule, blue = original (unoptimized) real loss curve, orange = new LR schedule.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/Research/better_lr_schedule.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/Research/better_lr_schedule.png" alt="better_lr_schedule"/></a></p>

<p dir="auto">We propose the RWKV language model, with alternating time-mix and channel-mix layers:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A"><img src="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cbegin%7Balign%2A%7D%0A%5Ctext%7BTime-mix+%3A%7D+%26%26+%5Ctext%7BTM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_%7Bu%7D+%26%26%5Ctextbf%7BW%7D_%7Bt%2Cu%2Cc%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bu%2Cc%7D%5C%5C%0A%5Ctext%7BChannel-mix+%3A%7D+%26%26+%5Ctext%7BCM%7D_%7Bt%2Cc%7D+%26%26%3D%26%26%5Ctext%7Bsigmoid%7D%28%5Ctext%7BR%7D_%7Bt%2Cc%7D%29+%26%26%5Ccdot%26%26+%26%26%5Ctextstyle%5Csum_d+%26%26%5Ctextbf%7BW%7D_%7Bc%2Cd%7D+%26%26%5Ccdot%26%26+%5Ctext%7Bgelu%7D%28%5Ctext%7BK%7D_%7Bt%2Cd%7D%29+%26%26%5Ccdot%26%26+%5Ctext%7BV%7D_%7Bt%2Cd%7D%0A%5Cend%7Balign%2A%7D%0A" alt="\begin{align*}
\text{Time-mix :} &amp;&amp; \text{TM}_{t,c} &amp;&amp;=&amp;&amp;\text{sigmoid}(\text{R}_{t,c}) &amp;&amp;\cdot&amp;&amp; &amp;&amp;\textstyle\sum_{u} &amp;&amp;\textbf{W}_{t,u,c} &amp;&amp;\cdot&amp;&amp; \text{softmax}_t(\text{K}_{u,c}) &amp;&amp;\cdot&amp;&amp; \text{V}_{u,c}\\
\text{Channel-mix :} &amp;&amp; \text{CM}_{t,c} &amp;&amp;=&amp;&amp;\text{sigmoid}(\text{R}_{t,c}) &amp;&amp;\cdot&amp;&amp; &amp;&amp;\textstyle\sum_d &amp;&amp;\textbf{W}_{c,d} &amp;&amp;\cdot&amp;&amp; \text{gelu}(\text{K}_{t,d}) &amp;&amp;\cdot&amp;&amp; \text{V}_{t,d}
\end{align*}
"/></a></p>
<ul dir="auto">
<li>
<p dir="auto">The R, K, V are generated by linear transforms of input, and W is parameter. The idea of RWKV is to decompose attention into R(target) * W(src, target) * K(src). So we can call R &#34;receptance&#34;, and sigmoid means it&#39;s in 0~1 range.</p>
</li>
<li>
<p dir="auto">The Time-mix is similar to AFT (<a href="https://arxiv.org/abs/2105.14103" rel="nofollow">https://arxiv.org/abs/2105.14103</a>). There are two differences.</p>
</li>
</ul>
<p dir="auto">(1) We changed the normalization (denominator). For masked language models, we define:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D"><img src="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Bsoftmax%7D_t%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29+%3D+%5Cfrac%7B%5Cexp%28%5Ctext%7BK%7D_%7Bu%2Cc%7D%29%7D%7B%5Csum_%7Bv+%5Cleq+t%7D%5Cexp%28%5Ctext%7BK%7D_%7Bv%2Cc%7D%29%7D" alt="\text{softmax}_t(\text{K}_{u,c}) = \frac{\exp(\text{K}_{u,c})}{\sum_{v \leq t}\exp(\text{K}_{v,c})}"/></a></p>
<p dir="auto"><strong>(UPDATE: We are using the original AFT normalization in v2)</strong></p>
<p dir="auto">Initialize K and R matrices (and the output projection matrix) to ZERO for fast &amp; stable convergence.</p>
<p dir="auto">(2) We decompose W_{t,u,c} and introduce multi-head W (here h is the corresponding head of c):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29"><img src="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+W_%7Bt%2Cu%2Cc%7D%3Df_h%28t-u%29%5Ccdot+%5Calpha_h%28u%29+%5Ccdot+%5Cbeta_h%28t%29" alt="W_{t,u,c}=f_h(t-u)\cdot \alpha_h(u) \cdot \beta_h(t)"/></a></p>
<p dir="auto">Moreover we multiply the final output of Time-mix layer by Î³(t). The reason for the Î± Î² Î³ factors, is because the context size is smaller when t is small, and this can be compensated using the Î± Î² Î³ factors.</p>
<p dir="auto"><strong>(UPDATE: We remove Î± Î² Î³ factors in v2-RNN and restrict W to be of a simple form and hence able to rewrite it as RNN)</strong></p>
<ul dir="auto">
<li>
<p dir="auto">The Channel-mix is similar to GeGLU (<a href="https://arxiv.org/abs/2002.05202" rel="nofollow">https://arxiv.org/abs/2002.05202</a>) with an extra R factor. Initialize R and W matrices to ZERO for fast &amp; stable convergence.</p>
</li>
<li>
<p dir="auto">Finally, we add extra token-shift (time-shift mixing) as in (<a href="https://github.com/BlinkDL/minGPT-tuned">https://github.com/BlinkDL/minGPT-tuned</a>).</p>
</li>
</ul>

<p dir="auto">The token-shift explicitly uses (half the channels of this token) &amp; (half the channels of prev token) to generate all vectors (QKV, RWKV, ...).</p>
<div data-snippet-clipboard-copy-content="self.time_shift = nn.ZeroPad2d((0,0,1,-1))

x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)"><pre><code>self.time_shift = nn.ZeroPad2d((0,0,1,-1))

x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)
</code></pre></div>
<p dir="auto">Dividing channels by 2 and shift-1 works great for char-level English and char-level Chinese LM.</p>
<p dir="auto">However for BPE-level English LM, it&#39;s only effective if your embedding is large enough (at least 1024 - so the usual small L12-D768 model is not enough).</p>
<p dir="auto">My theory on the effectiveness of token-shift:</p>
<p dir="auto">When we train a GPT, the hidden representation of a token has to accomplish two different objects:</p>
<ol dir="auto">
<li>
<p dir="auto">Predict the next token. Sometimes this is easy (obvious next token).</p>
</li>
<li>
<p dir="auto">Collect all previous context info, so later tokens can use it. This is always hard.</p>
</li>
</ol>
<p dir="auto">The shifted channels can focus on (2), so we have good propagation of info. It&#39;s like some kind of residual connection, or a small RNN inside the transformer.</p>
<p dir="auto">You can use token-shift in usual QKV self-attention too. I looked at the weights, and found V really likes the shifted channels, less so for Q. Makes sense if you think about it. I also found you may want to use less mixing in higher layers.</p>
<p dir="auto">p.s. There is a MHA_pro model in this repo with strong performance. Give it a try :)</p>

<p dir="auto">In usual transformer, a small model has difficulty copying tokens (such as person names) in the context. We add extra Q &amp; K to the final output such that the model can directly copy (or avoid) tokens in the context. Afterwards the model will teach itself NER (named entity recognition) if you look at the learned weights.</p>
<div data-snippet-clipboard-copy-content="q = self.head_q(x)[:,:T,:] # projecting to 256-d
k = self.head_k(x)[:,:T,:] # projecting to 256-d
c = (q @ k.transpose(-2, -1)) * (1.0 / 256)
c = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)
c = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       
x = self.head(x) + c"><pre><code>q = self.head_q(x)[:,:T,:] # projecting to 256-d
k = self.head_k(x)[:,:T,:] # projecting to 256-d
c = (q @ k.transpose(-2, -1)) * (1.0 / 256)
c = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)
c = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()       
x = self.head(x) + c
</code></pre></div>
<p dir="auto">Note: when a token occurs multiple times in the context, it might be better to use max(prob) instead of sum(prob).</p>

<p dir="auto">We also propose a new sampling method called top-a (as in src/utils.py):</p>
<p dir="auto">(1) Find the max probability p_max after softmax.</p>
<p dir="auto">(2) Remove all entries whose probability is lower than 0.2 * pow(p_max, 2). So it&#39;s adaptive, hence &#34;top-a&#34;.</p>
<p dir="auto">(3) Feel free to tune the 0.2 and 2 factor. Tune 0.2 first.</p>
<p dir="auto">The idea of top-a:</p>
<ol dir="auto">
<li>If max_prob=0.9, then remove all tokens with prob &lt; 0.162 (so, removing all alternatives)</li>
<li>If max_prob=0.5, then remove all tokens with prob &lt; 0.05  (so, allowing more choices)</li>
<li>If max_prob=0.1, then remove all tokens with prob &lt; 0.002 (so, allowing lots of possibilities)</li>
</ol>
<div data-snippet-clipboard-copy-content="probs = F.softmax(logits, dim=-1)

limit = torch.pow(torch.max(probs), 2) * 0.02
logits[probs &lt; limit] = -float(&#39;Inf&#39;)"><pre><code>probs = F.softmax(logits, dim=-1)

limit = torch.pow(torch.max(probs), 2) * 0.02
logits[probs &lt; limit] = -float(&#39;Inf&#39;)
</code></pre></div>

<p dir="auto">Character-level loss on simplebooks-92 dataset <a href="https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip" rel="nofollow">https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-vs-MHA.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-vs-MHA.png" alt="RWKV-vs-MHA"/></a></p>
<p dir="auto">Gray: usual MHA+Rotary+GeGLU - performance not as good. 17.2M params.</p>
<p dir="auto">Red: RWKV (&#34;linear&#34; attention) - VRAM friendly - quite faster when ctx window is long - good performance. 16.6M params.</p>
<p dir="auto">Green: MHA+Rotary+GeGLU+Token_shift. 17.2M params.</p>
<p dir="auto">Blue: MHA_pro (MHA with various tweaks &amp; RWKV-type-FFN) - slow - needs more VRAM - good performance. 16.6M params.</p>
<div data-snippet-clipboard-copy-content="@software{peng_bo_2021_5196578,
  author       = {PENG Bo},
  title        = {BlinkDL/RWKV-LM: 0.01},
  month        = aug,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {0.01},
  doi          = {10.5281/zenodo.5196577},
  url          = {https://doi.org/10.5281/zenodo.5196577}
}"><pre><code>@software{peng_bo_2021_5196578,
  author       = {PENG Bo},
  title        = {BlinkDL/RWKV-LM: 0.01},
  month        = aug,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {0.01},
  doi          = {10.5281/zenodo.5196577},
  url          = {https://doi.org/10.5281/zenodo.5196577}
}
</code></pre></div>

<p dir="auto">We use careful initialization for RWKV to get fast convergence - orthogonal matrices with proper scaling, and special time_w curves. Check model.py for details.</p>
<p dir="auto">Some learned time_w examples:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/BlinkDL/RWKV-LM/blob/main/RWKV-time-w.png"><img src="https://github.com/BlinkDL/RWKV-LM/raw/main/RWKV-time-w.png" alt="RWKV-time-w"/></a></p>
</article>
          </div></div>
  </body>
</html>
