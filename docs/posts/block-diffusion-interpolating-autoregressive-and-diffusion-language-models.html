<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://m-arriola.com/bd3lms/">Original</a>
    <h1>Block Diffusion: Interpolating Autoregressive and Diffusion Language Models</h1>
    
    <div id="readability-page-1" class="page">

  <section>
    
    
  
</section>


<!-- Teaser video-->
<section>
  <div>
    <div>
      <h2>
        <strong>Autoregression</strong>: 
        <span>✅ High quality</span> 
        <span>✅ Arbitrary-length</span>
        <span>✅ KV caching</span>
        <span>❌ Not parallelizable</span> 
      </h2>
      <p><img src="https://m-arriola.com/bd3lms/static/images/ar.gif"/></p><h2>
        <strong>Diffusion</strong>: 
        <span>❌ Lower quality</span> 
        <span>❌ Fixed-length</span> 
        <span>❌ No KV caching</span> 
        <span>✅ Parallelizable</span> 
      </h2>
      <p><img src="https://m-arriola.com/bd3lms/static/images/mdlm.gif"/></p><h2>
        <!-- <strong>Previous Diffusion LMs</strong>: 
        <span style="color: red; padding-left: 10px;">❌ Lower quality</span> 
        <span style="color: red; padding-left: 10px;">❌ Fixed-length</span> 
        <span style="color: red; padding-left: 10px;">❌ No KV caching</span> 
        <span style="color: green; padding-left: 10px;">✅ Parallelizable</span> 
        <br><br> -->
        <strong>Block Diffusion</strong>: 
        <span>✅ High quality</span> 
        <span>✅ Arbitrary-length</span> 
        <span>✅ KV caching</span>
        <span>✅ Parallelizable</span> 
      </h2>
      <p><img src="https://m-arriola.com/bd3lms/static/images/bd3.gif"/>
      </p></div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- AR VS DIFFUSION -->
<section id="SEC1">
  <div>
    <div>
      <h2>Autoregressive vs. Diffusion Language Models</h2>
      <p>
        In the language modeling task, we have a sequence of \( L \) tokens \( \mathbf{x} = (\mathbf{x}^1, \dots, \mathbf{x}^L ) \) drawn from the data distribution \( q(\mathbf{x}) \). We aim to fit a model \( p_\theta(\mathbf{x}) \) of \( q \).
      </p><p>
        Autoregressive models define a factorized distribution of the form:
      </p>
      <p>
        \[ \log p_\theta(\mathbf{x}) = \sum_{\ell=1}^L \log p_\theta(\mathbf{x}^\ell \mid \mathbf{x}^{\lt \ell}) \]
      </p>
      <p>However, the sequential dependencies between tokens require that AR sampling is restricted to \( L \) sampling steps, which may be slow for long sequences.</p>
      <p>Diffusion models overcome this limitation by modeling tokens independently, admitting parallel generation. Diffusion models instead fit a model to undo a forward corruption process \( q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \text{Cat}(\mathbf{x_t} ; Q_t \mathbf{x}_{t-1} ) \) using transition matrices \( Q_t \). D3PM (<a href="https://arxiv.org/abs/2107.03006">Austin et. al</a>) defines this as</p>
      <p>
        \[ p_\theta(\mathbf{x}_s | \mathbf{x}_t) = \prod_{\ell=1}^L p_\theta (\mathbf{x}_s^\ell | \mathbf{x}_t) = \sum_{\mathbf{x}} \left[q(\mathbf{x}_s^{\ell} | \mathbf{x}_t^\ell, \mathbf{x}^\ell) p_\theta(\mathbf{x}^{\ell} | \mathbf{x}_t)  \right] \]
      </p>
      <p>where the denoising base model \( p_\theta(\mathbf{x}^\ell | \mathbf{x}_t) \) predicts clean token \( \mathbf{x}^\ell \) given noised tokens \( \mathbf{x}_t \). However, the diffusion objective minimizes a bound on the likelihood. As a result, diffusion models lag in terms of likelihood and sample quality. Furthermore, diffusion models are restricted to generate fixed length sequences.</p>
    </div>
  </div>
</section>

<!-- BD3-LMs -->




<section id="SEC2">
  <div>
    <div>
      <h2>BD3-LMs: Block Discrete Denoising Diffusion Language Models</h2>
      <p>We combine modeling paradigms to enjoy better likelihoods &amp; flexible-length generation from autoregressive models, as well as fast &amp; parallel generation from diffusion models.</p> 
      <h4>Block Diffusion Likelihood</h4>
      <p>We propose a modeling framework that autoregressively models blocks of tokens and performs diffusion within each block. Our likelihood factorizes over \( B \) blocks of length \( L&#39; \) as</p>
      <p>
        \[ \log p_\theta (\mathbf{x}) = \sum_{b=1}^B \log p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \]
      </p>
      <p>Each \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) is modeled using discrete diffusion ELBO over a block of \( L&#39; \) tokens. We obtain a principled learning objective \( \mathcal{L}_\text{BD}(\mathbf{x}, \theta) \) by optimizing the following likelihood bound:</p>
      <p>
        \[ \log p_\theta(\mathbf{x}) \geq \mathcal{L}_\text{BD}(\mathbf{x}, \theta) := \sum_{b=1}^{B} \mathcal{L}_{\text{diffusion}}(\mathbf{x}^b, \mathbf{x}^{\lt b}, \theta), \]
      </p><p>
      We model the per-block likelihood under a simple discrete diffusion parameterization (<a href="https://arxiv.org/abs/2406.07524">Sahoo et. al</a>, <a href="https://arxiv.org/abs/2406.04329">Shi et. al</a>, <a href="https://arxiv.org/abs/2406.03736">Ou et. al</a>). Our final objective becomes a sum of weighted cross-entropy terms:
      </p><p>
        \[ \mathcal{L}_\text{BD}(\mathbf{x}, \theta) :=  - \sum_{b=1}^{B} \mathbb{E}_{t \sim [0, 1]} \mathbb{E}_{q} \frac{1}{t} \log p_\theta(\mathbf{x}^b | \mathbf{x}_{t}^b, \mathbf{x}^{\lt b}) \]
      </p>
      <h4>Efficient Training &amp; Sampling Algorithms</h4>
      <p> Naively, we would compute the logits by applying \( \mathbf{x}_\theta^b( \mathbf{x}_t^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \) in a loop \( B\) times. Instead, we only require two forward passes. The first pass precomputes keys and values \( \mathbf{K}^{1:B}, \mathbf{V}^{1:B} \) for the full sequence \( \mathbf{x}\). In the second forward pass, we compute denoised predictions for all blocks simulatenously using  \( \mathbf{x}_\theta^b( \mathbf{x}_t^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \). </p>
      <p> To sample from BD3-LMs, we generate one block at a time, conditioned on previously sampled blocks. After generating a block, we cache its keys and values, similar to AR. We may use any diffusion sampling procedure \( \text{SAMPLE} ( \mathbf{x}_\theta^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \) to sample from the conditional distribution \( p_\theta (\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^{ &lt; b}) \) over \( T\) sampling steps per block.</p>
      <section>
        <div>
        <p><img src="https://m-arriola.com/bd3lms/static/images/alg.png" alt="mask"/></p><p><i>BD3-LM training and sampling algorithms.</i>
      </p>
        </div>
      </section>
    </div>
  </div>
</section>

<!-- GAP -->
<section id="SEC2">
  <div>
    <div>
      <h2>Understanding Likelihood Gaps Between Diffusion and AR Models</h2>
      <h4>Case Study: Single Token Generation</h4>
      <p>Our block diffusion parameterization is equivalent in expectation to the autoregressive NLL in the limiting case where \( L&#39;=1 \). Surprisingly, we find a two point perplexity gap between our block diffusion model for \( L&#39;=1 \) and AR when training both models on the LM1B dataset. We identify high training variance of the diffusion objective as responsible for the perplexity gap.</p>
      <section>
        <div>
        <p><img src="https://m-arriola.com/bd3lms/static/images/bs1.png" alt="mask"/></p><p><i>Training under the discrete diffusion ELBO suffers from high variance.</i>
      </p>
        </div>
      </section>
      <h4>Diffusion Gap from High Variance Training</h4>
        <p>Intuitively, if the sampled masking rate \( t \sim \mathcal{U}[0, 1] \) is too low, reconstructing \( \mathbf{x} \) is easy, which does not provide a useful learning signal. If we mask everything, the optimal reconstruction are the marginals of each token in the data distribution, which is easy to learn, and again not useful.</p><p>
        We seek to find noise schedules that minimize training variance caused by the diffusion objective and further reduce the perplexity gap.</p>
      </div>
  </div>
</section>

<!-- CLIPPED SCHEDULES -->
<section id="SEC2">
  <div>
    <div>
      <h2>Data-Driven Noise Schedules for Low-Variance Training</h2>
      <p>To avoid masking rates that cause high-variance training, we train BD3-LMs under &#34;clipped&#34; masking rates \( t \sim \mathcal{U}[\beta, \omega] \) for \( 0 \leq \beta, \omega \leq 1 \). By reducing the training variance, we improve likelihoods when we evaluate under uniformly sampled mask rates.</p>

      <p>As the optimal mask rates may differ depending on the block size \(L&#39;\), we adaptively learn \( \beta, \omega \) during training. In practice, we do so using a grid search during every validation step, after 5K gradient updates, to optimize \(\min_{\beta, \omega} \text{Var}_{\mathbf{X}, t} \left[ \mathcal{L}_{\text{BD}}(\theta, \beta, \omega; \mathbf{X}) \right] \). </p>
      
      <p>Below, we show that optimizing the noise schedule per block size reduces the variance of the loss estimator and achieves the best perplexities compared to alternative schedules.</p>
    </div>

    <p>
        <center>
        <i>Effect of training under different noise schedules on perplexity (PPL  ↓) on LM1B. All models are finetuned for 50K steps and are evaluated under uniformly sampled mask rates. For our clipped schedules, we compare the optimized clipping rates for \( L&#39;=4, 16 \).</i>
        </center>
      </p><table>
      
      <tbody><tr>
          <th>BD3-LMs</th>
          <th>Noise schedule</th>
          <th>PPL</th>
          <th>Var. ELBO</th>
      </tr>
  
      <!-- Multi-row for L' = 4 -->
      <tr>
          <td rowspan="4">L&#39; = 4</td>
          <td>Linear <em>t</em> ~ <em>U</em>[0, 1]</td>
          <td>30.18</td>
          <td>23.45</td>
      </tr>
      <tr>
          <td>Clipped <em>t</em> ~ <em>U</em>[0.45, 0.95]</td>
          <td>29.21</td>
          <td>6.24</td>
      </tr>
      <tr>
          <td>Clipped <em>t</em> ~ <em>U</em>[0.3, 0.8]</td>
          <td>29.38</td>
          <td>10.33</td>
      </tr>
      <tr>
          <td>Logarithmic</td>
          <td>30.36</td>
          <td>23.53</td>
      </tr>
  
      <!-- Multi-row for L' = 16 -->
      <tr>
          <td rowspan="4">L&#39; = 16</td>
          <td>Linear <em>t</em> ~ <em>U</em>[0, 1]</td>
          <td>31.72</td>
          <td>7.62</td>
      </tr>
      <tr>
          <td>Clipped <em>t</em> ~ <em>U</em>[0.45, 0.95]</td>
          <td>31.42</td>
          <td>3.60</td>
      </tr>
      <tr>
          <td>Clipped linear <em>t</em> ~ <em>U</em>[0.3, 0.8]</td>
          <td>31.12</td>
          <td>3.58</td>
      </tr>
      <tr>
          <td>Cosine</td>
          <td>31.41</td>
          <td>13.00</td>
      </tr>
  </tbody></table>
  </div>
</section>

<!-- RESULTS -->
<section id="SEC2">
  <div>
    <div>
      <h2>Results</h2>
      <h4>Likelihood Evaluation</h4>
      <p>BD3-LMs achieve state-of-the-art likelihoods among diffusion models. As shown below, BD3-LMs interpolate between diffusion and autoregressive likelihoods by tuning the block length \( L&#39; \). </p>
      
      
      <center>
                <i>Test perplexities (PPL; ↓) on OWT for models trained for 262B tokens.</i>
            </center><table>
            
        
        <thead>
            <tr>
                <th>Model</th>
                <th>PPL (↓)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>AR</td>
                <td>17.54</td>
            </tr>
            <tr>
                <td>SEDD</td>
                <td>≤ 24.10</td>
            </tr>
            <tr>
                <td>MDLM</td>
                <td>≤ 22.98</td>
            </tr>
            <tr>
                <td><strong>BD3-LMs</strong> L&#39; = 16</td>
                <td>≤ 22.27</td>
            </tr>
            <tr>
                <td><span>BD3-LMs</span> L&#39; = 8</td>
                <td>≤ 21.68</td>
            </tr>
            <tr>
                <td><span>BD3-LMs</span> L&#39; = 4</td>
                <td><strong>≤ 20.73</strong></td>
            </tr>
        </tbody>
    </table>

      <h4>Arbitrary-length sequence generation</h4>
      <p>One key drawback of many existing diffusion language models is that they cannot generate full-length documents that are longer than the length of the output context chosen at training time. For example, OpenWebText contains documents up to 131K tokens, whereas discrete diffusion model SEDD (<a href="https://arxiv.org/abs/2310.16834">Lou et. al</a>) is restricted to generate 1024 tokens. Below, we show that BD3-LMs can generate variable-length documents by decoding an arbitrary number of blocks.</p>
      
      <p>
        <center>
          <i>Generation length statistics from sampling 500 documents from models trained on OWT.</i>
        </center>
      </p><table>
        
        <thead>
          <tr>
            <th></th>
            <th>Median # tokens</th>
            <th>Max # tokens</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OWT train set</td>
            <td>717</td>
            <td>131K</td>
          </tr>
          <tr>
            <td>AR</td>
            <td>4008</td>
            <td>131K</td>
          </tr>
          <tr>
            <td>SEDD</td>
            <td>1021</td>
            <td>1024</td>
          </tr>
          <tr>
            <td><strong>BD3-LM</strong> L&#39;=16</td>
            <td>798</td>
            <td>9982</td>
          </tr>
        </tbody>
      </table>
      
      
      <p> We assess the generation quality of BD3-LMs on variable-length sequences, comparing all methods using the same number of generation steps (NFEs). Below, we measure the generative perplexity of sampled sequences under GPT2-Large. BD3-LMs achieve the best generative perplexities compared to all previous diffusion methods.</p>


      <!-- <p>We also compare with SSD-LM (<a href="https://arxiv.org/abs/2210.17432">Han et. al</a>), an alternative block-autoregressive formulation that performs Gaussian diffusion over word embeddings, as in other works such as Diffusion-LM (<a href="https://arxiv.org/abs/2205.14217">Li et. al</a>). Our approach instead applies discrete noise and can be seen as the analogous extension of <a href="https://arxiv.org/abs/2107.03006">Austin et. al</a>. Unlike BD3-LMs, SSD-LM does not support likelihood estimation. Thus, to compare the models, we generated unconditionally sequences of lengths 1024 and 2048. Our discrete approach yields samples with improved generative perplexity using an order magnitude fewer generation steps.</p> -->
        
      
    <p>
        <center>
          <i>Generative perplexity (Gen. PPL; ↓) and number of function evaluations (NFEs; ↓) of 300 variable-length samples. All models are trained on OWT with a context length L = 1024 and use nucleus sampling.</i>
        </center>
      </p><table>
      
      <thead>
        <tr>
          <th>Category</th>
          <th>Model</th>
          <th colspan="2"><center>L = 1024</center></th>
          <th colspan="2"><center>L = 2048</center></th>
        </tr>
        <tr>
          <th></th>
          <th></th>
          <th>Gen. PPL (↓)</th>
          <th>NFEs</th>
          <th>Gen. PPL (↓)</th>
          <th>NFEs</th>
        </tr>
      </thead>
      <tbody>
        <!-- Autoregressive Section -->
        <tr>
          <td><i>Autoregressive</i></td>
          <td>AR</td>
          <td><center>14.1</center></td>
          <td><center>1K</center></td>
          <td><center>13.2</center></td>
          <td><center>2K</center></td>
        </tr>
    
        <!-- Diffusion Section -->
        <tr>
          <td rowspan="2"><i>Diffusion</i></td>
          <td>SEDD</td>
          <td><center>52.0</center></td>
          <td><center>1K</center></td>
          <td><center>-</center></td>
          <td><center>-</center></td>
        </tr>
        <tr>
          <td>MDLM</td>
          <td><center>46.8</center></td>
          <td><center>1K</center></td>
          <td><center>41.3</center></td>
          <td><center>2K</center></td>
        </tr>
        
        <!-- Block-diffusion Section -->
        <tr>
          <td rowspan="5"><i>Block diffusion</i></td>
          <td>SSD-LM L&#39; = 25</td>
          <td><center>37.2</center></td>
          <td><center>40K</center></td>
          <td><center>35.3</center></td>
          <td><center>80K</center></td>
        </tr>
        <tr>
          <td></td>
          <td><center>281.3</center></td>
          <td><center>1K</center></td>
          <td><center>261.9</center></td>
          <td><center>2K</center></td>
        </tr>
        <tr>
          <td><strong>BD3-LMs</strong> L&#39; = 16</td>
          <td><center>33.4</center></td>
          <td><center>1K</center></td>
          <td><center>31.5</center></td>
          <td><center>2K</center></td>
        </tr>
        <tr>
          <td>                   L&#39; = 8</td>
          <td><center>30.4</center></td>
          <td><center>1K</center></td>
          <td><center>28.2</center></td>
          <td><center>2K</center></td>
        </tr>
        <tr>
          <td>                   L&#39; = 4</td>
          <td><center><strong>25.7</strong></center></td>
          <td><center>1K</center></td>
          <td><center><strong>23.6</strong></center></td>
          <td><center>2K</center></td>
        </tr>
        
      </tbody>
    </table>
    <p>For MDLM, we use their block-wise decoding technique (which does not feature block diffusion training as in BD3-LMs) for L=2048. We also compare to SSD-LM (<a href="https://arxiv.org/abs/2210.17432">Han et. al</a>) an alternative block-autoregressive method (also known as semi-autoregression) that performs Gaussian diffusion over word embeddings but cannot perform likelihood estimation. Our discrete approach yields samples with improved generative perplexity using an order of magnitude fewer generation steps.</p>

    </div>
  </div>
</section>


<!-- CONCLUSION -->
<section id="SEC2">
  <div>
    <div>
      <h2>Conclusion</h2>
      <p>We presented Block Discrete Diffusion Language models, a new model class that combines strength of both autoregressive and diffusion approaches while overcoming their limitations. Block diffusion overcomes key drawbacks of existing discrete diffusion models: the quality gap to AR model and their inability to generate arbitrary-length sequences or support KV caching. By doing so, BD3-LMs set a new state-of-the-art among discrete diffusion models. Our work presents a promising step forward in building powerful diffusion language models that are competitive with standard LLMs, while offering parallel token generation and improved controllability of samples.</p>
    </div>
  </div>
</section>




<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>
        @inproceedings{
          arriola2025interpolating,
          title={Interpolating Autoregressive and Discrete Denoising Diffusion Language Models},
          author={Marianne Arriola and Aaron Gokaslan and Justin T Chiu and Jiaqi Han and Zhihan Yang and Zhixuan Qi and Subham Sekhar Sahoo and Volodymyr Kuleshov},
          booktitle={The Thirteenth International Conference on Learning Representations},
          year={2025},
          url={https://openreview.net/forum?id=tyEyYT267x}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  </div>
  </body>
</html>
