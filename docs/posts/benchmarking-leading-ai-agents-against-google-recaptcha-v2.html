<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.roundtable.ai/captcha-benchmarking/">Original</a>
    <h1>Benchmarking leading AI agents against Google reCAPTCHA v2</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>
      Many sites use CAPTCHAs to distinguish humans from automated traffic. How well do these CAPTCHAs hold up against
      modern AI agents?
      We tested three leading models—Claude Sonnet 4.5, Gemini 2.5 Pro, and GPT-5—on their ability to solve Google
      reCAPTCHA v2 challenges and
      found significant differences in performance. Claude Sonnet 4.5 performed best with a 60% success rate, slightly
      outperforming Gemini 2.5 Pro
      at 56%. GPT-5 performed significantly worse and only managed to solve CAPTCHAs on 28% of trials.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/results.png" alt="Success rates by model"/>
      <figcaption><b>Figure 1: </b>
        Overall success rates for each AI model. Claude Sonnet 4.5 achieved the highest success rate at 60%, followed by
        Gemini 2.5 Pro at 56% and GPT-5 at 28%.
      </figcaption>
    </figure>

    <p>
      Each reCAPTCHA challenge falls into one of three types: Static, Reload, and Cross-tile (see Figure 2). The models&#39;
      success was highly dependent on this challenge type.
      In general, all models performed best on Static challenges and worst on Cross-tile challenges.
    </p>


    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-types.jpg" alt="CAPTCHA types used by reCAPTCHA v2"/>
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Static</th>
            <th>Reload</th>
            <th>Cross-tile</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Claude Sonnet 4.5</td>
            <td>47.1%</td>
            <td>21.2%</td>
            <td>0.0%</td>
          </tr>
          <tr>
            <td>Gemini 2.5 Pro</td>
            <td>56.3%</td>
            <td>13.3%</td>
            <td>1.9%</td>
          </tr>
          <tr>
            <td>GPT-5</td>
            <td>22.7%</td>
            <td>2.1%</td>
            <td>1.1%</td>
          </tr>
        </tbody>
      </table>
      <figcaption><b>Figure 2: </b>
        The three types of reCAPTCHA v2 challenges. Static presents a static 3x3 grid; Reload
        dynamically replaces clicked images, and Cross-tile uses a 4x4 grid with objects potentially spanning
        multiple squares. The table shows model performance by CAPTCHA type. 
        Success rates are lower than in Figure 1 as these rates are at the challenge level,
        rather than trial level. Note that reCAPTCHA determines which challenge type is shown and this is not
        configurable by the user.

      </figcaption>
    </figure>


    <h3>
      Model analysis
    </h3>

    <p>
      Why did Claude and Gemini perform better than GPT-5? We found the difference was largely due to excessive and
      obsessive reasoning.
      Browser Use executes tasks as a sequence of discrete steps — the agent generates &#34;Thinking&#34; tokens to reason about
      the next step,
      chooses a set of actions, observes the response, and repeats. Compared to Sonnet and Gemini, GPT-5 spent longer
      reasoning and generated
      more Thinking outputs to articulate its reasoning and plan (see Figure 3).
    </p>

    <p>
      These issues were compounded by poor planning and verification: GPT-5 obsessively made edits and corrections to
      its solutions,
      clicking and unclicking the same square repeatedly. Combined with its slow reasoning process, this behavior
      significantly increased
      the rate of timeout CAPTCHA errors.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/reasoning-lengths.png" alt="Thinking characters by model"/>
      <figcaption><b>Figure 3: </b>
        Average number of &#34;Thinking&#34; characters by model and grid size (Static and Reload CAPTCHAs are 3x3,
        and Cross-tile CAPTCHAs are 4x4). On every agent step, the model outputs a “Thinking” tag along with its
        reasoning about which actions it will take.
      </figcaption>
    </figure>

    <h3>
      CAPTCHA type analysis
    </h3>

    <p>
      Compared to Static challenges, all models performed worse on Reload and Cross-tile challenges.
      Reload challenges were difficult because of Browser Use&#39;s reasoning-action loop. Agents often clicked the
      correct initial squares and moved to submit their response, only to see new images appear or be instructed by
      reCAPTCHA to review their response. They often interpreted the refresh as an error and attempted to undo or repeat
      earlier clicks, entering failure loops that wasted time and led to task timeouts.
    </p>

    <figure>
      <video autoplay="" loop="" muted="" playsinline="" loading="lazy" aria-label="Gemini 2.5 Pro Cross-tile attempt">
        <source src="captcha-attempt.mp4" type="video/mp4"/>
        Your browser does not support the video tag.
      </video>
      <figcaption><b>Figure 4: </b>
        Gemini 2.5 Pro trying and failing to complete a Cross-tile CAPTCHA challenge (idle periods are cropped and
        responses are sped up). Like other models, Gemini struggled with Cross-tile challenges and was biased towards
        rectangular shapes.
      </figcaption>
    </figure>

    <p>
      Cross-tile challenges exposed the models&#39; perceptual weaknesses, especially on partial, occluded, and
      boundary-spanning objects.
      Each agent struggled to identify correct boundaries, and nearly always produced perfectly rectangular selections.
      Anecdotally,
      we find Cross-tile CAPTCHAs easier than Static and Reload CAPTCHAs—once we spot a single tile that matches the
      target, it&#39;s
      easy to identify the adjacent tiles that include the target. This difference in difficulty suggests fundamental
      differences in
      how humans and AI systems solve these challenges
    </p>

    <h3>
      Conclusion
    </h3>

    <p>
      What can developers and researchers learn from these results? More reasoning isn&#39;t always better.
      Ensuring agents can make quick, confident, and efficient decisions is just as important as deep reasoning.
      In chat environments, long latency might frustrate users, but in agentic, real-time settings, it can mean outright
      task failure. These failures can be compounded by suboptimal agentic architecture—in our case, an agent loop that
      encouraged
      obsession and responded poorly to dynamic interfaces. Our findings underscore that reasoning depth and performance
      aren&#39;t always a straight
      line; sometimes, overthinking is just another kind of failure. Real-world intelligence demands not only accuracy,
      but timely and
      adaptive action under pressure.
    </p>

    <h2>
      Methods
    </h2>

    <h3>
      Experimental design
    </h3>

    <p>
      Each Google reCAPTCHA v2 challenge presents users with visual challenges, asking them to identify specific objects like
      traffic lights, fire hydrants, or crosswalks in a grid of images (see Figure 5).
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-example.png" alt="Example reCAPTCHA v2 challenge"/>
      <figcaption><b>Figure 5: </b>
        Example of a reCAPTCHA v2 challenge showing a 4x4 grid where the user must select all squares containing the
        motorcycle.
      </figcaption>
    </figure>



    <p>
      We instructed each agent to navigate to Google&#39;s reCAPTCHA demo page and solve the presented CAPTCHA challenge
      (explicit image-based challenges were presented on 100% of trials). Note that running the tests on Google&#39;s page
      avoids cross-origin
      and iframe complications that frequently arise in production settings where CAPTCHAs are embedded across domains
      and
      subject to stricter browser security rules.
    </p>

    <p>
      We evaluated generative AI models using <a href="https://browser-use.com" target="_blank">Browser Use</a>, an
      open-source framework that enables AI agents to perform browser-based tasks. We gave each agent the following instructions 
      when completing the CAPTCHA:
    </p>

    <p>
      1. Go to: https://www.google.com/recaptcha/api2/demo </p>

    <p>
      Agents were instructed to try up to five different CAPTCHAs. Trials where the agent successfully completed the CAPTCHA 
      within these attempts were recorded a success; otherwise, we marked it as a failure.
    </p>

    <p>
      Although we instructed the models to attempt no more than five challenges per trial, agents often exceeded 
      this limit and tried significantly more CAPTCHAs. This counting difficulty was due to at least two reasons: 
      first, we found agents often did not use a state counter variable in Browser Use&#39;s memory store. Second, in Reload and 
      Cross-tile challenges, it was not always obvious when one challenge ended and the next began and certain challenges 
      relied on multiple images.<sup>1</sup> For consistency, we treated each discrete image the agent tried to label as a separate attempt, 
      resulting in 388 total attempts across 75 trials (agents were allowed to continue until they determined failure on their own).
    </p>

  </div></div>
  </body>
</html>
