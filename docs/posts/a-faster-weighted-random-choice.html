<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.bruce-hill.com/a-faster-weighted-random-choice">Original</a>
    <h1>A Faster Weighted Random Choice</h1>
    
    <div id="readability-page-1" class="page"><div>
<article>
<p><img src="https://swling.com/media/weighted-random/weighted_random.png" alt="A
Faster Weighted Random Choice"/>
</p>

<p>Performing fast random selection with non-uniform weights is
trickier than you might imagine.</p>
<p>Estimated reading time: 9 mins</p>
<span>Bruce Hill</span>
<span>February 2, 2017</span>

<p>Consider this problem: You have a list of items and a list of their weights.
You want to randomly choose an item, with probability proportional to the item’s
weight. For these examples, the algorithms will take a list of weights and
return the index of the randomly selected item.</p>
<h2 id="linear-scan">Linear Scan</h2>
<p>The simplest algorithm works by generating a random number and walking along
the list of weights in a linear fashion. This has the advantage of requiring no
additional space, but it runs in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>
time, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the number of weights.</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>def</span> weighted_random(weights):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    remaining_distance <span>=</span> random() <span>*</span> <span>sum</span>(weights)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span>for</span> i, weight <span>in</span> <span>enumerate</span>(weights):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        remaining_distance <span>-=</span> weight</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span>if</span> remaining_distance <span>&lt;</span> <span>0</span>:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>            <span>return</span> i</span></code></pre></div>
<h3 id="linear-scan-demo">Linear Scan Demo</h3>

<p>This algorithm can be improved slightly by sorting the weights in descending
order so that the bigger weights will be reached more quickly, but this is only
a constant speedup factor–the algorithm is still
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>
after the sorting has been done.</p>
<h2 id="binary-search">Binary Search</h2>
<p>With a little bit of preprocessing, it’s possible to speed up the algorithm
by storing the running totals of the weights, and using a binary search instead
of a linear scan. This adds an additional storage cost of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>,
but it speeds the algorithm up to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\log(N))</annotation></semantics></math>
for each random selection. Personally, I’m not a big fan of implementing binary
search algorithms, because it’s so easy to make off-by-one errors. This is a
pretty dang fast algorithm though.</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>def</span> prepare_binary_search(weights):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span># Computing the running totals takes O(N) time</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    running_totals <span>=</span> <span>list</span>(itertools.accumulate(weights))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span>def</span> weighted_random():</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        target_distance <span>=</span> random()<span>*</span>running_totals[<span>-</span><span>1</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        low, high <span>=</span> <span>0</span>, <span>len</span>(weights)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span>while</span> low <span>&lt;</span> high:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            mid <span>=</span> <span>int</span>((low <span>+</span> high) <span>/</span> <span>2</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            distance <span>=</span> running_totals[mid]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            <span>if</span> distance <span>&lt;</span> target_distance:</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                low <span>=</span> mid <span>+</span> <span>1</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span>elif</span> distance <span>&gt;</span> target_distance:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                high <span>=</span> mid</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            <span>else</span>:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>                <span>return</span> mid</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span>return</span> low</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span>return</span> weighted_random</span></code></pre></div>
<h3 id="binary-search-demo">Binary Search Demo</h3>

<h2 id="hopscotch-selection">Hopscotch Selection</h2>
<p>However, it’s possible to do even better by using the following algorithm
that I’ve come up with. The algorithm works by first sorting the weights in
descending order, then hopping along the list to find the selected element. The
size of hops is calculated based on the invariant that all weights after the
current position are not larger than the current weight. The algorithm tends to
quickly hop over runs of weights with similar size, but it sometimes makes
overly conservative hops when the weight size changes. Fortunately, when the
weight size changes, it always gets smaller during the search, which means that
there is a corresponding decrease in the probability that the search will need
to progress further. This means that although the worst case scenario is a
linear traversal of the whole list, the average number of iterations is much
smaller than the average number of iterations for the binary search algorithm
(for large lists).</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>def</span> prepare_hopscotch_selection(weights):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span># This preparation will run in O(N*log(N)) time,</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span># or however long it takes to sort your weights</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    sorted_indices <span>=</span> <span>sorted</span>(<span>range</span>(<span>len</span>(weights)),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                            reverse<span>=</span><span>True</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                            key<span>=</span><span>lambda</span> i:weights[i])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    sorted_weights <span>=</span> [weights[s] <span>for</span> s <span>in</span> sorted_indices]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    running_totals <span>=</span> <span>list</span>(itertools.accumulate(sorted_weights))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span>def</span> weighted_random():</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        target_dist <span>=</span> random()<span>*</span>running_totals[<span>-</span><span>1</span>]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        guess_index <span>=</span> <span>0</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span>while</span> <span>True</span>:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            <span>if</span> running_totals[guess_index] <span>&gt;</span> target_dist:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>                <span>return</span> sorted_indices[guess_index]</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            weight <span>=</span> sorted_weights[guess_index]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span># All weights after guess_index are &lt;= weight, so</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            <span># we need to hop at least this far to reach target_dist</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            hop_distance <span>=</span> target_dist <span>-</span> running_totals[guess_index]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            hop_indices <span>=</span> <span>1</span> <span>+</span> <span>int</span>(hop_distance <span>/</span> weight)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            guess_index <span>+=</span> hop_indices</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span>return</span> weighted_random</span></code></pre></div>
<h3 id="hopscotch-demo">Hopscotch demo</h3>

<h3 id="amortized-analysis">Amortized Analysis</h3>
<p>Performing a good amortized analysis of this algorithm is rather difficult,
because its runtime depends on the distribution of the weights. The two most
extreme cases I can think of are if the weights are all exactly the same, or if
the weights are successive powers of 2.</p>
<h4 id="uniform-weights">Uniform Weights</h4>
<p>If all weights are equal, then the algorithm always terminates within 2
iterations. If it needs to make a hop, the hop will go to exactly where it needs
to. Thus, the runtime is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>
in all cases.</p>
<h4 id="power-of-2-weights">Power of 2 Weights</h4>
<p>If the weights are successive powers of 2 (e.g. [32,16,8,4,2,1]), then if the
algorithm generates a very high random number, it might traverse the whole list
of weights, one element at a time. This is because every weight is strictly
greater than the sum of all smaller weights, so the calculation guesses it will
only need to hop one index. However, it’s important to remember that this is not
a search algorithm, it’s a weighted random number generator, which means that
<em>each successive item is half as likely to be chosen as the previous
one,</em> so across k calls, the average number of iterations will be &lt;2k. In
other words, as the time to find an element increases linearly, the probability
of selecting that element decreases exponentially, so the amortized runtime in
this case works out to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>.
If the weights are powers of some number larger than 2, then the search time is
still the same, but it’s <em>even less likely</em> to need to walk to the later
elements. If the weights are powers of some number between 1 and 2, then the
algorithm will be able to jump ahead multiple elements at a time, since the sum
of remaining weights won’t necessarily be smaller than the current weight.</p>
<p>These are the two most extreme cases I could think of to analyze, and the
algorithm runs in amortized constant time for both. The algorithm quickly jumps
across regions with similar-sized weights, and slowdowns caused by variance in
weight sizes are offset by the fact that bigger weights near the front of the
list (and hence faster) are more likely to be selected when there’s more
variance in the weight sizes. My hunch is that this algorithm runs in amortized
constant time for any weight distribution, but I’ve been unable to prove or
disprove that hunch.</p>
<h2 id="tests">Tests</h2>
<p>Here are some tests that show how the different algorithms perform as the
number of weights increases, using different weight distributions. The graphs
use a logarithmic x-axis, and you can see that the binary search algorithm is
roughly a straight line, corresponding to its
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(\log(N))</annotation></semantics></math>
runtime, and the linear scan variants have an exponential curve, corresponding
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>.
The hopscotch algorithm looks to me as if it’s asymptotically approaching a
constant value, which would correspond to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>,
though it’s hard to say. It certainly does appear to be sublogarithmic
though.</p>
<p><img src="https://swling.com/media/weighted-random/uniform_performance.png" alt="Plot of performance with uniform weights" width="300" height="225"/>
<img src="https://swling.com/media/weighted-random/similar_performance.png" alt="Plot of performance with similar weights" width="300" height="225"/></p>
<p><img src="https://swling.com/media/weighted-random/pareto_performance.png" alt="Plot of performance with pareto-distribution weights" width="300" height="225"/>
<img src="https://swling.com/media/weighted-random/exponential_performance.png" alt="Plot of performance with exponential distribution weights" width="300" height="225"/></p>
<h2 id="final-words">Final Words</h2>
<p>Python’s <code>random.random()</code> generates numbers in the half-open
interval <code>[0,1)</code>, and the implementations here all assume that
<code>random()</code> will never return 1.0 exactly. It’s important to be wary
of things like Python’s <code>random.uniform(a,b)</code>, which generates
results in the closed interval <code>[a,b]</code>, because this can break some
of the implementations here.</p>
<p>The implementations here also assume that all weights are strictly greater
than zero. If you might have zero-weight items, the algorithm needs to be sure
to never return those items, even if <code>random()</code> returns some number
that exactly lines up with one of the running totals.</p>
<p>The version of the hopscotch algorithm I present here is the version that I
initially conceived, and I think it’s the easiest to grok. However, if your
weights do vary by many orders of magnitude, it is probably best to store the
running totals from lowest to highest weight, to minimize floating point errors.
It’s best to add small numbers together until the running total gets big, and
then start adding big numbers to that, rather than adding very small numbers to
a big running total. This requires adjusting the algorithm somewhat, but if
you’re worried about floating point errors, then you’re probably more than
capable of tweaking the code.</p>
<p>If anyone is able to do a good amortized analysis of this algorithm’s runtime
for arbitrary weights, I’d love to hear about it. The algorithm is simple to
implement and it seems to run much faster than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(log(N))</annotation></semantics></math>.
My gut says that it runs in amortized
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>
time, but you can’t always trust your gut.</p>
<h2 id="addendum-the-fastest-weighted-random-choice-algorithm">Addendum: The
Fastest Weighted Random Choice Algorithm</h2>
<p>There’s one more weighted random algorithm, <a href="https://en.wikipedia.org/wiki/Alias_method" target="_blank" rel="noreferrer">originally discovered by A.J. Walker in 1974</a> (described in
<a href="http://www.keithschwarz.com/darts-dice-coins/" target="_blank" rel="noreferrer">this excellent page by Keith Schwarz</a>), that I think is the
fastest and most efficient algorithm out there. It’s really elegant, and it runs
in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math>
setup time, and guaranteed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math>
run time for every random selection. The algorithm is hard to find on Google,
despite being relatively simple to implement and clearly better than all other
contenders (alas, including my own algorithm), so here is my Python
implementation:</p>
<p><span>You can also see <a href="https://swling.com/media/weighted-random/alias_with_index.py">a variant with big and small
indices instead of generators.</a> It’s not really faster, but it is easier to
port to low-level languages.</span></p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>def</span> prepare_aliased_randomizer(weights):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    N <span>=</span> <span>len</span>(weights)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    avg <span>=</span> <span>sum</span>(weights)<span>/</span>N</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    aliases <span>=</span> [(<span>1</span>, <span>None</span>)]<span>*</span>N</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    smalls <span>=</span> ((i, w<span>/</span>avg) <span>for</span> i,w <span>in</span> <span>enumerate</span>(weights) <span>if</span> w <span>&lt;</span> avg)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    bigs <span>=</span> ((i, w<span>/</span>avg) <span>for</span> i,w <span>in</span> <span>enumerate</span>(weights) <span>if</span> w <span>&gt;=</span> avg)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    small, big <span>=</span> <span>next</span>(smalls, <span>None</span>), <span>next</span>(bigs, <span>None</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span>while</span> big <span>and</span> small:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        aliases[small[<span>0</span>]] <span>=</span> (small[<span>1</span>], big[<span>0</span>])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        big <span>=</span> (big[<span>0</span>], big[<span>1</span>] <span>-</span> (<span>1</span><span>-</span>small[<span>1</span>]))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span>if</span> big[<span>1</span>] <span>&lt;</span> <span>1</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            small <span>=</span> big</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            big <span>=</span> <span>next</span>(bigs, <span>None</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span>else</span>:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            small <span>=</span> <span>next</span>(smalls, <span>None</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span>def</span> weighted_random():</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        r <span>=</span> random()<span>*</span>N</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        i <span>=</span> <span>int</span>(r)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        odds, alias <span>=</span> aliases[i]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span>return</span> alias <span>if</span> (r<span>-</span>i) <span>&gt;</span> odds <span>else</span> i</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span>return</span> weighted_random</span></code></pre></div>
<h3 id="alias-method-demo">Alias Method Demo</h3>

<p>For now, I’ll refrain from a full explanation, but essentially, the algorithm
works by slicing up the weights and putting them into <code>N</code> buckets.
Each bucket gets exactly 1 or 2 of the weight-slices in such a way that the
slices in each bucket add up to the same amount, and no slices are left over at
the end. The bigger-than-average weights each have small slices get shaved off
to fill the gaps in the buckets of the smaller-than-average weights until the
big weight is whittled down enough to be smaller-than-average, and the process
continues. Why this is guaranteed to works out so perfectly is not immediately
obvious, but you can grok it if you spend a while thinking about it and working
through some examples.</p>
<p>When you want to do a weighted random selection, you just pick a random
bucket (a constant-time operation: <code>buckets[int(rand*N)]</code>) and then
choose either the first or second slice in the bucket, with odds proportional to
the sizes of the two slices, another constant-time operation:
<code>i if (rand*N)%1 &lt;= slice1/(slice1+slice2) else alias</code>
(<code>alias</code> is the index of the second slice in the bucket). There’s
some really clever optimizations, like always having bucket <code>i</code>’s
first slice be from <code>weights[i]</code>, and scaling the slice sizes so that
the values stored in the bucket always add up to 1. It’s all very clever! Again,
please check out and share <a href="http://www.keithschwarz.com/darts-dice-coins/" target="_blank" rel="noreferrer">the more detailed explanation.</a></p>

</article>

</div></div>
  </body>
</html>
