<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/897917/7a4775f9f1223e8a/">Original</a>
    <h1>NFS: The Early Years</h1>
    
    <div id="readability-page-1" class="page"><div>
<!-- $Id: slink-trial,v 1.1 2005-11-04 21:27:01 corbet Exp $ -->
<center>
<table>
<tbody><tr><td>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the trial offer on the right.  Thank you
for visiting LWN.net!
</p></td><td>
<div>
<h3>Free trial subscription</h3>
           <p>
           Try LWN for free for 1 month: no payment
           or credit card required.  <a href="https://lwn.net/Promo/slink-trial2-3/claim">Activate
           your trial subscription now</a> and see why thousands of
           readers subscribe to LWN.net.
           
</p></div>
</td>
</tr>

</tbody></table>
</center>

<p>I recently had cause to reflect on the changes to the <a href="https://en.wikipedia.org/wiki/Network_File_System">NFS (Network File
System)</a> protocol over the years and found that it was a story worth
telling.  It would be easy for such a story to become swamped by the
details, as there are many of those, but one idea does stand out from
the rest.  The earliest version of NFS has been described as a
&#34;stateless&#34; protocol, a term I still hear used occasionally.  Much of
the story of NFS follows the growth in the acknowledgment of, and
support for, state.  This article looks at the evolution of NFS (and its
handling of state) during the
early part of its life; a second installment will bring the story up to the
present.
</p>

<p>By &#34;state&#34; I mean any information that is remembered by both the client
and the server, and that can change on one side, thus necessitating a
change on the other.  As we will see, there are many elements of state.
One simple example is file content when it is cached on the client,
either to eliminate read requests or to combine write requests.  The client
needs to know when cached data must be flushed or purged so that the
client and server remain largely synchronized.  Another obvious form of
state is file locks, for which the server and client must always agree on
what locks the client holds at any time.  Each side must be able to
discover 
when the other has crashed so that locks can be discarded or recovered.</p>
<h4>NFSv2 — the first version</h4>

<p>Presumably there was a &#34;version 1&#34; of NFS developed inside <a href="https://en.wikipedia.org/wiki/Sun_Microsystems">Sun
Microsystems</a>, but the first to be publicly available was version 2,
which appeared in 1984.  The protocol is described in <a href="https://www.rfc-editor.org/rfc/rfc1094.html">RFC 1094</a>, though
this is not seen as an authoritative document; rather, the
implementation from Sun defined the protocol.  There were other network
filesystems being developed around the same time, such as <a href="https://en.wikipedia.org/wiki/Andrew_File_System">AFS</a> (the
Andrew File System), and <a href="https://en.wikipedia.org/wiki/Remote_File_Sharing">RFS</a> (Remote
File Sharing).  One distinctive
difference that NFS had, when compared to these, is that it was simple.
One might argue that it was too simple, as it could not correctly
implement some POSIX semantics.  However, this simplicity meant that it
could provide good performance for a lot of common workloads.</p>

<p>The early 1980s was the time of the &#34;<a href="https://en.wikipedia.org/wiki/3M_computer">3M
Computer</a>&#34; which suggested a goal
for personal workstations of one megabyte of memory, one MIPS of
processing power, and one megapixel (monochrome) of display.  This seems
almost comically underpowered by today&#39;s standard, particularly when one
considers that a price tag of a mega-penny ($10,000) was thought to be
acceptable.  
But this was the sort of hardware on which NFSv2 had to run — and had
to run well — in order to be accepted.
History suggests that it
was adequate to the task.</p>
<h4>Consequences of being &#34;stateless&#34;</h4>

<p>The NFSv2 protocol has no explicit support for any state management.
There is no concept of &#34;opening&#34; a file, no support for locking, nor
any mention of caching in the RFC.  There are only simple, self-contained
access requests, all of which involve file handles.</p>

<p>The &#34;file handle&#34; is a central unifying feature of NFSv2: it is an
opaque, 32-byte identifier for a file that is stable and unique within a given
NFS server across all time.  NFSv2 allows the client to look up the file
handle for a given name in a given directory (identified by some other
file handle), to inspect and change attributes (ownership, size,
timestamps, etc.) given a file handle, and to read and write blocks of
data at a given offset of a given file handle.</p>

<p>As far as possible, the operations chosen for NFSv2 are idempotent, so
that, if any request were repeated, it would have the same result on the
second or third attempt as it had on the first.  This is necessary for
true stateless operation over an imperfect network.  NFS was originally
implemented over UDP, which does not guarantee delivery, so the client
had to be prepared to resend a request if it didn&#39;t get a reply.  The
client cannot know if it was the request or the reply that was lost, and
a truly stateless server cannot remember if any given request has been
seen already so that it can suppress a repeat.  Consequently, when the client
resends a request, it might repeat an operation that has already been
performed, so idempotent operations are best.</p>

<p>Unfortunately, not all filesystem operations under POSIX can be
idempotent.  A good example is MKDIR, which should make a directory if
the given name is not in use, or return an error if the name is already
used, even if it is used for a directory.  This means that repeating the
request can result in an incorrect error result.  Standard practice for
minimizing this problem is to implement a Duplicate Request Cache (DRC)
on the server.  This is a record of recent, non-idempotent requests that
have been handled, along with the result that was returned.  Effectively, this
means that both the client (which must naturally track requests that have
not yet received a reply) and the server maintain a list of outstanding
requests that changes over time.  These lists match our definition of
&#34;state&#34;, so the original NFSv2 was not actually stateless in practice,
even if it was according to the specification.
</p><p>
As the server cannot
know when the client sees a reply, it cannot know when a request is no
longer outstanding, so it must use some heuristics to discard old cache
entries.  It will inevitably remember many requests that it doesn&#39;t need
to, and may discard some that will soon be needed.  While this is clearly
not ideal, experience suggests that it is reasonably effective for
normal workloads.</p>

<p>Maintaining this cache requires that the server knows which client each
request came from, so it needs some reliable way to identify clients.
This is a need that we will see repeated as state management becomes
more explicit with the development of the protocol.  For the DRC, the
client identifier used is derived from the client&#39;s IP address and port number.  When
TCP support was added to NFS, the protocol type needed to be included
together with the host address and port number.  As TCP provides reliable
delivery, it might seem that the DRC is not needed, but this isn&#39;t
entirely true.  It is possible for a TCP connection to &#34;break&#34; if a
network problem causes the client and server to be unable to communicate for
an extended period of time.  NFS is prepared to wait indefinitely, but
TCP is not.  If TCP does break the connection, the client cannot know
the status of any outstanding requests, so it must retransmit them on a
new connection, and the server might still see duplicate requests.  To
make sure this works, NFS clients are careful to reconnect using the
same source port as the earlier connection.</p>

<p>A duplicate request cache is not perfect, partly because the heuristic
may discard entries before the client has actually received the reply,
and partly because it is not preserved across server reboots, so a
request might be acted upon both immediately before and after a server
crash.  In many cases, this is an occasional inconvenience but not a huge
problem; will anyone really suffer if &#34;mkdir&#34; occasionally returns
<tt>EEXIST</tt> when it shouldn&#39;t?  But there is one situation that turned out
to be quite problematic and isn&#39;t handled by the DRC at all.  That is
exclusive create.</p>

<p>Before Unix had any concept of file locks (as it didn&#39;t in Edition 7
Unix, which became the base for BSD), it was common to use lock files.  If
exclusive access was required
to some file, such as <tt>/usr/spool/mail/neilb</tt>, the
convention was that the application must first create a lock file with a
related name, such as <tt>/usr/spool/mail/neilb.lock</tt>.  This must be an
&#34;exclusive&#34; creation using the flags <tt>O_CREAT|O_EXCL</tt>, which would fail
if the file already existed.  An application that found that it
couldn&#39;t create the file because some other application had done so
already would wait and try again.</p>

<p>Exclusive create is not an idempotent operation — by design — and NFSv2
has no support for it at all.  Clients could perform a lookup and, if
that reported no existing file, they could then create the file.  This
two-step sequence is clearly susceptible to races, so it is not reliable.
This failing of NFS does not appear to have decreased its popularity,
but certainly resulted in a lot of cursing over the years.  It also
resulted in some innovation; there are other ways to create lock
files.
</p><p>
One way is to generate a string that will be unique across all
clients —
possibly with host name, process ID, and timestamp — and then create a
temporary file with this string as both name and content.  This file is
then (hard) linked to the name for the lock file.  If the hard-link
succeeds, the lock has been claimed.  If it fails because the name
already exists, then the application can read the content of that file.
If it matches the generated unique string, then the error was due to a
retransmit and again the lock has been claimed.  Otherwise the
application needs to sleep and try again.</p>

<p>Another unfortunate consequence of avoiding state management involves
files that are unlinked while they are still open.  POSIX is perfectly
happy with these unlinked-but-open files and assures that the file will
continue to behave normally until it is finally closed, at which point
it will cease to exist.  An NFS server, since it does not know which files
are open on which client, finds it difficult to be so accommodating, so
NFS client implementations don&#39;t rely on help from the server.  Instead,
when handling a request to unlink (remove) a file that is open, the
client will instead rename the file to something obscure and unique,
like <tt>.nfs-xyzzy</tt>, and will then unlink this name when the file is
finally closed.  This relieves the server from needing to track the
state of the client, but is an occasional inconvenience to the client.
If an application opens the only file in some directory, unlinks the
file, then tries to remove the directory, that last step will fail as
the directory is not empty but contains an obscure <tt>.nfs-XX</tt> name —
unless the client moves the obscure name into a parent or converts the
RMDIR into another rename operation.  In practice this sequence of operations
is so rare that NFS clients don&#39;t bother to make it work.</p>
<h4>The NFS ecosystem</h4>

<p>When I said above that NFSv2 didn&#39;t support file locking, that is only
half the story — it is accurate but not complete.  NFS was, in fact,
part of a suite of protocols that could be used together to provide a
more complete service.  NFS didn&#39;t support locks, but there was
another protocol that did.  The protocols that could be used with NFS
include:</p>
<ul>
<li>

<p>NLM (the Network Lock Manager).  This allows the client to request a
byte-range lock on a given file (identified using an NFS file handle),
and allows the server to grant it (or not), either immediately or
later.  Naturally this is an explicitly stateful protocol, as both
the client and server must maintain the same list of locks for each
client.</p>
</li>
<li>

<p>STATMON (the Status Monitor).  When a node — whether client or server
— crashes or otherwise reboots, any transient state, such as file
locks, is lost, so its peer needs to respond.  A server will purge the
locks held by that client, while a client will try to reclaim the locks that
were lost.   The chosen method with NLM is to have each node record
a list of peers in stable storage, and to notify them all when it
reboots; they can then clean up.  This task of recording and then
notifying peers was the job of STATMON.  Of course, if a client crashed
while holding a lock and never rebooted, the server would never know
that the lock was no longer held.  This could, at times, be
inconvenient.</p>
</li>
<li>

<p>MOUNT. When mounting an NFSv2 filesystem, you need to know the file
handle for the root of the filesystem, and NFS has no way to provide
this.  The task is handled instead by the MOUNT protocol.  This
protocol expects the server to keep track of which clients have
mounted which filesystems, so this useful information can be reported.
However, as MOUNT doesn&#39;t interact with STATMON, clients can reboot
and effectively unmount filesystems without telling the server.  While
implementations do still record the list of active mounts, nobody
trusts them.</p>

<p>In later versions, MOUNT also handled security negotiations.  A server
might require some sort of cryptographic security (such as Kerberos) for
accessing some filesystems, and this requirement is communicated to
the client using the MOUNT protocol.</p>
</li>
<li>

<p>RQUOTA (remote quotas).  NFS can report various attributes of files
and of filesystems, but one attribute that is not supported is quotas
— possibly because these are attributes of users, not of files.  To
fill this gap for people who need it to be filled, there exists the
RQUOTA protocol.</p>
</li>
<li>

<p>NFSACL (POSIX draft ACLs).  Much as we have RQUOTA for quotas, we have
NFSACL for access control lists.  This allows both examining the ACLs
and (unlike RQUOTA) setting them.</p>
</li>
</ul>

<p>Beyond these, there are other protocols that are only loosely connected,
such as
&#34;Yellow Pages&#34;, also known as the Network Information Server (NIS),
which helped a collection of machines have consistent username-to-UID
mappings; &#34;rpc.ugid&#34;, which tried to help out when they didn&#39;t; and maybe
even NTP which ensures that an NFS client and server had the same idea
of the current time.  These aren&#39;t really part of NFS in any meaningful
sense, but are part of the ecosystem that allowed NFS to flourish.</p>
<h4>NFSv3 — bigger is better.</h4>

<p>NFSv3 came along about ten years later (1995).  By this time, workstations
were faster (and more colorful) and disk drives were bigger.  32 Bits
were no longer enough to represent the number of bytes in a file, blocks in a
filesystem, or inodes in a filesystem, and 32 bytes were no longer
enough for a file handle, so these sizes were all doubled.  NFSv3 also
gained the READDIRPLUS operation to receive the names is a directory
together with file attributes, so that <tt>ls -l</tt> could be
implemented more
efficiently.  Note that deciding when to use READDIRPLUS and when to use
the simpler READDIR is far from trivial.  The Linux NFS client is still,
in 2022, receiving improvements to the heuristics.</p>

<p>There were two particular areas of change that relate to state
management, one which addressed the exclusive-create problem discussed
above, and one which helped with maintaining a cache of data on the
client.  The first of these extended the CREATE operation.</p>

<p>In NFSv3, a CREATE request can indicate whether the request is UNCHECKED,
GUARDED, or EXCLUSIVE.  The first of these allows the operation to succeed
whether the
file already exists or not.  The second must fail if the file exists, but
it is like MKDIR in that a retransmission may result in an error
where there shouldn&#39;t be one, so it is not particularly helpful.
EXCLUSIVE is more interesting.</p>

<p>The EXCLUSIVE create request is accompanied by eight bytes of unique client
identification (our recurring theme) called a &#34;verifier&#34;.  The RFC
(<a href="https://www.rfc-editor.org/rfc/rfc1813.html">RFC 1813</a>)
suggests that &#34;perhaps&#34; this verifier could contain the client&#39;s IP
address or some other unique data.  The Linux NFS client uses four bytes of
the <tt>jiffies</tt> internal timer and four bytes of the requesting process&#39;s
process ID number.  The server is required to store this verifier to
stable storage atomically while creating the file.  If the server is later
asked to create a file which already exists, the stored client identifier
must be compare with that in the request and, if they match, the server
must report a successful exclusive creation on the assumption that this
is a resend of an earlier request.</p>

<p>The Linux NFS server stores this verifier in the <tt>mtime</tt> and
<tt>atime</tt> fields of the
file it creates.  The NFSv3 protocol acknowledges this possibility and
requires that, once the client receives the reply indicating successful creation,
it must issue a SETATTR request to set correct values for any
file attributes that the server might have overloaded to store the
verifier.  This SETATTR step acknowledges to the server the completion
of some non-idempotent request — exactly what we thought might have been
helpful for the DRC implementation.</p>
<h4>Client-side caching and close-to-open cache consistency</h4>

<p>The NFSv2 RFC did not describe client-side caching, but that doesn&#39;t
mean that implementations didn&#39;t do any.  They had to be careful though.
It is only safe to cache data if there is good reason to expect that the
data hasn&#39;t changed on the server.  NFS practice provides two ways for the
client to convince itself that cached data is safe to use.</p>

<p>The NFS server can report various attributes of a file, particularly
size and last-change time.  If neither of these change from previously seen
values, it might be
reasonable to assume that the file content hasn&#39;t changed.  NFSv2 allows
the change timestamp to be reported to the closest microsecond, but that
doesn&#39;t guarantee that the server maintains that level of precision.
Even twenty years after NFSv2 was first used, there were important Linux
filesystems that could only report one-second granularity for time stamps.
So, if an NFS client sees a timestamp that is at least one second in the
past, and then reads data, it is safe to cache that data until it sees
the timestamp change.  If it sees a timestamp that is within one second
of &#34;now&#34;, then it is much less safe to make assumptions.</p>

<p>NFSv3 introduced an FSINFO request that allowed the server to report
various limits and preferences, and included a &#34;time_delta&#34;, which is the
time granularity that can be assumed for change time and other
timestamps.  This allows client-side caching to be a little more
precise.</p>
<p>

As noted above, it is considered safe to use cached data for a file until
its attributes are seen
to change.  The client could choose never to look at the file attributes
again and, thus, never see a change, but that is not permitted.  The way
to affirm data safety consists of two rules about when the client must check the
attributes.
</p><p>
The first rule is simple: check occasionally.  The protocol doesn&#39;t specify
minimum or maximum timeouts but most implementations allow these to be
configured.  Linux defaults to a three-second timeout which is extended
exponentially as long as nothing appears to be changing, to a maximum of
one minute.  This means that the client might provide data from cache that
is up to 60 seconds old, but no longer.  The second rule builds on an
assumption that multiple applications never open the same file at the same
time, unless they use locking or they are all read-only.

</p><p>When a client opens a file, it must verify any cached data (by checking
timestamps) and discard any that it cannot be confident of.  As long as
the file remains open, the client can assume that no changes will happen on the
server that it doesn&#39;t request itself.  When it closes the file, the
client must flush all changes to the server before the close completes.
If each client does this, then any application that opens a file will
see all changes made by any other application on any client that closed
the file before this open happened, so this model is sometimes
referred to as &#34;close-to-open consistency&#34;.</p>

<p>When byte-range locking is used, the same basic model applies, but the
open operation becomes the moment when the client is granted a lock, and the
close is when it releases the lock.  After being granted a lock,
the client must revalidate or purge any cached data in the range of the
lock and, before releasing a lock, it must flush cached changes in this
region to the server.</p>

<p>As the above relies on the change time to validate the cache, and as the
change time updates whenever any client writes to the file, the logical
implication is that, when a client writes to a file, it must purge its
own cache since the timestamp has changed.  It is quite justified to
maintain the cache until the file is closed (or the region is unlocked),
but not beyond.  This need is particularly visible when byte-range
locking is used.  One client might lock one region, write to it, and
unlock.  Another client might lock, write, and unlock a different
region, with the write requests happening at exactly the same time.
There is no way that either client can tell if another client wrote to the
file or
not, as the timestamp covers the whole file, not just one range.  So
they must both purge their whole cache before the next time
the file is opened or locked.</p>

<p>At least, there was no way to tell before NFSv3 introduced weak cache
consistencies (wcc) attributes.  The reply to an NFSv3 WRITE request
allows the server to report some attributes — size and time stamps —
both before and after the write request, and requires that, if it does
report them, then no other write happened between the two sets of
attributes.  A client can use this information to detect when a change
in timestamps was due purely to its own writes, and when they were
due to some other client.  It can, thus, determine whether it is the only
client writing to a file (a fairly common situation) and, when so, preserve
its cache even though the timestamp is changing.
Wcc attributes are also available in replies to
SETATTR and to requests that modify a directory, such as CREATE or
REMOVE, so a client can also tell if it is the sole actor in a directory, and
manage its cache accordingly.
</p>

<p>This is &#34;weak&#34; cache consistency, as it still requires the client to
check the timestamps occasionally.  Strong cache consistency requires
the server to explicitly tell the client that change is imminent, and we
don&#39;t get that until a later version of the protocol.  Despite being
weak, it is still a clear step forward in allowing the client to
maintain knowledge about the state of the server, and so another nail in
the coffin of the fiction of a stateless protocol.</p>

<p>As an aside, the Linux NFS server doesn&#39;t provide these wcc attributes
for writes to files.  To do this, it would need to hold the file lock
while collecting attributes and performing the write.  Since Linux
2.3.7, the underlying filesystem has been responsible for taking the
lock during a write, so <tt>nfsd</tt> cannot provide the attributes atomically.
Linux NFS does provide wcc attributes for changes to directories, though.</p>
<h4>NFS — the next generation</h4>

<p>These early versions of NFS were developed within Sun Microsystems.  The
code was made available for other Unix vendors to include in their
offerings and, while these vendors were able to tweak the implementation as
needed, they were not able to change the protocol; that was controlled
by Sun.</p>

<p>As the new millennium approached, interest in NFS increased and
independent implementations appeared.  This resulted in a wider range of
developers with opinions — well-informed opinions — on how NFS could be
improved.  To satisfy these developers without risking dangerous
fragmentation, a process was needed for those opinions to be heard and
answered.  The nature of this process and the changes that appeared in
subsequent versions of the NFS protocol will be the subject of a
forthcoming conclusion to this story.</p></div></div>
  </body>
</html>
