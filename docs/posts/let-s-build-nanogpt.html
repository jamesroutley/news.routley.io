<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/lets-build-nanogpt/">Original</a>
    <h1>Let&#39;s Build NanoGPT</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A look at episode #7: <a href="https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener noreffer ">Let‚Äôs build GPT: from scratch, in code, spelled out</a> from <a href="https://karpathy.ai/" target="_blank" rel="noopener noreffer ">Andrej Karpathy</a> amazing tutorial series.</p>

<p>
  <iframe src="https://www.youtube.com/embed/kCc8FmEb1nY" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>For the final episode of the series üò≠ we keep all the little things about reading, partitioning and tokenizing the dataset from previous videos. And start a new model from scratch to generate some shakespeare sounding text.</p>
<h2 id="the-model">The model</h2>
<p>The model is inspired GPT-2 and the <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreffer ">Attention is All You Need paper</a>. This is a <strong>very dense lecture</strong> and it takes a lot of time to unpack.</p>
<p>It starts as an N-gram model and slowly morph into a decoder-only transformer.</p>
<figure><a href="https://news.mit.edu/2023/transformer.png" title="transformer" data-thumbnail="transformer.png" data-sub-html="&lt;h2&gt;Decoder-only Transformer&lt;/h2&gt;&lt;p&gt;transformer&lt;/p&gt;">
        <img src="https://news.mit.edu/svg/loading.min.svg" data-src="transformer.png" data-srcset="transformer.png, transformer.png 1.5x, transformer.png 2x" data-sizes="auto" alt="transformer.png"/>
    </a><figcaption>Decoder-only Transformer</figcaption>
    </figure>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>The embeddings vectors we have been using for makemore were carrying the meaning of the tokens, but didn‚Äôt represent their position in the inputs. Positional Encoding is meant to change that by shifting the embeddings in space depending on their position in the sentence. The video use learned positional embeddings (for simplicity?):</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>  <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>  <span>self</span><span>.</span><span>token_embedding_table</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>vocab_size</span><span>,</span> <span>n_embd</span><span>)</span>
</span></span><span><span>  <span>self</span><span>.</span><span>position_embedding_table</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>block_size</span><span>,</span> <span>n_embd</span><span>)</span>
</span></span><span><span>  <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>idx</span><span>):</span>
</span></span><span><span>  <span>tok_emb</span> <span>=</span> <span>self</span><span>.</span><span>token_embedding_table</span><span>(</span><span>idx</span><span>)</span>
</span></span><span><span>  <span>pos_emb</span> <span>=</span> <span>self</span><span>.</span><span>position_embedding_table</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>T</span><span>,</span> <span>device</span><span>=</span><span>device</span><span>))</span>
</span></span><span><span>  <span>x</span> <span>=</span> <span>tok_emb</span> <span>+</span> <span>pos_emb</span>
</span></span><span><span>  <span>...</span>
</span></span></code></pre></div><p>but the paper originally suggest using FFT looking values:</p>
<p>$$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$$</p>
<h3 id="self-attention">Self-Attention</h3>
<p>This is to me the densest part of it all. And had to pause the video and go have a look at <a href="https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks" target="_blank" rel="noopener noreffer ">other material</a> to get <a href="https://e2eml.school/transformers.html" target="_blank" rel="noopener noreffer ">more angles</a> on it.</p>
<p>The endgoal is to have embeddings talk to each other to enrich their meaning with the sentence context (by using <code>Query</code> aka. ‚Äúwhat do I care about‚Äù, combined with <code>Key</code> aka. ‚Äúwhat I know about‚Äù, to scale <code>Value</code> aka. ‚Äúslightly transformed embeddings‚Äù).</p>
<p>With my current understanding I believe the intuition behind using the dot product of the <code>Query</code> and <code>Key</code> to be analogous to mechanism used in nearest neighbor search. We use some mathematical tool (<code>dot product</code> for attention, and <code>cosine similarity</code> for KNN) to compare vectors in a way that gives us higher values for more similar vectors. And use that to scale how much we care about a given pair of vectors.</p>
<h2 id="improve-training">Improve Training</h2>
<p>The second half of the video focus on how to speedup training.</p>
<figure><a href="https://news.mit.edu/2023/snail.png" title="snail" data-thumbnail="snail.png" data-sub-html="&lt;h2&gt;Stable diffusion&#39;s take of a very fast snail&lt;/h2&gt;&lt;p&gt;snail&lt;/p&gt;">
        <img src="https://news.mit.edu/svg/loading.min.svg" data-src="snail.png" data-srcset="snail.png, snail.png 1.5x, snail.png 2x" data-sizes="auto" alt="snail.png"/>
    </a><figcaption>Stable diffusion&#39;s take of a very fast snail</figcaption>
    </figure>
<h3 id="residual-connections">Residual Connections</h3>
<p>This lools like bypassing a component of the model. Adding an extra trace on the PCB that goes around your chip, in the paper it looks like that:</p>
<figure><a href="https://news.mit.edu/2023/residual-connection.png" title="residual-connection" data-thumbnail="residual-connection.png" data-sub-html="&lt;h2&gt;Residual Connections&lt;/h2&gt;&lt;p&gt;residual-connection&lt;/p&gt;">
        <img src="https://news.mit.edu/svg/loading.min.svg" data-src="residual-connection.png" data-srcset="residual-connection.png, residual-connection.png 1.5x, residual-connection.png 2x" data-sizes="auto" alt="residual-connection.png"/>
    </a><figcaption>Residual Connections</figcaption>
    </figure>
<p>And in code like this:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>  <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>sa</span><span>(</span><span>self</span><span>.</span><span>ln1</span><span>(</span><span>x</span><span>))</span>
</span></span><span><span>  <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>ffwd</span><span>(</span><span>self</span><span>.</span><span>ln2</span><span>(</span><span>x</span><span>))</span>
</span></span><span><span>  <span>return</span> <span>x</span>
</span></span></code></pre></div><p>It works because <code>+</code> distribute the gradient, so instead of only receiving the gradient of the gradient, we get our own copy too, and we get to train faster üôå.</p>
<h3 id="layernorm">LayerNorm</h3>
<p>This is the same intuition as <code>BatchNorm</code> from the makemore series. Without all the messy internal state we have to carry around, because we only normalize a row instead of a column, so all the data we need is always there to be computed on the fly.</p>
<p>It looks a little something like:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>LayerNorm</span><span>:</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>,</span> <span>eps</span><span>=</span><span>1e-5</span><span>):</span>
</span></span><span><span>        <span>self</span><span>.</span><span>eps</span> <span>=</span> <span>eps</span>
</span></span><span><span>        <span>self</span><span>.</span><span>gamma</span> <span>=</span> <span>torch</span><span>.</span><span>ones</span><span>(</span><span>dim</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>dim</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>xmean</span> <span>=</span> <span>x</span><span>.</span><span>mean</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>        <span>xvar</span> <span>=</span> <span>x</span><span>.</span><span>var</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>        <span>xhat</span> <span>=</span> <span>(</span><span>x</span> <span>-</span> <span>xmean</span><span>)</span> <span>/</span> <span>torch</span><span>.</span><span>sqrt</span><span>(</span><span>xvar</span> <span>+</span> <span>self</span><span>.</span><span>eps</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>out</span> <span>=</span> <span>self</span><span>.</span><span>gamma</span> <span>*</span> <span>xhat</span> <span>+</span> <span>self</span><span>.</span><span>beta</span>
</span></span><span><span>        <span>return</span> <span>self</span><span>.</span><span>out</span>
</span></span><span><span>    
</span></span><span><span>    <span>def</span> <span>parameters</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>        <span>return</span> <span>[</span><span>self</span><span>.</span><span>gamma</span><span>,</span> <span>self</span><span>.</span><span>beta</span><span>]</span>
</span></span></code></pre></div><p>It works for the same reason BatchNorm worked, by squeezing the values close to softmax sweet spot <code>[-1, 1]</code> so we don‚Äôt get vanishing gradients.</p>
<h3 id="dropout">Dropout</h3>
<p>This one is more of a regularization trick to prevent overfitting. It works by randomly shutting down some percentage of neurons for a layer.</p>
<h2 id="final-words">Final words</h2>
<p>Andrej Karpathy is an amazing teacher. The title <a href="https://karpathy.ai/zero-to-hero.html" target="_blank" rel="noopener noreffer ">Neural Networks: Zero to Hero</a> delivers all it promised and more. The lectures share the same aura of brilliance as the Feynman series on physics. And the code has the simple elegance you‚Äôd find in a Peter Norvig essay. For a world-class course on ML, thank you Mr. Karpathy.</p>
<h2 id="the-code">The code</h2>
<p>Here‚Äôs my take on the tutorial with additional notes. You can get the code on <a href="https://github.com/peluche/makemore" target="_blank" rel="noopener noreffer ">GitHub</a> or bellow.</p>


</div></div>
  </body>
</html>
