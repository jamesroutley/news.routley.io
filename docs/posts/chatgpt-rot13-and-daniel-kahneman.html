<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jameswillia.ms/posts/chatgpt-rot13.html">Original</a>
    <h1>ChatGPT, Rot13, and Daniel Kahneman</h1>
    
    <div id="readability-page-1" class="page"><div>
      

      
      
<article>
  
  


  

  


  <p>I thought it would be interesting to see whether ChatGPT could solve some basic ciphers (Caesar, Vignere, etc.). I decided to start at the bottom, with perhaps the easiest possible cipher: <a href="https://en.wikipedia.org/wiki/ROT13">rot13</a>. I asked it to decode the rot-13 encoding of “Why did the chicken cross the road”:</p>
<figure><a href="https://jameswillia.ms/images/chatgpt-rot13.png">
    <img src="https://jameswillia.ms/images/chatgpt-rot13.png"/> </a>
</figure>

<p>So the first thing to note here is that ChatGPT is not able to solve the task, even for very small cases.</p>
<p>But nonetheless, I found this exchange interesting. ChatGPT’s approach to solving this brought to mind Daniel Kahneman’s book <a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast and Slow</a>. To me, it feels like ChatGPT is attempting to apply System 1 thinking (fast, instinctual) to a problem that requires a System 2 approach, and ends up getting exactly the results you’d expect from that: answers that looks vaguely sensible, but are actually way off base.</p>
<p>As a human, when performing rot13 decoding, you apply System 2 thinking: working through each letter, doing calculations in your head to find the right letter – perhaps remembering common letter mappings as you go. But in this case, it feels like ChatGPT is approaching the problem like a language learner put on spot: going wordwise, hazily recognising words, and filling in the gaps from there.</p>
<p>I wonder whether the distinction between the tasks ChatGPT is good at, and those that it isn’t, is whether the task is more amenable to System 1 or System 2 thinking? When I think of things that ChatGPT has been observed to be poor at, for example:</p>
<ul>
<li>evaluating complex mathematical equations</li>
<li>writing mathematical proofs</li>
<li>solving cryptic crossword clues</li>
</ul>
<p>all of them (generally) require some degree of System 2 thinking as a human. On the other hand, many of the tasks it is good at are things that humans use System 1 thinking to do:</p>
<ul>
<li>translating languages (once fluent)</li>
<li>writing jokes</li>
<li>summarising text</li>
</ul>
<p>When it makes mistakes, ChatGPT also displays the kind of overconfidence that System 1 thinking results in.</p>
<p>Interestingly, ChatGPT can kind of follow along with our System 2 reasoning (e.g: where I explained that it answering plaintext with a different number of letters from the ciphertext could not make sense, because rot13 is a one-to-one mapping). But it seems incapable of taking that reasoning and applying it again, even when the next application is very similar.</p>

</article>
 
      

      

      
  
  
  
 

      </div></div>
  </body>
</html>
