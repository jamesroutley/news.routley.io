<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/">Original</a>
    <h1>YJIT is the most memory-efficient Ruby JIT</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This year, the YJIT team and I have gotten a paper accepted at
<a href="https://2023.splashcon.org/home/mplr-2023">MPLR 2023</a> (Managed Programming Languages and Runtimes),
which is now freely available through <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">ACM open access</a>.
The paper, titled <em>“Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach”</em>, goes
into details of the strategy taken to evaluate YJIT’s performance in a production context.
One of our key findings, when comparing YJIT against other existing Ruby JITs such as JRuby
and TruffleRuby, is that YJIT is the most memory-efficient Ruby JIT (by a long shot).</p>

<p>A video recording of our presentation at MPLR is also
<a href="https://www.youtube.com/watch?v=fMGuQXNqlaE&amp;t=9900s">available on YouTube</a>.</p>

<h2 id="background">Background</h2>

<p>Many published papers about JIT compilers only look at peak performance in terms of
running time on benchmarks after a certain amount of warm-up time.
This can be deceptive because the amount of time needed for a JIT compiler to warm up can be
arbitrarily long. Typically, the JIT compiler implementation is given as many benchmark
iterations as it needs to reach peak performance, and the peak performance as measured then
is reported. The amount of time needed to reach peak performance is often not discussed.
The same goes for memory usage.</p>

<p>I believe that
those metrics are typically ignored by academic compiler researchers
because they may reveal an inconvenient reality. If you give your JIT compiler
an arbitrary amount of time and memory to reach peak performance, it’s easier to throw all
possible known optimizations at a piece of code and reach high peak performance numbers.
However, if your JIT compiler uses an arbitrarily high amount of memory and needs a very long
time to warm up, even though it may have the fastest peak performance, it may be
unusable in most real-world production environments.</p>

<p>When deploying code into production, peak performance is not the only thing that matters.
On our production servers at Shopify, there is not a huge amount of memory available for
the JIT compiler to use. Almost all of the memory is used to run multiple server processes, and
also to cache various resources in RAM. This has forced us to spend a significant amount of
effort on optimizing YJIT’s memory usage to make the compiler more resource-efficient.</p>

<p>At Shopify, we deploy frequently, with consecutive deployments sometimes less than 20 minutes
apart. This adds an extra layer of challenge because, despite these frequent deployments,
we can’t tolerate significant increases in response time.
If a JIT compiler needs a significant amount of time to
warm up, or suddenly deoptimizes large amounts of code, this can translate into requests timing out, and customers
abandoning their shopping carts, which ultimately would result in lost revenue. As such, smooth,
predictable warm-up and stable performance are of critical importance.</p>

<h2 id="methodology">Methodology</h2>

<p>In our paper, we look at YJIT’s warm-up, memory usage and peak performance on benchmarks,
as well as on our deployment on Shopify’s StoreFront Renderer (SFR). For context, SFR
renders all Shopify storefronts, which is the first thing buyers see when they navigate to a store hosted by Shopify.
It is mostly written in Ruby, depends on over 220 Ruby gems, renders over 4.5
million Shopify stores in over 175 countries, and is served by multiple clusters distributed worldwide. It is capable of serving over 1.27
million requests per second, and has processed over 197 billion USD in transaction
volume in 2022.
YJIT is currently deployed to all SFR servers. For this paper, we’ve performed
experiments using Ruby head on a subset of servers in all clusters. We’ve also
included some control servers which ran the same Ruby commit without YJIT. Data
for the SFR experiments was gathered over a 48-hour period.</p>

<p>For our experiments on benchmarks, we used 11 headline benchmarks from the
yjit-bench suite. These are all benchmarks that are based on real-world Ruby gems,
with a bias towards web workloads. This includes benchmarks such as <code>railsbench</code>,
<code>hexapdf</code>, <code>activerecord</code> and <code>liquid-render</code>, which is a benchmark using Shopify’s liquid
template language gem.
We benchmarked YJIT, RJIT (Takashi Kokubun’s experimental Ruby JIT written
in Ruby), JRuby, as well as both the JVM and native versions of TruffleRuby.
More details on our experimental setup
are provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>.</p>

<p>We also maintain a website, <a href="https://speed.yjit.org">speed.yjit.org</a>, which tracks
YJIT’s performance and memory overhead as well as various other statistics on this
benchmark suite over time. Recently, as we were looking for more challenging and realistic
benchmarks, we’ve also turned the codebase of the
<a href="https://lobste.rs/">lobste.rs</a> website
<a href="https://railsatscale.com/2023-08-25-we-turned-lobsters-into-a-rails-benchmark-for-yjit/">into a benchmark</a> as well.</p>

<h2 id="key-findings">Key Findings</h2>

<h3 id="performance-on-benchmarks">Performance on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_speedup.png" alt="" width="90%"/></p>

<p>The above graph shows the average execution time on each benchmark for each of the Ruby JIT
implementations we benchmarked. The time is normalized to the time taken by the CRuby interpreter,
where the time taken by the interpreter has value 1.0, with values below 1 being faster than
the interpreter.
We were very generous in terms of warm-up time. Each benchmark was run for 1000 iterations, and
the first half of all the iterations were discarded as warm-up time, giving each JIT a more
than fair chance to reach peak performance.</p>

<p>As can be seen, TruffleRuby has the best peak performance on most (but not all) benchmarks.
YJIT outperforms the CRuby interpreter on every benchmark by a wide margin. We can also see
that YJIT performs very competitively compared to JRuby (a JVM-based Ruby JIT), outperforming
it on most benchmarks.</p>

<h3 id="warm-up-time-on-benchmarks">Warm-Up Time on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/railsbench_warmup.png" alt="" width="90%"/></p>

<p>This graph shows a plot of the performance over time for each Ruby JIT for the <code>railsbench</code> benchmark.
The x-axis is the total execution time in seconds, while the y-axis is the time per benchmark iteration. This
allows us to visualize how the performance of each VM evolves over time. As can be seen, YJIT
almost immediately outperforms the CRuby interpreter, with RJIT not too far behind. JRuby takes over a
minute to reach peak performance, but does not reliably outperform CRuby on this benchmark.</p>

<p>TruffleRuby eventually outperforms the other JITs, but it takes about two minutes to do so. It is
also initially quite a bit slower than the CRuby interpreter, taking over 110
seconds to catch up to the interpreter’s speed. This would be
problematic in a production context such as Shopify’s, because it can lead to much slower
response times for some customers, which could translate into lost business. Such wide variations
in performance can also make the scaling of server resources more difficult. We should also note
that while <code>railsbench</code> is a somewhat challenging benchmark, it is much smaller than our actual
production deployment. Warm-up data for other benchmarks is provided in the paper.</p>

<h3 id="memory-usage-on-benchmarks">Memory Usage on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_memory.png" alt="" width="90%"/></p>

<p>The above graph is in my opinion the most interesting graph of the paper. It is a plot of the memory
usage (RSS) of each Ruby implementation for each benchmark.
Because of the widely varying scale between data points,
the use of a logarithmic was considered <img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/trollface.png" alt="" height="15"/>.
However, we have decided to use a
linear scale to maintain a more accurate sense of proportions. Do note, however, that there is
a cut in the graph between 5GiB and 17GiB.</p>

<p>As can be seen in the graph above, thanks in large part to the work put in by our team to
optimize its memory
overhead, YJIT has the lowest memory overhead of any Ruby JIT by far. The JVM-based
Ruby implementations often use one or even two orders of magnitude more memory than YJIT and the
CRuby interpreter. The memory overhead compared to CRuby can be as much as several gigabytes.
This is on benchmarks that often require less than 100MiB to run with the CRuby interpreter,
which makes such a high amount of memory overhead seem disproportionate.</p>

<p>One significant caveat here is that we are measuring the total memory usage of each system. That
is, the memory overhead of the JVM itself has an impact. The way that JRuby and TruffleRuby
internally represent Ruby objects in memory, which is different from the way CRuby represents
objects in memory, also has an impact.
However, the bottom line is the same. Using several gigabytes more memory than CRuby to run simple
benchmarks likely bodes poorly for many production deployments.
For smaller production deployments, for example, a project running on inexpensive Virtual Private Servers (VPS),
there may be only 1GiB or 2GiB of RAM available in total.
For a company like Shopify running a large fleet of servers,
the amount of server processes that can be run on a single machine, and how much memory can be used
for caching matters.</p>

<p>There is another caveat, which is that JRuby and TruffleRuby, unlike CRuby, do not use a GVL
(Global VM Lock, analogous to Python’s GIL). In theory, this means that they can more effectively
use multithreading, and amortize some of their memory overhead across multiple server threads.
Still, there is a case to be made that the memory overhead of JRuby and TruffleRuby is something
that is often overlooked and probably should be better optimized.
Aside from production deployments, the <code>ruby-lsp</code> benchmark is a benchmark of the Ruby language
server, which can be used by code editors such as VSCode.
We can see that on this benchmark, the JVM-based Ruby implementations can use multiple gigabytes
of memory, and despite that, perform worse than the CRuby interpreter. This is far from ideal
for a Ruby gem that is meant to be run on developer laptops.</p>

<p>I would also like to note that RJIT, Takashi Kokubun’s experimental pure Ruby JIT, looks
quite good in this comparison. However, in the previous graph, the inclusion of the
JVM-based Ruby JITs distorts the sense of scale. The next graph below shows the same memory
usage comparison, but with only CRuby, YJIT and RJIT included. Currently, there are
situations where RJIT uses several times more memory than CRuby and YJIT:</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_mem_yjit_rjit_only.png" alt="" width="90%"/></p>

<p>YJIT, being that it is written in Rust (a systems language), has
access to more tools to optimize memory usage in places where individual bits count. Matching
YJIT’s memory usage in a pure Ruby JIT would be difficult and likely would necessitate
augmenting Ruby with special systems programming primitives. For example, to be able to
efficiently pack structs and bit fields in memory and to also pack structs and
arrays inside of other structs.
Encoding data structures in memory as compactly as possible is likely challenging to do
in a JVM-based JIT implementation as well.</p>

<h3 id="performance-in-production">Performance In Production</h3>

<p>The following graph looks at the latency of YJIT on our SFR deployment when compared to
servers that run the same Ruby commit with YJIT disabled. If you are wondering why no other
Ruby JITs are included in this graph, it is because, at this time, other Ruby JITs could not
be deployed in production for this application, either due to compatibility issues or due to memory constraints.
On average, YJIT is 14.1% faster than the CRuby interpreter during the period examined.
Importantly, because YJIT is able to compile new code very fast, it is also faster than the
interpreter even on the slowest p99 requests.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_latency.png" alt="" width="90%"/></p>

<p>If a 14.1% speedup seems underwhelming to you, do keep in mind that the latency numbers
provided measure the total time needed to generate a response. This includes not only
time spent in JIT-compiled code, but also time spent in C functions that we cannot optimize,
and time the server spends waiting for database requests and other I/O operations.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_speedup.png" alt="" width="90%"/></p>

<p>The graph above shows the speedup provided by YJIT over the interpreter. The purple vertical
lines represent deployments.
During the time period we examined, there were 35 deployments of new code to production,
and the shortest interval during consecutive deployments was just 19 minutes 21 seconds.
The key takeaway here is that even though we perform frequent
deployments during the daytime, because YJIT is able to warm up very fast, it remains
consistently faster than the interpreter.
Again, more information is provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>,
including data about YJIT’s memory usage in our production deployments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve recently published a paper about YJIT at MPLR 2023, in which we evaluate YJIT’s performance
on both benchmarks as well as on Shopify’s flagship production deployment, which serves an
enormous amount of traffic worldwide.
In this paper, we make it a point to examine not just peak performance, but to also discuss and
evaluate warm-up time and total memory usage.</p>

<p>The YJIT team has spent a significant amount of effort optimizing YJIT so that it doesn’t just
show good peak performance numbers, but also does this while being memory-efficient.
This effort has paid off, with YJIT having the least memory overhead of any Ruby JIT, which
has been crucial in enabling YJIT to handle Shopify’s SFR deployment.</p>

<p>Since our MPLR paper was published, we’ve kept improving
YJIT’s performance.
As of this writing, I am looking at our internal dashboard, and YJIT is providing a 27.2% average
speedup over the interpreter on our SFR deployment.
With the Ruby 3.3 release approaching, there will be a lot to be excited about this holiday season,
as we are once
again gearing up for a very strong Ruby release. This year, YJIT 3.3 will deliver better performance,
while using less memory, and also warming up faster than YJIT 3.2.
Expect another post on the <a href="https://railsatscale.com/">Rails at Scale blog</a> with more benchmark results soon!</p>

<p>For more information on how to use YJIT, see the <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT README</a>.
Should you wish to cite <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">our MPLR 2023 paper</a>, I’ve also
included the bibtex snippet below:</p>

<div><div><pre><code>@inproceedings{yjit_mplr_2023,
author = {Chevalier-Boisvert, Maxime and Kokubun, Takashi and Gibbs, Noah and Wu, Si Xing (Alan) and Patterson, Aaron and Issroff, Jemma},
title = {Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622982},
doi = {10.1145/3617651.3622982},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {20–33},
numpages = {14},
keywords = {dynamically typed, optimization, just-in-time, virtual machine, ruby, compiler, bytecode},
location = {Cascais, Portugal},
series = {MPLR 2023}
}
</code></pre></div></div>

  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
