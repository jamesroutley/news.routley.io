<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed">Original</a>
    <h1>Cerebras Inference: AI at Instant Speed</h1>
    
    <div id="readability-page-1" class="page"><div><p><span data-contrast="auto">Cerebras inference is designed to serve models from billions to trillions of parameters. When models exceed the memory capacity of a single wafer, we split them at layer boundaries and map them to multiple CS-3 systems. 20B models fit on a single CS-3 while 70B models fit on as few as four systems. In the weeks to come we will be adding larger models such as Llama3-405B and Mistral Large with industry leading speed and cost per token.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<h3>16-bit model weights for the highest accuracy</h3>
<p><span data-contrast="auto">Some companies try to overcome the memory bandwidth bottleneck by reducing weight precision from 16-bit to 8-bit, often without informing their users. This approach, however, can result in loss of accuracy. Cerebras inference runs Llama3.1 8B and 70B models using the original 16-bit weights released by Meta, ensuring the most accurate and reliable model output. Our evaluations, along with third-party benchmarks, show that 16-bit models score up to 5% higher than their 8-bit counterparts, resulting in substantially better performance in multi-turn conversations, math, and reasoning tasks.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<h3>Cerebras Inference API</h3>
</div><p><span lang="EN-US" xml:lang="EN-US" data-contrast="auto"><span>Cerebras</span><span> inference is available today via </span></span><a href="http://link/" target="_blank" rel="noreferrer noopener"><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span data-ccp-charstyle="Hyperlink">chat</span></span></a><span lang="EN-US" xml:lang="EN-US" data-contrast="auto"><span> and </span></span><a href="http://link/" target="_blank" rel="noreferrer noopener"><span lang="EN-US" xml:lang="EN-US" data-contrast="none"><span data-ccp-charstyle="Hyperlink">API</span></span></a><span lang="EN-US" xml:lang="EN-US" data-contrast="auto"><span> access. Built on the familiar OpenAI Chat Completions format, it allows developers to integrate our powerful inference capabilities by simply swapping out the API key. </span><span>Cerebras</span><span> inference API offers some of the most generous rate limits in the industry at </span><span>60 tokens per minute and </span><span>1</span><span> million tokens per day</span><span>, making it the ideal platform for AI developers to </span><span>built</span><span> interactive and agentic applications.</span></span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p><div><p><span data-contrast="auto">Cerebras inference API offers the best combination of performance, speed, accuracy and cost. At 450 tokens per second, it’s the only solution that runs Llama3.1-70B at instantaneous speed. We use Meta’s original 16-bit model weights, ensuring the highest accuracy. For launch we are providing developers with 1 million free tokens daily. For at scale deployments, our pricing is a fraction of popular GPU clouds.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<p><span data-contrast="auto">For initial launch, we are offering Llama3.1 8B and 70B models. We will be adding support for larger models such as Llama3 405B and Mistral Large 2 in the coming weeks. Please let us know what models you’d like to see added.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<h3>Why fast inference matters</h3>
<p><span data-contrast="auto">The implications of high-speed inference extend far beyond raw metrics. By dramatically reducing processing time, we’re enabling more complex AI workflows and enhancing real-time LLM intelligence. Traditional LLMs output everything they think immediately, without stopping to consider the best possible answer. New techniques like scaffolding, on the other hand, function like a thoughtful agent who explores different possible solutions before deciding. This “thinking before speaking” approach provides over 10x performance on demanding tasks like code generation, fundamentally boosting the intelligence of AI models without additional training. But these techniques require up to 100x more tokens at runtime, and thus is only possible in real time running on Cerebras hardware.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<p><span data-contrast="auto">With record-breaking performance, industry-leading pricing, and open API access, Cerebras Inference sets a new standard for open LLM development and deployment. As the only solution capable of delivering both high-speed training and inference, Cerebras opens entirely new capabilities for AI. We can’t wait to see the new and exciting applications developers will build with Cerebras Inference.</span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<p><b><span data-contrast="auto">Links:</span></b><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></p>
<ul>
<li>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="5" data-list-defn-props="{&#34;335552541&#34;:1,&#34;335559685&#34;:720,&#34;335559991&#34;:360,&#34;469769226&#34;:&#34;Symbol&#34;,&#34;469769242&#34;:[8226],&#34;469777803&#34;:&#34;left&#34;,&#34;469777804&#34;:&#34;&#34;,&#34;469777815&#34;:&#34;hybridMultilevel&#34;}" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><a href="https://inference.cerebras.ai/"><span data-contrast="none">Try chat</span></a><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></li>
<li data-leveltext="" data-font="Symbol" data-listid="5" data-list-defn-props="{&#34;335552541&#34;:1,&#34;335559685&#34;:720,&#34;335559991&#34;:360,&#34;469769226&#34;:&#34;Symbol&#34;,&#34;469769242&#34;:[8226],&#34;469777803&#34;:&#34;left&#34;,&#34;469777804&#34;:&#34;&#34;,&#34;469777815&#34;:&#34;hybridMultilevel&#34;}" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><a href="https://cloud.cerebras.ai" target="_blank" rel="noopener"><span data-contrast="none">API Access</span></a><span data-contrast="auto"> </span><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></li>
<li data-leveltext="" data-font="Symbol" data-listid="5" data-list-defn-props="{&#34;335552541&#34;:1,&#34;335559685&#34;:720,&#34;335559991&#34;:360,&#34;469769226&#34;:&#34;Symbol&#34;,&#34;469769242&#34;:[8226],&#34;469777803&#34;:&#34;left&#34;,&#34;469777804&#34;:&#34;&#34;,&#34;469777815&#34;:&#34;hybridMultilevel&#34;}" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><a href="https://discord.com/channels/1085960591052644463/1087099163432468540"><span data-contrast="none">Discord</span></a><span data-ccp-props="{&#34;201341983&#34;:0,&#34;335559739&#34;:160,&#34;335559740&#34;:278}"> </span></li>
</ul>
</li>
</ul>
</div></div>
  </body>
</html>
