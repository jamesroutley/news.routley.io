<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinalderson.com/posts/which-programming-languages-are-most-token-efficient/">Original</a>
    <h1>Which programming languages are most token-efficient?</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    
    <div>
        <p>I&#39;ve been trying to think through what happens to programming languages and tooling if humans are increasingly no longer writing it. I wrote about how good agents are at <a href="https://martinalderson.com/posts/ported-photoshop-1-to-csharp-in-30-minutes/">porting code recently</a>, and it got me thinking a bit more about what constraints LLMs have vs humans.</p>
<p>One of the biggest constraints LLMs have is on context length. This is a difficult problem to solve, as memory usage rises significantly with longer context window in current transformer architectures. And with the current memory shortages, I don&#39;t think the world is drowning in memory right now.</p>
<p>As such, for software development agents, how &#39;token efficient&#39; a programming language actually could make a big difference and I wonder if it starts becoming a factor in language selection in the future. Given a significant amount of a coding agents context window is going to be code, a more token efficient language should allow longer sessions and require fewer resources to deliver.</p>
<p>We&#39;ve seen <a href="https://toonformat.dev/">TOON</a> (an encoding of JSON to be more token efficient), but what about programming languages?</p>
<h2>Methodology</h2>
<p>I came across the <a href="https://rosettacode.org/wiki/Rosetta_Code">RosettaCode</a> project while doing some research thinking around this. It describes itself a programming chrestomathy site (which I love, by the way). It has over a thousand programming &#39;tasks&#39; that people build in various languages. It has contributions in nearly 1,000 different programming languages.</p>
<p>I found a <a href="https://github.com/acmeism/RosettaCodeData">GitHub mirror</a> of the dataset, so grabbed Claude Code and asked it to make a comparison of them, using the Xenova/gpt-4 tokenizer from Hugging Face - which is a community port of OpenAI&#39;s GPT4 tokenizer.</p>
<p>I then told Claude Code to suggest a selection of the most popular programming languages, which roughly matches my experience, and then find tasks that had solutions contributed in <em>all</em> 19 of these languages, and then ran them through the tokenizer. I didn&#39;t include TypeScript because there were very few tasks in the Rosetta Code dataset.</p>
<blockquote>
<p>There are many, many potential limits and biases involved in this dataset and approach! It&#39;s meant as a interesting look at somewhat like-for-like solutions to some programming tasks, not a scientific study.</p>
</blockquote>
<h2>Results</h2>
<p><img src="https://martinalderson.com/img/token-efficiency-chart.png" alt="Token efficiency comparison across programming languages"/></p><blockquote>
<p><strong>Update:</strong> A lot of people asked about APL. I reran on a smaller set of like-for-like coding tasks - it came 4th at 110 tokens. Turns out APL&#39;s famous terseness isn&#39;t a plus for LLMs: the tokenizer is badly optimised for its symbol set, so all those unique glyphs (⍳, ⍴, ⌽, etc.) end up as multiple tokens each.</p>
</blockquote>
<blockquote>
<p><strong>Update 2:</strong> A reader reached out about J - a language I&#39;d never heard of. It&#39;s an array language like APL but uses ASCII instead of special symbols. It dominates at just 70 tokens average, nearly half of Clojure (109 tokens). Array languages can be extremely token-efficient when they avoid exotic symbol sets. If token efficiency turns out to be a key driver, this is perhaps a very interesting way for languages to evolve.</p>
</blockquote>
<p>There was a very meaningful gap of 2.6x between C (the least token efficient language I compared) and Clojure (the most efficient).</p>
<p>Unsurprisingly, dynamic languages were much more token efficient (not having to declare <em>any</em> types saves a lot of tokens) - though JavaScript was the most verbose of the dynamic languages analysed.</p>
<p>What did surprise me though was just <em>how</em> token efficient some of the functional languages like Haskell and F# were - barely less efficient than the most efficient dynamic languages. This is no doubt to their very efficient type inference systems. I think using typed languages for LLMs has an awful lot of benefits - not least because it can compile and get rapid feedback on any syntax errors or method hallucinations. With LSP it becomes even more helpful.</p>
<p>Assuming 80% of your context window is code reads, edits and diffs, using Haskell or F# would potentially result in a significantly longer development session than using Go or C#.</p>
<p>It&#39;s really interesting to me that we are in this strange future where we have petaflops of compute but code verbosity of our &#39;small&#39; context windows actually might matter. LLMs continue to break my mental model of how we should be looking at software engineering.</p>



    </div>
</article></div>
  </body>
</html>
