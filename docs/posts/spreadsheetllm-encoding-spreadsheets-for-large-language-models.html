<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2407.09025">Original</a>
    <h1>SpreadsheetLLM: Encoding Spreadsheets for Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+Y">Yuzhang Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+J">Jianbo Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+H">Haoyu Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+J">Junyu Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+S">Shiyu Xia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+M">Mengyu Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Y">Yun Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cambronero,+J">Jos√© Cambronero</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Y">Yeye He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+S">Shi Han</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+D">Dongmei Zhang</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2407.09025">View PDF</a>
    <a href="https://arxiv.org/html/2407.09025v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs&#39; powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs&#39; token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4&#39;s in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Jianbo Zhao [<a href="https://arxiv.org/show-email/309f2264/2407.09025">view email</a>]      </p></div></div>
  </body>
</html>
