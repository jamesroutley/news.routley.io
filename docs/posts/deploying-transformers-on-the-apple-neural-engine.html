<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://machinelearning.apple.com/research/neural-engine-transformers">Original</a>
    <h1>Deploying Transformers on the Apple Neural Engine</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><section><div><div><p><span>content type </span><span><a href="https://machinelearning.apple.com/research/?type=article">article</a></span><span aria-hidden="true"> | </span><span>published </span>June 2022</p></div></div></section><section><div><p>An increasing number of the machine learning (ML) models we build at Apple each year are either partly or fully adopting the <a href="https://arxiv.org/abs/1706.03762" target="_blank" aria-label="Transformer architecture - Opens in a new window" rel="noopener nofollow">Transformer architecture</a>. This architecture helps enable experiences such as <a href="https://machinelearning.apple.com/research/panoptic-segmentation">panoptic segmentation in Camera with HyperDETR</a>, <a href="https://machinelearning.apple.com/research/on-device-scene-analysis">on-device scene analysis in Photos</a>, <a href="https://support.apple.com/guide/iphone/use-voiceover-for-images-and-videos-iph37e6b3844/ios">image captioning for accessibility</a>, <a href="https://apps.apple.com/us/app/translate/id1514844618">machine translation</a>, and many others. This year at WWDC 2022, Apple is making available an open-source reference <a href="https://pytorch.org" target="_blank" aria-label="PyTorch - Opens in a new window" rel="noopener nofollow">PyTorch</a> implementation of the Transformer architecture, giving developers worldwide a way to seamlessly deploy their state-of-the-art Transformer models on Apple devices.</p><p>This implementation is specifically optimized for the Apple Neural Engine (ANE), the energy-efficient and high-throughput engine for ML inference on Apple silicon. It will help developers minimize the impact of their ML inference workloads on app memory, app responsiveness, and device battery life. Increasing the adoption of on-device ML deployment will also benefit user privacy, since data for inference workloads remains on-device, not on the server.</p><p>In this article we share the principles behind this reference implementation to provide generalizable guidance to developers on optimizing their models for ANE execution. Then, we put these principles into action and showcase how to deploy an example pretrained Transformer model, the popular <a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english" target="_blank" aria-label="Hugging Face distilbert - Opens in a new window" rel="noopener nofollow">Hugging Face distilbert</a>, in just a few lines of code. Notably, this model, which works out-of-the-box and on device using Core ML already, is up to 10 times faster and consumes 14 times less memory after our optimizations. </p><h2>The Transformer Architecture</h2><p>Published in 2017, the <a href="https://arxiv.org/abs/1706.03762" target="_blank" aria-label="Transformer - Opens in a new window" rel="noopener nofollow">Transformer</a> architecture has had a great impact in many fields, including natural language processing and computer vision, in a short period of time. Models built on this architecture produce state-of-the-art results across a wide spectrum of tasks while incurring little to no domain-specific components. This versatility has resulted in a proliferation of applications built on this architecture. Most notably, the <a href="https://huggingface.co/models" target="_blank" aria-label="Hugging Face model hub - Opens in a new window" rel="noopener nofollow">Hugging Face model hub</a> hosts tens of thousands of pretrained Transformer models, such as variants of <a href="https://openai.com/blog/better-language-models/" target="_blank" aria-label="GPT-2 - Opens in a new window" rel="noopener nofollow">GPT-2</a> and <a href="https://arxiv.org/abs/1810.04805" target="_blank" aria-label="BERT - Opens in a new window" rel="noopener nofollow">BERT</a>, which were trained and shared by the ML community. Some of these models average tens of millions of monthly downloads, contributing to the research momentum behind training bigger, better, and increasingly multitask Transformer models and giving birth to the development of efficient deployment strategies on both the <a href="https://github.com/huggingface/optimum" target="_blank" aria-label="device side - Opens in a new window" rel="noopener nofollow">device side</a> and the <a href="https://huggingface.co/blog/bert-inferentia-sagemaker" target="_blank" aria-label="server side - Opens in a new window" rel="noopener nofollow">server side</a>. </p><h2>The Apple Neural Engine</h2><p>The first generation of the Apple Neural Engine (ANE) was released as part of the A11 chip found in iPhone X, our flagship model from 2017. It had a peak throughput of 0.6 teraflops (TFlops) in half-precision floating-point data format (float16 or FP16), and it efficiently powered on-device ML features such as Face ID and Memoji.</p><p>Fast-forward to 2021, and the fifth-generation of the 16-core ANE is capable of 26 times the processing power, or 15.8 TFlops, of the original. Since 2017, use of the ANE has been steadily increasing from a handful of Apple applications to numerous applications from both Apple and the developer community. The availability of the Neural Engine also expanded from only the iPhone in 2017 to iPad starting with the A12 chip and to Mac starting with the M1 chip.</p><figure id="figure1"><div><h3>ANE Peak Throughput (FP16)</h3></div><figcaption aria-hidden="true">Figure 1: The evolution of the Apple Neural Engine, 2017 to 2021. The 16-core Neural Engine on the A15 Bionic chip on iPhone 13 Pro has a peak throughput of 15.8 teraflops, an increase of 26 times that of iPhone X.</figcaption></figure><p>When training ML models, developers benefit from accelerated training on GPUs with <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/" target="_blank" aria-label="PyTorch - Opens in a new window" rel="noopener nofollow">PyTorch</a> and <a href="https://developer.apple.com/metal/tensorflow-plugin/">TensorFlow</a> by leveraging the Metal Performance Shaders (MPS) back end. For deployment of trained models on Apple devices, they use <a href="https://coremltools.readme.io/docs" target="_blank" aria-label="coremltools - Opens in a new window" rel="noopener nofollow">coremltools</a>, Apple’s open-source unified conversion tool, to convert their favorite PyTorch and TensorFlow models to the Core ML model package format. Core ML then seamlessly blends CPU, GPU, and ANE (if available) to create the most effective hybrid execution plan exploiting all available engines on a given device. It lets a wide range of implementations of the same model architecture benefit from the ANE even if the entire execution cannot take place there due to idiosyncrasies of different implementations. This workflow is designed to make it easy for developers to deploy models on Apple devices without having to worry about the capabilities of any particular device or implementation.</p><h2>Principles Behind Optimizing Transformers for the Neural Engine</h2><p>Although the implementation flexibility that hybrid execution offers is simple and powerful, we can trade this flexibility off in favor of a particular and principled implementation that deliberately harnesses the ANE, resulting in significantly increased throughput and reduced memory consumption. Other benefits include mitigating the inter-engine context-transfer overhead and opening up the CPU and the GPU to execute non-ML workloads while ANE is executing the most demanding ML workloads.</p><p>In this section, we describe some of the generalizable principles behind our reference implementation for Transformers with the goal of empowering developers to optimize models they intend to deploy on the ANE.</p><h3>Principle 1: Picking the Right Data Format</h3><p>In general, the Transformer architecture processes a 3D input tensor that comprises a batch of B sequences of S embedding vectors of dimensionality C. We represent this tensor in the (B, C, 1, S) data format because the most conducive data format for the ANE (hardware and software stack) is 4D and channels-first.</p><p>The native <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py#L101" target="_blank" aria-label="torch.nn.Transformer - Opens in a new window" rel="noopener nofollow">torch.nn.Transformer</a> and many other PyTorch implementations use either the (B, S, C) or the (S, B, C) data formats, which are both channels-last and 3D data formats. These data formats are compatible with <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" aria-label="nn.Linear layers - Opens in a new window" rel="noopener nofollow">nn.Linear layers</a>, which constitute a major chunk of compute in the Transformer. To migrate to the desirable (B, C, 1, S) data format, we swap all <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" aria-label="nn.Linear - Opens in a new window" rel="noopener nofollow">nn.Linear</a> layers with <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" target="_blank" aria-label="nn.Conv2d layers - Opens in a new window" rel="noopener nofollow">nn.Conv2d layers</a>. Furthermore, to preserve compatibility with previously trained checkpoints using the baseline implementation, we register a <a href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/huggingface/distilbert.py#L221" target="_blank" aria-label="load_state_dict_pre_hook - Opens in a new window" rel="noopener nofollow">load_state_dict_pre_hook</a> to automatically unsqueeze the nn.Linear weights twice in order to match the expected nn.Conv2d weights shape as shown <a href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/huggingface/distilbert.py#L503" target="_blank" aria-label="here - Opens in a new window" rel="noopener nofollow">here</a>.</p><p>The mapping of the sequence (S) axis to the last axis of the 4D data format is very important because the last axis of an ANE buffer is not packed; it must be contiguous and aligned to 64 bytes. This constraint applies only to the last axis, and the ANE compiler determines whether the rest of the axes are packed for best performance. Although unpacked buffers allow faster read times, if used improperly, they cause larger than necessary buffers to be allocated. For example, if the last axis is used as a singleton one by the model implementation’s data format, it will be padded to 64 bytes, which results in 32 times the memory cost in 16-bit and 64 times the memory cost in 8-bit precision. Such an increase in buffer size will significantly reduce the chance of L2 cache residency and increase the chance of hitting DRAM. This is not desirable from a power and latency perspective.</p><h3>Principle 2: Chunking Large Intermediate Tensors</h3><p>For the multihead attention function in the Transformer, we <a href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/reference/multihead_attention.py#L81" target="_blank" aria-label="split - Opens in a new window" rel="noopener nofollow">split</a> the query, key, and value tensors to create an explicit list of single-head attention functions, each of which operates on smaller chunks of input data. Smaller chunks increase the chance of L2 cache residency as well as increasing multicore utilization during compilation.</p><h3>Principle 3: Minimizing Memory Copies</h3><p>Given that at least one axis of ANE buffers is not packed, reshape and transpose operations are likely to trigger memory copies unless specifically handled. In our reference implementation, we avoid all reshapes and incur only <a href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/reference/multihead_attention.py#L87" target="_blank" aria-label="one transpose - Opens in a new window" rel="noopener nofollow">one transpose</a> on the key tensor right before the query and key matmul. We avoid further reshape and transpose operations during the batched matrix multiplication operation for scaled dot-product attention by relying on a particular einsum operation. Einsum is a notational convention that implies summation over a set of indexed terms in a formula. We use the bchq,bkhc-&gt;bkhq <a href="https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/reference/multihead_attention.py#L95" target="_blank" aria-label="einsum formula - Opens in a new window" rel="noopener nofollow">einsum formula</a>, which represents a batched matmul operation whose data format directly maps to hardware without intermediate transpose and reshape operations. </p><h3>Principle 4: Handling Bandwidth-Boundness</h3><p>Even after all the optimizations, many Transformer configurations become bandwidth-bound on the ANE when the sequence length is relatively short. This is due to the fact that large parameter tensors are being fetched from memory, only to be applied on too few inputs before the next parameter tensor is fetched. Fetching from memory dominates overall latency in these cases. Bandwidth-boundness manifests very clearly in <a href="#figure2">Figure 2</a>. The optimized model’s latency stays approximately constant across sequence lengths of 32, 64, and 128 (with batch size 1) even though the computational load quadruples. One way to escape the bandwidth-bound regime is to increase the batch size for batch inference workloads. Another way is to reduce the parameter tensor size by quantization or pruning such that memory fetching becomes cheaper and faster.</p><h2>Principles Packaged into Code: ane_transformers</h2><p>We share the reference implementation built on these principles and distribute it on PyPI to accelerate Transformers running on Apple devices with an ANE, on A14 and later or M1 and later chips. The package is called ane_transformers and the first on-device application using this package was <a href="https://machinelearning.apple.com/research/panoptic-segmentation">HyperDETR</a>, as described in our previous article.</p><p>Next, we showcase the application of these principles to a pretrained Transformer model: <a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you" target="_blank" aria-label="distilbert from Hugging Face - Opens in a new window" rel="noopener nofollow">distilbert from Hugging Face</a>. Code for this example is also made available through ane_transformers.</p><h2>Case Study: Hugging Face distilbert</h2><p>As the scaling laws of deep learning continue to hold, the ML community is training bigger and more capable Transformer models. However, most open-source implementations of the Transformer architecture are optimized for either large-scale training hardware or no hardware at all. Furthermore, despite significant concurrent advancements in model compression, the state-of-the-art models are scaling faster than compression techniques are improving.</p><p>Hence, there is immense need for on-device inference optimizations to translate these research gains into practice. These optimizations will enable ML practitioners to deploy much larger models on the same input set, or to deploy the same models to run on much larger sets of inputs within the same compute budget.</p><p>In this spirit, we take the principles behind our open-source reference PyTorch implementation for the Transformer architecture and apply them to the popular distilbert model from the Hugging Face model hub in a <a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you" target="_blank" aria-label="case study - Opens in a new window" rel="noopener nofollow">case study</a>. Applying these optimizations resulted in a forward pass that is up to 10 times faster with a simultaneous reduction of peak-memory consumption of 14 times on iPhone 13. Using our reference implementation, on a sequence length of 128 and a batch size of 1, the iPhone 13 ANE achieves an average latency of 3.47 ms at 0.454 W and 9.44 ms at 0.072 W. Even using our reference implementation, the ANE peak throughput is far from being saturated for this particular model configuration, and the performance may be further improved with using the quantization and pruning techniques as discussed earlier.</p><figure id="figure2"><div><div><div><div id="default-panel-iPhone 12 ANE and iOS15" role="tabpanel" aria-labelledby="default-tab-iPhone 12 ANE and iOS15"></div></div></div></div><figcaption aria-hidden="true">Figure 2: Performance curves (latency, memory) for Hugging Face distilbert on various Apple devices and operating system versions. Shaded ranges indicate performance under varying power consumption.</figcaption></figure><p>To contextualize the numbers we just reported, a recent <a href="https://huggingface.co/blog/bert-inferentia-sagemaker" target="_blank" aria-label="article - Opens in a new window" rel="noopener nofollow">article</a> from Hugging Face and AWS reported “the average latency . . . is 5-6 ms for a sequence length of 128” for the same model in our case study, when deployed on the server side using <a href="https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls" target="_blank" aria-label="ML-optimized ASIC hardware - Opens in a new window" rel="noopener nofollow">ML-optimized ASIC hardware</a> from AWS. Based on this external data point, we are happy to report that our on-device inference latency compares favorably to those measured on an optimized server-side inference engine while executing on a device that is orders of magnitude more energy-constrained. <a href="#figure2">Figure 2</a> shows the latency and memory consumption of the same model across different sequence lengths, batch sizes, and devices.</p><h2>Putting It All Together: From PyTorch to Xcode</h2><p>Finally, we show how these optimizations are applied through just a few lines of code, and then we profile the model in Xcode using the new Core ML Performance Report feature available in Xcode 14.</p><p>To begin, we initialize the baseline <a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english" target="_blank" aria-label="distilbert model - Opens in a new window" rel="noopener nofollow">distilbert model</a> from the Hugging Face model hub:</p><pre><code><span>import</span> transformers
model_name <span>=</span> <span>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
baseline_model <span>=</span> transformers<span>.</span>AutoModelForSequenceClassification<span>.</span>from_pretrained<span>(</span>
    model_name<span>,</span>
    return_dict<span>=</span><span>False</span><span>,</span>
    torchscript<span>=</span><span>True</span><span>,</span>
<span>)</span><span>.</span><span>eval</span><span>(</span><span>)</span>
</code></pre><p>Then we initialize the mathematically equivalent but optimized model, and we restore its parameters using that of the baseline model:</p><pre><code><span>from</span> ane_transformers<span>.</span>huggingface <span>import</span> distilbert <span>as</span> ane_distilbert
optimized_model <span>=</span> ane_distilbert<span>.</span>DistilBertForSequenceClassification<span>(</span>
    baseline_model<span>.</span>config<span>)</span><span>.</span><span>eval</span><span>(</span><span>)</span>
optimized_model<span>.</span>load_state_dict<span>(</span>baseline_model<span>.</span>state_dict<span>(</span><span>)</span><span>)</span>
</code></pre><p>Next we create sample inputs for the model:</p><pre><code>tokenizer <span>=</span> transformers<span>.</span>AutoTokenizer<span>.</span>from_pretrained<span>(</span>model_name<span>)</span>
tokenized <span>=</span> tokenizer<span>(</span>
    <span>[</span><span>&#34;Sample input text to trace the model&#34;</span><span>]</span><span>,</span>
    return_tensors<span>=</span><span>&#34;pt&#34;</span><span>,</span>
    max_length<span>=</span><span>128</span><span>,</span>  
    padding<span>=</span><span>&#34;max_length&#34;</span><span>,</span>
<span>)</span>
</code></pre><p>We then trace the optimized model to obtain the <a href="https://coremltools.readme.io/docs/pytorch-conversion#generate-a-torchscript-version" target="_blank" aria-label="expected model format (TorchScript) - Opens in a new window" rel="noopener nofollow">expected model format (TorchScript)</a> for the <a href="https://coremltools.readme.io/docs" target="_blank" aria-label="coremltools - Opens in a new window" rel="noopener nofollow">coremltools</a> conversion tool.</p><pre><code><span>import</span> torch
traced_optimized_model <span>=</span> torch<span>.</span>jit<span>.</span>trace<span>(</span>
    optimized_model<span>,</span>
    <span>(</span>tokenized<span>[</span><span>&#34;input_ids&#34;</span><span>]</span><span>,</span> tokenized<span>[</span><span>&#34;attention_mask&#34;</span><span>]</span><span>)</span>
<span>)</span>
</code></pre><p>Finally, we use coremltools to generate the Core ML model package file and save it.</p><pre><code><span>import</span> coremltools <span>as</span> ct
<span>import</span> numpy <span>as</span> np
ane_mlpackage_obj <span>=</span> ct<span>.</span>convert<span>(</span>
    traced_optimized_model<span>,</span>
    convert_to<span>=</span><span>&#34;mlprogram&#34;</span><span>,</span>
    inputs<span>=</span><span>[</span>
        ct<span>.</span>TensorType<span>(</span>
                <span><span>f&#34;input_</span><span><span>{</span>name<span>}</span></span><span>&#34;</span></span><span>,</span>
                    shape<span>=</span>tensor<span>.</span>shape<span>,</span>
                    dtype<span>=</span>np<span>.</span>int32<span>,</span>
                <span>)</span> <span>for</span> name<span>,</span> tensor <span>in</span> tokenized<span>.</span>items<span>(</span><span>)</span>
            <span>]</span><span>,</span>
<span>)</span>
out_path <span>=</span> <span>&#34;HuggingFace_ane_transformers_distilbert_seqLen128_batchSize1.mlpackage&#34;</span>
ane_mlpackage_obj<span>.</span>save<span>(</span>out_path<span>)</span>
</code></pre><p>To verify performance, developers can now launch Xcode and simply add this model package file as a resource in their projects. After clicking on the Performance tab, the developer can generate a performance report on locally available devices, for example, on the Mac that is running Xcode or another Apple device that is connected to that Mac. <a href="#figure3">Figure 3</a> shows a performance report generated for this model on an iPhone 13 Pro Max with iOS 16.0 installed.</p><figure id="figure3"><div><h3>Xcode Core ML Performance Report</h3><div></div></div><figcaption aria-hidden="true">Figure 3: Core ML performance reports generated in Xcode. Developers can review runtime statistics, layer dispatch to each engine, and accomplish additional tasks.</figcaption></figure><p>Transformers are becoming ubiquitous in ML as their capabilities scale up with their size. Deploying Transformers on-devices requires efficient strategies, and we are thrilled to provide guidance to developers on this topic. Learn more about the implementation described in this post on the <a href="https://github.com/apple/ml-ane-transformers" target="_blank" aria-label="Machine Learning ANE Transformers GitHub. - Opens in a new window" rel="noopener nofollow">Machine Learning ANE Transformers GitHub.</a> </p><p> Many people contributed to this work, including Atila Orhon, Aseem Wadhwa, Youchang Kim, Francesco Rossi, and Vignesh Jagadeesh. </p><p>Apple Developer. “Framework: Core ML.” 2022. <a href="https://developer.apple.com/documentation/coreml">[link.]</a></p><p>Apple Developer. “Getting Started with tensorflow-metal PluggableDevice.” 2022. <a href="https://developer.apple.com/metal/tensorflow-plugin/">[link.]</a></p><p>Apple Developer. &#34;Accelerate machine learning with Metal.&#34; 2022. <a href="https://machinelearning.apple.com/research/developer.apple.com/wwdc22/10063" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>“Apple Neural Engine Transformers.” GitHub. 2022. <a href="https://github.com/apple/ml-ane-transformers" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>Hugging Face. “Models.” 2022. <a href="https://huggingface.co/models" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>PyTorch. “From Research to Production.” 2022. <a href="https://pytorch.org/index.html#news-items" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>PyTorch. “Introducing Accelerated PyTorch Training on Mac.” 2022. <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>“Trending Research.” Papers with Code. 2022. <a href="https://paperswithcode.com" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>Apple. “On-Device Panoptic Segmentation for Camera Using Transformers.” Machine Learning Research, October 2021. <a href="https://machinelearning.apple.com/research/panoptic-segmentation">link.</a></p><p>Apple. “Use VoiceOver for Images and Videos on iPhone.” iPhone User Guide. Cupertino, CA: Apple, 2022. <a href="https://support.apple.com/guide/iphone/use-voiceover-for-images-and-videos-iph37e6b3844/ios">[link.]</a></p><p>Core ML Tools. “Introduction: Use coremltools to Convert Models from Third-Party Libraries to Core ML.” 2022. <a href="https://coremltools.readme.io/docs" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” 2019. <a href="https://arxiv.org/abs/1810.04805" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>OpenAI. “Better Language Models and Their Implications.” February 14, 2019. <a href="https://openai.com/blog/better-language-models/" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>Schmid, Philipp. “Accelerate BERT Inference with Hugging Face Transformers and AWS Inferentia.” Hugging Face, 2022. <a href="https://huggingface.co/blog/bert-inferentia-sagemaker" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p><p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” 2017. <a href="https://arxiv.org/abs/1706.03762" target="_blank" aria-label="[link.] - Opens in a new window" rel="noopener nofollow">[link.]</a></p></div></section><section><div><div><div><div><p>Scene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or &#34;signals&#34;) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.</p><p><a href="https://machinelearning.apple.com/research/on-device-scene-analysis" aria-label="See full article details regarding A Multi-Task Neural Architecture for On-Device Scene Analysis">See article details</a></p></div></div><div><div><p>At Apple we use machine learning to teach our products to understand the world more as humans do. Of course, understanding the world better means building great assistive experiences. Machine learning can help our products be intelligent and intuitive enough to improve the day-to-day experiences of people living with disabilities. We can build machine-learned features that support a wide range of users including those who are blind or have low vision, those who are deaf or are hard of hearing, those with physical motor limitations, and also support those with cognitive disabilities.</p><p><a href="https://machinelearning.apple.com/research/mobile-applications-accessible" aria-label="See full article details regarding Making Mobile Applications Accessible with Machine Learning">See article details</a></p></div></div></div></div></section><section></section></div></div>
  </body>
</html>
