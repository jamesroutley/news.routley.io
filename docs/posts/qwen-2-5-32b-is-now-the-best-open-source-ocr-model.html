<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/getomni-ai/benchmark/blob/main/README.md">Original</a>
    <h1>Show HN: Qwen-2.5-32B is now the best open source OCR model</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><section aria-labelledby="file-name-id-wide file-name-id-mobile"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><img src="https://camo.githubusercontent.com/628b0587ccd21fe4c90b6d165c5ab572fd0a2a1c78d59c4ce1587d2cc6f79d17/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6f6d6e692d6f63722d62656e63686d61726b2e706e67" alt="Omni OCR Benchmark" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/omni-ocr-benchmark.png"/></a></p>

<p dir="auto">A benchmarking tool that compares OCR and data extraction capabilities of different large multimodal models such as gpt-4o, evaluating both text and json extraction accuracy. The goal of this benchmark is to publish a comprehensive benchmark of OCRaccuracy across traditional OCR providers and multimodal Language Models. The evaluation dataset and methodologies are all Open Source, and we encourage expanding this benchmark to encompass any additional providers.</p>
<p dir="auto"><a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr" rel="nofollow"><strong>Open Source LLM Benchmark Results (Mar 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a href="https://getomni.ai/ocr-benchmark" rel="nofollow"><strong>Benchmark Results (Feb 2025)</strong></a> | <a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark" rel="nofollow"><strong>Dataset</strong></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NDk3MjAsIm5iZiI6MTc0MzU0OTQyMCwicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMTcwMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWEzMWUxMjg0ZGU5MDFjZjQ1MDUzNjI3YjZjZGI3ZDA2YzlkNWUwM2U3ZTdlYmU0YzY0ZWRiOWYwNzU1ZTg3ZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Ie-b03PbNposM6U4Wg696RL18b1pUEd5b1KNo1OL93o"><img src="https://private-user-images.githubusercontent.com/30934424/429220141-2be179ad-0abd-4f0e-b73a-7d5a70390367.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDM1NDk3MjAsIm5iZiI6MTc0MzU0OTQyMCwicGF0aCI6Ii8zMDkzNDQyNC80MjkyMjAxNDEtMmJlMTc5YWQtMGFiZC00ZjBlLWI3M2EtN2Q1YTcwMzkwMzY3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDAxVDIzMTcwMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWEzMWUxMjg0ZGU5MDFjZjQ1MDUzNjI3YjZjZGI3ZDA2YzlkNWUwM2U3ZTdlYmU0YzY0ZWRiOWYwNzU1ZTg3ZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Ie-b03PbNposM6U4Wg696RL18b1pUEd5b1KNo1OL93o" alt="image"/></a></p>

<p dir="auto">The primary goal is to evaluate JSON extraction from documents. To evaluate this, the Omni benchmark runs <strong>Document ⇒ OCR ⇒ Extraction</strong>. Measuring how well a model can OCR a page, and return that content in a format that an LLM can parse.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67"><img src="https://camo.githubusercontent.com/3888f1759b84adb355b4f91d0da2ee6fc772fc7b44fed778aff22d8e6b8cb0f1/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6d6574686f646f6c6f67792d6469616772616d2e706e67" alt="methodology" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/methodology-diagram.png"/></a></p>


<p dir="auto">We use a modified <a href="https://github.com/zgrossbart/jdd">json-diff</a> to identify differences between predicted and ground truth JSON objects. You can review the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/evaluation/json.ts">evaluation/json.ts</a> file to see the exact implementation. Accuracy is calculated as:</p>
<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="48ab4fa3f8d3dd346b396ed5891458ab">$$\text{Accuracy} = 1 - \frac{\text{number of difference fields}}{\text{total fields}}$$</math-renderer>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67"><img src="https://camo.githubusercontent.com/1cb5a1143e4200a729f6c296a5306caceb6de7dec623d0f59da135530a4fa6d6/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f6a736f6e5f61636375726163792e706e67" alt="json-diff" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/json_accuracy.png"/></a></p>

<p dir="auto">While the primary benchmark metric is JSON accuracy, we have included <a href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow">levenshtein distance</a> as a measurement of text similarity between extracted and ground truth text.
Lower distance indicates higher similarity. Note this scoring method heavily penalizes accurate text that does not conform to the exact layout of the ground truth data.</p>
<p dir="auto">In the example below, an LLM could decode both blocks of text without any issue. All the information is 100% accurate, but slight rearrangements of the header text (address, phone number, etc.) result in a large difference on edit distance scoring.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67"><img src="https://camo.githubusercontent.com/d1d49357f37a1fa8e1ba6673a550fbbcc70d4245bf41c6be215d8ce25927dbc2/68747470733a2f2f6f6d6e6961692d696d616765732e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f656469745f64697374616e63652e706e67" alt="text-similarity" data-canonical-src="https://omniai-images.s3.us-east-1.amazonaws.com/edit_distance.png"/></a></p>

<ol dir="auto">
<li>Clone the repo and install dependencies: <code>npm install</code></li>
<li>Prepare your test data
<ol dir="auto">
<li>For local data, add individual files to the <code>data</code> folder.</li>
<li>To pull from a DB, add <code>DATABASE_URL</code> in your <code>.env</code></li>
</ol>
</li>
<li>Copy the <code>models.example.yaml</code> file to <code>models.yaml</code>. Set up API keys in <code>.env</code> for the models you want to test. Check out the <a href="#supported-models">supported models</a> here.</li>
<li>Run the benchmark: <code>npm run benchmark</code></li>
<li>Results will be saved in the <code>results/&lt;timestamp&gt;/results.json</code> file.</li>
</ol>

<p dir="auto">To enable specific models, create a <code>models.yaml</code> file in the <code>src</code> directory. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models.example.yaml">models.example.yaml</a> file for the required variables.</p>
<div dir="auto" data-snippet-clipboard-copy-content="models:
  - ocr: gemini-2.0-flash-001 # The model to use for OCR
    extraction: gpt-4o # The model to use for JSON extraction

  - ocr: gpt-4o
    extraction: gpt-4o
    directImageExtraction: true # Whether to use the model&#39;s native image extraction capabilities"><pre><span>models</span>:
  - <span>ocr</span>: <span>gemini-2.0-flash-001 </span><span><span>#</span> The model to use for OCR</span>
    <span>extraction</span>: <span>gpt-4o </span><span><span>#</span> The model to use for JSON extraction</span>

  - <span>ocr</span>: <span>gpt-4o</span>
    <span>extraction</span>: <span>gpt-4o</span>
    <span>directImageExtraction</span>: <span>true </span><span><span>#</span> Whether to use the model&#39;s native image extraction capabilities</span></pre></div>
<p dir="auto">You can view configuration for each model in the <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models">src/models/</a> folder.</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic</td>
<td><code>claude-3-5-sonnet-20241022</code></td>
<td>✅</td>
<td>✅</td>
<td><code>ANTHROPIC_API_KEY</code></td>
</tr>
<tr>
<td>OpenAI</td>
<td><code>gpt-4o</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
<tr>
<td>Gemini</td>
<td><code>gemini-2.0-flash-001</code>, <code>gemini-1.5-pro</code>, <code>gemini-1.5-flash</code></td>
<td>✅</td>
<td>✅</td>
<td><code>GOOGLE_GENERATIVE_AI_API_KEY</code></td>
</tr>
<tr>
<td>Mistral</td>
<td><code>mistral-ocr</code></td>
<td>✅</td>
<td>❌</td>
<td><code>MISTRAL_API_KEY</code></td>
</tr>
<tr>
<td>OmniAI</td>
<td><code>omniai</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OMNIAI_API_KEY</code>, <code>OMNIAI_API_URL</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemma 3</td>
<td><code>google/gemma-3-27b-it</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Qwen 2.5</td>
<td><code>qwen2.5-vl-32b-instruct</code>, <code>qwen2.5-vl-72b-instruct</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>Llama 3.2</td>
<td><code>meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo</code>, <code>meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo</code></td>
<td>✅</td>
<td>❌</td>
<td></td>
</tr>
<tr>
<td>ZeroX</td>
<td><code>zerox</code></td>
<td>✅</td>
<td>✅</td>
<td><code>OPENAI_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Provider</th>
<th>Models</th>
<th>OCR</th>
<th>JSON Extraction</th>
<th>Required ENV Variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWS</td>
<td><code>aws-text-extract</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_REGION</code></td>
</tr>
<tr>
<td>Azure</td>
<td><code>azure-document-intelligence</code></td>
<td>✅</td>
<td>❌</td>
<td><code>AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT</code>, <code>AZURE_DOCUMENT_INTELLIGENCE_KEY</code></td>
</tr>
<tr>
<td>Google</td>
<td><code>google-document-ai</code></td>
<td>✅</td>
<td>❌</td>
<td><code>GOOGLE_LOCATION</code>, <code>GOOGLE_PROJECT_ID</code>, <code>GOOGLE_PROCESSOR_ID</code>, <code>GOOGLE_APPLICATION_CREDENTIALS_PATH</code></td>
</tr>
<tr>
<td>Unstructured</td>
<td><code>unstructured</code></td>
<td>✅</td>
<td>❌</td>
<td><code>UNSTRUCTURED_API_KEY</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li>LLMS are instructed to use the following <a href="https://github.com/getomni-ai/benchmark/blob/main/src/models/shared/prompt.ts">system prompts</a> for OCR and JSON extraction.</li>
<li>For Google Document AI, you need to include <code>google_credentials.json</code> in the <code>data</code> folder.</li>
</ul>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/getomni-ai/benchmark/blob/main/assets/dashboard-gif.gif"><img src="https://github.com/getomni-ai/benchmark/raw/main/assets/dashboard-gif.gif" alt="dashboard" data-animated-image=""/></a></p>
<p dir="auto">You can use benchmark dashboard to easily view the results of each test run. Check out the <a href="https://github.com/getomni-ai/benchmark/blob/main/dashboard/README.md">dashboard documentation</a> for more details.</p>

<p dir="auto">This project is licensed under the MIT License - see the LICENSE file for details.</p>
</article></div></section></div></div></div></div>
  </body>
</html>
