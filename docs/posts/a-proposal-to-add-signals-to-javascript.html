<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/proposal-signals/proposal-signals">Original</a>
    <h1>A proposal to add signals to JavaScript</h1>
    
    <div id="readability-page-1" class="page"><div id="text-backpropagation-in-action">
<p>
Let us now put our code into action for a specific neural network. With \(\act\) as above, let
</p><p>

\begin{equation*}
F(b_1,w_1,b_2,w_2)
=\Obj_y\bigl(\act(b_2+w_2\act(b_1+w_1x))\bigr),
\end{equation*}

</p><p>
where the weights \(w_1\), \(w_2\) are of dimension \(4\times3\), \(2\times4\), and the objective function \(\Obj_y\) is the (squared and normalised) euclidean distance
</p><p>

\begin{equation*}
\Obj_y(z)
={\textstyle\frac12}\bigl((z^1-y^1)^2+(z^2-y^2)^2\bigr),
\end{equation*}

</p><p>
from some 2-vector \(y\). The gradient \(D\Obj_y(z)\) is \((z-y)^\top\), which reads the same in Python:
</p>

<div>
<pre><span><span>DΦ</span></span> <span>=</span> <span>lambda</span> <span>y</span>: <span>lambda</span> <span>z</span>: (z <span>-</span> y).<span><span><span>T</span></span></span>
</pre>
</div>

<p>
For the sake of illustration, assign generic values to the training-pair representative \((x,y)\) and to the biases and weights \(b_i\), \(w_i\):
</p>

<div>
<pre><span>def</span> <span>rand</span>(<span>*</span><span>d</span>, <span>rng</span><span>=</span>np.<span>random</span>.<span><span>default_rng</span></span>()):
    <span><span>&#34;&#34;&#34;Random floating-point array of dimension d.&#34;&#34;&#34;</span></span>
    <span>return</span> rng.<span><span>random</span></span>(d)

<span>x</span>, <span>y</span> <span>=</span> <span>rand</span>(<span>3</span>, <span>1</span>), <span>rand</span>(<span>2</span>, <span>1</span>)
<span>bws</span> <span>=</span> (<span>b1</span>,         <span>w1</span>),         (<span>b2</span>,         <span>w2</span>) \
    <span>=</span> (<span>rand</span>(<span>4</span>, <span>1</span>), <span>rand</span>(<span>4</span>, <span>3</span>)), (<span>rand</span>(<span>2</span>, <span>1</span>), <span>rand</span>(<span>2</span>, <span>4</span>))
</pre>
</div>

<p>
Now call <a href="#org383659b"><var>gradient_F</var></a> to compute the gradient of \(F\) via backpropagation:
</p>

<div>
<pre>(<span>δb2</span>, <span>δw2</span>), (<span>δb1</span>, <span>δw1</span>) <span>=</span> <span>gradient_F</span>(bws, x, <span><span>DΦ</span></span>(y))
</pre>
</div>

<p>
By verifying the <a href="#org7fe8ba9">backpropagation equations</a> we verify equality of <var>δb</var>​\(_i\), <var>δw</var>​\(_i\) with \(\∂F/∂{b_i}\), \(\∂F/∂{w_i}\):
</p>

<div>
<pre><span>def</span> <span>verify_eqn_B</span>(<span>x</span>, <span><span>Dφ</span></span>, <span>δb1</span>, <span>δw1</span>, <span>δb2</span>, <span>δw2</span>):
    <span><span>&#34;&#34;&#34;Verify the backpropagation equations (B).&#34;&#34;&#34;</span></span>
    <span>z2</span> <span>=</span> b2 <span>+</span> w2<span>@</span>(<span>a1</span> <span>:=</span> <span>σ</span>(<span>z1</span> <span>:=</span> b1 <span>+</span> w1<span>@</span>x))
    <span>assert</span> np.<span><span>allclose</span></span>(δb2, <span><span>Dφ</span></span>(<span>σ</span>(z2))<span>*</span><span><span>Dσ</span></span>(z2))
    <span>assert</span> np.<span><span>allclose</span></span>(δw2, <span>tensor</span>(δb2, a1))
    <span>assert</span> np.<span><span>allclose</span></span>(δb1, (δb2<span>@</span>w2)<span>*</span><span><span>Dσ</span></span>(z1))
    <span>assert</span> np.<span><span>allclose</span></span>(δw1, <span>tensor</span>(δb1, x))

<span>verify_eqn_B</span>(x, <span><span>DΦ</span></span>(y), δb1, δw1, δb2, δw2)
</pre>
</div>

<p>
Typically, we want the gradient not for a single training pair, rather for a <i>batch</i> of them. For instance, consider a batch of 10 training-pair representatives, arranged as corresponding columns of a pair of 2-dimensional arrays <var>xs</var>, <var>ys</var>:
</p>

<div>
<pre><span>batch_size</span> <span>=</span> <span>10</span>
<span>xs</span>, <span>ys</span> <span>=</span> <span>rand</span>(<span>3</span>, batch_size), <span>rand</span>(<span>2</span>, batch_size)
</pre>
</div>

<p>
Since every operation in our concrete implementation of backpropagation is vectorised, the call to compute the gradient for a batch of training-pair representatives is identical to that for a single one:
</p>

<div>
<pre>(<span>δb2s</span>, <span>δw2s</span>), (<span>δb1s</span>, <span>δw1s</span>) <span>=</span> <span>gradient_F</span>(bws, xs, <span><span>DΦ</span></span>(ys))
</pre>
</div>

<p>
Each <i>row</i> of <var>δb</var>​\(_i\)​<var>s</var>, <var>δw</var>​\(_i\)​<var>s</var> equals \(\∂F/∂{b_i}\), \(\∂F/∂{w_i}\) for the training-pair representative in the corresponding <i>column</i> of <var>xs</var>, <var>ys</var>. As before, we can verify equality by verifying the <a href="#org7fe8ba9">backpropagation equations</a> for each training pair:
</p>

<div>
<pre><span>for</span> <span>i</span> <span>in</span> <span><span>range</span></span>(batch_size):
    <span>x</span>, <span>y</span> <span>=</span> xs[:, [i]], ys[:, [i]]
    <span>δb1</span>, <span>δw1</span> <span>=</span> δb1s[[i]], δw1s[[i]]
    <span>δb2</span>, <span>δw2</span> <span>=</span> δb2s[[i]], δw2s[[i]]
    <span>verify_eqn_B</span>(x, <span><span>DΦ</span></span>(y), δb1, δw1, δb2, δw2)
</pre>
</div>

<p>
Thanks to vectorisation, the same call to <var>verify_eqn_B</var> does this in one go:
</p>

<div>
<pre><span>verify_eqn_B</span>(xs, <span><span>DΦ</span></span>(ys), δb1s, δw1s, δb2s, δw2s)
</pre>
</div>
</div></div>
  </body>
</html>
