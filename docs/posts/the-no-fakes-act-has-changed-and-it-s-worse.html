<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.eff.org/deeplinks/2025/06/no-fakes-act-has-changed-and-its-so-much-worse">Original</a>
    <h1>The NO FAKES act has changed, and it&#39;s worse</h1>
    
    <div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div>
    <div><div><div><p>A bill purporting to target the issue of misinformation and defamation caused by generative AI has mutated into something that could change the internet forever, harming speech and innovation from here on out.</p>
<p>The Nurture Originals, Foster Art and Keep Entertainment Safe (NO FAKES) Act aims to address understandable concerns about generative AI-created “replicas” by creating a broad new intellectual property right. That approach was <a href="https://www.eff.org/deeplinks/2024/04/congress-should-just-say-no-no-fakes">the first mistake</a>: rather than giving people targeted tools to protect against harmful misrepresentations<span data-huuid="17382269887519388031">—</span>balanced against the need to protect legitimate speech such as parodies and satires<span data-huuid="17382269887519388031">—</span>the original NO FAKES just federalized an image-licensing system.</p>
<p><a href="https://act.eff.org/action/tell-congress-throw-out-the-no-fakes-act-and-start-over">Take Action</a></p>
<p><a href="https://act.eff.org/action/tell-congress-throw-out-the-no-fakes-act-and-start-over">Tell Congress to Say No to NO FAKES</a></p>
<p>The updated bill doubles down on that initial mistaken approach by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them, with few safeguards against abuse.</p>
<p>The new version of NO FAKES requires almost every internet gatekeeper to create a system that will a) take down speech upon receipt of a notice; b) keep down any recurring instance<span data-huuid="17382269887519388031">—</span>meaning, adopt inevitably overbroad replica filters on top of the already deeply flawed copyright filters;  c) take down and filter tools that might have been used to make the image; and d) unmask the user who uploaded the material based on nothing more than the say so of person who was allegedly “replicated.”</p>
<p>This bill would be a disaster for internet speech and innovation.</p>
<h4><strong>Targeting Tools</strong></h4>
<p>The first version of NO FAKES focused on digital replicas. The new version goes further, targeting tools that can be used to produce images that aren’t authorized by the individual, anyone who owns the rights in that individual’s image, or the law. Anyone who makes, markets, or hosts such tools is on the hook. There are some limits<span data-huuid="17382269887519388031">—</span>the tools must be primarily designed for, or have only limited commercial uses other than making unauthorized images<span data-huuid="17382269887519388031">—</span>but those limits will offer cold comfort to developers given that they can be targeted based on nothing more than a bare allegation. These provisions effectively give rights-holders the veto power on innovation they’ve long sought in the copyright wars, based on the same tech panics. </p>
<h4><strong>Takedown Notices and Filter Mandate</strong></h4>
<p>The first version of NO FAKES set up a notice and takedown system patterned on the DMCA, with even fewer safeguards. NO FAKES expands it to cover more service providers and require those providers to not only take down targeted materials (or tools) but keep them from being uploaded in the future.  In other words, adopt broad filters or lose the safe harbor.</p>
<p>Filters are already a huge problem <a href="https://www.eff.org/wp/unfiltered-how-youtubes-content-id-discourages-fair-use-and-dictates-what-we-see-online">when it comes to copyright</a>, and at least in that instance all it <em>should </em>be doing is flagging for human review if an upload appears to be a whole copy of a work. The reality is that these systems often flag things that are <em>similar </em>but not the same (like two different people playing the same piece of <a href="https://www.eff.org/takedowns/sony-finally-admits-it-doesnt-own-bach-and-it-only-took-public-pressure">public domain music</a>). They also flag things for infringement based on <a href="https://www.eff.org/takedowns/mistake-so-bad-even-youtube-says-its-copyright-bot-really-blew-it">mere seconds of a match</a>, and they frequently do not take into account <a href="https://www.eff.org/takedowns">context that would make the use authorized by law</a>.</p>
<p>But copyright filters are not yet required by law. NO FAKES would create a legal mandate that will inevitably lead to hecklers’ vetoes and other forms of over-censorship.</p>
<p>The bill does contain carve outs for parody, satire, and commentary, but those will also be cold comfort for those who cannot afford to litigate the question.</p>
<h4><strong>Threats to Anonymous Speech</strong></h4>
<p>As currently written, NO FAKES also allows anyone to get a subpoena from a court clerk—not a judge, and without any form of proof—forcing a service to hand over identifying information about a user.</p>
<p>We&#39;ve already seen abuse of a similar system in action. In copyright cases, those unhappy with the criticisms being made against them get such subpoenas to silence critics. Often that the criticism includes the complainant&#39;s own words as proof of the criticism, an ur-example of fair use. But the subpoena is issued anyway and, unless the service is incredibly on the ball, the user can be unmasked.</p>
<p>Not only does this chill further speech, the unmasking itself can cause harm to users. Either reputationally or in their personal life.</p>
<h4><strong>Threats to Innovation</strong></h4>
<p>Most of us are very unhappy with the state of Big Tech. It seems like not only are we increasingly forced to use the tech giants, but that the quality of their services is actively degrading. By increasing the sheer amount of infrastructure a new service would need to comply with the law, NO FAKES makes it harder for any new service to challenge Big Tech. It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES.</p>
<p>Requiring removal of tools, apps, and services could likewise stymie innovation. For one, it would harm people using such services for otherwise lawful creativity.  For another, it would discourage innovators from developing new tools. Who wants to invest in a tool or service that can be forced offline by nothing more than an allegation?</p>
<p>This bill is a solution in search of a problem. Just a few months ago, Congress passed Take It Down, which targeted images involving intimate or sexual content. That deeply flawed bill pressures platforms to actively monitor online speech, including speech that is presently encrypted. But if Congress is really worried about privacy harms, it should at least wait to see the effects of the last piece of internet regulation before going further into a new one. Its failure to do so makes clear that this is not about protecting victims of harmful digital replicas.</p>
<p>NO FAKES is designed to consolidate control over the commercial exploitation of digital images, not prevent it. Along the way, it will cause collateral damage to all of us.</p>
<p><a href="https://act.eff.org/action/tell-congress-throw-out-the-no-fakes-act-and-start-over">Take Action</a></p>
<p><a href="https://act.eff.org/action/tell-congress-throw-out-the-no-fakes-act-and-start-over">Tell Congress to Say No to NO FAKES</a></p>

</div></div></div>  </div>

          </article>
    </div></div>
  </body>
</html>
