<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lamroger.com/posts/2024-05-24-on-small-language-models/">Original</a>
    <h1>On Small Language Models (SLMs)</h1>
    
    <div id="readability-page-1" class="page"><div><p>Haven&#39;t posted in a bit but I&#39;m back! Had a wonderful few weeks. My class on Generative AI wrapped up - very happy with the progress the students made. With the field still in its infancy, I encouraged them to continue to experiment and learn on their own since that&#39;s what we&#39;re all doing.</p><p>I also came back from Lisbon and had a great time. It&#39;s been a while since I&#39;ve traveled overseas so it was a nice change of scenery. The streets of LA hit a little different now.</p><p>&#34;If you are a student interested in building the next generation of AI systems, don&#39;t work on LLMs&#34; - <a href="https://twitter.com/ylecun/status/1793326904692428907">Yann LeCun</a></p><p>LLMs really are completely inaccessible... Millions of dollars in compute to train. Clusters of compute and data. They&#39;re cool to learn and amazing to see in action but not aligned with the hacker ethos.</p><p>I&#39;m still very excited with OpenELM, MLX in Pytorch, and the M-series processors in Macs. While Meta is leading in open sourcing models, it&#39;s looking like Apple is the one leading in hardware. I love remote-ing into my M2 Mac Mini from my Intel Macbook Pro. Together with Tailscale, it&#39;s practically seamless. Very excited to see the benchmarks with the anticipated M4 Mac Mini at WWDC.</p><p>I got to play with the OpenELM 270M model. Following their <a href="https://huggingface.co/apple/OpenELM-270M">HF Readme</a>, I had to request access to the Llama 2 model for the tokenizer but that was a quick approval (1-2 hrs).</p><p>The results were not great. Pretty bad TBH - not the fault of the engineers at Apple but the limitations of our techniques today.</p><p>Running with this prompt:</p><pre><code>python generate_openelm.py --model apple/OpenELM-270M --hf_access_token &lt;hf_token&gt; --prompt &#39;Once upon a time there was&#39; --generate_kwargs repetition_penalty=1.2
</code></pre><p><img src="https://sre.google/images/2024-05-24-on-small-language-models/completion_270m.png" alt="Completion on 270M params" loading="lazy"/></p><pre><code>python generate_openelm.py --model apple/OpenELM-270M-Instruct --hf_access_token &lt;hf_token&gt; --prompt &#39;[INST] What is the capital of California? [/INST]&#39; --generate_kwargs repetition_penalty=1.2
</code></pre><p><img src="https://sre.google/images/2024-05-24-on-small-language-models/instruct_270m.png" alt="Instruct on 270M params" loading="lazy"/></p><p>Looking forward to debugging a bit more to see if I can massage a valid response out of it. Probably looking at techniques from larger (but still smaller) models like this blog post from <a href="https://replicate.com/blog/how-to-prompt-llama">Replicate</a>.</p><p>I still look forward to training my own small model.</p><p>More soon!</p></div></div>
  </body>
</html>
