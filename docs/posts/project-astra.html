<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theverge.com/2024/5/14/24156296/google-ai-gemini-astra-assistant-live-io">Original</a>
    <h1>Project Astra</h1>
    
    <div id="readability-page-1" class="page"><div><p>“I’ve had this vision in my mind for quite a while,” says Demis Hassabis, the head of Google DeepMind and the <a href="https://www.theverge.com/23778745/demis-hassabis-google-deepmind-ai-alphafold-risks">leader of Google’s AI efforts</a>. Hassabis has been thinking about and working on AI for decades, but four or five years ago, something really crystallized. One day soon, he realized, “We would have this universal assistant. It’s multimodal, it’s with you all the time.” Call it the <em>Star Trek</em> Communicator; call it the voice from <em>Her</em>; call it whatever you want. “It’s that helper,” Hassabis continues, “that’s just useful. You get used to it being there whenever you need it.” </p><p>At <a href="https://www.theverge.com/2024/5/14/24155647/google-io-news-announcements-rumors-gemini-ai">Google I/O, the company’s annual developer conference</a>, Hassabis <a href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#ai-agents:~:text=Progress%20developing%20universal%20AI%20agents">showed off a very early version</a> of what he hopes will become that universal assistant. Google calls it Project Astra, and it’s a real-time, multimodal AI assistant that can see the world, knows what things are and where you left them, and can answer questions or help you do almost anything. In an incredibly impressive demo video that Hassabis swears is not faked or doctored in any way, an Astra user in Google’s London office asks the system to identify a part of a speaker, find their missing glasses, review code, and more. It all works practically in real time and in a very conversational way.</p><p>Astra is just one of many Gemini announcements at this year’s I/O. There’s a new model, <a href="https://www.theverge.com/2024/5/14/24155511/google-gemini-ai-flash-1-5-pro-updates-io">called Gemini 1.5 Flash</a>, designed to be faster for common tasks like summarization and captioning. Another new model, <a href="https://www.theverge.com/2024/5/14/24156255/google-veo-ai-generated-video-model-openai-sora-io">called Veo</a>, can generate video from a text prompt. Gemini Nano, the model designed to be used locally on devices like your phone, is supposedly faster than ever as well. The context window for <a href="https://www.theverge.com/2024/2/15/24073457/google-gemini-1-5-ai-model-llm">Gemini Pro</a>, which refers to how much information the model can consider in a given query, is doubling to 2 million tokens, and Google says the model is better at following instructions than ever. Google’s making fast progress both on the models themselves and on getting them in front of users.</p><div><div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A still from a video showing a phone identifying a speaker tweeter with AI." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p></div></figure></div></div><div><figcaption><em>Astra is multimodal by design — you can talk, type, draw, photograph, and video to chat with it.</em></figcaption> <p><cite>Image: Google</cite></p></div></div></div><p>Going forward, Hassabis says, the story of AI will be less about the models themselves and all about what they can do for you. And that story is all about agents: bots that don’t just talk with you but actually accomplish stuff on your behalf. “Our history in agents is longer than our generalized model work,” he says, pointing to the game-playing AlphaGo system from nearly a decade ago. Some of those agents, he imagines, will be ultra-simple tools for getting things done, while others will be more like collaborators and companions. “I think it may even be down to personal preference at some point,” he says, “and understanding your context.” </p><p>Astra, Hassabis says, is much closer than previous products to the way a true real-time AI assistant ought to work. When Gemini 1.5 Pro, the latest version of Google’s mainstream large language model, was ready, Hassabis says he knew the underlying tech was good enough for something like Astra to begin to work well. But the model is only part of the product. “We had components of this six months ago,” he says, “but one of the issues was just speed and latency. Without that, the usability isn’t quite there.” So, for six months, speeding up the system has been one of the team’s most important jobs. That meant improving the model but also optimizing the rest of the infrastructure to work well and at scale. Luckily, Hassabis says with a laugh, “That’s something Google does very well!”</p><p>A lot of Google’s AI announcements at I/O are about giving you more and easier ways to use Gemini. A new product called Gemini Live is a voice-only assistant that lets you have easy back-and-forth conversations with the model, interrupting it when it gets long-winded or calling back to earlier parts of the conversation. A new feature in Google Lens allows you to search the web by shooting and narrating a video. A lot of this is enabled by Gemini’s large context window, which means it can access a huge amount of information at a time, and Hassabis says it’s crucial to making it feel normal and natural to interact with your assistant.</p><div><div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="An image showing the benefits of Google’s new Gemini 1.5 Flash model." src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fill"/></span></p></div></figure></div></div><div><figcaption><em>Gemini 1.5 Flash exists to make AI assistants faster above all else.</em></figcaption> <p><cite>Image: Google</cite></p></div></div></div><p>Know who agrees with that assessment, by the way? OpenAI, which has been talking about AI agents for a while now. In fact, the company <a href="https://www.theverge.com/2024/5/13/24155652/chatgpt-voice-mode-gpt4o-upgrades">demoed a product strikingly similar to Gemini Live</a> barely an hour after Hassabis and I chatted. The two companies are increasingly fighting for the same territory and seem to share a vision for how AI might change your life and how you might use it over time. </p><p>How exactly will those assistants work, and how will you use them? Nobody knows for sure, not even Hassabis. One thing Google is focused on right now is trip planning — it built a new tool for using Gemini to build an itinerary for your vacation that you can then edit in tandem with the assistant. There will eventually be many more features like that. Hassabis says he’s bullish on phones and glasses as key devices for these agents but also says “there is probably room for some exciting form factors.” Astra is still in an early prototype phase and only represents one way you might want to interact with a system like Gemini. The DeepMind team is still researching how best to bring multimodal models together and how to balance ultra-huge general models with smaller and more focused ones.</p><p>We’re still very much in the “speeds and feeds” era of AI, in which every incremental model matters and we obsess over parameter sizes. But pretty quickly, at least according to Hassabis, we’re going to start asking different questions about AI. Better questions. Questions about what these assistants can do, how they do it, and how they can make our lives better. Because the tech is a long way from perfect, but it’s getting better really fast.</p></div></div>
  </body>
</html>
