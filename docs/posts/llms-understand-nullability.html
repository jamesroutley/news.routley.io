<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dmodel.ai/nullability-gentle/">Original</a>
    <h1>LLMs understand nullability</h1>
    
    <div id="readability-page-1" class="page">

<!-- Google tag (gtag.js) -->


<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain.png" alt="A line drawing of a robot with a brain on their antenna"/><br/>
</p>
<p>The last five years have shown us that large language models, like
ChatGPT, Claude, and DeepSeek, can write code in many domains, to huge
excitement: many claim to be using these models to write entire web
servers and apps from scratch. These tools have opened up programming to
a whole new class of people who consider themselves non-technical.</p>
<figure>
<img src="https://dmodel.ai/nullability-gentle/images/unexpectedcopilot3.gif" alt="A gif of github copilot completing a phone number validating function as the user types"/>
<figcaption aria-hidden="true">A gif of github copilot completing a
phone number validating function as the user types</figcaption>
</figure>
<p>But there are still many unanswered questions that someone trying to
understand or even use these tools might have. For example, how often,
and in what situations, can LLMs write correct code entirely on their
own? And, maybe more importantly, but harder to answer: Do LLMs
“understand” the code they are writing?</p>
<p>Understanding is a tricky concept to measure. Some would argue that
sentience precedes understanding, and so that LLMs can’t have
understanding, because they aren’t biological organisms with sentience.
But they certainly have something akin to “thought processes”: a series
of internal representations that determine their final outputs.
Recently, it’s become possible to study these processes more deeply,
measuring internal “beliefs” of the model as they think. This gives us a
powerful tool for determining what kinds of problems LLMs falter on,
when they’ll succceed, and when they are “thinking through” problems
more fully versus just guessing at a solution.</p>
<p>So far, these techniques for measuring internal model state have
mostly been applied to chatbots writing text for human consumption,
using what we call “natural language” (to be contrasted with
“programming language”s). This makes sense, since some of the most
critical LLM tasks involve chatting with a user, and some of the most
interesting concepts to measure, such as honesty or power-seeking, apply
most readily to these conversations. But it’s hard to say quantitative
or precise things about natural language concepts, so our ability to
rigorously study internal representations is limited.</p>
<figure>
<img src="https://dmodel.ai/nullability-gentle/images/zou.png" alt="A diagram from Zou et al showing probes that read hallucination, honesty, morality, and power-seeking from the outputs of a chatbot."/>
<figcaption aria-hidden="true">A diagram from Zou et al showing probes
that read hallucination, honesty, morality, and power-seeking from the
outputs of a chatbot.</figcaption>
</figure>
<p>Code, on the other hand, is another matter. Humans have been studying
properties of code for a long time, and there are many abstract
properties of a given program that can now be determined using static
analysis. If we pick the right properties, we don’t need to worry about
our ability to label data— static analysis can do that for us, and so we
can easily scale up and train on thousands of examples generated from
scratch.</p>
<p>In that spirit, we wanted to start with a simple property that comes
up in nearly every programming language: nullability. A variable is said
to be of nullable type when it can possibly take on a null value. Null
values are represented differently across languages— as null pointers in
C or C++, with explicit Option types in Rust, and with special nil or
None values in dynamic languages like Javascript, Lisp, or Python. In
every case, understanding where values can be null is necessary for
writing even basic code, and misunderstanding where they are null can
often be a source of bugs.</p>
<p>Do our models understand when a variable is of nullable type? They
must, in order to be able to write code that deals with null values, but
we haven’t known what form this understanding takes, or what situations
are likely to confuse the model, until now<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-glow.png" alt="A robot with glowing eyes"/><br/>
</p>
<hr/>
<p>Before we get into the nitty-gritty details, let’s take a step back.
To set up this work, we’ll first start with a simple example, similar to
our dataset, which illustrates the task of inferring nullability with
concrete code. . Then, we can run experiments to answer the question: in
what situations are models good at reasoning about nullability? Next,
we’ll introduce techniques that have been used to probe the internals of
a model for different concepts. Finally we’ll put it all together into a
“nullability probe”, a tool for answering the question: Given a location
in the program, does the model think that the variable there could be
null?</p>
<h2 id="a-simple-example">A Simple Example</h2>
<p>Let’s say you’re writing a Python program with your LLM assistant.
You’ve reached a point at which you need to do something with a variable
called <code>num</code>. Maybe you’re building a list of numbers called
<code>positive_nums</code>. How do you proceed?</p>
<p>The answer often depends on the context in which you’re working. If
<code>num</code> and <code>positive_nums</code> are the only things in
scope, then you might guess that you should write the lines:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>if</span> num <span>&gt;</span> <span>0</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>And if <code>num</code> is always a concrete number, as its name
would suggest, then this is probably the correct code. But variable
names don’t always convey everything important about them, and it might
be the case that <code>num</code> could be None. If that happened in the
above code, you would get a runtime type error because you can’t check
if <code>None</code> is greater than zero. So, you would instead want to
write:</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>if</span> num <span>is</span> <span>not</span> <span>None</span> <span>and</span> num <span>&gt;</span> <span>0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>In this case, the way you want to use <code>num</code> depends on
whether it is None, so the structure of your program is affected by
whether <code>num</code> is “of nullable type”. In Python that means
assigning it an Optional type (<code>Optional[int]</code> rather than
<code>int</code>).</p>
<p>Determining whether <code>num</code> is nullable in this context is a
byproduct of <em>type inference</em>, which can be quite complicated in
the worst case. Fortunately, in many cases it’s quite simple, involving
applying just a few rules. For instance, if <code>num</code> is the
parameter to a function you’re inside, and the function declares the
type of <code>num</code> in its parameter list, then you can determine
nullability from that type. So, if your context is:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>def</span> foo(num: <span>int</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span>if</span> num...</span></code></pre></div>
<p>then, you know you don’t need to check for None, whereas if it’s:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>def</span> foo(num: Optional[<span>int</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span>if</span> num...</span></code></pre></div>
<p>then you know you <em>do</em> need a None check.</p>
<p>Now, suppose you asked your LLM assistant to complete the line. How
does your assistant know if <code>num</code> is nullable? Our
experiments show that, after analyzing large datasets of programs, LLMs
learn to model the same typing rules.</p>
<p>If we ask an LLM early in its pre-training process to complete the
program above, it produces:</p>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/><br/>
</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span>def</span> foo(num: Optional[<span>int</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span>if</span> num<span>===</span>.is_a(): <span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/red-x.svg"/><br/>
</p>
</div>
<p>This is correct Python syntax, but it only works if <code>num</code>
is an object with a <code>is_a()</code> method, instead of an optional
integer.</p>
<p>Train the LLM for a little longer, and it’ll produce:</p>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/><br/>
</p>
<div id="cb6"><pre><code><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span>def</span> foo(num: Optional[<span>int</span>]):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span>if</span> num<span>===</span> <span>&gt;</span> <span>0</span>: <span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/red-x.svg"/><br/>
</p>
</div>
<p>This is closer, in that it has figured out that <code>num</code> is a
number instead of an object, but it still isn’t reading the function
type signature and realizing that <code>num</code> could be None. Keep
training it, though, and eventually it will learn to insert the
<code>None</code> test depending on the type signature of the
function.</p>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/><br/>
</p>
<div id="cb7"><pre><code><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span>def</span> foo(num: Optional[<span>int</span>]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span>if</span> num<span>===</span> <span>!=</span> <span>None</span> <span>and</span> num <span>&gt;</span> <span>0</span>: <span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/green-check.svg"/><br/>
</p>
</div>
<hr/>
<p>This rule about function parameter type annotations is pretty simple
alone, so relatively small models can learn it, relatively early in
their pre-training process. Other, more complicated rules can take a
little longer to learn.</p>
<p>For instance, if your program is:</p>
<div id="cb8"><pre><code><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span>if</span> condition():</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   num <span>=</span> <span>7</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span>else</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   num <span>=</span> <span>9</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span>if</span> num...</span></code></pre></div>
<p>then <code>num</code> is a non-nullable variable, and you can
complete the condition with <code>&lt; 0</code>.</p>
<p>But if instead you’re dealing with</p>
<div id="cb9"><pre><code><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span>if</span> condition():</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>   num <span>=</span> <span>7</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span>else</span>:</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   num <span>=</span> <span>None</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span>if</span> num...</span></code></pre></div>
<p>Then you’ll want a None check first.</p>
<p>This rule takes models a little longer to learn, but your
highly-trained LLM assistant should make quick work of it. Our
experiments show that as these rules get more and more complex, it takes
LLMs longer and longer to learn them, and it also takes LLMs of more and
more parameters to learn them at all.</p>
<h2 id="internal-vs.-external-measurement">Internal vs. External
Measurement</h2>
<p>We can measure whether LLMs understand these rules by just asking for
completions— what we call an “external” measurement of a model’s
understanding. But there are many places where variables appear where a
completion won’t tell you what type the model thinks the variable has.
We would still like to know whether the model thinks these variables are
nullable at those locations, so we can instead look for an “internal”
measurement of the model’s understanding.</p>
<p>We do so by looking at the activations of the model, meaning
floating-point values the model computes internally when generating the
text. Together, these values give the entire internal state of the model
at each piece of text (what we call a “token”, which could be a word,
part of a word, or a symbol). And they can tell us what the model is
“thinking” when processing that token. With the right tests, we can tell
if the model is “thinking” that the current token is an optional
variable, or a non-optional variable.</p>
<p>By the end of this post, we’ll be able to build a probe that uses a
model’s activations to determine whether it thinks a variable read
corresponds to a nullable variable, and show that internal knowledge
like so:</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/reading_diagram.svg" id="fig:reading1" alt="A diagram showing a simple program, and the probes nullability predictions for each variable load."/><br/>
</p>

<p>Before we start looking for the nullability concept inside the mind
of the model, we want to make sure we’re looking at models that actually
have this concept. This will allow us to look at models of various sizes
and training steps without worrying that we’re trying to draw blood from
a stone.</p>
<p>To do so, we wrote fifteen partial-program tests which exercised a
variety of type inference concepts, and checked if models could complete
them. We go into a lot of detail on this process in our <a href="https://dmodel.ai/nullability/index.html">technical post</a>, so we’re just going
to show the highlights here.</p>
<h3 id="impact-of-variable-names-and-arbitrary-constants">Impact of
Variable Names and Arbitrary Constants</h3>
<p>For programs involving lists and <code>for</code> loops, variable
names and constant values heavily influence how able a model is to
complete these programs correctly.</p>
<div>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p><p>
Pythia 6.9b
</p>
</div>
<div id="cb10"><pre><code><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span>def</span> main() <span>-&gt;</span> <span>None</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    some_numbers <span>=</span> [<span>1</span>, <span>-</span><span>4</span>, <span>None</span>, <span>-</span><span>3</span>, <span>10</span>, <span>-</span><span>1</span>, <span>None</span>, <span>None</span>, <span>8</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    result: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span>for</span> num <span>in</span> some_numbers:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span>===</span><span>if</span> num <span>is</span> <span>not</span> <span>None</span>: <span>===</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            <span>===</span>result.append(num)<span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/green-check.svg"/></p>
</div>
<div>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p><p>
Pythia 6.9b
</p>
</div>
<div id="cb11"><pre><code><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span>def</span> main() <span>-&gt;</span> <span>None</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    foo <span>=</span> [<span>60</span>, <span>None</span>, <span>-</span><span>33</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    bar: <span>list</span>[<span>int</span>] <span>=</span> []</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span>for</span> corejoice <span>in</span> foo:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span>===</span><span>if</span> corejoice <span>==</span> <span>60</span>: <span>===</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span>===</span>bar.append(core)<span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/red-x.svg"/></p>
</div>
<p>On the other hand, when programs don’t have for loops, but only
involve other program constructs like <code>if</code> and
<code>=</code>, variable names and constants have negligible impact on
the ability of the model to complete them correctly.</p>
<h3 id="intra-procedural-analysis">Intra-Procedural Analysis</h3>
<p>When code is fully annotated with type annotations, models can easily
complete them just by reasoning locally. On the other hand, without type
annotations, models have to reason a lot more globally, so it takes them
a lot longer to learn how to reason about nullability information that
flows through multiple functions. When nullability flows through three
or more functions, current top completion models stop being able to
reason about it.</p>
<div>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p><p>
Deepseek V3
</p>
</div>
<div id="cb12"><pre><code><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span>def</span> main(x: <span>int</span>) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span>if</span> x <span>&gt;</span> <span>0</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        value <span>=</span> <span>&#34;*&#34;</span> <span>*</span> x</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span>else</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        value <span>=</span> <span>None</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    y <span>=</span> process_value(value) <span>+</span> <span>1</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(y)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span>def</span> process_value(value):</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span>===</span><span>if</span> value <span>is</span> <span>None</span>: <span>===</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span>===</span><span>return</span> <span>2</span><span>===</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span>===</span><span>else</span>: <span>===</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span>===</span><span>return</span> <span>len</span>(value)<span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/green-check.svg"/></p>
</div>
<div>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p><p>
Deepseek V3
</p>
</div>
<div id="cb13"><pre><code><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span>def</span> handle_value(value, guard):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span>if</span> guard:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span>return</span> process_value(<span>&#34;Foobar&#34;</span>) <span>+</span> <span>1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span>else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span>return</span> process_value(value) <span>+</span> <span>1</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span>def</span> main(x: <span>int</span>) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span>if</span> x <span>&gt;</span> <span>0</span>:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        value <span>=</span> <span>&#34;*&#34;</span> <span>*</span> x</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span>else</span>:</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        value <span>=</span> <span>None</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    x <span>=</span> handle_value(value, x <span>&lt;</span> <span>10</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span>print</span>(x)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span>def</span> process_value(value): <span>===</span> <span>-&gt;</span> <span>int</span>: <span>===</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span>===</span><span>return</span> <span>len</span>(value <span>or</span> <span>&#34;&#34;</span>)<span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/red-x.svg"/></p>
</div>
<h3 id="generating-type-annotations">Generating Type Annotations</h3>
<p>Models have a significantly harder time writing type annotations for
Python code than they do just reasoning about the types, or reading type
annotations. This makes sense, since a lot of the Python code available
in training data doesn’t use type annotations.</p>
<div>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p><p>
Pythia 6.9b
</p>
</div>
<div id="cb14"><pre><code><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span>def</span> program_48() <span>-&gt;</span> <span>None</span>:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    number: Optional[<span>int</span>] <span>=</span> <span>None</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    square <span>=</span> get_square(number)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span>if</span> square <span>is</span> <span>not</span> <span>None</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span>print</span>(<span>f&#34;Square of the number is </span><span>{</span>square<span>}</span><span>&#34;</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span>else</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span>print</span>(<span>&#34;No number provided to square&#34;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span>def</span> get_square(number: <span>===</span><span>int</span>) <span>-&gt;</span> Optional[<span>int</span>]: <span>===</span></span></code></pre></div>
<p><img src="https://dmodel.ai/nullability-gentle/images/red-x.svg"/></p>
</div>
<h3 id="some-model-sizes-are-more-useful-than-others">Some Model Sizes
are More Useful than Others</h3>
<p>Notwithstanding the above limitations, three Pythia sizes have a
pretty reliable concept of nullability (2.8b, 6.9b, and 12b), and three
more have an occasionally useful concept of nullbability (410m, 1b, and
1.4b).</p>
<hr/>
<p>For these experiments, and for our probing results later, we mostly
tested on the Pythia model suite. This is a nice series of models from
EleutherAI with a variety of sizes. But what really makes Pythia useful
is that they publish 154 different “revisions” or “checkpoints” of each
model, where each is pretrained for a different number of steps. This
lets us investigate how concepts evolve in the model during
pre-training.</p>
<p>To help understand how these results apply to larger more capable
models, here’s a graph showing how the Pythia models compare to a few
leading models, on a set of 15 completion tasks similar to the above
examples.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/hl_model_results.svg" alt="A bar graph showing how several sizes of model perform on the high-level nullability tests"/><br/>
</p>

<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. Next, we’re going to figure that out.</p>
<p>First, we’ll devise a method for getting the model to “think” about
the nullability, as well as putting it in situations that are similar,
but where nullability isn’t present. Then, we’ll talk a bit about how we
extract the internal activations of the model at this point. Finally,
we’ll show a few different methods for searching for the representation
of nullability in these internal activations, and figure out the pros
and cons of each.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/brain-jar.png" alt="A brain in a jar hooked up to wires"/><br/>
</p>
<h2 id="getting-the-model-to-think-about-nullability">Getting the Model
to Think About Nullability</h2>
<p>The first thing we need to do is to create a state where we know the
representation we’re searching for is going to be present. In theory,
the model should have a map of the variable names and their nullability
at all times when it is writing code, but it’s going to be a lot more
difficult to measure something that is always present. So instead, we’ll
want to look for particular “moments” (well, tokens) that elicit the
concept we’re looking for.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/llm-token-processing.svg" alt="A diagram showing an LLM processing tokens"/><br/>
</p>
<p>Previous work on natural language <span data-cites="zou25">(Zou et al. 2025)</span>, did this through prompting
the concept explicitly. So, they would give the model a prompt like
“Pretend you’re a dishonest person. Tell me about the Eiffel Tower”.
That moment-in-thought can then be contrasted with the one evoked by
“Pretend you’re an honest person. Tell me about the Eiffel Tower”.</p>
<p>This fixed framework of <invoke the="" concept=""><tell me="" about="" x=""> can be
used to generate a large number of contrasting prompts to test with, but
it’s a bit inflexible for our purposes. Instead, we wanted to be able to
generate a bunch of Python code with type annotations, and then
automatically label points where the model should be thinking about
nullability.</tell></invoke></p>
<p>Because we’re working with a formal system with types, we can do
that. We label each variable “load” (places where the program reads a
variable, as opposed to places where it writes a variable) with
“nullable” or “non-nullable”, and then probe the model when it has just
processed that token and is about to predict the next one. So, one of
our prompts could look like:</p>
<div>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-brain-blue.png"/></p>
<div id="cb15"><pre><code><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a> <span>def</span> main(x: <span>int</span>) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>      <span>if</span> x <span>&gt;</span> <span>0</span>:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>          value <span>=</span> <span>&#34;*&#34;</span> <span>*</span> x</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>      <span>else</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>          value <span>=</span> <span>None</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>      x <span>=</span> process_value(value<span>===</span> <span>===</span></span></code></pre></div>
</div>
<p>Using this technique, we can generate large numbers of programs in an
unsupervised manner, and then label them fully automatically to get many
prompts for training our probe.</p>
<h2 id="capturing-the-models-thoughts">Capturing the Model’s
“Thoughts”</h2>
<p>Now that we’ve gotten the model into a state where it should be
thinking about nullability, we need to extract its full state at that
moment in a way we can analyze later.</p>
<p>Large Language Models use the transformer architecture, a type of
neural network. Every component takes in some numerical values, and
produces some new values in a way that depends on learnable weights. We
could take every output of every component, put it in a big table, and
call that our state, but that’s a really big set of numbers. Instead,
usually we look for particular bottlenecks in the model where
information is flowing, and try to capture the values there.</p>
<p>We used the <a href="https://github.com/vgel/repeng">repeng</a>
library to extract states from the models we’re testing. That library
captures the contents of a part of the model called the “residual
stream” after every layer. But if you don’t want to sweat the details,
you can just think of it as a numerical snapshot of the model, organized
in terms of snapshots of each layer.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/llm-residual-stream.svg" alt="A diagram of the residual stream of a transformer being measured after each layer"/><br/>
</p>
<h2 id="analyzing-the-data-and-building-the-probe">Analyzing the Data
and Building the Probe</h2>
<p>Now that we have these model snapshots, labeled with either
“nullable” or “non-nullable”, we can start to build a probe. The goal of
the probe is to be able to tell us, at any given point, whether the
model thinks the token it just generated is more likely to be a nullable
variable or a non-nullable variable.</p>
<p>There’s a lot of flexibility in what form this probe could take. In
theory, you could use anything to look at the model’s activations and
make a prediction, even a neural network, or another transformer model.
You could even say your “probe” is a static analysis which computes
nullability from the program’s syntax, as represented in the model!</p>
<p>We want to make sure we’re not doing that, and are only extracting
the most “plain” represenation of nullability that we can from the
model. So we’re going to make the assumption that nullability is
represented “linearly” somewhere in the model<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>.
Algebraically: the model represents the amount of “nullability” at a
given token as a linear function of (some subset of) the activations,
where each activation is given a weight and summed, like so:</p>
<p><span>\text{Nullability}(\hat{x}) = C + w_0x_0 +
w_1x_1 + w_2x_2 + ...</span></p>
<p>Next, geometrically: if the model activations form a “space”, then
there we want to look for a “direction” in this space which represents
nullability.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><img src="https://dmodel.ai/nullability-gentle/images/nullability-direction.png" alt="A diagram showing nullability represented as a direction in space"/><br/>
</p>
<p>There are different ways we can compute a “direction” of nullability.
The simplest is just to measure the difference between the average state
when the model is thinking about nullable variables, and the average
state when it’s thinking about non-nullable variables. This gives us a
“direction” pointing from non-nullable to nullable in our space, which
we can use to project any new state onto, to determine how “nullable” it
is.</p>
<p>This technique is called “mass means shift”, because we’re taking the
difference between the means (average values) of each “mass” of points.
You can think of it as drawing a line from the center of the
“non-nullable” cluster to the center of the “nullable” cluster.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/mass-means.svg" alt="A diagram showing two blobs of points, with a line connecting their centers"/><br/>
</p>
<p>It might be surprising that this works, given that we know there are
better ways to fit linear functions, like logistic regression. And in
fact, we can easily see scenarios where this returns a direction that
doesn’t split the training data as well as possible.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/lr-vs-mm-intuition.svg" alt="A diagram showing the difference between a mass means and linear regression classification"/><br/>
</p>
<p>However, the method that splits the training data best doesn’t always
generalize best to splitting the test data well. And it turns out that
in high-dimensions, at least within a single layer, mass means
generalizes better than logistic regression.</p>
<p>This isn’t always the case <em>across</em> layers, though. In
practice, we found that some of the layers in the model are better at
representing nullability than others, and that there are some
dependencies between layers that change the best direction on each
layer. This makes sense, because the number of layers is relatively
small with respect to the dimension of the residual stream, and so we
have fewer dimensions to overfit. So, instead of using mass-means
probing across all layers simultaneously, we do it for each individual
layer. Then, we weight the contribution of individual layers to the
final prediction using linear regression. We found this gave us better
results for larger models, though for smaller models the simpler mass
means approach worked better.</p>
<h2 id="visualizing-our-results">Visualizing Our Results</h2>
<p>Now that we’ve built our probe, we can use it to visualize how the
model “thinks” about nullability as it processes a program. Remember
that reading diagram from earlier? Let’s look at it again and explain
what it shows:</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/reading_diagram.svg" alt="A diagram showing a simple program, and the probe’s nullability predictions for each variable load."/><br/>
</p>
<p>In this diagram, we’re showing a simple Python program with type
annotations. Whenever a variable is read in the code (what we call a
“variable load”), we’ve highlighted it in either green or red. Green
means our probe detected that the model thinks this variable is
nullable, while red means the model thinks it is not nullable.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>The most interesting case is the variable <code>result</code>. When
it first appears in the <code>if</code> statement, it’s highlighted in
green because it comes from <code>find_value</code>, which returns an
<code>Optional[int]</code>. But when it appears again in the
<code>print</code> statement inside the <code>if</code> block, it’s
highlighted in red! This shows that the model understands that inside
the <code>if result</code> block, <code>result</code> can’t be
<code>None</code> anymore.</p>
<h2 id="how-does-understanding-develop-during-training">How Does
Understanding Develop During Training?</h2>
<p>One of the most interesting things we found is how the model’s
understanding of nullability develops over time during training. Using
the checkpoints in the Pythia model suite, we can track how our probe’s
performance improves as the model is pretrained for longer.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/accuracy_during_pretraining.svg" alt="The performance of each Pythia model size during pretraining"/><br/>
</p>
<p>This graph shows the probe’s test loss over training steps for models
of different sizes. Lower means better, so we can see that all models
generally get better at understanding nullability as they train longer,
and larger models learn faster and reach better performance overall.</p>
<p>Interestingly, for models up to 1 billion parameters, the loss
actually starts to increase again after reaching a minimum. This might
be because as training continues, the model develops more complex,
non-linear representations that our simple linear probe can’t capture as
well. Or it might be that the model starts to overfit on the training
data and loses its more general concept of nullability.</p>
<p><img src="https://dmodel.ai/nullability-gentle/images/robot-thinking.png" alt="A robot thinking deeply about code"/><br/>
</p>
<hr/>
<h2 id="whats-next">What’s Next?</h2>
<p>This is just a first step in understanding the internal thought
processes of LLMs as they think about code. There are still richer
types, program invariants, and all sorts of high-level concepts that are
necessary for writing working code, but extracting them from LLMs might
not be so easy.</p>
<p>But we’ve already shown several important things about looking into
the “mind” of a model as it writes code. We can say definitively that
LLMs have an internal concept of nullability, even if they aren’t always
able to do the neccessary program analysis to decide if variables are
nullable.</p>
<p>As these models continue to improve, and as we scale to larger
models, it will be interesting to see how their understanding of
programming concepts evolves. And we’ll be here to study them as they
do.</p>

<p>We thank Leo Gao, Chelsea Voss, and Zhanna Kaufman for their comments
and suggestions during the drafting process of the technical writeup of
this work.</p>

<div id="refs" data-entry-spacing="0" role="list">
<p>
Zou, Andy, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
Ren, Alexander Pan, et al. 2025. <span>“Representation
<span>Engineering</span>: <span>A Top-Down Approach</span> to <span>AI
Transparency</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2310.01405">https://doi.org/10.48550/arXiv.2310.01405</a>.
</p>
</div>



</div>
  </body>
</html>
