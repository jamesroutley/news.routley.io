<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/b4rtaz/distributed-llama/discussions/255">Original</a>
    <h1>Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5</h1>
    
    <div id="readability-page-1" class="page"><div data-snippet-clipboard-copy-content="[ğŸ”€ TP-Link LS1008G Switch]
      | | | |
      | | | |_______ ğŸ”¸ raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ ğŸ”¹ raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ ğŸ”¹ raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ ğŸ”¹ raspberrypi4 (WORKER 3) 10.0.0.4"><pre><code>[ğŸ”€ TP-Link LS1008G Switch]
      | | | |
      | | | |_______ ğŸ”¸ raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ ğŸ”¹ raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ ğŸ”¹ raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ ğŸ”¹ raspberrypi4 (WORKER 3) 10.0.0.4
</code></pre></div><div data-snippet-clipboard-copy-content="b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt &#34;&lt;|im_start|&gt;user
Please explain me where is Poland as I have 1 year&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
&#34; --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
ğŸ“„ AddBos: 0
ğŸ“„ BosId: 151643 (&lt;|endoftext|&gt;)
ğŸ“„ EosId: 151645 (&lt;|im_end|&gt;) 
ğŸ“„ RegularVocabSize: 151643
ğŸ“„ SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
ğŸ’¡ Arch: Qwen3 MoE
ğŸ’¡ HiddenAct: Silu
ğŸ’¡ Dim: 2048
ğŸ’¡ HeadDim: 128
ğŸ’¡ QDim: 4096
ğŸ’¡ KvDim: 512
ğŸ’¡ HiddenDim: 6144
ğŸ’¡ VocabSize: 151936
ğŸ’¡ nLayers: 48
ğŸ’¡ nHeads: 32
ğŸ’¡ nKvHeads: 4
ğŸ’¡ OrigSeqLen: 262144
ğŸ’¡ nExperts: 128
ğŸ’¡ nActiveExperts: 8
ğŸ’¡ MoeHiddenDim: 768
ğŸ’¡ SeqLen: 4096
ğŸ’¡ NormEpsilon: 0.000001
ğŸ’¡ RopeType: Falcon
ğŸ’¡ RopeTheta: 10000000
ğŸ“€ RequiredMemory: 5513 MB
â­• Socket[0]: connecting to 10.0.0.1:9999 worker
â­• Socket[0]: connected
â­• Socket[1]: connecting to 10.0.0.3:9999 worker
â­• Socket[1]: connected
â­• Socket[2]: connecting to 10.0.0.4:9999 worker
â­• Socket[2]: connected
â­• Network is initialized
ğŸ§  CPU: neon dotprod fp16
ğŸ’¿ Loading weights...
ğŸ’¿ Weights loaded
ğŸš Network is in non-blocking mode
&lt;|im_start|&gt;user
Please explain me where is Poland as I have 1 year&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant

ğŸ”·ï¸ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
ğŸ”¶ Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
ğŸ”¶ Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
ğŸ”¶ Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
ğŸ”¶ Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
ğŸ”¶ Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
ğŸ”¶ Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
ğŸ”¶ Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
ğŸ”¶ Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
ğŸ”¶ Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
ğŸ”¶ Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
ğŸ”¶ Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  â€“
ğŸ”¶ Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
ğŸ”¶ Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
ğŸ”¶ Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
ğŸ”¶ Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

ğŸ”¶ Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
â­• Network is closed"><pre><code>b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt &#34;&lt;|im_start|&gt;user
Please explain me where is Poland as I have 1 year&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
&#34; --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
ğŸ“„ AddBos: 0
ğŸ“„ BosId: 151643 (&lt;|endoftext|&gt;)
ğŸ“„ EosId: 151645 (&lt;|im_end|&gt;) 
ğŸ“„ RegularVocabSize: 151643
ğŸ“„ SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
ğŸ’¡ Arch: Qwen3 MoE
ğŸ’¡ HiddenAct: Silu
ğŸ’¡ Dim: 2048
ğŸ’¡ HeadDim: 128
ğŸ’¡ QDim: 4096
ğŸ’¡ KvDim: 512
ğŸ’¡ HiddenDim: 6144
ğŸ’¡ VocabSize: 151936
ğŸ’¡ nLayers: 48
ğŸ’¡ nHeads: 32
ğŸ’¡ nKvHeads: 4
ğŸ’¡ OrigSeqLen: 262144
ğŸ’¡ nExperts: 128
ğŸ’¡ nActiveExperts: 8
ğŸ’¡ MoeHiddenDim: 768
ğŸ’¡ SeqLen: 4096
ğŸ’¡ NormEpsilon: 0.000001
ğŸ’¡ RopeType: Falcon
ğŸ’¡ RopeTheta: 10000000
ğŸ“€ RequiredMemory: 5513 MB
â­• Socket[0]: connecting to 10.0.0.1:9999 worker
â­• Socket[0]: connected
â­• Socket[1]: connecting to 10.0.0.3:9999 worker
â­• Socket[1]: connected
â­• Socket[2]: connecting to 10.0.0.4:9999 worker
â­• Socket[2]: connected
â­• Network is initialized
ğŸ§  CPU: neon dotprod fp16
ğŸ’¿ Loading weights...
ğŸ’¿ Weights loaded
ğŸš Network is in non-blocking mode
&lt;|im_start|&gt;user
Please explain me where is Poland as I have 1 year&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant

ğŸ”·ï¸ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
ğŸ”¶ Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
ğŸ”¶ Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
ğŸ”¶ Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
ğŸ”¶ Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
ğŸ”¶ Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
ğŸ”¶ Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
ğŸ”¶ Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
ğŸ”¶ Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
ğŸ”¶ Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
ğŸ”¶ Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
ğŸ”¶ Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  â€“
ğŸ”¶ Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
ğŸ”¶ Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
ğŸ”¶ Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
ğŸ”¶ Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

ğŸ”¶ Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
â­• Network is closed
</code></pre></div></div>
  </body>
</html>
