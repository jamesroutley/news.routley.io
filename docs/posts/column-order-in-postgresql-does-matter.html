<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cybertec-postgresql.com/en/column-order-in-postgresql-does-matter/">Original</a>
    <h1>Column order in PostgreSQL does matter</h1>
    
    <div id="readability-page-1" class="page"><div id="page"><div id="content"><div id="primary"><main id="main" role="main"><div><div><article id="post-40095"><div><header></header><p>I’ve recently seen some really broad tables (hundreds of columns) in a somewhat inefficiently structured database. Our PostgreSQL support customer complained about strange runtime behavior which could not be easily explained. To help other PostgreSQL users in this same situation, I decided to reveal the secrets of a fairly common performance problem many people don’t understand: Column order and column access.</p><h2>Creating a large table</h2><p>The first question is: How can we create a table containing many columns? The easiest way is to simply generate the <code>CREATE TABLE</code> statement using generate_series:</p><pre title="">test=# SELECT &#39;CREATE TABLE t_broad (&#39;

|| string_agg(&#39;t_&#39; || x

|| &#39; varchar(10) DEFAULT &#39;&#39;a&#39;&#39; &#39;, &#39;, &#39;)

|| &#39; )&#39;

FROM generate_series(1, 4) AS x;

?column?       ----------------------------------------------------------

CREATE TABLE t_broad (

t_1 varchar(10) DEFAULT &#39;a&#39; ,

t_2 varchar(10) DEFAULT &#39;a&#39; ,

t_3 varchar(10) DEFAULT &#39;a&#39; , t_4 v

archar(10) DEFAULT &#39;a&#39;  )

(1 row)

test=# \gexec

CREATE TABLE
</pre><p>For the sake of simplicity I have only used 4 columns here. Once the command has been generated we can use \gexec to execute the string we have just compiled. <a href="https://www.postgresql.org/docs/current/app-psql.html" target="_blank" rel="noopener">\gexec</a> is a really powerful thing: It treats the previous result as SQL input which is exactly what we want here. It leaves us with a table containing 4 columns.</p><p>However, let’s drop the table and create a really large one.</p><pre title="">

test=# DROP TABLE t_broad ;

DROP TABLE

</pre><h2>Create an extremely wide table</h2><p>The following statement creates a table containing 1500 columns. Mind that the upper limit is 1600 columns:</p><pre title="">test=# SELECT &#39;CREATE TABLE t_broad (&#39;

|| string_agg(&#39;t_&#39; || x

|| &#39; varchar(10) DEFAULT &#39;&#39;a&#39;&#39; &#39;, &#39;, &#39;) || &#39; )&#39;

FROM generate_series(1, 1500) AS x;
</pre><p>In real life such a table is far from efficient and should usually not be used to store data. It will simply create too much overhead and in most cases it is not good modelling in the first place.</p><p>Let’s populate the table and add 1 million rows:</p><pre title="">

test=# \timing

Timing is on.

test=# INSERT INTO t_broad

SELECT &#39;a&#39; FROM generate_series(1, 1000000);

INSERT 0 1000000

Time: 67457,107 ms (01:07,457)

test=# VACUUM ANALYZE ;

VACUUM

Time: 155935,761 ms (02:35,936)

</pre><p>Note that the table has default values so we can be sure that those columns actually contain something. Finally I have executed <a href="https://www.cybertec-postgresql.com/en/speeding-up-things-with-hint-bits/" target="_blank" rel="noopener"><code>VACUUM</code> to make sure that all hint bits</a> and alike are set.</p><p>The table we have just created is roughly 4 GB in size which can easily be determined using the following line:</p><pre title="">

test=# SELECT pg_size_pretty(pg_total_relation_size(&#39;t_broad&#39;));

pg_size_pretty

----------------

3907 MB

(1 row)

</pre><h2>Accessing various columns</h2><p>PostgreSQL stores data in rows. As you might know data can be stored column- or row-oriented. Depending on your use case one or the other option might be beneficial. In the case of OLTP a row-based approach is usually far more efficient.</p><p>Let’s do a <code>count(*)</code> and see how long it takes:</p><pre title="">

test=# SELECT count(*) FROM t_broad;

count

---------

1000000

(1 row)



Time: 416,732 ms

</pre><p>We can run the query in around 400 ms which is quite ok. As expected, the optimizer will go for a parallel sequential scan:</p><pre title="">

test=# explain SELECT count(*) FROM t_broad;

QUERY PLAN

--------------------------------------------------------------------

Finalize Aggregate (cost=506208.55..506208.56 rows=1 width=8)

-&gt; Gather (cost=506208.33..506208.54 rows=2 width=8)

Workers Planned: 2

-&gt; Partial Aggregate

(cost=505208.33..505208.34 rows=1 width=8)

-&gt; Parallel Seq Scan on t_broad

(cost=0.00..504166.67 rows=416667 width=0)

JIT:

Functions: 4

Options: Inlining true, Optimization true, Expressions true,

Deforming true

(8 rows)

</pre><p>Let’s compare this to a count on the first column. You’ll see a small difference in performance. The reason is that <code>count(*)</code> has to check for the existence of the row while <code>count(column)</code> has to check if a <code>NULL </code>value is fed to the aggregate or not. In case of <code>NULL</code> the value has to be ignored:</p><pre title="">test=# SELECT count(t_1) FROM t_broad;

count

---------

1000000

(1 row)



Time: 432,803 ms

</pre><p>But, let’s see what happens if we access column number 100? The time to do that will differ significantly:</p><pre title="">test=# SELECT count(t_100) FROM t_broad;

count

---------

1000000

(1 row)



Time: 857,897 ms

</pre><p>The execution time has basically doubled. The performance is even worse if we do a count on column number 1000:</p><pre title="">

test=# SELECT count(t_1000) FROM t_broad;

count

---------

1000000

(1 row)



Time: 8570,238 ms (00:08,570)

</pre><p>Wow, we are already 20 times slower than before. This is not a small difference but a major problem which has to be understood.</p><h2>Debunking PostgreSQL performance issues: column order</h2><p>To understand why the problem happens in the first place we need to take a look at how PostgreSQL stores data: After the tuple header which is present in every row we got a couple of <code>varchar</code> columns. We just used <code>varchar</code> here to prove the point. The same issues will happen with other data types – the problem is simply more apparent with <code>varchar</code> as it is more complicated internally than, say, <code>integer</code>.</p><p>How does PostgreSQL access a column? It will fetch the row and then dissect this tuple to calculate the position of the desired column inside the row. So if we want to access column #1000 it means that we have to figure out how long those first 999 columns before our chosen one really are. This can be quite complex. For <code>integer</code> we simply have to add 4, but in case of <code>varchar</code>, the operation turns into something really expensive. Let’s inspect how PostgreSQL stores <code>varchar</code> (just to see why it is so expensive):</p><ul><li>1 bit indicating short (127 bytes) vs. long string (&gt; 127 bit)</li><li>7 bit or 31 bit length (depending on first bit)</li><li>“data” + \0 (to terminate the string )</li><li>alignment (to make sure the next column starts at a multiple of CPU-word length)</li></ul><p>Now imagine what that means if we need to loop over 1000 columns? It does create some non-trivial overhead.</p><h2>Finally …</h2><p>The key insight here is that using extremely large tables is often not beneficial from a performance standpoint. It makes sense to use sensible table layouts to have a good compromise between performance and convenience.</p><p>If you are interested in other ways to improve performance, read <a href="https://www.cybertec-postgresql.com/en/cluster-improving-postgresql-performance/" target="_blank" rel="noopener">my blog on CLUSTER</a>.</p></div></article></div></div></main></div></div></div></div>
  </body>
</html>
