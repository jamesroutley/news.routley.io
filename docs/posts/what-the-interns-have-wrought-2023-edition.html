<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.janestreet.com/what-the-interns-have-wrought-2023/">Original</a>
    <h1>What the interns have wrought, 2023 edition</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>We’re once again at the end of our internship season, and it’s my task
to provide a few highlights of what the dev interns accomplished while
they were here.</p>

<p>The program was big! We had 152 software engineering interns, drawn
from 58 schools across 19 different countries.  And that’s not even
counting the 31 tech interns in areas like production engineering, IT
engineering, network engineering, and more.</p>

<p>The intern program is so big and diverse that it’s impossible to
faithfully summarize it with just a few projects. But, something is
better than nothing, so here are the projects I’ll discuss this time
around:</p>

<ul>
  <li>
    <p>Rajeev Godse wrote a query language based on Linear Temporal Logic
for querying complex events out of system logs.</p>
  </li>
  <li>
    <p>Semyon Savkin worked on our AI Assistants team, building an
efficient tokenizer in OCaml.</p>
  </li>
  <li>
    <p>Sasha Hydrie added concurrency support to our tracing syntax, making
it suitable for use with our asynchronous programming frameworks.</p>
  </li>
</ul>

<p>Now let’s dive into the details!</p>



<p>Concord (which we’ve
<a href="https://www.janestreet.com/tech-talks/building-an-exchange/">talked</a>
<a href="https://signalsandthreads.com/state-machine-replication-and-why-you-should-care/">about</a>
before) is a platform for building systems that let counterparties
connect to us to trade directly with or through us.</p>

<p>Concord’s architecture is built around a single, totally-ordered
stream of transactions.  One nice thing about this approach is that
the transaction stream is a great debugging aid: an incredibly
detailed source-of-truth that you can dive into when you’re trying to
figure out why the system is misbehaving.</p>

<p>Unfortunately, the tooling we had for digging into this treasure trove
was a bit limited.  All we really had was the ability to find and grab
individual messages.  But sometimes you want more than that! You want
to search for sequences of events that match some specified criteria.</p>

<p>We did have <em>some</em> tooling for this. In particular, one of our
engineers had built a stream query system based on <a href="https://en.wikipedia.org/wiki/Linear_temporal_logic">linear temporal
logic</a>, or, LTL
for short.  LTL is a well-studied logic that takes basic propositional
logic and adds to it two key operators: <code>next</code> and <code>until</code>.</p>

<p>Roughly, <code>p next q</code> means that predicate <code>p</code> holds currently, and that
predicate <code>q</code> holds thereafter.  And <code>p until q</code> means that <code>p</code> holds
currently, and will continue to hold until <code>q</code> starts holding.</p>

<p>If it’s not obvious to you how to use these two operators to build
meaningful queries, well, join the club.  It can be a bit of a puzzle
to figure out how to convert meaningful queries into the fairly
low-level logical statements that LTL is built on.  To make matters
worse, the only syntax we had for writing these queries was a fairly
awkward s-expression based format.  As a result, almost no one used
the LTL query engine.</p>

<p>That’s where Rajeev’s project came in. Rajeev’s goal was to build an
easier-to-use, SQL-like query language to act as a frontend to the
LTL query engine.  The language wouldn’t be quite as expressive as
LTL, but it would be a lot easier to use.</p>

<p>We don’t really have space to go into detail on how the language
works, but here’s an example of a query for retrieving and printing
out retail orders paired with the first execution received by each
order:</p>

<div><pre><code>FIND wholesaling.retail_order.new Retail_order
THEN FIRST wholesaling.route.fill.external Fill
  WHERE .retail_order_id = Retail_order.retail_order_id;
PRINT
, Fill.time - Retail_order.time AS order_to_fill_time
, Retail_order.retail_order_id AS retail_order_id
, Retail_order.time AS arrival_time
;
</code></pre>
</div>

<p>He built the parser for this new language on top of
<a href="https://github.com/inhabitedtype/angstrom">Angstrom</a>, a
parser-combinator library for OCaml.  It wasn’t too hard to get a
working parser; the biggest challenge was getting good error
messages.  But after some careful wrestling with the system, Rajeev
was able to get it to track enough context to generate good error
messages in the cases that mattered.</p>

<p>In addition to getting the basic system in place, Rajeev had time to
add a few interesting temporal operators to the language, including:</p>

<ul>
  <li>
    <p><code>LAST p BEFORE q</code>, which matches messages <code>M1</code> and <code>M2</code> such that
<code>M2</code> satisfies <code>q</code> and <code>M1</code> is the last message satisfying <code>p</code>
before <code>M2</code>.</p>
  </li>
  <li>
    <p><code>NO MESSAGE p BEFORE q</code>, which matches <code>M</code> satisfying <code>q</code> such that
no messages before <code>M</code> satisfy <code>p</code>.</p>
  </li>
</ul>

<p>All in, the project was a real success. The new temporal query
language has become the go-to tool on the team for debugging
performance problems, and there have been requests from other teams to
generalize the language so it can be used against other systems as
well. This feels like an exciting new part of our toolkit for
supporting production systems.</p>



<p>If you’ve ever used an AI chatbot you’ll appreciate the importance of
keeping track of your token usage—both as a way to keep costs in
order and to mind rate limits. Surfacing these token counts in real
time to users helps them understand and moderate their own usage.</p>

<p>The project we needed tokenization for is our own web front-end to the
various AI chatbots out there.  We started off using OpenAI’s
tokenization library, <a href="https://github.com/openai/tiktoken">tiktoken</a>, which we set up by
starting a Python server that we could hit over HTTP.</p>

<p>But, this was a bit of a grungy setup, and we only had access to the
token counter from the server, not the client. A pure OCaml
implementation would solve both problems at once, since our client is
an OCaml program too.</p>

<p>Semyon Savkin’s intern project was to write such an
implementation. Token counting is not trivial—it’s not like you just
split your input string on spaces—and an early challenge was finding
an OCaml regex library that supported all the features used by the
regex in tiktoken. Nothing that we found was suitable, especially
given the constraint that it had to work in the browser. Fortunately,
the regex was simple enough that it was not too difficult for Semyon
to handcraft the code for the automaton.</p>

<p>The goal at first was to check that the program’s behavior conformed
100% with the reference implementation, so Semyon wrote a stress test
program to spot any differences.  But it soon became clear that this
was too strict of a requirement, since even a slight difference in the
unicode version can cause (very rare) tokenization differences. So
Semyon needed to find a way to relax the tests enough to allow for
small deviations, without losing too much bug-finding power.</p>

<p>Our initial implementation used a very functional style, with lists
and maps. The code was nice and simple, but just not fast enough. So,
Semyon spent some time profiling and experimenting, and ended up with
a more imperative implementation leveraging hash-tables and arrays,
which, along with algorithmic improvements, made a big difference.</p>

<p>By the end of the internship, Semyon had produced two fully
functioning tokenizers. We compared the results against both the
Python server and also the reference implementations as accessed
through the Python API, which were written in Rust. When measured in
bytes per microsecond, we blew the Python server out of the water for
short messages, due to network latency. But even doing an
apples-to-apples comparison with the Rust implementations, we found
that our implementation was marginally faster on average for OpenAI
tokenization, and a bit less than twice as fast on average for
Anthropic tokenization:</p>

<p><img alt="Tokenizer performance" src="https://blog.janestreet.com/what-the-interns-have-wrought-2023/tokenizer-perf.png"/>
</p>

<p>One thing to note about the above graph is that, despite being faster,
our variance was worse, which is probably due to GC pauses.  This
could probably be brought down by being more careful about allocation,
but the variance just wasn’t a problem for this application.</p>

<p>We didn’t really expect to beat the performance of OpenAI’s
implementation, so that was a pleasant surprise!</p>



<p><code>ppx_tracing</code> is an OCaml syntax extension that provides
high-performance introspection capabilities for OCaml programs. To use
it, all you have to do is add a small <code>@trace</code> annotation to an
existing function:</p>

<div><pre><code><span>let</span><span>[@</span><span>trace</span> <span>&#34;demo&#34;</span> <span>&#34;compute&#34;</span> <span>~</span><span>n</span><span>:(</span><span>n</span> <span>:</span> <span>int</span><span>)]</span> <span>compute</span> <span>n</span> <span>=</span> <span>(* ... *)</span>
</code></pre>
</div>

<p>Then, you just have to call <code>Tracing_probes.start</code> somewhere in your
executable, and at runtime you’ll get a UI for viewing traces, built
on top of Perfetto:</p>

<p><img alt="ppx_tracing UI" src="https://blog.janestreet.com/what-the-interns-have-wrought-2023/perfetto.png"/>
</p>

<p>When we released ppx_tracing internally, there was one main issue that
kept people from really wanting to use it: it didn’t work with
asynchronous code.  In particular, it couldn’t represent suspending
execution in one function and resuming in another.</p>

<p>This was a significant limitation, since most real-world programs do
<em>something</em> asynchronous, say writing to a file or fetching data over
the network.  These operations are provided by the <code>Async</code> library,
which lets us wrap asynchronous computations in “deferreds”—the
OCaml equivalent of a Promise in Javascript.  Let’s consider the
following <code>Async</code> program for checking disk space usage:</p>

<div><pre><code><span>let</span><span>[@</span><span>trace</span> <span>&#34;demo&#34;</span> <span>&#34;process&#34;</span><span>]</span> <span>rec</span> <span>process_directory</span> <span>path</span> <span>=</span>
  <span>let</span><span>%</span><span>bind</span> <span>stat</span> <span>=</span> <span>Filesystem_async</span><span>.</span><span>stat</span> <span>path</span> <span>in</span>
  <span>[%</span><span>trace</span><span>.</span><span>instant</span> <span>&#34;demo&#34;</span> <span>&#34;stat&#34;</span><span>];</span>
  <span>let</span> <span>num_bytes</span> <span>=</span> <span>Int63</span><span>.</span><span>to_int</span> <span>stat</span><span>.</span><span>size</span> <span>|&gt;</span> <span>Option</span><span>.</span><span>value</span> <span>~</span><span>default</span><span>:</span><span>0</span> <span>in</span>
  <span>match</span> <span>stat</span><span>.</span><span>kind</span> <span>with</span>
  <span>|</span> <span>Regular</span> <span>-&gt;</span>
    <span>return</span> <span>num_bytes</span>
  <span>|</span> <span>Directory</span> <span>-&gt;</span>
    <span>let</span><span>%</span><span>bind</span> <span>files</span> <span>=</span> <span>Filesystem_async</span><span>.</span><span>ls_dir</span> <span>path</span> <span>in</span>
    <span>let</span><span>%</span><span>bind</span> <span>entry_sizes</span> <span>=</span>
      <span>Deferred</span><span>.</span><span>List</span><span>.</span><span>map</span> <span>~</span><span>how</span><span>:(</span><span>`</span><span>Max_concurrent_jobs</span> <span>10</span><span>)</span> <span>files</span> <span>~</span><span>f</span><span>:(</span><span>fun</span> <span>file</span> <span>-&gt;</span>
        <span>let</span> <span>res</span> <span>=</span> <span>process_directory</span> <span>(</span><span>path</span> <span>/?.</span> <span>file</span><span>)</span> <span>in</span>
        <span>res</span><span>)</span>
    <span>in</span>
    <span>return</span> <span>(</span><span>List</span><span>.</span><span>fold</span> <span>entry_sizes</span> <span>~</span><span>init</span><span>:</span><span>0</span> <span>~</span><span>f</span><span>:(</span> <span>+</span> <span>))</span>
  <span>|</span> <span>_</span> <span>-&gt;</span> <span>return</span> <span>0</span>
<span>;;</span>
</code></pre>
</div>

<p>Running the program with tracing enabled produces a confusing
result. Here, spans representing calls to <code>process_directory</code> only
capture the time taken to allocate a new deferred.  Further, since the
trace only keeps track of one pending function call, we can’t know
which invocation of <code>process_directory</code> generated each instant event.</p>

<p><img alt="Confusing asynchronous trace" src="https://blog.janestreet.com/what-the-interns-have-wrought-2023/async-1.png"/>
</p>

<p>The crux of Sasha Hydrie’s intern project was to figure out how to
integrate with the Async scheduler to keep track of multiple
concurrent execution contexts.  This required associating a unique ID
with each async function call.</p>

<p>Luckily, the <code>Async</code> scheduler can store metadata along with each
deferred computation.  When switching to a new task, the scheduler
makes its data (known as the “execution context”) globally accessible.
Therefore, upon entering an async function, we can store a unique
“correlation ID” in the context.  This gives us the ability to
distinguish between multiple invocations of <code>process_directory</code>.
Later on, when the task actually executes, we can query the current ID
to see which function we’re in.</p>

<p>Sasha extended the PPX with a new annotation (<code>@trace.async</code>)
implementing this behavior.  The resulting IDs must end up in the
final trace file, so he also updated our Fuchsia trace format tooling
to support the relevant event types.  Now, updating our example with
<code>@trace.async</code> gives a much more sensible result—we can see how long
each call is actually in flight, and how many are executing in
parallel.  Further, instant events are able to query the active
correlation ID to determine which call they are a part of.</p>

<p><img alt="Better asynchronous trace" src="https://blog.janestreet.com/what-the-interns-have-wrought-2023/async-2.png"/>
</p>

<p>However, not all functions are asynchronous. We don’t want every
synchronous function to create a new track, but we also don’t want to
mix them all together.  Therefore, Sasha added a new async mode to the
PPX, where synchronous function calls will also query the current
correlation ID and dynamically attach to a parent async call.</p>

<p>Already Sasha’s work has enabled wider adoption of ppx_tracing; the
tool should now work in most real-world programs. The team behind
<a href="https://blog.janestreet.com/code-review-that-isnt-boring/">Iron</a>, our in-house code review system, used it to improve
their server’s startup time by about 50%, within days of the final
project being released.</p>

<p>Given the success of the project, there is, of course, more work yet
to do: we’re hoping to integrate ppx_tracing with a new distributed
tracing system we’ve built, as well as perform more detailed tracing
of the Async scheduler itself.</p>



<p>Summarizing the whole summer in just a handful of projects is an
impossible task, but I hope this gives you a flavor of the kinds of
things that our interns do.</p>

<p>If this sounds like fun, then you should
<a href="https://www.janestreet.com/join-jane-street/programs-and-events/internships-all-cycles/">apply</a>. The
internship is a great chance to learn, both about software
engineering, and about the world of trading. And if you’d like to know
more about our interview process, take a look
<a href="https://blog.janestreet.com/applying-to-jane-street/">here</a>.</p>


    </div><p>Yaron Minsky joined Jane Street back in 2002, and claims the dubious honor
of having convinced the firm to start using OCaml.
</p></div>
  </body>
</html>
