<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/google/gemma.cpp">Original</a>
    <h1>Gemma.cpp: lightweight, standalone C&#43;&#43; inference engine for Gemma models</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma
foundation models from Google.</p>
<p dir="auto">For additional information about Gemma, see
<a href="https://ai.google.dev/gemma" rel="nofollow">ai.google.dev/gemma</a>. Model weights, including gemma.cpp
specific artifacts, are <a href="https://www.kaggle.com/models/google/gemma" rel="nofollow">available on
kaggle</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-who-is-this-project-for" aria-hidden="true" tabindex="-1" href="#who-is-this-project-for"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Who is this project for?</h2>
<p dir="auto">Modern LLM inference engines are sophisticated systems, often with bespoke
capabilities extending beyond traditional neural network runtimes. With this
comes opportunities for research and innovation through co-design of high level
algorithms and low-level computation. However, there is a gap between
deployment-oriented C++ inference runtimes, which are not designed for
experimentation, and Python-centric ML research frameworks, which abstract away
low-level computation through compilation.</p>
<p dir="auto">gemma.cpp provides a minimalist implementation of Gemma 2B and 7B models,
focusing on simplicity and directness rather than full generality. This is
inspired by vertically-integrated model implementations such as
<a href="https://github.com/ggerganov/ggml">ggml</a>,
<a href="https://github.com/karpathy/llama2.c">llama.c</a>, and
<a href="https://github.com/srush/llama2.rs">llama.rs</a>.</p>
<p dir="auto">gemma.cpp targets experimentation and research use cases. It is intended to be
straightforward to embed in other projects with minimal dependencies and also
easily modifiable with a small ~2K LoC core implementation (along with ~4K LoC
of supporting utilities). We use the <a href="https://github.com/google/highway">Google
Highway</a> Library to take advantage of
portable SIMD for CPU inference.</p>
<p dir="auto">For production-oriented edge deployments we recommend standard deployment
pathways using Python frameworks like JAX, Keras, PyTorch, and Transformers
(<a href="https://www.kaggle.com/models/google/gemma" rel="nofollow">all model variations here</a>).</p>
<p dir="auto">Community contributions large and small are welcome. This project follows
<a href="https://opensource.google.com/conduct/" rel="nofollow">Google&#39;s Open Source Community
Guidelines</a>.</p>
<p dir="auto"><em>Active development is currently done on the <code>dev</code> branch. Please open pull
requests targeting <code>dev</code> branch instead of <code>main</code>, which is intended to be more
stable.</em></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-hidden="true" tabindex="-1" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-system-requirements" aria-hidden="true" tabindex="-1" href="#system-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>System requirements</h3>
<p dir="auto">Before starting, you should have installed:</p>
<ul dir="auto">
<li><a href="https://cmake.org/" rel="nofollow">CMake</a></li>
<li><a href="https://clang.llvm.org/get_started.html" rel="nofollow">Clang C++ compiler</a>, supporting at
least C++17.</li>
<li><code>tar</code> for extracting archives from Kaggle.</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-step-1-obtain-model-weights-and-tokenizer-from-kaggle" aria-hidden="true" tabindex="-1" href="#step-1-obtain-model-weights-and-tokenizer-from-kaggle"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 1: Obtain model weights and tokenizer from Kaggle</h3>
<p dir="auto">Visit <a href="https://www.kaggle.com/models/google/gemma" rel="nofollow">the Gemma model page on
Kaggle</a> and select <code>Model Variations |&gt; Gemma C++</code>. On this tab, the <code>Variation</code> dropdown includes the options below.
Note bfloat16 weights are higher fidelity, while 8-bit switched floating point
weights enable faster inference. In general, we recommend starting with the
<code>-sfp</code> checkpoints.</p>
<p dir="auto">2B instruction-tuned (<code>it</code>) and pre-trained (<code>pt</code>) models:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>2b-it</code></td>
<td>2 billion parameter instruction-tuned model, bfloat16</td>
</tr>
<tr>
<td><code>2b-it-sfp</code></td>
<td>2 billion parameter instruction-tuned model, 8-bit switched floating point</td>
</tr>
<tr>
<td><code>2b-pt</code></td>
<td>2 billion parameter pre-trained model, bfloat16</td>
</tr>
<tr>
<td><code>2b-pt-sfp</code></td>
<td>2 billion parameter pre-trained model, 8-bit switched floating point</td>
</tr>
</tbody>
</table>
<p dir="auto">7B instruction-tuned (<code>it</code>) and pre-trained (<code>pt</code>) models:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>7b-it</code></td>
<td>7 billion parameter instruction-tuned model, bfloat16</td>
</tr>
<tr>
<td><code>7b-it-sfp</code></td>
<td>7 billion parameter instruction-tuned model, 8-bit switched floating point</td>
</tr>
<tr>
<td><code>7b-pt</code></td>
<td>7 billion parameter pre-trained model, bfloat16</td>
</tr>
<tr>
<td><code>7b-pt-sfp</code></td>
<td>7 billion parameter pre-trained model, 8-bit switched floating point</td>
</tr>
</tbody>
</table>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto"><strong>Important</strong>: We strongly recommend starting off with the <code>2b-it-sfp</code> model to
get up and running.</p>
</div>
<h3 tabindex="-1" dir="auto"><a id="user-content-step-2-extract-files" aria-hidden="true" tabindex="-1" href="#step-2-extract-files"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 2: Extract Files</h3>
<p dir="auto">After filling out the consent form, the download should proceed to retrieve a
tar archive file <code>archive.tar.gz</code>. Extract files from <code>archive.tar.gz</code> (this can
take a few minutes):</p>

<p dir="auto">This should produce a file containing model weights such as <code>2b-it-sfp.sbs</code> and
a tokenizer file (<code>tokenizer.spm</code>). You may want to move these files to a
convenient directory location (e.g. the <code>build/</code> directory in this repo).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-step-3-build" aria-hidden="true" tabindex="-1" href="#step-3-build"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 3: Build</h3>
<p dir="auto">The build system uses <a href="https://cmake.org/" rel="nofollow">CMake</a>. To build the gemma inference
runtime, create a build directory and generate the build files using <code>cmake</code>
from the top-level project directory. For the 8-bit switched floating point
weights (sfp), run cmake with no options:</p>

<p dir="auto"><strong>or</strong> if you downloaded bfloat16 weights (any model <em>without</em> <code>-sfp</code> in the name),
instead of running cmake with no options as above, run cmake with WEIGHT_TYPE
set to <a href="https://github.com/google/highway">highway&#39;s</a> <code>hwy::bfloat16_t</code> type
(this will be simplified in the future, we recommend using <code>-sfp</code> weights
instead of bfloat16 for faster inference):</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -B build -DWEIGHT_TYPE=hwy::bfloat16_t"><pre>cmake -B build -DWEIGHT_TYPE=hwy::bfloat16_t</pre></div>
<p dir="auto">After running whichever of the above <code>cmake</code> invocations that is appropriate for
your weights, you can enter the <code>build/</code> directory and run <code>make</code> to build the
<code>./gemma</code> executable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd build
make -j [number of parallel threads to use] gemma"><pre><span>cd</span> build
make -j [number of parallel threads to use] gemma</pre></div>
<p dir="auto">Replace <code>[number of parallel threads to use]</code> with a number - the number of
cores available on your system is a reasonable heuristic.</p>
<p dir="auto">For example, <code>make -j4 gemma</code> will build using 4 threads. If this is successful,
you should now have a <code>gemma</code> executable in the <code>build/</code> directory. If the
<code>nproc</code> command is available, you can use <code>make -j$(nproc) gemma</code> as a
reasonable default for the number of threads.</p>
<p dir="auto">If you aren&#39;t sure of the right value for the <code>-j</code> flag, you can simply run
<code>make gemma</code> instead and it should still build the <code>./gemma</code> executable.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">On Windows Subsystem for Linux (WSL) users should set the number of
parallel threads to 1. Using a larger number may result in errors.</p>
</div>
<h3 tabindex="-1" dir="auto"><a id="user-content-step-4-run" aria-hidden="true" tabindex="-1" href="#step-4-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 4: Run</h3>
<p dir="auto">You can now run <code>gemma</code> from inside the <code>build/</code> directory.</p>
<p dir="auto"><code>gemma</code> has the following required arguments:</p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Description</th>
<th>Example value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--model</code></td>
<td>The model type.</td>
<td><code>2b-it</code>, <code>2b-pt</code>, <code>7b-it</code>, <code>7b-pt</code>, ... (see above)</td>
</tr>
<tr>
<td><code>--compressed_weights</code></td>
<td>The compressed weights file.</td>
<td><code>2b-it-sfp.sbs</code>, ... (see above)</td>
</tr>
<tr>
<td><code>--tokenizer</code></td>
<td>The tokenizer file.</td>
<td><code>tokenizer.spm</code></td>
</tr>
</tbody>
</table>
<p dir="auto"><code>gemma</code> is invoked as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./gemma \
--tokenizer [tokenizer file] \
--compressed_weights [compressed weights file] \
--model [2b-it or 2b-pt or 7b-it or 7b-pt or ...]"><pre>./gemma \
--tokenizer [tokenizer file] \
--compressed_weights [compressed weights file] \
--model [2b-it or 2b-pt or 7b-it or 7b-pt or ...]</pre></div>
<p dir="auto">Example invocation for the following configuration:</p>
<ul dir="auto">
<li>Compressed weights file <code>2b-it-sfp.sbs</code> (2B instruction-tuned model, 8-bit
switched floating point).</li>
<li>Tokenizer file <code>tokenizer.spm</code>.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="./gemma \
--tokenizer tokenizer.spm \
--compressed_weights 2b-it-sfp.sbs \
--model 2b-it"><pre>./gemma \
--tokenizer tokenizer.spm \
--compressed_weights 2b-it-sfp.sbs \
--model 2b-it</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-troubleshooting-and-faqs" aria-hidden="true" tabindex="-1" href="#troubleshooting-and-faqs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Troubleshooting and FAQs</h3>
<p dir="auto"><strong>Running <code>./gemma</code> fails with &#34;Failed to read cache gating_ein_0 (error 294) ...&#34;</strong></p>
<p dir="auto">The most common problem is that <code>cmake</code> was built with the wrong weight type and
<code>gemma</code> is attempting to load <code>bfloat16</code> weights (<code>2b-it</code>, <code>2b-pt</code>, <code>7b-it</code>,
<code>7b-pt</code>) using the default switched floating point (sfp) or vice versa. Revisit
step #3 and check that the <code>cmake</code> command used to build <code>gemma</code> was correct for
the weights that you downloaded.</p>
<p dir="auto">In the future we will handle model format handling from compile time to runtime
to simplify this.</p>
<p dir="auto"><strong>Problems building in Windows / Visual Studio</strong></p>
<p dir="auto">Currently if you&#39;re using Windows, we recommend building in WSL (Windows
Subsystem for Linux). We are exploring options to enable other build
configurations, see issues for active discussion.</p>
<p dir="auto"><strong>Model does not respond to instructions and produces strange output</strong></p>
<p dir="auto">A common issue is that you are using a pre-trained model, which is not
instruction-tuned and thus does not respond to instructions. Make sure you are
using an instruction-tuned model (<code>2b-it-sfp</code>, <code>2b-it</code>, <code>7b-it-sfp</code>, <code>7b-it</code>)
and not a pre-trained model (any model with a <code>-pt</code> suffix).</p>
<p dir="auto"><strong>How do I convert my fine-tune to a <code>.sbs</code> compressed model file?</strong></p>
<p dir="auto">We&#39;re working on a python script to convert a standard model format to <code>.sbs</code>,
and hope have it available in the next week or so. Follow <a href="https://github.com/google/gemma.cpp/issues/11" data-hovercard-type="issue" data-hovercard-url="/google/gemma.cpp/issues/11/hovercard">this
issue</a> for updates.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" tabindex="-1" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto"><code>gemma</code> has different usage modes, controlled by the verbosity flag.</p>
<p dir="auto">All usage modes are currently interactive, triggering text generation upon
newline input.</p>
<table>
<thead>
<tr>
<th>Verbosity</th>
<th>Usage mode</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--verbosity 0</code></td>
<td>Minimal</td>
<td>Only prints generation output. Suitable as a CLI tool.</td>
</tr>
<tr>
<td><code>--verbosity 1</code></td>
<td>Default</td>
<td>Standard user-facing terminal UI.</td>
</tr>
<tr>
<td><code>--verbosity 2</code></td>
<td>Detailed</td>
<td>Shows additional developer and debug info.</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto"><a id="user-content-interactive-terminal-app" aria-hidden="true" tabindex="-1" href="#interactive-terminal-app"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Interactive Terminal App</h3>
<p dir="auto">By default, verbosity is set to 1, bringing up a terminal-based interactive
interface when <code>gemma</code> is invoked:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./gemma [...]
  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __
 / _` |/ _ \ &#39;_ ` _ \| &#39;_ ` _ \ / _` | / __| &#39;_ \| &#39;_ \
| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |
 \__, |\___|_| |_| |_|_| |_| |_|\__,_(_)___| .__/| .__/
  __/ |                                    | |   | |
 |___/                                     |_|   |_|

tokenizer                     : tokenizer.spm
compressed_weights            : 2b-it-sfp.sbs
model                         : 2b-it
weights                       : [no path specified]
max_tokens                    : 3072
max_generated_tokens          : 2048

*Usage*
  Enter an instruction and press enter (%Q quits).

*Examples*
  - Write an email to grandma thanking her for the cookies.
  - What are some historical attractions to visit around Massachusetts?
  - Compute the nth fibonacci number in javascript.
  - Write a standup comedy bit about WebGPU programming.

&gt; What are some outdoorsy places to visit around Boston?

[ Reading prompt ] .....................


**Boston Harbor and Islands:**

* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.
* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.
* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.
* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.

**Forest and Nature:**

* **Forest Park:** Hike through a scenic forest with diverse wildlife.
* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.
* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.

..."><pre>$ <span>./gemma [...]</span>
<span>  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __</span>
<span> / _` |/ _ \ &#39;_ ` _ \| &#39;_ ` _ \ / _` | / __| &#39;_ \| &#39;_ \</span>
<span>| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |</span>
<span> \__, |\___|_| |_| |_|_| |_| |_|\__,_(_)___| .__/| .__/</span>
<span>  __/ |                                    | |   | |</span>
<span> |___/                                     |_|   |_|</span>

<span>tokenizer                     : tokenizer.spm</span>
<span>compressed_weights            : 2b-it-sfp.sbs</span>
<span>model                         : 2b-it</span>
<span>weights                       : [no path specified]</span>
<span>max_tokens                    : 3072</span>
<span>max_generated_tokens          : 2048</span>

<span>*Usage*</span>
<span>  Enter an instruction and press enter (%Q quits).</span>

<span>*Examples*</span>
<span>  - Write an email to grandma thanking her for the cookies.</span>
<span>  - What are some historical attractions to visit around Massachusetts?</span>
<span>  - Compute the nth fibonacci number in javascript.</span>
<span>  - Write a standup comedy bit about WebGPU programming.</span>

&gt; <span>What are some outdoorsy places to visit around Boston<span>?</span></span>

<span>[ Reading prompt ] .....................</span>


<span>**Boston Harbor and Islands:**</span>

<span>* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.</span>
<span>* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.</span>
<span>* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.</span>
<span>* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.</span>

<span>**Forest and Nature:**</span>

<span>* **Forest Park:** Hike through a scenic forest with diverse wildlife.</span>
<span>* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.</span>
<span>* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.</span>

<span>...</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-usage-as-a-command-line-tool" aria-hidden="true" tabindex="-1" href="#usage-as-a-command-line-tool"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage as a Command Line Tool</h3>
<p dir="auto">For using the <code>gemma</code> executable as a command line tool, it may be useful to
create an alias for gemma.cpp with arguments fully specified:</p>
<div dir="auto" data-snippet-clipboard-copy-content="alias gemma2b=&#34;~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --compressed_weights ~/gemma.cpp/build/2b-it-sfp.sbs --model 2b-it --verbosity 0&#34;"><pre><span>alias</span> gemma2b=<span><span>&#34;</span>~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --compressed_weights ~/gemma.cpp/build/2b-it-sfp.sbs --model 2b-it --verbosity 0<span>&#34;</span></span></pre></div>
<p dir="auto">Replace the above paths with your own paths to the model and tokenizer paths
from the download.</p>
<p dir="auto">Here is an example of prompting <code>gemma</code> with a truncated input
file (using a <code>gemma2b</code> alias like defined above):</p>
<div dir="auto" data-snippet-clipboard-copy-content="cat configs.h | tail -35 | tr &#39;\n&#39; &#39; &#39; | xargs -0 echo &#34;What does this C++ code do: &#34; | gemma2b"><pre>cat configs.h <span>|</span> tail -35 <span>|</span> tr <span><span>&#39;</span>\n<span>&#39;</span></span> <span><span>&#39;</span> <span>&#39;</span></span> <span>|</span> xargs -0 <span>echo</span> <span><span>&#34;</span>What does this C++ code do: <span>&#34;</span></span> <span>|</span> gemma2b</pre></div>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">CLI usage of gemma.cpp is experimental and should take context length
limitations into account.</p>
</div>
<p dir="auto">The output of the above command should look like:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cat configs.h | tail -35 | tr &#39;\n&#39; &#39; &#39; | xargs -0 echo &#34;What does this C++ code do: &#34; | gemma2b
[ Reading prompt ] ......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
The code defines two C++ structs, `ConfigGemma7B` and `ConfigGemma2B`, which are used for configuring a deep learning model.

**ConfigGemma7B**:

* `seq_len`: Stores the length of the sequence to be processed. It&#39;s set to 7168.
* `vocab_size`: Stores the size of the vocabulary, which is 256128.
* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 28.
* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 3072.
* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 3072 / 2.

**ConfigGemma2B**:

* `seq_len`: Stores the length of the sequence to be processed. It&#39;s also set to 7168.
* `vocab_size`: Size of the vocabulary, which is 256128.
* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 18.
* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 2048.
* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 2048 / 2.

These structs are used to configure a deep learning model with specific parameters for either Gemma7B or Gemma2B architecture."><pre>$ <span>cat configs.h <span>|</span> tail -35 <span>|</span> tr <span><span>&#39;</span>\n<span>&#39;</span></span> <span><span>&#39;</span> <span>&#39;</span></span> <span>|</span> xargs -0 <span>echo</span> <span><span>&#34;</span>What does this C++ code do: <span>&#34;</span></span> <span>|</span> gemma2b</span>
<span>[ Reading prompt ] ......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................</span>
<span>The code defines two C++ structs, `ConfigGemma7B` and `ConfigGemma2B`, which are used for configuring a deep learning model.</span>

<span>**ConfigGemma7B**:</span>

<span>* `seq_len`: Stores the length of the sequence to be processed. It&#39;s set to 7168.</span>
<span>* `vocab_size`: Stores the size of the vocabulary, which is 256128.</span>
<span>* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 28.</span>
<span>* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 3072.</span>
<span>* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 3072 / 2.</span>

<span>**ConfigGemma2B**:</span>

<span>* `seq_len`: Stores the length of the sequence to be processed. It&#39;s also set to 7168.</span>
<span>* `vocab_size`: Size of the vocabulary, which is 256128.</span>
<span>* `n_layers`: Number of layers in the deep learning model. It&#39;s set to 18.</span>
<span>* `dim_model`: Dimension of the model&#39;s internal representation. It&#39;s set to 2048.</span>
<span>* `dim_ffw_hidden`: Dimension of the feedforward and recurrent layers&#39; hidden representations. It&#39;s set to 16 * 2048 / 2.</span>

<span>These structs are used to configure a deep learning model with specific parameters for either Gemma7B or Gemma2B architecture.</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-incorporating-gemmacpp-as-a-library-in-your-project" aria-hidden="true" tabindex="-1" href="#incorporating-gemmacpp-as-a-library-in-your-project"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Incorporating gemma.cpp as a Library in your Project</h3>
<p dir="auto">The easiest way to incorporate gemma.cpp in your own project is to pull in
gemma.cpp and dependencies using <code>FetchContent</code>. You can add the following to your
CMakeLists.txt:</p>
<div data-snippet-clipboard-copy-content="include(FetchContent)

FetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)
FetchContent_MakeAvailable(sentencepiece)

FetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)
FetchContent_MakeAvailable(gemma)

FetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG da250571a45826b21eebbddc1e50d0c1137dee5f)
FetchContent_MakeAvailable(highway)"><pre><code>include(FetchContent)

FetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)
FetchContent_MakeAvailable(sentencepiece)

FetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)
FetchContent_MakeAvailable(gemma)

FetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG da250571a45826b21eebbddc1e50d0c1137dee5f)
FetchContent_MakeAvailable(highway)
</code></pre></div>
<p dir="auto">Note for the gemma.cpp <code>GIT_TAG</code>, you may replace <code>origin/main</code> for a specific
commit hash if you would like to pin the library version.</p>
<p dir="auto">After your executable is defined (substitute your executable name for
<code>[Executable Name]</code> below):</p>
<div data-snippet-clipboard-copy-content="target_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)
FetchContent_GetProperties(gemma)
FetchContent_GetProperties(sentencepiece)
target_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})
target_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})"><pre><code>target_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)
FetchContent_GetProperties(gemma)
FetchContent_GetProperties(sentencepiece)
target_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})
target_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-building-gemmacpp-as-a-library" aria-hidden="true" tabindex="-1" href="#building-gemmacpp-as-a-library"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Building gemma.cpp as a Library</h3>
<p dir="auto">gemma.cpp can also be used as a library dependency in your own project. The
shared library artifact can be built by modifying the make invocation to build
the <code>libgemma</code> target instead of <code>gemma</code>.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">If you are using gemma.cpp in your own project with the <code>FetchContent</code> steps
in the previous section, building the library is done automatically by <code>cmake</code>
and this section can be skipped.</p>
</div>
<p dir="auto">First, run <code>cmake</code>:</p>

<p dir="auto">Then, run <code>make</code> with the <code>libgemma</code> target:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd build
make -j [number of parallel threads to use] libgemma"><pre><span>cd</span> build
make -j [number of parallel threads to use] libgemma</pre></div>
<p dir="auto">If this is successful, you should now have a <code>libgemma</code> library file in the
<code>build/</code> directory. On Unix platforms, the filename is <code>libgemma.a</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgements-and-contacts" aria-hidden="true" tabindex="-1" href="#acknowledgements-and-contacts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements and Contacts</h2>
<p dir="auto">gemma.cpp was started in fall 2023 by <a href="mailto:austinvhuang@google.com">Austin Huang</a>
and <a href="mailto:janwas@google.com">Jan Wassenberg</a>, and subsequently released February 2024
thanks to contributions from Phil Culliton, Paul Chang, and Dan Zheng.</p>
<p dir="auto">This is not an officially supported Google product.</p>
</article></div></div>
  </body>
</html>
