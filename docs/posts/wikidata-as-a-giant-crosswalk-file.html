<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.dbreunig.com/2024/10/04/wikidata-is-a-giant-crosswalk-file.html">Original</a>
    <h1>Wikidata as a Giant Crosswalk File</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><img src="https://www.dbreunig.com/img/crosswalks.png" alt="The potential for many crosswalks"/></p>

<h3 id="building-a-map-platform-join-table-with-duckdb-and-some-ruby">Building a Map Platform Join Table with DuckDB and Some Ruby</h3>

<p><a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a> is Wikipedia’s structuralist younger brother. It’s contents are seemingly exhaustive, but rather than readable articles, Wikidata expresses itself with <em>structured data</em>. Pick a subject and check out it’s page; it’s like reading the back of a baseball card for, well, anything.</p>

<p>And burried in those stats and metadata are <em>external IDs</em>: identifiers from other sites and systems, which we can use to grab more data and develop cross-platform applications. Wikidata has <em>thousands</em> of ‘em.</p>

<p>Today we’re going to build a cross-walk table for <em>places</em> (a topic <a href="https://www.dbreunig.com/2024/07/31/towards-standardizing-place.html">near and dear to my heart</a>) that you can do with just DuckDB, a short Ruby script, and one hard-earned bash line.</p>

<p>If you want to follow along, <a href="https://www.wikidata.org/wiki/Wikidata:Database_download">grab a recent JSON extract of the Wikidata corpus</a>. But be aware: it’s just shy of 140 GB.</p>

<h3 id="wrangling-the-download">Wrangling the Download</h3>

<p><em>Please</em> do not extact the file you just downloaded. It’s big enough to potentially cause problems on your machine and will definately be unweildy. We need to break it down into chunks, so we can concurrently process it later.</p>

<p>Now Wikidata very helpfully <a href="https://doc.wikimedia.org/Wikibase/master/php/docs_topics_json.html">produces this file so each item is on it’s own line</a>.</p>

<p><em>However</em>, for reasons unknown to me, they wrap these neatly separated rows with brackets (<code>[</code> and <code>]</code>) and add a comma to each line so it’s a valid, JSON array <em>containing 100+ million items</em>. So close, Wikidata. Please check out <a href="https://jsonlines.org">JSON Lines</a>…</p>

<p>We are not going to attempt to load a this massive array. Instead, we’re running this command:</p>

<div><div><pre><code>zcat ../latest-all.json.gz | <span>sed</span> <span>&#39;s/,$//&#39;</span> | <span>split</span> <span>-l</span> 100000 - wd_items_cw <span>--filter</span><span>=</span><span>&#39;gzip &gt; $FILE.gz&#39;</span>
</code></pre></div></div>

<p>This hard-won line streams the uncompressed content into <code>sed</code>, which removes the trailing commas, then chunks the output into batches of 100,000 records which are finally gzipped into files.</p>

<p>Now we can use DuckDB!</p>

<div><div><pre><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>&#39;wd_items_*.jsonl.gz&#39;</span><span>;</span>
</code></pre></div></div>

<p>It takes a bit, but returns a count of ~107 million entities.</p>

<h3 id="exploring-the-wikidata-schema">Exploring the Wikidata Schema</h3>

<p>Let’s look at a record to see what we’re dealing with:</p>

<div><div><pre><code><span>COPY</span> <span>(</span><span>select</span> <span>*</span> <span>from</span> <span>&#39;wd_items_*.jsonl.gz&#39;</span> <span>limit</span> <span>1</span><span>)</span> <span>to</span> <span>&#39;sample_record.json&#39;</span><span>;</span>
</code></pre></div></div>

<p>This is the record for <a href="https://www.wikidata.org/wiki/Q31">Belgium</a> and it’s a big one: 48,252 lines of formatted JSON. Let’s take a high-level tour of the entity structure:</p>

<ul>
  <li>There’s the basic, top-level values like <code>type</code>, <code>id</code>, and a timestamp noting when it was last modified.</li>
  <li>The <code>labels</code> dictionary contains localized strings of the item’s name in many languages (Belgium has 323 labels). This dictionary uses a language code as a key (“en” for English, for example), which maps to a dictionary containing the localized value and a seemingly redundant langauge code. (I can’t figure out why the language code’s there twice! Let me know below, if you do.)</li>
  <li>The <code>description</code> and <code>aliases</code> dictionary are similar in form to the <code>labels</code> dictionary, just with localized descriptions and aliases, respectively.</li>
  <li>The <code>sitelinks</code> dictionary contains links to associated pages on other WikiMedia platforms. For example, <a href="https://en.wikiquote.org/wiki/Belgium">the English-langauge Wikiquote page for Belgium</a>.</li>
  <li>But what we care about today is the <code>claims</code> dictionary…</li>
</ul>

<p>Wikidata claims are a key-value system similar in nature to OpenStreetMaps’ <a href="https://wiki.openstreetmap.org/wiki/Tags">tag system</a>, and likley adopted for similar reasons. It’s a flexible, <a href="https://en.wikipedia.org/wiki/Folksonomy">folksonomic</a> system that facillitates broad collaboration and diverse data elements. Wikidata claims can also join one element to another, and describe the relationship. For example, Belgium is <code>part of</code> the <a href="https://www.wikidata.org/wiki/Q458">European Union</a>. There can also be more than one claim of the same type: Belgiun is also <code>part of</code> the <a href="https://www.wikidata.org/wiki/Q215669">Allies of the First World War</a>, <a href="https://www.wikidata.org/wiki/Q476033">Europe</a>, and the <a href="https://www.wikidata.org/wiki/Q8932">Low Countries</a></p>

<p>The claims we’re after today are those describing an external IDs. Thankfully, they’re labeled clearly for us:</p>

<div><div><pre><code><span>{</span><span>
    </span><span>&#34;mainsnak&#34;</span><span>:</span><span> </span><span>{</span><span>
        </span><span>&#34;snaktype&#34;</span><span>:</span><span> </span><span>&#34;value&#34;</span><span>,</span><span>
        </span><span>&#34;property&#34;</span><span>:</span><span> </span><span>&#34;P7127&#34;</span><span>,</span><span>
        </span><span>&#34;datavalue&#34;</span><span>:</span><span> </span><span>{</span><span>
            </span><span>&#34;value&#34;</span><span>:</span><span> </span><span>&#34;belgium&#34;</span><span>,</span><span>
            </span><span>&#34;type&#34;</span><span>:</span><span> </span><span>&#34;string&#34;</span><span>
        </span><span>},</span><span>
        </span><span>&#34;datatype&#34;</span><span>:</span><span> </span><span>&#34;external-id&#34;</span><span>
    </span><span>},</span><span>
    </span><span>&#34;type&#34;</span><span>:</span><span> </span><span>&#34;statement&#34;</span><span>,</span><span>
    </span><span>&#34;qualifiers&#34;</span><span>:</span><span> </span><span>null</span><span>,</span><span>
    </span><span>&#34;qualifiers-order&#34;</span><span>:</span><span> </span><span>null</span><span>,</span><span>
    </span><span>&#34;id&#34;</span><span>:</span><span> </span><span>&#34;Q31$be99eedf-4d68-b90b-e95b-21438633aa8d&#34;</span><span>,</span><span>
    </span><span>&#34;rank&#34;</span><span>:</span><span> </span><span>&#34;normal&#34;</span><span>,</span><span>
    </span><span>&#34;references&#34;</span><span>:</span><span> </span><span>null</span><span>
</span><span>}</span><span>
</span></code></pre></div></div>

<p>This claim describes the <a href="https://www.alltrails.com">AllTrails</a> <a href="https://www.wikidata.org/wiki/Property:P7127">ID</a> for <a href="https://www.alltrails.com/belgium">Belgium</a>. The <code>value</code> under <code>mainsnak</code> and <code>datavalue</code> is the AllTrails ID itself. The <code>property</code> under <code>mainsnak</code> is the claim identifier for AllTrails IDs. (And no, I don’t know why they’re named <a href="https://www.mediawiki.org/wiki/Wikibase/DataModel#Snaks">snaks</a>)</p>

<p>There are <em>214</em> external ID claims for Belgium!
Want the <a href="https://id.loc.gov/authorities/names/n80126041.html">Library of Congress</a> number?
Or how about the ID for the libraries of <a href="https://viaf.org/processed/N6Ivtls000316878">Ireland</a>, <a href="https://viaf.org/processed/UIY|000076881">Iceland</a>, or <a href="https://koha.nlg.gr/cgi-bin/koha/opac-authoritiesdetail.pl?marc=1&amp;authid=2020">Greece</a>? 
You can find stories about Belgium on <a href="https://www.bbc.com/news/topics/cz4pr2gdgrdt">the BBC</a>, <a href="https://www.theguardian.com/world/belgium">The Guardian</a>, or <a href="https://www.c-span.org/organization/belgium/19875/">C-SPAN</a>. 
Find all the videogames Belgium appears in on <a href="https://www.giantbomb.com/wd/3035-254/">Giant Bomb</a> or find code tagged with <code>belgium</code> on <a href="https://github.com/topics/belgium">Github</a>.</p>

<p>Most interesting to me: geo identifiers like <a href="https://www.openstreetmap.org/node/1684793666">OpenStreetMap</a>, <a href="https://www.google.com/maps/place/Belgium/@50.501038,4.4661,529159m/data=!3m2!1e3!4b1!4m6!3m5!1s0x47c17d64edf39797:0x47ebf2b439e60ff2!8m2!3d50.503887!4d4.469936!16zL20vMDE1NGo?entry=ttu&amp;g_ep=EgoyMDI0MTAwMi4xIKXMDSoASAFQAw%3D%3D">Google Maps</a>, and <a href="https://spelunker.whosonfirst.org/id/85632997/">Who’s On First</a> all have claims. And for many records, Apple Maps IDs are there too.</p>

<p>Today we’re going to build a giant crosswalk file for all the geographic entities on Wikidata.</p>

<h2 id="preparing-the-entities">Preparing the Entities</h2>

<p>Our folder full of gzipped JSONL files is good, but there’s a ton of metadata in there we don’t need and is in a more difficult format than necessary. We’ll use a small Ruby script to prep the data:</p>



<p>Our <code>process_file</code> function walks through each gzipped file, line by line, skipping items if they don’t have English-language labels or descriptions. We assemble a subset of claims with confident rankings and data values, then save the item as a JSON line in our filtered JSONL file. Gone are all the localizations and the complexity of the claims, since we’re only taking one claim per claim type (so just dictionaries, no arrays). That’s a good trade off for our use case today.</p>

<p>Processing 535 chunked files is a great use case for Ruby <a href="https://www.honeybadger.io/blog/ractors/">Ractors</a>, which let us run our processing code concurrently across multiple cores. It takes awhile to run, but this script turns our ~135 GBs of gzipped files into ~45 GBs of uncompressed JSONL.</p>

<h3 id="building-the-crosswalk">Building the Crosswalk</h3>

<p>With the <code>claims</code> normalized into a dictionary linking only to objects of the same type (no arrays), it is treated as a <a href="https://duckdb.org/docs/sql/data_types/map">map</a>, not a <a href="https://duckdb.org/docs/sql/data_types/struct">struct</a> by DuckDB. Which will let us build our crosswalk just using DuckDB.</p>

<p>We’re going to use the <a href="https://duckdb.org/docs/sql/functions/list#unnestlist"><code>UNNEST</code></a> function which lets us turn an array into rows for each item. <code>UNNEST</code> is a SQL feature in many databases which allow arrays, but it’s both a function <em>and</em> a pun in DuckDB.</p>

<div><div><pre><code><span>SELECT</span> <span>COUNT</span><span>(</span><span>unnest</span><span>(</span><span>map_values</span><span>(</span><span>claims</span><span>)))</span> <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span><span>;</span>
</code></pre></div></div>

<p>Which returns 66,429,868 claims across 3,634,596 different entities.</p>

<p>Let’s whittle that down to only the claims describing external IDs:</p>

<div><div><pre><code><span>SELECT</span> <span>name</span><span>,</span> <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span> 
<span>FROM</span> <span>(</span>
    <span>SELECT</span> <span>name</span><span>,</span> <span>UNNEST</span><span>(</span><span>map_values</span><span>(</span><span>claims</span><span>))</span> <span>as</span> <span>claim</span> 
    <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span>
<span>)</span> 
<span>WHERE</span> <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datatype</span> <span>=</span> <span>&#39;external-id&#39;</span><span>;</span>
</code></pre></div></div>

<p>23,689,931 claims, or ~35% of all claims. Let’s parse that <code>mainsnak</code> a bit to produce our first, proper crosswalk:</p>

<div><div><pre><code><span>SELECT</span> 
    <span>name</span> <span>as</span> <span>wikidata_id</span><span>,</span> 
    <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>property</span> <span>as</span> <span>property</span><span>,</span> 
    <span>CAST</span><span>(</span><span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datavalue</span><span>.</span><span>value</span> <span>AS</span> <span>VARCHAR</span><span>)</span> <span>as</span> <span>external_id</span> 
<span>FROM</span> <span>(</span>
    <span>SELECT</span> <span>name</span><span>,</span> <span>UNNEST</span><span>(</span><span>map_values</span><span>(</span><span>claims</span><span>))</span> <span>as</span> <span>claim</span> 
    <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span>
<span>)</span>
<span>WHERE</span> <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datatype</span> <span>=</span> <span>&#39;external-id&#39;</span><span>;</span>
</code></pre></div></div>

<p>The <code>datavalue</code> is being returned as a <code>JSON</code> for some reason, so we’re casting it as <code>VARCHAR</code>. I’m sure there’s a more elegant way, but still: <em>this builds us a crosswalk table with 23,689,931 ID pairs, in about 45 seconds on my Mac Studio.</em></p>

<p>But we can do better. Boot up DuckDB targeting a database file with: <code>duckdb geo_cross.db</code></p>

<p>Load our previous query as a table:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>wikidata_external_join</span> <span>AS</span> 
<span>SELECT</span> 
    <span>name</span> <span>as</span> <span>wikidata_id</span><span>,</span> 
    <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>property</span> <span>as</span> <span>property</span><span>,</span> 
    <span>CAST</span><span>(</span><span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datavalue</span><span>.</span><span>value</span> <span>AS</span> <span>VARCHAR</span><span>)</span> <span>as</span> <span>external_id</span> 
<span>FROM</span> <span>(</span>
    <span>SELECT</span> <span>name</span><span>,</span> <span>UNNEST</span><span>(</span><span>map_values</span><span>(</span><span>claims</span><span>))</span> <span>as</span> <span>claim</span> 
    <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span>
<span>)</span> 
<span>WHERE</span> <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datatype</span> <span>=</span> <span>&#39;external-id&#39;</span><span>;</span>
</code></pre></div></div>

<p>And create a table with the names and descriptions for each item:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>wikidata_entities</span> <span>AS</span> <span>SELECT</span> <span>name</span> <span>as</span> <span>id</span><span>,</span> <span>label</span> <span>as</span> <span>name</span><span>,</span> <span>description</span> <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span><span>;</span>
</code></pre></div></div>

<p>And just for fun, let’s grab one non-external-id claim: <code>P625</code>.</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>coordinates</span> <span>AS</span>
<span>SELECT</span> 
    <span>name</span> <span>as</span> <span>wikidata_id</span><span>,</span>  
    <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datavalue</span><span>.</span><span>value</span><span>.</span><span>latitude</span> <span>as</span> <span>latitude</span><span>,</span>
    <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>datavalue</span><span>.</span><span>value</span><span>.</span><span>longitude</span> <span>as</span> <span>longitude</span>
<span>FROM</span> <span>(</span>
    <span>SELECT</span> <span>name</span><span>,</span> <span>UNNEST</span><span>(</span><span>map_values</span><span>(</span><span>claims</span><span>))</span> <span>as</span> <span>claim</span> 
    <span>FROM</span> <span>&#39;filter/*.jsonl&#39;</span>
<span>)</span>
<span>WHERE</span> <span>claim</span><span>[</span><span>1</span><span>].</span><span>mainsnak</span><span>.</span><span>property</span> <span>=</span> <span>&#39;P625&#39;</span><span>;</span>
</code></pre></div></div>

<p>Property <code>P625</code> is <a href="https://www.wikidata.org/wiki/Property:P625">coordinate location</a>, which handily contains a JSON dictionary with <code>latitude</code> and <code>longitude</code> values. There are 1,453,456 items with coordinates!</p>

<p>Let’s plot them out:</p>

<p><img src="https://www.dbreunig.com/img/wikidata_points_bay.png" alt="Wikidata entities with coordinate pairs, in the Bay Area"/></p>

<p>Pretty good! Our loaded <code>geo_cross.db</code> is only ~500 MBs. The only bit missing is a table of labels for the ids in the <code>property</code> column of the join table. That’s out of scope for this article, but I will leave you with <a href="https://gist.github.com/dbreunig/91ba405a0580394545ba94598fac9c74">this CSV listing seven different external map identifiers</a>:</p>



<p>With this you can run:</p>

<div><div><pre><code><span>CREATE</span> <span>TABLE</span> <span>properties</span> <span>AS</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>&#39;geo_id_properties.csv&#39;</span><span>;</span>
</code></pre></div></div>

<p>Then with <a href="https://gist.github.com/dbreunig/843629f720bc9b6095c6de7b018e0ce1">a hacked-together SQL pivot</a>, voila: <a href="https://drive.google.com/file/d/1GWghk10hfvCoRHb1nQ6sPTv4S26glJN1/view?usp=share_link">a 371,937 record matchfile, with coordinate pairs</a>.</p>

<p>And you could take this much further. Wikidata has claims for parent/child relationships among adminstrative areas. And claims specifying something is <code>part of</code> another entity…</p>

<p>Wikidata is a sleeper of a crosswalk file. It tracks over <em>7,669</em> different external IDs. Sure, there’s noise in there,but there’s so much potential.</p>

<hr/>



  </div></div>
  </body>
</html>
