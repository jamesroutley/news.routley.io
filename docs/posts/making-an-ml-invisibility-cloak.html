<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">Original</a>
    <h1>Making an ML Invisibility Cloak</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content">
                          

<h3 id="overview">Overview</h3>

<p>This paper studies the art and science of creating adversarial attacks on object detectors.  Most work on real-world adversarial attacks has focused on <em>classifiers</em>, which assign a holistic label to an entire image, rather than <em>detectors</em> which localize objects within an image.  Detectors work by considering thousands of “priors” (potential bounding boxes) within the image with different locations, sizes, and aspect ratios.  To fool an object detector, an adversarial example must fool every prior in the image, which is much more difficult than fooling the single output of a classifier.</p>

<p>In this work, we present a systematic study of adversarial attacks on state-of-the-art object detection frameworks.  Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Our ultimate goal is to build a wearable “invisibility” cloak that renders the wearer imperceptible to detectors.</p>

<blockquote>
<p><a href="https://arxiv.org/abs/1910.14667">Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors</a></p>
</blockquote>

<!--You can purchase your own invisibility clothes below.  Proceeds will benefit the College Park Foundation and other charities selected by the authors.  Note:  sizes tend to run small.

> [Invisibility Cloak Store @ RageOn](https://www.rageon.com/a/users/invisibilitycloak)  
-->

<div>
  
<p><img src="https://www.cs.umd.edu/~tomg/img/invisible/cover_bright.png"/>
</p>
</div>

<h3 id="video-demo">Video Demo</h3>

<p>While a full-scale demo had been delayed due to COVID, here’s a short composite
of some of our test footage.</p>

<video width="90%" controls="">
  <source src="../../img/invisible_video.mp4" type="video/mp4"/>
Your browser does not support the video tag.
</video>

<h3 id="approach">Approach</h3>

<p>We load images from the COCO detection dataset, and pass them
through a detector.  When a person is detected, and pattern is rendered over that person with random
perspective, brightness, and contrast deformations.  A gradient descent algorithm is then used to
find the pattern that minimizes the “objectness scores” (confidence in the presence of an object)
for every object prior.</p>

<div>
  
<p><img src="https://www.cs.umd.edu/~tomg/img/invisible/feat_map.png"/>
</p>
<br/>
</div>

<h3 id="gallery">Gallery</h3>

<p><img src="https://www.cs.umd.edu/~tomg/img/invisible/cover2.png" alt="image"/></p>

<p><img src="https://www.cs.umd.edu/~tomg/img/invisible/IMG_6594_7_org.png" alt="image"/></p>

<p><img src="https://www.cs.umd.edu/~tomg/img/invisible/IMG_6636_9_org.png" alt="image"/></p>

<h3 id="thanks">Thanks</h3>

<p>Thanks to Facebook AI for their support on this project!</p>

                        </div></div>
  </body>
</html>
