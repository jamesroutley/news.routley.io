<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gpuopen.com/learn/pytorch-windows-amd-llm-guide/">Original</a>
    <h1>A beginner&#39;s guide to deploying LLMs with AMD on Windows using PyTorch</h1>
    
    <div id="readability-page-1" class="page"><div> <p>If you’re interested in deploying advanced AI models on your local
hardware, leveraging a modern AMD GPU or APU can provide an efficient
and scalable solution. You don’t need dedicated AI infrastructure to
experiment with Large Language Model (LLMs); a capable Microsoft® Windows® PC with
PyTorch installed and equipped with a recent AMD graphics card is all you
need.</p>
<p>PyTorch for AMD on Windows and Linux is now available as a <a href="https://repo.radeon.com/rocm/windows/rocm-rel-6.4.4/">public
preview</a>. You can
now use native PyTorch for AI inference on AMD Radeon™ RX 7000 and 9000
series GPUs and select AMD Ryzen™ AI 300 and AI Max APUs, enabling
seamless AI workload execution on AMD hardware in Windows without any
need for workarounds or dual-boot configurations. If you are new and
just getting started with AMD ROCm™, be sure to check out our getting started
guides <a href="https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/">here</a>.</p>
<p>This guide is designed for developers seeking to set up, configure, and
execute LLMs locally on a Windows PC using PyTorch with an AMD GPU or
APU. No previous experience with PyTorch or deep learning frameworks is
needed.</p>
<h2 id="what-youll-need-the-prerequisites">What you’ll need (the prerequisites)</h2>
<ul>
<li>The currently supported AMD platforms and hardware for PyTorch on
Windows are listed here:</li>
</ul>
















































<ul>
<li>
<p>OS Supported: Microsoft® Windows® 11</p>
</li>
<li>
<p><a href="https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-WINDOWS-PYTORCH-PREVIEW.html">AMD Software: PyTorch on Windows Preview Edition 25.20.01.14 driver</a></p>
</li>
<li>
<p>Python 3.12 - <a href="https://www.python.org/downloads/release/python-3120/">Python Release Python 3.12.0 |
Python.org</a></p>
<ul>
<li>During the Python installation, make sure to check the box that says <strong>“Add Python to PATH”</strong></li>
</ul>
</li>
</ul>
<h2 id="part-1-setting-up-your-workspace">Part 1: Setting up your workspace</h2>
<h3 id="step-1-open-the-command-prompt">Step 1: Open the Command Prompt</h3>
<p>First, we need to open the Command Prompt</p>
<ul>
<li>Click the <strong>Start Menu</strong>, type <code dir="auto">cmd</code>, and press <strong>Enter</strong>. A black terminal window will pop up.</li>
</ul>
<p><img src="https://gpuopen.com/images/pytorch-windows-amd-llm-guide-00.BEWOEAK8.png" alt="Terminal Window" width="800" height="257" loading="lazy" decoding="async"/></p>
<h3 id="step-2-create-and-activate-a-virtual-environment">Step 2: Create and activate a virtual environment</h3>
<p>A “virtual environment” is like a clean, empty sandbox for a Python project.</p>
<p>In your Command Prompt, type the following command and press <strong>Enter</strong>. This
creates a new folder named <code dir="auto">llm-pyt</code> that will house our project.</p>
<p><code dir="auto">python -m venv llm-pyt</code></p>
<p>Next, we need to “activate” this environment. Think of this as stepping inside the sandbox.</p>
<p><code dir="auto">llm-pyt\Scripts\activate</code></p>
<p>You’ll know it worked because you’ll see <code dir="auto">(llm-pyt)</code> appear at the beginning of your command line prompt.</p>
<p><img src="https://gpuopen.com/images/pytorch-windows-amd-llm-guide-01.DYyqpWV-.png" alt="llm-pyt prompt" width="600" height="275" loading="lazy" decoding="async"/></p>
<h3 id="step-3-install-pytorch-and-other-essential-libraries">Step 3: Install PyTorch and other essential libraries</h3>
<p>Now we’ll install the software libraries that do the heavy lifting. The
most important one is <strong>PyTorch</strong>, an open-source framework for building
and running AI models. We need a special version of PyTorch built to
work with AMD’s ROCm technology.</p>
<p>We will also install <strong>Transformers</strong> and <strong>Accelerate</strong>, two libraries
from Hugging Face that make it incredibly easy to download and run
state-of-the-art AI models.</p>
<p>Run the following command in your activated Command Prompt. This command
tells Python’s package installer (<code dir="auto">pip</code>) to download and install
PyTorch for ROCm, along with the other necessary tools.</p>
<div><figure><pre data-language="bash" dir="ltr"><code><div><p><span>pip</span><span> </span><span>install</span><span> </span><span>--no-cache-dir</span><span> </span><span>https://repo.radeon.com/rocm/windows/rocm-rel-6.4.4/torch-2.8.0a0%2Bgitfc14c65-cp312-cp312-win_amd64.whl</span></p></div><div><p><span>pip</span><span> </span><span>install</span><span> </span><span>--no-cache-dir</span><span> </span><span>https://repo.radeon.com/rocm/windows/rocm-rel-6.4.4/torchaudio-2.6.0a0%2B1a8f621-cp312-cp312-win_amd64.whl</span></p></div><div><p><span>pip</span><span> </span><span>install</span><span> </span><span>--no-cache-dir</span><span> </span><span>https://repo.radeon.com/rocm/windows/rocm-rel-6.4.4/torchvision-0.24.0a0%2Bc85f008-cp312-cp312-win_amd64.whl</span></p></div><div><p><span>pip</span><span> </span><span>install</span><span> </span><span>transformers</span><span> </span><span>accelerate</span></p></div></code></pre></figure></div>
<h2 id="part-2-putting-your-new-llm-setup-to-the-test">Part 2: Putting your new LLM setup to the test</h2>
<p>The moment of truth. Let’s give our new setup a task: running a small
but powerful language model called <strong>Llama 3.2 1B.</strong></p>
<h3 id="step-1-launch-the-interactive-python-session">Step 1: Launch the interactive Python session</h3>
<p>Make sure your Command Prompt still has the (<code dir="auto">llm-pyt</code>) environment
active. If you closed it, just re-open <code dir="auto">cmd</code> and run <code dir="auto">llm-pyt\Scripts\activate</code>.</p>
<p>Now, start Python:</p>

<h3 id="step-2-run-the-language-model">Step 2: Run the language model</h3>
<p>Copy the entire code block below. Paste it into your Python terminal
(where you see the &gt;&gt;&gt;) and press <strong>Enter</strong>.</p>
<p>The first time you do this, it will download the model (which is a few
gigabytes), so it may take several minutes. Subsequent runs will be much
faster.</p>
<div><figure><pre data-language="python" dir="ltr"><code><div><p><span>import</span><span> torch</span></p></div><div><p><span>from</span><span> transformers </span><span>import</span><span> pipeline</span></p></div><div><p><span>model_id </span><span>=</span><span> </span><span>&#34;unsloth/Llama-3.2-1B-Instruct&#34;</span></p></div><div><p><span>pipe </span><span>=</span><span> pipeline(</span></p></div><div><p><span>    </span><span>&#34;text-generation&#34;</span><span>,</span></p></div><div><p><span>    </span><span>model</span><span>=</span><span>model_id,</span></p></div><div><p><span>    </span><span>dtype</span><span>=</span><span>torch.float16,</span></p></div><div><p><span>    </span><span>device_map</span><span>=</span><span>&#34;auto&#34;</span></p></div><div><p><span>)</span></p></div><div><p><span>pipe(</span><span>&#34;The key to life is&#34;</span><span>)</span></p></div></code></pre></figure></div>
<p>You should see an output similar to this:</p>
<div><figure><pre data-language="plaintext" dir="ltr"><code><div><p><span>[{&#39;generated_text&#39;: &#39;The key to life is not to get what you want, but to give what you have.</span></p></div><div><p><span>The best way to make life more meaningful is to practice gratitude, and to cultivate a sense</span></p></div><div><p><span>of contentment with what you have. If you want to make life more interesting, you must be</span></p></div><div><p><span>willing to take risks, and to embrace the unknown. The best way to avoid disappointment is</span></p></div><div><p><span>to be patient and persistent, and to trust in the process. By following these principles,</span></p></div><div><p><span>you can live a more fulfilling life, and make the most of the time you have.&#39;}]</span></p></div></code></pre></figure></div>
<p>You can return to your command prompt by typing <code dir="auto">exit()</code> and pressing
<strong>Enter</strong>.</p>

<h2 id="level-up-create-an-interactive-ai-chatbot">Level Up: Create an interactive AI chatbot</h2>
<p>Running a single prompt is fun, but a real conversation is better. In
this section, we’ll create an interactive chat loop that “remembers”
the conversation, allowing you to have a back-and-forth with the AI.</p>
<h3 id="step-1-create-the-chatbot-script">Step 1: Create the chatbot script</h3>
<ol>
<li>
<p>Open a new file in your text editor.</p>
</li>
<li>
<p>Copy and paste the chatbot code below.</p>
</li>
</ol>
<div><figure><pre data-language="python" dir="ltr"><code><div><p><span>import</span><span> torch</span></p></div><div><p><span>from</span><span> transformers </span><span>import</span><span> pipeline</span></p></div><div></div><div><p><span>print</span><span>(</span><span>&#34;Loading chat model...&#34;</span><span>)</span></p></div><div></div><div><p><span>model_id </span><span>=</span><span> </span><span>&#34;unsloth/Llama-3.2-1B-Instruct&#34;</span></p></div><div><p><span>pipe </span><span>=</span><span> pipeline(</span></p></div><div><p><span>    </span><span>&#34;text-generation&#34;</span><span>,</span></p></div><div><p><span>    </span><span>model</span><span>=</span><span>model_id,</span></p></div><div><p><span>    </span><span>dtype</span><span>=</span><span>torch.float16,</span></p></div><div><p><span>    </span><span>device_map</span><span>=</span><span>&#34;auto&#34;</span><span>,</span></p></div><div><p><span>)</span></p></div><div></div><div><p><span># This list will store our conversation history</span></p></div><div><p><span>messages </span><span>=</span><span> []</span></p></div><div></div><div><p><span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Chatbot ready! Type &#39;quit&#39; or &#39;exit&#39; to end the conversation.&#34;</span><span>)</span></p></div><div><p><span>print</span><span>(</span><span>&#34;-&#34;</span><span> </span><span>*</span><span> </span><span>20</span><span>)</span></p></div><div></div><div><p><span>while</span><span> </span><span>True</span><span>:</span></p></div><div><p><span>    </span><span># Get input from the user</span></p></div><div><p><span><span>    </span></span><span>user_input </span><span>=</span><span> </span><span>input</span><span>(</span><span>&#34;You: &#34;</span><span>)</span></p></div><div></div><div><p><span>    </span><span># Check if the user wants to exit</span></p></div><div><p><span>    </span><span>if</span><span> user_input.lower() </span><span>in</span><span> [</span><span>&#34;quit&#34;</span><span>, </span><span>&#34;exit&#34;</span><span>]:</span></p></div><div><p><span>        </span><span>print</span><span>(</span><span>&#34;Chat session ended.&#34;</span><span>)</span></p></div><div><p><span>        </span><span>break</span></p></div><div></div><div><p><span>    </span><span># Add the user&#39;s message to the conversation history</span></p></div><div><p><span><span>    </span></span><span>messages.append({</span><span>&#34;role&#34;</span><span>: </span><span>&#34;user&#34;</span><span>, </span><span>&#34;content&#34;</span><span>: user_input})</span></p></div><div></div><div><p><span>    </span><span># Generate the AI&#39;s response using the full conversation history</span></p></div><div><p><span><span>    </span></span><span>outputs </span><span>=</span><span> pipe(messages, </span><span>max_new_tokens</span><span>=</span><span>500</span><span>, </span><span>do_sample</span><span>=</span><span>True</span><span>, </span><span>temperature</span><span>=</span><span>0.7</span><span>)</span></p></div><div></div><div><p><span>    </span><span># The pipeline returns the full conversation. The last message is the new one.</span></p></div><div><p><span><span>    </span></span><span>assistant_response </span><span>=</span><span> outputs[</span><span>0</span><span>][</span><span>&#39;generated_text&#39;</span><span>][</span><span>-</span><span>1</span><span>][</span><span>&#39;content&#39;</span><span>]</span></p></div><div></div><div><p><span>    </span><span># Add the AI&#39;s response to our history</span></p></div><div><p><span><span>    </span></span><span>messages.append({</span><span>&#34;role&#34;</span><span>: </span><span>&#34;assistant&#34;</span><span>, </span><span>&#34;content&#34;</span><span>: assistant_response})</span></p></div><div></div><div><p><span>    </span><span># Print just the AI&#39;s new response</span></p></div><div><p><span>    </span><span>print</span><span>(</span><span>f</span><span>&#34;AI: </span><span>{</span><span>assistant_response</span><span>}</span><span>&#34;</span><span>)</span></p></div></code></pre></figure></div>
<ol start="3">
<li>Save this new file as <code dir="auto">run_chat.py</code> in the same user folder.</li>
</ol>
<p><strong>Step 2: Run your chatbot</strong></p>
<p>In your Command Prompt, run the new script:</p>

<p>The terminal will now prompt you with <em>You:.</em> Type a question and press
<strong>Enter</strong>. The AI will respond, and you can ask follow-up questions. The
chatbot will remember the context of the conversation.</p>
<p><img src="https://gpuopen.com/images/pytorch-windows-amd-llm-guide-02.IZmVIM7_.png" alt="Results" width="1000" height="693" loading="lazy" decoding="async"/></p>

<div><p><strong>Note:</strong></p><p>When you run the LLM, you will see a warning message like this:</p><div><figure><pre data-language="plaintext" dir="ltr"><code><div><p><span>UserWarning: 1Torch was not compiled with memory efficient attention.</span></p></div><div><p><span>(Triggered internally at C:\develop\pytorch-test\aten\src\ATen\native\transformers\hip\sdp_utils.cpp:726.)</span></p></div></code></pre></figure></div><p><strong>Don’t worry, this is expected, and your code is working correctly!</strong></p><p><strong>What it means in simple terms:</strong> PyTorch 2.0+ introduced a feature called “Memory-Efficient Attention” to speed things up. The current version of PyTorch for AMD on Windows doesn’t include this specific optimization out-of-the-box. When PyTorch can’t find it, it prints this warning and automatically falls back to the standard, reliable method.</p></div>
<h2 id="summary">Summary</h2>
<p>By following this blog, you should be able to get started with running
transformer-based LLMs with PyTorch on Windows using AMD consumer
graphics hardware.</p>
<p>You can learn more about our road to AMD ROCm on Radeon for Windows and Linux in the blog from Andrej Zdravkovic, SVP and AMD Chief Software Officer, <a href="https://www.amd.com/en/blogs/2025/the-road-to-rocm-on-radeon-for-windows-and-linux.html">here</a>.</p>

<details id="endnotes"><summary>View endnotes</summary>
<em>PyTorch, the PyTorch logo and any related marks are trademarks of The Linux Foundation.</em><p><em>Windows is a trademark of the Microsoft group of companies.</em></p></details> </div></div>
  </body>
</html>
