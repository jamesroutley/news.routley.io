<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://owehrens.com/whisper-nvidia-rtx-4090-vs-m1pro-with-mlx/">Original</a>
    <h1>Whisper: Nvidia RTX 4090 vs. M1 Pro with MLX</h1>
    
    <div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        

            <p>How fast is my Whisper Benchmark with the MLX Framework from Apple? Nvidia 4090 / M1 Pro / M2 Ultra / M3</p>

        <div>
        <section>

            <ul>
                <li>
                    <a href="https://owehrens.com/author/oliver/">
                        <img src="https://www.gravatar.com/avatar/d7192560bc009a2a4a0afe1c2d0dd0e3?s=250&amp;r=x&amp;d=mp" alt="Oliver Wehrens"/>
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2023-12-09">Dec 9, 2023</time>
                        <span><span>•</span> 3 min read</span>
                </p>
            </div>

        </section>
        </div>


    </header>

    <section>
        <p>(... see down below for M2 Ultra / M3 Max Update)</p><p>Apple <a href="https://github.com/ml-explore/mlx">released a machine learning framework</a> for Apple Silicon. Along with that are <a href="https://github.com/ml-explore/mlx-examples">some examples</a> to see how things are working. They also use a whisper for benchmarking. So I dug out my benchmark and used that to measure performance.</p><p>I simply added a new file to the repo (and the whisper large model was already downloaded). See the <a href="https://github.com/ml-explore/mlx-examples/tree/main/whisper">original source dir</a>.</p><!--kg-card-begin: markdown--><pre><code>import datetime
from pprint import pprint

from whisper import transcribe

if __name__ == &#39;__main__&#39;:
    audio_file = &#34;whisper/assets/audio.wav&#34;
    start_time = datetime.datetime.now()
    x = transcribe(audio=audio_file, model=&#39;large&#39;)
    end_time = datetime.datetime.now()
    pprint(x)
    print(end_time - start_time)
</code></pre>
<!--kg-card-end: markdown--><p>It reports back a list of segements with the following structure:</p><!--kg-card-begin: markdown--><pre><code>{&#39;avg_logprob&#39;: -0.18728541468714807,
               &#39;compression_ratio&#39;: 1.3786764705882353,
               &#39;end&#39;: 589.92,
               &#39;id&#39;: 139,
               &#39;no_speech_prob&#39;: 0.0017877654172480106,
               &#39;seek&#39;: 56892,
               &#39;start&#39;: 586.92,
               &#39;temperature&#39;: 0.0,
               &#39;text&#39;: &#39; Ich heiße Moses Fendel, danke fürs Zuhören und &#39;
                       &#39;tschüß.&#39;,
               &#39;tokens&#39;: [51264,
                          3141,
                          39124,
                          68,
                          17580,
                          479,
                          521,
                          338,
                          11,
                          46434,
                          46577,
                          1176,
                          3232,
                          26377,
                          674,
                          256,
                          6145,
                          774,
                          2536,
                          13,
                          51414]},
</code></pre>
<!--kg-card-end: markdown--><p>The structure is the same as I get with Python whisper on my RTX 4090. </p><p>The audio file is the same as in my other <a href="https://owehrens.com/openai-whisper-on-apple-m1-cpp-version/">benchmarks</a> with M1 and <a href="https://owehrens.com/whisper-performance-on-nvidia-rtx-4090/">4090</a>. </p><figure><img src="https://owehrens.com/content/images/2023/12/MLX-Power-1.png" alt="" loading="lazy" width="1790" height="798" srcset="https://owehrens.com/content/images/size/w600/2023/12/MLX-Power-1.png 600w, https://owehrens.com/content/images/size/w1000/2023/12/MLX-Power-1.png 1000w, https://owehrens.com/content/images/size/w1600/2023/12/MLX-Power-1.png 1600w, https://owehrens.com/content/images/2023/12/MLX-Power-1.png 1790w" sizes="(min-width: 720px) 720px"/></figure><h2 id="result">Result</h2><p>The result for a 10 Minute audio is 0:03:36.296329 <em>(216 seconds)</em>. Compare that to <em>0:03:06.707770 (186 seconds)</em> on my Nvidia 4090. The 2000 € GPU is still 30 seconds or ~ 16% faster. All graphics core where fully utilized during the run and I quit all programs, disabled desktop picture or similar for that run. </p><p><strong>Update: </strong>I ran the same tests multiple times, the time is measured now without loading the model into memory in both cases.</p><p>My Macbook Hardware Specs:</p><ul><li>14&#34; MacBook with M1 Pro, 8 (6 performance and 2 efficiency) cores (2021 model)</li><li>32 GB RAM</li><li>16 GPU Cores</li></ul><p>PC Spec:</p><ul><li>Intel Core I7-12700KF 8x 3.60GHz</li><li>2x32 GB RAM 3200 MHz DDR4, Kingston FURY Beast</li><li>SSD M.2 PCIe 2280 - 1000GB Kingston KC3000 PCIe 4.0 NVMe</li><li>GeForce RTX 4090, 24GB GDDR6X / Palit RTX 4090 GameRock OmniBlack<br/></li></ul><p>The new M3 Max Chip has 30 GPU cores (configurable up to 40 GPU cores)  and 14 CPU cores (up to 16 CPU cores). If these GPU cores (double amount) are 30% faster than the ones in my Laptop: This should beat the RTX 4090 easily. <a href="https://www.apple.com/macbook-pro/specs/">That machine is as of now 3200 $.</a>  </p><h2 id="m2-ultra-m3-max-update">M2 Ultra / M3 Max Update</h2><p><a href="https://twitter.com/ivanfioravanti/status/1734644638357573679">Ivan over at Twitter</a> ran the same audio file on M2 Ultra with 76 GPUs and M3 Max with 40 GPUs. Much faster than my M1 but both are similar speed.</p><figure><img src="https://owehrens.com/content/images/2023/12/bench2.jpg" alt="" loading="lazy" width="1188" height="335" srcset="https://owehrens.com/content/images/size/w600/2023/12/bench2.jpg 600w, https://owehrens.com/content/images/size/w1000/2023/12/bench2.jpg 1000w, https://owehrens.com/content/images/2023/12/bench2.jpg 1188w" sizes="(min-width: 720px) 720px"/><figcaption>Ivan tested it on M2+M3</figcaption></figure><h2 id="comparison">Comparison</h2><figure><img src="https://owehrens.com/content/images/2023/12/graph.jpg" alt="" loading="lazy" width="1178" height="718" srcset="https://owehrens.com/content/images/size/w600/2023/12/graph.jpg 600w, https://owehrens.com/content/images/size/w1000/2023/12/graph.jpg 1000w, https://owehrens.com/content/images/2023/12/graph.jpg 1178w" sizes="(min-width: 720px) 720px"/><figcaption>Whisper performance</figcaption></figure><p>Keep in mind, this is not 100% accurate. The rough idea should be visible. Other processes running, loading times, cold, warm start can influence the numbers. </p><h2 id="power-consumption">Power consumption</h2><p>Difference between idle PC / M1Pro and GPU running PC / M1Pro</p><ul><li>PC +242 W (Nvidia 4090 running vs. idle)</li><li>MacBook +38 W (16 M1 GPU cores running vs. idle)</li></ul><p>I measured that with a Shelly plug. This might not be 100% accurate but gives an idea where it is going.</p><p><em>Dear Reddit comments:</em></p><p>Way to go Apple.</p><h3 id="why-im-doing-this">Why I&#39;m doing this?</h3><p>I run a podcast search engine over at <a href="http://podpodgogo.com">https://podpodgogo.com</a>. I transcribe tens of thousands episodes, make them full text searchable and run some data mining on them.</p><p><strong>Update Dec 11th: </strong>Added specs and more tests without loading the model.</p><p><strong>Update Dec 12th: </strong>The 4090 is the fastest <em>consumer</em> graphics card. Also updated numbers for M2/M3.</p>
    </section>


</article>
</div></div>
  </body>
</html>
