<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nscale.com/blog/nscale-benchmarks-amd-mi300x-gpus-with-gemm-tuning-improves-throughput-and-latency-by-up-to-7-2x">Original</a>
    <h1>AMD MI300x GPUs with GEMM tuning improves throughput and latency by up to 7.2x</h1>
    
    <div id="readability-page-1" class="page"><div><h2>Introduction:</h2><p>In Nscale&#39;s latest technical deep dive, we explore a critical aspect of AI model optimisation: throughput benchmarking, performance tuning, and latency reduction using GEMM (General Matrix Multiplication) tuning.</p><p>Maximising the performance of GPU-accelerated tasks involves more than just raw speed. Optimising GEMM ensures efficient processing, higher throughput, and the ability to handle complex models and datasets effectively.</p><p>In this blog, we will explore the benchmarking of vLLM throughput across multiple models and delve into the significant impact of GEMM tuning. Powerful libraries such as <a href="https://github.com/ROCm/rocBLAS?tab=readme-ov-file">rocBLAS</a> (ROCm Basic Linear Algebra Subprograms) and <a href="https://github.com/ROCm/hipBLASLt">hipBLASlt</a> (Heterogeneous-Compute Interface for Portability, Basic Linear Algebra Subprograms) are instrumental in this process. </p><p>These libraries provide optimised implementations of GEMM operations along with a range of tuning parameters, allowing developers to fine-tune their applications and unlock the full potential of their underlying hardware, ultimately maximising vLLM performance.</p><h3>What is GEMM Tuning?</h3><p>GEMM tuning is a powerful technique for enhancing the performance of matrix-multiplication operations. This process includes selecting the most appropriate algorithm based on factors such as memory, cache, and compute capabilities.&#34;</p><p>By fine-tuning parameters and selecting optimal algorithms, we ensure the GEMM operation maximises efficiency in using available computing resources. This translates to significant speed improvements for AI and machine learning models.</p><h4>Key Aspects of GEMM Tuning</h4><h5><strong>Algorithm Selection:</strong></h5><p>When tuning GEMM operations, several key aspects must be considered to optimise performance. The first step is algorithm selection, which involves choosing the most efficient method for matrix multiplication based on the hardware&#39;s characteristics.</p><p>One common approach is the blocked algorithm, where input matrices are divided into smaller blocks. Matrix multiplication is then performed on these blocks, which enhances cache utilisation and parallelism by working on smaller data sets.</p><p>Another method is the tiled algorithm, similar to the blocked approach but with a greater emphasis on data locality and minimising memory access. This method breaks the matrix into smaller tiles and performs multiplication on these tiles, making it ideal for efficient use of shared memory.</p><p>For GPUs, parallel matrix multiplication algorithms are essential as they exploit parallelism inherent in GPU architectures. These algorithms utilise multiple threads or cores to compute matrix elements concurrently.</p><p>The choice between blocked, tiled, or other variants depends on factors such as matrix size, memory hierarchy, and the specific libraries in use on AMD GPUs and accelerators. Tools like rocBLAS (where BLAS stands for Basic Linear Algebra Subprograms) have built-in optimisations that automatically select the most appropriate algorithm based on these factors.</p><h5><strong>Parameter Adjustments:</strong></h5><p>Adjusting parameters such as block size or tile size to optimise performance. These adjustments help to align the operation with the hardware&#39;s architecture.</p><h5><strong>Kernel Configuration:</strong></h5><p>Configuring the kernels used in GEMM operations to optimise how computations are distributed across threads, registers, and memory. Proper kernel configuration minimises latency and maximises throughput by efficiently managing the workload distribution.</p><h2>Benchmarking Prerequisites:</h2><p>For our benchmarking tests, we used a pre-configured Docker image and ensured the following software and libraries were installed:</p><ul role="list"><li><a href="https://hub.docker.com/r/eliovp/rocm6.1.2_py3.10_torch2.5_vllm0.5_bkc">Docker Image</a></li><li>ROCm 6.1.2</li><li>Python 3.10.12</li><li>PyTorch 2.5.0</li><li>Triton 2.1.0</li><li>Flash Attention 2.0.4</li><li>rocBLAS 4.1.2</li><li>hipBLASlt 0.8.0</li><li>Rccl 2.18.6</li><li>vLLM 0.5.0</li></ul><p>We conducted our benchmarks using the benchmark_throughput.py script from the vLLM repository, which can be found <a href="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py">here</a>.</p><h3>Benchmark Runs</h3><p>Our benchmarking process involved two primary runs. Initially, we executed a benchmark using the out-of-the-box settings to establish a baseline performance measurement. This run provided insight into the standard performance metrics without applying specific optimisations. </p><p>Subsequently, we performed a second benchmark run after implementing GEMM tuning. This tuning process involved optimising the GEMM operations using rocBLAS and hipBLASit libraries, tailored to enhance computational efficiency and overall throughput, all of our benchmarks were conducted using the bf16 datatype.</p><h3>Metrics Compared</h3><p>Our analysis compared several key performance metrics between the two benchmark runs. </p><ul role="list"><li><strong>Generation Speed (tokens per second): </strong>Allowed us to gauge the efficiency of token generation for both input and output processes.</li><li><strong>Requests per Second:</strong> Providing a clear indication of the system&#39;s ability to manage multiple concurrent requests effectively. </li><li><strong>Overall Throughput (tokens processed per second):</strong> Encapsulates the combined efficiency of generation speed and request handling, offering a comprehensive view of the system&#39;s performance under different configurations.<strong>‍</strong></li><li><strong>Average Latency (seconds): Measuring the time taken to generate a response.</strong></li></ul><h3>Settings for Benchmark Runs</h3><p>We configured each benchmark run with the following settings:</p><ul role="list"><li><strong>Input Prompt Length for Each Request:</strong> 256 tokens</li><li><strong>Output Length for Each Request:</strong> 256 tokens</li><li><strong>Tensor Parallel Size:</strong> 1 (utilising a single GPU, specifically the MI300X)<strong>‍</strong></li><li><strong>Batch Sizes</strong>: 1, 2, and 4</li></ul><p>‍</p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/ME4wpBel1k4" title="LLama 2 70B benchmark performance increase"></iframe></p></figure><p>‍</p><h2>Key Observations:</h2><p>Let’s delve into the notable advancements achieved through GEMM tuning of LLMs such as Llama, Mistral, Mixtral, and Falcon. We will analyse a series of graphs and data visualisations that elucidate the impact of Tuned GEMM on the performance and efficiency of these models.</p><h3>Throughput/s:</h3><figure><p><img src="https://cdn.prod.website-files.com/65c7c6f130d274d75920a0f5/667e9147d174d001dd7e75d8_AD_4nXfL7lQT2tKfRoboRWG3vTaMBlplrUOx94Sa9_sERYBxHxCSpR0G-SZzRcDm8ivVImjKU2E09EePx5lq5gD-2URaLgxpl0wyn6gQYPPhnOy925YkItZd9qqSG4y8m6xg6F_F_YcHf5HFgmZxOlixEqQQeJiZ.png" alt=""/></p><figcaption>Fig. 1 vLLM MI300X Benchmarks (Average Throughput Improvement Factor)</figcaption></figure><p>‍</p><p>The graph shows a significant increase in generation speed when GeMM tuning is enabled</p><p><strong>GEMM Tuning Impact:</strong> Enabling GEMM tuning increases throughput by up to 7.2x, as seen with the LLaMA-2-70B model.</p><p><strong>Model Size:</strong> Larger models like LLaMA-2-70B and LLaMA-3-70B show the most significant improvements in throughput, with increases of 7.2x and 5.9x, respectively.</p><p><strong>Batch Size:</strong> Higher batch sizes generally lead to greater throughput, amplified by GEMM tuning. For instance, throughput for the Falcon 7B model rises from 244.74 tokens/second at batch size 1 to 952.38 tokens/second at batch size 4 without GEMM tuning. With tuning, it climbs further to 2736.58 tokens/second. </p><p><strong>Comparison Across Models:</strong> Among the models tested, LLaMA-2-70B and LLaMA-3-70B exhibit the highest throughput due to their complexity and size. Conversely, smaller models like Qwen 1.5 4B and Falcon 7B show relatively higher throughput, indicating more efficient processing for less complex models.</p><h3>Latency:</h3><figure><p><img src="https://cdn.prod.website-files.com/65c7c6f130d274d75920a0f5/667f2b98dc2bf1480fe98953_Latency-Average-Summary.jpg" loading="lazy" alt=""/></p><figcaption>Fig. 3 vLLM MI300X Benchmark Average Latency Improvement Factor</figcaption></figure><p>‍</p><p>The graph depicts the consistent reduction in latency achieved through GEMM tuning.</p><p><strong>GEMM Tuning Impact:</strong> Latency reduces significantly across all models. For instance, latency for the LLaMA-2-7B model drops from 1.00 to 0.35 seconds. During testing, we observed that with GEMM tuning enabled, the latency of the LLaMA-2-7B model with a batch size of 1 dropped by 66.5% from 1.97 seconds to 0.66 seconds. This pattern holds true until a batch size of 4, highlighting the significant performance enhancement GEMM tuning offers.</p><p><strong>Model Size</strong>: Larger models inherently exhibit higher latency. The LLaMA-2-70B model, for example, shows a latency of 1.00 seconds without GEMM tuning and 0.14 seconds with tuning enabled. In comparison, smaller models like LLaMA-2-7B show much lower latency under similar conditions. This trend is consistent across batch sizes, emphasising that model size directly impacts processing time.</p><p><strong>Batch Size:</strong> While larger batch sizes typically increase latency, GEMM tuning mitigates this, maintaining lower latency. In our testing of the LLaMA-2-7B model without GEMM tuning, the latency rises from 1.97 seconds at batch size 1 to 2.11 seconds at batch size 4. With GEMM tuning enabled, the increase is from 0.66 seconds to 0.77 seconds. This suggests that while GEMM tuning mitigates the latency increase to some extent, processing larger batches naturally requires more computational effort and time.</p><p><strong>Comparison Across Models:</strong> Models like Qwen 1.5 4B and Falcon 7B also show reduced latency, emphasising the effectiveness of GEMM tuning across different complexities.</p><h2>Conclusion:</h2><p>Our comprehensive benchmarking study of AMD MI300X GPUs with GEMM tuning reveals improvements in both throughput and latency, with gains of up to 7.2x in specific models. By optimising GEMM operations using rocBLAS and hipBLASlt libraries, we significantly enhanced the performance and efficiency of various large language models, including LLaMA, Mistral, Mixtral, and Falcon.</p><p>Key findings from our benchmarks include:</p><ul role="list"><li><strong>Throughput Improvements:</strong> Enabling GEMM tuning resulted in notable increases in throughput across all models. Larger models such as LLaMA-2-70B and LLaMA-3-70B demonstrated the highest improvements, benefiting significantly from the optimised operations.</li><li><strong>Latency Reductions:</strong> GEMM tuning consistently reduced latency across different batch sizes and models, with notable decreases observed in large and smaller models. This optimisation ensures quicker processing times, which is crucial for real-time AI applications.</li><li><strong>Batch Size Impact:</strong> While increasing batch size typically affects latency and throughput, GEMM tuning effectively mitigated these impacts, maintaining high-performance levels even with larger batch sizes.</li></ul><p>These results underscore the critical role of GEMM tuning in maximising the capabilities of AI models on AMD GPUs. By leveraging advanced tuning techniques, developers can unlock the full potential of their hardware, ensuring efficient processing and superior performance for complex and demanding AI workloads. </p><p>Discover the complete and updated benchmark results by accessing our full table of results <a href="https://docs.google.com/spreadsheets/d/1hHd9riWh-hQJnNOzEI_ZZN1LpdY_bBSbvDOVPdfYslM/edit?gid=1853523197#gid=1853523197" target="_blank">here</a>. Stay informed with the latest benchmark results.</p></div></div>
  </body>
</html>
