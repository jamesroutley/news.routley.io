<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://obrhubr.org/dithering-in-colour">Original</a>
    <h1>Dithering in Colour</h1>
    
    <div id="readability-page-1" class="page"><div>
			<nav>
	
	  
	
	  
	
	  
	
	  
	
</nav>



<main>
	

<hr/>

<main>
	<p>After reading a post on the HN frontpage from <a href="https://amanvir.com/blog/writing-my-own-dithering-algorithm-in-racket">amanvir.com</a> about dithering, I decided to join in on the fun. Here’s my attempt at implementing Atkinson dithering with support for colour palettes and correct linearisation.</p>

<h2 id="dithering-into-arbitrary-palettes">Dithering into arbitrary palettes</h2>

<p>The linked post from <a href="https://amanvir.com/">Aman</a> does an excellent job of explaining dithering into a black and white palette using <a href="https://en.wikipedia.org/wiki/Atkinson_dithering">Atkinson Dithering</a>. I can also recommend <a href="https://surma.dev/things/ditherpunk/">surma.dev</a>’s post, he explains more than just error diffusion (for example <a href="https://en.wikipedia.org/wiki/Ordered_dithering">ordered dithering</a>).</p>

<p>However both of them convert their input images to grayscale before dithering. If the sum of the pixel and the accumulated error is lighter than the threshold, they pin it to pure white, otherwise to pure black: <code>colour = 255 if colour &gt;= 127 else 0</code>.</p>

<p>But why restrict ourselves to monochromatic palettes? Instead of converting the image to grayscale before dithering, we could use any palette!</p>

<p><img src="https://obrhubr.org/assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002.webp" alt="Albrecht Dürer painting dithered in RGB, CMYK and a Gameboy-like palette." srcset="/assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002.webp 976w, /assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002-800.webp 800w, /assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002-720.webp 720w, /assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002-640.webp 640w, /assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002-480.webp 480w, /assets/dithering-in-colour/7d8b4f1ff8653e94a0db1013497a5002-320.webp 320w" sizes="(max-width: 800px) 100vw, (min-width: 801px) 800px" width="976" height="1200"/></p>

<p>To dither into “black and white”, we simply compared the scalar value of the pixel to a threshold. If we want to work with colours, we will have to account for all channels (red, green and blue values of the pixel). Instead of a simple comparison between two scalars, we have to find the closest colour 3d (colour) space.</p>

<p>For each distinct colour in the palette, the distance to the pixel’s colour is computed using euclidean distance. We also accumulate the error for each colour channel individually, similar to what is done in monochrome error diffusion dithering.</p>

<p><img src="https://obrhubr.org/assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b.webp" alt="Distance in 3d colour space." srcset="/assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b.webp 2262w, /assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b-800.webp 800w, /assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b-720.webp 720w, /assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b-640.webp 640w, /assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b-480.webp 480w, /assets/dithering-in-colour/c57d6c5d831cb40c5012fe0eaa8b254b-320.webp 320w" sizes="(max-width: 800px) 100vw, (min-width: 801px) 800px" width="2262" height="2255"/></p>

<p>If you want to play with dithering and different palettes yourself, check out <a href="http://ditherit.com/">ditherit.com</a>, which has a pretty nice web interface.</p>

<h2 id="linearising">Linearising</h2>

<p>We have just committed a mortal sin of image processing. I didn’t notice it, you might not have noticed either, but colour-space enthusiasts will be knocking on your door shortly.</p>

<p>First, we failed to linearise the sRGB input image, which results in overly bright dithered outputs. And second, we didn’t take into account human perception, as green is perceived brighter than red for example.</p>

<p>Images are usually stored in the sRGB colour space, which is gamma encoded. An issue arises when we want to quantitatively compare brightness in sRGB. Because it’s not a linear colour space, the difference in brightness going from <code>10</code> to <code>20</code> is not the same as from <code>100</code> to <code>110</code>, for example.</p>

<p><img src="https://obrhubr.org/assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46.webp" alt="Dithering a black-to-white gradient will be wrong without linearising first." srcset="/assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46.webp 987w, /assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46-800.webp 800w, /assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46-720.webp 720w, /assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46-640.webp 640w, /assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46-480.webp 480w, /assets/dithering-in-colour/20e8d36702ad74b4796e2902b80a2f46-320.webp 320w" sizes="(max-width: 800px) 100vw, (min-width: 801px) 800px" width="987" height="160"/></p>

<p>This means that dithering in sRGB directly will produce results that are too bright. Before dithering, we need to linearise - convert to a linear colour-space.</p>

<p><a href="https://surma.dev/things/ditherpunk/">Surma explains linearisation pretty well</a> and you should also check out <a href="https://stackoverflow.com/questions/596216/formula-to-determine-perceived-brightness-of-rgb-color/56678483#56678483">this stackoverflow answer</a>, which is very thorough. <a href="https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/">This post from John Novak</a> is the best explanation of gamma you can find and I recommend reading it.</p>

<p>If we also want to take human perception into account, we need to assign different weights to each channel. By scaling the colours before comparing, we preserve <a href="https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale">perceptual luminance</a>. The linked Wikipedia post lists the following values: <code>0.2126 R + 0.7152 G + 0.0722 B</code>.</p>

<p>If you want to play with a correct implementation, there is the <a href="https://github.com/makew0rld/dither">dither</a> library and the corresponding command line utility <a href="https://github.com/makew0rld/didder">didder</a> from <a href="https://github.com/makew0rld">makew0rld</a>. Check out the <a href="https://www.makeworld.space/2021/02/dithering.html">authors explanation about linearisation on his blog</a>.</p>

<p>If you want to play with my python implementation, <a href="https://github.com/obrhubr/ditherpy">check it out on GitHub</a>.</p>



<p><em>This has become more of a link collection than a post. But I hope that someone finds it helpful to have all resources and a basic explanation in one place… If you know more than me about colours and noticed any errors, please reach out!</em></p>





</main>


		</main></div></div>
  </body>
</html>
