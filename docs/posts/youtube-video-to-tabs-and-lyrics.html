<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/JoinMusic/fish">Original</a>
    <h1>YouTube Video to Tabs and Lyrics</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">YouTube video to chords, lyrics, beat and melody.</p>
<p dir="auto">A transformer-based hybrid multimodal model, various transformer models address different problems in the field of music information retrieval, these models generate corresponding information dependencies that mutually influence each other.</p>
<p dir="auto">An AI-powered multimodal project focused on music, generate chords, beats, lyrics, melody, and tabs for any song.</p>
<blockquote>
<p dir="auto">The online experience, <a href="https://lamucal.com" rel="nofollow">See the site here</a></p>
</blockquote>
 <p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JoinMusic/fish/blob/master/image/tnn.png"><img src="https://github.com/JoinMusic/fish/raw/master/image/tnn.png"/></a></p> 
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JoinMusic/fish/blob/master/image/model.png"><img src="https://github.com/JoinMusic/fish/raw/master/image/model.png"/></a></p>   
<p dir="auto"><code>U-Net</code> network model for audio source separation, <code>Pitch-Net</code>, <code>Beat-Net</code>, <code>Chord-Net</code> and <code>Segment-Net</code> based on the transformer model. Apart from establishing the correlation between the frequency and time, the most important aspect is to establish the mutual influence between different networks.</p>
<p dir="auto">The entire AI-powered process is implemented in <code>aitabs.py</code>, while the various network structure models can be referenced in the <code>models</code> folder.</p>
<blockquote>
<p dir="auto"><strong>Note</strong>: <code>U-Net</code> and <code>Segment-Net</code> use the stft spectrum of audio as input. <code>Beat-Net</code> uses three spectrograms of drums, bass, and other instruments as input,<code>Chord-Net</code> uses one spectrogram of the background music.</p>
</blockquote>

<ul dir="auto">
<li>
<p dir="auto"><strong>Chord</strong>, music chord detection, including major, minor, 7, maj7, min7, 6, m6, sus2, sus4, 5, and inverted chords. Determining the <strong>key</strong> of a song.</p>
</li>
<li>
<p dir="auto"><strong>Beat</strong>, music beat, downbeat detection and <strong>tempo</strong> (BPM) tracking</p>
</li>
<li>
<p dir="auto"><strong>Pitch</strong>, tracking the pitch of the melody in the vocal track.</p>
</li>
<li>
<p dir="auto"><strong>Music Structure</strong>, music segment boundaries and labels, include intro, verse, chorus, bridge and etc.</p>
</li>
<li>
<p dir="auto"><strong>Lyrics</strong>, music lyrics recognition and automatic lyrics to audio alignment, use ASR (whisper) to recognize the lyrics of the vocal track. The alignment of lyrics and audio is achieved through fine-tuning the wav2vec2 pre-training model. Currently, it supports dozens of languages, including English, Spanish, Portuguese, Russian, Japanese, Korean, Arabic, Chinese, and more.</p>
</li>
<li>
<p dir="auto"><strong>AI Tabs</strong>, Generate playable sheet music, including chord charts and six-line staves, using chords, beats, music structure information, lyrics, rhythm, etc. It supports editing functionalities for chords, rhythm, and lyrics.</p>
</li>
<li>
<p dir="auto"><strong>Other</strong>, audio source separation, speed adjustment, pitch shifting, etc.</p>
</li>
</ul>
<p dir="auto">For more AI-powered feature experiences, see the <a href="https://lamucal.com" rel="nofollow">website</a>:</p>

<p dir="auto">Using a combination of audio STFT, MFCC, and chroma features, with a Transformer model for timbre feature
modeling and high-level abstraction, this approach can maximize the avoidance of overfitting and underfitting
problems compared to using a single feature, and has better generalization capabilities. With a small amount of
data and minimal training, it can achieve better results.</p>
<blockquote>
<p dir="auto">The online experience, <a href="https://lamucal.com/ai-cover" rel="nofollow">See the site here</a></p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JoinMusic/fish/blob/master/image/net1.png"><img src="https://github.com/JoinMusic/fish/raw/master/image/net1.png"/></a></p>   
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/JoinMusic/fish/blob/master/image/net2.png"><img src="https://github.com/JoinMusic/fish/raw/master/image/net2.png"/></a></p>   
<p dir="auto">The model begins by processing the audio signal through a <code>U-Net</code>, which isolates the vocal track.
The vocal track is then simultaneously fed into <code>PitchNet</code> and <code>HuBERT</code> (Wav2Vec2). <code>PitchNet</code> is
responsible for extracting pitch features, while <code>HuBERT</code> captures detailed features of the vocals.</p>
<p dir="auto">The core of the model is <code>CombineNet</code>, which receives features from the <code>Features</code> module. This
module consists of three spectrograms: STFT, MFCC, and Chroma, each extracting different aspects
of the audio. These features are enhanced by the TimbreBlock before being passed to the Encoder.
During this process, noise is introduced via STFT transformation and combined with the features
before entering the Encoder for processing. The processed features are then passed to the Decoder,
where they are combined with the previous features to generate the final audio output.</p>
<p dir="auto"><code>CombineNet</code> is based on an encoder-decoder architecture and is trained to generate a mask that
is used to extract and replace the timbre, ultimately producing the final output audio.</p>
<p dir="auto">The entire AI-powered process is implemented in <code>run.py</code>, while the various network structure
models can be referenced in the <code>models</code> folder.</p>

<p dir="auto">The results of training on a 1-minute speech of Donald Trump are as follows:</p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p dir="auto"><strong>Train 10 epoch(Hozier&#39;s Too Sweet)</strong></p>
</td>
<td>
<p dir="auto"><strong>Train 100 epoch(Hozier&#39;s Too Sweet)</strong></p>
</td>
</tr>
<tr>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description te10_20s.webm">te10_20s.webm</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/5940737/357002677-992747d6-3e47-442c-ab63-0742c83933ee.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM3NDYzNDMsIm5iZiI6MTcyMzc0NjA0MywicGF0aCI6Ii81OTQwNzM3LzM1NzAwMjY3Ny05OTI3NDdkNi0zZTQ3LTQ0MmMtYWI2My0wNzQyYzgzOTMzZWUud2VibT9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE1VDE4MjA0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4NDhlMDczNjliNjk1MTJiZjQyNDhkM2E0NGJlNThiMDUzMjAxMzJlZWNhNTZmYjc3YjVjNWZhN2ZlZmE2NmUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.N_YfXZhz7uEMAgk08I6AzpWpuwm2wQcIU7IeiVE8YSo" data-canonical-src="https://private-user-images.githubusercontent.com/5940737/357002677-992747d6-3e47-442c-ab63-0742c83933ee.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM3NDYzNDMsIm5iZiI6MTcyMzc0NjA0MywicGF0aCI6Ii81OTQwNzM3LzM1NzAwMjY3Ny05OTI3NDdkNi0zZTQ3LTQ0MmMtYWI2My0wNzQyYzgzOTMzZWUud2VibT9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE1VDE4MjA0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc4NDhlMDczNjliNjk1MTJiZjQyNDhkM2E0NGJlNThiMDUzMjAxMzJlZWNhNTZmYjc3YjVjNWZhN2ZlZmE2NmUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.N_YfXZhz7uEMAgk08I6AzpWpuwm2wQcIU7IeiVE8YSo" controls="controls" muted="muted">

  </video>
</details>

</td>
<td>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description te100_20s.webm">te100_20s.webm</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/5940737/357002830-877d2cae-d7b7-4355-807f-424ada7df3a1.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM3NDYzNDMsIm5iZiI6MTcyMzc0NjA0MywicGF0aCI6Ii81OTQwNzM3LzM1NzAwMjgzMC04NzdkMmNhZS1kN2I3LTQzNTUtODA3Zi00MjRhZGE3ZGYzYTEud2VibT9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE1VDE4MjA0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3MTI2ODZjZjViN2EwZTRlMzBiYjBmNGZlZjZlN2I3MmIwNWNlMDMyZTE0ZjEzOTU2ZjFiN2E3ZTM2OTJkYTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2JCZwK7M01NsIoE6seVw-m8GHY4W9aKb2vr2Ob9ocGY" data-canonical-src="https://private-user-images.githubusercontent.com/5940737/357002830-877d2cae-d7b7-4355-807f-424ada7df3a1.webm?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjM3NDYzNDMsIm5iZiI6MTcyMzc0NjA0MywicGF0aCI6Ii81OTQwNzM3LzM1NzAwMjgzMC04NzdkMmNhZS1kN2I3LTQzNTUtODA3Zi00MjRhZGE3ZGYzYTEud2VibT9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODE1VDE4MjA0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI3MTI2ODZjZjViN2EwZTRlMzBiYjBmNGZlZjZlN2I3MmIwNWNlMDMyZTE0ZjEzOTU2ZjFiN2E3ZTM2OTJkYTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2JCZwK7M01NsIoE6seVw-m8GHY4W9aKb2vr2Ob9ocGY" controls="controls" muted="muted">

  </video>
</details>

</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto">You can experience creating your own voice online, <a href="https://lamucal.com/ai-cover" rel="nofollow">See the site here</a></p>
</article></div></div>
  </body>
</html>
