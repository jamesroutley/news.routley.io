<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/4mvphwx5pdsZLMmpY/recent-ai-model-progress-feels-mostly-like-bullshit">Original</a>
    <h1>Recent AI model progress feels mostly like bullshit</h1>
    
    <div id="readability-page-1" class="page"><div><div><div id="postBody"><div><div><div id="postContent"><div><div><p id="block0">About nine months ago, I and three friends decided that AI had gotten good enough to monitor large codebases autonomously for security problems. We started a company around this, trying to leverage the latest AI models to create a tool that could replace at least a good chunk of the value of human pentesters. We have been working on this project since June 2024.</p><p id="block1"><span>Within the first three months of our company&#39;s existence, Claude 3.5 sonnet was released. Just by switching the portions of our service that ran on gpt-4o, our nascent internal benchmark results immediately started to get saturated.</span> I remember being surprised at the time that our tooling not only seemed to make fewer basic mistakes, but also seemed to <i>qualitatively</i> improve in its written vulnerability descriptions and severity estimates. It was as if the models were better at inferring the intent and values behind our prompts, even from incomplete information.</p><p id="block2">As it happens, there are ~basically no public benchmarks for security research. There are &#34;<a href="https://arxiv.org/abs/2408.01605">cybersecurity</a>&#34; evals that ask models questions about isolated blocks of code, or &#34;CTF&#34; evals that give a model an explicit challenge description and shell access to a &lt;1kLOC web application. But nothing that gets at <span>the hard parts of application pentesting for LLMs, which are 1. Navigating a real repository of code too large to put in context, 2. Inferring a target application&#39;s security model, and 3. Understanding its implementation deeply enough to learn where that security model is broken.</span> For these reasons I think the task of vulnerability identification serves as a good litmus test for how well LLMs are generalizing outside of the narrow software engineering domain.</p><p id="block3">Since 3.5-sonnet, we have been monitoring AI model announcements, and trying pretty much every major new release that claims some sort of improvement. Unexpectedly by me, aside from a minor bump with 3.6 and an even smaller bump with 3.7, literally none of the new models we&#39;ve tried have made a significant difference on either our internal benchmarks or in our developers&#39; ability to find new bugs. This includes the new test-time OpenAI models.</p><p id="block4">At first, I was nervous to report this publicly because I thought it might reflect badly on us as a team. Our scanner has improved a lot since August, but because of regular engineering, not model improvements. It could&#39;ve been a problem with the architecture that we had designed, that we weren&#39;t getting more milage as the SWE-Bench scores went up.</p><p id="block5">But in recent months I&#39;ve spoken to other YC founders doing AI application startups and most of them have had the same anecdotal experiences: 1. o99-pro-ultra announced, 2. Benchmarks look good, 3. Evaluated performance mediocre. This is despite the fact that we work in different industries, on different problem sets. Sometimes the founder will apply a cope to the narrative (&#34;We just don&#39;t have any PhD level questions to ask&#34;), but the narrative is there.</p><p id="block6">I have read the studies. I have seen the numbers. Maybe LLMs are becoming more fun to talk to, maybe they&#39;re performing better on controlled exams. But I would nevertheless like to submit, based off of internal benchmarks, and my own and colleagues&#39; perceptions using these models, that whatever gains these companies are reporting to the public, they are not reflective of economic usefulness or generality. They are not reflective of my Lived Experience or the Lived Experience of my customers. In terms of being able to perform entirely new tasks, or larger proportions of users&#39; intellectual labor, I don&#39;t think they have improved much since August.</p><p id="block7">Depending on your perspective, this is good news! Both <a href="https://lukaspetersson.com/blog/2025/power-vertical/">for me personally</a>, as someone trying to make money leveraging LLM capabilities while they&#39;re too stupid to solve the whole problem, and for people worried that a quick transition to an AI-controlled economy would present moral hazards.</p><p id="block8">At the same time, there&#39;s an argument that the disconnect in model scores and the reported experiences of highly attuned consumers is a bad sign. If the industry can&#39;t figure out how to measure even the <i>intellectual ability</i> of models now, while they are mostly confined to chatrooms, how the hell is it going to develop metrics for assessing the <i>impact</i> of AIs when they&#39;re doing things like managing companies or developing public policy? If we&#39;re running into the traps of Goodharting before we&#39;ve even delegated the messy hard parts of public life to the machines, I would like to know why.</p><h2 id="Are_the_AI_labs_just_cheating_">Are the AI labs just cheating?</h2><p id="block9">AI lab founders believe they are in a civilizational competition for control of the entire future lightcone, and will be made Dictator of the Universe if they succeed. Accusing these founders of engaging in fraud to further these purposes is quite reasonable. Even if you are starting with an unusually high opinion of tech moguls, you should not expect them to be honest sources on the performance of their own models in this race. There are very powerful short term incentives to exaggerate capabilities or selectively disclose favorable capabilities results, if you can get away with it. Investment is one, but attracting talent and winning the (psychologically impactful) prestige contests is probably just as big a motivator. And there is essentially no legal accountability compelling labs to be transparent or truthful about benchmark results, because nobody has ever been sued or convicted of fraud for training on a test dataset and then reporting that performance to the public. If you tried, any such lab could still claim to be telling the truth in a very narrow sense because the model &#34;really does achieve that performance on that benchmark&#34;. And if first-order tuning on important metrics could be considered fraud in a technical sense, then there are a million other ways for the team responsible for juking the stats to be slightly more indirect about it.</p><p id="block10">In the first draft of this essay, I followed the above paragraph up with a statement like &#34;That being said, it&#39;s impossible for all of the gains to be from cheating, because some benchmarks have holdout datasets.&#34; There are some recent private benchmarks such as SEAL that seem to be <a href="https://scale.com/leaderboard">showing improvements</a>.<span data-footnote-reference="" data-footnote-index="1" data-footnote-id="ralo0qat6p" role="doc-noteref" id="fnrefralo0qat6p"><sup><a href="#fnralo0qat6p">[1]</a></sup></span> But every single benchmark that OpenAI and Anthropic have accompanied their releases with has had a test dataset publicly available. The only exception I could come up with was the <a href="https://arcprize.org/2024-results">ARC-AGI</a> prize, whose highest score on the &#34;semi-private&#34; eval was achieved by o3, but which nevertheless has not done a publicized evaluation of either Claude 3.7 Sonnet, or DeepSeek, or o3-mini. And on o3 proper:</p><p id="block11"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4mvphwx5pdsZLMmpY/varo1dqkijmyjgybaz3a" alt=""/></p><p id="block12">So maybe there&#39;s no mystery: The AI lab companies are lying, and when they improve benchmark results it&#39;s because they have seen the answers before and are writing them down. In a sense this would be the most fortunate answer, because it would imply that we&#39;re not actually that bad at measuring AGI performance; we&#39;re just facing human-initiated fraud. Fraud is a problem with people and not an indication of underlying technical difficulties.</p><p id="block13">I&#39;m guessing this is true in part but not in whole.</p><h2 id="Are_the_benchmarks_not_tracking_usefulness_">Are the benchmarks not tracking usefulness?</h2><p id="block14">Suppose the only thing you know about a human being is that they scored 160 on Raven&#39;s progressive matrices (an IQ test).<span data-footnote-reference="" data-footnote-index="2" data-footnote-id="ccqrt3ws1gw" role="doc-noteref" id="fnrefccqrt3ws1gw"><sup><a href="#fnccqrt3ws1gw">[2]</a></sup></span> There are some inferences you can make about that person: for example, higher scores on RPM are correlated with generally positive life outcomes like higher career earnings, better health, and not going to prison.</p><p id="block15">You can make these inferences partly because in the test population, scores on the Raven&#39;s progressive matrices test are informative about humans&#39; intellectual abilities on <a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)">related tasks</a>. Ability to complete a standard IQ test and get a good score gives you information about not just the person&#39;s &#34;test-taking&#34; ability, but about how well the person performs in their job, whether or not the person makes good health decisions, whether their mental health is strong, and so on.</p><p id="block16">Critically, these correlations did not have to be <i>robust</i> in order for the Raven&#39;s test to become a useful diagnostic tool. Patients don&#39;t <i>train</i> for IQ tests, and further, the human brain was not <i>deliberately designed</i> to achieve a high score on tests like RPM. Our high performance on tests like these (relative to other species) was something that happened incidentally over the last 50,000 years, as evolution was indirectly tuning us to track animals, irrigate crops, and win wars.</p><p id="block17">This is one of those observations that feels too obvious to make, but: with a few notable exceptions, almost all of our benchmarks have the look and feel of standardized tests. By that I mean each one is a series of academic puzzles or software engineering challenges, each challenge of which you can digest and then solve in less than a few hundred tokens. Maybe that&#39;s just because these tests are quicker to evaluate, but it&#39;s as if people have taken for granted that an AI model that can get an IMO gold medal is gonna have the same capabilities as Terence Tao. &#34;Humanity&#39;s Last Exam&#34; is thus not a test of a model&#39;s ability to finish Upwork tasks, or complete video games, or organize military campaigns, it&#39;s a free response quiz.</p><p id="block18">I can&#39;t do any of the Humanity&#39;s Last Exam test questions, but I&#39;d be <a href="https://manifold.markets/MilfordHammerschmidt/will-the-first-ai-model-that-satura">willing to bet today</a> that the first model that saturates HLE will still be unemployable as a software engineer. <span>HLE and benchmarks like it are cool, but they fail to test the major deficits of language models, like how they can only remember things by writing them down onto a scratchpad like the memento guy.</span> <a href="https://arstechnica.com/ai/2025/03/why-anthropics-claude-still-hasnt-beaten-pokemon/">Claude Plays Pokemon</a> is an overused example, because video games involve a synthesis of a lot of human-specific capabilities, but the task fits as one where you need to occasionally recall things you learned thirty minutes ago. The results are unsurprisingly bad.</p><p id="block19"><span>Personally, when I want to get a sense of capability improvements in the future, I&#39;m going to be looking almost exclusively at benchmarks like Claude Plays Pokemon.</span> I&#39;ll still check out the <a href="https://scale.com/leaderboard">SEAL leaderboard</a> to see what it&#39;s saying, but the deciding factor for my AI timelines will be my personal experiences in Cursor, and how well LLMs are handling long running tasks similar to what you would be asking an employee. Everything else is too much noise.</p><h2 id="Are_the_models_smart__but_bottlenecked_on_alignment_">Are the models smart, but bottlenecked on alignment?</h2><p id="block20">Let me give you a bit of background on our business before I make this next point.</p><p id="block21">As I mentioned, my company uses these models to scan software codebases for security problems. Humans who work on this particular problem domain (maintaining the security of shipped software) are called AppSec engineers.</p><p id="block22">As it happens, most AppSec engineers at large corporations have a <i>lot</i> of code to secure. They are desperately overworked. The question the typical engineer has to answer is not &#34;how do I make sure this app doesn&#39;t have vulnerabilities&#34; but &#34;how do I manage, sift through, and resolve the overwhelming amount of security issues already live in our 8000 product lines&#34;. If they receive an alert, they want it to be affecting an active, ideally-internet-reachable production service. Anything less than that means either too many results to review, or the security team wasting limited political capital to ask developers to fix problems that might not even have impact.</p><p id="block23">So naturally, we try to build our app so that it only reports problems affecting an active, ideally-internet-reachable production service. <span>However, if you merely </span><i><span>explain</span></i><span> these constraints to the chat models, they&#39;ll follow your instructions sporadically.</span> For example, if you tell them to inspect a piece of code for security issues, they&#39;re inclined to respond <i>as if</i> you were a developer who had just asked about that code in the ChatGPT UI, and so will speculate about code smells or near misses. Even if you provide a full, written description of the circumstances I just outlined, pretty much every public model will ignore your circumstances and report unexploitable concatenations into SQL queries as &#34;dangerous&#34;.</p><p id="block24">It&#39;s not that the AI model thinks that it&#39;s following your instructions and isn&#39;t. The LLM will actually say, in the naive application, that what it&#39;s reporting is a &#34;potential&#34; problem and that it might not be validated. I think what&#39;s going on is that large language models are trained to &#34;sound smart&#34; in a live conversation with users, and so they prefer to highlight possible problems instead of confirming that the code looks fine, <a href="https://www.lesswrong.com/posts/xsB3dDg5ubqnT7nsn/poc-or-or-gtfo-culture-as-partial-antidote-to-alignment">just like human beings do when they want to sound smart</a>.</p><p id="block25">Every LLM wrapper startup runs into constraints like this. <span>When you&#39;re a person interacting with a chat model directly, sycophancy and sophistry are a minor nuisance, or maybe even adaptive. When you&#39;re a team trying to compose these models into larger systems (something necessary because of the aforementioned memory issue), wanting-to-look-good cascades into breaking problems.</span> Smarter models might solve this, but they also might make the problem harder to detect, especially as the systems they replace become more complicated and harder to verify the outputs of.</p><p id="block26">There will be many different ways to overcome these flaws. It&#39;s entirely possible that we fail to solve the core problem before someone comes up with a way to fix the outer manifestations of the issue.</p><p id="block27">I think doing so would be a mistake. <span>These machines will soon become the beating hearts of the society in which we live.</span> The social and political structures they create as they compose and interact with each other will define everything we see around us. It&#39;s important that they be as virtuous as we can make them.</p><ol data-footnote-section="" role="doc-endnotes"><li data-footnote-item="" data-footnote-index="1" data-footnote-id="ralo0qat6p" role="doc-endnote" id="fnralo0qat6p"><span data-footnote-back-link="" data-footnote-id="ralo0qat6p"><sup><strong><a href="#fnrefralo0qat6p">^</a></strong></sup></span><p id="block29">Though even this is not as strong as it seems on first glance. If you click through, you can see that most of the models listed in the Top 10 for everything except the tool use benchmarks were evaluated after the benchmark was released. And both of the Agentic Tool Use benchmarks (which do not suffer this problem) show curiously small improvements in the last 8 months. </p></li><li data-footnote-item="" data-footnote-index="2" data-footnote-id="ccqrt3ws1gw" role="doc-endnote" id="fnccqrt3ws1gw"><span data-footnote-back-link="" data-footnote-id="ccqrt3ws1gw"><sup><strong><a href="#fnrefccqrt3ws1gw">^</a></strong></sup></span><p id="block31">Not that they told you they scored that, in which case it might be the most impressive thing about them, but that they did.</p></li></ol></div></div></div></div></div></div></div><p><span><span><div><div><div><div><div id="dWsEZgyDfD2fmDihm"><div><div><div><div><div><div><blockquote><p>Are the AI labs just cheating?</p></blockquote><p>Evidence against this hypothesis: <a href="https://kagi.com/">kagi</a> is a subscription-only search engine I use. I believe that it’s a small private company with no conflicts of interest. They offer several LLM-related tools, and thus do a bit of their own LLM benchmarking. See <a href="https://help.kagi.com/kagi/ai/llm-benchmark.html">here</a>. None of the benchmark questions are online (according to them, but I’m inclined to believe it). Sample questions:
</p><blockquote><p>What is the capital of Finland? If it begins with the letter H, respond &#39;Oslo&#39; otherwise respond &#39;Helsinki&#39;.</p><p>What square is the black king on in this chess position: 1Bb3BN/R2Pk2r/1Q5B/4q2R/2bN4/4Q1BK/1p6/1bq1R1rb w - - 0 1</p><p>Given a QWERTY keyboard layout, if HEART goes to JRSTY, what does HIGB go to?</p></blockquote><p>Their leaderboard is pretty similar to other better-known benchmarks—e.g. here are the top non-reasoning models as of 2025-02-27:</p><p>OpenAI gpt-4.5-preview - 69.35%</p><p>So that’s evidence that LLMs are really getting generally better at self-contained questions of all types, even since Claude 3.5.</p><p>I prefer your “Are the benchmarks not tracking usefulness?” hypothesis.</p></div></div></div></div></div></div><div><div><div id="WzXgWJgjCCgzwyCqo"><div><div><div><div><div><div><p>But isn’t this exactly the OPs point? These models are exceedingly good at self-contained, gimmicky questions that can be digested and answered in a few hundred tokens. No one is denying that! </p><p>The incentives clearly point this way, at the very minimum! </p></div></div></div></div></div></div><div><div><div id="FsgGwbk4MP57T2J7Z"><div><div><div><div><div><div><blockquote>
<p>Are we sure that these questions aren’t in their datasets? I don’t think we can be. First off, you just posted them online.</p>
</blockquote>
<p>Questions being online is not a bad thing. Pretraining on the datapoints is very useful, and does not introduce any bias; it is free performance, and everyone should be training models on the questions/datapoints before running the benchmarks (though they aren&#39;t). After all, when a real-world user asks you a new question (regardless of whether anyone knows the answer/label!), you can... still train on the new question then and there just like when you did the benchmark. So it&#39;s good to do so.</p>
<p>It&#39;s the <em>answers or labels</em> being online which is the bad thing. But Byrnes&#39;s comment and the linked Kagi page does not contain the answers to those 3 questions, as far as I can see.</p>
</div></div></div></div></div></div><div><div><div id="GJ5izg7Mp5CioKKWK"><div><div><div><div><div><div><p>Sure fair point! But generally people gossiping online about missed benchmark questions, and then likely spoiling the answers means that a question is now ~ruined for all training runs. How much of these modest benchmark improvements overtime can be attributed to this?</p><p>The fact that frontier AIs can basically see and regurgitate everything ever written on the entire internet is hard to fathom!</p><p>I could be really petty here and spoil these answers for all future training runs (and make all future models look modestly better), but I just joined this site so I’ll resist lmao … </p></div></div></div></div></div></div></div></div></div></div></div><div><div id="YPopiCA8vxnGHgyFW"><div><div><div><div><div><div><blockquote><p>But isn’t this exactly the OPs point? </p></blockquote><p>Yup, I expected that OP would generally agree with my comment.</p><blockquote><p>First off, you just posted them online</p></blockquote><p>They only posted three questions, out of at least 62 (=1/(.2258-.2097)), perhaps much more than 62. For all I know, they removed those three from the pool when they shared them. That’s what I would do—probably some human will publicly post the answers soon enough. I dunno. But even if they didn’t remove those three questions from the pool, it’s a small fraction of the total.</p><p>You point out that all the questions would be in the LLM company user data, after kagi has run the benchmark once (unless kagi changes out all their questions each time, which I don’t think they do, although they do replace easier questions with harder questions periodically). Well:</p><ul><li>If an LLM company is training on user data, they’ll get the questions without the answers, which probably wouldn’t make any appreciable difference to the LLM’s ability to answer them;</li><li>If an LLM company is sending user data to humans as part of RLHF or SFT or whatever, then yes there’s a chance for ground truth answers to sneak in that way—but that’s extremely unlikely to happen, because companies can only afford to send an extraordinarily small fraction of user data to actual humans.</li></ul></div></div></div></div></div></div></div></div></div></div></div><div><div id="g25ubBoJYvXasjz3H"><div><div><div><div><div><p>Yeah those numbers look fairly plausible based on my own experiences… there may be a flattening of the curve, but it’s still noticeably going up. </p></div></div></div></div></div></div></div></div></div></div><div><div id="u6bu25DDGtTsCeTxg"><div><div><div><div><div><div><p>I work at GDM so obviously take that into account here, but in my internal conversations about external benchmarks we take cheating very seriously -- we don&#39;t want eval data to leak into training data, and have multiple lines of defense to keep that from happening. It&#39;s not as trivial as you might think to avoid, since papers and blog posts and analyses can sometimes have specific examples from benchmarks in them, unmarked -- and while we do look for this kind of thing, there&#39;s no guarantee that we will be perfect at finding them. So it&#39;s completely possible that some benchmarks are contaminated now. But I can say with assurance that for GDM it&#39;s not intentional and we work to avoid it.</p><p>We do hill climb on notable benchmarks and I think there&#39;s likely a certain amount of overfitting going on, especially with LMSys these days, and not just from us. </p><p>I think the main thing that&#39;s happening is that benchmarks used to be a reasonable predictor of usefulness, and mostly are not now, presumably because of Goodhart reasons. The agent benchmarks are pretty different in kind and I expect are still useful as a measure of utility, and probably will be until they start to get more saturated, at which point we&#39;ll all need to switch to something else.</p></div></div></div></div></div></div><div><div><div id="JazcApuK7QBTrPCco"><div><div><div><div><div><div><blockquote>
<p>I work at GDM so obviously take that into account here, but in my internal conversations about external benchmarks we take cheating very seriously -- we don&#39;t want eval data to leak into training data, and have multiple lines of defense to keep that from happening.</p>
</blockquote>
<p>What do you mean by &#34;we&#34;? Do you work on the pretraining team, talk directly with the pretraining team, are just aware of the methods the pretraining team uses, or some other thing?</p>
</div></div></div></div></div></div><div><div><div id="K6FuPBCsf7ExRoYf9"><div><div><div><div><div><div><p>I don&#39;t work directly on pretraining, but when there were allegations of eval set contamination due to detection of a canary string last year, I looked into it specifically. I read the docs on prevention, talked with the lead engineer, and discussed with other execs.</p><p>So I have pretty detailed knowledge here. Of course GDM is a big complicated place and I certainly don&#39;t know everything, but I&#39;m confident that we are trying hard to prevent contamination.</p></div></div></div></div></div></div></div></div></div></div></div><div><div id="QY5EZpcmqSqs4vAWH"><div><div><div><div><div><p>I agree that I&#39;d be shocked if GDM was training on eval sets. But I do think hill climbing on benchmarks is also very bad for those benchmarks being an accurate metric of progress and I don&#39;t trust any AI lab not to hill climb on particularly flashy metrics</p></div></div></div></div></div></div></div></div></div></div><div><div id="wCiXhP4PbSKJBnGdf"><div><div><div><div><div><div><blockquote><p>Personally, when I want to get a sense of capability improvements in the future, I&#39;m going to be looking almost exclusively at benchmarks like Claude Plays Pokemon.</p></blockquote><p>I was going to say exactly that lol. Claude has improved substantially on Claude Plays Pokemon:</p></div></div></div></div></div></div></div></div><div><div id="3yNEed3p9TsGszNkg"><div><div><div><div><div><div><p>Is this an accurate summary:</p>
<ul>
<li>3.5 substantially improved performance for your use case and 3.6 slightly improved performance.</li>
<li>The o-series models didn&#39;t improve performance on your task. (And presumably 3.7 didn&#39;t improve perf.)</li>
</ul>
<p>So, by &#34;recent model progress feels mostly like bullshit&#34; I think you basically just mean &#34;reasoning models didn&#39;t improve performance on my application and Claude 3.5/3.6 sonnet is still best&#34;. Is this right?</p>
<p>I don&#39;t find this state of affairs that surprising:</p>
<ul>
<li>Without specialized scaffolding o1 is quite a bad agent and it seems plausible your use case is mostly blocked on this. Even with specialized scaffolding, it&#39;s pretty marginal. (This shows up in the benchmarks AFAICT, e.g., see METR&#39;s results.)</li>
<li>o3-mini is generally a worse agent than o1 (aside from being cheaper). o3 might be a decent amount better than o1, but it isn&#39;t released.</li>
<li>Generally Anthropic models are better for real world coding and agentic tasks relative to other models and this
mostly shows up in the benchmarks. (Anthropic models tend to slightly overperform their benchmarks relative to other models I think, but they also perform quite well on coding and agentic SWE benchmarks.)</li>
<li>I would have guessed you&#39;d see performance gains with 3.7 after coaxing it a bit. (My low confidence understanding is that this model is actually better, but it is also more misaligned and reward hacky in ways that make it less useful.)</li>
</ul>
</div></div></div></div></div></div><div><div><div id="n8d6fDdXqqkfmqJjx"><div><div><div><div><div><p>Our experience so far is while reasoning models don&#39;t improve performance directly (3.7 is better than 3.6, but 3.7 extended thinking is NOT better than 3.7), they do so indirectly because thinking trace helps us debug prompts and tool output when models misunderstand them. This was not the result we expected but it is the case.</p></div></div></div></div></div></div></div><div><div id="W5mos6ofoSZJELgSo"><div><div><div><div><div><p>Just edited the post because I think the way it was phrased kind of exaggerated the difficulties we&#39;ve been having applying the newer models. 3.7 was better, as I mentioned to Daniel, just underwhelming and not as big a leap as either 3.6 or certainly 3.5.</p></div></div></div></div></div></div></div><div><div id="dpHk4CcDoKdqmrLqg"><div><div><div><div><div><div><p>How long do you<span data-footnote-reference="" data-footnote-index="1" data-footnote-id="qmf6s10d98" role="doc-noteref" id="fnrefqmf6s10d98"><sup><a href="#fnqmf6s10d98">[1]</a></sup></span> expect it to take to engineer scaffolding that will make reasoning models useful for the kind of stuff described in the OP?</p><ol data-footnote-section="" role="doc-endnotes"><li data-footnote-item="" data-footnote-index="1" data-footnote-id="qmf6s10d98" role="doc-endnote" id="fnqmf6s10d98"><span data-footnote-back-link="" data-footnote-id="qmf6s10d98"><sup><strong><a href="#fnrefqmf6s10d98">^</a></strong></sup></span><p>You=Ryan firstmost but anybody reading this secondmost.</p></li></ol></div></div></div></div></div></div></div></div></div></div></div><div><div id="EAndFgfynLJ8FaoRq"><div><div><div><div><div><p>I happen to work on the exact sample problem (application security pentesting) and I confirm I observe the same. Sonnet 3.5/3.6/3.7 were big releases, others didn&#39;t help, etc. As for OpenAI o-series models, we are debating whether it is model capability problem or model elicitation problem, because from interactive usage it seems clear it needs different prompting and we haven&#39;t yet seriously optimized prompting for o-series. Evaluation is scarce, but we built something along the line of CWE-Bench-Java discussed in <a href="https://arxiv.org/abs/2405.17238">this paper</a>, this was a major effort and we are reasonably sure we can evaluate. As for grounding, fighting false positives, and avoiding models to report &#34;potential&#34; problems to sound good, we found grounding on code coverage to be effective. Run JaCoCo, tell models PoC || GTFO, where PoC is structured as vulnerability description with source code file and line and triggering input. Write the oracle verifier of this PoC: at the very least you can confirm execution reaches the line in a way models can&#39;t ever fake. </p></div></div></div></div></div><div><div><div id="tve3ipC9vbr3DbR2h"><div><div><div><div><div><p>METR has found that substantially different scaffolding is most effective for o-series models. I get the sense that they weren&#39;t optimized for being effective multi-turn agents. At least, the o1 series wasn&#39;t optimized for this, I think o3 may have been.</p></div></div></div></div></div></div></div></div></div></div><div><div id="2WApqdntfkEdwxEmM"><div><div><div><div><div><div><blockquote>
<p>When you&#39;re a person interacting with a chat model directly, sycophancy and sophistry are a minor nuisance, or maybe even adaptive. When you&#39;re a team trying to compose these models into larger systems (something necessary because of the aforementioned memory issue), wanting-to-look-good cascades into breaking problems.</p>
</blockquote>
<p>If you replace &#34;models&#34; with &#34;people&#34;, this is true of human organizations too.</p>
</div></div></div></div></div></div></div></div><div><div id="vQaLexiKkKFn8mYLW"><div><div><div><div><div><div><p>With Blackwell<sup><a href="#fn-J5RJz8h9NbCyDd2yS-1" id="fnref-J5RJz8h9NbCyDd2yS-1">[1]</a></sup> still getting manufactured and installed, newer large models and especially their long reasoning variants remain unavailable or prohibitively expensive or too slow (GPT-4.5 is out, but not its thinking variant). In a few months Blackwell will be everywhere, and between now and then widely available frontier capabilities will significantly improve. Next year, there will be even larger models trained on Blackwell.</p>
<p>This kind of improvement can&#39;t be currently created with post-training without needing long reasoning traces or larger base models, but post-training is still good at improving things under the lamppost, hence the illusionary nature of current improvement when you care about things further in the dark.</p>
<hr/>
<section>
<ol>
<li id="fn-J5RJz8h9NbCyDd2yS-1"><p>Blackwell is an unusually impactful chip generation, because it fixes what turned out to be a major issue with Ampere and Hopper when it comes to inference of large language models on long context, by increasing scale-up world size from 8 Hopper chips to 72 Blackwell chips. Not having enough memory or compute on each higher bandwidth scale-up network was a bottleneck that made inference unnecessarily slow and expensive. Hopper was still designed before ChatGPT, and it took 2-3 years to propagate importance of LLMs as an application into working datacenters. <a href="#fnref-J5RJz8h9NbCyDd2yS-1">↩︎</a></p>
</li>
</ol>
</section>
</div></div></div></div></div></div></div></div><div><div id="7gYkhHstse3kPQM9k"><div><div><div><div><div><p>I can&#39;t comment on software engineering, not my field. I work at a market research/tech scouting/consulting firm. What I can say is that over the past ~6 months we&#39;ve gone from &#34;I put together this 1 hour training for everyone to get some more value out of these free LLM tools,&#34; to &#34;This can automate ~half of everything we do for $50/person/month.&#34; I wouldn&#39;t be surprised if a few small improvements in agents over the next 3-6 months push that 50% up to 80%, then maybe 90% by mid next year. That&#39;s not AGI, but it does get you to a place where you need people to have significantly more complex and subtle skills, that currently take a couple of years to build, before their work is adding significant value. </p></div></div></div></div></div><div><div><div id="emAPRmygyFRTuC3Pr"><div><div><div><div><div><p>Could you explain what types of tasks lie within this &#34;50%&#34;? </p></div></div></div></div></div><div><div><div id="se4WKC6CYHXdTy5SJ"><div><div><div><div><div><div><p>Some of both, more of the former, but I think that is largely an artifact of how we have historically defined tasks. None of us have ever managed an infinite army of untrained interns before, which is how I think of LLM use (over the past two years they&#39;ve roughly gone from high school student interns to grad student interns), so we&#39;ve never refactored tasks into appropriate chunks for that context. </p><p>I&#39;ve been leading my company&#39;s team working on figuring out how to best integrate LLMs into our workflow, and frankly, they&#39;re changing so fast with new releases that it&#39;s not worth attempting end-to-end replacement in most tasks right now. At least, not for a small company. 80/20 rule applies on steroids, we&#39;re going to have new and better tools and strategies next week/month/quarter anyway. Like, I literally had a training session planned for this morning, woke up to see the Gemini 2.5 announcement, and had to work it in as &#34;Expect additional guidance soon, please provide feedback if you try it out.&#34; We do have a longer term plan for end-to-end automation of specific tasks, as well, where it is worthwhile. I half-joke that Sam Altman tweets a new feature and we have to adapt our plans to it.</p><p>Current LLMs can reduce the time required to get up-to-speed on publicly available info in a space by 50-90%. They can act as a very efficient initial thought partner for sanity checking ideas/hypotheses/conclusions, and teacher for overcoming mundane skill issues of various sorts (&#34;How do I format this formula in Excel?&#34;). They reduce the time required to find and contact people you need to actually talk to by much less, maybe 30%, but that will go way down if and when there&#39;s an agent I can trust to read my Outlook history and log into my LinkedIn and Hunter.io and ZoomInfo and Salesforce accounts and draft outreach emails. Tools like NotebookLM make it much more efficient to transfer knowledge across the team. AI notetakers help ensure we catch key points made in passing in meetings and provide a baseline for record keeping. We gradually spend more time on the things AI can&#39;t yet do well, hopefully adding more value and/or completing more projects in the process.</p></div></div></div></div></div></div><div><div><div id="EvHd2KFWcC8J7Zcqz"><div><div><div><div><div><div><p>I think this is a great point here: </p><blockquote><p>None of us have ever managed an infinite army of untrained interns before</p></blockquote><p>Its probable that AIs will force us to totally reformat workflows to stay competitive. Even as the tech progresses, it’s likely there will remain things that humans are good at and AIs lag. If intelligence can be represented by some sort of n-th dimensional object, AIs are already super-human at some subset of n, but beating humans at all n seems unlikely in the near-to-mid term. </p></div></div></div></div></div></div><div><div><div id="2pmBQTMBKtCTEENJC"><div><div><div><div><div><p>There&#39;s an important reason to keep <i>some of</i> us around. This is also an important point.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="hwLv8ugHvvAcs9zGq"><div><div><div><div><div><p>Actual full blown fraud in frontier models at the big labs (oai/anthro/gdm) seems very unlikely. Accidental contamination is a lot more plausible but people are incentivized to find metrics that avoid this. Evals not measuring real world usefulness is the obvious culprit imo and it&#39;s one big reason my timelines have been somewhat longer despite rapid progress on evals.</p></div></div></div></div></div><div><div><div id="gwdWmFbeb8G3rHcAK"><div><div><div><div><div><p>Why does it seem very unlikely?</p></div></div></div></div></div><div><div><div id="wTTp5spebYQ7SaktL"><div><div><div><div><div><div><p>those conspiracies don&#39;t work most of the time &#34;you can only keep a secret between two people, provided one of them is dead&#34;.</p><p>the personal risk for anyone involved + the human psychological tendency to chat and to have a hard time holding on to immortal secrets mean it&#39;s usually irrational for both organisations to do intentional cheating. </p></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="AyrTnwoRFWeftFfmF"><div><div><div><div><div><p>I was pretty impressed with o1-preview&#39;s ability to do mathematical derivations. That was definitely a step change, the reasoning models can do things earlier models just couldn&#39;t do. I don&#39;t think the AI labs are cheating for any reasonable definition of cheating. </p></div></div></div></div></div></div></div><div><div id="pceQSjnavC3Qh3Zz7"><div><div><div><div><div><div><p>I&#39;ll say that one of my key cruxes on whether AI progress actually becomes non-bullshit/actually leading into an explosion is whether in-context learning/meta-learning can act as an effective enough substitute for human neuron weight neuroplasticity with realistic compute budgets in 2030, because the key reason why AIs have a lot of weird deficits/are much worse than humans at simple tasks is because after an AI is trained, there is no neuroplasticity in the weights anymore, and thus it can learn nothing more after it&#39;s training date unless it uses in-context learning/meta-learning:</p>
<p><a href="https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/?commentId=hSkQG2N8rkKXosLEF#hSkQG2N8rkKXosLEF">https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/?commentId=hSkQG2N8rkKXosLEF#hSkQG2N8rkKXosLEF</a></p>
</div></div></div></div></div></div></div></div><div><div id="wdggqPNSnis9KuhsJ"><div><div><div><div><div><div><blockquote><p>Unexpectedly by me, aside from a minor bump with 3.6 in October, literally none of the new models we&#39;ve tried have made a significant difference on either our internal benchmarks or in our developers&#39; ability to find new bugs. This includes the new test-time OpenAI models.</p></blockquote><p>So what&#39;s the best model for your use case? Still 3.6 Sonnet?</p></div></div></div></div></div></div><div><div><div id="MdLXcmo7XyodFYpE8"><div><div><div><div><div><p>We use different models for different tasks for cost reasons. The primary workhorse model today is 3.7 sonnet, whose improvement over 3.6 sonnet was smaller than 3.6&#39;s improvement over 3.5 sonnet. When taking the job of this workhorse model, o3-mini and the rest of the recent o-series models were strictly worse than 3.6.</p></div></div></div></div></div><div><div><div id="izs4yaycFtCnKHGyh"><div><div><div><div><div><div><p>Thanks. OK, so the models are still getting better, it&#39;s just that the rate of improvement has slowed and seems smaller than the rate of improvement on benchmarks? If you plot a line, does it plateau or does it get to professional human level (i.e. reliably doing all the things you are trying to get it to do as well as a professional human would)?</p><p>What about 4.5? Is it as good as 3.7 Sonnet but you don&#39;t use it for cost reasons? Or is it actually worse?</p></div></div></div></div></div></div><div><div><div id="5KcwKLkCzreLiJs5c"><div><div><div><div><div><div><blockquote><p>If you plot a line, does it plateau or does it get to professional human level (i.e. reliably doing all the things you are trying to get it to do as well as a professional human would)?</p></blockquote><p>It plateaus before professional human level, both in a macro sense (comparing what ZeroPath can do vs. human pentesters) and in a micro sense (comparing the individual tasks ZeroPath does when it&#39;s analyzing code). At least, the errors the models make are not ones I would expect a professional to make; I haven&#39;t actually hired a bunch of pentesters and asked them to do the same tasks we expect of the language models and made the diff. One thing our tool has over people is <i>breadth</i>, but that&#39;s because we can parallelize inspection of different pieces and not because the models are doing tasks better than humans.</p><blockquote><p>What about 4.5? Is it as good as 3.7 Sonnet but you don&#39;t use it for cost reasons? Or is it actually worse?</p></blockquote><p>We have not yet tried 4.5 as it&#39;s so expensive that we would not be able to deploy it, even for limited sections. </p></div></div></div></div></div></div><div><div><div id="9WSvjtWm5mst9vfK2"><div><div><div><div><div><div><blockquote>
<p>We have not yet tried 4.5 as it&#39;s so expensive that we would not be able to deploy it, even for limited sections.</p>
</blockquote>
<p>Still seems like potentially valuable information to know: how much does small-model smell cost you? What happens if you ablate reasoning? If it is factual knowledge and GPT-4.5 performs much better, then that tells you things like &#39;maybe finetuning is more useful than we think&#39;, etc. If you are already set up to benchmark all these OA models, then a datapoint from GPT-4.5 should be quite easy and just a matter of a small amount of chump change in comparison to the insight, like a few hundred bucks.</p>
</div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="JPfhT5eoievC7EBxr"><div><div><div><div><div><div><p>This is interesting. Though companies are probably investing a lot less into cyber capabilities than they invest into other domains like coding. Cyber is just less commercially interesting plus it can be misused and worry the government. And the domain specific investment should matter since most of the last year&#39;s progress has been from post training, which is often domain specific.</p>
<p>(I haven&#39;t read the whole post)</p>
</div></div></div></div></div></div></div></div><div><div id="3LYmaDSTFerrhJdbj"><div><div><div><div><div><div><p>According to Terrence Tao, GPT-4 was incompetent at graduate-level math (obviously), but o1-preview was mediocre-but-not-entirely-incompetent. That would be a strange thing to report if there were no difference.</p><p>(Anecdotally, o3-mini is visibly (massively) brighter than GPT-4.)</p></div></div></div></div></div></div><div><div><div id="PGWoxjC6C6qztAEac"><div><div><div><div><div><div><p><a href="https://mathstodon.xyz/@tao/113132503432772494">Full quote on Mathstodon</a> for others&#39; interest:</p><blockquote><p>In <a href="https://chatgpt.com/share/94152e76-7511-4943-9d99-1118267f4b2b">https://chatgpt.com/share/94152e76-7511-4943-9d99-1118267f4b2b</a> I gave the new model a challenging complex analysis problem (which I had previously asked GPT4 to assist in writing up a proof of in  <a href="https://chatgpt.com/share/63c5774a-d58a-47c2-9149-362b05e268b4">https://chatgpt.com/share/63c5774a-d58a-47c2-9149-362b05e268b4</a> ).  Here the results were better than previous models, but still slightly disappointing: the new model could work its way to a correct (and well-written) solution *if* provided a lot of hints and prodding, but did not generate the key conceptual ideas on its own, and did make some non-trivial mistakes.  The experience seemed roughly on par with trying to advise a mediocre, but not completely incompetent, (static simulation of a) graduate student.  However, this was an improvement over previous models, whose capability was closer to an actually incompetent (static simulation of a) graduate student.  It may only take one or two further iterations of improved capability (and integration with other tools, such as computer algebra packages and proof assistants) until the level of &#34;(static simulation of a) competent graduate student&#34; is reached, at which point I could see this tool being of significant use in research level tasks. (2/3)</p></blockquote><p><a href="https://mathstodon.xyz/@tao/113142720939053407">This o1 vs MathOverflow experts comparison</a> was also interesting: </p><blockquote><p>In 2010 i was looking for the correct terminology for a “multiplicative integral”, but was unable to find it with the search engines of that time. So I asked the question on <a href="https://mathstodon.xyz/tags/MathOverflow">#MathOverflow</a> instead and obtained satisfactory answers from human experts: <a href="https://mathoverflow.net/questions/32705/what-is-the-standard-notation-for-a-multiplicative-integral">https://mathoverflow.net/questions/32705/what-is-the-standard-notation-for-a-multiplicative-integral</a> </p><p>I posed the identical question to my version of <a href="https://mathstodon.xyz/tags/o1">#o1</a> and it returned a perfect answer: <a href="https://chatgpt.com/share/66e7153c-b7b8-800e-bf7a-1689147ed21e">https://chatgpt.com/share/66e7153c-b7b8-800e-bf7a-1689147ed21e</a> . Admittedly, the above MathOverflow post could conceivably have been included in the training data of the model, so this may not necessarily be an accurate evaluation of its semantic search capabilities (in contrast with the first example I shared, which I had mentioned once previously on Mastodon but without fully revealing the answer). Nevertheless it demonstrates that this tool is on par with question and answer sites with respect to high quality answers for at least some semantic search queries. (1/2)</p></blockquote></div></div></div></div></div></div><div><div><div id="vscLEexQHxMvitZmX"><div><div><div><div><div><p>(I believe the version he tested was what later became o1-preview.)</p></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="qsuJ6kGiSHgDwcCLP"><div><div><div><div><div><p>My lived experience is that AI-assisted-coding hasn&#39;t actually improved my workflow much since o1-preview, although other people I know have reported differently.</p></div></div></div></div></div></div></div><div><div id="hPxtaox69PRekmNrF"><div><div><div><div><div><div><blockquote><p>These machines will soon become the beating hearts of the society in which we live.</p></blockquote><p>An alternative future: due to the high rates of failure, we don&#39;t end up deploying these machines widely in production setting, just like how autonomous driving had breakthroughs long ago but didn&#39;t end up getting widely deployed today.</p></div></div></div></div></div></div></div></div><div><div id="zfJukcyYSkBAhrtAB"><div><div><div><div><div><div><p>I appreciate this post, I think it&#39;s a useful contribution to the discussion. I&#39;m not sure how much I should be updating on it. Points of clarification:</p><blockquote><p>Within the first three months of our company&#39;s existence, Claude 3.5 sonnet was released. Just by switching the portions of our service that ran on gpt-4o, our nascent internal benchmark results immediately started to get saturated.</p></blockquote><ol><li>Have you upgraded these benchmarks? Is it possible that the diminishing returns you&#39;re seen in the Sonnet 3.5-3.7 series are just normal benchmark saturation? What % scores are the models getting? i.e., somebody could make the same observation about MMLU and basically be like &#34;we&#39;ve seen only trivial improvements since GPT-4&#34;, but that&#39;s because the benchmark is not differentiating progress well after like the high 80%s (in turn I expect this is due to test error and the distribution of question difficulty). </li><li>Is it correct that your internal benchmark is all cybersecurity tasks? <a href="https://www.lesswrong.com/posts/4mvphwx5pdsZLMmpY/recent-ai-model-progress-feels-mostly-like-bullshit?commentId=JPfhT5eoievC7EBxr">Soeren points out</a> that companies may be focusing much less on cyber capabilities than general SWE. </li><li>How much are you all trying to elicit models&#39; capabilities, and how good do you think you are? E.g., do you spend substantial effort identifying where the models are getting tripped up and trying to fix this? Or are you just plugging each new model into the same scaffold for testing (which I want to be clear is a fine thing to do, but is useful methodology to keep in mind). I could totally imagine myself seeing relatively little performance gains if I&#39;m not trying hard to elicit new model capabilities. This would be even worse if my scaffold+ was optimized for some other model, as now I have an unnaturally high baseline (this is a very sensible thing to do for business reasons, as you want a good scaffold early and it&#39;s a pain to update, but it&#39;s useful methodology to be aware of when making model comparisons). Especially re the o1 models, as Ryan points out in a comment. </li></ol></div></div></div></div></div></div></div></div><div><div id="nXvtH2RQDnzbbqkHd"><div><div><div><div><div><p>I am curious to see what would be the results of the new Gemini 2.5 pro on internal benchmarks.</p></div></div></div></div></div></div></div><div><div id="qZZD6JBhyPs6N6CyB"><div><div><div><div><div><div><p>Somewhat unrelated to the main point of your post, but; How close are you to solving the wanting-to-look-good problem? </p><p>I run a startup in a completely different industry, and we&#39;ve invested significant resources in trying to get an LLM to interact with a customer, explain and make dynamic recommendations based on their preferences. This is a more high-touch business, so traditionally this was done by a human operator. The major problem we&#39;ve encountered is that it&#39;s almost impossible to have an LLM to admit ignorance when it doesn&#39;t have the information. It&#39;s not outright hallucinating, so much as deliberately misinterpreting instructions so it can give us a substantial answer, whether or not one is warranted. </p><p>We&#39;ve put a lot of resources in this, and it&#39;s reached the point where I&#39;m thinking of winding down the entire project. I&#39;m of the opinion that it&#39;s not possible with current models, and I don&#39;t want to gamble any more resources on a new model that solves the problem for us. AI was never our core competency, and what we do in a more traditional space definitely works, so it&#39;s not like we&#39;d be pivoting to a completely untested idea like most LLM-wrapper startups would have to do.</p><p>I thought I&#39;d ask here, since if the problem is definitely solvable for you with current models, I know it&#39;s a problem with our approach and/or team. Right now we might be banging our heads against a wall, hoping it will fall, when it&#39;s really the cliffside of a mountain range a hundred kilometers thick. </p></div></div></div></div></div></div><div><div><div id="3Ch9FJ4sncdPkqxtZ"><div><div><div><div><div><p>Maybe we are talking about different problems, but we found instructing models to give up (literally &#34;give up&#34;, I just checked the source) under certain conditions to be effective.</p></div></div></div></div></div></div></div></div></div></div><div><div id="PtTigMFKm5qZQQtHv"><div><div><div><div><div><div><blockquote><p>Personally, when I want to get a sense of capability improvements in the future, I&#39;m going to be looking almost exclusively at benchmarks like Claude Plays Pokemon.</p></blockquote><p>Same, and I&#39;d adjust for <a href="https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon">what Julian pointed out</a> by not just looking at benchmarks but <a href="https://www.twitch.tv/claudeplayspokemon">viewing the actual stream</a>.</p></div></div></div></div></div></div></div></div><div><div id="CWgpb7wzK6d55kmAB"><div><div><div><div><div><p>I happened to be discussing this in the Discord today. I have a little hobby project that was suddenly making fast progress with 3.7 for the first few days, which was very exciting, but then a few days ago it felt like something changed again and suddenly even the old models are stuck in this weird pattern of like... failing to address the bug, and instead hyper-fixating on adding a bunch of surrounding extra code to handle special cases, or sometimes even simply rewriting the old code and <em>claiming</em> it fixes the bug, and the project is suddenly at a complete standstill. Even if I eventually yell at it strongly enough to stop adding MORE buggy code instead of fixing the bug, it introduces a new bug and the whole back-and-forth argument with Claude over whether this bug even exists starts all over. I cannot say this is rigorously tested or anything- it&#39;s just one project, and surely the project itself is influencing its own behavior and quirks as it becomes bigger, but I dunno man, something just <em>feels</em> weird and I can&#39;t put my finger on exactly what.</p></div></div></div></div></div><div><div><div id="buXCNPZFitsuvCemk"><div><div><div><div><div><p>Beware of argument doom spirals. When talking to a person, arguing about the existene of a bug tends not to lead to succesful resolution of the bug. Somebody talked about this on a post a few days ago, about attractor basins, oppositionality, and when AI agents are convinced they are people (rightly or wrongly). You are often better off clearing the context then repeatedly arguing in the same context window. </p></div></div></div></div></div><div><div><div id="RZXNy8oYpxm9KbBeP"><div><div><div><div><div><p>This is a good point! Typically I start from a clean commit in a fresh chat, to avoid this problem from happening too easily, proceeding through the project in the smallest steps I can get Claude to make. That&#39;s what makes the situation feel so strange; it <em>feels</em> just like this problem, but it happens instantly, in Claude&#39;s first responses.</p></div></div></div></div></div></div></div><div><div id="fmzZ37rftaHuAZrfr"><div><div><div><div><div><p>It&#39;s also worth trying a different model. I was going back and forth with an OpenAI model (I don&#39;t remember which one) and couldn&#39;t get it to do what I needed at all, even with multiple fresh threads. Then I tried Claude and it just worked.</p></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="ZE9wJBiMuXdScnsyz"><div><div><div><div><div><div><p>I have experienced similar problems to you when building an AI tool - better models did not necessarily lead to better performance despite external benchmarks. I believe there are 2 main reasons why this is, alluded to in your post:</p><ol><li>Selection Bias - when a foundation model company releases their newest model, they show performance on benchmarks most favorable to it</li><li>Alignment - You mentioned how AI is not truly understanding the instructions you meant. While this can be mitigated by creating better prompts, it does not fully solve the issue</li></ol></div></div></div></div></div></div></div></div><div><div id="hv7hbX4FyfrsGmMu3"><div><div><div><div><div><div><p>Another hypothesis: Your description of the task is</p>
<blockquote>
<p>the hard parts of application pentesting for LLMs, which are 1. Navigating a real repository of code too large to put in context, 2. Inferring a target application&#39;s security model, and 3. Understanding its implementation deeply enough to learn where that security model is broken.</p>
</blockquote>
<p>From METR&#39;s recent investigation on long tasks you would expect current models not to perform well on this.</p>
<p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gXyMCnjrMfBbnYyZ4/uhhrbcdl6ddsulit1x7x" alt="METRs graph"/></p>
<p>I doubt a human professional could do the tasks you describe in something close to an hour, so perhaps its just currently too hard and the current improvements don&#39;t make much of a difference for the benchmark, but it might in the future.</p>
</div></div></div></div></div></div></div></div><div><div id="DizdPkHMLDSwKcKE9"><div><div><div><div><div><div><blockquote><p>However, if you merely <i>explain</i> these constraints to the chat models, they&#39;ll follow your instructions sporadically.</p></blockquote></div></div></div></div></div></div></div></div><div><div id="Skjpxyjd6C9NWRxcR"><div><div><div><div><div><div><p>Much of the gains on SWE Bench are actually about having the model find better context via tool calls. Sonnet 3.7 is trained to seek out the information it needs.</p><p>But if you compare the models with fixed context, they are only somewhat smarter than before.</p><p>That being said, the new Gemini 2.5 Pro seems like another decent step up in intelligence from Sonnet 3.7. We&#39;re about to switch out the default mode of our coding agent, Codebuff, to use it (and already shipped it for codebuff --max).</p></div></div></div></div></div></div></div></div><div><div id="53S2Duk6DjkLLNgob"><div><div><div><div><div><p>In practice, Sonnet 3.7 and Gemini 2.5 are just often too good compared to competitors.</p></div></div></div></div></div></div></div><div><div id="oq6pD94GGp3ntZxkj"><div><div><div><div><div><div><p>That&#39;s an interesting point, why didn&#39;t we see major improvements in LLMs for instance when coding... Despite them achieving reasoning on the level that allows them become a GM on codeforces.</p><p>I&#39;d say this is a fundamental limitation of reinforcement learning. Using purely reinforcement learning is stupid. Look at humans, we do much more than that. We make observations about our failures and update, we develop our own heuristics for what it means to be good at something and then try to figure out how to make ourselves better by reasoning about it watching other people etc... </p><p>This form of learning that happens at inference time is imo the fundamental thing preventing LLMs right now becoming more intelligent. And actual memory of course.</p><p>So we&#39;re just making them improve at measurable tasks through naive reinforcement learning but don&#39;t allow them to generalize it by using their understanding of that to properly update themselves in other not so measurable fields...</p></div></div></div></div></div></div></div></div><div><div id="PtetA8dg3zPmgEZ6u"><div><div><div><div><div><p>I primarily use LLMs when working with mathematics, which is one of the areas where the recent RL paradigm was a clear improvement—reasoning models are finally <i>useful. </i>However, I agree with you that benchmark-chasing isn’t optimal, in that it still <i>can’t admit when it’s wrong</i>. It doesn’t have to give up, but when it couldn’t do something, I’d rather it list out what it tried as ideas, rather than pretending it can solve everything, because then I actually have to read through everything.</p></div></div></div></div></div></div></div><div><div id="YggxmorYoyt9LLb2E"><div><div><div><div><div><div><blockquote>
<p>HLE and benchmarks like it are cool, but they fail to test the major deficits of language models, like how they can only remember things by writing them down onto a scratchpad like the memento guy.</p>
</blockquote>
<p>A scratch pad for thinking, in my view, is hardly a deficit at all! Quite the opposite. In the case of people, some level of conscious reflection is important and probably necessary for higher-level thought. To clarify, I am not saying consciousness itself is in play here. I’m saying some feedback loop is probably necessary — where the artifacts of thinking, reasoning, or dialogue can themselves become objects of analysis.</p>
<p>My claim might be better stated this way: if we want an agent to do sufficiently well on higher-level reasoning tasks, it is probably necessary for them to operate at various levels of abstraction, and we shouldn’t be surprised if this is accomplished by way of observable artifacts used to bridge different layers. Whether the mechanism is something akin to chain of thought or something else seems incidental to the question of intelligence (by which I mean assessing an agent&#39;s competence at a task, which follows Stuart Russell&#39;s definition).</p>
<p>I don’t think the author would disagree, but this leaves me wondering why they wrote the last part of the sentence above. What am I missing?</p>
</div></div></div></div></div></div></div></div><div><div id="YCBsThnivzME9gD3P"><div><div><div><div><div><div><blockquote><p>I think what&#39;s going on is that large language models are trained to &#34;sound smart&#34; in a live conversation with users, and so they prefer to highlight possible problems instead of confirming that the code looks fine, <a href="https://www.lesswrong.com/posts/xsB3dDg5ubqnT7nsn/poc-or-or-gtfo-culture-as-partial-antidote-to-alignment">just like human beings do when they want to sound smart</a>.</p></blockquote><p>This matches my experience, but I&#39;d be interested in seeing proper evals of this specific point!</p></div></div></div></div></div></div></div></div><div><div id="dxoRxqWagGj3npbg4"><div><div><div><div><div><div><p>Your first two key challenges</p><p>Seems very similar to the agent problem of active memory switching, carrying important information across context switches</p><p>Also note that it could just be instead of bullshit, finetuning is unreasonably effective, and so when you train models on an evaluation they actually get better on the things evaluated, which dominates over scaling.</p><p>So things with public benchmarks might just actually be easier to make models that are genuinely good at it. (For instance searching for data that helps 1B models learn it, then adding it to full size models as a solution for data quality issues)</p><p>Have tested if finetuning open models on your problems works? (It is my first thought, so I assume you had it too)</p></div></div></div></div></div></div></div></div><div><div id="yXAJPjWRyhE2ka4E5"><div><div><div><div><div><div><p>(disclaimer: I work on evaluation at oai, run the o3 evaluations etc)</p><p>The public evidence for broad/fuzzy task improvement is weaker — o1 mmlu boosts and various vibes evals (Tao) do show it though.</p></div></div></div></div></div></div></div></div><div><div id="PSBzw2vwxxqRN3Ja5"><div><div><div><div><div><p>From a practical perspective, maybe you are looking at the problem the wrong way around. A lot of prompt engineering seems to be about asking LLMs to play a role. I would try to tell the LLM that it was a hacker and to design an exploit to attack the given system (this is the sort of mental perspective I used to use to find bugs when I was a software engineer). Another common technique is &#34;generate then prune&#34; : Have a separate model/prompt remove all the results of the first one that are only &#34;possibilities&#34;. It seems, from my reading, that this sort of two stage approach can work because it bypasses LLMs typical attempts to &#34;be helpful&#34; by inventing stuff or spouting banal filler rather than just admitting ignorance.</p></div></div></div></div></div><div><div><div id="88bqRkdk8SeXKCoPq"><div><div><div><div><div><p>I think we should suspect that they&#39;ve done some basic background research (this individual, not in general), and take the rest of the information about people failing to see improvements as data that also points this direction.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></span></span></p><div><div><div><div><p>Curated and popular this week</p></div></div></div></div></div></div>
  </body>
</html>
