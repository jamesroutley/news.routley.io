<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/kelindar/search">Original</a>
    <h1>Go library for in-process vector search and embeddings with llama.cpp</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/kelindar/search/blob/main/.github/logo.png"><img width="330" height="110" src="https://github.com/kelindar/search/raw/main/.github/logo.png" alt="kelindar/search"/></a>
</p>

<p dir="auto">This library was created to provide an <strong>easy and efficient solution for embedding and vector search</strong>, making it perfect for small to medium-scale projects that still need some <strong>serious semantic power</strong>. Itâ€™s built around a simple idea: if your dataset is small enough, you can achieve accurate results with brute-force techniques, and with some smart optimizations like <strong>SIMD</strong>, you can keep things fast and lean.</p>
<p dir="auto">The libraryâ€™s strength lies in its simplicity and support for <strong>GGUF BERT models</strong>, letting you leverage sophisticated embeddings without getting bogged down by the complexities of traditional search systems. It offers <strong>GPU acceleration</strong>, enabling quick computations on supported hardware. If your dataset has fewer than 100,000 entries, this library is a great fit for integrating semantic search into your Go applications with minimal hassle.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/kelindar/search/blob/main/.github/demo.gif"><img src="https://github.com/kelindar/search/raw/main/.github/demo.gif" alt="demo" data-animated-image=""/></a></p>

<ul dir="auto">
<li><strong>llama.cpp without cgo</strong>: The library is built to work with <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> without using cgo. Instead, it relies on <a href="https://github.com/ebitengine/purego">purego</a> , which allows calling shared C libraries directly from Go code without the need for cgo. This design significantly simplifies the integration, deployment, and cross-compilation, making it easier to build Go applications that interface with native libraries.</li>
<li><strong>Support for BERT Models</strong>: The library supports BERT models via <a href="https://github.com/ggerganov/llama.cpp/pull/5423" data-hovercard-type="pull_request" data-hovercard-url="/ggerganov/llama.cpp/pull/5423/hovercard">llama.cpp</a>. Vast variations of BERT models can be used, as long as they are using GGUF format.</li>
<li><strong>Precompiled Binaries with Vulkan GPU Support</strong>: Available for Windows and Linux in the <a href="https://github.com/kelindar/search/blob/main/dist">dist</a> directory, compiled with Vulkan for GPU acceleration. However, you can compile the library yourself with or without GPU support.</li>
<li><strong>Search Index for Embeddings</strong>: The library supports the creation of a search index from computed embeddings, which can be saved to disk and loaded later. This feature is suitable for basic vector-based searches in small-scale applications, but it may face efficiency challenges with large datasets due to the use of brute-force techniques.</li>
</ul>

<p dir="auto">While simple vector search excels in small-scale applications,avoid using this library if you have the following requirements.</p>
<ul dir="auto">
<li><strong>Large Datasets</strong>: The current implementation is designed for small-scale applications, and datasets exceeding 100,000 entries may suffer from performance bottlenecks due to the brute-force search approach. For larger datasets, approximate nearest neighbor (ANN) algorithms and specialized data structures should be considered for efficiency.</li>
<li><strong>Complex Query Requirements</strong>: The library focuses on simple vector similarity search and does not support advanced query capabilities like multi-field filtering, fuzzy matching, or SQL-like operations that are common in more sophisticated search engines.</li>
<li><strong>High-Dimensional Complex Embeddings</strong>: Large language models (LLMs) generate embeddings that are both high-dimensional and computationally intensive. Handling these embeddings in real-time can be taxing on the system unless sufficient GPU resources are available and optimized for low-latency inference.</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">ðŸ“š How to Use the Library</h2><a id="user-content--how-to-use-the-library" aria-label="Permalink: ðŸ“š How to Use the Library" href="#-how-to-use-the-library"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This example demonstrates how to use the library to generate embeddings for text and perform a simple vector search. The code snippet below shows how to load a model, generate embeddings for text, create a search index, and perform a search.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Install library</strong>: Precompiled binaries for Windows and Linux are provided in the <a href="https://github.com/kelindar/search/blob/main/dist">dist</a> directory. If your target architecture or platform isn&#39;t covered by these binaries, you&#39;ll need to compile the library from the source. Drop these binaries in <code>/usr/lib</code> or equivalent.</p>
</li>
<li>
<p dir="auto"><strong>Load a model</strong>: The <code>search.NewVectorizer</code> function initializes a model using a GGUF file. This example loads the <em>MiniLM-L6-v2.Q8_0.gguf</em> model. The second parameter, indicates the number of GPU layers to enable (0 for CPU only).</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="m, err := search.NewVectorizer(&#34;../dist/MiniLM-L6-v2.Q8_0.gguf&#34;, 0)
if err != nil {
    // handle error
}
defer m.Close()"><pre><span>m</span>, <span>err</span> <span>:=</span> <span>search</span>.<span>NewVectorizer</span>(<span>&#34;../dist/MiniLM-L6-v2.Q8_0.gguf&#34;</span>, <span>0</span>)
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
    <span>// handle error</span>
}
<span>defer</span> <span>m</span>.<span>Close</span>()</pre></div>
<ol start="3" dir="auto">
<li><strong>Generate text embeddings</strong>: The <code>EmbedText</code> method is used to generate vector embeddings for a given text input. This converts your text into a dense numerical vector representation given the model you loaded in the previous step.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="embedding, err := m.EmbedText(&#34;Your text here&#34;)"><pre><span>embedding</span>, <span>err</span> <span>:=</span> <span>m</span>.<span>EmbedText</span>(<span>&#34;Your text here&#34;</span>)</pre></div>
<ol start="4" dir="auto">
<li><strong>Create an index and adding vectors</strong>: Create a new index using <code>search.NewIndex</code>. The type parameter <code>[string]</code> in this example specifies that each vector is associated with a string value. You can add multiple vectors with corresponding labels.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="index := search.NewIndex[string]()
index.Add(embedding, &#34;Your text here&#34;)"><pre><span>index</span> <span>:=</span> <span>search</span>.<span>NewIndex</span>[<span>string</span>]()
<span>index</span>.<span>Add</span>(<span>embedding</span>, <span>&#34;Your text here&#34;</span>)</pre></div>
<ol start="5" dir="auto">
<li><strong>Search the index</strong>: Perform a search using the <code>Search</code> method, which takes an embedding vector and a number of results to retrieve. This example searches for the 10 most relevant results and prints them along with their relevance scores.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="results := index.Search(embedding, 10)
for _, r := range results {
    fmt.Printf(&#34;Result: %s (Relevance: %.2f)\n&#34;, r.Value, r.Relevance)
}"><pre><span>results</span> <span>:=</span> <span>index</span>.<span>Search</span>(<span>embedding</span>, <span>10</span>)
<span>for</span> <span>_</span>, <span>r</span> <span>:=</span> <span>range</span> <span>results</span> {
    <span>fmt</span>.<span>Printf</span>(<span>&#34;Result: %s (Relevance: %.2f)<span>\n</span>&#34;</span>, <span>r</span>.<span>Value</span>, <span>r</span>.<span>Relevance</span>)
}</pre></div>

<p dir="auto">First, clone the repository and its submodules with the following commands. The <code>--recurse-submodules</code> flag is used to clone the <code>ggml</code> submodule, which is a header-only library for matrix operations.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git submodule update --init --recursive
git lfs pull"><pre>git submodule update --init --recursive
git lfs pull</pre></div>

<p dir="auto">Make sure you have a C/C++ compiler and CMake installed. For Ubuntu, you can install them with the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get update
sudo apt-get install build-essential cmake"><pre>sudo apt-get update
sudo apt-get install build-essential cmake</pre></div>
<p dir="auto">Then you can compile the library with the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build &amp;&amp; cd build
cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc ..
cmake --build . --config Release"><pre>mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc ..
cmake --build <span>.</span> --config Release</pre></div>
<p dir="auto">This should generate <code>libllama_go.so</code> that statically links everything necessary. You can also install the library by coping it into <code>/usr/lib</code>.</p>

<p dir="auto">Make sure you have a C/C++ compiler and CMake installed. For Windows, a simple option is to use <a href="https://visualstudio.microsoft.com/downloads/" rel="nofollow">Build Tools for Visual Studio</a> (make sure CLI tools are included) and <a href="https://cmake.org/download/" rel="nofollow">CMake</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build &amp;&amp; cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
cmake --build . --config Release"><pre>mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake -DCMAKE_BUILD_TYPE=Release ..
cmake --build <span>.</span> --config Release</pre></div>
<p dir="auto">If you are using Visual Studio, solution files are generated. You can open the solution file with Visual Studio and build the project from there. The <code>bin</code> directory would then contain <code>llamago.dll</code>.</p>

<p dir="auto">To enable GPU support (e.g. Vulkan), you&#39;ll need to add an appropriate flag to the CMake command, please refer to refer to the <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#vulkan">llama.cpp</a> build documentation for more details. For example, to compile with Vulkan support on Windows make sure Vulkan SDK is installed and then run the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir build &amp;&amp; cd build
cmake -DCMAKE_BUILD_TYPE=Release -DGGML_VULKAN=ON ..
cmake --build . --config Release"><pre>mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake -DCMAKE_BUILD_TYPE=Release -DGGML_VULKAN=ON ..
cmake --build <span>.</span> --config Release</pre></div>
</article></div></div>
  </body>
</html>
