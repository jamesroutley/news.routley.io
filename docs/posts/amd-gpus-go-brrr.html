<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-amd-brr">Original</a>
    <h1>AMD GPUs Go Brrr</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Team</strong>: William Hu, Drew Wadsworth, Sean Siddens, Stanley Winata, Daniel Fu, Ryan Swann, Muhammad Osama, Christopher R√©, Simran Arora</p>
<p>AI is <a href="https://openai.com/index/five-new-stargate-sites/">compute</a> <a href="https://www.bloomberg.com/news/newsletters/2025-08-24/why-is-manhattan-being-crushed-by-this-giant-meta-data-center">hungry</a>. So <a href="https://www.together.ai/blog/based">we&#39;ve</a> <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">been</a> <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-truly-subquadratic">asking</a>: How do we build AI from the hardware up? How do we lead AI developers to do what the hardware prefers?</p>
<p>AMD GPUs are now offering state-of-the-art speeds and feeds. However, this performance is locked away from AI workflows due to the <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-hk">lack of mature AMD software</a>. We share HipKittens, an opinionated collection of programming primitives to help developers realize the hardware&#39;s capabilities: optimized register tiles, 8-wave and 4-wave kernel patterns instead of wave-specialization to schedule work within processors, and chiplet-optimized cache reuse patterns to schedule work across processors.</p>
<p>Checkout <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-hk">part one of this series</a> for an intro to HipKittens and checkout this post for a technical deep dive.</p>
<h3>What do AMD CDNA GPUs look like? A lay of the land.</h3>
<p>An AMD MI355X GPU has 256 processors called ‚Äúcompute units‚Äù (CUs) and a CU contains four SIMDs. A SIMD has different execution units. A 64-thread ‚Äúwave‚Äù (contrasting a 32-thread warp on NVIDIA) occupies a single SIMD. We show the MI355X memory hierarchy below.</p>
<p>Unsurprisingly, making AMD GPUs go brr boils down to keeping the ‚Äúmatrix cores‚Äù (tensor cores on NVIDIA) fed. There are a few differences in how we think about this hardware:</p>
<ol>
<li><strong>What it&#39;s not.</strong> An MI355X has 70% the SRAM of a B200 (165KB instead of 228KB), lacks asynchronous matrix multiplication instructions that operate on inputs in shared or tensor memory (wgmma, tcgen05), lacks register reallocation (the ability for some waves to give their registers to others), lacks tensor memory acceleration (dedicated hardware for global memory access), and lacks first class mbarrier primitives (for fine-grained synchronization).</li>
<li><strong>What it is.</strong> On the other hand, AMD GPUs have a 2x larger register file per processor than the B200 and offers 60% more processors per GPU (256 compute units versus 160 streaming multiprocessors). AMD offers tiny and fine-grained matrix core instructions, while NVIDIA tensor cores instructions are generally called with large input operands. AMD has a TMA-like direct global to shared memory loads via <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">buffer_load_dword</mtext></mrow><annotation encoding="application/x-tex">\verb|buffer_load_dword|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>buffer_load_dword</span></span></span></span></span></span> instructions, which bypass the register file.</li>
<li><strong>Towards chiplet architectures.</strong> AMD is also leading the charge in the shift from monolithic grids to chiplets. AMD splits the 256 processors into 8 chiplets called ‚ÄúXCDs‚Äù of 32 CUs. NVIDIA B200s include 2 chips. The AMD cache is disaggregated: an AMD XCD has a private L2 cache and there is an extra last level cache (LLC) that sits between the L2 and HBM memory.</li>
</ol>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/memory.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/chip.png"/></figure>
<table><thead><tr><th>Spec</th><th>NVIDIA B200 SXM5</th><th>AMD MI355X OAM</th></tr></thead><tbody><tr><td>BF16 matrix / tensor</td><td>2.2 PFLOPs</td><td>2.5 PFLOPs</td></tr><tr><td>MXFP8 matrix / tensor</td><td>4.5 PFLOPs</td><td>5.0 PFLOPs</td></tr><tr><td>MXFP6 matrix / tensor</td><td>4.5 PFLOPs</td><td>10.1 PFLOPs</td></tr><tr><td>MXFP4 matrix / tensor</td><td>9.0 PFLOPs</td><td>10.1 PFLOPs</td></tr><tr><td>Memory capacity</td><td>180 GB</td><td>288 GB</td></tr><tr><td>Memory bandwidth</td><td>8.0 TB/s</td><td>8.0 TB/s</td></tr></tbody></table>
<figcaption><p>Table 1: <strong>Hardware overview.</strong> Peak memory and compute speeds for the latest generation GPU platforms.</p></figcaption>
<p>These differences impact the ways in which we design kernels on AMD.</p>
<ol>
<li><strong>Optimized memory access:</strong> Of course this matters on NVIDIA, but AMD‚Äôs layouts, HIPCC compiler limitations, and (undocumented) quirky behaviors of different I/O instructions yields new challenges.</li>
<li><strong>Scheduling within processors:</strong> We need to rely on our register file and small matrix core instructions instead of shared memory and bulky wgmma/tcgen05 instructions to establish deep pipelines and hide memory costs. Wave specialization / producer consumer, which reigns supreme in NVIDIA kernels, is not the right answer on AMD.</li>
<li><strong>Scheduling across processors:</strong> We need to start thinking about NUMA effects at the cache level as we schedule work across thread blocks.</li>
</ol>
<p>We walk through these three topics next.</p>
<h3>HipKittens memory access patterns</h3>
<p>As in <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">ThunderKittens</a>, in HK, developers program using tiles as the basic data structure. Tiles exist in shared or register memory, and are parametrized by a data type, size dimensions (a multiple of the matrix core instruction shape), and layout. HK provides a library of PyTorch-like functions that operate on tiles, for instance <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">exp</mtext></mrow><annotation encoding="application/x-tex">\verb|exp|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>exp</span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">mma</mtext></mrow><annotation encoding="application/x-tex">\verb|mma|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>mma</span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">sub</mtext></mrow><annotation encoding="application/x-tex">\verb|sub|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>sub</span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">add</mtext></mrow><annotation encoding="application/x-tex">\verb|add|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>add</span></span></span></span></span></span>, <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">row_max</mtext></mrow><annotation encoding="application/x-tex">\verb|row_max|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>row_max</span></span></span></span></span></span> compute ops and load, store memory ops. A tile is collectively owned by threads in a wave (warp). The functions use template metaprogramming to generalize to different input tiles and are lightweight, directly wrapping assembly (PTX, CDNA ISA) and C++.</p>
<p>A memory layout determines how data elements map to thread ownership. A matrix core instruction expects a particular register layout depending on the data type and instruction shape. We also want to maximize the granularity and coalescing of global memory loads. Between registers and HBM, shared memory is split into banks (4-byte regions) that can serve data simultaneously. If threads from a wave request data from the same bank at the same time, their accesses are serialized; efficient kernels use ‚Äúswizzle patterns‚Äù to organize data in a way that avoids these bank conflicts.</p>
<p>A few challenges for memory access in HK include:</p>
<ul>
<li><strong>Register scheduling:</strong> A core tenant of HK and TK is to give developers full control over register allocation by remaining C++ embedded. Compilers like Triton prevent register management altogether, but surprisingly we find that even the HIPCC compiler imposes severe limitations (no wonder AMD uses raw assembly!).<sup id="fnref-1"><a href="#fn-1">1</a></sup> For instance, 4-wave (1-wave per SIMD) kernels compiled via HIPCC cannot use data held in certain types of registers as inputs to matrix instructions. This motivated us to add explicit register scheduling to HK, where developers pin specific registers when creating registers tiles, effectively replacing the compiler‚Äôs register management capabilities. Developers thus have the control necessary to write peak performance kernels!</li>
</ul>
<details><summary>Learn more about explicit register scheduling</summary><p>When a single wave is mapped per SIMD, the 512 registers are actually divided into 256 accumulator general purpose registers (AGPRs) and 256 vector general purpose registers (VGPRs). AGPRs have fundamental hardware limitations (e.g., vector arithmetic instructions cannot operate on them), but they can still crucially serve as the input or outputs for MFMA instructions. HIPCC, however, cannot generate code that uses AGPRs as input to MFMA instructions, leading to inefficient register management for register heavy workloads and redundant accvgpr_read/write instructions that move data between VGPRs and AGPRs.</p></details>
<ul>
<li><strong>Register layouts</strong>: NVIDIA tensor core layouts are regular ‚Äì as we vary the data type or matrix shape, the layout is composed of an underlying ‚Äúcore matrix‚Äù structure. Thus, frameworks like TK and Gluon can apply a unified swizzling rule to avoid bank conflicts. However, AMD layouts differ significantly based on the data type and matrix shape. In fact, we show that it‚Äôs not possible to use a single swizzle pattern for all layouts. Further, sometimes we want to use multiple matrix shapes within the same kernel meaning that our swizzle pattern needs to be compatible with multiple types of layouts concurrently.</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/matrix.png"/><figcaption><p>Figure: AMD register layouts for matrix instructions are less structured. NVIDIA layouts are all composed from an underlying core matrix structure.</p></figcaption></figure>
<ul>
<li><strong>Instruction phases:</strong> Waves (and NVIDIA warps) execute shared memory read/write instructions <a href="https://forums.developer.nvidia.com/t/how-to-understand-the-bank-conflict-of-shared-mem/260900">in phases</a>, where a subset of threads in the wave access shared memory concurrently. NVIDIA instructions sequentially assign threads to phases (e.g., threads 0-7 in phase one, 8-15 in phase two). However, the phase groups are both non-sequential and differ entirely based on AMD CDNA memory instruction. For example, we found that a <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_read_b128</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_read_b128|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_read_b128</span></span></span></span></span></span> instruction reading 128 bits for each thread executes across 4 phases and has access to 64 banks. On the other hand, a <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_write_b64</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_write_b64|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_write_b64</span></span></span></span></span></span> instruction writing 64 bits for each thread executes across 4 phases and has access to only 32 banks. This behavior is not well-documented even within AMD(! üòî), so we created and release solvers that reverse-engineer this behavior.</li>
</ul>
<details><summary>Learn why we can&#39;t use a single swizzle pattern for all AMD layouts.</summary><p>Proof by contradiction:</p><p>To show why a single swizzling pattern is insufficient across different register tile
shapes and layouts on AMD GPUs, consider the following two access patterns that surface in attention backwards:</p><ol>
<li>A row-layout 16x16 bf16 tile is written to shared memory. For this tile configuration, each thread holds 4 contiguous bf16 values - 64 bits in memory - and the most optimal instruction to issue this write is <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_write_b64</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_write_b64|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_write_b64</span></span></span></span></span></span>. Avoiding bank conflicts for this access requires a swizzle pattern that respects the phase ordering and bank behavior previously mentioned. In this case, a swizzle that abides by these constraints is computed as <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">t</mi></mrow><annotation encoding="application/x-tex">\mathrm{offset}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>offset</span></span></span></span></span></span> ^ <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">t</mi></mrow><mi mathvariant="normal">%</mi><mn>512</mn><mo stretchy="false">)</mo><mo>&gt;</mo><mo>&gt;</mo><mn>7</mn><mo stretchy="false">)</mo><mo>&lt;</mo><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">= ((\mathrm{offset} \% 512) &gt;&gt; 7) &lt;&lt; 3</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>=</span><span></span></span><span><span></span><span>((</span><span><span>offset</span></span><span>%512</span><span>)</span><span></span><span>&gt;&gt;</span><span></span></span><span><span></span><span>7</span><span>)</span><span></span><span>&lt;&lt;</span><span></span></span><span><span></span><span>3</span></span></span></span></span>, where 64-bit chunks of memory is shifted around memory using an XOR swizzle.</li>
<li>A row-layout 16x32 bf16 tile is read from shared memory. For this tile, each thread holds 8 contiguous bf16 values - 128 bits in memory - and the most optimal instruction to issue this read is <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_read_b128</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_read_b128|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_read_b128</span></span></span></span></span></span>.</li>
</ol><p>Regardless of the swizzling pattern required for <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_read_b128</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_read_b128|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_read_b128</span></span></span></span></span></span>, the granularities of these two instructions are in conflict with each other. <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_read_b128</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_read_b128|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_read_b128</span></span></span></span></span></span> requires at least 128 bits of memory to be contiguous in shared memory, and the swizzle pattern for <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">ds_write_b64</mtext></mrow><annotation encoding="application/x-tex">\verb|ds_write_b64|</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>ds_write_b64</span></span></span></span></span></span> breaks apart memory into 64-bit chunks. As a result, different swizzling patterns need to be used for each.</p></details>
<ul>
<li><strong>Address generation:</strong> AMD GPUs support direct asynchronous HBM to shared memory loads. Like TMA, these loads bypass the register file. The instruction takes as input per-thread addresses in HBM from which each thread will read data. While DSLs like TK directly swizzle the shared memory addresses, swizzling shared memory on AMD is instead accomplished by swizzling on the HBM addresses.</li>
</ul>
<p>We provide developers with optimized tile layouts and memory access patterns by default within HK. Checkout our paper to learn more about how we implement solutions to the above challenges.</p>
<h3>HipKittens schedules within a processor</h3>
<p>Ideally we would have simple, reusable patterns for scheduling the compute and memory within kernels that generalize across AI workloads. Wave specialization / producer consumer serves this purpose on NVIDIA, but what about on AMD?</p>
<p><strong>Wave specialization struggles on AMD.</strong> Wave specialization is the dominant paradigm for achieving high occupancy on modern NVIDIA GPUs. Producer waves focus on memory movement while consumer waves focus on computation. This strategy underpins today‚Äôs state-of-the-art AI kernels‚Äîincluding <a href="https://arxiv.org/abs/2407.08608">FlashAttention-3</a>, <a href="https://openreview.net/forum?id=fGgQS5VW09">COMET for MOE models</a>, and <a href="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">high-performance GEMMs</a> ‚Äîas well as kernel DSLs such as <a href="https://github.com/HazyResearch/ThunderKittens/blob/main/kernels/matmul/H100/matmul.cu">ThunderKittens LSCF</a> and <a href="https://arxiv.org/abs/2504.17577">TileLang</a>.</p>
<p>But, we show that wave specialization underperforms on AMD due to the lack of register reallocation. On the MI355X, registers are statically divided across all waves. Producer waves that only need a few registers for address calculation are allocated more registers than they need; consumer waves cannot recoup those registers and must either spill registers to scratch memory or run at a lower arithmetic intensity. Both are disastrous for performance. Wave specialization limits the output tile size and makes our kernels more memory bound.  For GEMMs, data loaded from memory is O(MK + NK) while compute is O(MNK). Decreasing the M or N in our per thread block output tile size lowers arithmetic intensity.
<sup id="fnref-2"><a href="#fn-2">2</a></sup></p>
<table><thead><tr><th># P / # C</th><th>MFMA Shape</th><th>Output</th><th>TFLOPS</th></tr></thead><tbody><tr><td>HK 4 / 8</td><td>16√ó16√ó32</td><td>128√ó256</td><td>893</td></tr><tr><td>HK 4 / 12</td><td>16√ó16√ó32</td><td>192√ó256</td><td>1278</td></tr><tr><td>HK 0 / 8</td><td>16√ó16√ó32</td><td>192√ó256</td><td>1281</td></tr><tr><td>HK 0 / 8</td><td>16√ó16√ó32</td><td>256√ó256</td><td><strong>1605</strong></td></tr><tr><td>TK</td><td>256√ó256√ó16</td><td>256√ó256</td><td>1538</td></tr><tr><td>CUTLASS</td><td>256√ó256√ó16</td><td>256√ó256</td><td>1570</td></tr></tbody></table>
<figcaption><p>Figure: <strong>Wave specialization underperforms on AMD GPUs.</strong> We benchmark AMD GEMMs on the MI355X using different numbers of producers (P) and consumer (C) waves. We report the matrix core intrinsic shape, output tile size computed per thread block, and TFLOPs (500 iterations warmup / 100 iterations measured). The CUTLASS GEMM is selected and tuned using the CUTLASS profiler tool on a B200 GPU.</p></figcaption>
<p>As an aside, it might be surprising that AMD matches NVIDIA GEMM performance without all the bells and whistles of wgmma/tma, producer consumer, TMA, mbarriers, large shared memory for deep multi-stage pipelining etc. But‚Ä¶ AMD has a <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>√ó</mo></mrow><annotation encoding="application/x-tex">2\times</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>√ó</span></span></span></span></span> larger register file and AMD‚Äôs smaller tensor core shapes (e.g., <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>√ó</mo><mn>16</mn><mo>√ó</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">16\times16\times32</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>16</span><span></span><span>√ó</span><span></span></span><span><span></span><span>16</span><span></span><span>√ó</span><span></span></span><span><span></span><span>32</span></span></span></span></span>) provide an alternative path to establish deep pipelines by using finer-granularity load and compute stages.</p>
<p>Scheduling patterns for AMD. Our attempt to use wave specialization - a strategy that works well on NVIDIA GPUs - did not yield the expected speedups on AMD hardware. All is not lost! We found two scheduling patterns that consistently yield high occupancy AMD GPUs, while using tile programming primitives (no raw assembly)!</p>
<ol>
<li><strong>8-wave ping-pong:</strong> We assign two waves per SIMD and at any given time, one is executing a cluster of memory instructions while the other wave executes a cluster of compute instructions. The waves swap at the end of cluster execution. With this approach, the developer can use large HK tiles since a thread issues many of the same instructions at once!</li>
<li><strong>4-wave interleave:</strong> We assign one wave per SIMD and threads in this wave finely switch between issuing memory and compute operations. Here, the developer uses small HK tiles (essentially matching the size of the matrix core instruction shape) to achieve the fine-grained schedule.</li>
</ol>
<p>These two patterns tradeoff programmability and performance, where 8-wave and its large tile primitives lead to compact code and 4-wave fine-grained interleaving expands code size. Surprisingly, the 8-wave schedule is sufficient to achieve SoTA-level performance on GEMMs and attention forwards. For GQA non-causal attention backwards, 8-wave also outperforms all AMD baselines by <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.8</mn><mo>√ó</mo></mrow><annotation encoding="application/x-tex">1.8\times</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1.8</span><span>√ó</span></span></span></span></span>, and our HK 4-wave further outperforms by <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2.3</mn><mo>√ó</mo></mrow><annotation encoding="application/x-tex">2.3\times</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2.3</span><span>√ó</span></span></span></span></span>.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/8_wave.png"/><figcaption><p>Figure: <strong>HK 8-wave ping pong pattern.</strong> We include a profiler snippet of the HK BF16 GEMM.</p></figcaption></figure>
<h3>HipKittens schedules across processors</h3>
<p>Modern GPUs are moving toward chiplet-based architectures, shifting away from traditional monolithic dies. AMD‚Äôs MI355X, for instance, integrates eight chiplets (XCDs), each with its own L2 cache, while NVIDIA‚Äôs B200 pairs two dies together. This shift enables higher scalability and yield but introduces a new performance challenge: disaggregated memory hierarchies. Each cluster of compute units now has local caches, and memory locality is no longer uniform across the chip.</p>
<p>On AMD GPUs, thread blocks are scheduled to chiplets in a round-robin fashion, meaning that the order in which blocks are launched‚Äîthe grid schedule‚Äîdirectly affects how effectively data is reused in cache. Even perfectly tuned kernels can lose bandwidth if their grid layout is cache-unfriendly.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/default.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/w5.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/w7.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/14592_default.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/14592_w8_c0.png"/><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-amd-brr/14592_w8_c64.png"/><figcaption><p>Figure: <strong>Visualization of three different grid schedules for the output matrix of a BF16 GEMM.</strong> The color represents the XCD assignment for the first set of thread blocks scheduled across the GPU&#39;s 256 processors. <strong>Top row</strong> is for a <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9216</mn><mo>√ó</mo><mn>9216</mn><mo>√ó</mo><mn>9216</mn></mrow><annotation encoding="application/x-tex">9216\times9216\times9216</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>9216</span><span></span><span>√ó</span><span></span></span><span><span></span><span>9216</span><span></span><span>√ó</span><span></span></span><span><span></span><span>9216</span></span></span></span></span> shaped GEMM and the <strong>bottom row</strong> is for a <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>14592</mn><mo>√ó</mo><mn>14592</mn><mo>√ó</mo><mn>14592</mn></mrow><annotation encoding="application/x-tex">14592\times14592\times14592</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>14592</span><span></span><span>√ó</span><span></span></span><span><span></span><span>14592</span><span></span><span>√ó</span><span></span></span><span><span></span><span>14592</span></span></span></span></span> shaped GEMM. The <strong>left most column</strong> shows the assignments under a naive row-major layout, the <strong>middle column</strong> shows an approach that optimizes L2 cache reuse, and the <strong>right column</strong> shows the output from our algorithm, balancing L2 and LLC cache reuse.</p></figcaption></figure>
<table><thead><tr><th>Block Order</th><th>L2 %</th><th>LLC %</th><th>Mem. BW</th><th>TFLOPS</th></tr></thead><tbody><tr><td><strong>Matrix Multiply (M=N=K=9216)</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>Row-major</td><td>55%</td><td>95%</td><td>15.1 TB/s</td><td>1113</td></tr><tr><td>XCD (W 7/C 216)</td><td>79%</td><td>24%</td><td>14.9 TB/s</td><td>991</td></tr><tr><td>XCD (W 5/C 25)</td><td>75%</td><td>93%</td><td>18.3 TB/s</td><td>1145</td></tr><tr><td><strong>Matrix Multiply (M=N=K=14592)</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>Row-major</td><td>36%</td><td>76%</td><td>10.7 TB/s</td><td>900</td></tr><tr><td>XCD (W 8/C 542)</td><td>79%</td><td>7%</td><td>13.9 TB/s</td><td>980</td></tr><tr><td>XCD W 8/C 64</td><td>78%</td><td>55%</td><td>16.6 TB/s</td><td>1068</td></tr></tbody></table>
<figcaption><p>Table: Performance results corresponding to the above chiplet swizzling figures.</p></figcaption>
<p>Above for a GEMM <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mi>A</mi><mi>B</mi><mo>+</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">D=AB + C</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span>B</span><span></span><span>+</span><span></span></span><span><span></span><span>C</span></span></span></span></span>, we show different patterns for assigning thread blocks the responsibility of computing different tiles of the output matrix <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span>. When thread blocks are scheduled in na√≠ve row-major order, cache reuse is suboptimal (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>55</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\approx55\%</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>‚âà</span><span></span></span><span><span></span><span>55%</span></span></span></span></span>) because blocks that share the same L2 cache often load different, non-overlapping tiles of A and B. Further, optimizing purely for L2 locality can cause each XCD to fetch disjoint portions of A and B, leading to redundant loads at the next cache level.</p>
<p>To address this, HipKittens introduces a chiplet-aware scheduling strategy that reorganizes the grid launch order to better exploit locality at both the L2 (per-chiplet) and LLC (shared) cache levels for GEMM workloads. The key idea is to group thread blocks that operate on nearby regions of the output matrix so that they naturally reuse overlapping tiles of input data across cache hierarchies.</p>
<h3>Putting it all together</h3>
<p>Let&#39;s take a look at a few kernels written in HK.</p>
<ol>
<li>First, here&#39;s the hot loop of our attention forwards pass kernel (the entire kernel is <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">\approx 500</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>‚âà</span><span></span></span><span><span></span><span>500</span></span></span></span></span> lines of code). We can see that the kernel uses HK&#39;s 8-wave ping pong schedule where waves alternate between compute instruction clusters and memory clusters.</li>
</ol>

<ol start="2">
<li>Here&#39;s the core hot loop structure for our BF16 GEMM kernel. Again, we can see that waves alternate between compute clusters and memory clusters using HK&#39;s 8-wave ping pong schedule.</li>
</ol>

<h3>Multi-silicon AI is coming!</h3>
<p>HipKittens delivers competitive performance on AMD CDNA3 and CDNA4 through three key insights: optimized memory access, AMD-centric wave scheduling patterns within a processor, and chiplet-aware grid scheduling across processors to exploits AMD&#39;s disaggregated cache hierarchy. Our kernels consistently achieve peak performance amongst AMD baselines across workloads (and compete with peak Blackwell kernels as well).</p>
<p>Realizing AI&#39;s full potential requires diverse, open hardware.<sup id="fnref-1"><a href="#fn-1">1</a></sup> Today, that means making AMD GPUs truly accessible.</p>
<p>We want more AI in the world. AI has relied on and innovated on a single hardware provider, but we need to be able to use and experiment with all the compute we can. We need to be able to use the fastest hardware out there. We‚Äôre happy to help address these problems with HipKittens!</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2025-11-09-hk/hk_wave.png"/><figcaption><p>Figure: Surfing towards multi-silicon AI!</p></figcaption></figure>
<p><strong>Links</strong>: <a href="https://arxiv.org/abs/2511.08083">Arxiv</a> | <a href="https://github.com/HazyResearch/HipKittens">Code</a></p>
</div></div>
  </body>
</html>
