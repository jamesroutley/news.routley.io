<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/">Original</a>
    <h1>Custom SQL functions for data analytics in PostgreSQL</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p><strong>SQL is the lingua franca for analytics. </strong>As data proliferates, we need to find new ways to store, explore, and analyze it. <a href="https://www.timescale.com/blog/blog/why-sql-beating-nosql-what-this-means-for-future-of-data-time-series-database-348b777b847a/">We believe SQL is the best language for data analysis</a>. We‚Äôve championed the benefits of SQL for several years, even when many were swapping it for custom domain-specific languages. Full SQL support was one of the <a href="https://www.timescale.com/blog/when-boring-is-awesome-building-a-scalable-time-series-database-on-postgresql-2900ea453ee2/">key reasons</a> we chose to build TimescaleDB on top of PostgreSQL, the <a href="https://survey.stackoverflow.co/2022/#most-popular-technologies-database-prof">most loved database among developers</a>, rather than creating a custom query language. And we were right‚ÄîSQL is making a comeback (although it never really went away) and has become the universal language for data analysis, with many NoSQL databases adding SQL interfaces to keep up.</p><p>In addition, most developers are familiar with SQL, along with most data scientists, data analysts, and other professionals who work with data. Whether you&#39;ve taken classes at university, done an online course, or attended a boot camp, chances are that you probably have learned a bit of SQL along the way. So you and your fellow developers already know it, making it easier for teams to onboard new members and quickly extract value from the data. With a proprietary language, learning the language is in itself a barrier to using the data‚Äîyou‚Äôll have to ask another team to write the queries or rely on a separate data lake.</p><p><a href="https://www.timescale.com/blog/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/"><strong>Time-series data</strong></a><strong> is ubiquitous. </strong>At Timescale, our mission is to serve developers worldwide and enable them to build exceptional data-driven products that measure everything that matters: software applications, industrial equipment, financial markets, blockchain activity, user actions, consumer behavior, machine learning models, climate change, and more.</p><p>And time-series data comes at you fast, sometimes generating millions of data points per second. Because of the sheer volume and rate of information, time-series data can be complex to query and analyze, even in SQL.</p><p><strong>TimescaleDB hyperfunctions make it easier to manipulate and analyze time-series datasets with fewer lines of SQL code</strong>. Hyperfunctions are purpose-built for the most common and difficult time-series and analytical queries developers write today in SQL. Using hyperfunctions makes you more productive when querying time-series data, which means you can spend less time creating reports, dashboards, and visualizations involving time-series, and spend more time acting on the insights that your work unearths!</p><h2 id="meet-hyperfunctions">Meet Hyperfunctions</h2><p>There are over 70 different TimescaleDB hyperfunctions ready to use today. Here are some of the most popular ones and how they can help you:<br/></p><ul><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#solved-how-to-query-arbitrary-time-intervals-with-date_trunc">Time-based analysis</a>:</strong> <code>time_bucket()</code> and <code>time_bucket_ng()</code> make time-based analysis simpler and easier by enabling you to analyze data over arbitrary time intervals using succinct queries.</li><li><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#first-and-last-"><code><strong>first()</strong></code><strong> and <code>last()</code></strong></a> allow you to get the value of one column as ordered by another (2x faster in TimescaleDB 2.7!).</li><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#simpler-time-weighted-averages">Time-weighted averages</a>:</strong> <code>time_weight()</code> and related hyperfunctions for working with time-weighted averages offer a more elegant way to get an unbiased average when working with irregularly sampled data.</li><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#enhanced-query-readability-and-maintenance-with-function-pipelines-">Function pipelines</a></strong> enable you to analyze data by composing multiple functions, leading to a simpler, cleaner way of expressing complex logic in PostgreSQL (currently experimental).</li><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#better-data-summaries-using-percentile-approximation">Percentile approximation</a></strong> brings percentile analysis to more workflows, enabling you to understand the distribution of your data efficiently (e.g., 10th percentile, mean, or 50th percentile, 90th percentile, etc.) without performing expensive computations over gigantic time-series datasets. When used with<a href="https://www.timescale.com/blog/how-we-made-data-aggregation-better-and-faster-on-postgresql-with-timescaledb-2-7/"> continuous aggregates</a>, you can compute percentiles over any time range of your dataset in near real-time and use them for baselining and normalizing incoming data.</li><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#easier-frequency-analysis-with-frequency-aggregates">Frequency analysis</a>:</strong> <code>Freq_agg()</code> and related frequency analysis hyperfunctions more efficiently find the most common elements out of a set of vastly more varied values vs. brute force calculation.</li><li><strong><a href="https://docs.timescale.com/api/latest/hyperfunctions/histogram/">Histogram</a></strong> shows the data distribution and can offer a better understanding of the segments compared to an average (<a href="https://statisticsbyjim.com/basics/histograms/">more on histograms</a>).</li><li><strong><strong><strong><a href="https://www.timescale.com/blog/slow-grafana-performance-learn-how-to-fix-it-using-downsampling/">Downsampling</a>: </strong></strong></strong>ASAP smoothing smooths datasets to highlight the most important features when graphed. Largest Triangle Three Buckets Downsampling or <code>lttb()</code> reduces the number of elements in a dataset while retaining important features when graphed. <a href="https://www.timescale.com/blog/slow-grafana-performance-learn-how-to-fix-it-using-downsampling/">See how to apply our downsampling hyperfunctions in Grafana.</a></li><li><strong><a href="https://www.timescale.com/blog/how-to-write-better-queries-for-time-series-data-analysis-using-custom-sql-functions/#more-memory-efficient-count-distinct-queries">Memory efficient COUNT DISTINCTs</a></strong>: HyperLogLog is a probabilistic cardinality estimator that uses significantly less memory than the equivalent COUNT DISTINCT query. It is ideal for use in a continuous aggregate for large datasets.</li></ul><p>We created new SQL functions for each of these time-series analysis and manipulation capabilities. This contrasts with other efforts to improve the developer experience by introducing new SQL syntax. While introducing new syntax with new keywords and constructs may have been easier from an implementation perspective, we made the deliberate decision not to do so since we believe it leads to a worse experience for the end-user. </p><p>New SQL syntax means existing drivers, libraries, and tools may no longer work. That can leave developers with more problems than solutions as their favorite tools, libraries, or drivers may not support the new syntax or require time-consuming modifications. On the other hand, new SQL functions mean that your query will run in every visualization tool, database admin tool, or data analysis tool. </p><p>We have the freedom to create custom functions, aggregates, and procedures that help developers better understand and work with their data, and ensure all their drivers and interfaces still work as expected!</p><p>TimescaleDB hyperfunctions come pre-loaded and ready to use on every hosted and managed database service in Timescale Cloud, the easiest way to get started with TimescaleDB.<a href="https://www.timescale.com/timescale-signup"> Get started with a free Timescale Cloud trial</a>‚Äîno credit card required. Or <a href="https://docs.timescale.com/install/latest/">download for free</a> with TimescaleDB self-managed.</p><p>If you‚Äôd like to jump straight into using TimescaleDB hyperfunctions on a real-world dataset, <a href="https://docs.timescale.com/timescaledb/latest/tutorials/nfl-analytics/#analyze-data-using-timescaledb-continuous-aggregates-and-hyperfunctions">start our tutorial</a>, which uses hyperfunctions to uncover insights about players and teams from the NFL (American football). ¬†</p><p>Can‚Äôt find the function you need?<strong> </strong>Open an issue on our <a href="https://github.com/timescale/timescaledb-toolkit/issues">GitHub project</a> or contact us on <a href="https://timescaledb.slack.com/">Slack</a> or via the <a href="http://timescale.com/forum/">Timescale Community Forum</a>. We love to work with our users to simplify SQL!</p><h2 id="solved-how-to-query-arbitrary-time-intervals-with-datetrunc">Solved: How to Query Arbitrary Time-Intervals With date_trunc</h2><p>When using PostgreSQL, the <code><a href="https://www.postgresql.org/docs/current/functions-datetime.html">date_trunc</a></code><a href="https://www.postgresql.org/docs/current/functions-datetime.html"> function</a> can be useful when you want to aggregate information over an interval of time. <code>date_trunc</code> truncates a <code>TIMESTAMP</code> or an <code>INTERVAL</code> value based on a specified date part (e.g., hour, week, or month) and returns the truncated timestamp or interval. For example, <code>date_trunc</code> can aggregate by one second, one hour, one day, or one week. However, you often want to see aggregates by the time intervals that matter most to your use case, which may be intervals like 30 seconds, 5 minutes, 12 hours, etc. This can get pretty complicated in SQL, just look at the query below which analyzes taxi ride activity in five-minute time intervals:</p><p><strong>Regular PostgreSQL: Taxi rides taken every five minutes</strong></p><!--kg-card-begin: markdown--><pre><code>SELECT
  EXTRACT(hour from pickup_datetime) as hours,
  trunc(EXTRACT(minute from pickup_datetime) / 5)*5 AS five_mins,
  COUNT(*)
FROM rides
WHERE pickup_datetime &lt; &#39;2016-01-02 00:00&#39;
GROUP BY hours, five_mins;
</code></pre>
<!--kg-card-end: markdown--><p><strong>The </strong><code><strong>time_bucket()</strong></code><strong> hyperfunction makes it easy to query your data in whatever time interval is most relevant to your analysis use case</strong>. <code>time_bucket()</code> enables you to aggregate data by arbitrary time intervals (e.g., 10 seconds, 5 minutes, 6 hours, etc.), and gives you flexible groupings and offsets, instead of just second, minute, hour, and so on.</p><p>In addition to allowing more flexible time-series queries, <code>time_bucket()</code> also allows you to write these queries in a simpler way. Just look much simpler the query from the example above is to write and understand when using the <code>time_bucket()</code> hyperfunction:</p><p><strong>TimescaleDB hyperfunctions: Taxi rides taken every five minutes</strong></p><!--kg-card-begin: markdown--><pre><code>-- How many rides took place every 5 minutes for the first day of 2016?
SELECT time_bucket(&#39;5 minute&#39;, pickup_datetime) AS five_min, count(*)
FROM rides
WHERE pickup_datetime &lt; &#39;2016-01-02 00:00&#39;
GROUP BY five_min
ORDER BY five_min;
</code></pre>
<!--kg-card-end: markdown--><p>If you‚Äôd like even more flexibility when aggregating your data, you can test out the experimental hyperfunction <code><a href="https://docs.timescale.com/api/latest/hyperfunctions/time_bucket_ng/">time_bucket_ng(</a>)</code>, which is an updated version of the original <code>time_bucket()</code> hyperfunction. <code>time_bucket_ng()</code> enables you to bucket your data by years and months, in addition to second, minute, and hour time intervals. This allows you to easily do monthly cohort analysis or other multiple month-based reports in SQL.</p><!--kg-card-begin: markdown--><pre><code>SELECT timescaledb_experimental.time_bucket_ng(&#39;3 month&#39;, date &#39;2021-08-01&#39;);
 time_bucket_ng
----------------
 2021-07-01
(1 row)
</code></pre>
<!--kg-card-end: markdown--><p><code>time_bucket_ng()</code> also features custom timezone support, which enables you to write queries like the one below, which illustrates using it to bucket data in the Europe/Moscow region:</p><!--kg-card-begin: markdown--><pre><code>-- note that timestamptz is displayed differently depending on the session parameters
SET TIME ZONE &#39;Europe/Moscow&#39;;

SELECT timescaledb_experimental.time_bucket_ng(&#39;1 month&#39;, timestamptz &#39;2001-02-03 12:34:56 MSK&#39;, timezone =&gt; &#39;Europe/Moscow&#39;);
     time_bucket_ng
------------------------
 2001-02-01 00:00:00+03
</code></pre>
<!--kg-card-end: markdown--><p>Missing data or gaps is a common occurrence when capturing hundreds or thousands of time-series readings per second or minute. This can happen due to irregular sampling intervals, or you have experienced an outage of some sort. </p><p>The <code>time_bucket_gapfill()</code> hyperfunction enables you to create additional rows of data in any gaps, ensuring that the returned rows are in chronological order and contiguous. To learn more about gappy data, read our blog <a href="https://www.timescale.com/blog/sql-functions-for-time-series-analysis/">Mind the Gap: Using SQL Functions for Time-Series Analysis</a>.</p><p>Here‚Äôs an example of <code>time_bucket_gapfill()</code> in action, where we find the daily average temperature for a certain device and use the <code>locf()</code> function to carry the last observation forward in the case we have gaps in our data:</p><!--kg-card-begin: markdown--><pre><code>SELECT
  time_bucket_gapfill(&#39;1 day&#39;, time, now() - INTERVAL &#39;1 week&#39;, now()) AS day,
  device_id,
  avg(temperature) AS value,
  locf(avg(temperature))
FROM metrics
WHERE time &gt; now () - INTERVAL &#39;1 week&#39;
GROUP BY day, device_id
ORDER BY day;

           day          | device_id | value | locf
------------------------+-----------+-------+------
 2019-01-10 01:00:00+01 |         1 |       |
 2019-01-11 01:00:00+01 |         1 |   5.0 |  5.0
 2019-01-12 01:00:00+01 |         1 |       |  5.0
 2019-01-13 01:00:00+01 |         1 |   7.0 |  7.0
 2019-01-14 01:00:00+01 |         1 |       |  7.0
 2019-01-15 01:00:00+01 |         1 |   8.0 |  8.0
 2019-01-16 01:00:00+01 |         1 |   9.0 |  9.0
(7 rows)
</code></pre>
<!--kg-card-end: markdown--><p>The last observation carried forward or <code><a href="https://docs.timescale.com/api/latest/hyperfunctions/gapfilling-interpolation/locf/">locf(</a>)</code> function allows you to carry forward the last seen value in an aggregation group. You can only use it in an aggregation query with <code>time_bucket_gapfill</code>.</p><p>To learn more about using the <code>time_bucket</code> family of hyperfunctions, read the <a href="https://docs.timescale.com/api/latest/hyperfunctions/time_bucket/">docs</a>, and get started with our <a href="https://docs.timescale.com/timescaledb/latest/tutorials/nyc-taxi-cab/">tutorial</a> which uses <code>time_bucket()</code> to analyze a real-world IoT dataset.</p><h2 id="simpler-time-weighted-averages">Simpler Time-Weighted Averages</h2><p>If you‚Äôre in a situation where you don&#39;t have regularly sampled data, getting a representative average over a period of time can be a complex and time-consuming query to write. For example, irregularly sampled data, and thus the need for time-weighted averages, frequently occurs in the following cases:</p><ul><li>Industrial IoT, where teams ‚Äúcompress‚Äù data by only sending points when the value changes.</li><li>Remote sensing, where sending data back from the edge can be costly, so you only send high-frequency data for the most critical operations.</li><li>Trigger-based systems, where the sampling rate of one sensor is affected by the reading of another (i.e., a security system that sends data more frequently when a motion sensor is triggered).</li></ul><p>Time-weighted averages are a way to get an unbiased average when you are working with irregularly sampled data.</p><!--kg-card-begin: markdown--><pre><code>CREATE TABLE freezer_temps (
	freezer_id int,
	ts timestamptz,
	temperature float);
</code></pre>
<!--kg-card-end: markdown--><p>And some irregularly sampled time-series data representing the freezer temperature:</p><!--kg-card-begin: markdown--><pre><code>INSERT INTO freezer_temps VALUES 
( 1, &#39;2020-01-01 00:00:00+00&#39;, 4.0), 
( 1, &#39;2020-01-01 00:05:00+00&#39;, 5.5), 
( 1, &#39;2020-01-01 00:10:00+00&#39;, 3.0), 
( 1, &#39;2020-01-01 00:15:00+00&#39;, 4.0), 
( 1, &#39;2020-01-01 00:20:00+00&#39;, 3.5), 
( 1, &#39;2020-01-01 00:25:00+00&#39;, 8.0), 
( 1, &#39;2020-01-01 00:30:00+00&#39;, 9.0), 
( 1, &#39;2020-01-01 00:31:00+00&#39;, 10.5), -- door opened!
( 1, &#39;2020-01-01 00:31:30+00&#39;, 11.0), 
( 1, &#39;2020-01-01 00:32:00+00&#39;, 15.0), 
( 1, &#39;2020-01-01 00:32:30+00&#39;, 20.0), -- door closed
( 1, &#39;2020-01-01 00:33:00+00&#39;, 18.5), 
( 1, &#39;2020-01-01 00:33:30+00&#39;, 17.0), 
( 1, &#39;2020-01-01 00:34:00+00&#39;, 15.5), 
( 1, &#39;2020-01-01 00:34:30+00&#39;, 14.0), 
( 1, &#39;2020-01-01 00:35:00+00&#39;, 12.5), 
( 1, &#39;2020-01-01 00:35:30+00&#39;, 11.0), 
( 1, &#39;2020-01-01 00:36:00+00&#39;, 10.0), -- temperature stabilized
( 1, &#39;2020-01-01 00:40:00+00&#39;, 7.0),
( 1, &#39;2020-01-01 00:45:00+00&#39;, 5.0);

</code></pre>
<!--kg-card-end: markdown--><p>Calculating the time-weighted average temperature of the freezer using regular SQL functions would look something like this:</p><p><strong>Time-weighted averages using regular SQL</strong></p><!--kg-card-begin: markdown--><pre><code>WITH setup AS (
	SELECT lag(temperature) OVER (PARTITION BY freezer_id ORDER BY ts) as prev_temp, 
		extract(&#39;epoch&#39; FROM ts) as ts_e, 
		extract(&#39;epoch&#39; FROM lag(ts) OVER (PARTITION BY freezer_id ORDER BY ts)) as prev_ts_e, 
		* 
	FROM  freezer_temps), 
nextstep AS (
	SELECT CASE WHEN prev_temp is NULL THEN NULL 
		ELSE (prev_temp + temperature) / 2 * (ts_e - prev_ts_e) END as weighted_sum, 
		* 
	FROM setup)
SELECT freezer_id,
    avg(temperature), -- the regular average
	sum(weighted_sum) / (max(ts_e) - min(ts_e)) as time_weighted_average 
</code></pre>
<!--kg-card-end: markdown--><p>But, with the TimescaleDB <code>time_weight()</code> hyperfunction, we reduce this potentially tedious to write and confusing to read query to a much simpler five-line query:<br/></p><!--kg-card-begin: markdown--><pre><code>SELECT freezer_id, 
	avg(temperature), 
	average(time_weight(&#39;Linear&#39;, ts, temperature)) as time_weighted_average 
FROM freezer_temps
GROUP BY freezer_id;

freezer_id |  avg  | time_weighted_average 
------------+-------+-----------------------
          1 | 10.2  |     6.636111111111111
</code></pre>
<!--kg-card-end: markdown--><p>To learn more about using time-weighted average hyperfunctions, read the<a href="https://docs.timescale.com/api/latest/hyperfunctions/time-weighted-averages/"> docs</a> and see our explainer blog post: <a href="https://www.timescale.com/blog/what-time-weighted-averages-are-and-why-you-should-care/">What time-weighted averages are and why you should care</a>.</p><h2 id="better-data-summaries-using-percentile-approximation">Better Data Summaries Using Percentile Approximation</h2><p>Many developers choose to use averages and other summary statistics more frequently than percentiles because they are significantly ‚Äúcheaper‚Äù to calculate over large time-series datasets, both in computational resources and time.</p><p>As we were designing hyperfunctions, we thought about how we could capture the benefits of percentiles (e.g., robustness to outliers, better correspondence with real-world impacts) while avoiding some of the pitfalls of calculating exact percentiles. </p><p>TimescaleDB‚Äôs<strong> percentile approximation hyperfunctions</strong> enable you to understand your data distribution efficiently (e.g., 10th percentile, mean, or 50th percentile, 90th percentile, etc.) without performing expensive computations over gigantic time-series datasets.</p><p>With relatively large datasets, you can often accept some accuracy trade-offs to avoid running into issues of high memory footprint and network costs while enabling percentiles to be computed more efficiently in parallel and used on streaming data. (In this post, you can learn more about the <a href="https://www.timescale.com/blog/how-percentile-approximation-works-and-why-its-more-useful-than-averages/">design decisions and trade-offs made in TimescaleDB‚Äôs percentile approximation hyperfunctions design</a>.)</p><p>TimescaleDB has a whole family of <a href="https://docs.timescale.com/api/latest/hyperfunctions/percentile-approximation/">percentile approximation hyperfunctions</a>. The simplest way to call them is to use the <a href="https://docs.timescale.com/api/latest/hyperfunctions/percentile-approximation/percentile_agg/">percentile_agg aggregate</a> along with the<a href="https://docs.timescale.com/api/latest/hyperfunctions/percentile-approximation/approx_percentile/"> approx_percentile accessor</a>. For example, here‚Äôs how we might calculate the 10th, 50th (mean), and 90th percentiles of the response time of a particular API:</p><!--kg-card-begin: markdown--><pre><code>SELECT 
    approx_percentile(0.1, percentile_agg(response_time)) as p10, 
    approx_percentile(0.5, percentile_agg(response_time)) as p50, 
    approx_percentile(0.9, percentile_agg(response_time)) as p90 
FROM responses;
</code></pre>
<!--kg-card-end: markdown--><p>Hyperfunctions for percentile approximation can also be used in <a href="https://www.timescale.com/blog/how-we-made-data-aggregation-better-and-faster-on-postgresql-with-timescaledb-2-7/">TimescaleDB¬¥s continuous aggregates</a> which make aggregate queries on very large datasets run faster. Continuous aggregates continuously and incrementally store the results of an aggregation query in the background. So, when you run the query, only the changed data needs to be computed, not the entire dataset.</p><p>That is a huge advantage compared to exact percentiles because you can now do things like baselining and alerting on longer periods without recalculating from scratch every time!</p><p>For example, here‚Äôs how you can use continuous aggregates to identify recent outliers and investigate potential problems. First, we create a one-hour aggregation from the hypertable <code>responses</code>:</p><!--kg-card-begin: markdown--><pre><code>CREATE TABLE responses(
	ts timestamptz, 
	response_time DOUBLE PRECISION);
SELECT create_hypertable(&#39;responses&#39;, &#39;ts&#39;);
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code>CREATE MATERIALIZED VIEW responses_1h_agg
WITH (timescaledb.continuous)
AS SELECT 
    time_bucket(&#39;1 hour&#39;::interval, ts) as bucket,
    percentile_agg(response_time)
FROM responses
GROUP BY time_bucket(&#39;1 hour&#39;::interval, ts);
</code></pre>
<!--kg-card-end: markdown--><p>To find outliers, we can find the data in the last 30 seconds greater than the 99th percentile:</p><!--kg-card-begin: markdown--><pre><code>SELECT * FROM responses 
WHERE ts &gt;= now()-&#39;30s&#39;::interval
AND response_time &gt; (
	SELECT approx_percentile(0.99, percentile_agg)
	FROM responses_1h_agg
	WHERE bucket = time_bucket(&#39;1 hour&#39;::interval, now()-&#39;1 hour&#39;::interval)
);
</code></pre>
<!--kg-card-end: markdown--><p>To learn more about using percentile approximation hyperfunctions, read the <a href="https://www.timescale.com/blog/how-percentile-approximation-works-and-why-its-more-useful-than-averages/">docs</a>, try our <a href="https://docs.timescale.com/timescaledb/latest/tutorials/nfl-analytics/">tutorial using real-world NFL data</a> and see our <a href="https://www.timescale.com/blog/how-percentile-approximation-works-and-why-its-more-useful-than-averages/">explainer blog post on why percentile approximation is more useful than averages</a>.</p><h2 id="first-and-last">first( )and last( )</h2><p>Another common problem is finding the first or last values for multiple time series. That often occurs in IoT scenarios, where you want to monitor devices in different locations, but each device sends back data at different times (as devices can go offline, experience connectivity issues, ¬†batch transmit data, or simply have different sampling rates).</p><p>The <code>last</code> hyperfunction allows you to get the value of one column as ordered by another. For example, <code>last(temperature, time)</code> returns the latest temperature value based on time within an aggregate group. </p><p>This way, you can write queries more easily which, for example, will find the last recorded temperature at multiple locations, as each location might have different rates of data being sampled and recorded:</p><!--kg-card-begin: markdown--><pre><code>SELECT location, last(temperature, time)
  FROM conditions
  GROUP BY location;
</code></pre>
<!--kg-card-end: markdown--><p>Similarly, the <code>first</code> hyperfunction also allows you to get the value of one column as ordered by another. <code>first(temperature, time)</code> returns the earliest temperature value based on time within an aggregate group:</p><!--kg-card-begin: markdown--><pre><code>SELECT device_id, first(temp, time)
FROM metrics
GROUP BY device_id;
</code></pre>
<!--kg-card-end: markdown--><p><code>first()</code> and <code>last()</code> can also be used in more complex queries, such as finding the latest value within a specific time interval. In the example below, we find the last temperature recorded for each device in five minutes throughout the past day:</p><!--kg-card-begin: markdown--><pre><code>SELECT device_id, time_bucket(&#39;5 minutes&#39;, time) AS interval,
  last(temp, time)
FROM metrics
WHERE time &gt; now () - INTERVAL &#39;1 day&#39;
GROUP BY device_id, interval
ORDER BY interval DESC;
</code></pre>
<!--kg-card-end: markdown--><p>In TimescaleDB 2.7, we‚Äôve made <a href="https://github.com/timescale/timescaledb/pull/3943">improvements</a> to make queries with the <code>first()</code> and <code>last()</code> hyperfunctions up to twice as fast and make memory usage near constant.</p><div><p>üöÄ</p><p><em><strong>Note:</strong> The last and first commands do not use indexes but perform a sequential scan through their groups. They are primarily used for ordered selection within a GROUP BY aggregate and not as an alternative to an ORDER BY time DESC LIMIT 1 clause to find the latest value (which uses indexes).</em></p></div><p>To learn more, see the docs for <a href="https://docs.timescale.com/api/latest/hyperfunctions/first/"><code>first(</code></a><code>)</code> and <code><a href="https://docs.timescale.com/api/latest/hyperfunctions/last/">last(</a>)</code>.</p><h2 id="more-memory-efficient-count-distinct-queries">More Memory Efficient COUNT DISTINCT Queries<br/></h2><p>Calculating the exact number of distinct values in a large dataset with high cardinality requires lots of computational resources, which can impact the query performance and experience of your database&#39;s concurrent users. </p><p>To solve this issue, TimescaleDB provides hyperfunctions to calculate <a href="https://docs.timescale.com/api/latest/hyperfunctions/approx_count_distincts/">approximate COUNT DISTINCTs</a>. Approximate count distincts do not calculate the exact cardinality of a dataset, but rather estimate the number of unique values, in order to improve compute time. We use <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>, a probabilistic cardinality estimator that uses significantly less memory than the equivalent <code>COUNT DISTINCT</code> query. ¬†</p><p><code><a href="https://docs.timescale.com/api/latest/hyperfunctions/approx_count_distincts/hyperloglog/">Hyperloglog(</a>)</code> is an approximation object for <code>COUNT DISTINCT</code> queries. And the <code><a href="https://docs.timescale.com/api/latest/hyperfunctions/approx_count_distincts/distinct_count/">distinct_count(</a>)</code> accessor function gets the number of distinct values from a HyperLogLog object, as illustrated in the example below, which efficiently estimates the number of unique NFTs and collections in a hypothetical NFT marketplace:<br/></p><!--kg-card-begin: markdown--><pre><code>SELECT
  distinct_count(hyperloglog(32768, asset_id)) AS nft_count,
  distinct_count(hyperloglog(32768, collection_id)) AS collection_count
FROM nft_sales
WHERE payment_symbol = &#39;ETH&#39; AND time &gt; NOW()-INTERVAL &#39;3 months&#39;
</code></pre>
<!--kg-card-end: markdown--><p>You can also use the <code><a href="https://docs.timescale.com/api/latest/hyperfunctions/approx_count_distincts/stderror/">std_error(</a>)</code> function to estimate the relative standard error of the HyperLogLog compared to running <code>COUNT DISTINCT</code> directly. </p><h2 id="enhanced-query-readability-and-maintenance-with-function-pipelines">Enhanced Query Readability and Maintenance With Function Pipelines </h2><div><p>üöÄ</p><p><em><strong>Note:</strong> In the spirit of </em><a href="https://www.timescale.com/blog/blog/move-fast-but-dont-break-things-introducing-the-experimental-schema-with-new-experimental-features-in-timescaledb-2-4/"><em>moving fast and not breaking things</em></a><em>, the hyperfunctions in this section are released as experimental‚Äîplease play around with them but don‚Äôt use them in production.</em></p></div><p>At Timescale, we‚Äôre huge fans of SQL. But as we‚Äôve seen in many examples above, SQL can get quite unwieldy for certain kinds of analytical and time-series queries. Enter TimescaleDB Function Pipelines.</p><p><strong>TimescaleDB Function Pipelines</strong> radically improve the developer ergonomics of analyzing data in PostgreSQL and SQL, by applying principles from <a href="https://en.wikipedia.org/wiki/Functional_programming">functional programming</a> and popular tools like <a href="https://pandas.pydata.org/docs/index.html">Python‚Äôs Pandas</a> and <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/">PromQL</a>. In short, they improve your coding productivity, making your SQL code easier for others to comprehend and maintain.</p><p>Inspired by functional programming languages, Function Pipelines enable you to analyze data by composing multiple functions, leading to a simpler, cleaner way of expressing complex logic in PostgreSQL.</p><p>And the best part: we built Function Pipelines in a fully PostgreSQL-compliant way! We did not change any SQL syntax, meaning that any tool that speaks PostgreSQL will be able to support data analysis using function pipelines.</p><p>To understand the power of TimescaleDB Function Pipelines, consider the following PostgreSQL query.</p><p><strong><strong>Regular PostgreSQL query:</strong></strong><br/></p><!--kg-card-begin: markdown--><pre><code>SELECT device_id, 
	sum(abs_delta) as volatility
FROM (
	SELECT device_id, 
		abs(val - lag(val) OVER (PARTITION BY device_id ORDER BY ts))
        	as abs_delta 
	FROM measurements
	WHERE ts &gt;= now() - &#39;1 day&#39;::interval) calc_delta
GROUP BY device_id;
</code></pre>
<!--kg-card-end: markdown--><p><strong>And a much more readable query using TimescaleDB Function Pipelines:</strong></p><!--kg-card-begin: markdown--><pre><code>SELECT device_id, 
	timevector(ts, val) -&gt; sort() -&gt; delta() -&gt; abs() -&gt; sum() 
    		as volatility
FROM measurements
WHERE ts &gt;= now() - &#39;1 day&#39;::interval
GROUP BY device_id;
</code></pre>
<!--kg-card-end: markdown--><p>It is now much clearer what this query is doing, performing the following sets of tasks:</p><ul><li>Getting the last day‚Äôs data from the measurements table, grouped by <code>device_id</code></li><li>Sorting the data by the time column</li><li>Calculating the delta (or change) between values</li><li>Taking the absolute value of the delta</li><li>And then taking the sum of the result of the previous steps</li></ul><p>To learn more about using TimescaleDB Function Pipelines, read the <a href="https://docs.timescale.com/timescaledb/latest/how-to-guides/hyperfunctions/function-pipelines/">docs </a>and see our explainer blog post: <a href="https://www.timescale.com/blog/function-pipelines-building-functional-programming-into-postgresql-using-custom-operators/">Function Pipelines: Building Functional Programming Into PostgreSQL Using Custom Operators</a>.</p><h2 id="easier-frequency-analysis-with-frequency-aggregates">Easier Frequency Analysis With Frequency Aggregates</h2><div><p>üöÄ</p><p><em><strong>Note</strong>: In the spirit of </em><a href="https://www.timescale.com/blog/blog/move-fast-but-dont-break-things-introducing-the-experimental-schema-with-new-experimental-features-in-timescaledb-2-4/"><em>moving fast and not breaking things</em></a><em>, the hyperfunctions in this section are released as experimental‚Äîplease play around with them but don‚Äôt use them in production.</em></p></div><p>When working with large datasets, frequency analysis is often computationally expensive, as it requires computing over the entire dataset to get an accurate result. Instead of counting all elements of a dataset, you might prefer to<em> estimate</em> the frequency of the most common elements of a set.</p><p>The <strong>frequency aggregate hyperfunction</strong> or <code>freq_agg()</code> enables you to track the approximate frequency within a column of all values with a minimum frequency. </p><p>This example creates a frequency aggregate over a field <code>ZIP</code> (representing ZIP codes) in a HomeSales table. This aggregate tracks any <code>ZIP</code> value occurring in at least 5 % of rows, allowing you to keep track of common ZIP codes efficiently:</p><!--kg-card-begin: markdown--><pre><code>CREATE toolkit_experimental.freq_agg(0.05, ZIP) FROM HomeSales;
</code></pre>
<!--kg-card-end: markdown--><p>You can also use the frequency aggregate hyperfunction to estimate the maximum and minimum frequency of a particular value in your dataset. For example, we can find the most frequent rounded square roots of integers 1-100 using the frequency aggregate hyperfunction as follows:</p><!--kg-card-begin: markdown--><pre><code>SELECT value, min_freq, max_freq 
FROM toolkit_experimental.into_values(
    (SELECT toolkit_experimental.freq_agg(0.15, ceiling(sqrt(v))::int) 
     FROM generate_series(1,100) v), 
    0::int
);

value | min_freq | max_freq 
-------+----------+----------
    10 |     0.19 |     0.24
     9 |     0.17 |      0.2
     8 |     0.15 |     0.16
     7 |     0.13 |     0.13
     6 |     0.11 |     0.11
     5 |     0.09 |     0.09
     4 |     0.07 |     0.07
</code></pre>
<!--kg-card-end: markdown--><p><strong>Fun fact:</strong> <code>Freq_agg</code> is implemented under the hood using the Space-Saving algorithm by <a href="https://www.linkedin.com/in/ahmed-metwally/">Metwally</a>, <a href="https://www.linkedin.com/in/divyakant-agrawal-272632/">Agrawal</a>, and <a href="https://www.linkedin.com/in/amr-el-abbadi-0325b4/">El Abbadi</a> in their paper <a href="https://cs.ucsb.edu/sites/default/files/documents/2005-23.pdf">Efficient Computation of Frequent and Top-k Elements in Data Streams</a>.</p><p>Moreover, we often want to know the top N values of a dataset. To do this, we can use the <strong><code>topn_agg</code> hyperfunction, </strong>which estimates the top values present in a column.</p><p>Here‚Äôs an example of using <code>topn_agg</code> to find the top five most common rounded square roots from 100,000 random numbers in the range (1,1000):</p><!--kg-card-begin: markdown--><pre><code>SELECT toolkit_experimental.topn(
    toolkit_experimental.topn_agg(5, ceiling(sqrt(random() * 1000))::int), 0::int)
FROM generate_series(1,100000);

topn 
------
   31
   30
   29
   28
   27
</code></pre>
<!--kg-card-end: markdown--><p>To learn more about using TimescaleDB hyperfunctions for frequency analysis, <a href="https://docs.timescale.com/api/latest/hyperfunctions/frequency-analysis/">read the docs</a> and <a href="https://www.timescale.com/forum/t/timescale-toolkit-1-6-0-released/437">see this explainer</a> in the Timescale Forum which covers Frequency Aggregate, TopN Aggregate, State Aggregate, and Gauge Aggregate.</p><h2 id="supercharge-your-productivity-with-hyperfunctions-today">Supercharge Your Productivity With Hyperfunctions Today</h2><p><strong>Get started today: </strong>TimescaleDB hyperfunctions come pre-loaded and ready to use on every hosted and managed database service in Timescale Cloud, the easiest way to get started with TimescaleDB.<a href="https://www.timescale.com/timescale-signup"> Get started with a free Timescale Cloud trial</a>‚Äîno credit card required. Or <a href="https://docs.timescale.com/install/latest/">download for free</a> with TimescaleDB self-managed.</p><p>If you‚Äôd like to jump straight into using TimescaleDB hyperfunctions on a real-world dataset, <a href="https://docs.timescale.com/timescaledb/latest/tutorials/nfl-analytics/#analyze-data-using-timescaledb-continuous-aggregates-and-hyperfunctions">start our tutorial</a>, which uses hyperfunctions to uncover insights about players and teams from the NFL (American football). ¬†</p><p><strong>Learn more: </strong>If you‚Äôd like to learn more about TimescaleDB hyperfunctions and how to use them for your use case, read our <a href="https://docs.timescale.com/timescaledb/latest/how-to-guides/hyperfunctions/">How-To Guide</a> and the hyperfunctions <a href="https://docs.timescale.com/api/latest/hyperfunctions/">documentation</a>. </p><p>Can‚Äôt find the function you need?<strong> </strong>Open an issue on our <a href="https://github.com/timescale/timescaledb-toolkit/issues">GitHub project</a> or contact us on <a href="https://timescaledb.slack.com/">Slack</a> or via the <a href="http://timescale.com/forum/">Timescale Community Forum</a>. We love to work with our users to simplify SQL!</p>
        <div>
          <p>The open-source relational database for time-series and analytics.</p>
          <p>
            Try Timescale for free
          </p>

        </div>
      </div></div>
  </body>
</html>
