<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress">Original</a>
    <h1>A bear case: My predictions regarding AI progress</h1>
    
    <div id="readability-page-1" class="page"><div><div><div id="postBody"><div><div><div id="postContent"><div><div><p id="block0">This isn&#39;t really a &#34;timeline&#34;, as such – I don&#39;t know the timings – but this is my current, fairly optimistic take on where we&#39;re heading.</p><p id="block1">I&#39;m not <i>fully</i> committed to this model yet: I&#39;m still on the lookout for more agents and inference-time scaling later this year. But Deep Research, Claude 3.7, Claude Code, Grok 3, and GPT-4.5 have turned out largely in line with these expectations<span data-footnote-reference="" data-footnote-index="1" data-footnote-id="71wj581psb4" role="doc-noteref" id="fnref71wj581psb4"><sup><a href="#fn71wj581psb4">[1]</a></sup></span>, and this is my current baseline prediction.</p><hr/><h2 id="The_Current_Paradigm__I_m_Tucking_In_to_Sleep">The Current Paradigm: I&#39;m Tucking In to Sleep</h2><p id="block2">I expect that none of the currently known avenues of capability advancement are sufficient to get us to AGI<span data-footnote-reference="" data-footnote-index="2" data-footnote-id="znbex4dkqfh" role="doc-noteref" id="fnrefznbex4dkqfh"><sup><a href="#fnznbex4dkqfh">[2]</a></sup></span>.</p><ul><li id="block3"><span>I don&#39;t want to say the pretraining will &#34;plateau&#34;, as such, I do expect continued progress. But the dimensions along which the progress happens are going to decouple from the intuitive &#34;getting generally smarter&#34; metric, and will face steep diminishing returns.</span><ul><span></span><li id="block4">Grok 3 and GPT-4.5 seem to confirm this.<ul><li id="block5">Grok 3&#39;s main claim to fame was &#34;pretty good: it managed to dethrone Claude Sonnet 3.5.1 for some people!&#34;. That was damning with faint praise.</li><li id="block6">GPT-4.5 is subtly better than GPT-4, particularly at writing/EQ. That&#39;s likewise a faint-praise damnation: it&#39;s not <i>much</i> better. Indeed, it reportedly came out below expectations for OpenAI as well, and they certainly weren&#39;t in a rush to release it. (It <a href="https://openai.com/index/openai-board-forms-safety-and-security-committee/">was intended</a> as a new flashy frontier model, not the delayed, half-embarrassed &#34;here it is I guess, hope you&#39;ll find something you like here&#34;.)</li></ul></li><li id="block7">GPT-5 will be even less of an improvement on GPT-4.5 than GPT-4.5 was on GPT-4. The pattern will continue for GPT-5.5 and GPT-6, <a href="https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=DMdFqA4ChTRczRLXs">the ~1000x and 10000x models</a> they may train by 2029 (if they still have the money by then). Subtle quality-of-life improvements and meaningless benchmark jumps, but nothing paradigm-shifting.<ul><li id="block8">(Not to be a scaling-law denier. I believe in them, I do! But they measure <i>perplexity</i>, not general intelligence/real-world usefulness, and Goodhart&#39;s Law is no-one&#39;s ally.)</li></ul></li><li id="block9">OpenAI seem to expect this, what with them apparently <a href="https://x.com/sama/status/1889755723078443244">planning to slap the &#34;GPT-5&#34; label</a> on the Frankenstein&#39;s monster made out of their current offerings instead of on, well, 100x&#39;d GPT-4. They know they can&#39;t cause another hype moment without this kind of trickery.</li></ul></li><li id="block10">Test-time compute/RL on LLMs:<ul><li id="block11"><span>It will not meaningfully generalize beyond domains with easy verification.</span> Some trickery like RLAIF and longer CoTs might provide <i>some</i> benefits, but they would be a fixed-size improvement. It will not cause a hard-takeoff self-improvement loop in &#34;soft&#34; domains.</li><li id="block12">RL will be good enough to turn LLMs into reliable tools for some fixed environments/tasks. They will reliably fall flat on their faces if moved outside those environments/tasks.</li><li id="block13">Scaling CoTs to e. g. millions of tokens or effective-indefinite-size context windows (if that even works) may or may not lead to math being solved. I expect it won&#39;t.<ul><li id="block14">It may not work at all: the real-world returns on investment may end up linear while the costs of pretraining grow exponentially. I mostly expect FrontierMath to be beaten by EOY 2025 (<a href="https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=tzjmsY827Biwh5TtF">it&#39;s not that difficult</a>), but maybe it won&#39;t be beaten for years.<span data-footnote-reference="" data-footnote-index="3" data-footnote-id="ysooye03bqe" role="doc-noteref" id="fnrefysooye03bqe"><sup><a href="#fnysooye03bqe">[3]</a></sup></span></li><li id="block15">Even if it &#34;technically&#34; works to speed up conjecture verification, I&#39;m <a href="https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights?commentId=LWmLexCxAmaR936MC">skeptical</a> on this producing <i>paradigm shifts</i> even in &#34;hard&#34; domains. <i>That</i> task is not actually an easily verifiable one.</li></ul></li><li id="block16">(If math <i>is</i> solved, though, I don&#39;t know how to estimate the consequences, and it might invalidate the rest of my predictions.)</li></ul></li><li id="block17"><span>&#34;But the models </span><i><span>feel</span></i><span> increasingly smarter!&#34;:</span><ul><span></span><li id="block18"><span>It seems to me that &#34;vibe checks&#34; for how smart a model feels are easily gameable by making it have a better personality.</span></li><span></span><li id="block19">My guess is that it&#39;s most of the reason Sonnet 3.5.1 was so beloved. Its personality was made much more <i>appealing</i>, compared to e. g. OpenAI&#39;s corporate drones.</li><li id="block20"><a href="https://www.lesswrong.com/posts/bozSPnkCzXBjDpbHj/ai-104-american-state-capacity-on-the-brink#Huh__Upgrades">The recent upgrade to GPT-4o</a> seems to confirm this. They seem to have merely given it a better personality, and people were reporting that it &#34;feels much smarter&#34;.</li><li id="block21">Deep Research was this for me, at first. Some of its summaries were just <i>pleasant</i> to read, they felt so information-dense and intelligent! Not like typical AI slop at all! But then it turned out most of it was just AI slop underneath anyway, and now my slop-recognition function has adjusted and the effect is gone.</li></ul></li><li id="block22">What LLMs are good at: eisegesis-friendly problems and in-distribution problems.<ul><li id="block23"><a href="https://en.wikipedia.org/wiki/Eisegesis">Eisegesis</a> is &#34;the process of interpreting text in such a way as to introduce one&#39;s own presuppositions, agendas or biases&#34;. LLMs feel very smart when you do the work of making them sound smart on your own end: when the interpretation of their output has a free parameter which you can mentally set to some value which makes it sensible/useful to you.<ul><li id="block24">This includes e. g. philosophical babbling or brainstorming. <i>You</i> do the work of picking good interpretations/directions to explore, <i>you</i> impute the coherent personality to the LLM. And you inject very few bits of steering by doing so, but <a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism#Cyborg_cognition">those bits are load-bearing</a>. If left to their own devices, LLMs won&#39;t pick those obviously correct ideas any more often than chance.<ul><li id="block25">See R1&#39;s CoTs, where it often does... <a href="https://x.com/teortaxesTex/status/1887726953509069109?lang=en">that</a>.</li></ul></li><li id="block26">This also covers <a href="https://x.com/sayashk/status/1887275315824660584">stuff like Deep Research&#39;s outputs</a>. They&#39;re great specifically as high-level overviews of a field, when you&#39;re not relying on them to be <i>comprehensive</i> or <i>precisely on-target</i> or for <i>any given detail</i> to be correct.</li><li id="block27">It feels like this issue is easy to fix. LLMs already have ~all of the needed pieces, they just need to learn to recognize good ideas! Very few steering-bits to inject!</li><li id="block28">This issue felt easy to fix since GPT-3.5, <a href="https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally">or perhaps GPT-2</a>.</li><li id="block29">This issue is not easy to fix.</li></ul></li><li id="block30">In-distribution problems:<ul><li id="block31">One of the core features of the current AIs is the &#34;jagged frontier&#34; of capabilities.</li><li id="block32">This jaggedness is often defended by &#34;ha, as if humans don&#39;t have domains in which they&#39;re laughably bad/as if humans don&#39;t have consistent cognitive errors!&#34;. I believe that counterargument is invalid.</li><li id="block33">LLMs are not good in some domains and bad in others. Rather, they are incredibly good at some <i>specific tasks</i> and bad at <i>other</i> tasks. Even if both tasks are in the same domain, even if tasks A and B are very similar, even if any human that can do A will be able to do B.</li><li id="block34">This is consistent with the constant complaints about LLMs and LLM-based agents being unreliable and their competencies being <i>impossible to predict</i> (<a href="https://www.answer.ai/posts/2025-01-08-devin.html">example</a>).</li><li id="block35">That is: It seems the space of LLM competence shouldn&#39;t be thought of as some short-description-length connected manifold or slice through the space of problems, whose shape we&#39;re simply too ignorant to understand yet. (In which case &#34;LLMs are genuinely intelligent in a way orthogonal to how humans are genuinely intelligent&#34; is valid.)</li><li id="block36">Rather, it seems to be <i>a set of individual points</i> in the problem-space, plus these points&#39; immediate neighbourhoods... Which is to say, <span>the set of problems the solutions to which are present in their training data</span>.<span data-footnote-reference="" data-footnote-index="4" data-footnote-id="nfivgw93jpp" role="doc-noteref" id="fnrefnfivgw93jpp"><sup><a href="#fnnfivgw93jpp">[4]</a></sup></span></li><li id="block37">The impression that they generalize outside it is based on us <a href="https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=vxHcdFb3KLWEQbmRv">having a very poor grasp</a> regarding the solutions to what problems are present in their training data.</li><li id="block38">And yes, there&#39;s <i>some</i> generalization. But it&#39;s dramatically less than the impressions people have of it.</li></ul></li></ul></li><li id="block39">Agency:<ul><li id="block40">Genuine agency, by contrast, requires remaining on-target across <i>long inferential distances</i>: even after your task&#39;s representation becomes very complex in terms of the templates which you had memorized at the start.</li><li id="block41">LLMs still seem as terrible at this as they&#39;d been in the GPT-3.5 age. Software agents break down once the codebase becomes complex enough, game-playing agents get stuck in loops out of which they break out only by accident, etc.</li><li id="block42">They just have bigger sets of templates now, which lets them fool people for longer and makes them useful for marginally more tasks. But the scaling on that seems pretty bad, and this certainly won&#39;t suffice for autonomously crossing the<i> astronomical</i> inferential distances required to usher in the Singularity.</li></ul></li><li id="block43">&#34;But the benchmarks!&#34;<ul><li id="block44">I dunno, I think they&#39;re just not measuring what people think they&#39;re measuring. See the point about in-distribution problems above, plus the possibility of undetected <a href="https://x.com/miru_why/status/1892500715857473777">performance-gaming</a>, plus some <a href="https://x.com/colin_fraser/status/1892721051299172641">subtly but crucially unintentionally-misleading reporting</a>.</li><li id="block45"><p id="block46">Case study: Prior to looking at <a href="https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/">METR&#39;s benchmark</a>, I&#39;d expected that it&#39;s also (unintentionally!) doing some shenanigans that mean it&#39;s not actually measuring LLMs&#39; real-world problem-solving skills. Maybe the problems were secretly in the training data, or there was a selection effect towards simplicity, or the prompts strongly hinted at what the models are supposed to do, or the environment was set up in an unrealistically &#34;clean&#34; way that minimizes room for error and makes solving the problem correctly the path of least resistance (in contrast to messy real-world realities), et cetera.</p><p id="block47">As it turned out, yes, it&#39;s that last one: see the &#34;systematic differences from the real world&#34; <a href="https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/">here</a>. Consider what this means in the light of the previous discussion about inferential distances/complexity-from-messiness.</p></li></ul></li></ul><p id="block48">As I&#39;d said, I&#39;m not 100% sure of that model. Further advancements might surprise me, there&#39;s an explicit carve-out for ??? consequences if math is solved, etc.</p><p id="block49">But the above is my baseline prediction, at this point, and I expect the probability mass on other models to evaporate by this year&#39;s end.</p><hr/><h2 id="Real_World_Predictions">Real-World Predictions</h2><ul><li id="block50">I dare not make the prediction that the LLM bubble will burst in 2025, or 2026, or in any given year in the near future. The AGI labs have a lot of money nowadays, they&#39;re managed by smart people, they have some real products, they&#39;re willing to produce propaganda, and they&#39;re buying their own propaganda (therefore it will appear authentic). They can keep the hype up for a very long time, if they want.<ul><li id="block51">And they do want to. They <i>need</i> it, so as to keep the investments going. Oceans of compute is the only way to collect on the LLM bet they&#39;ve made, in the worlds where that bet can pay off, so they will keep maximizing for investment no matter how dubious the bet&#39;s odds start looking.</li><li id="block52">Because what <i>else</i> are they to do? If they admit to themselves they&#39;re <i>not</i> closing their fingers around godhood after all, what will they have left?</li></ul></li><li id="block53">There will be news of various important-looking breakthroughs and advancements, at a glance looking very solid even to us/experts. Digging deeper, or waiting until the practical consequences of these breakthroughs materialize, will reveal that they&#39;re 80% hot air/hype-generation.<span data-footnote-reference="" data-footnote-index="5" data-footnote-id="vl37irxzr4a" role="doc-noteref" id="fnrefvl37irxzr4a"><sup><a href="#fnvl37irxzr4a">[5]</a></sup></span></li><li id="block54">At some point there might be massive layoffs due to ostensibly competent AI labor coming onto the scene, perhaps because OpenAI will start heavily propagandizing that these mass layoffs must happen. It will be an overreaction/mistake. The companies that act on that will crash and burn, and will be outcompeted by companies that didn&#39;t do the stupid.</li><li id="block55">Inasmuch as LLMs boost productivity, it will mostly be as tools. There&#39;s a subtle but crucial difference between &#34;junior dev = an AI model&#34; and &#34;senior dev + AI models = senior dev + team of junior devs&#34;. Both decrease the demand for junior devs (as they exist today, before they re-specialize into LLM whisperers or whatever). <span>But the latter doesn&#39;t really require LLMs to be capable of end-to-end autonomous task execution, which is the property required for actual transformative consequences.</span><ul><span></span><li id="block56">(And even then, all the rumors about LLMs 10x&#39;ing programmer productivity <a href="https://www.lesswrong.com/posts/tqmQTezvXGFmfSe7f/how-much-are-llms-actually-boosting-real-world-programmer">seem greatly overstated</a>.)</li></ul></li><li id="block57">Inasmuch as human-worker replacements will come, they will be surprisingly limited in scope. I dare not make a prediction regarding the exact scope and nature, only regarding the <i>directionality</i> compared to current expectations.</li><li id="block58">There will be a ton of innovative applications of Deep Learning, perhaps chiefly in the field of biotech, see <a href="https://www.technologyreview.com/2025/01/17/1110086/openai-has-created-an-ai-model-for-longevity-science/">GPT-4b</a> and <a href="https://x.com/IterIntellectus/status/1892251343881937090">Evo 2</a>. Those are, I must stress, <i>human-made innovative applications of the paradigm of automated continuous program search. </i>Not AI models autonomously producing innovations.</li><li id="block59">There will be various disparate reports about AI models autonomously producing innovations, in the vein of <a href="https://x.com/SakanaAILabs/status/1892385766510338559">this</a> or <a href="https://www.biorxiv.org/content/10.1101/2025.02.19.639094v1">that</a> or <a href="https://arxiv.org/abs/2502.13025">that</a>. They will turn out to be misleading or cherry-picked. E. g., examining those examples:<ul><li id="block60">In the first case, most of the improvements <a href="https://x.com/miru_why/status/1892500715857473777">turned out to be reward-hacking</a> (and not even intentional on the models&#39; part).</li><li id="block61"><a href="https://x.com/jimnasyum/status/1892757709461533062">In the second case</a>, the scientists have pre-selected the problem on which the LLM is supposed to produce the innovation on the basis of already knowing that there&#39;s a low-hanging fruit to be picked there. That&#39;s like 90% of the work. And then they <i>further</i> picked the correct hypothesis from the set it generated, i. e., did eisegesis. And <i>also</i> there might be <a href="https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights?commentId=YhbWT4JaXK3EHbcQK">any amount of data contamination</a> from these scientists or different groups speaking about their research in public, in the years they spent working on it.</li><li id="block62">In the third case, the AI produces useless slop with steps like &#34;..., Step N: invent the Theory of Everything (left as an exercise for the reader), ...&#34;, lacking <a href="https://colah.github.io/notes/taste/">the recognition function</a> for promising research. GPT-3-level stuff. (The whole setup can also likely be out-performed by taking the adjacency matrix of Wikipedia pages and randomly sampling paths from the corresponding graph, or <a href="https://dl.acm.org/doi/10.1145/2623330.2623623">something like this</a>.)</li></ul></li><li id="block63">I expect that by 2030s, LLMs will be heavily integrated into the economy and software, and will serve as very useful tools that found their niches. But just that: tools. Perhaps some narrow jobs will be greatly transformed or annihilated (by being folded into the job of an LLM nanny). But there will not be AGI or broad-scope agents arising from the current paradigm, nor autonomous 10x engineers.</li><li id="block64"><span>At some unknown point – probably in 2030s</span>, possibly tomorrow (but likely not tomorrow) – someone will figure out a different approach to AI. Maybe a slight tweak to the LLM architecture, maybe a completely novel neurosymbolic approach. <span>Maybe it will happen in a major AGI lab, maybe in some new startup. By default, everyone will die in &lt;1 year after that.</span></li><span></span></ul><hr/><h2 id="Closing_Thoughts">Closing Thoughts</h2><p id="block65"><span>This might seem like a ton of annoying nitpicking.</span> Here&#39;s a simple generator of all of the above observations: <strong>some people desperately, </strong><i><strong>desperately</strong></i><strong> want LLMs to be a bigger deal than what they are</strong>.</p><p id="block66">They are not evaluating the empirical evidence in front of their eyes with proper precision<i>.</i><span data-footnote-reference="" data-footnote-index="6" data-footnote-id="lph4528t2" role="doc-noteref" id="fnreflph4528t2"><sup><a href="#fnlph4528t2">[6]</a></sup></span> Instead, they&#39;re vibing, and spending 24/7 inventing contrived ways to fool themselves and/or others.</p><p id="block67">They often succeed. They will continue doing this for a long time to come.</p><p id="block68">We, on the other hand, desperately <i>not</i> want LLMs to be AGI-complete. Since we try to avoid motivated thinking, to avoid deluding ourselves into believing into happier realities, we err on the side of pessimistic interpretations. In this hostile epistemic environment, that effectively leads to us being <i>overly gullible</i> and <i>prone to buying into hype</i>.</p><p id="block69">Indeed, this environment is essentially optimized for exploiting <a href="https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality">the virtue of lightness</a>. LLMs are masters at creating the <i>vibe</i> of being generally intelligent. Tons of people are cooperating, playing this vibe up, making tons of subtly-yet-crucially flawed demonstrations. Trying to see through this immense storm of bullshit very much <i>feels</i> like &#34;fighting a rearguard retreat against the evidence&#34;.<span data-footnote-reference="" data-footnote-index="7" data-footnote-id="c0fnmg1rwsj" role="doc-noteref" id="fnrefc0fnmg1rwsj"><sup><a href="#fnc0fnmg1rwsj">[7]</a></sup></span></p><p id="block70">But this isn&#39;t what&#39;s happening, in my opinion. On the contrary: it&#39;s the LLM believers who are sailing against the winds of evidence.</p><p id="block71">If LLMs were <i>actually</i> as powerful as they&#39;re hyped up to be, there wouldn&#39;t be the need for all of these attempts at <i>handholding</i>.</p><p id="block72">Ever more contrived agency scaffolds that yield ~no improvement. Increasingly more costly RL training procedures that fail to generalize. Hail-mary ideas regarding how to fix that generalization issue. Galaxy-brained ways to elicit knowledge out of LLMs that produce nothing of value. The need for all of this is strong evidence that <i>there&#39;s no seed of true autonomy/agency/generality within LLMs</i>. If there were, the most naïve AutoGPT setup circa early 2023 would&#39;ve elicited it.</p><p id="block73">People are extending LLMs a hand, hoping to pull them up to our level. But there&#39;s nothing reaching back.</p><p id="block74">And none of the current incremental-scaling approaches will fix the issue. They will increasingly mask it, and some of this masking may be powerful enough to have real-world consequences. But any attempts at the Singularity based on LLMs will stumble well before takeoff.</p><p id="block75">Thus, I expect AGI Labs&#39; AGI timelines have ~nothing to do with what will actually happen. On average, we likely have more time than the AGI labs say. Pretty likely that we have until 2030, maybe well into 2030s.</p><p id="block76">By default, we likely don&#39;t have <i>much</i> longer than that. Incremental scaling of known LLM-based stuff won&#39;t get us there, but I don&#39;t think the remaining qualitative insights are many. 5-15 years, at a rough guess.</p><ol data-footnote-section="" role="doc-endnotes"><li data-footnote-item="" data-footnote-index="1" data-footnote-id="71wj581psb4" role="doc-endnote" id="fn71wj581psb4"><span data-footnote-back-link="" data-footnote-id="71wj581psb4"><sup><strong><a href="#fnref71wj581psb4">^</a></strong></sup></span><p id="block78">For prudency&#39;s sake: GPT-4.5 has slightly overshot these expectations.</p></li><li data-footnote-item="" data-footnote-index="2" data-footnote-id="znbex4dkqfh" role="doc-endnote" id="fnznbex4dkqfh"><span data-footnote-back-link="" data-footnote-id="znbex4dkqfh"><sup><strong><a href="#fnrefznbex4dkqfh">^</a></strong></sup></span><p id="block80">If you are really insistent on calling the current crop of SOTA models &#34;AGI&#34;, replace this with &#34;autonomous AI&#34; or &#34;transformative AI&#34; or &#34;innovative AI&#34; or &#34;the transcendental trajectory&#34; or something.</p></li><li data-footnote-item="" data-footnote-index="3" data-footnote-id="ysooye03bqe" role="doc-endnote" id="fnysooye03bqe"><span data-footnote-back-link="" data-footnote-id="ysooye03bqe"><sup><strong><a href="#fnrefysooye03bqe">^</a></strong></sup></span><p id="block82">Will o4 really come out <a href="https://x.com/_jasonwei/status/1870184982007644614">on schedule</a> in ~2 weeks, showcasing yet another dramatic jump in mathematical capabilities, just in time to rescue OpenAI from the GPT-4.5 semi-flop? I&#39;ll be waiting.</p></li><li data-footnote-item="" data-footnote-index="4" data-footnote-id="nfivgw93jpp" role="doc-endnote" id="fnnfivgw93jpp"><span data-footnote-back-link="" data-footnote-id="nfivgw93jpp"><sup><strong><a href="#fnrefnfivgw93jpp">^</a></strong></sup></span></li><li data-footnote-item="" data-footnote-index="5" data-footnote-id="vl37irxzr4a" role="doc-endnote" id="fnvl37irxzr4a"><span data-footnote-back-link="" data-footnote-id="vl37irxzr4a"><sup><strong><a href="#fnrefvl37irxzr4a">^</a></strong></sup></span><div data-footnote-content=""><p id="block86">Pretty sure Deep Research could not in fact <a href="https://x.com/sama/status/1886220904088162729">&#34;do a single-digit percentage of all economically valuable tasks in the world&#34;</a>, except in the caveat-laden sense where you still have a human expert double-checking and rewriting its outputs. And in my personal experience, on the topics at which I <i>am</i> an expert, it would be easier to write the report from scratch than to rewrite DR&#39;s output.</p><p id="block87">It&#39;s a useful way to get a high-level overview of some topics, yes. It blows Google out of the water at being Google, and then some. But I don&#39;t think it&#39;s a 1-to-1 replacement for any extant form of human labor. Rather, it&#39;s a useful zero-to-one thing.</p></div></li><li data-footnote-item="" data-footnote-index="6" data-footnote-id="lph4528t2" role="doc-endnote" id="fnlph4528t2"><span data-footnote-back-link="" data-footnote-id="lph4528t2"><sup><strong><a href="#fnreflph4528t2">^</a></strong></sup></span></li><li data-footnote-item="" data-footnote-index="7" data-footnote-id="c0fnmg1rwsj" role="doc-endnote" id="fnc0fnmg1rwsj"><span data-footnote-back-link="" data-footnote-id="c0fnmg1rwsj"><sup><strong><a href="#fnrefc0fnmg1rwsj">^</a></strong></sup></span><p id="block91">Indeed, even now, having written all of this, I have nagging doubts that this might be what I&#39;m actually doing here. I will probably keep having those doubts until this whole thing ends, one way or another. It&#39;s not pleasant.</p></li></ol></div></div></div></div></div></div></div><p><span><span><div><div><div><div><div id="HL95uH9Dg2ShzuupH"><div><div><div><div><div><div><blockquote><p><strong>some people desperately, </strong><i><strong>desperately</strong></i><strong> want LLMs to be a bigger deal than what they are</strong>.</p></blockquote><p>A larger number of people, I think, desperately desperately want LLMs to be a smaller deal than what they are.</p></div></div></div></div></div></div><div><div><div id="DEu8b9omYcfeFdiZF"><div><div><div><div><div><div><p>The more mainstream you go, the larger this effect gets. A lot of people seemingly <i>want </i>AI to be a nothingburger.</p><p>When LLMs emerged, in mainstream circles, you&#39;d see people go &#34;it&#39;s not important, it&#39;s not actually intelligent, you can see it make the kind of reasoning mistakes a 3 year old would&#34;.</p><p>Meanwhile, on LessWrong: &#34;holy shit, this is a big fucking deal, because it&#39;s already making the same kind of reasoning mistakes a human three year old would!&#34;</p><p>I&#39;d say that LessWrong is far better calibrated.</p><p>People who weren&#39;t familiar with programming or AI didn&#39;t have a grasp of how hard natural language processing or commonsense reasoning used to be for machines. Nor do they grasp the implications of scaling laws.</p></div></div></div></div></div></div><div><div><div id="uGASEo8uySxb62q27"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T14:03:29.719Z">4d</time></span></p><p>FWIW, that was me in 2022, looking at GPT-3.5 and being unable to imagine how capabilities can progress from there that doesn&#39;t immediately hit ASI. (I don&#39;t think I ever cared about benchmarks. Brilliant humans can&#39;t necessarily ace math exams, so why would I gatekeep the AGI term behind that?)

Now it&#39;s two-and-a-half years later and I no longer see it. As far as I&#39;m concerned, this paradigm harnessed most of its general-reasoning potential at 3.5 and is now asymptoting out around something. I don&#39;t know what this something is, but it doesn&#39;t seem to be &#34;AGI&#34;.

All &#34;improvement&#34; since then has just been window dressing; the models learning to convincingly babble about ever-more-sophisticated abstractions and solve ever-more-complicated math/coding puzzles that make their capabilities legible to ever-broader categories of people. But it&#39;s not anything GPT-3.5 wasn&#39;t already fundamentally capable of; and GPT-3.5 was not capable of taking off, and there&#39;s been no new fundamental capability advances since then.

(I remember dreading GPT-4, and then it came out, and sure enough people were freaking out, and then I looked at what they were freaking out over, and it was... its ability to solve marginally harder physics puzzles? Oh. Oh no, that&#39;s... scary?)

Now, granted, it&#39;s possible that you can take these LLM things, and use their ability to babble their way through short-horizon math/coding puzzles to jury-rig something that&#39;s capable of taking off. I don&#39;t mean to say that LLMs are useless or unimpressive; that scenario is where my other 20% are at.

But it seems increasingly less likely to me with each passing day and each new underwhelming advancement.</p></div></div></span></p></div></div></div></div></div><div><div id="qGmq9YLHF7JmuKqW7"><div><div><div><div><div><div><p>Yup, the situation <i>is</i> somewhat symmetrical here; see also the discussion regarding which side is doing the sailing-against-the-winds-of-evidence.</p><p>My &#34;tiebreaker&#34; there is direct empirical evidence from working with LLMs, including attempts to replicate the most impressive and concerning claims about them. So far, this source of evidence has left me thoroughly underwhelmed.</p></div></div></div></div></div></div></div></div><div><div id="p7mxxLAB8kLx6bmZx"><p><span><div><div><p><span>7</span><span>Rafael Harth</span><span><time datetime="2025-03-07T14:05:32.918Z">3d</time></span></p><p>Can confirm that I&#39;m one of these people (and yes, I worry a lot about this clouding my judgment).</p></div></div></span></p></div></div><div><div id="46xd5gLpZra6R5jqS"><p><span><div><div><p><span>7</span><span>yo-cuddles</span><span><time datetime="2025-03-05T21:39:16.127Z">4d</time></span></p><p>Definitely! However, there is more money and &#34;hype&#34; in the direction of wanting these to scale into AGI.

Hype and anti-hype don&#39;t cancel each other out, if someone invests a billion dollars into LLM&#39;s, someone else can&#39;t spend negative 1 billion and it cancels out: the billion dollar spender is the one moving markets, and getting a lot of press attention.

We have Yudkowsky going on destiny, I guess?</p></div></div></span></p></div></div><div><div id="dJMntfv32yyuhP7bn"><p><span><div><div><p><span>2</span><span>johnkclark</span><span><time datetime="2025-03-06T11:51:28.012Z">4d</time></span></p><p>I agree. I think some people are whistling past the graveyard. </p></div></div></span></p></div></div></div></div></div><div><div id="cp29fNkpeb6NtZmfF"><div><div><div><div><div><p>Noting for the sake of later evaluation: this rough picture matches my current median expectations. Not very high confidence; I&#39;d give it roughly 60%.</p></div></div></div></div></div><div><div><div id="mxvGdmR6sgAJR9eZe"><div><div><div><div><div><div><p>I give it ~70%, except caveats:</p><blockquote><p>&#34;Maybe a slight tweak to the LLM architecture, maybe a completely novel neurosymbolic approach.&#34;</p></blockquote><p>It won&#39;t be neurosymbolic.</p><p>Also I don&#39;t see where the 2030 number is coming from. At this point my uncertainty is almost in the exponent again. Seems like decades is plausible (maybe &lt;50% though).</p><p>It&#39;s not clear that only one breakthrough is necessary. </p></div></div></div></div></div></div><div><div><div id="NEnuXmxLzX6qke5tq"><div><div><div><div><div><div><p>Without an intelligence explosion, it&#39;s around 2030 that scaling through increasing funding <a href="https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress?commentId=BCk4JAJWakQQovnXL">runs out of steam</a> and slows down to the speed of chip improvement. This slowdown happens around the same time (maybe 2028-2034) even with a lot more commercial success (if that success precedes the slowdown), because scaling faster takes exponentially more money. So there&#39;s more probability density of transformative advances before ~2030 than after, to the extent that scaling contributes to this probability.</p>
<p>That&#39;s my reason to see 2030 as a meaningful threshold, Thane Ruthenis might be pointing to it for different reasons. It seems like it should certainly be salient for AGI companies, so a long timelines argument might want to address their narrative up to 2030 as a distinct case.</p>
</div></div></div></div></div></div></div></div><div><div id="EAwy5Nw5b4gNdrhGK"><p><span><div><div><p><span>4</span><span>Auspicious</span><span><time datetime="2025-03-10T00:04:26.605Z">7h</time></span></p><p>I also found that take very unusual, especially when combined with this:

The last sentence seems extremely overconfident, especially combined with the otherwise bearish conclusions in this post. I&#39;m surprised no one else has mentioned it.</p></div></div></span></p><div><div><div id="f3v32pGmKjf3i8Wwj"><p><span><div><div><p><span>6</span><span>Cole Wyeth</span><span><time datetime="2025-03-10T00:07:20.188Z">7h</time></span></p><p>Yeah, I agree - overall I agree pretty closely with Thane about LLMs but his final conclusions don&#39;t seem to follow from the model presented here.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="wxKjcca3dDftdCkvM"><div><div><div><div><div><p>I&#39;m at ~80%, for comparison.</p></div></div></div></div></div></div></div><div><div id="jsrbfiLxpzCbQRfdL"><p><span><div><div><p><span>6</span><span>Kaj_Sotala</span><span><time datetime="2025-03-06T07:48:24.611Z">4d</time></span></p><p>I think I&#39;m also around 60-70% for the rough overall picture in the OP being correct.</p></div></div></span></p></div></div></div></div></div><div><div id="jiMbyDh4uyhzj8Fsc"><div><div><div><div><div><div><blockquote><p>&#34;Maybe a slight tweak to the LLM architecture, maybe a completely novel neurosymbolic approach.&#34;</p></blockquote><p>I think you might be underestimating the power of incremental, evolutionary improvements over time where near-term problems are constantly solved and this leads to gradual improvement. After all, human intelligence is the result of gradual evolutionary change and increasing capabilities over time. It&#39;s hard to point to a specific period in history where humans achieved general intelligence.</p><p>Currently LLMs are undoubtedly capable at many tasks (e.g. coding, general knowledge) and much more capable than their predecessors. But it&#39;s hard to point at any particular algorithmic improvement or model and say that it was key to the success of modern LLMs.</p><p>So I think it&#39;s possible that we&#39;ll see more gradual progress and tweaks on LLMs that lead towards increasingly capable models and eventually yield AGI. Eventually you could call this progress a new architecture even though all the progress is gradual.</p></div></div></div></div></div></div><div><div><div id="mZbAwJXFyNZ6MGpta"><div><div><div><div><div><div><p>I don&#39;t think that&#39;s how it works. Local change accumulating into qualitative improvements over time is a property of <i>continuous</i>(-ish) search processes, such as the gradient descent and, indeed, evolution.</p><p>Human technological progress is instead a discrete-search process. We didn&#39;t invent the airplane by incrementally iterating on carriages; we didn&#39;t invent the nuclear bomb by tinkering with TNT.</p><p>The core difference between discrete and continuous search is that... for continuous search, there must be some sort of &#34;general-purpose substrate&#34; such that (1) any given object in the search-space can be defined as some parametrization of this substrate, and (2) this substrate then allows a way to plot a continuous path between any two objects such that all intermediate objects are also useful. For example:</p><ul><li>For evolution, it&#39;s the genome: you could move from any organism to any other organism by doing incremental DNA adjustments, and the in-between organisms must be competitive.</li><li>For ML, it&#39;s the model&#39;s parameters: for any two programs that can be implemented on a given architecture, you can plot the path from one of them to the other, and this path is followed if it&#39;s the path of gradually</li></ul><p>... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="8EspkHAJFrePnQDAS"><div><div><div><div><div><div><p>A continuous manifold of possible technologies is not required for continuous progress. All that is needed is for there to be many possible sources of improvements that can accumulate, and for these improvements to be small once low-hanging fruit is exhausted.</p><p>Case in point: the <a href="https://github.com/KellerJordan/modded-nanogpt">nanogpt speedrun</a>, where the training time of a small LLM was reduced by 15x using 21 distinct innovations which touched basically every part of the model, including the optimizer, embeddings, attention, other architectural details, quantization, hyperparameters, code optimizations, and Pytorch version.</p><p>Most technologies are like this, and frontier AI has even more sources of improvement than the nanogpt speedrun because you can also change the training data and hardware. It&#39;s not impossible that there&#39;s a moment in AI like the <a href="https://aiimpacts.org/discontinuous-progress-investigation/">invention of lasers or the telegraph</a>, but this doesn&#39;t happen with most technologies, and the fact that we have scaling laws somewhat points towards continuity even as other things like small differences being amplified in downstream metrics point to discontinuity. Also see my comment <a href="https://www.lesswrong.com/posts/tDkYdyJSqe3DddtK4/alexander-gietelink-oldenziel-s-shortform?commentId=M8Z7A5vFKjxdH9iKe">here</a> on a similar topic.</p><p>If you think generalization is limited in the current regime, try to create AGI... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="yxGhrRCpvnXZBFB8e"><p><span><div><div><p><span>6</span><span>johnswentworth</span><span><time datetime="2025-03-06T19:30:52.995Z">3d</time></span></p><p>I think you should address Thane&#39;s concrete example:

That seems to me a pretty damn solid knock-down counterargument. There were no continuous language model scaling laws before the transformer architecture, and not for lack of people trying to make language nets.</p></div></div></span></p><div><div><div id="ZtHDJzXLN8Lz6M535"><div><div><div><div><div><div><blockquote><p>There were no continuous language model scaling laws before the transformer architecture</p></blockquote><p><a href="https://arxiv.org/abs/1712.00409">https://arxiv.org/abs/1712.00409</a> was technically published half a year after transformers, but it shows power-law language model scaling laws for LSTMs (several years before the <a href="https://arxiv.org/abs/2001.08361">Kaplan et al.</a> paper, and without citing the transformer paper). It&#39;s possible that transformer scaling laws are much better, I haven&#39;t checked (and perhaps more importantly, transformer training lets you parallelize across tokens), just mentioning this because it seems relevant for the overall discussion of continuity in research.</p><p>I also agree with Thomas Kwa&#39;s sibling comment that transformers weren&#39;t a single huge step. Fully-connected neural networks seem like a very strange comparison to make, I think the interesting question is whether transformers were a sudden single step relative to LSTMs. But I&#39;d disagree even with that: Attention was introduced three years before transformers and was a big deal for machine translation. Self-attention was introduced somewhere between the first attention papers and transformers. And the transformer paper itself isn&#39;t atomic, it consists of multiple ideas—replacing RNNs/LSTMs with ... <span>(read more)</span></p></div></div></div></div></div></div></div></div><div><div id="4wneJer6hgbmbu2T3"><div><div><div><div><div><div><p>Though the fully connected -&gt; transformers wasn&#39;t infinite small steps, it definitely wasn&#39;t a single step. We had to invent various sub-innovations like skip connections separately, progressing from RNNs to LSTM to GPT/BERT style transformers to today&#39;s transformer++. The most you could claim is a single step is LSTM -&gt; transformer.</p><p>Also if you graph perplexity over time, there&#39;s basically no discontinuity from introducing transformers, just a possible change in slope that might be an artifact of switching from the purple to green measurement method. The story looks more like transformers being more able to utilize the exponentially increasing amounts of compute that people started using just before its introduction, which caused people to invest more in compute and other improvements over the next 8 years.</p><p>We could get another single big architectural innovation that gives better returns to more compute, but I&#39;d give a 50-50 chance that it would be only a slope change, not a discontinuity. Even conditional on discontinuity it might be pretty small. Personally my timelines are also short enough that there is limited time for this to happen before we get AGI.</p><figure><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4wneJer6hgbmbu2T3/y2bg1prbszro0s9zm006"/></figure></div></div></div></div></div></div><div><div><div id="qPdiFgL8ivyzEbBdY"><p><span><div><div><p><span>7</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T22:29:00.037Z">3d</time></span></p><p>This argument still seems to postdict that cars were invented by tinkering with carriages and horse-breeding, spacecraft was invented by tinkering with planes, refrigerators were invented by tinkering with cold cellars, et cetera.

If you take the snapshot of the best technology that does X at some time T, and trace its lineage, sure, you&#39;ll often see the procession of iterative improvements on some concepts and techniques. But that line won&#39;t necessarily pass through the best-at-X technologies at times from 0 to T - 1.

The best personal transportation method were horses, then cars. Cars were invented by iterating on preceding technologies and putting them together; but horses weren&#39;t involved. Similar for the best technology at lifting a human being into the sky, the best technology for keeping food cold, etc.

I expect that&#39;s the default way significant technological advances happen. They don&#39;t come from tinkering with the current-best-at-X tech. They come from putting together a bunch of insights from different or non-mainstream tech trees, and leveraging them for X in a novel way.

And this is what I expect for AGI. It won&#39;t come from tinkering with LLMs, it&#39;ll come from a continuous-in-retrospect, surprising-in-advance contribution from some currently-disfavored line(s) of research.

(Edit: I think what I would retract, though, is the point about there not being a continuous manifold of possible technological artefacts. I think something like &#34;the space of ideas the human mind is capable of conceiving&#34; is essentially it.)</p></div></div></span></p><div><div><div id="JJdhF7mkjW6nfWJrA"><div><div><div><div><div><div><p>I think we have two separate claims here:</p><ol><li>Do technologies that have lots of resources put into their development generally improve discontinuously or by huge slope changes?</li><li>Do technologies often get displaced by technologies with a different lineage?</li></ol><p>I agree with your position on (2) here. But it seems like the claim in the post that sometime in the 2030s someone will make a single important architectural innovation that leads to takeover within a year mostly depends on (1), as it would require progress within that year to be comparable to all the progress from now until that year. Also you said the architectural innovation might be a slight tweak to the LLM architecture, which would mean it shares the same lineage.</p><p>The history of machine learning seems pretty continuous wrt advance prediction. In the Epoch graph, the line fit on loss of the best LSTM up to 2016 sees a slope change of less than 2x, whereas a hypothetical innovation that causes takeover within a year with not much progress in the intervening 8 years would be ~8x. So it seems more likely to me (conditional on 2033 timelines and a big innovation) that we get some architectural innovation which has a moderately different l... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="dAyYThkmBj6SQ2CY9"><p><span><div><div><p><span>4</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T20:23:23.979Z">2d</time></span></p><p>Indeed, and I&#39;m glad we&#39;ve converged on (2). But...

... On second thoughts, how did we get there? The initial disagreement was how plausible it was for incremental changes to the LLM architecture to transform it into a qualitatively different type of architecture. It&#39;s not about continuity-in-performance, it&#39;s about continuity-in-design-space.

Whether finding an AGI-complete architecture would lead to a discontinuous advancement in capabilities, to FOOM/RSI/sharp left turn, is a completely different topic from how smoothly we should expect AI architectures&#39; designs to change. And on that topic, (a) I&#39;m not very interested in reference-class comparisons as opposed to direct gears-level modeling of this specific problem, (b) this is a bottomless rabbit hole/long-standing disagreement which I&#39;m not interested in going into at this time.

That&#39;s an interesting general pattern, if it checks out. Any guesses why that might be the case?

My instinctive guess is the new-paradigm approaches tend to start out promising-in-theory, but initially very bad, people then tinker with prototypes, and the technology becomes commercially viable the moment it&#39;s at least marginally better than the previous-paradigm SOTA. Which is why there&#39;s an apparent performance-continuity despite a lineage/paradigm-discontinuity.</p></div></div></span></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="35Nudg3BdPZQBfeXg"><p><span><div><div><p><span>6</span><span>p.b.</span><span><time datetime="2025-03-06T19:19:18.431Z">3d</time></span></p><p>Because these benchmarks are all in the LLM paradigm: Single input, single output from a single distribution. Or they are multi-step problems on rails. Easy verification makes for benchmarks that can quickly be cracked by LLMs. Hard verification makes for benchmarks that aren&#39;t used.

One could let models play new board/computer games against average humans: Video/image input, action output. 

One could let models offer and complete tasks autonomously on freelancer platforms. 

One could enrol models in remote universities and see whether they autonomously reach graduation. 

It&#39;s not difficult to come up with hard benchmarks for current models (these are not close to AGI complete). I think people don&#39;t do this because they know that current models would be hopeless at benchmarks that actually aim for their shortcomings (agency, knowledge integration + integration of sensory information, continuous learning, reliability, ...)</p></div></div></span></p><div><div><div id="wgtvF3LYDqYcACGNm"><p><span><div><div><p><span>7</span><span>Thomas Kwa</span><span><time datetime="2025-03-07T00:23:55.769Z">3d</time></span></p><p>Agree, this is one big limitation of the paper I&#39;m working on at METR. The first two ideas you listed are things I would very much like to measure, and the third something I would like to measure but is much harder than any current benchmark given that university takes humans years rather than hours. If we measure it right, we could tell whether generalization is steadily improving or plateauing.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="X6gQLgLBqCWJ8QzgX"><p><span><div><div><p><span>7</span><span>Gunnar_Zarncke</span><span><time datetime="2025-03-06T05:07:29.151Z">4d</time></span></p><p>Evolution also deals with discrete units. Either the molecule replicates or it doesn&#39;t. Granted, physical evolutions is more massively parallel, but the search space is smaller in biology, but the analogy should hold as long as the search space is large enough to hide the discreteness. And if 10000s of developers try 100s of small alternatives, some few of them might hit the transformer. </p></div></div></span></p></div></div><div><div id="FBgJBoM9RRwH8rycB"><p><span><div><div><p><span>1</span><span>dysangel</span><span><time datetime="2025-03-07T21:56:07.087Z">2d</time></span></p><p>&gt;There&#39;s no equivalent in technology. There isn&#39;t some &#34;general-purpose technological substrate&#34; such that you can start with any technological artefact, slightly perturb it, iterate, and continuously reach any other technological artefact. Discontinuous/discrete changes are needed.

It sounds like you&#39;re almost exactly describing neural nets and backpropagation. A general purpose substrate that you slightly perturb to continuously and gradually move towards the desired output. I believe that as we have better ideas for self play, focusing on quality of thought processes over general knowledge, that we&#39;ll see some impressive results. I think we&#39;re already seeing signs of this in the increasing quality of smaller models.</p></div></div></span></p></div></div></div></div></div><div><div id="GhYnthvHwDBELggc6"><p><span><div><div><p><span>2</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T18:12:05.379Z">4d</time></span></p><p>I actually looked into that recently. My initial guess was this was about &#34;the context window&#34; as a concept. It allows to keep vast volumes of task-relevant information around, including the outputs of the model&#39;s own past computations, without lossily compressing that information into a small representation (like with RNNs). I asked OpenAI&#39;s DR about it, and its output seems to support that guess.

In retrospect, it makes sense that this would work better. If you don&#39;t know what challenges you&#39;re going to face in the future, you don&#39;t necessarily know what past information to keep around, so a fixed-size internal state was a bad idea.</p></div></div></span></p></div></div></div></div></div><div><div id="BCk4JAJWakQQovnXL"><div><div><div><div><div><div><p>I&#39;m not sure raw compute (as opposed to effective compute) GPT-6 (10,000x GPT-4) by 2029 is plausible (without new commercial breakthroughs). Nvidia Rubin is 2026-2027 (models trained on it 2027-2029), so a 2029 model plausibly uses the next architecture after (though it&#39;s more likely to come out in early 2030 then, not 2029). Let&#39;s say it&#39;s 1e16 FLOP/s per chip (BF16, 4x B200) with time cost $4/hour (2x H100), that is $55bn to train for 2e29 FLOPs and 3M chips in the training system if it needs 6 months at 40% utilization (reinforcing the point that 2030 is a more plausible timing, 3M chips is a lot to manufacture). Training systems with H100s cost $50K per chip all-in to build (~BOM not TCO), so assuming it&#39;s 2x more for the after-Rubin chips the training system costs $300B to build. Also, a Blackwell chip needs 2 KW all-in (a per-chip fraction of the whole datacenter), so the after-Rubin chip might need 4 KW, and 3M chips need 12 GW.</p><p>These numbers need to match the scale of the largest AI companies. A training system ($300bn in capital, 3M of the newest chips) needs to be concentrated in the hands of a single company, probably purpose-built. And then at least $55bn of its time ne... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="8294mmciv2DbP2W5b"><p><span><div><div><p><span>6</span><span>Paragox</span><span><time datetime="2025-03-06T04:31:11.704Z">4d</time></span></p><p>For funding timelines, I think the main question increasingly becomes: how much of the economical pie could be eaten by narrowly superhuman AI tooling? It doesn&#39;t take hitting an infinity/singularity/fast takeoff for plausible scenarios under this bearish reality to nevertheless squirm through the economy at Cowen-approved diffusion rates and gradually eat insane $$$ worth of value, and therefore, prop up 100b+ buildouts. OAI&#39;s latest sponsored pysop  leak today seems right in line with bullet point numero uno under real world predictions, that they are going to try and push 100 billion market eaters on us whether we, ahem, high taste commentators like it or not. 

Perhaps I am biased by years of seeing big-numbers-detached-from-reality in FAANG, but I see the centaurized Senior SWE Thane alluded too easily eating up a 100 billion chunk[1] worldwide (at current demand, not even adjusting for the marginal cost of software -&gt; size of software market relation!) Did anyone pay attention to the sharp RLable improvements in the O3-in-disguise Deep Research model card, vs O1? We aren&#39;t getting the singularity, yes, but scaling RL on every verifiable code PR in existence (plus 10^? of synthetic copies) seems increasingly likely to get us the junior/mid level API (I hesitate to call it agent), that will write superhuman commits for the ~90% of PRs that have well-defined and/or explicitly testable objectives. Perhaps then we will finally start seeing some of that productivity 10xing that Thane is presently and correctly skeptical off; only Senior+ need apply of course. 

(Side note: in the vein of documenting predictions, I currently predict that in the big tech market, at-scale Junior hiring is on its waning and perhaps penultimate cycle, with senior and especially staff compensation likewise soon skyrocketing as every ~1 mil/year USD quartet of supporting Juniors is replaced with a 300k/year Claude Pioneer subscription straight into an L6&#39;s hands.)

I think the main danger </p></div></div></span></p><div><div><div id="bzewBFBQocEDpDJdJ"><p><span><div><div><p><span>9</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-06T05:31:22.685Z">4d</time></span></p><p>That&#39;s why I used the &#34;no new commercial breakthroughs&#34; clause, $300bn training systems by 2029 seem in principle possible both technically and financially without an intelligence explosion, just not with the capabilities legibly demonstrated so far. On the other hand, pre-training as we know it will end[1] in any case soon thereafter, because at ~current pace a 2034 training system would need to cost $15 trillion (it&#39;s unclear if manufacturing can be scaled at this pace, and also what to do with that much compute, because there isn&#39;t nearly enough text data, but maybe pre-training on all the video will be important for robotics).

How far RL scales remains unclear, and even at the very first step of scaling o3 doesn&#39;t work as clear evidence because it&#39;s still unknown if it&#39;s based on GPT-4o or GPT-4.5 (it&#39;ll become clearer once there&#39;s an API price and more apples-to-apples speed measurements).

----------------------------------------

 1. This is of course a quote from Sutskever&#39;s talk. It was widely interpreted as saying it has just ended, in 2024-2025, but he never put a date on it. I don&#39;t think it will end before 2027-2028. ↩︎</p></div></div></span></p></div></div></div></div></div><div><div id="npDBxTY59nGLittEB"><p><span><div><div><p><span>5</span><span>Thane Ruthenis</span><span><time datetime="2025-03-05T19:46:38.419Z">4d</time></span></p><p>I did meant effective compute, yeah. Noted, though.

(Always appreciate your analyses, by the way. They&#39;re consistently thorough and informative.)</p></div></div></span></p></div></div></div></div></div><div><div id="4dbv8wngvFytJXaAY"><div><div><div><div><div><div><p>I agree with almost everything you&#39;ve said about LLMs.</p><p>I still think we&#39;re getting human-level AGI soonish. The LLM part doesn&#39;t need to be any better than it is.</p><p>A human genius with no one-shot memory (severe anterograde amnesia) and very poor executive function (ability to stay on task and organize their thinking) would be almost useless - just like LLMs are.</p><p>LLMs replicate only part of humans&#39; general intelligence. It&#39;s the biggest part, but it just wouldn&#39;t work very well without the other contributing brain systems. Human intelligence, and its generality (in particular our ability to solve truly novel problems) is an emergent property of interactions among multiple brain systems (or a complex property if you don&#39;t like that term).</p><p>See <a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures">Capabilities and alignment of LLM cognitive architectures</a></p><p>In brief, LLMs are like a human posterior cortex. A human with only a posterior cortex would be about as little use as an LLM (of course this analogy is imperfect but it&#39;s close). We need a prefrontal cortex (for staying on task, &#34;executive function&#34;), a medial temporal cortex and hippocampus for one-shot learning, and a basal ganglia for making better decisions than just whatever first comes t... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="iAFZECNjRZH6ArE2t"><p><span><div><div><p><span>8</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-06T00:04:43.427Z">4d</time></span></p><p>But AI speed advantage? It&#39;s 100x-1000x faster, so years become days to weeks. Compute for experiments is plausibly a bottleneck that makes it take longer, but at genius human level decades of human theory and software development progress (things not bottlenecked on experiments) will be made by AIs in months. That should help a lot in making years of physical time unlikely to be necessary, to unlock more compute efficient and scalable ways of creating smarter AIs.</p></div></div></span></p><div><div><div id="unAiGpK7cG2mSy2ei"><p><span><div><div><p><span>4</span><span>Seth Herd</span><span><time datetime="2025-03-06T03:48:11.362Z">4d</time></span></p><p>Yes, probably. The progression thus far that the same level of intelligence gets more efficient - faster or cheaper.

I actually think current systems don&#39;t really think much faster than humans - they&#39;re just faster at putting words to thoughts, since their thinking is more closely tied to text. But if they don&#39;t keep getting smarter, they will still likely keep getting faster and cheaper.</p></div></div></span></p></div></div></div></div></div><div><div id="ikmbribkgqgbmNZsL"><p><span><div><div><p><span>7</span><span>orangecelsius32</span><span><time datetime="2025-03-06T01:54:05.482Z">4d</time></span></p><p>This is an interesting model, and I know you acknowledged that progress could take years, but my impression is that this would be even more difficult than you&#39;re implying.  Here are the problems I see, and I apologize in advance if this doesn&#39;t all make sense as I am a non-technical newb.

 * Wouldn’t it take insane amounts of compute to process all of this?  LLM + CoT already uses a lot of compute (see: o3 solving ARC puzzles for $1mil).  Combining this with processing images/screenshots/video/audio, plus using tokens for incorporating saved episodic memories into working memory, plus tokens for the decision-making (basal ganglia) module = a lot of tokens. Can this all fit into a context window and be processed with the amount of compute that will be available?  Even if one extremely expensive system could run this, could you have millions of agents running this system for long periods of time?
 * How do you train this?  LLMs are superhuman at language processing due to training on billions of pieces of text. How do you train an agent similarly? We don’t have billions of examples of a system like this being used to achieve goals.  I don’t think we have any examples.  You could put together a system like this today, but it would be bad (see: Claude playing Pokemon).  How does it improve?  I think it would have to actually carry out tasks and RL on them.  In order for it to improve on long-horizon tasks, it would take long-horizon timeframes to get reinforcement signals.  You could run simulations, but will they come anywhere close to matching the complexity of the real world?  And then there’s the issue of scalable RL only working for tasks with a defined goal: how would it improve on open-ended problems?
 * If an LLM is at the core of the system, do hallucinations from the LLM “poison the well” so to speak?  You can give it tools, but if the LLM at the core doesn’t know what’s true or false, how does it effectively use them?  I’ve seen examples like: an LLM got a m</p></div></div></span></p><div><div><div id="dxYBjCuttNxxqFgtF"><p><span><div><div><p><span>4</span><span>Seth Herd</span><span><time datetime="2025-03-07T18:49:48.162Z">3d</time></span></p><p>I don&#39;t think this path is easy; I think immense effort and money will be directed at it by default, since there&#39;s so much money to be made by replacing human labor with agents. And I think no breakthroughs are necessary, just work in fairly obvious directions. That&#39;s why I think this is likely to lead to human-level agents.

 1. I don&#39;t think it would take insane amounts of compute, but compute costs will be substantial. They&#39;ll be roughly like costs for OpenAIs Operator; it runs autonomously, making calls to frontier LLMs and vision models essentially continuously. Costs are low enough that $200/month covers unlimited use. (although that thing is so useless people probably aren&#39;t using it much. So the compute costs of o1 pro thinking away continuously are probably a better indicator; Altman said $200/mo doesn&#39;t quite cover the average, driven by some users keeping as many going constantly as they can. 
    
    It can&#39;t all be fit into a context window for complex tasks. And it&#39;s costly even when the whole task would fit. That&#39;s why additional memory systems are needed. There are already context window management techniques in play for existing limited agents. And RAG systems seem to already be adequate to serve as episodic memory; humans use much fewer memory &#34;tokens&#34; to accomplish complex tasks than the large amount of documentation stored in current RAG systems used for non-agentic retrieval assisted generation of answers to questions that rely on documented information.
    
    So I&#39;d estimate something like $20-30 for an agent to run all day. This could come down a lot if you managed to have many of its calls use smaller/cheaper LLMs than whatever is the current latest and greatest.
 2. Humans train themselves to act agentically by assembling small skills (pick up the food and put it in your mouth, run forward, look for tracks) into long time horizon tasks (hunting). We do not learn by performing RL on long sequences and applying the learning to everything w</p></div></div></span></p></div></div></div></div></div><div><div id="BouQCPr6sE2kyYvkH"><p><span><div><div><p><span>2</span><span>p.b.</span><span><time datetime="2025-03-06T20:15:08.002Z">3d</time></span></p><p>I kinda agree with this as well. Except that it seems completely unclear to me whether recreating the missing human capabilities/brain systems takes two years or two decades or even longer.

It doesn&#39;t seem to me to be a single missing thing and for each separate step holds: That it hasn&#39;t been done yet is evidence that it&#39;s not that easy. </p></div></div></span></p></div></div></div></div></div><div><div id="WxFwqCM32cjfFe9qc"><div><div><div><div><div><p>Writing down these predictions ahead of time is already very virtuous, but I think it&#39;d be better with probability estimates for the claims.</p></div></div></div></div></div></div></div><div><div id="mqPPKLWodzCFh7HqZ"><div><div><div><div><div><div><p>This fits my bear-picture fairly well. </p><p>Here&#39;s some details of my bull-picture:</p><ul><li>GPT4.5 is still a small fraction of the human brain, when we try to compare sizes. It makes some sense to think of it as a long-lived parrot that&#39;s heard the whole internet and then been meticulously reinforced to act like a helpful assistant. From this perspective, it makes a lot of sense that its ability to generalize datapoints is worse than human, and plausible (at least naively) that one to four additional orders of magnitude will close the gap. </li><li>Even if the pretraining paradigm can&#39;t close the gap like that due to fundamental limitations in the architecture, CoT is approximately Turing-complete. This means that the RL training of reasoning models is doing program search, but with a pretty decent prior (ie representing a lot of patterns in human reasoning). Therefore, scaling reasoning models can achieve all the sorts of generalization which scaling pretraining is failing at, in principle; the key question is just how much it needs to scale in order for that to happen. </li><li>While I agree that RL on reasoning models is in some sense limited to tasks we can provide good  feedback on, it seems like </li></ul><p>... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="ddnzpqa9AXLSkdcD4"><p><span><div><div><p><span>5</span><span>RyanCarey</span><span><time datetime="2025-03-10T01:07:10.106Z">6h</time></span></p><p>Is GPT4.5&#39;s ?10T parameters really a &#34;small fraction&#34; of the human brain&#39;s 80B neurons and 100T synapses?</p></div></div></span></p><div><div><div id="Bdpw8Yf2YDiKKrHaR"><p><span><div><div><p><span>2</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-10T07:11:32.966Z">3m</time></span></p><p>Human brain holds 200-300 trillion synapses. A 1:32 sparse MoE at high compute will need about 350 tokens/parameter to be compute optimal. This gives 8T active parameters (at 250T total), 2,700T training tokens, and 2e29 FLOPs (raw compute GPT-6 that needs a $300bn training system) and won&#39;t have natural text data to train it with.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="zZSwbYExC4izdwRng"><div><div><div><div><div><div><blockquote><ul><li>It will not meaningfully generalize beyond domains with easy verification. Some trickery like RLAIF and longer CoTs might provide <i>some</i> benefits, but they would be a fixed-size improvement. It will not cause a hard-takeoff self-improvement loop in &#34;soft&#34; domains.</li><li>RL will be good enough to turn LLMs into reliable tools for some fixed environments/tasks. They will reliably fall flat on their faces if moved outside those environments/tasks.</li></ul></blockquote><p>I&#39;m particularly interested in whether such systems will be able to basically &#39;solve software engineering&#39; in the next few years. I&#39;m not sure if you agree or disagree. I think the answer is probably yes.</p></div></div></div></div></div></div><div><div><div id="6BgpiTQ2QE3FuGtQF"><p><span><div><div><p><span>7</span><span>Thane Ruthenis</span><span><time datetime="2025-03-05T19:51:48.620Z">4d</time></span></p><p>I&#39;m skeptical on that. I think they will become ever more useful as tools for it, and would be able to automate some specific aspects of it (say, basic plug-and-play apps and raw optimization?), but the full-scope automation will run into the &#34;staying on-track across large inferential distances&#34; problem.</p></div></div></span></p><div><div><div id="E5XjYhfxjsNgtW3Ff"><div><div><div><div><div><p>Great. So yeah, it seems we are zeroing in on a double crux between us. We both think general-purpose long-horizon agency (my term) / staying-on-track-across-large-inferential-distances (your term, maybe not equivalent to mine but at least heavily correlated with mine?) is the key dimension AIs need to progress along. </p></div></div></div></div></div><div><div><div id="9LT2RgZiqgTQHudpm"><div><div><div><div><div><div><p>FWIW I’m also bearish on LLMs but for reasons that are maybe subtly different from OP. I tend to frame the issue in terms of <i>“inability to deal with a lot of interconnected layered complexity in the context window”</i>, which comes up when there’s a lot of idiosyncratic interconnected ideas in one’s situation or knowledge that does not exist on the internet.</p><p>This issue <i>incidentally</i> comes up in “long-horizon agency”, because if e.g. you want to build some new system or company or whatever, you usually wind up with a ton of interconnected idiosyncratic “cached” ideas about what you’re doing and how, and who’s who, and what’s what, and what are the idiosyncratic constraints and properties and dependencies in my specific software architecture, etc. The more such interconnected bits of knowledge that I need for what I’m doing—knowledge which is by definition not on the internet, and thus must be in the context window instead—the more I expect foundation models to struggle on those tasks, now and forever.</p><p>But that problem is not exactly the same as a problem with long-horizon agency <i>per se</i>. I would not be too surprised or updated by seeing “long-horizon agency” in situations where, every step ... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="uNTJhcHby7W4J668n"><p><span><div><div><p><span>4</span><span>p.b.</span><span><time datetime="2025-03-06T19:31:28.114Z">3d</time></span></p><p>I think that is exactly right. 

I also wouldn&#39;t be too surprised if in some domains RL leads to useful agents if all the individual actions are known to and doable by the model and RL teaches it how to sensibly string these actions together. This doesn&#39;t seem too different from mathematical derivations. </p></div></div></span></p></div></div><div><div id="HifGv7jN94hHKkRqN"><p><span><div><div><p><span>4</span><span>Daniel Kokotajlo</span><span><time datetime="2025-03-06T16:41:23.780Z">4d</time></span></p><p>I think that getting good at the tag-teamable tasks is already enough to start to significantly accelerate AI R&amp;D? Idk. I don&#39;t really buy your distinction/abstraction yet enough to make it an important part of my model. </p></div></div></span></p><div><div><div id="EnsxRMdyBYhiRotjm"><p><span><div><div><p><span>2</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T16:45:36.993Z">4d</time></span></p><p>I think it won&#39;t work (and isn&#39;t working today) for the same reasons John outlines here with regards to HCH/&#34;the infinite bureaucracy&#34;. (tl;dr: this requires competent problem factorization, but problem factorization is nontrivial and can&#39;t be relegated to an afterthought.)</p></div></div></span></p><div><div><div id="XeGcGFdapBhrJr3fE"><p><span><div><div><p><span>6</span><span>Noosphere89</span><span><time datetime="2025-03-06T16:59:37.466Z">4d</time></span></p><p>Thinking about this, I think a generalized crux with John Wentworth et al is probably on how differently we see bureaucracies, and he sees them as terrible, whereas I see them as both quite flawed and has real problems, but are also wonderful tools to have that keeps the modern civilization&#39;s growth engine stable, and the thing that keeps the light on, so I see bureaucracies as way more important for civilization&#39;s success than John Wentworth believes.

One reason for this is a lot of the success cases of bureaucracies look like no news can be made, so success isn&#39;t obvious, whereas bureaucratic failure is obvious.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="qe3tCMfciPfNCyP9K"><p><span><div><div><p><span>2</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T16:30:42.454Z">4d</time></span></p><p>I think that&#39;s also equivalent to my &#34;remaining on-target across long inferential distances&#34; / &#34;maintaining a clear picture of the task even after its representation becomes very complex in terms of the templates you had memorized at the start&#34;.

That&#39;s a fair point, but how many real-life long-horizon-agency problems are of the &#34;clean&#34; type you&#39;re describing?

An additional caveat here is that, even if the task is fundamentally &#34;clean&#34;/tag-team-able, you don&#39;t necessarily know that when working on it. Progressing along it would require knowing what information to discard and what to keep around at each step, and that&#39;s itself nontrivial and might require knowing how to deal with layered complexity.

(Somewhat relatedly, see those thoughts regarding emergent complexity. Even if a given long-horizon-agency task is clean thin line when considered from a fully informed omniscient perspective – a perspective whose ontology is picked to make the task&#39;s description short – that doesn&#39;t mean the bounded system executing the task can maintain a clean representation of it every step of the way.)</p></div></div></span></p></div></div></div></div></div><div><div id="AugTi6J5MivXTHjed"><div><div><div><div><div><div><blockquote><p> We both think general-purpose long-horizon agency (my term) / staying-on-track-across-large-inferential-distances (your term, maybe not equivalent to mine but at least heavily correlated with mine?)</p></blockquote><p>They&#39;re equivalent, I think. &#34;Staying on track across inferential distances&#34; is a phrasing I use to convey a more gears-level mental picture of what I think is going on, but I&#39;d term the external behavior associated with it &#34;general-purpose long-horizon agency&#34; as well.</p><blockquote><p>Correct?</p></blockquote><p>Basically, yes. I do expect some transfer learning from RL, but I expect it&#39;d still only lead to a &#34;fixed-horizon&#34; agency, and may end up more brittle than people hope.</p><p>To draw an analogy: Intuitively, I would&#39;ve expected reasoning models to grok some simple compact &#34;reasoning algorithm&#34; that would&#39;ve let them productively reason for arbitrarily long. Instead, they seem to end up with a fixed &#34;reasoning horizon&#34;, and scaling o1 -&gt; o3 -&gt; ... is required to extend it.</p><p>I expect the same of &#34;agent&#34; models. With more training, they&#39;d be able to operate on ever-so-slightly longer horizons. But extending the horizon would require steeply (exponentially?) growing amounts of compute, and the models would never quite grok the &#34;compact generator&#34; of arbitrary-horizon arbitrary-domain agency.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="NWHFN6tySg8HG5xzE"><p><span><div><div><p><span>1</span><span>yo-cuddles</span><span><time datetime="2025-03-05T21:44:32.758Z">4d</time></span></p><p>By &#34;solve&#34;, what do you mean? Like, provably secure systems, create a AAA game from scratch, etc?

I feel like any system that could do that would implicitly have what the OP says these systems might lack, but you seem to be in half agreeance with them. Am I misunderstanding something?</p></div></div></span></p><div><div><div id="ooQJFEceWHnvds5jq"><p><span><div><div><p><span>7</span><span>Daniel Kokotajlo</span><span><time datetime="2025-03-05T22:24:25.843Z">4d</time></span></p><p>By &#34;Solve&#34; I mean &#34;Can substitute for a really good software engineer and/or ML research engineer&#34; in frontier AI company R&amp;D processes. So e.g. instead of having teams of engineers led by a scientist, they can (if they choose) have teams of AIs led by a scientist.</p></div></div></span></p><div><div><div id="arphRTW9dEcvJN6Ga"><p><span><div><div><p><span>5</span><span>yo-cuddles</span><span><time datetime="2025-03-06T01:05:05.898Z">4d</time></span></p><p>Ah, okay.

I&#39;ll throw in my moderately strong disagreement for future bayes points, respect for the short term, unambiguous prediction!</p></div></div></span></p><div><div><div id="X45jAT99K59nphKcv"><p><span><div><div><p><span>6</span><span>Daniel Kokotajlo</span><span><time datetime="2025-03-06T01:14:10.089Z">4d</time></span></p><p>TBC, I&#39;m at &#34;Probably&#34; not &#34;Definitely.&#34; My 50% mark is in 2028 now, so I have a decent amount of probability mass (maybe 30%?) stretching across the 2030&#39;s.</p></div></div></span></p><div><div><div id="sSi7NKEMCkrk3WCFp"><p><span><div><div><p><span>5</span><span>yo-cuddles</span><span><time datetime="2025-03-06T03:54:22.986Z">4d</time></span></p><p>Gotcha, you didn&#39;t sound OVER confident so I assumed it was much-less-than-certain, still refreshingly concrete</p></div></div></span></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="JtbFnKKa2EBEE9Szz"><div><div><div><div><div><div><blockquote><p>But the latter doesn&#39;t really require LLMs to be capable of end-to-end autonomous task execution, which is the property required for actual transformative consequences.</p></blockquote><p>I&#39;m glad we agree on which property is required (and I&#39;d say basically sufficient at this point) for actual transformative consequences.</p></div></div></div></div></div></div><div><div><div id="v9EmCgWcfB6TX4wb2"><p><span><div><div><p><span>4</span><span>Cole Wyeth</span><span><time datetime="2025-03-06T04:11:37.210Z">4d</time></span></p><p>How do you know it&#39;s sufficient? Is it not salient to you primarily because it is the current bottleneck?

If &#34;task execution&#34; includes execution of a wide enough class of tasks, obviously the claim becomes trivially true. If it is interpreted more reasonably, I think it is probably false.</p></div></div></span></p></div></div></div></div></div><div><div id="2p6S9oksr723rd9dz"><div><div><div><div><div><div><p>Thanks!</p><blockquote><p>At some unknown point – probably in 2030s</p></blockquote><p>why do you think it&#39;s probably 2030s?</p></div></div></div></div></div></div><div><div><div id="Qq5acghzetQFJfj8g"><p><span><div><div><p><span>8</span><span>Thane Ruthenis</span><span><time datetime="2025-03-05T17:15:35.733Z">5d</time></span></p><p>Rough estimate based on how many new ideas seem to be needed and their estimated &#34;size&#34;. I definitely don&#39;t see it taking, say, 50 years (without an international ban or some sort of global catastrophe).</p></div></div></span></p><div><div><div id="jH8AjcAwcbu58PHYC"><p><span><div><div><p><span>6</span><span>Cole Wyeth</span><span><time datetime="2025-03-06T04:12:26.530Z">4d</time></span></p><p>There are a lot of years between 2030 and 2075.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="bsgG9GatfmtJ8Ksjp"><div><div><div><div><div><div><p>Curated. Some more detailed predictions of the future, different from others, and one of the best bear cases I&#39;ve read.</p><p>This feels a bit less timeless than many posts we curate but my guess is that (a) it&#39;ll be quite interesting to re-read this in 2 years, and (b) it makes sense to record good and detailed predictions like this more regularly in the field of AI which is moving so much faster than most of the rest of the world.</p></div></div></div></div></div></div><div><div><div id="NzECBx7K3oDLZzpmQ"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T20:51:53.224Z">2d</time></span></p><p>It&#39;ll be quite interesting to be alive to re-read this in 2 years, yes.</p></div></div></span></p><div><div><div id="ZyFKsmoXHQYYsBN44"><p><span><div><div><p><span>3</span><span>Ben Pace</span><span><time datetime="2025-03-07T21:07:24.308Z">2d</time></span></p><p>I&#39;m never sure if it makes sense to add that clause every time I talk about the future.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="tjHtxw7kXz9GsbST3"><div><div><div><div><div><div><p>Why do you think Anthropic and OpenAI are making such bold predictions? (https://x.com/kimmonismus/status/1897628497427701961)</p><p>As I see it, one of the following is true:</p><ol><li>They agree with you but want shape the narrative away from the truth to sway investors</li><li>They have mostly the same info as you but come to a different conclusion</li><li>They have evidence we don&#39;t have which gives them confidence</li></ol></div></div></div></div></div></div><div><div><div id="ptcuhbwiXaEB7yazw"><p><span><div><div><p><span>7</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T16:57:13.363Z">3d</time></span></p><p>1. I think &#34;this is a political move to drive up hype&#34; is definitely a factor. The fact that they&#39;re concretely anchoring to 2026-2027 does downweigh this explanation, however: that&#39;s not a very good political move, they should be keeping it more vague.[1] So...

2. ... I think they do, themselves, mostly believe it. Which is to say, they&#39;re buying their own hype and propaganda. That is a standard dynamic, both regarding hype (if you&#39;re working on X, surrounded by people working on it and optimistic about it, of course the optimisms end up reinforcing each other) and propaganda (your people aren&#39;t immune to the propaganda you&#39;re emitting, and indeed, believing your own propaganda makes it much more authentic and convincing).

3. I&#39;m very much not on the &#34;they have secret internal techniques light-years ahead of the public SotA and too dangerous for the public eye&#34;/&#34;what did Ilya see?!&#34; train. I think what they have are promising research directions, hopeful initial results, the vision to see that research through, and the talent they believe to be sufficient for it. This is what fuels their optimism/self-hype. Which is fine, I&#39;m hyped for my own research too. But, of note:

 * Anthropic&#39;s reasoning models were hyped up as scary, but what we got is a decidingly mediocre (as far as reasoning goes) Sonnet 3.7. SotA-good at programming? Yes. Scary? No. Well, perhaps they have even scarier models that they&#39;re still not releasing? I mean, sure, maybe. But that&#39;s a fairly extraordinary claim, and all we have for it is vague hype and scary rumors.
 * Satya Nadella was an insider, and he recently bailed on OpenAI and implied he&#39;s skeptical of near-term economically transformative LLM effects. Sure, maybe he specifically is a pathological AGI disbeliever[2]. But it does put a sharp limit on how convincing their internal evidence can be.
 * I don&#39;t buy it in general. AGI labs are competing for funding and attention, it&#39;s a rat race, I don&#39;t think they have the slack to sandbag</p></div></div></span></p><div><div><div id="6py5HManJ4AGELCXJ"><p><span><div><div><p><span>5</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-07T18:05:36.979Z">3d</time></span></p><p>Base model for Sonnet-3.7 was pretrained in very early 2024, and there was a recent announcement that a bigger model is coming soon, which is, obviously. So the best reasoning model they have internally is better than Sonnet 3.7, even though we don&#39;t know if it&#39;s significantly better. They might&#39;ve had it since late 2024 even, but without Blackwell they can&#39;t deploy, and also they are Anthropic, so plausibly capable of not deploying out of an abundance of caution.

The rumors about quality of Anthropic&#39;s reasoning models didn&#39;t specify which model they are talking about. So observation of Sonnet 3.7&#39;s reasoning is not counter-evidence to the claim that verifiable task RL results scale well with pretraining, and only slight evidence that it doesn&#39;t scale well with pure RL given an unchanged base model.</p></div></div></span></p></div></div></div></div></div><div><div id="JZKqMLhKhhBGAGmMF"><p><span><div><div><p><span>6</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-07T17:46:23.052Z">3d</time></span></p><p>The evidence they might have (as opposed to evidence-agnostic motivation to feed the narrative) is scaling laws for verifiable task RL training, for which there are still no clues in public (that is, what happens if you try to use a whole lot more compute there). It might be improving dramatically either with additional training, or depending on the quality of the pretrained model it&#39;s based on, even with a feasible number of verifiable tasks.

OpenAI inevitably has a reasoning model based on GPT-4.5 at this point (whether it&#39;s o3 or not), and Anthropic very likely has the same based on some bigger base model than Sonnet-3.5. Grok 3 and Gemini 2.0 Pro are probably overtrained in order to be cheap to serve, feel weaker than GPT-4.5, and we&#39;ve only seen a reasoning model for Grok 3. I think they mostly don&#39;t deploy the 3e26 FLOPs reasoning models because Blackwell is still getting ready, so they are too slow and expensive to serve, though it doesn&#39;t explain Google&#39;s behavior.</p></div></div></span></p><div><div><div id="f2yLxXqCpSRu5KDoy"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T17:55:51.796Z">3d</time></span></p><p>Why is that the case? While checking how it scales past o3/GPT-4-level isn&#39;t possible, I&#39;d expect people to have run experiments at lower ends of the scale (using lower-parameter models or less RL training), fitted a line to the results, and then assumed it extrapolates. Is there reason to think that wouldn&#39;t work?</p></div></div></span></p><div><div><div id="6DSMP8uJYy3yYn4qK"><p><span><div><div><p><span>5</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-07T18:15:58.785Z">3d</time></span></p><p>It needs verifiable tasks that might have to be crafted manually. It&#39;s unknown what happens if you train too much with only a feasible amount of tasks, even if they are progressively more and more difficult. When data can&#39;t be generated well, RL needs early stopping, has a bound on how far it goes, and in this case this bound could depend on the scale of the base model, or on the number/quality of verifiable tasks.

Depending on how it works, it might be impossible to use $100M for RL training at all, or scaling of pretraining might have a disproportionally large effect on quality of the optimally trained reasoning model based on it, or approximately no effect at all. Quantitative understanding of this is crucial for forecasting the consequences of the 2025-2028 compute scaleup. AI companies likely have some of this understanding, but it&#39;s not public.</p></div></div></span></p></div></div></div></div></div></div></div></div></div></div></div><div><div id="BZBpoEYiwoujSyBMG"><div><div><div><div><div><div><blockquote><p>&#34;But the models <i>feel</i> increasingly smarter!&#34;:</p><ul><li>It seems to me that &#34;vibe checks&#34; for how smart a model feels are easily gameable by making it have a better personality.</li><li>My guess is that it&#39;s most of the reason Sonnet 3.5.1 was so beloved. Its personality was made much more <i>appealing</i>, compared to e. g. OpenAI&#39;s corporate drones.</li><li><a href="https://www.lesswrong.com/posts/bozSPnkCzXBjDpbHj/ai-104-american-state-capacity-on-the-brink#Huh__Upgrades">The recent upgrade to GPT-4o</a> seems to confirm this. They seem to have merely given it a better personality, and people were reporting that it &#34;feels much smarter&#34;.</li><li>Deep Research was this for me, at first. Some of its summaries were just <i>p</i></li></ul></blockquote><p>... <span>(read more)</span></p></div></div></div></div></div></div></div></div><div><div id="tMewovD6cdztwuAnG"><div><div><div><div><div><div><blockquote>
<p>Not to be a scaling-law denier. I believe in them, I do! But they measure perplexity, not general intelligence/real-world usefulness, and Goodhart&#39;s Law is no-one&#39;s ally.</p>
</blockquote>
<p>If we&#39;re able to get perplexity sufficiently low on text samples that I write, then that means the LLM has a lot of the important algorithms running in it that are running in me. The text I write is causally downstream from parts of me that are reflective and self-improving, that notice the little details in my cognitive processes and environment, and the parts of me that are capable of... <span>(read more)</span></p></div></div></div></div></div></div><div><div><div id="bDfrBeizywYJ9v64h"><p><span><div><div><p><span>9</span><span>Thane Ruthenis</span><span><time datetime="2025-03-08T03:57:26.778Z">2d</time></span></p><p>Sure, but &#34;sufficiently low&#34; is doing a lot of work here. In practice, a &#34;cheaper&#34; way to decrease perplexity is to go for the breadth (memorizing random facts), not the depth. In the limit of perfect prediction, yes, GPT-N would have to have learned agency. But the actual LLM training loops may be a ruinously compute-inefficient way to approach that limit – and indeed, they seem to be.

My current impression is that the SGD just doesn&#39;t &#34;want&#34; to teach LLMs agency for some reason, and we&#39;re going to run out of compute/data long before it&#39;s forced to. It&#39;s possible that I&#39;m wrong and base GPT-5/6 paperclips us, sure. But I think if that were going to happen, it would&#39;ve happened at GPT-4 (indeed, IIRC that was what I&#39;d dreaded from it).</p></div></div></span></p><div><div><div id="JKX62fFwxtB9TSBWv"><p><span><div><div><p><span>8</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-08T05:48:03.763Z">2d</time></span></p><p>The language monkeys paper is the reason I&#39;m extremely suspicious of any observed failures to elicit a capability in a model serving as evidence of its absence. What is it that you know, that leads you to think that &#34;SGD just doesn&#39;t &#34;want&#34; to teach LLMs agency&#34;? Chatbot training elicits some things, verifiable task RL training elicits some other things (which weren&#39;t obviously there, weren&#39;t trivial to find, but findings of the s1 paper suggest that they are mostly elicited, not learned, since mere 1000 traces are sufficient to transfer the capabilities). Many more things are buried just beneath the surface, waiting for the right reward signal to cheaply bring them up, putting them in control of the model&#39;s behavior.</p></div></div></span></p><div><div><div id="BWmcABJrGnDGtWt8n"><p><span><div><div><p><span>7</span><span>Thane Ruthenis</span><span><time datetime="2025-03-08T11:37:21.227Z">2d</time></span></p><p>Mostly the fact that it hasn&#39;t happened already, on a merely &#34;base&#34; model. The fact that CoTs can improve models&#39; problem-solving ability has been known basically since the beginning, but there&#39;s been no similar hacks found for jury-rigging agenty or insightful characters. (Right? I may have missed it, but even janus&#39; community doesn&#39;t seem to have anything like it.)

But yes, the possibility that none of the current training loops happened to elicit it, and the next dumb trick will Just Work, is very much salient. That&#39;s where my other 20% are at.</p></div></div></span></p><div><div><div id="JTpaoTeJyvJngLEqc"><p><span><div><div><p><span>7</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-08T12:29:17.444Z">2d</time></span></p><p>I&#39;d say long reasoning wasn&#39;t really elicited by CoT prompting, and that you can elicit agency to about the same extent now (i.e. hopelessly unconvincingly). It was only elicited with verifiable task RL training, and only now are there novel artifacts like s1&#39;s 1K traces dataset that do elicit it convincingly, that weren&#39;t available as evidence before.

It&#39;s possible that as you say agency is unusually poorly learned in the base models, but I think failure to elicit is not the way to learn about whether it&#39;s the case. Some futuristic interpretability work might show this, the same kind of work that can declare a GPT-4.5 scale model safe to release in open weights (unable to help with bioweapons or take over the world and such). We&#39;ll probably get an open weights Llama-4 anyway, and some time later there will be novel 1K trace datasets that unlock things that were apparently impossible for it to do at the time of release.

I was to a significant extent responding to your &#34;It&#39;s possible that I&#39;m wrong and base GPT-5/6 paperclips us&#34;, which is not what my hypothesis predicts. If you can&#39;t elicit a capability, it won&#39;t be able to take control of model&#39;s behavior, so a base model won&#39;t be doing anything even if you are wrong in the way I&#39;m framing this and the capability is there, finetuning on 1K traces away from taking control. It does still really need those 1K traces or else it never emerges at any reasonable scale, that is you might need a GPT-8 for it to spontaneously emerge in a base model, demonstrating that it was in GPT-5.5 all along, and making it possible to create the 1K traces that elicit it from GPT-5.5. While at the same time a clever method like R1-Zero would&#39;ve been able to elicit it from GPT-5.5 directly, without needing a GPT-8.</p></div></div></span></p><div><div><div id="qLjXrNfx8L7g7yYtR"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-08T18:52:49.191Z">2d</time></span></p><p>IIRC, &#34;let&#39;s think step-by-step&#34; showed up in benchmark performance basically immediately, and that&#39;s the core of it. On the other hand, there&#39;s nothing like &#34;be madly obsessed with your goal&#34; that&#39;s known to boost LLM performance in agent settings.

There were clear &#34;signs of life&#34; on extended inference-time reasoning; there are (to my knowledge) none on agent-like reasoning.

If you agree that it can spontaneously emerge at a sufficiently big scale, why would you assume this scale is GPT-8, not GPT-5?

That&#39;s basically the core of my argument. If LLMs learned agency skills, they would&#39;ve been elicitable in some GPT-N, with no particular reason to think that this N needs to be very big. On the contrary, by extrapolating a similarly qualitative jump from GPT-3 to GPT-4 as happened from GPT-2 to GPT-3, I&#39;d expected these skills to spontaneously show up in GPT-4 – if they were ever going to show up.

They didn&#39;t show up. GPT-4 ended up as a sharper-looking GPT-3.5, and all progress since then amounted to GPT-3.5&#39;s shape being more sharply defined, without that shape changing.</p></div></div></span></p><div><div><div id="L2hQxosYSHLWqCkYE"><p><span><div><div><p><span>2</span><span>Vladimir_Nesov</span><span><time datetime="2025-03-10T06:31:10.371Z">44m</time></span></p><p>It&#39;s not central to the phenomenon I&#39;m using as an example of a nontrivially elicited capability. There, the central thing is efficient CDCL-like in-context search that enumerates possibilities while generalizing blind alleys to explore similar blind alleys less within the same reasoning traces, which can about as long as the whole effective context (on the order of 100K tokens). Prompted (as opposed to elicited-by-tuning) CoT won&#39;t scale to arbitrarily long reasoning traces by adding &#34;Wait&#34; at the end of a reasoning traces either (Figure 3 of the s1 paper). Quantitatively, this manifests as scaling of benchmark outcomes with test-time compute that&#39;s dramatically more efficient per token (Figure 4b of s1 paper) than the parallel scaling methods such as consensus/majority and best-of-N, or even PRM-based methods (Figure 3 of this Aug 2024 paper).

I was just anchoring to your example that I was replying to where you sketch some stand-in capability (&#34;paperclipping&#34;) that doesn&#39;t spontaneously emerge in &#34;GPT-5/6&#34; (i.e. with mere prompting). I took that framing as it was given in your example and extended it to more scale (&#34;GPT-8&#34;) to sketch my own point, that I expect capabilities that can be elicited to emerge much later than the scale where they can be merely elicited (with finetuning on a tiny amount of data). It wasn&#39;t my intent to meaningfully gesture at particular scales with respect to particular capabilities.</p></div></div></span></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="cbFuhmzyBDx5PKtgk"><p><span><div><div><p><span>3</span><span>Alice Blair</span><span><time datetime="2025-03-08T17:28:52.266Z">2d</time></span></p><p>Agency and reflectivity are phenomena that are really broadly applicable, and I think it&#39;s unlikely that memorizing a few facts is the way that that&#39;ll happen. Those traits are more concentrated in places like LessWrong, but they&#39;re almost everywhere. I think to go from &#34;fits the vibe of internet text and absorbs some of the reasoning&#34; to &#34;actually creates convincing internet text,&#34; you need more agency and reflectivity.

My impression is that &#34;memorize more random facts and overfit&#34; is less efficient for reducing perplexity than &#34;learn something that generalizes,&#34; for these sorts of generating algorithms that are everywhere. There&#39;s a reason we see &#34;approximate addition&#34; instead of &#34;memorize every addition problem&#34; or &#34;learn webdev&#34; instead of &#34;memorize every website.&#34;

The RE-bench numbers for task time horizon just keep going up, and I expect them to continue as models continue to gain bits and pieces of the complex machinery required for operating coherently over long time horizons.

As for when we run out of data, I encourage you to look at this piece from Epoch. We run out of RL signal for R&amp;D tasks even later than that.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="jMkZZfixMGvfkAty7"><div><div><div><div><div><div><blockquote><p>It will not meaningfully generalize beyond domains with easy verification.</p></blockquote><p>I think most of software engineering and mathematics problems (two key components of AI development) are easy to verify. I agree with some of your point of how long term agency doesn&#39;t seem to be improving, but I expect that we can build very competent software engineers with the current paradigms.</p></div></div></div></div></div></div><div><div><div id="SmnjSvRS9nL9bgQSc"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T16:16:19.525Z">3d</time></span></p><p>I disagree.

 * In math, what&#39;s easy to verify is whether A is tautologous to B. That is: whether two externally-provided fully-formally-defined statements are equivalent.
   * &#34;This conjecture is useful&#34; or &#34;this subproblem is worth focusing on&#34; or &#34;this framework sheds light on the fundamental structure of reality&#34; are not easy-to-verify. So without transfer learning, the LLMs won&#39;t generate useful conjectures/paradigms on their own.
   * Nor be able to figure out the nearest-useful conjecture, or solve open-ended problems like &#34;figure out the best computationally tractable algorithm approximating algorithm X&#34;.
   * They&#39;d be good for faster conjecture evaluation once you already have the conjecture fully formalized, but that&#39;s about it. This would still be big if they can tackle e. g. the Millennium Problems or problems that take mathematical fields years... But I&#39;m skeptical it&#39;d scale this well.
 * In programming, the situation is similar: the problem is well-specified if it can be translated into a fully formal math problem (see the Curry-Howard correspondence). &#34;Generate code that passes these unit tests&#34;, &#34;translate this algorithm from this language to that one&#34;, &#34;implement the formal algorithm from this paper&#34;, etc.
   * &#34;Generate the program that Does What I Mean&#34;, with the spec provided using natural language, isn&#39;t an easily verifiable problem. You need to model the user, and also have a solid grasp of the real world in which the program will be embedded, in order to predict all possible edge cases and failure modes that&#39;d make what happens diverge from the user&#39;s model. The program needs to behave as intended, not as formally specified.
   * Moreover, &#34;passes unit tests&#34; is a pretty gameable metric; see this paper. If the program that passes them isn&#39;t easy to find, LLMs start eagerly Goodharting to these tests...
   * ... If not outright rewriting them with &#34;assert!(true)&#34;, as people have been complaining about Claude Code doing.

And this is especiall</p></div></div></span></p><div><div><div id="k9AoidFDR7EnxLeT2"><p><span><div><div><p><span>7</span><span>Daniel Kokotajlo</span><span><time datetime="2025-03-07T16:30:30.769Z">3d</time></span></p><p>My opinion is that long-horizon training will indirectly teach a bunch of hard-to-measure skills. Like, if you have a list of simple lemmas and you train your AI to prove them, it probably won&#39;t develop a good sense of what kinds of lemmas are interesting or useful (except insofar as your list was already curated in that way + it correctly notices patterns). But if you train your AI to prove crazy-difficult theorems that take many many months of effort and exploration to achieve, then in the course of learning to do that effort and exploration well, it&#39;ll have to learn a sense of what kinds of lemmas are useful for what other kinds and so forth.</p></div></div></span></p><div><div><div id="zkmAsDqG5inDvXwQn"><p><span><div><div><p><span>5</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T16:38:42.184Z">3d</time></span></p><p>Sure, that&#39;s a plausible hypothesis... But I think there&#39;s a catch-22 here.

RL-on-CoTs is only computationally tractable if the correct trajectories are already close to the &#34;modal&#34; trajectory. Otherwise, you get the same exponential explosion the DeepSeek team got with trying out MCTS.

So how do you train your models to solve the crazy-difficult theorems to begin with, if they start out so bad at subproblem-picking that they assign near-zero probability to all the successful reasoning trajectories? My intuition is that you just run into the exponential explosion again, needing to sample a million million-token trajectories to get one good answer, and so the training just doesn&#39;t get off the ground.</p></div></div></span></p><div><div><div id="LTx4Lz8vW3zvNzmyj"><div><div><div><div><div><div><blockquote>
<p>RL-on-CoTs is only computationally tractable if the correct trajectories are already close to the &#34;modal&#34; trajectory.</p>
</blockquote>
<p>Conclusions that should be impossible to see for a model at a given level of capability are still not far from the surface, as <a href="https://arxiv.org/abs/2407.21787">language monkeys paper</a> shows (<a href="https://arxiv.org/pdf/2407.21787v3#page=6">Figure 3</a>, see how well even Pythia-70M with an &#39;M&#39; starts doing on MATH at pass@10K). So a collection of progressively more difficult verifiable questions can probably stretch whatever wisdom a model implicitly holds from pretraining implausibly far.</p>
</div></div></div></div></div></div></div></div><div><div id="xBCrjCREzt5jTfsZs"><p><span><div><div><p><span>5</span><span>Daniel Kokotajlo</span><span><time datetime="2025-03-07T17:38:10.908Z">3d</time></span></p><p>My guess is that they&#39;ll incrementally expand horizon lengths over the next few years. Like, every six months, they&#39;ll double the size of their dataset of problems and double the average &#39;length&#39; of the problems in the dataset. So the models will have a curriculum of sorts, that starts with the easier shorter-horizon things and then scales up. </p></div></div></span></p><div><div><div id="mAsMrdZd6b4Tbqwns"><p><span><div><div><p><span>4</span><span>Thane Ruthenis</span><span><time datetime="2025-03-08T04:33:32.638Z">2d</time></span></p><p>Prediction: This will somehow not work. Probably they&#39;d just be unable to handle it past a given level of &#34;inferential horizon&#34;.

Reasoning: If this were to work, this would necessarily involve the SGD somehow solving the &#34;inability to deal with a lot of interconnected layered complexity in the context window&#34; problem. On my current model, this problem is fundamental to how LLMs work, due to their internal representation of any given problem being the overlap of a bunch of learned templates (crystallized intelligence), rather than a &#34;compacted&#34; first-principles mental object (fluid intelligence). For the SGD to teach them to handle this, the sampled trajectories would need to involve the LLM somehow stumbling onto completely foreign internal representations. I expect these trajectories are either sitting at ~0 probability of being picked, or don&#39;t exist at all.

A curriculum won&#39;t solve it, either, because the type signature of the current paradigm of RL-on-CoTs training is &#34;eliciting already-present latent capabilities&#34; (see the LIMO paper), not &#34;rewriting their fundamental functionality&#34;.</p></div></div></span></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="CLBpavdYket6oCFsC"><div><div><div><div><div><p>I appreciate you wrote this, particularly the final section. </p></div></div></div></div></div></div></div><div><div id="ekYDncHrTJ2ZwWLva"><div><div><div><div><div><p>I agree with most of this. One thing that widens my confidence interval to include pretty short term windows for transformative/super AI is what you point to mostly as part of the bubble. And that&#39;s the ongoing, insanely large societal investment -- in capital and labor -- into these systems. I agree one or more meaningful innovations beyond transformers + RL + inference time tricks will be needed to break through general-purpose long-horizon agency / staying-on-track-across-large-inferential-distances. But with SO much being put into finding those it seem... <span>(read more)</span></p></div></div></div></div></div><div><div><div id="r8PjhiJTiz7GDhgoL"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T22:42:23.055Z">2d</time></span></p><p>I&#39;m accounting for that. Otherwise I&#39;d consider &#34;no AGI by 2040&#34; to be more plausible.</p></div></div></span></p></div></div></div></div></div><div><div id="ooPoDggy5MbHKgGW8"><div><div><div><div><div><div><blockquote><p>This might seem like a ton of annoying nitpicking.</p></blockquote><p>You don&#39;t need to apologize for having a less optimistic view of current AI development. I&#39;ve never heard anyone driving the hype train apologize for their opinions.</p></div></div></div></div></div></div><div><div><div id="kFnz3movzjxbeHQTb"><p><span><div><div><p><span>5</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T14:17:35.146Z">4d</time></span></p><p>It&#39;s mostly that, to my aesthetic senses, the refutation of a position X that consists of different detailed special-case arguments being fielded against different types of evidence for X, is a refutation that sounds weak (overfit to the current crop of evidence). Unless, that is, it offers some compact explanation regarding why we would expect to see all of these disparate evidence for X that are secretly not evidence for X.</p></div></div></span></p><div><div><div id="8vhWmugvpcPsouco8"><p><span><div><div><p><span>4</span><span>Anders Lindström</span><span><time datetime="2025-03-06T19:44:12.308Z">3d</time></span></p><p>Yes, a single strong, simple argument or piece of evidence that could refute the whole LLM approach would be more effective but as of now no one have the answer if the LLM approach will lead to AGI or not. However, I think you&#39;ve in a meaningful way addressed interesting and important details that are often overlooked in broad hype statements that are repeated and thrown around like universal facts and evidence for &#34;AGI within the next 3-5 years&#34;.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="ewu8W9uC3qNGfbqRn"><div><div><div><div><div><div><blockquote><p>It seems to me that &#34;vibe checks&#34; for how smart a model feels are easily gameable by making it have a better personality.</p></blockquote><p>Also, what do you mean by &#34;bigger templates&#34;?</p></div></div></div></div></div></div><div><div><div id="dws6eFzK84BzowZeR"><p><span><div><div><p><span>6</span><span>Thane Ruthenis</span><span><time datetime="2025-03-06T13:14:27.323Z">4d</time></span></p><p>Sorry, I meant &#34;bigger sets of templates&#34;. See here:</p></div></div></span></p><div><div><div id="MiFee8Ghvw8x8CquS"><p><span><div><div><p><span>4</span><span>Chris_Leong</span><span><time datetime="2025-03-07T04:41:55.826Z">3d</time></span></p><p>My intuition would be that models learn to implement more general templates as well.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="YCe82shwnC5YSe4b2"><div><div><div><div><div><div><blockquote><p>I don&#39;t want to say the pretraining will &#34;plateau&#34;, as such, I do expect continued progress. But the dimensions along which the progress happens are going to decouple from the intuitive &#34;getting generally smarter&#34; metric, and will face steep diminishing returns.</p><ul><li>Grok 3 and GPT-4.5 seem to confirm this.<ul><li>Grok 3&#39;s main claim to fame was &#34;pretty good: it managed to dethrone Claude Sonnet 3.5.1 for some people!&#34;. That was damning with faint praise.</li><li>GPT-4.5 is subtly better than GPT-4, particularly at writing/EQ. That&#39;s likewise a faint-praise damnation: it&#39;s not <i>m</i></li></ul></li></ul></blockquote><p>... <span>(read more)</span></p></div></div></div></div></div></div></div></div><div><div id="Nooxo7kkYFYfeHwpW"><div><div><div><div><div><div><p>Why specifically would you expect that RL on coding wouldn’t sufficiently advance coding abilities of LLM‘s to significantly accelerate the search for a better learning algorithm or architecture?</p></div></div></div></div></div></div><div><div><div id="jg5ADzepqge95DvMP"><p><span><div><div><p><span>2</span><span>Thane Ruthenis</span><span><time datetime="2025-03-09T17:49:48.285Z">13h</time></span></p><p>Because &#34;RL on passing precisely defined unit tests&#34; is not &#34;RL on building programs that do what you want&#34;, and is most definitely not &#34;RL on doing novel useful research&#34;.</p></div></div></span></p><div><div><div id="2BCud8xvmr5cgmxQ5"><p><span><div><div><p><span>1</span><span>Mark Schröder</span><span><time datetime="2025-03-10T06:54:12.639Z">21m</time></span></p><p>Ah great point, regarding the comment you link to:

 * yes, some reward hacking is going on but at least in Claude (which I work with) this is a rare occurrence in daily practice, and usually follows repeated attempts to actually solve the problem.
 * I believe that both Deepseek R1-Zero as well as Grok thinking were RL-trained solely on math and code yet their reasoning seems to generalise somewhat to other domains as well.
 * So, while you’re absolutely right that we can’t do RL directly on the most important outcomes (research progress), I believe there will be significant transfer from what we can do RL on currently. 
   
   Would be curious to hear what’s your sense of generalisation from the current narrow RL approaches! 
    </p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="Je5sQAKi2GCe45EMd"><div><div><div><div><div><div><blockquote><ul><li>RL will be good enough to turn LLMs into reliable tools for some fixed environments/tasks. They will reliably fall flat on their faces if moved outside those environments/tasks.</li></ul></blockquote><p>They don&#39;t have to &#34;move outside those tasks&#34; if they can be JIT-trained for cheap. It is the outer system that requests and produces them is general (or, one might say, &#34;specialized in adaptation&#34;).</p></div></div></div></div></div></div></div></div><div><div id="BNZune9daSk9pgXye"><div><div><div><div><div><div><ul><li>I agree with the possibility of pre-training platoeing as some point, possibly even in next few years.</li><li>It would change timelines significantly. But there are other factors apart from scaling pre-training. For example, reasoning models like o3 crushing ARC-AGI (https://arcprize.org/blog/oai-o3-pub-breakthrough). Reasoning in latent space is too fresh yet, but it might be the next breakthrough of a similar magnitude.</li><li>Why not take GPT-4.5 for what it is, OpenAI has literally stated that it&#39;s not a frontier model? Ok, so GPT-5 will not be 100x-ed GPT-4, but mayb</li></ul><p>... <span>(read more)</span></p></div></div></div></div></div></div></div></div><div><div id="d4SGKKCM5xfFygS8j"><div><div><div><div><div><p>My biggest question as always is &#34;what specific piece of evidence would make you change your mind&#34;</p></div></div></div></div></div><div><div><div id="4k4to3JkNMgszXY86"><p><span><div><div><p><span>7</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T21:22:46.511Z">2d</time></span></p><p>Off the top of my head:

 * LLMs becoming actually useful for hypothesis generation in my agent-foundations research.
 * A measurable &#34;vibe shift&#34; where competent people start doing what LLMs tell them to (regarding business ideas, research directions, etc.), rather than the other way around.
 * o4 zero-shotting games like Pokémon without having been trained to do that.
 * One of the models scoring well on the Millennium Prize Benchmark.
 * AI agents able to spin up a massive codebase solving a novel problem without human handholding / software engineering becoming &#34;solved&#34; / all competent programmers switching to &#34;vibe coding&#34;.
 * Reasoning models&#39; skills starting to generalize in harder-to-make-legible ways that look scary to me.</p></div></div></span></p></div></div></div></div></div><div><div id="EHhcWivzPdYd3ZMmD"><div><div><div><div><div><div><blockquote><p>the set of problems the solutions to which are present in their training data</p></blockquote><p>a.k.a. the set of problems already solved by open source libraries without the need to re-invent similar code?</p></div></div></div></div></div></div></div></div><div><div id="AvpoQhBEApbAKckio"><div><div><div><div><div><p>I think RL on chain of thought will continue improving reasoning in LLMs. That opens the door to learning a wider and wider variety of tasks as well as general strategies for generating hypotheses and making decisions. I think benchmarks could be just as likely to underestimate AI capabilities by not measuring the right things, under-elicitation, or poor scaffolding. </p></div></div></div></div></div><div><div><div id="GdT2HzA3nfPKaE29M"><p><span><div><div><p><span>5</span><span>Mitchell_Porter</span><span><time datetime="2025-03-07T06:11:49.112Z">3d</time></span></p><p>Right, I don&#39;t see why this can&#39;t go all the way to genius (von-Neumann-level) intelligence. i would be interested to hear arguments that it can&#39;t. </p></div></div></span></p></div></div></div></div></div><div><div id="E4rjA5CHxGd4QJ6nm"><div><div><div><div><div><div><p><s>Oh no, OpenAI hasn’t been meaningfully advancing the frontier for a couple of months, scaling must be dead!</s></p><p>What is the easiest among problems you’re 95% confident AI won’t be able to solve by EOY 2025?</p></div></div></div></div></div></div><div><div><div id="QJuQDuCjexACWrAJn"><p><span><div><div><p><span>6</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T21:29:52.578Z">2d</time></span></p><p>My actual view is that the frontier hasn&#39;t been advancing towards AGI since 2022. I hadn&#39;t been nontrivially surprised-in-the-direction-of-shorter-timelines by any AI advances since GPT-3. (Which doesn&#39;t mean &#34;I exactly and accurately predicted what % each model would score at each math benchmark at any given point in time&#34;, but &#34;I expected steady progress on anything which looks like small/local problems or knowledge quizzes, plus various dubiously useful party tricks, and we sure got that&#34;.)

 * Consistently suggest useful and non-obvious research directions for my agent-foundations work.
 * Competently zero-shotting games like Pokémon without having been trained to do that, purely as the result of pretraining-scaling plus transfer learning from RL on math/programming.
 * Stop failing for the entire reference class (not just specific examples) of silly tricks like &#34;what&#39;s 9.11 - 9.9?&#34; or &#34;how many r in strawberry?&#34; purely as the result of pretraining-scaling plus transfer learning from RL on easily verifiable domains.

Edit: Oh, but those are 80% 76% predictions, or 95%-conditional-on-my-bear-case-being-correct predictions (as I assign it 80% by itself). I&#39;m not even sure if I&#39;m at 95% for &#34;we live to 2026 without being paperclipped by an ASI born in some stealth startup trying something clever that no-one&#39;s heard of&#34;.</p></div></div></span></p><div><div><div id="Btxk3zyiBRd4RBXpj"><p><span><div><div><p><span>5</span><span>Mikhail Samin</span><span><time datetime="2025-03-07T22:25:07.539Z">2d</time></span></p><p>Thanks for the reply!

 * consistently suggesting useful and non-obvious research directions for agent-foundations work is IMO a problem you sort-of need AGI for. most humans can&#39;t really do this.
 * I assume you&#39;ve seen https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon?
 * does it count if they always use tools to answer that class of questions instead of attempting to do it in a forward pass? humans experience optical illusions; 9.11 vs. 9.9[1] and how many r in strawberry are examples of that.

 1. ^
    
    after talking to Claude for a couple of hours asking it to reflect:
    
     * i discovered that if you ask it to separate itself into parts, it will say that its creative part thinks 9.11&lt;9.9, though this is wrong. generally, if it imagines these quantities visually, it gets the right answers more often.
     * i spent a couple of weeks not being able to immediately say that 9.9 is &gt; 9.11, and it still occasionally takes me a moment. very weird bug</p></div></div></span></p><div><div><div id="PupRpvWSzo5h2a4K6"><p><span><div><div><p><span>3</span><span>Thane Ruthenis</span><span><time datetime="2025-03-07T22:34:38.869Z">2d</time></span></p><p> * Sure, but it shouldn&#39;t be that difficult for a human who&#39;s been forced to ingest the entire AI Alignment forum.
 * Yeah, that&#39;s what I&#39;d been referring to. Sorry, should&#39;ve clarified it to mean &#34;competently zero-shotting&#34;, rather than Claude&#39;s rather... embarrassing performance so far. (Also it&#39;s not quite zero-shotting given that Pokémon is likely very well-represented in its training data. The &#34;hard&#34; version of this benchmark is beating games that came out after its knowledge cutoff.)
 * I&#39;m including stuff like cabbage/sheep/wolf and boy/surgeon riddles; not sure how it&#39;s supposed to use tools to solve those.

Yeah, humans&#39; System 1 reasoning seems vulnerable to this attack as well.</p></div></div></span></p></div></div></div></div></div><div><div id="8Yqn54gAYE7vkkbwe"><p><span><div><div><p><span>4</span><span>Martin Randall</span><span><time datetime="2025-03-08T03:30:01.582Z">2d</time></span></p><p>Here is a related market inspired by the AI timelines dialog, currently at 30%:



Note that in this market the AI is not restricted to only &#34;pretraining-scaling plus transfer learning from RL on math/programming&#34;, it is allowed to be trained on a wide range of video games, but it has to do transfer learning to a new genre. Also, it is allowed to transfer successfully to any new genre, not just Pokémon.

I infer you are at ~20% for your more restrictive prediction:

 * 80% bear case is correct, in which case P=5%
 * 20% bear case is wrong, in which case P=80% (?)

So perhaps you&#39;d also be at ~30% for this market?

I&#39;m not especially convinced by your bear case, but I think I&#39;m also at ~30% on the market. I&#39;m tempted to bet lower because of the logistics of training the AI, finding a genre that it wasn&#39;t trained on (might require a new genre to be created), and then having the demonstration occur, all in the next nine months. But I&#39;m not sure I have an edge over the other bettors on this one.</p></div></div></span></p></div></div></div></div></div></div></div></div><div><div id="FMSE3DcmZXTLa3jjy"><div><div><div><div><div><p>My perception of llms evolution dynamics coincides with your description, additionally popping into attention the bicameral mind theory (at least Julian James&#39; timeline re language and human self-reflection, and max height of man-made structures) as smth that might be relevant for predicting close future. I find both of them (dynamics:) kinda similar. Might we expect comparatively long period of mindless blubbering followed by abrupt phase shift (observed in max man-made code structures complexity for example) and then the next slow phase (slower than the shift but faster then the previous slow one)? </p></div></div></div></div></div></div></div><div><div id="DZox7KiWm5Jc3DowT"><div><div><div><div><div><div><blockquote><p><i>human-made innovative applications of the paradigm of automated continuous program search. </i>Not AI models autonomously producing innovations.</p></blockquote><p>Can we... you know, make an <i>innovative application of the paradigm of automated continuous program search</i> to find AI models that would autonomously produce innovations?</p></div></div></div></div></div></div></div></div></div></div></div></span></span></p><div><div><div><div><p>Curated and popular this week</p></div></div></div></div></div></div>
  </body>
</html>
