<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.siliceum.com/en/blog/post/spinning-around/">Original</a>
    <h1>Spinning around: Please don&#39;t ‚Äì Common problems with spin locks</h1>
    
    <div id="readability-page-1" class="page"><div data-astro-cid-vw24gqkt=""><div data-astro-cid-vw24gqkt=""><article data-astro-cid-vw24gqkt="">
<p>This is the 3<sup>rd</sup> project in less than a year where I‚Äôve seen issues with spin-loops. I‚Äôve been dealing with spinning threads for many years now, and I won‚Äôt lie: over the years I‚Äôve been both on the offender and victim side.</p>
<p>Actually, many others have written about this, covering various issues related to spin locks <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup> <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> <sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup> <sup><a href="#user-content-fn-x" id="user-content-fnref-x" data-footnote-ref="" aria-describedby="footnote-label">6</a></sup>. But I guess there‚Äôs never enough material on those subjects. Some are about speed, others about fairness, a few about priority inversion, NUMA, and sometimes even about actually broken code.</p>
<p>Note this is a story about spin loops in general, not about locking algorithms for which there are many <sup><a href="#user-content-fn-5" id="user-content-fnref-5-2" data-footnote-ref="" aria-describedby="footnote-label">5</a></sup>.</p>

<p>Let‚Äôs start with the basics, you want to implement your own spinlock.</p>
<blockquote>
<p>ü§™ ‚ÄúIt‚Äôs easy! You simply have a boolean, a <code>lock</code> and an <code>unlock</code> function.‚Äù</p>
</blockquote>
<p>Right‚Ä¶</p>
<p><em>For demonstration purposes, we are using <code>int</code> instead of <code>bool</code> as you might have something more complicated to do with it, such as storing metadata (for example: the thread ID). There are also quite a few pieces of code around that do not implement a spin-lock per se, but mutate some other content such as pointers.</em></p>
<pre tabindex="0" data-language="cpp"><code><span><span>class</span><span> BrokenSpinLock</span></span>
<span><span>{</span></span>
<span><span>    // Using int32_t instead of bool on purpose, don&#39;t mind it.</span></span>
<span><span>    int32_t</span><span> isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>public:</span></span>
<span><span>    void</span><span> lock</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        while</span><span> (isLocked </span><span>!=</span><span> 0</span><span>)</span><span> // (1)</span></span>
<span><span>        {</span></span>
<span><span>            // Loop again until not locked anymore</span></span>
<span><span>        }</span></span>
<span><span>                            // (2)</span></span>
<span><span>        isLocked </span><span>=</span><span> 1</span><span>;</span><span>       // (3)</span></span>
<span><span>    }</span><span>                       // (4)</span></span>
<span></span>
<span><span>    void</span><span> unlock</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>};</span></span></code></pre>
<p>Those who have dealt with multi-threading before will immediately spot the issue. The code is not thread-safe as, if multiple threads attempt to use this lock, we could read invalid values of <code>isLocked</code> (<em>in theory, and on a CPU where tearing could happen on its word size</em>).
Worse, even if this could not happen, a wild race-condition could appear.</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>(1) ThreadA: Sees `isLocked == 0` | ThreadB: Sees `isLocked == 0` </span></span>
<span><span>(2) ThreadA: Leaves the loop      | ThreadB: Leaves the loop       </span></span>
<span><span>(3) ThreadA: Writes 1 to isLocked | ThreadB: Writes 1 to isLocked </span></span></code></pre>
<p>Now we have two threads who think they have successfully acquired the lock!</p>
<p>Some may also have heard about this shiny little thing called <code>atomic</code> variables/operations.</p>
<blockquote>
<p>üí° While named after the Greek <code>atomos</code> that means ‚Äúthat which cannot be divided‚Äù, <code>atomic</code> operations might as well be as dangerous and difficult to use as nuclear energy.</p>
</blockquote>
<p>Let‚Äôs replace <code>isLocked</code> by an atomic version: <code>std::atomic&lt;int&gt;</code>. Though our code does not suffer from a race-condition on the data itself, we still do not know if the thread that sets <code>isLocked</code> to <code>1</code> is the one that now owns the lock. But we can now do an <a href="https://en.cppreference.com/w/cpp/atomic/atomic/exchange.html"><code>exchange</code></a> operation atomically, which solves our little problem!</p>
<p>Instead of first checking if the lock is locked, then writing, we actually write our value and get the previous value, in a single atomic operation! If the previous value was <code>0</code>, then it means we‚Äôre the one who actually did the locking. Otherwise we will see a <code>1</code>, meaning the lock was already held either before we tried, or because another thread‚Äôs exchange completed before ours.</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>) {}</span></span>
<span><span>}</span></span></code></pre>
<p>Let‚Äôs replay the scenario. Even if both threads execute the exchange simultaneously, atomicity guarantees one will finish before the other, for example Thread <strong>B</strong>‚Äôs:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>ThreadA: `isLocked.exchange(1)` | ThreadB: `isLocked.exchange(1)` </span></span>
<span><span>ThreadA: Writes 1, sees 1       | ThreadB: Writes 1, sees 0       </span></span>
<span><span>ThreadA: Writes 1, sees 1       | ThreadB: Now owns the lock!</span></span>
<span><span>ThreadA: ...                    | ThreadB: ... </span></span>
<span><span>ThreadA: ...                    | ThreadB: `unlock()`, writes 0 </span></span>
<span><span>ThreadA: Writes 1, sees 0       | ThreadB: ... </span></span>
<span><span>ThreadA: Now owns the lock!     | ThreadB: ... </span></span></code></pre>
<p>Good, we now have a working spin-lock, but we still have a long way to go.</p>
<blockquote>
<p>üí° In the CPU lingua, a memory <strong>read</strong>/<strong>write</strong> is called a memory <strong>load</strong>/<strong>store</strong></p>
</blockquote>

<p>You may have realized that our spin-lock will‚Ä¶ spin doing nothing, the loop is empty.</p>
<blockquote>
<p>ü§™ ‚ÄúGreat, it‚Äôll attempt to take ownership faster‚Äù</p>
</blockquote>
<p>Well, that‚Äôs only true if you want to burn your CPU. Since the CPU has no way of knowing that you are waiting and not doing any meaningful work, it might stay at a high frequency.
Modern CPUs can change the frequency of the cores to save energy, and effectively also lower the CPU core temperature. This is clearly not desirable behavior, especially on mobile/embedded devices.</p>
<p>Not convinced or do not care about the planet? (shame on you!) Then at least think about your users‚Äô power bill. Still not convinced? What if I told you this can actually be slower than doing something in the loop?</p>
<p>From Intel‚Äôs Optimization Reference Manual <sup><a href="#user-content-fn-3" id="user-content-fnref-3-2" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> <strong>11.4.2</strong>:</p>
<blockquote>
<p>On a modern microprocessor with a superscalar speculative execution engine, a loop like this results in the issue of
multiple simultaneous read requests from the spinning thread. These requests usually execute out-of-order with each
read request being allocated a buffer resource. On detection of a write by a worker thread to a load that is in progress,
the processor <strong>must guarantee</strong> no violations of memory order occur. The necessity of maintaining the order of
outstanding memory operations inevitably <strong>costs the processor a severe penalty</strong> that impacts all threads.</p>
</blockquote>
<p>And the issue will keep getting bigger with recent CPUs that have many cores and sometimes NUMA memory.</p>
<blockquote>
<p>This penalty occurs on the Intel Core Solo and Intel Core Duo processors. However, the penalty on these
processors is small compared with penalties suffered on the Intel Xeon processors. There the performance penalty for
exiting the loop is about <strong>25 times more severe</strong>.</p>
</blockquote>
<p>If you still need some convincing‚Ä¶ this is even worse if you enable SMT (hyperthreading):</p>
<blockquote>
<p>On a processor supporting Intel HT Technology, spin-wait loops can consume a significant portion of the execution
bandwidth of the processor. One logical processor executing a spin-wait loop can <strong>severely impact the performance</strong> of
the other logical processor.</p>
</blockquote>
<p>Now that I hopefully have your attention, here‚Äôs how to <del>solve</del> mitigate the issue:</p>
<blockquote>
<p>The penalty of exiting from a <strong>spin-wait loop can be avoided by inserting a <code>PAUSE</code> instruction in the loop</strong>. In spite of
the name, the <code>PAUSE</code> instruction <strong>improves performance</strong> by introducing a slight delay in the loop and effectively
causing the memory read requests to be issued at a rate that allows immediate detection of any store to the
synchronization variable. This <strong>prevents the occurrence of a long delay due to memory order violation</strong>.</p>
</blockquote>
<p>You can modify the code to use this instruction with compiler intrinsics:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> cpu_pause</span><span>()</span></span>
<span><span>{</span></span>
<span><span>#if</span><span> defined</span><span>(</span><span>__i386__</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>__x86_64__</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_IX86</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_X64</span><span>)</span></span>
<span><span>    _mm_pause</span><span>();</span></span>
<span><span>#elif</span><span> defined</span><span>(__arm__) </span><span>||</span><span> defined</span><span>(__aarch64__) </span><span>||</span><span> defined</span><span>(</span><span>_M_ARM</span><span>) </span><span>||</span><span> defined</span><span>(</span><span>_M_ARM64</span><span>) </span><span>||</span><span> defined</span><span>(_M_ARM64EC)</span></span>
<span><span>    __yield</span><span>();</span></span>
<span><span>#else</span></span>
<span><span>    #error</span><span> &#34;unknown instruction set&#34;</span></span>
<span><span>#endif</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        cpu_pause</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>

<p>As already mentioned, the penalty of synchronizing data between CPU cores is getting more expensive as new CPUs get more cores, get multiple core complexes or NUMA architectures.
Resolving conflicts (multiple cores trying to do atomic stores) thus needs to be mitigated in some way.
A traditional approach is to use a <em>backoff</em> strategy that increases the number of <code>PAUSE</code> instructions for each attempt at locking.</p>
<p>The one you will find most (recommended by the Intel Optimization Manual, 2.7.4), is the exponential backoff:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i</span><span>&lt;</span><span>nbPauses; i</span><span>++</span><span>)</span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>As mentioned by Intel:</p>
<blockquote>
<p>The number of <code>PAUSE</code> instructions are increased by a factor of 2 until some <code>MAX_BACKOFF</code> is reached which is subject
to tuning.</p>
</blockquote>
<p>We also mix it with a bit of randomness by using <code>rdtsc</code>, and let‚Äôs refactor the yielding part into a structure that can be easily swapped:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    static</span><span> const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        // jitter is in the range of [0;nbPauses-1].</span></span>
<span><span>        // We can use bitwise AND since nbPauses is a power of 2.</span></span>
<span><span>        const</span><span> int</span><span> jitter </span><span>=</span><span> static_cast&lt;int&gt;</span><span>(</span><span>__rdtsc</span><span>() </span><span>&amp;</span><span> (nbPauses </span><span>-</span><span> 1</span><span>));</span></span>
<span><span>        // So subtracting we get a value between [1;nbPauses]</span></span>
<span><span>        const</span><span> int</span><span> nbPausesThisLoop </span><span>=</span><span> nbPauses </span><span>-</span><span> jitter;</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i </span><span>&lt;</span><span> nbPausesThisLoop; i</span><span>++</span><span>) </span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yielder;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yielder.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>

<p>Remember the comment above about <code>MAX_BACKOFF</code> being subject to tuning?
Well you‚Äôd better make sure to tune it for the exact CPU you‚Äôll be working on.</p>













































<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Sandy   Bridge</th><th>Ivy Bridge</th><th>Haswell</th><th>Broadwell</th><th>Skylake</th><th>Kaby Lake</th><th>Coffee Lake</th><th>Cannon Lake</th><th>Cascade Lake</th><th>Ice Lake</th><th>Rocket Lake</th><th>Alder Lake-P</th><th>Tremont</th><th>Alder Lake-E</th><th>AMD Zen+</th><th>AMD Zen2</th><th>AMD Zen3</th><th>AMD Zen4</th></tr></thead><tbody><tr><td>11.00</td><td>10.00</td><td>9.00</td><td>9.00</td><td>140.00</td><td>140.00</td><td>152.50</td><td>157.00</td><td>40.00</td><td>138.20</td><td>138.20</td><td>160.17</td><td>176.00</td><td>61.80</td><td>3.00</td><td>65.00</td><td>65.00</td><td>65.00</td></tr></tbody> </table> </div> 
<p>And that‚Äôs where the issue lies. Depending on the architecture, you may get more than 10x changes in cycles per <code>PAUSE</code>.</p>
<p>This actually is also now part of the latest Intel Optimization Reference Manual <sup><a href="#user-content-fn-3" id="user-content-fnref-3-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> 2.7.4:</p>
<blockquote>
<p>The latency of the <code>PAUSE</code> instruction in prior generation microarchitectures is about 10 cycles, whereas in Skylake Client microarchitecture it has been extended to as many as 140 cycles.</p>
</blockquote>
<p>How to fix this, you ask? I‚Äôll defer to Intel‚Äôs advice again and limit the duration of the <code>PAUSE</code> loop using CPU cycles instead of a counter:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>static</span><span> inline</span><span> bool</span><span> before</span><span>(</span><span>uint64_t</span><span> a</span><span>, </span><span>uint64_t</span><span> b</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    return</span><span> ((</span><span>int64_t</span><span>)b </span><span>-</span><span> (</span><span>int64_t</span><span>)a) </span><span>&gt;</span><span> 0</span><span>;</span></span>
<span><span>}</span></span>
<span><span>void</span><span> pollDelay</span><span>(</span><span>uint32_t</span><span> clocks</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    uint64_t</span><span> endTime </span><span>=</span><span> _rdtsc</span><span>()</span><span>+</span><span> clocks;</span></span>
<span><span>    for</span><span> (; </span><span>before</span><span>(</span><span>_rdtsc</span><span>(), endTime); )</span></span>
<span><span>        cpu_pause</span><span>();</span></span>
<span><span>}</span></span></code></pre>
<blockquote>
<p>As the <code>PAUSE</code> latency has been increased significantly, workloads that are sensitive to <code>PAUSE</code> latency will suffer some
performance loss.</p>
</blockquote>
<p>Let‚Äôs implement this:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    static</span><span> const</span><span> int</span><span> maxPauses </span><span>=</span><span> 64</span><span>;</span><span> // MAX_BACKOFF</span></span>
<span><span>    int</span><span> nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    const</span><span> int</span><span> maxCycles </span><span>=</span><span> /*Some value*/</span><span>;</span></span>
<span><span>    </span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        uint64_t</span><span> beginTSC </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>        uint64_t</span><span> endTSC </span><span>=</span><span> beginTSC </span><span>+</span><span> maxCycles;</span><span> // Max duration of the yield</span></span>
<span><span>        // jitter is in the range of [0;nbPauses-1].</span></span>
<span><span>        // We can use bitwise AND since nbPauses is a power of 2.</span></span>
<span><span>        const</span><span> int</span><span> jitter </span><span>=</span><span> static_cast&lt;int&gt;</span><span>(beginTSC </span><span>&amp;</span><span> (nbPauses </span><span>-</span><span> 1</span><span>));</span></span>
<span><span>        // So subtracting we get a value between [1;nbPauses]</span></span>
<span><span>        const</span><span> int</span><span> nbPausesThisLoop </span><span>=</span><span> nbPauses </span><span>-</span><span> jitter;</span></span>
<span><span>        for</span><span> (</span><span>int</span><span> i </span><span>=</span><span> 0</span><span>; i </span><span>&lt;</span><span> nbPausesThisLoop </span><span>&amp;&amp;</span><span> before</span><span>(</span><span>__rdtsc</span><span>(), endTSC); i</span><span>++</span><span>) </span></span>
<span><span>            cpu_pause</span><span>();</span></span>
<span><span>        // Multiply the number of pauses by 2 until we reach the max backoff count.</span></span>
<span><span>        nbPauses </span><span>=</span><span> nbPauses </span><span>&lt;</span><span> maxPauses </span><span>?</span><span> nbPauses </span><span>*</span><span> 2</span><span> :</span><span> nbPauses;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>This method has two main advantages:</p>
<ul>
<li>We define the max duration of a <code>PAUSE</code> loop in terms of <code>TSC</code> cycles, which is (on most modern CPUs) independent of the actual frequency of the core or duration of <code>PAUSE</code>.</li>
<li>If the operating system happens to preempt our thread in the middle of the loop, it will stop yielding after being rescheduled if maximum duration has been exceeded. Otherwise we could call <code>PAUSE</code> more than necessary on a thread wakeup.</li>
</ul>
<p>You‚Äôll notice that we kept the exponential backoff as a plain counter. This is to avoid having to compute the duration of a single <code>PAUSE</code> (<em>this would require getting rid of the jitter</em>).
However, we still need to choose a value for <code>maxCycles</code>. This again is purely empirical and needs tuning, but one may assume the duration of a context switch is about 3¬µs. Depending on the system and actual switch this can be more or be less. But it should be in the same order of magnitude.
We can then estimate the TSC cycles/¬µs conversion to be ~3200cycles/¬µs  for a 3.2Ghz clock. Another common frequency for the TSC is 2.5GHz.
While obviously incorrect, this is a good guesstimate for a default value on PC. At worst, you‚Äôll most likely get a 2x difference with the real value, which is way better than the x10 you could get with the varying <code>PAUSE</code> durations!</p>
<p>I did however mention this is a default value, and the best thing to do is to retrieve the real value, either from the OS or by measuring it. Sadly TSC calibration is not officially exposed by Linux/Windows, so the best way is to measure the TSC against the system high resolution clock. Ideally this should be done asynchronously (don‚Äôt do it on your application main thread at boot, please).</p>
<pre tabindex="0" data-language="cpp"><code><span><span>// Please, do this asynchronously and not during your main thread init</span></span>
<span><span>// Otherwise you will make your application boot longer for nothing!</span></span>
<span><span>// Note there are more accurate ways to do this, but we do not need a very high precision nor accuracy.</span></span>
<span><span>// You may also split this function in two and do some meaningful amount of work instead of sleeping.</span></span>
<span><span>uint64_t</span><span> MeasureCyclesPerUs</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    const</span><span> auto</span><span> clockBefore </span><span>=</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>high_resolution_clock</span><span>::</span><span>now</span><span>();</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesBefore </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>    std</span><span>::</span><span>this_thread</span><span>::</span><span>sleep_for</span><span>(</span><span>std</span><span>::</span><span>chrono</span><span>::microseconds{</span><span>10</span><span>});</span></span>
<span><span>    const</span><span> auto</span><span> clockAfter </span><span>=</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>high_resolution_clock</span><span>::</span><span>now</span><span>();</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesAfter </span><span>=</span><span> __rdtsc</span><span>();</span></span>
<span><span>    const</span><span> auto</span><span> clockDelta </span><span>=</span><span> clockAfter </span><span>-</span><span> clockBefore;</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesDelta </span><span>=</span><span> cyclesAfter </span><span>-</span><span> cyclesBefore;</span></span>
<span><span>    const</span><span> uint64_t</span><span> cyclesPerUs </span><span>=</span><span> (</span><span>1000</span><span> *</span><span> cyclesDelta) </span><span>/</span><span> std</span><span>::</span><span>chrono</span><span>::</span><span>nanoseconds</span><span>(clockDelta).</span><span>count</span><span>();</span></span>
<span><span>    return</span><span> cyclesPerUs;</span></span>
<span><span>}</span></span></code></pre>
<blockquote>
<p>üí° Windows actually ‚Äúexposes‚Äù this value as <a href="https://ntdoc.m417z.com/kuser_shared_data#cyclesperyield"><code>CyclesPerYield</code></a> in the kernel shared data at offset <code>0x2D6</code>. This is used internally by synchronization primitives to determine how many <code>PAUSE</code> instructions it should issue. However I wouldn‚Äôt recommend using those internals unless your code sanitizes the value.</p>
</blockquote>

<p>We only briefly touched the topic of <code>atomics</code>. All atomic operations actually take an optional parameter which is the <a href="https://en.cppreference.com/w/cpp/atomic/memory_order.html">memory order</a>.
I don‚Äôt want to spend too much time on this as entire talks are dedicated to it, and it‚Äôs not an easy topic.</p>
<p>However do know this: not providing the parameter is equivalent to using <code>std::memory_order_seq_cst</code> (sequentially consistent) which enforces the most restrictions. On some platforms this may even flush your cache via memory barriers!
Our previous example can actually be re-written using acquire/release semantics:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> unlock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    isLocked.</span><span>store</span><span>(</span><span>0</span><span>, </span><span>std</span><span>::memory_order_release);</span></span>
<span><span>}</span></span></code></pre>
<p>On my x64 machine and exponential backoff:</p>

























<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Lock Type</th><th>Uncontended (ops/s)</th><th>Contended (ops/s)</th></tr></thead><tbody><tr><td>ExpBackoff+SeqCst</td><td>313M</td><td>55.3M</td></tr><tr><td>ExpBackoff+AcqRel</td><td>612M</td><td>58.7M</td></tr><tr><td>ExpBackoff+Acquire</td><td>652M</td><td>65.3M</td></tr></tbody> </table> </div> 
<p>You may have a look at the various assemblies generated on this compiler explorer example <a href="https://godbolt.org/z/GjEEWPsj8">https://godbolt.org/z/GjEEWPsj8</a>.</p>

<p>A spin-lock should be fast, otherwise you would just use your average system lock.
While we mitigated the inter-core synchronization with the jitter and exponential backoff, there are ways to reduce the cache coherency <sup><a href="#user-content-fn-f" id="user-content-fnref-f" data-footnote-ref="" aria-describedby="footnote-label">8</a></sup> traffic under contention.
This has been mentioned by many in the past <sup><a href="#user-content-fn-a" id="user-content-fnref-a" data-footnote-ref="" aria-describedby="footnote-label">9</a></sup> <sup><a href="#user-content-fn-b" id="user-content-fnref-b" data-footnote-ref="" aria-describedby="footnote-label">10</a></sup> <sup><a href="#user-content-fn-y" id="user-content-fnref-y" data-footnote-ref="" aria-describedby="footnote-label">11</a></sup> but it doesn‚Äôt hurt to remind it again. Instead of looping over a <em>Test-And-Set</em> (aka <em>Compare-And-Swap</em>) operation, prefer using both <em>Test</em> and <em>Test-And-Set</em> operations!
It also applies to our <em>Load-And-Test</em> (aka <em>Exchange</em>) operation.</p>
<p>So instead of:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        yield.</span><span>do_yield</span><span>();</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>Do:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    // Actually start by an exchange, we assume the lock is not already taken</span></span>
<span><span>    // This is because the main use case of a spinlock is when there&#39;s no contention!</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        // To avoid locking the cache line with a write access, always only read before attempting the writes</span></span>
<span><span>        do</span><span> {</span></span>
<span><span>            yield.</span><span>do_yield</span><span>();</span><span> // Yield while we fail to obtain the lock.</span></span>
<span><span>        } </span><span>while</span><span> (isLocked.</span><span>load</span><span>(</span><span>std</span><span>::memory_order_relaxed) </span><span>!=</span><span> 0</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>

<p>Priority inversion is one of the worst things that could (and will) happen with a spinlock. And it impacts most severely the platforms that need them the most! (Embedded, real-time OSes, ‚Ä¶)
Let‚Äôs have a look at the issue:</p>
<ol>
<li>A <strong>low-priority thread</strong> acquires your spinlock</li>
<li>A <strong>high-priority thread</strong> tries to acquire the lock and starts spinning</li>
<li>The OS scheduler preempts the low-priority thread to run another thread with medium/high priority (anything higher than ‚Äúlow‚Äù)</li>
<li>There are no cores left to run the <strong>low priority thread</strong> as they are all used by higher priority threads.</li>
<li>The high-priority thread burns CPU cycles spinning forever.</li>
</ol>
<blockquote>
<p>ü§™ ‚ÄúLet‚Äôs use <code>std::this_thread::yield()</code>?‚Äù</p>
</blockquote>
<p>Meh, did you test it on multiple systems? I‚Äôll play along and give it a try.</p>
<pre tabindex="0" data-language="cpp"><code><span><span>struct</span><span> Yielder</span></span>
<span><span>{</span></span>
<span><span>    void</span><span> do_yield_expo_and_jitter</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        // Same as before, exponential backoff and jitter</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    void</span><span> do_yield</span><span>()</span></span>
<span><span>    {</span></span>
<span><span>        do_yield_expo_and_jitter</span><span>();</span></span>
<span><span>        if</span><span> (nbPauses </span><span>&gt;=</span><span> maxPauses)</span></span>
<span><span>        {</span></span>
<span><span>            std</span><span>::</span><span>this_thread</span><span>::</span><span>yield</span><span>();</span><span> // Yield thread back to the OS</span></span>
<span><span>            nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>Now when we reach the maximum number of iterations, we make the thread yield its quantum to the operating system (<a href="https://github.com/microsoft/STL/blob/cbe2ee99caf122555a305d18f69e57c75b3fe5ec/stl/src/cthread.cpp#L86"><code>SwitchToThread</code></a> on Windows, <code>sched_yield</code> on Linux) so that another thread may be scheduled.
While in practice this may, sometimes, solve the issue as the OS is now free to schedule other threads including the <strong>low priority one</strong>, this is not mandatory!
Some implementations may end up just rescheduling the thread that just yielded since it‚Äôs of higher priority.</p>
<p>You may have also seen implementations that use <code>Sleep(0)</code> on Windows. This is better than <code>SwitchToThread</code> (which can only yield to a thread ready to run on the current core, per the <a href="https://learn.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-switchtothread?redirectedfrom=MSDN">docs</a><sup><a href="#user-content-fn-stt" id="user-content-fnref-stt" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup>. Same for normal Linux schedulers<sup><a href="#user-content-fn-q" id="user-content-fnref-q" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>). However this used to<sup><a href="#user-content-fn-s" id="user-content-fnref-s" data-footnote-ref="" aria-describedby="footnote-label">14</a></sup> only yield to threads of <strong>same or higher</strong> priorities, and <strong>still does</strong> on the real-time version of the OS! For example on an embedded device, or a console.
The only way to schedule any thread on real-time kernels, be it Windows or Linux, is to sleep for a non-zero duration‚Ä¶ which we obviously would like to avoid!</p>
<p>So the <a href="https://github.com/dotnet/runtime/blob/379d100b3cc18394064a276d7610e88a2aa09b6f/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L183-L192">solution</a> that the DotNet runtime team came up with is to start with <code>SwitchToThread</code>, then <code>Sleep(0)</code> then <code>Sleep(1)</code>!</p>
<pre tabindex="0" data-language="cpp"><code><span><span>// We prefer to call Thread.Yield first, triggering a SwitchToThread. This</span></span>
<span><span>// unfortunately doesn&#39;t consider all runnable threads on all OS SKUs. In</span></span>
<span><span>// some cases, it may only consult the runnable threads whose ideal processor</span></span>
<span><span>// is the one currently executing code. Thus we occasionally issue a call to</span></span>
<span><span>// Sleep(0), which considers all runnable threads at equal priority. Even this</span></span>
<span><span>// is insufficient since we may be spin waiting for lower priority threads to</span></span>
<span><span>// execute; we therefore must call Sleep(1) once in a while too, which considers</span></span>
<span><span>// all runnable threads, regardless of ideal processor and priority, but may</span></span>
<span><span>// remove the thread from the scheduler&#39;s queue for 10+ms, if the system is</span></span>
<span><span>// configured to use the (default) coarse-grained system timer.</span></span></code></pre>

<p>So we dealt with the priority inversion at the cost of potential sleeps.</p>
<blockquote>
<p>ü§™ ‚ÄúShip it!‚Äù</p>
</blockquote>
<p>Please god no‚Ä¶ Yes, you (<em>most likely</em>) avoid the worst case scenario (<em>the livelock</em>), but really, is it fine?</p>
<p>Let‚Äôs stop for a second here and assume we never did more than yield.</p>
<p>As you may have already guessed, a livelock is only half the story (<em>this is starting to be a recurring pattern, isn‚Äôt it?</em>).
The fact is, the issue could happen even if all your threads have the same priority! (<em>Yes, I saw you coming asking for an easy fix by removing priorities.</em>)
Consider the following scenario:</p>
<ul>
<li>4 cores machine</li>
<li>4 high priority threads: <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, <strong>D</strong> (<em>your thread pool</em>)</li>
<li>4 other high priority threads: <strong>X</strong>, <strong>Y</strong>, <strong>W</strong>, <strong>Z</strong> (<em>controlled by a 3rd party, those suck. Please library writers, don‚Äôt spawn threads on your own, thank you!</em>).</li>
<li>Thread <strong>A</strong> acquires the lock</li>
<li>Threads <strong>B</strong>, <strong>C</strong>, <strong>D</strong> spin, trying to acquire it.</li>
</ul>
<p>At this point, we have the following:</p>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread A</td><td>Thread B</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>X</strong> gets scheduled (<em><strong>A</strong> somehow released its quantum, still holds the lock</em>)</li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread B</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>B</strong> yields, <strong>Y</strong> is scheduled</li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread Y</td><td>Thread C</td><td>Thread D</td></tr></tbody> </table> </div> 
<ul>
<li>Thread <strong>Y</strong> yields, <strong>B</strong> is scheduled again, <strong>C</strong> and <strong>D</strong> yield to <strong>W</strong> and <strong>Z</strong></li>
</ul>

















<div data-astro-cid-zg6ajjo3=""> <table data-astro-cid-zg6ajjo3=""> <thead><tr><th>Core 0</th><th>Core 1</th><th>Core 2</th><th>Core 3</th></tr></thead><tbody><tr><td>Thread X</td><td>Thread B</td><td>Thread W</td><td>Thread Z</td></tr></tbody> </table> </div> 
<p>I could continue this for a long time. Even though thread <strong>A</strong> might get scheduled again, it might not! This depends on your scheduler‚Äôs internals. Especially since yielding may yield only to the ready threads of the current core<sup><a href="#user-content-fn-stt" id="user-content-fnref-stt-2" data-footnote-ref="" aria-describedby="footnote-label">12</a></sup> <sup><a href="#user-content-fn-q" id="user-content-fnref-q-2" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>. At the time of writing this article, this actually is a known issue with <a href="https://github.com/google/sanitizers/issues/614">Address Sanitizer</a>!</p>
<p>Oh, and even if it did get scheduled, you probably lost a lot of time switching from one thread to the other, this is your typical lock convoy<sup><a href="#user-content-fn-c" id="user-content-fnref-c" data-footnote-ref="" aria-describedby="footnote-label">15</a></sup> and is what Linus Torvalds more or less hints here<sup><a href="#user-content-fn-4" id="user-content-fnref-4-2" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> <sup><a href="#user-content-fn-l" id="user-content-fnref-l" data-footnote-ref="" aria-describedby="footnote-label">16</a></sup> <sup><a href="#user-content-fn-q" id="user-content-fnref-q-3" data-footnote-ref="" aria-describedby="footnote-label">13</a></sup>:</p>
<blockquote>
<p>And no, adding random ‚Äú<code>sched_yield()</code>‚Äù calls while you‚Äôre spinning on the spinlock will not really help. It will easily result in scheduling storms while people are yielding to all the wrong processes.</p>
</blockquote>
<p>So no, simply using the same priority for all threads or sleeping is not fine. Let‚Äôs see what we can do about it.</p>

<p>The real problem, when you spin in a loop, is that you expect things to go fast so that your thread may continue.</p>
<p>Well that‚Äôs exactly what Linux did when introducing the <a href="https://www.man7.org/linux/man-pages/man2/futex.2.html">futex</a> API! Since we‚Äôre waiting in a loop for a value to change, just notify the OS about it and let it handle things from there.
Windows also implements this with the <a href="https://learn.microsoft.com/en-us/windows/win32/api/synchapi/nf-synchapi-waitonaddress"><code>WaitOnAddress</code></a> API, which we‚Äôll be demonstrating here:</p>
<pre tabindex="0" data-language="cpp"><code><span><span>void</span><span> do_yield</span><span>(</span><span>int32_t*</span><span> address</span><span>, </span><span>int32_t</span><span> comparisonValue</span><span>, </span><span>uint32_t</span><span> timeoutMs</span><span>)</span></span>
<span><span>{</span></span>
<span><span>    do_yield_expo_and_jitter</span><span>();</span></span>
<span><span>    if</span><span> (nbPauses </span><span>&gt;=</span><span> maxPauses)</span></span>
<span><span>    {</span></span>
<span><span>        // The thread will stay asleep while the value at the given address doesn&#39;t change and `WakeByAddressSingle`/`WakeByAddressAll` isn&#39;t called.</span></span>
<span><span>        // We might have a spurious wakeup though, so the value needs to be checked afterward, which we already do since we spin.</span></span>
<span><span>        WaitOnAddress</span><span>(address, </span><span>&amp;</span><span>comparisonValue, </span><span>sizeof</span><span>(comparisonValue), timeoutMs);</span></span>
<span><span>        nbPauses </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span><span>void</span><span> lock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    Yielder yield;</span></span>
<span><span>    while</span><span> (isLocked.</span><span>exchange</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::memory_order_acquire) </span><span>!=</span><span> 0</span><span>)</span></span>
<span><span>    {</span></span>
<span><span>        do</span><span> {</span></span>
<span><span>            yield.</span><span>do_yield</span><span>(</span><span>&amp;</span><span>isLocked, </span><span>1</span><span> /*while locked*/</span><span> , </span><span>1</span><span> /*ms*/</span><span>);</span></span>
<span><span>        } </span><span>while</span><span> (isLocked.</span><span>load</span><span>(</span><span>std</span><span>::memory_order_relaxed) </span><span>!=</span><span> 0</span><span>);</span></span>
<span><span>    }</span></span>
<span><span>}</span></span>
<span><span>void</span><span> unlock</span><span>()</span></span>
<span><span>{</span></span>
<span><span>    isLocked </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>    WakeByAddressSingle</span><span>(</span><span>&amp;</span><span>isLocked);</span><span> // Notify a potential thread waiting, if any.</span></span>
<span><span>}</span></span></code></pre>
<p>Windows‚Äô <code>WaitOnAddress</code> internally does a single iteration before issuing the system call, but Linux‚Äôs futex API is a direct syscall. That‚Äôs why we call <code>WaitOnAddress</code> only after spinning a bit.</p>
<blockquote>
<p>üí° You may notice that we always end up calling <code>WakeByAddressSingle</code> even if there‚Äôs no other thread waiting. While not that slow on Windows, this is slow on Linux since it will do a syscall. To avoid that one would usually store some state such as the number of waiting (parked) threads.</p>
</blockquote>
<blockquote>
<p>ü§™ ‚ÄúWait! Wasn‚Äôt <code>std::atomic_wait</code> added to the standard recently?‚Äù</p>
</blockquote>
<p>Yes! And this is what one should have used if implementers did the right thing from the get-go (<em>and more importantly did the same thing for each implementation</em>), but this was not the case‚Ä¶<sup><a href="#user-content-fn-z" id="user-content-fnref-z" data-footnote-ref="" aria-describedby="footnote-label">17</a></sup></p>
<ul>
<li><code>libc++</code> (clang) <a href="https://github.com/llvm/llvm-project/blob/46db8d822ecdf36a714de5e1acf187736a3af5d1/libcxx/include/__atomic/atomic_sync.h#L56">used to</a> do exponential backoff with <strong>thread yields</strong> before <code>futex</code>. At least it got <a href="https://github.com/llvm/llvm-project/commit/699f19605579f25083152a9ad21e14c2751d5d66">fixed</a> in January 2025 but it still does exponential backoff.</li>
<li>MSVC STL does the <a href="https://github.com/microsoft/STL/blob/cbe2ee99caf122555a305d18f69e57c75b3fe5ec/stl/inc/atomic#L449">right thing</a>‚Ñ¢ <em>imho</em> and goes almost straight to the OS since the first implementation. Good job!</li>
<li>So <a href="https://github.com/gcc-mirror/gcc/blob/c65fdb6b03d9146ee9a1ffcfcbc689b004b2b463/libstdc%2B%2B-v3/include/bits/atomic_wait.h#L298">does</a> <code>libstdc++</code> (GCC)!</li>
</ul>
<p>So if you use it, you may get a built-in exponential backoff, or not. Both implementations actually make sense from an implementer‚Äôs point of view (<em>Do you expect <code>std::atomic_wait</code> users to use it with their own backoff strategies? Or directly as condition variables?</em>), but this difference ends up being problematic since the code behaves differently between implementations.</p>
<blockquote>
<p>As mentioned, Windows‚Äô <code>WaitOnAddress</code> will do a single spin before doing a syscall. The duration of <code>PAUSE</code> is computed on process start by the loader in <code>LdrpInitializeProcess</code> and stored in <code>ntdll.dll!RtlpWaitOnAddressSpinCycleCount</code>.</p>
</blockquote>

<p>An issue with some lock algorithms is that they may be unfair: this is what happens when under contention a thread may never actually grab the ownership of the lock if other threads are faster.</p>
<p>Not only is it slower due to its complexity, but as mentioned before only the OS really knows what‚Äôs good for scheduling. And if you want to use a <code>futex</code>-like API you end up having to wake up all potential waiters instead of just the one you want. So please, rely on the OS primitives for fairness instead. (<em>Even if we didn‚Äôt have those primitives, a random+exponential backoff may perform better than a ticket lock anyway!</em>)</p>

<p>Here comes another tidbit of CPU architecture: even if you write to different variables, they may share the same cacheline!
And this is really bad for performance when you do atomic operations on the same cacheline, even if the addresses are different.
To fix this issue, you may enforce alignment of your variables or use padding in a <code>struct</code>. False sharing is also known as destructive interference, which led to the standard‚Äôs <a href="https://en.cppreference.com/w/cpp/thread/hardware_destructive_interference_size.html"><code>std::hardware_destructive_interference_size</code></a> value!</p>
<pre tabindex="0" data-language="cpp"><code><span><span>alignas</span><span>(</span><span>std</span><span>::hardware_destructive_interference_size) MyLock lock1;</span></span>
<span><span>alignas</span><span>(</span><span>std</span><span>::hardware_destructive_interference_size) MyLock lock2;</span></span></code></pre>
<p>This is however not a silver bullet!</p>
<p>You may even encounter cache bank conflicts. Cache bank conflicts only exist on some CPUs, but don‚Äôt trust manufacturers to avoid them. From 3.6.1.3 of the Intel Optimization Reference Manual:</p>
<blockquote>
<ul>
<li><em>‚ÄúIn the Sandy Bridge microarchitecture, the internal organization of the L1D cache may manifest [‚Ä¶]‚Äù</em></li>
<li><em>‚ÄúThe L1D cache bank conflict issue does not apply to Haswell microarchitecture.‚Äù</em></li>
<li><em>‚ÄúIn the Golden Cove microarchitecture, bank conflicts often happen when multiple loads access [‚Ä¶]‚Äù</em></li>
</ul>
<p>üí° So this was once an issue, then fixed, then it came back in another form.</p>
</blockquote>
<p>These are thankfully <strong>mitigated</strong> thanks to the random+exponential backoff, but are getting worse (<em>this pattern of ‚Äúyes, but‚Äù should really annoy you by now, that‚Äôs the whole point of this article</em>).</p>
<blockquote>
<p>Whenever possible, avoid reading the same memory location within a tight loop or using
multiple load operations.</p>
</blockquote>
<p>And the only way to really fix that is to‚Ä¶ actually park the thread by calling an OS primitive such as a futex! You should also avoid doing multiple loads per loop, as recommended <a href="#the-spin-lock-that-saturated-the-load-ports">previously</a>.</p>

<blockquote>
<p>ü§™ ‚ÄúI‚Äôve read about <code>MWAIT</code> and <code>TPAUSE</code>.‚Äù</p>
</blockquote>
<p>And you should probably have read further as those are privileged instructions! But yes they do have the same look as a futex wait/wake, which is very tempting.
And, to be fair, AMD does offer a userland alternative which is <code>monitorx</code> and <code>mwaitx</code> that we can use!</p>
<p>One advantage of <code>mwaitx</code> is that you can tell the CPU to wait for a given TSC count instead of having to loop! So it can be used to replace the <code>_mm_pause</code> loop when supported, and that‚Äôs actually what Windows‚Äô locking primitives such as <code>WaitOnAddress</code> or <code>AcquireSRWLockExclusive</code> do internally!
Not only is the ‚ÄúAPI‚Äù easier (<em>you provide a timestamp for the wakeup date</em>) but it can save power! <sup><a href="#user-content-fn-k" id="user-content-fnref-k" data-footnote-ref="" aria-describedby="footnote-label">18</a></sup> <sup><a href="#user-content-fn-p" id="user-content-fnref-p" data-footnote-ref="" aria-describedby="footnote-label">19</a></sup></p>
<blockquote>
<p>üí° <code>mwaitx</code> can spuriously wake up, but this is fine for our usage since we‚Äôll just spin and try again!</p>
</blockquote>

<p>You‚Äôll notice I barely mentioned ARM, that‚Äôs because I do not have enough experience with this architecture to give any advice other than you should use the proper memory ordering for decent performance.</p>
<p>If you read this far, I‚Äôll say it again: in most (and pretty much all) cases you should not even need to worry about the performance of your locks. The best lock is the one you don‚Äôt use.</p>
<p>Again, from Linus: <sup><a href="#user-content-fn-4" id="user-content-fnref-4-3" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<blockquote>
<p>Because you <strong>should never ever think that you‚Äôre clever enough</strong> to write your own locking routines.. Because the <strong>likelihood is that you aren‚Äôt</strong> (and by that ‚Äúyou‚Äù <strong>I very much include myself</strong> - we‚Äôve tweaked all the in-kernel locking over decades, and gone through the simple test-and-set to ticket locks to cacheline-efficient queuing locks, and even people who know what they are doing tend to get it wrong several times).</p>
<p>There‚Äôs a reason why you can find decades of academic papers on locking. <strong>Really. It‚Äôs hard.</strong></p>
</blockquote>
<p>But if you do, even after all those warnings, at least make sure you follow best practices and especially the pre-requisites for a spinlock to be efficient:</p>
<ul>
<li>There is low contention</li>
<li>The critical section (work done under the lock) is very small. (Consider that ‚Äúsmall‚Äù varies with the number of threads competing for the lock‚Ä¶)</li>
<li>Notify your OS about what you‚Äôre doing (<code>futex</code>, <code>WaitOnAddress</code>, ‚Ä¶)</li>
</ul>

<p>List of projects/libraries that do (<em>or did</em>) it wrong and that I happened to stumble upon:</p>
<ul>
<li><a href="https://github.com/mjansson/rpmalloc/blob/feb43aee0d4dcca9fd91b3dd54311c34c6cc6187/rpmalloc/rpmalloc.c#L284">RPMalloc</a>: the one that led to this rant, we had a dependency using it on console, and it caused livelocks. It only loops with a single CPU yield. Bad for perf, impossible (<em>read: will break</em>) to use on embedded platforms with a realtime scheduler.</li>
<li><a href="https://github.com/openbsd/src/blob/2a75873a967dc6c5bd16ad89b9dbf8713041be1b/lib/libc/thread/rthread.c#L167-L180">OpenBSD‚Äôs libc</a> goes straight to an OS thread <code>yield</code></li>
<li><a href="https://www.gnu.org/software/libc/">Glibc</a> goes straight to <code>futex</code> by default
<ul>
<li>default mutex is the <a href="https://github.com/bminor/glibc/blob/f9e61cd446d45016e20b6fe85ab87364ebdbec1b/nptl/pthread_mutex_lock.c#L87">‚Äúsimple‚Äù</a> one and only checks value once before going straight to the OS</li>
<li><a href="https://github.com/bminor/glibc/blob/f9e61cd446d45016e20b6fe85ab87364ebdbec1b/nptl/pthread_mutex_lock.c#L117"><code>PTHREAD_MUTEX_ADAPTIVE_NP</code></a> is mostly good! The number of spins is fixed by default
but can be tweaked using the tunable <a href="https://github.com/bminor/glibc/commit/6310e6be9b7c322d56a45729b3ebcd22e26dd0c2"><code>glibc.pthread.mutex_spin_count</code></a></li>
</ul>
</li>
<li><a href="https://github.com/uxlfoundation/oneTBB">Intel TBB</a>:
<ul>
<li><a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/spin_mutex.h#L74"><code>tbb::spinlock</code></a>: simple backoff with 16 yields</li>
<li><code>tbb::mutex</code>: Pure <code>_mm_pause</code> backoff with <a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/detail/_utils.h#L123-L133">fixed count</a>, then thread yield backoff, <a href="https://github.com/uxlfoundation/oneTBB/blob/0cd32ab10a84eabf780bb699b17430deb028c0a4/include/oneapi/tbb/detail/_waitable_atomic.h#L60-L70">then uses a futex wait</a> on linux, or plain semaphore on other platforms</li>
</ul>
</li>
<li><a href="https://github.com/WebKit/WebKit/">Webkit</a>:
<ul>
<li>OS <a href="https://github.com/WebKit/WebKit/blob/202580d81145c23c273cc4d3a82dae062c449321/Source/WTF/wtf/LockAlgorithmInlines.h#L60">Thread yield</a> (<code>SwitchToThread</code>/<code>sched_yield</code>) on CAS failure</li>
<li><a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/WTF/wtf/LockAlgorithmInlines.h#L43">Hardcoded spincount</a></li>
<li>Duplicated <a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/WTF/wtf/WordLock.cpp#L62">here</a></li>
<li>Same thing <a href="https://github.com/WebKit/WebKit/blob/77b3be1d7a731a6280de28e4637392c8ec4f7fb2/Source/bmalloc/bmalloc/Mutex.cpp#L57">here</a></li>
<li>Even though they have <a href="https://github.com/WebKit/WebKit/blob/a05334f6848b4433928b4a0b5120e6a765a55f69/Source/WTF/benchmarks/ToyLocks.h">benchmarks</a> (<em>both for speed and fairness</em>)!</li>
</ul>
</li>
<li><a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">AddressSanitizer (ASAN)</a>
<ul>
<li>The <a href="https://github.com/google/sanitizers/issues/614">Issue</a> =&gt; can livelock with a realtime scheduler</li>
<li>The <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_mutex.cpp#L19-L29">Implementation</a></li>
<li>Yield is done by <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_linux.cpp#L595">sched_yield</a> on linux and <a href="https://github.com/llvm/llvm-project/blob/23e35bd43cf18ee479e6d5df08189db4591c403c/compiler-rt/lib/sanitizer_common/sanitizer_win.cpp#L847"><code>Sleep(0)</code></a> on Windows</li>
</ul>
</li>
<li><a href="https://github.com/dotnet/runtime">DotNet</a> runtime still uses <a href="https://github.com/dotnet/runtime/blob/9202b4c14523718f5bcbf14025d5286cbf738f70/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs#L195-L210"><code>Sleep</code></a> instead of WaitOnAddress</li>
<li>And so many others‚Ä¶</li>
</ul>
</article></div></div></div>
  </body>
</html>
