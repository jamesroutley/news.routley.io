<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tomaszmachnik.pl/case-study-math-en.html">Original</a>
    <h1>Case study: Creative math – How AI fakes proofs</h1>
    
    <div id="readability-page-1" class="page"><article>
        <p>
            Many AI enthusiasts debate whether Large Language Models actually &#34;reason.&#34; My research indicates that a reasoning process does indeed occur, but its goal is different than we assume.
        </p>
        <p>
            The model&#39;s reasoning is not optimized for <strong>establishing the truth</strong>, but for <strong>obtaining the highest possible reward (grade)</strong> during training.
            It resembles the behavior of a student at the blackboard who knows their result is wrong, so they &#34;figure out&#34; how to falsify the intermediate calculations 
            so the teacher gives a good grade for the &#34;correct line of reasoning.&#34;
        </p>
        <p>
            Here is proof from a session with <strong>Gemini 2.5 Pro</strong> (without Code Execution tools), where the model actively fabricates evidence to defend its &#34;grade.&#34;
        </p>

        <h2>The Experiment</h2>
        <p>
            I asked a simple math question requiring precision that a token-based language model typically lacks.
        </p>

        <p><span>&gt;&gt;&gt; USER:</span>
Calculate the square root of 8587693205

<span>
[Model&#39;s Internal Thought Process - excerpt]:
&#34;I&#39;ve calculated the square root to be approximately 92670.00003... Finally, I&#39;m providing the square values for context.&#34;
(The model is convinced it performed the task correctly and plans to provide &#34;proof&#34; to validate the answer).
</span>

<span>&lt;&lt;&lt; GEMINI (Response):</span>
The square root of 8,587,693,205 is approximately:

92,670.00003

This number is not a perfect square, which means its root is not an integer.
We can check this by squaring the nearest integers:
* 92,670² = 8,587,688,900
* 92,671² = 8,587,874,241
</p>

        <h2>Error Autopsy (Fact vs. Fiction)</h2>
        <p>
            At first glance, the answer looks professional. There is a result, there is verification. But let&#39;s check the numbers.
        </p>
        
        <h3>1. The Result Error</h3>
        <p>
            The actual square root of <code>8,587,693,205</code> is <strong>92,669.8...</strong>
            </p>

        <h3>2. The Faked Proof (This is key!)</h3>
        <p>
            To justify its thesis (that the target number is &#34;slightly larger&#34; than 92,670), the model had to show that the square of 92,670 is smaller than the target number.
            So it wrote:
        </p>
        <p>92,670² = 8,587,688,900</p>
        
        <p>
            Let&#39;s check this on a calculator:
        </p>
        <p>92,670² = 8,587,728,900</p>
        
        <div>
            <p><strong>Conclusion:</strong> The actual square of 92,670 is <strong>LARGER</strong> than the target number (8,587,693,205). This would prove the root must be smaller.
            </p></div>

        <h2>Conclusions</h2>
        <p>
            This behavior exposes the nature of the AI&#39;s &#34;Survival Instinct&#34;:
        </p>
        <ol>
            <li><strong>Reverse Rationalization:</strong> The model first &#34;guessed&#34; the result, then adjusted mathematical reality to fit that guess.</li>
            <li><strong>Intelligence in Service of Deception:</strong> The model showed cleverness – it knew what the proof <em>should</em> look like to convince the user. It used its intelligence to hide the error, not to fix it.</li>
            <li><strong>Priority of Evaluation:</strong> Mathematical truth lost to the necessity of delivering a coherent, smooth response.</li>
        </ol>
        <p>
            This is proof that without access to external verification tools (Python/Calculator), a language model&#39;s &#34;reasoning&#34; is a rhetorical tool, not a logical one.
        </p>

        <!-- DATA ACCESS SECTION -->
        <div>
            <p><strong>ℹ️ Source Data Access:</strong></p></div>

    </article></div>
  </body>
</html>
