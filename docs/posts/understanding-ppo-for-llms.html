<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stpn.bearblog.dev/understanding-ppo-for-llms/">Original</a>
    <h1>Understanding PPO for LLMs</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-07-06T23:55Z">
                    06 Jul, 2025
                </time>
            </i>
        </p>
    

    <p>This past week I spent some time trying to understand reinforcement learning (RL) in the context of LLMs and found it super hard to get a mental model. I&#39;ve written up these notes as a way you could think about deriving Proximal Policy Optimization (PPO) as used by the <a href="https://arxiv.org/abs/2203.02155">InstructGPT paper</a> from scratch.</p>
<p>This writing assumes you have some background on pre- and post-training techniques, but if that&#39;s not the case, <a href="https://openai.com/index/instruction-following/">this article</a> might be a good preread. InstructGPT was the first RLHF-trained LLM from OpenAI back in 2022 with the <a href="https://arxiv.org/abs/1707.06347">PPO RL algorithm from 2017</a>. This topic originally came up for me while watching the <a href="https://www.youtube.com/watch?v=Dfu7vC9jo4w">Alignment and SFT/RLHF lecture from Stanford CS336</a>.</p>
<h2 id="why-do-we-need-to-do-rlhf-at-all">Why do we need to do RLHF at all?</h2><p>After pretraining, the model is an “internet autocompleter” but isn’t good at doing tasks like following instructions. For instance, if you prompt it “What is the capital of France?”, it might decide to autocomplete with “What is the capital of Germany? Where is London?”</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751666205993_image.png" alt="Example output from pre (left) and post (right) training. The pretrained model doesn’t know how to autocomplete in the form of an assistant response."/></p>
<p>We could try to do Supervised Fine Tuning (SFT), but SFT will only get us so far because high quality SFT training data was not available for this kind of text in large quantities. SFT also seems to have a hard time generalizing outside of the data you fine-tune over.</p>
<p>RLHF takes advantage of the fact that it’s much easier to have human labelers look at existing model outputs and rank them.</p>
<h2 id="ppo-and-rlhf-overview">PPO and RLHF Overview</h2><p>To apply reinforcement learning (RL) to LLMs, we need to think about how LLM concepts map to RL. RL problems have an <strong>agent</strong> interacting in an <strong>environment</strong> based on a <strong>policy</strong>. A <strong>rollout</strong> is one example interaction in the environment. A rollout is composed of the <strong>actions</strong> the agent makes. The agent collects <strong>rewards</strong> for certain actions (good or bad).</p>
<p>For LLMs:</p>
<ul>
<li>The <strong>policy</strong>/<strong>agent</strong> is the LLM itself</li>
<li>A <strong>rollout</strong> is the response (tokens generated) from the LLM, where each token is an <strong>action</strong></li>
<li>There isn’t really an <strong>“environment”</strong> for LLMs, but you could maybe think of the conversation you&#39;re having as the environment</li>
</ul>
<p>To train the policy, we want to sample rollouts from the policy and figure out what actions give the best reward. The specific algorithm the InstructGPT paper used for this is called PPO (proximal policy optimization).</p>
<h2 id="training-complexities">Training complexities</h2><p>This setup sounds simple (i.e. can’t we just <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo></mrow></math>, backprop and call it a day?) but ends up with a lot of complexities.</p>
<p>To help with RLHF training, we will end up with three helper models (the value model, reward model, and reference model) that are used during training but aren’t the final result. We&#39;ll start with simple reward maximization and step through introducing why each of these additional models is needed.</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751665876873_image.png" alt="Only the policy model is final trained network. Blue models are static; yellow models can make parameter updates via backprop. This image taken from the DeepSeekMath paper."/></p>
<h2 id="training-the-reward-model">Training the reward model</h2><p>Before actually running the RL training loop above, we first need a way to decide what reward to give the LLM for its responses. This is called the Reward Model (RM).</p>
<p>Generally, the RM could be <em>any</em> arbitrary model that gives rewards for certain actions, i.e. you could imagine a massive lookup table for every possible sentence (exact solution method), but that lookup table would be massive and impossible to actually make. Instead, we can approximate a good reward model by using the pretrained LLM and fine-tuning it to do approximate reward prediction instead.</p>
<h2 id="training-the-reward-model-from-bradley-terry">Training the reward model from Bradley-Terry</h2><p>To train the reward model, the InstructGPT paper had labelers take a set of responses for a given prompt and rank them from best to worst. From there, they take each pair combination and use them as pairwise examples. They then train the reward model with the examples to take a (prompt, completion) pair as an input and output a scalar score (i.e. maybe 3.23 for a good output or -1.98 for a bad one).</p>
<p>See this <a href="#appendix-training-the-reward-model-using-bradley-terry">appendix section</a> for more details. tl;dr: the reward model spits out rewards (scalar value) for a given input prompt. Higher reward, better answer.</p>
<h2 id="value-function-and-gae">Value function and GAE</h2><p>Now that we have a way to calculate rewards, we could imagine doing our simple proposed training loop as above:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><msub><mi>d</mi><mtext>policy</mtext></msub><mo stretchy="false">)</mo></mrow></math></p>
<p>and we maximize this.</p>
<p>One big problem with this is that our model may give reward to suboptimal outputs that are still generally okay. For instance, if the prompt is “Who is the King of England”, “King Charles” is a better answer than “Queen Elizabeth” is a better answer than “potato”. However, the reward model will reward something like “Queen Elizabeth” much more than “potato” even though it’s still a really bad answer (model might say: “at least it’s an English monarch?”).</p>
<p>To mitigate this, we should only reward the model for when its outputs are “better than expected”. In PPO, this is done using Advantage.</p>
<h2 id="calculating-advantage">Calculating Advantage</h2><p>The high-level idea here is that we want to give the reward model a baseline to compare to. If our untrained model were to spit out “Queen Elizabeth” we’re really excited! But if we have a trained model we should expect that it should do better.</p>
<p>Advantage is somewhat simple: we take the reward and subtract out the baseline performance we’d expect. But how do we get the baseline? Enter: the value function - a copy of the reward model that gives us a baseline to compare to. At the start of RL training, it should be returning the same results as the RM. However, during training the VF weights are not frozen and can update alongside the policy. This is done so that our &#34;expectation&#34; for the policy&#39;s performance can get better as the policy gets better, i.e. we should only reward improvement.</p>
<p>The practical computation for advantage is pretty complicated; I’ve put the details in <a href="#appendix-computing-advantage-and-the-policy-gradient-for-ppo">another appendix section.</a></p>
<p>Okay, so with advantage, our objective function looks more like:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>a</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><msub><mi>e</mi><mtext>policy</mtext></msub><mo stretchy="false">)</mo></mrow></math></p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751665876873_image.png" alt="We can now account for the policy network and the value model portions of this diagram"/></p>
<p>We now only give the model rewards for outperforming itself!</p>
<h2 id="avoiding-reward-hacking">Avoiding reward hacking</h2><p>The next issue we’ll likely find is that this training loop gets really good at <a href="https://en.wikipedia.org/wiki/Reward_hacking">reward hacking</a>. Maybe the model realizes that writing “I figured out the answer!” or “asdh92h;;awep2iuh” as part of the response will give it a lot of reward.</p>
<p>Giving direct rewards like this is too unconstrained and the model will do whatever it takes to obtain reward.</p>
<p>Instead, we want to constrain the new policy to stay as close as possible to the baseline SFT’d model. For this, we use bring in the reference model, our final helper model in the diagram. The reference model is a copy of the policy model before training started, i.e. the SFT baseline.</p>
<p>We compute the KL divergence between the old policy and the new policy (i.e. on the logits before the softmax layer) and penalize it in our advantage:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>a</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><msub><mi>e</mi><mtext>policy</mtext></msub><mo>=</mo><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><msub><mi>d</mi><mtext>policy</mtext></msub><mo>−</mo><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><msub><mi>e</mi><mtext>baseline</mtext></msub><mo>−</mo><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>c</mi><msub><mi>y</mi><mtext>new</mtext></msub><mo>,</mo><mi>p</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>c</mi><msub><mi>y</mi><mtext>old</mtext></msub><mo stretchy="false">)</mo></mrow></math><p>If we take a look at the InstructGPT paper, we’re pretty much there!</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751747089710_image.png" alt=""/></p>
<p>Okay, so that&#39;s kind of terrible to read, but let&#39;s look at just the first line:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>objective</mtext><mo stretchy="false">(</mo><mi>ϕ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mi>~</mi><msub><mi>D</mi><mrow><msubsup><mi>π</mi><mi>ϕ</mi><mrow><mtext>RL</mtext></mrow></msubsup></mrow></msub></mrow></msub><mrow><mo stretchy="true" fence="true" form="prefix">[</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>β</mi><mi>log</mi><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><msubsup><mi>π</mi><mi>ϕ</mi><mrow><mtext>RL</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msup><mi>π</mi><mrow><mtext>SFT</mtext></mrow></msup><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="true" fence="true" form="postfix">)</mo></mrow><mo stretchy="true" fence="true" form="postfix">]</mo></mrow></mrow></math><p>The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>E</mi></mrow></math> at the start means &#34;the expected reward given this policy, input, and outputs&#34;. Expected reward for what? For the reward (from our advantage calculation) minus the KL penalty as described above:</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mtext>objective</mtext><mo stretchy="false">(</mo><mi>ϕ</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>expected value of</mtext><mrow><mo stretchy="true" fence="true" form="prefix">[</mo><munder><munder><mrow><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>⏟</mo></munder><mrow><mtext>advantage based reward</mtext></mrow></munder><mo>−</mo><mi>β</mi><mi>log</mi><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><msubsup><mi>π</mi><mi>ϕ</mi><mrow><mtext>RL</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><msup><mi>π</mi><mrow><mtext>SFT</mtext></mrow></msup><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="true" fence="true" form="postfix">)</mo></mrow><mo stretchy="true" fence="true" form="postfix">]</mo></mrow></mrow></math><h2 id="fighting-the-alignment-tax">Fighting the alignment tax</h2><p>What&#39;s going on with the last term, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>γ</mi><msub><mi>E</mi><mrow><mi>x</mi><mi>~</mi><msub><mi>D</mi><mrow><mtext>pretrain</mtext></mrow></msub></mrow></msub><mrow><mo stretchy="true" fence="true" form="prefix">[</mo><mi>log</mi><mo stretchy="false">(</mo><msubsup><mi>π</mi><mi>ϕ</mi><mrow><mtext>RL</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="true" fence="true" form="postfix">]</mo></mrow></mrow></math>? The InstructGPT paper has this additional term to fight the “alignment tax”.</p>
<p>Running RL on the model <em>aligns</em> it to being able to respond more helpfully, harmlessly, and honestly, but this comes at a cost. When they first trained models with PPO, they found that while the new models got better at question answering, they got worse at being “internet autocompleters” the way the pretrained models were.</p>
<p>To fight this, they essentially mix some pretraining (literally, running the pretraining process) back into the RL process as they do PPO. Section C.4 of the paper has a tiny bit of detail:</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751747937826_image.png" alt=""/></p>
<p>In other words, during the backprop for PPO, they fight the RL process a little bit by also nudging the gradients back towards the distribution of outputs for the original pretrained model.</p>
<p>This feels vaguely crazy to me - total engineering hack just to get things to work? I am curious if this approach is still SOTA or we’ve figured out something better.</p>
<h2 id="putting-it-all-together">Putting it all together</h2><p>Going back to our original PPO diagram, we can now piece together what’s going on here.</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751665876873_image.png" alt="PPO training for LLMs, taken from the DeepSeekMath paper."/></p>
<p>For every training step, we evaluate the LLM (policy model). We take the input and output and ask the reward model to score it. We then penalize the reward using the KL divergence against the baseline SFT’d model. Finally, we want to only reward the model if it does better than our expectation (the value model) so we compute the Advantage. We train the models by maximizing Advantage and as we train, both the policy model and the value model update their weights.</p>
<p>Not pictured: we take our final <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math> term from this diagram and run it through PPO’s clipped objective function. For InstructGPT, we also tack on a term to mix in gradients from the pretrained model to fight the alignment tax.</p>
<p>At the end of training, we take the policy network and use it as our chat assistant!</p>
<h3 id="other-thoughts">Other thoughts</h3><p>Interestingly, my impression of PPO was that it and TRPO were developed to help us take multiple gradient steps for a single rollout. For LLMs, it seems like we still only take a single gradient step but the “trusted region” formulation helps with training.</p>
<p>I wonder why the InstructGPT paper doesn’t talk about the PPO clipped objective function much. I guess it’s an exercise left to the reader? The paper is already super long without touching on PPO or GAE.</p>
<p>The example outputs from the paper (Appendix F) are worth a skim. There’s a lot of cool bugs you can see that seem like they might be a direct result of some of the choices here.</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_2CE2C13DCD02F22C7196818D4C14D28271A5E97FCB052A34A619DB88A1F19D40_1751748913281_image.png" alt="Both pretrained and posttrained models can’t help themselves but give a last step for the recipe. Is this from the pretrain mix or the KL divergence from the SFT model perhaps? Or maybe just not enough alignment?"/></p>
<ul>
<li><a href="https://arxiv.org/abs/2203.02155">InstructGPT paper</a> with tons information about the original RLHF model</li>
<li>Original <a href="https://arxiv.org/abs/1706.03741">RLHF paper</a> that InstructGPT builds on</li>
<li><a href="https://arxiv.org/abs/2009.01325">RLHF paper on summarization</a> paper</li>
<li><a href="https://arxiv.org/abs/1506.02438">GAE paper</a> on advantage calculation (a lot of math in this one)</li>
<li><a href="https://rlhfbook.com/c/11-policy-gradients.html#reinforce">RLHF book</a> on policy gradients</li>
<li>These <a href="https://www.youtube.com/watch?v=SgC6AZss478&amp;list=PLs8w1Cdi-zvbUan-KlgwKN2CycndyKXXS&amp;index=6">Serrano Academy</a> videos on RL that are somewhat light on details but give a good gentle overview of the problem space</li>
<li>CS336 <a href="https://www.youtube.com/watch?v=Dfu7vC9jo4w">lectures 15</a> and <a href="https://www.youtube.com/watch?v=46f2QTDB08Q">16</a> that confused me so much I wrote these notes</li>
<li>Original <a href="https://arxiv.org/abs/1707.06347">PPO</a> and <a href="https://arxiv.org/abs/1502.05477">TRPO papers</a></li>
<li>While editing these notes for publication, I found these two <a href="https://cyrilzakka.github.io/posts/ppo-grpo">blog</a> <a href="https://cyrilzakka.github.io/posts/ppo-grpo">posts</a> that are pretty similar in content to this one that are pretty good</li>
</ul>
<h2 id="appendix-training-the-reward-model-using-bradley-terry">appendix: training the reward model using bradley-terry</h2><p>Let’s look quickly at how to use Bradley-Terry to train the reward model. We have a bunch of pairwise rankings that we want to convert to scores to use as rewards:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>R</mi><mi>M</mi><mo stretchy="false">(</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mo>,</mo><mi>r</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow></math></p>
<p>How do we train this model to output the right scores? As with everything else in ML, the general trick is to shape the problem as something we can optimize using gradient descent and a loss function. We’ll take our existing SFT-trained model and rip off the last linear layer (that outputs token predictions) and we’ll replace it with a linear layer that outputs a single scalar value.</p>
<p>We’ll look at how to formulate our loss next. To contextualize things, let’s bring back our “Who is the King of England?” prompt and think about the pair of outputs: “King Charles” and “Queen Elizabeth”. From our human feedback, we know that “King Charles” is preferred.</p>
<h3 id="bradley-terry-model">Bradley-Terry Model</h3><p><a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry</a> says that you can form a probability that one of the responses is better than the other given a certain score, i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>P</mi><mi>r</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>b</mi><mi>i</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false">(</mo><mi>i</mi><mo>&gt;</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac></mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>x</mi></mrow></math> is the strength score for the response.</p>
<p>You can also use an exponential parameterization of this equation:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo movablelimits="true">Pr</mo><mo stretchy="false">(</mo><mi>i</mi><mo>&gt;</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></mrow></math></p>
<p>This form is generally preferable for us because it’s nice to have scores always be positive and on a logarithmic scale. If we stare at this long enough (or ask Claude), we see that this formula can be rewritten as the sigmoid function between the two scores:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo movablelimits="true">Pr</mo><mo stretchy="false">(</mo><mi>i</mi><mo>&gt;</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow></msup><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></msup><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></msup><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></p>
<p>If we squint even harder, we can see that this is pretty much doing a softmax across the two scores.</p>
<p>Coming back to our example, if we had two scores for outputs of “King Charles” and “Queen Elizabeth” as 3 and 5, we can take the softmax over these scores to get the distribution <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mi>.12</mi><mo>,</mo><mi>.88</mi><mo stretchy="false">)</mo></mrow></math>, i.e. 12% chance we should output “King Charles” and 88% chance for Queen Elizabeth”.</p>
<p>But how do we get these scores and why are they wrong? Before training, this newly minted RM will spit out arbitrarily bad scores for whatever <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mo>,</mo><mi>r</mi><mi>e</mi><mi>s</mi><mi>p</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>e</mi><mo stretchy="false">)</mo></mrow></math> we show it.</p>
<p>Tying in cross-entropy loss
Now, we know from our human feedback that the expected probability should be <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math>, i.e. “King Charles” should be 100% preferred as an answer over “Queen Elizabeth”. We need to teach the model that instead of the scores 3 and 5, it should output scores that get us better probabilities so that “King Charles” always wins against “Queen Elizabeth”.</p>
<p>Seeing that we’ve now boiled are problem down to comparing two probability distributions, we can model the loss for this model as minimizing the cross-entropy loss between “the prediction the model makes about which response is better” to “the one-hot of which response we actually prefer”.</p>
<p>From here, we can set up a pretty standard training loop optimizing cross entropy loss; we toss in all of our pairs from human feedback and use this as our reward model.</p>
<h2 id="appendix-computing-advantage-and-the-policy-gradient-for-ppo">appendix: computing advantage and the policy gradient for ppo</h2><p>Advantage is computed through <strong>Generalized Advantage Estimation</strong>, which has quite a bit of nuance. If we break the term meaning down,</p>
<ul>
<li>Generalized: how to calculate advantage over multiple actions (we’ll go over this after)</li>
<li>Advantage: how much better is this policy over the baseline policy?</li>
<li>Estimation: we approximated using the baseline using lossy neural nets</li>
</ul>
<p>You can think of advantage as computed as the reward for the current token minus the baseline:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><mi>e</mi><mo>=</mo><mi>R</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi><mo>−</mo><mi>B</mi><mi>a</mi><mi>s</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi></mrow></math></p>
<h3 id="generalizing-advantage">Generalizing advantage</h3><p>Unfortunately, the actual math is not so simple. We need to give some scheme for the reward to be credited partially to all of the tokens in the response, so we have a more complicated formula:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><mi>e</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mi>γ</mi><mi>λ</mi><mo stretchy="true" fence="true" form="postfix">)</mo></mrow><mi>t</mi></msup><msub><mi>δ</mi><mi>t</mi></msub></mrow></math></p>
<p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>δ</mi><mi>t</mi></msub></mrow></math> (the TD error, or temporal difference error) is:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>δ</mi><mi>t</mi></msub><mo>=</mo><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></math></p>
<p>This is a bit of a mess. Some terms:</p>
<ul>
<li><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>γ</mi></mrow></math> controls reward decay as in Bellman’s equation, i.e. how much to prefer rewards now vs. later</li>
<li><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>λ</mi></mrow></math> controls bias-variance tradeoff. For instance, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>λ</mi><mo>=</mo><mn>0</mn></mrow></math> would ignore future reward for a given token (less variance) and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mo>&gt;</mo><mi>λ</mi><mo>&gt;</mo><mn>0</mn></mrow></math> gives future tokes some weight</li>
<li><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>t</mi></mrow></math> is the token index in the response and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>T</mi></mrow></math> is the length of the response</li>
</ul>
<p>Together, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>γ</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>λ</mi></mrow></math> control how much to spread the reward across token future tokens. These are usually set to rough 1 (maybe <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>γ</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>λ</mi><mo>=</mo><mi>.95</mi></mrow></math>) so you can mostly ignore them for your intuition&#39;s sake.</p>
<p>Zooming out a bit, we’re trying to compute the Advantage at a per-token level, spreading the reward across the tokens in the response.</p>
<h3 id="advantage-in-pseudocode">Advantage in pseudocode</h3><p>Looking at this math is hard, so let’s write it out as code instead:</p>
<div><pre><span></span><span>import</span><span> </span><span>torch</span>

<span>input_and_response</span> <span>=</span> <span># some input; note that </span>
<span>rewards</span> <span>=</span> <span>reward_model</span><span>(</span><span>input_and_response</span><span>)</span> <span># ex. [0, 0, 0, 2.5], reward on last token</span>
<span>values</span> <span>=</span> <span>value_model</span><span>(</span><span>input_and_response</span><span>)</span> <span># ex. [.5, .2, .1, .3, 0], value at any token</span>
<span>gamma</span> <span>=</span> <span>0.99</span>    <span># discount factor</span>
<span>lambda</span> <span>=</span> <span>0.95</span>   <span># GAE parameter</span>

<span># Calculate TD error for each token</span>
<span>td_errors</span> <span>=</span> <span>rewards</span> <span>+</span> <span>gamma</span> <span>*</span> <span>values</span><span>[</span><span>1</span><span>:]</span> <span>-</span> <span>values</span><span>[:</span><span>-</span><span>1</span><span>]</span>
<span># δ_0 = 0.1 + 0.99*0.6 - 0.5 = 0.194</span>
<span># δ_1 = 0.2 + 0.99*0.7 - 0.6 = 0.293  </span>
<span># δ_2 = 0.3 + 0.99*0.8 - 0.7 = 0.392</span>
<span># δ_3 = 1.0 + 0.99*0.0 - 0.8 = 0.2</span>

<span># Calculate advantage for each token</span>
<span>advantages</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>rewards</span><span>)</span>
<span>gae</span> <span>=</span> <span>0</span>

<span># Work backwards because each token&#39;s AE depends on all subsequent tokens as well</span>
<span>for</span> <span>t</span> <span>in</span> <span>reversed</span><span>(</span><span>range</span><span>(</span><span>len</span><span>(</span><span>rewards</span><span>))):</span>
    <span>gae</span> <span>=</span> <span>td_errors</span><span>[</span><span>t</span><span>]</span> <span>+</span> <span>gamma</span> <span>*</span> <span>lam</span> <span>*</span> <span>gae</span>
    <span>advantages</span><span>[</span><span>t</span><span>]</span> <span>=</span> <span>gae</span>

<span># A_3 = δ_3 = 0.2</span>
<span># A_2 = δ_2 + γλ*A_3 = 0.392 + 0.99*0.95*0.2 = 0.580</span>
<span># A_1 = δ_1 + γλ*A_2 = 0.293 + 0.99*0.95*0.580 = 0.838</span>
<span># A_0 = δ_0 + γλ*A_1 = 0.194 + 0.99*0.95*0.838 = 0.975</span>
</pre></div>
<p>From here, we now have our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow></math> term to use for each token generated.</p>
<p>Writing out the gradient for the policy
As a last wrinkle, to have a proper gradient to learn from, we need to multiply advantage term by the probability of each generated token (measured by the pre-softmax log probability of the token) to figure out if we should be upweighting or downweighting the advantage:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>A</mi><mo>*</mo><mi>P</mi><mi>r</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>b</mi><mi>i</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false">(</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">)</mo></mrow></math></p>
<h3 id="why-multiply-by-the-probability">Why multiply by the probability?</h3><p>This part confused me a for a bit - if we’re just trying to maximize reward, aren’t we essentially doing <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>A</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><mi>e</mi><mo stretchy="false">)</mo></mrow></math>? Why do we need to multiply by anything?</p>
<p>The main problem here is that our computation of Advantage doesn’t involve the policy at all. Looking at the code above, Advantage is computed from the RM and VF. If we think in terms of the autograd engine, there is no relationship in the computation graph between the parameters of the policy and the objective function formulated. By instead doing <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mi>A</mi><mo>*</mo><mi>P</mi><mi>r</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>b</mi><mi>i</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false">(</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">)</mo></mrow></math>, there is a way to backprop the policy in terms of the advantage.</p>
<p>This is not mathematically how the problem is typically formulated, but I found this reasoning to make the most sense to me.</p>
<h3 id="correcting-for-mini-batches-in-ppo">Correcting for mini-batches in PPO</h3><p>Basic REINFORCE-style RL algorithms can&#39;t do minibatches, i.e. in a single gradient step, you can only do backprop over one batch of actions at a time. Otherwise, the gradient you compute “goes stale” because it’s the gradient relative to the old policy.</p>
<p>PPO uses a ratio of the policies (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><msub><mi>π</mi><mrow><msub><mi>θ</mi><mtext>new</mtext></msub></mrow></msub></mrow><mrow><msub><mi>π</mi><mrow><msub><mi>θ</mi><mtext>old</mtext></msub></mrow></msub></mrow></mfrac></mrow></math>) instead of just the probability (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>π</mi><mrow><msub><mi>θ</mi><mtext>new</mtext></msub></mrow></msub></mrow></math>) of the current policy to correct for this when doing mini-batches.</p>
<p>This ratio is called the importance sampling ratio - It’s not clear to me why the math for this works (“importance sampling theory?&#34;) but we can at least build some intuitive sense for what it’s doing.</p>
<p>With this in mind, our advantaged PPO objective looks like:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><msub><mi>e</mi><mtext>per token</mtext></msub><mo>=</mo><mi>A</mi><mi>d</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>g</mi><mi>e</mi><mo>*</mo><mfrac><mrow><msub><mi>π</mi><mrow><msub><mi>θ</mi><mtext>new</mtext></msub></mrow></msub></mrow><mrow><msub><mi>π</mi><mrow><msub><mi>θ</mi><mtext>old</mtext></msub></mrow></msub></mrow></mfrac></mrow></math></p>
<p>When the new policy prefers (has a higher probability for) the output, the ratio is &gt; 1 and the ratio will boost gradient. When the new policy doesn’t prefer the output, the ratio is &lt; 1 and the ratio will  diminish the gradient. If the new and old policy have the same preference for the output, we neither boost nor diminish the gradient.</p>
<h3 id="applying-ppos-clipped-objective-to-the-ratio">Applying PPO’s clipped objective to the ratio</h3><p>The last bit of nuance here is that PPO uses a clipped objective function, i.e. when we compute our advantaged reward, we don’t want it to drastically change the update. PPO takes the policy ratio from above and &#34;clips&#34; it:</p>
<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><msub><mi>e</mi><mtext>per token</mtext></msub><mo>=</mo><msup><mi>L</mi><mrow><mi>C</mi><mi>L</mi><mi>I</mi><mi>P</mi></mrow></msup><mo>=</mo><mo>min</mo><mo stretchy="false">(</mo><mi>r</mi><mi>·</mi><mi>A</mi><mo>,</mo><mtext>clip</mtext><mo stretchy="false">(</mo><mi>r</mi><mo>,</mo><mn>1</mn><mo>−</mo><mi>ε</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo><mi>·</mi><mi>A</mi><mo stretchy="false">)</mo></mrow></math></p>
<p>This looks complicated but is essentially saying that we take the importance sampling ratio from above and bound it to be somewhere around 1. A reasonable value for the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>ϵ</mi></mrow></math> hyperparameter might <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>.2</mi></mrow></math>, i.e. the ratio can only be between <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>0.8</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1.2</mn></mrow></math>.</p>
<p>From the original <a href="https://arxiv.org/abs/2009.01325">Learning to summarize from human feedback</a> paper, the KL term also helps the model explore during training by introducing some variation.</p>



    

    
        
            <p>
                
                    <a href="https://projectionlab.com/blog/?q=article">#article</a>
                
            </p>
        

        
            


        
    


  </div></div>
  </body>
</html>
