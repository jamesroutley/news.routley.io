<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/">Original</a>
    <h1>Amazon Nova</h1>
    
    <div id="readability-page-1" class="page"><section property="articleBody"> 
       <div id="amazon-polly-audio-tab"> 
            
            
            
           <p><a href="https://aws.amazon.com/polly/" target="_blank" rel="noopener noreferrer"><img src="https://a0.awsstatic.com/aws-blog/images/Voiced_by_Amazon_Polly_EN.png" alt="Voiced by Polly" width="554" height="56"/></a> 
           </p> </div> 
       <p>Today, we’re thrilled to announce <a href="http://aws.amazon.com/nova">Amazon Nova</a>, a new generation of state-of-the-art <a href="https://aws.amazon.com/what-is/foundation-models/">foundation models (FMs)</a> that deliver frontier intelligence and industry leading price performance, available exclusively in <a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a>.</p> 
       <p>You can use Amazon Nova to lower costs and latency for almost any <a href="https://aws.amazon.com/ai/generative-ai/">generative AI</a> task. You can build on Amazon Nova to analyze complex documents and videos, understand charts and diagrams, generate engaging video content, and build sophisticated AI agents, from across a range of intelligence classes optimized for enterprise workloads.</p> 
       <p>Whether you’re developing document processing applications that need to process images and text, creating marketing content at scale, or building AI assistants that can understand and act on visual information, Amazon Nova provides the intelligence and flexibility you need with two categories of models: understanding and creative content generation.</p> 
       <p>Amazon Nova understanding models accept text, image, or video inputs to generate text output. Amazon creative content generation models accept text and image inputs to generate image or video output.</p> 
       <p><span><strong>Understanding models: Text and visual intelligence<br/> </strong></span>The Amazon Nova models include three understanding models (with a fourth one coming soon) designed to meet different needs:</p> 
       <p><strong>Amazon Nova Micro</strong> – A text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat and brainstorming, and simple mathematical reasoning and coding. Amazon Nova Micro also supports customization on proprietary data using fine-tuning and model distillation to boost accuracy.</p> 
       <p><strong>Amazon Nova Lite</strong> – A very low-cost multimodal model that is lightning fast for processing image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy. The model processes inputs up to 300K tokens in length and can analyze multiple images or up to 30 minutes of video in a single request. Amazon Nova Lite also supports text and multimodal fine-tuning and can be optimized to deliver the best quality and costs for your use case with techniques such as model distillation.</p> 
       <p><strong>Amazon Nova Pro</strong> – A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Pro is capable of processing up to 300K input tokens and sets new standards in multimodal intelligence and agentic workflows that require calling APIs and tools to complete complex workflows. It achieves state-of-the-art performance on key benchmarks including visual question answering (<a href="https://arxiv.org/abs/1904.08920">TextVQA</a>) and video understanding (<a href="https://arxiv.org/abs/1904.03493">VATEX</a>). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and excels at analyzing financial documents. With an input context of 300K tokens, it can process code bases with over fifteen thousand lines of code. Amazon Nova Pro also serves as a teacher model to distill custom variants of Amazon Nova Micro and Lite.</p> 
       <p><strong>Amazon Nova Premier</strong> – Our most capable multimodal model for complex reasoning tasks and for use as the best teacher for distilling custom models. Amazon Nova Premier is still in training. We’re targeting availability in early 2025.</p> 
       <p>Amazon Nova understanding models excel in <a href="https://aws.amazon.com/what-is/retrieval-augmented-generation/">Retrieval-Augmented Generation (RAG)</a>, function calling, and agentic applications. This is reflected in Amazon Nova model scores in the <a href="https://arxiv.org/abs/2406.04744">Comprehensive RAG Benchmark (CRAG)</a> evaluation, <a href="https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html">Berkeley Function Calling Leaderboard (BFCL)</a>, <a href="https://arxiv.org/abs/2404.05955">VisualWebBench</a>, and <a href="https://arxiv.org/abs/2306.06070">Mind2Web</a>.</p> 
       <p>What makes Amazon Nova particularly powerful for enterprises is its customization capabilities. Think of it as tailoring a suit: you start with a high-quality foundation and adjust it to fit your exact needs. You can fine-tune the models with text, image, and video to understand your industry’s terminology, align with your brand voice, and optimize for your specific use cases. For instance, a legal firm might customize Amazon Nova to better understand legal terminology and document structures.</p> 
       <p>You can see the latest benchmark scores for these models on the <a href="http://aws.amazon.com/nova">Amazon Nova product page</a>.</p> 
       <p><span><strong>Creative content generation: Bringing concepts to life<br/> </strong></span>The Amazon Nova models also include two creative content generation models:</p> 
       <p><strong>Amazon Nova Canvas</strong> – A state-of-the-art image generation model producing studio-quality images with precise control over style and content, including rich editing features such as inpainting, outpainting, and background removal. Amazon Nova Canvas excels on human evaluations and key benchmarks such as <a href="https://arxiv.org/abs/2303.11897#:~:text=We%20introduce%20TIFA%20(Text%2Dto,visual%20question%20answering%20(VQA).">text-to-image faithfulness evaluation with question answering (TIFA)</a> and <a href="https://arxiv.org/abs/2304.05977">ImageReward</a>.</p> 
       <p><strong>Amazon Nova Reel</strong> – A state-of-the-art video generation model. Using Amazon Nova Reel, you can produce short videos through text prompts and images, control visual style and pacing, and generate professional-quality video content for marketing, advertising, and entertainment. Amazon Nova Reel outperforms existing models on human evaluations of video quality and video consistency.</p> 
       <p>All Amazon Nova models include built-in safety controls and creative content generation models include watermarking capabilities to promote responsible AI use.</p> 
       <p>Let’s see how these models work in practice for a few use cases.</p> 
       <p><span><strong>Using Amazon Nova Pro for document analysis<br/> </strong></span>To demonstrate the capabilities of document analysis, I downloaded the <a href="https://docs.aws.amazon.com/decision-guides/latest/generative-ai-on-aws-how-to-choose/guide.html">Choosing a generative AI service</a> decision guide in PDF format from the AWS documentation.</p> 
       <p>First, I choose <strong>Model access</strong> in the <a href="https://console.aws.amazon.com/bedrock">Amazon Bedrock console</a> navigation pane and request access to the new <strong>Amazon Nova</strong> models. Then, I choose <strong>Chat/text</strong> in the <strong>Playground</strong> section of the navigation pane and select the <strong>Amazon Nova Pro</strong> model. In the chat, I upload the decision guide PDF and ask:</p> 
       <p><code>Write a summary of this doc in 100 words. Then, build a decision tree.</code></p> 
       <p>The output follows my instructions producing a structured decision tree that gives me a glimpse of the document before reading it.</p> 
       <p><a href="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/summary-decision-tree.png"><img src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/summary-decision-tree.png" alt="Console screenshot." width="1726" height="1250"/></a></p> 
       <p><span><strong>Using Amazon Nova Pro for video analysis<br/> </strong></span>To demonstrate video analysis, I prepared a video by joining two short clips (more on this in the next section):</p> 
        
       <p>This time, I use the <a href="https://aws.amazon.com/sdk-for-python/">AWS SDK for Python (Boto3)</a> to invoke the Amazon Nova Pro model using the <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html">Amazon Bedrock Converse API</a> and analyze the video:</p> 
       <pre><code>import boto3

AWS_REGION = &#34;us-east-1&#34;
MODEL_ID = &#34;amazon.nova-pro-v1:0&#34;
VIDEO_FILE = &#34;the-sea.mp4&#34;

bedrock_runtime = boto3.client(&#34;bedrock-runtime&#34;, region_name=AWS_REGION)
with open(VIDEO_FILE, &#34;rb&#34;) as f:
    video = f.read()

user_message = &#34;Describe this video.&#34;

messages = [ { &#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [
    {&#34;video&#34;: {&#34;format&#34;: &#34;mp4&#34;, &#34;source&#34;: {&#34;bytes&#34;: video}}},
    {&#34;text&#34;: user_message}
] } ]

response = bedrock_runtime.converse(
    modelId=MODEL_ID,
    messages=messages,
    inferenceConfig={&#34;temperature&#34;: 0.0}
 )

response_text = response[&#34;output&#34;][&#34;message&#34;][&#34;content&#34;][0][&#34;text&#34;]
print(response_text)</code></pre> 
       <p>Amazon Nova Pro can analyze videos that are uploaded with the API (as in the previous code) or that are stored in an <a href="https://aws.amazon.com/s3/">Amazon Simple Storage Service (Amazon S3)</a> bucket.</p> 
       <p>In the script, I ask to describe the video. I run the script from the command line. Here’s the result:</p> 
       <p><code>The video begins with a view of a rocky shore on the ocean, and then transitions to a close-up of a large seashell resting on a sandy beach.</code></p> 
       <p>I can use a more detailed prompt to extract specific information from the video such as objects or text. Note that Amazon Nova currently does not process audio in a video.</p> 
       <p><span><strong>Using Amazon Nova for video creation<br/> </strong></span>Now, let’s create a video using Amazon Nova Reel, starting from a text-only prompt and then providing a reference image.</p> 
       <p>Because generating a video takes a few minutes, the Amazon Bedrock API introduced three new operations:</p> 
       <p><strong>StartAsyncInvoke</strong> – To start an asynchronous invocation</p> 
       <p><strong>GetAsyncInvoke</strong> – To get the current status of a specific asynchronous invocation</p> 
       <p><strong>ListAsyncInvokes</strong> – To list the status of all asynchronous invocations with optional filters such as status or date</p> 
       <p>Amazon Nova Reel supports camera control actions such as zooming or moving the camera. This Python script creates a video from this text prompt:</p> 
       <p><code>Closeup of a large seashell in the sand. Gentle waves flow all around the shell. Sunset light. Camera zoom in very close.</code></p> 
       <p>After the first invocation, the script periodically checks the status until the creation of the video has been completed. I pass a random seed to get a different result each time the code runs.</p> 
       <pre><code>import random
import time

import boto3

AWS_REGION = &#34;us-east-1&#34;
MODEL_ID = &#34;amazon.nova-reel-v1:0&#34;
SLEEP_TIME = 30
S3_DESTINATION_BUCKET = &#34;&lt;BUCKET&gt;&#34;

video_prompt = &#34;Closeup of a large seashell in the sand. Gentle waves flow all around the shell. Sunset light. Camera zoom in very close.&#34;

bedrock_runtime = boto3.client(&#34;bedrock-runtime&#34;, region_name=AWS_REGION)
model_input = {
    &#34;taskType&#34;: &#34;TEXT_VIDEO&#34;,
    &#34;textToVideoParams&#34;: {&#34;text&#34;: video_prompt},
    &#34;videoGenerationConfig&#34;: {
        &#34;durationSeconds&#34;: 6,
        &#34;fps&#34;: 24,
        &#34;dimension&#34;: &#34;1280x720&#34;,
        &#34;seed&#34;: random.randint(0, 2147483648)
    }
}

invocation = bedrock_runtime.start_async_invoke(
    modelId=MODEL_ID,
    modelInput=model_input,
    outputDataConfig={&#34;s3OutputDataConfig&#34;: {&#34;s3Uri&#34;: f&#34;s3://{S3_DESTINATION_BUCKET}&#34;}}
)

invocation_arn = invocation[&#34;invocationArn&#34;]
s3_prefix = invocation_arn.split(&#39;/&#39;)[-1]
s3_location = f&#34;s3://{S3_DESTINATION_BUCKET}/{s3_prefix}&#34;
print(f&#34;\nS3 URI: {s3_location}&#34;)

while True:
    response = bedrock_runtime.get_async_invoke(
        invocationArn=invocation_arn
    )
    status = response[&#34;status&#34;]
    print(f&#34;Status: {status}&#34;)
    if status != &#34;InProgress&#34;:
        break
    time.sleep(SLEEP_TIME)

if status == &#34;Completed&#34;:
    print(f&#34;\nVideo is ready at {s3_location}/output.mp4&#34;)
else:
    print(f&#34;\nVideo generation status: {status}&#34;)</code></pre> 
       <p>I run the script:</p> 
       <pre><code>Status: InProgress
. . .
Status: Completed

Video is ready at s3://BUCKET/PREFIX/output.mp4</code></pre> 
       <p>After a few minutes, the script completes and prints the output <a href="https://aws.amazon.com/s3/">Amazon Simple Storage Service (Amazon S3)</a> location. I download the output video using the <a href="https://aws.amazon.com/cli/">AWS Command Line Interface (AWS CLI)</a>:</p> 
       <div> 
        <pre><code>aws s3 cp s3://BUCKET/PREFIX/output.mp4 ./output-from-text.mp4</code></pre> 
       </div> 
       <p>This is the resulting video. As requested, the camera zooms in on the subject.</p> 
        
       <p><span><strong>Using Amazon Nova Reel with a reference image<br/> </strong></span>To have better control over the creation of the video, I can provide Amazon Nova Reel a reference image such as the following:</p> 
       <p><a href="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/20/nova-reel-seascape.png"><img loading="lazy" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/20/nova-reel-seascape.png" alt="A seascape image." width="1280" height="720"/></a></p> 
       <p>This script uses the reference image and a text prompt with a camera action (<code>drone view flying over a coastal landscape</code>) to create a video:</p> 
       <pre><code>import base64
import random
import time

import boto3

S3_DESTINATION_BUCKET = &#34;&lt;BUCKET&gt;&#34;
AWS_REGION = &#34;us-east-1&#34;
MODEL_ID = &#34;amazon.nova-reel-v1:0&#34;
SLEEP_TIME = 30
input_image_path = &#34;seascape.png&#34;
video_prompt = &#34;drone view flying over a coastal landscape&#34;

bedrock_runtime = boto3.client(&#34;bedrock-runtime&#34;, region_name=AWS_REGION)

# Load the input image as a Base64 string.
with open(input_image_path, &#34;rb&#34;) as f:
    input_image_bytes = f.read()
    input_image_base64 = base64.b64encode(input_image_bytes).decode(&#34;utf-8&#34;)

model_input = {
    &#34;taskType&#34;: &#34;TEXT_VIDEO&#34;,
    &#34;textToVideoParams&#34;: {
        &#34;text&#34;: video_prompt,
        &#34;images&#34;: [{ &#34;format&#34;: &#34;png&#34;, &#34;source&#34;: { &#34;bytes&#34;: input_image_base64 } }]
        },
    &#34;videoGenerationConfig&#34;: {
        &#34;durationSeconds&#34;: 6,
        &#34;fps&#34;: 24,
        &#34;dimension&#34;: &#34;1280x720&#34;,
        &#34;seed&#34;: random.randint(0, 2147483648)
    }
}

invocation = bedrock_runtime.start_async_invoke(
    modelId=MODEL_ID,
    modelInput=model_input,
    outputDataConfig={&#34;s3OutputDataConfig&#34;: {&#34;s3Uri&#34;: f&#34;s3://{S3_DESTINATION_BUCKET}&#34;}}
)

invocation_arn = invocation[&#34;invocationArn&#34;]
s3_prefix = invocation_arn.split(&#39;/&#39;)[-1]
s3_location = f&#34;s3://{S3_DESTINATION_BUCKET}/{s3_prefix}&#34;

print(f&#34;\nS3 URI: {s3_location}&#34;)

while True:
    response = bedrock_runtime.get_async_invoke(
        invocationArn=invocation_arn
    )
    status = response[&#34;status&#34;]
    print(f&#34;Status: {status}&#34;)
    if status != &#34;InProgress&#34;:
        break
    time.sleep(SLEEP_TIME)
if status == &#34;Completed&#34;:
    print(f&#34;\nVideo is ready at {s3_location}/output.mp4&#34;)
else:
    print(f&#34;\nVideo generation status: {status}&#34;)</code></pre> 
       <p>Again, I download the output using the AWS CLI:</p> 
       <div> 
        <pre><code>aws s3 cp s3://BUCKET/PREFIX/output.mp4 ./output-from-image.mp4</code></pre> 
       </div> 
       <p>This is the resulting video. The camera starts from the reference image and moves forward.</p> 
        
       <p><span><strong>Building AI responsibly<br/> </strong></span>Amazon Nova models are built with a focus on customer safety, security, and trust throughout the model development stages, offering you peace of mind as well as an adequate level of control to enable your unique use cases.</p> 
       <p>We’ve built in comprehensive safety features and content moderation capabilities, giving you the controls you need to use AI responsibly. Every generated image and video include digital watermarking.</p> 
       <p>The Amazon Nova foundation models are built with protections that match its increased capabilities. Amazon Nova extends our safety measures to combat the spread of misinformation, child sexual abuse material (CSAM), and chemical, biological, radiological, or nuclear (CBRN) risks.</p> 
       <p><span><strong>Things to know<br/> </strong></span><a href="http://aws.amazon.com/nova">Amazon Nova</a> models are available in <a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a> in the US East (N. Virginia) <a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/">AWS region</a>. Amazon Nova Micro, Lite, and Pro are also available in the US West (Oregon), and US East (Ohio) regions via <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html">cross-Region inference</a>. As usual with <a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a>, the pricing follows a pay-as-you-go model. For more information, see <a href="https://aws.amazon.com/bedrock/pricing/">Amazon Bedrock pricing</a>.</p> 
       <p>The new generation of Amazon Nova understanding models speaks your language. These models understand and generate content in over 200 languages, with particularly strong capabilities in English, German, Spanish, French, Italian, Japanese, Korean, Arabic, Simplified Chinese, Russian, Hindi, Portuguese, Dutch, Turkish, and Hebrew. This means you can build truly global applications without worrying about language barriers or maintaining separate models for different regions. Amazon Nova models for creative content generation support English prompts.</p> 
       <p>As you explore Amazon Nova, you’ll discover its ability to handle increasingly complex tasks. You can use these models to process lengthy documents up to 300K tokens, analyze multiple images in a single request, understand up to 30 minutes of video content, and generate images and videos at scale from natural language. This makes these models suitable for a variety of business use cases, from quick customer service interactions to deep analysis of corporate documentation and asset creation for advertising, ecommerce, and social media applications.</p> 
       <p>Integration with Amazon Bedrock makes deployment and scaling straightforward. You can leverage features like <a href="https://aws.amazon.com/bedrock/knowledge-bases/">Amazon Bedrock Knowledge Bases</a> to enhance your model with proprietary information, use <a href="https://aws.amazon.com/bedrock/agents/">Amazon Bedrock Agents</a> to automate complex workflows, and implement <a href="https://aws.amazon.com/bedrock/guardrails/">Amazon Bedrock Guardrails</a> to promote responsible AI use. The platform supports real-time streaming for interactive applications, batch processing for high-volume workloads, and detailed monitoring to help you optimize performance.</p> 
       <p>Ready to start building with Amazon Nova? Give the new models a try in the <a href="https://console.aws.amazon.com/bedrock">Amazon Bedrock console</a> today, visit the Amazon Nova models section of the <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html">Amazon Bedrock documentation</a>, and send feedback to <a href="https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock">AWS re:Post for Amazon Bedrock</a>. You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at <a href="https://community.aws/">community.aws</a>. Let us know what you build with these new models!</p> 
       <p>— <a href="https://twitter.com/danilop">Danilo</a></p> 
       <!-- '"` --> 
      </section></div>
  </body>
</html>
