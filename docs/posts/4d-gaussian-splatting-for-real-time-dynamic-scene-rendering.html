<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://guanjunwu.github.io/4dgs/">Original</a>
    <h1>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h1>
    
    <div id="readability-page-1" class="page">

<nav role="navigation" aria-label="main navigation">
  
  
</nav>


<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <div>
            <p><span><sup>1</sup>Huazhong University of Science and Technology</span>
            <span><sup>2</sup>Huawei Inc.</span></p>
            <p><span><sup>*</sup>Equal Contributions.</span>
            <span><sup>+</sup>Corresponding Authors.</span>

          </p></div>

          
        </div>
      </div>
    </div>
  </div>
</section>
<h2>
  <span>4D-GS</span> can learn a dynamic scene within 20 minutes and reach the rendering speed over 50 fps of the view on one RTX 3080 GPU.
</h2>

<section>
  
</section>
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800*800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods.          </p>
      </div>

    </div>

    <p><img src="https://guanjunwu.github.io/4dgs/static/images/teaserfig_00.jpg" alt="Interpolation end reference image."/></p><p>
      Our method achieves real-time rendering for dynamic scenes at high image resolutions while maintaining high rendering quality. The right figure is mainly tested on synthetic datasets, where the radius of the dot corresponds to the training time. &#34;Res&#34;: resolution.
    </p>
    <p><img src="https://guanjunwu.github.io/4dgs/static/images/pipeline_00.bmp" alt="Interpolation end reference image."/></p><p>
      The overall pipeline of our model. Given a group of 3D Gaussians S, we extract the center of each 3D Gaussian X and timestamp t to compute the voxel feature by multi-resolution voxel planes. Then a tiny MLP is used to decode the feature and get S` of each Gaussian at timestamp t.    </p>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>



<section>
  
</section>







<section>
 
<section>
  

  
</section>
  <div>

    <h2>HyperNeRF Datasets</h2>

    

    <h2>Fixed-View Rendering</h2>

    
    <!--/ Matting. -->

    <!-- Animation. -->
    <h2>Free-View Rendering</h2>

    

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div> -->
        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!-- / Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>  -->
        <!--/ Re-rendering. -->

      <!-- </div> -->
    <!-- </div> -->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    / Concurrent Work. -->

  <!-- </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>   -->
<section id="BibTeX">
  <div>
    <h2>Acknowledgement</h2>
    <p>
      We would like to express our sincere gratitude to <a href="https://github.com/zhouzhenghong-gt">Zhenghong Zhou</a> for his revisions to our code and discussions on the content of our paper.
    </p>
  </div>
</section>
<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <pre><code>@article{wu20234dgaussians,
      title={4D Gaussian Splatting for Real-Time Dynamic Scene Rendering},
      author={Wu, Guanjun and Yi, Taoran and Fang, Jiemin and Xie, Lingxi and Zhang, Xiaopeng and Wei Wei and Liu, Wenyu and Tian, Qi and Wang Xinggang},
      journal={arXiv preprint arXiv:2310.08528},
      year={2023}
    }</code></pre>
  </div>
</section>




</div></section></div>
  </body>
</html>
