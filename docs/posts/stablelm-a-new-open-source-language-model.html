<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models">Original</a>
    <h1>StableLM: A new open-source language model</h1>
    
    <div id="readability-page-1" class="page"><div data-block-type="2" id="block-fe952638833e0fcc94c4"><div>

<p>Today, Stability AI released a new open-source language model, <a href="https://github.com/stability-AI/stableLM/" target="_blank"><span>StableLM</span></a>. The Alpha version of the model is available in 3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow. Developers can freely inspect, use, and adapt our StableLM base models for commercial or research purposes, subject to the terms of the CC BY-SA-4.0 license.</p><p>In 2022, Stability AI drove the public release of <a href="https://github.com/Stability-AI/stablediffusion/" target="_blank"><span>Stable Diffusion</span></a>, a revolutionary image model that represents a transparent, open, and scalable alternative to proprietary AI. With the launch of the StableLM suite of models, Stability AI is continuing to make foundational AI technology accessible to all. Our StableLM models can generate text and code and will power a range of downstream applications. They demonstrate how small and efficient models can deliver high performance with appropriate training. </p><p>The release of StableLM builds on our experience in open-sourcing earlier language models with <a href="https://www.eleuther.ai/" target="_blank"><span>EleutherAI</span></a>, a nonprofit research hub. These language models include GPT-J, GPT-NeoX, and the Pythia suite, which were trained on <a href="https://pile.eleuther.ai/" target="_blank"><span>The Pile</span></a> open-source dataset. Many recent open-source language models continue to build on these efforts, including <a href="https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/" target="_blank"><span>Cerebras-GPT</span></a> and <a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" target="_blank"><span>Dolly-2</span></a>.</p><p>StableLM is trained on a new experimental dataset built on The Pile, but three times larger with 1.5 trillion tokens of content. We will release details on the dataset in due course. The richness of this dataset gives StableLM surprisingly high performance in conversational and coding tasks, despite its small size of 3 to 7 billion parameters (by comparison, GPT-3 has 175 billion parameters).</p><p>We are also releasing a set of research models that are instruction fine-tuned. Initially, these fine-tuned models will use a combination of five recent open-source datasets for conversational agents: <a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank"><span>Alpaca</span></a>, <a href="https://github.com/nomic-ai/gpt4all" target="_blank"><span>GPT4All</span></a>, <a href="https://github.com/databrickslabs/dolly" target="_blank"><span>Dolly</span></a>, <a href="https://huggingface.co/datasets/RyokoAI/ShareGPT52K" target="_blank"><span>ShareGPT</span></a>, and <a href="https://github.com/anthropics/hh-rlhf" target="_blank"><span>HH</span></a>. These fine-tuned models are intended for research use only and are released under a noncommercial CC BY-NC-SA 4.0 license, in-line with Stanford’s Alpaca license.</p><p>Check out some examples below, produced by our 7 billion parameter fine-tuned model:</p>



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1681906970296_3187"><div>

<p>Language models will form the backbone of our digital economy, and we want everyone to have a voice in their design. Models like StableLM demonstrate our commitment to AI technology that is transparent, accessible, and supportive:</p><ul data-rte-list="default"><li><p><strong>Transparent. </strong>We open-source our models to promote transparency and foster trust. Researchers can “look under the hood” to verify performance, work on interpretability techniques, identify potential risks, and help develop safeguards. Organizations across the public and private sectors can adapt (“fine-tune”) these open-source models for their own applications without sharing their sensitive data or giving up control of their AI capabilities.</p></li><li><p><strong>Accessible. </strong>We design for the edge so that everyday users can run our models on local devices. Using these models, developers can build independent applications compatible with widely-available hardware instead of relying on proprietary services from one or two companies. In this way, the economic benefits of AI are shared by a broad community of users and developers. Open, fine-grained access to our models allows the broad research and academic community to develop interpretability and safety techniques beyond what is possible with closed models.</p></li></ul><ul data-rte-list="default"><li><p><strong>Supportive. </strong>We build models to support our users, not replace them. We are focused on efficient, specialized, and practical AI performance – not a quest for god-like intelligence. We develop tools that help everyday people and everyday firms use AI to unlock creativity, boost their productivity, and open up new economic opportunities.  </p></li></ul><p>The models are now available in our <a href="https://github.com/stability-AI/stableLM/" target="_blank"><span>GitHub</span></a> repository. We will publish a full technical report in the near future, and look forward to ongoing collaboration with developers and researchers as we roll out the StableLM suite. In addition, we will be kicking off our crowd-sourced RLHF program, and working with community efforts such as <a href="https://github.com/LAION-AI/Open-Assistant" target="_blank"><span>Open Assistant</span></a> to create an open-source dataset for AI assistants.</p>



</div></div></div>
  </body>
</html>
