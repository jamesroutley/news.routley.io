<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://evanhahn.com/utf-21/">Original</a>
    <h1>UTF-21, a toy character encoding</h1>
    
    <div id="readability-page-1" class="page"><div><p>In short: I created UTF-21, an impractical alternative to character encodings like UTF-8.</p><h2 id="quick-crash-course-character-encoding--unicode">Quick crash course: character encoding &amp; Unicode</h2><p>Before you can understand my horrible creation, you need to understand a little about Unicode. You can <a href="#introducing-utf-21">skip this</a> if you want.</p><h3 id="each-character-has-a-number">Each character has a number</h3><p>Character encoding is the process of converting characters to numbers and back, typically for digital storage and transmission.</p><p>Youâ€™ve probably heard of <a href="https://man7.org/linux/man-pages/man7/ascii.7.html">ASCII</a>, which maps 128 characters to numbers. For example, <code>W</code> is number 87 and number 36 is <code>$</code>.</p><p>As you might expect, there are more than 128 characters in the world. Characters like <code>Ã±</code> and <code>ğŸ¥º</code> canâ€™t be represented as ASCII.</p><p><a href="https://unicode.org">Unicode</a> is like ASCII, but instead of 128 characters, there are 1,114,111 characters. Way more! That lets us store characters like <code>Ã±</code> (character #241) and <code>ğŸ¥º</code> (character #129402). Itâ€™s a little more complex than this, but thatâ€™s the rough idea.</p><p>Here are a few examples from the big Unicode table:</p><table><thead><tr><th>Character</th><th>Unicode scalar</th></tr></thead><tbody><tr><td><code>F</code></td><td><code>70</code></td></tr><tr><td><code>Ã±</code></td><td><code>241</code></td></tr><tr><td><code>ğŸ¥º</code></td><td><code>129402</code></td></tr></tbody></table><p>(Note that some glyphs, like <code>ğŸ‘©ğŸ¾â€ğŸŒ¾</code>, are made up of <em>multiple</em> characters and therefore have multiple scalars. For more, see <a href="https://hsivonen.fi/string-length/">this post</a>.)</p><p>If you want to represent this full rangeâ€”0 to ~1.1 millionâ€”you need 21 bits of data. How do people store these bits?</p><h3 id="storing-the-numbers">Storing the numbers</h3><p><a href="https://www.unicode.org/standard/principles.html">Unicode has three official ways of storing these numbers</a>: UTF-8, UTF-16, and UTF-32.</p><p>I think UTF-32 is the simplest. Each number is put into a 32-bit integer, or 4 bytes. This is called a â€œfixed-widthâ€ encoding. Because you only need 21 bits of data, more than a third of the space is wasted, but itâ€™s simpler and faster for some operations.</p><p>In constrast, UTF-8 and UTF-16 are â€œvariable-widthâ€ encodings. UTF-16 tries to fit characters into a single 16-bit number, and if it canâ€™t, it expands to two. UTF-8 is conceptually similar, but it uses 8-bit numbers (bytes) as the smallest unit. (Fun fact: UTF-8 is a superset of ASCII.)</p><p>For example, for the character <code>F</code>, which has a scalar value of <code>70</code> (<code>46</code> in hex):</p><table><thead><tr><th>Encoding</th><th>Bytes</th></tr></thead><tbody><tr><th>UTF-32</th><td><code>00 00 00 46</code></td></tr><tr><th>UTF-16</th><td><code>00 46</code></td></tr><tr><th>UTF-8</th><td><code>46</code></td></tr></tbody></table><p>And for the character <code>ğŸ¥º</code>, which has a scalar value of <code>129402</code> (<code>01f97a</code> in hex):</p><table><thead><tr><th>Encoding</th><th>Bytes</th></tr></thead><tbody><tr><th>UTF-32</th><td><code>00 01 f9 7a</code></td></tr><tr><th>UTF-16</th><td><code>d8 3e dd 7a</code></td></tr><tr><th>UTF-8</th><td><code>f0 9f a5 ba</code></td></tr></tbody></table><h2 id="introducing-utf-21">Introducing UTF-21</h2><p>UTF-8, UTF-16, and UTF-32 are widely used and a lot of smart people have worked on them.</p><p>Today, Iâ€™m introducing <strong>UTF-21</strong>, a toy character encoding made by me, a <a href="https://evanhahn.com/the-lone-developer-problem/">lone</a> dingus.</p><p>To represent the full range of Unicode scalars, you need 21 bits. Thatâ€™s precisely what UTF-21 does. Each scalar is represented by a 21-bit number, packed back-to-back with no space between.</p><p>For example, <code>F</code> has a scalar value of <code>70</code>, which is encoded like this in binary:</p><pre><code> 000000000000001000110
</code></pre><p><code>ğŸ¥º</code>, which has a scalar value of <code>129402</code>, is encoded like this:</p><pre><code> 000011111100101111010
</code></pre><p>Modern computers like to store data as <em>bytes</em>, not bits. Therefore, the end of the data is padded with zeroes until it fits in a byte. That means there will be between 0 and 7 bits of padding at the end of a UTF-21 data stream.</p><p>The string <code>FğŸ¥º</code> would be encoded like this in binary:</p><pre><code> 000000000000001000110 000011111100101111010 000000
</code></pre><p>The first 21 bits are for the <code>F</code>, the next 21 are for <code>ğŸ¥º</code>, and the last 6 are padding.</p><h2 id="how-does-it-perform">How does it perform?</h2><p>UTF-21 is just a toy project, but how does it stack up against the official UTFs?</p><ul><li><p>UTF-32: UTF-21 is always more efficient than UTF-32 because itâ€™s 11 bits smaller per character. Youâ€™d expect UTF-21 strings to be roughly 66% the size of their UTF-32 counterparts.</p><p>For example, <code>Hello world!</code> is 32 bytes in UTF-21 and 48 bytes in UTF-32.</p></li><li><p>UTF-16: UTF-21 is less efficient than UTF-16 when you donâ€™t need two code units (surrogate pairs) and more efficient when you do. It depends on your use case, but I suspect most strings will be more efficient in UTF-16.</p><p>For example, UTF-16 wins for the string <code>foo bar</code>: 14 bytes of UTF-16 and 19 bytes of UTF-21. But it loses for the string <code>ğŸŒğŸŒğŸŒ</code>: 12 bytes of UTF-16 and only 8 for UTF-21.</p></li><li><p>UTF-8: UTF-21 is less efficient than UTF-8 unless you need three or four bytes for the character, and then UTF-21 is more efficient.</p><p>For example, UTF-8 wins for the string <code>foo bar</code>â€”7 bytes versus 19â€”but loses for the string <code>ì•ˆë…•í•˜ì„¸ìš”</code>â€”14 bytes to 15.</p></li></ul><p>In short, itâ€™s more efficient than UTF-32 but probably worse than the others in most cases.</p><h2 id="why-did-i-do-this">Why did I do this?</h2><p>For fun!</p><p>UTF-21 probably goes under the category of <a href="https://austinhenley.com/blog/makinguselessstuff.html">â€œuseless stuffâ€</a>. Itâ€™s not particularly efficient or good, but I learned a bunch about how Unicode works and had a lot of fun building it.</p><p>I hope this was equally fun and informative to read!</p><p><em>Thanks to <a href="https://codepoints.net">Manuel Strehl</a> for reviewing an early draft of this post.</em></p></div></div>
  </body>
</html>
