<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bosun.ai/posts/rust-for-genai-performance/">Original</a>
    <h1>Comparing our Rust-based indexing and querying pipeline to Langchain</h1>
    
    <div id="readability-page-1" class="page"><article id="article" role="article"> <p>We frequently get asked why we’re building text (code) processing software in Rust and not in Python
since our performance would be bottlenecked by the LLM inference anyway. In this article I’ll do some
exploration on the performance of our Rust based indexing and querying pipeline, and how it compares to
Python’s Langchain. In this specific scenario our tool is significantly faster, which we didn’t
really expect, so I dove in and found out why.</p>
<p>Our motivation for using Rust to implement Swiftide is manifold: we want to build a fast and
efficient toolchain, and we want to have it be reliable and both easy to write and maintain. Rust
checks all these boxes, the ecosystem is strong and constantly growing and the tooling is excellent.</p>
<p>Realistically, although Rust boasts performance benefits like zero-cost abstractions and fearless
concurrency, that doesn’t mean it’s going to make your project run ten times faster. As fast as
the Rust code could be, when you’re dealing with large language models you’ll still be waiting on
those GPUs crunching away. When choosing Rust, the performance of the language itself can’t be the
only motivation, and it wasn’t for us. However, we’d be embarrassed if we’d built our Swiftide library
and it turned out to be slower than similar Python projects so we set out to establish a baseline
benchmark.</p>
<p><img src="https://bosun.ai/_astro/swiftide-3.3times-langchain.BJaxVNlj_ZioWed.webp.jpg" alt="Swiftide outperforming Langchain by 3.3 times" width="1204" height="426" decoding="async" fetchpriority="high" srcset="/_astro/swiftide-3.3times-langchain.BJaxVNlj_ZioWed.webp.jpg 1204w, /_astro/swiftide-3.3times-langchain.BJaxVNlj_ZioWed@904w.jpeg 904w" sizes="100vw"/></p>
<p>In this benchmark we attempt to stress the framework and processing code while still keeping the workload realistic.
The assignment is simple: process a dataset of text, generate embeddings for it,
and then insert the embeddings into a vector database. We’ll be using Qdrant for
the vector database, and FastEmbed for the embeddings. Initially we’ll be focusing on a
small dataset, the Rust Book, of which the embeddings take around 3 seconds on an NVIDIA A6000 GPU.</p>
<p>In the flamegraph below you can see just under 90% of the time is spent inside the ONNX runtime running
computing those embeddings.</p>
<p><img src="https://bosun.ai/_astro/swiftide-bench-rust-book-flamegraph.qpKk5sJ0_GepAC.svg.svg" alt="Flamegraph showing Rust spending most of its time in ONNX" width="1200" height="886" decoding="async" fetchpriority="high"/></p>
<p>If in Rust we’re spending 90% of the time in the ONNX runtime, then why is Langchain spending around
3 times that amount performing roughly the same work? To answer this I ran the code through the <code>cProfile</code> module, and
opened up the resulting profile in <code>snakeviz</code>. Clicking around a bit reveals the big culprit:</p>
<p><img src="https://bosun.ai/_astro/langchain-10s-unstructured.BS2dZSWO_1ENjdN.webp.jpg" alt="Snakeviz showing Langchain spending around 10 seconds parsing Markdown and HTML" width="2098" height="1280" decoding="async" fetchpriority="high" srcset="/_astro/langchain-10s-unstructured.BS2dZSWO_1ENjdN.webp.jpg 2098w, /_astro/langchain-10s-unstructured.BS2dZSWO_1ENjdN@1198w.jpeg 1198w, /_astro/langchain-10s-unstructured.BS2dZSWO_1ENjdN@898w.jpeg 898w" sizes="100vw"/></p>
<p>The langchain pipeline is spending a solid 10 seconds in the Markdown and HTML partitioning step. In
the Swiftide version we were only using a Markdown parser when chunking is enabled, so this is an
unfair comparison. The situation is quickly rectified by switching to the plain <code>TextLoader</code> in Langchain.</p>
<p>I could have left this run out of the article, as it was just a mistake on my part in setting up the benchmark
but I think it’s important to reflect on how easy it was to get this benchmark to be suddenly bound
by the CPU by invoking an inefficient (and in this case unnecessary) preprocessing step.</p>
<p>When we now re-run the benchmark with the <code>TextLoader</code> we see a slightly more reasonable performance difference:</p>
<p><img src="https://bosun.ai/_astro/langchain-1.5-times-rottentomatoes.C8T78e4e_ZgL0T.webp.jpg" alt="Swiftide outperforming langchain by 1.5 times" width="1204" height="324" decoding="async" fetchpriority="high" srcset="/_astro/langchain-1.5-times-rottentomatoes.C8T78e4e_ZgL0T.webp.jpg 1204w, /_astro/langchain-1.5-times-rottentomatoes.C8T78e4e_ZgL0T@904w.jpeg 904w" sizes="100vw"/></p>
<p>To put a bit more pressure on the frameworks, I switched to the medium sized benchmark (Rotten Tomatoes)
for this comparison, which is why it is taking a little bit longer. The ONNX FastEmbed step is now taking
around 20 seconds.</p>
<p>Langchain is a lot closer to Swiftide, but there is still a significant difference. A quick look
at the snakeviz profile reveals that there is now more of a death by a thousand cuts situation going on:</p>
<p><img src="https://bosun.ai/_astro/langchain-fastembed-rottentomatoes-profile.BxzT7Euu_ZUbVtC.webp.webp" alt="Snakeviz showing Langchain spending 20 secs in ONNX, and a lot of short timespans on other things" width="1930" height="1014" loading="lazy" decoding="async" srcset="/_astro/langchain-fastembed-rottentomatoes-profile.BxzT7Euu_ZUbVtC.webp.webp 1930w, /_astro/langchain-fastembed-rottentomatoes-profile.BxzT7Euu_ZUbVtC@1630w.webp 1630w, /_astro/langchain-fastembed-rottentomatoes-profile.BxzT7Euu_ZUbVtC@1330w.webp 1330w, /_astro/langchain-fastembed-rottentomatoes-profile.BxzT7Euu_ZUbVtC@1030w.webp 1030w" sizes="100vw"/></p>
<p>Keep in mind our goal is not really to compare Swiftide and Langchain directly here, just establish a baseline performance metric for
Swiftide. The point that I think we should be taking away from this is that the GPU processing step
is not necessarily the most expensive, and even if it is there might still be significant time
spent elsewhere in your pipeline.</p>
<p>Regardless of what the specifics are of what Langchain or any of the libraries you’re using are doing,
it’s likely that Rust could enable you to do it faster. Wether it’s just through easy and safe parallelism,
or through fast string processing libraries, Rust has the tools to come near the upper limit
of what is possible on your hardware. If that’s useful to you of course depends on your specific
needs.</p>
<hr/>
<p>To get started with Swiftide, head over to <a href="https://swiftide.rs">swiftide.rs</a> or check us out on <a href="https://github.com/bosun-ai/swiftide">github</a>.</p>
<p><a href="https://news.ycombinator.com/item?id=41709436">Discuss on HN</a>
<a href="https://www.reddit.com/r/rust/comments/1ftqfzs/should_you_use_rust_in_llm_based_tools_for/">Discuss on Reddit</a></p>
<ol>
<li>If you’d like to play around with the benchmarks yourself, you can find the code in <a href="https://github.com/bosun-ai/swiftide-bench-comparison">this github repository</a></li>
<li>Benchmarks for this blog were performed on a NVIDIA A6000 GPU, rented from <a href="https://www.hyperstack.cloud/">Hyperstack</a> (not sponsored).
The results on the Github Repo were performed on an Apple M1 Max.</li>
</ol> </article></div>
  </body>
</html>
