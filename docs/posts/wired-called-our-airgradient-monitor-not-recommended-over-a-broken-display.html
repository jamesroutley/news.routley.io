<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.airgradient.com/blog/wired-review-of-airgradient-one-not-recommended/">Original</a>
    <h1>Wired Called Our AirGradient Monitor &#39;Not Recommended&#39; over a Broken Display</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div><div><p>Two weeks ago, I had what I can only describe as a <strong>punch-to-the-stomach moment</strong> (which luckily doesn’t happen very often). The AirGradient ONE - our monitor that was recognized in one of the world’s most rigorous scientific evaluations - suddenly became <strong>“Not Recommended”</strong> by WIRED magazine in their <a rel="nofollow" target="_blank" href="https://www.wired.com/gallery/best-indoor-air-quality-monitors/">The Best Indoor Air Quality Monitors</a> review.</p><p>Yes, this is the same monitor that got two awards from the <a href="https://airlab.solutions/en/actualites/resultats-du-challenge-airlab-microcapteurs-2023-203" target="_blank">AirLab micro sensor challenge</a> , one of the most rigorous sensor testing programs, <strong>beating more than 30 other well known brands</strong>. And yes, this is also the same monitor that the University of Cambridge chose -after rigorous testing- for the largest study on classroom air quality in the world. And yes, it’s the same monitor that is loved by thousands of Home Assistant users for its easy and local integration.</p><p>Today, I want to share why we feel that this is a <strong>flawed review</strong> and the larger picture of how influential publications’ recommendations affect manufacturers as well as consumers.</p><p>As the founder of AirGradient, many of you know that <strong>transparency is how we operate</strong>. This is why we are open-source and why we openly talk about our successes as well as our failures. We work closely with scientists and communities to improve air quality.</p><p>We’re often more critical of ourselves than others are, constantly raising our own bar. So when something like this happens, it feels <strong>deeply unfair</strong>, like when a teacher gives you an F because your pencil broke during the exam, while giving As to students who didn’t even submit their work. But let’s not get ahead of ourselves; we will explain the details below.</p><p>We’re a small team building open-source monitors, competing against companies with <strong>massive marketing budgets and PR machines</strong>. We started as a volunteer project in Thailand, putting impact before profit.</p><p>What we lack in marketing power, we make up for with genuine care, transparency, and authenticity. Over <strong>50,000 users</strong> trust our open-source approach because we believe in giving people control over their data and the ability to repair their own devices. Our monitors offer much more value at a lower price, and our competitors are probably not very happy about this.</p><p>Many companies would try to sweep this “Not Recommended” review under the rug or send lawyers. But I feel I have a responsibility here to <strong>address this head-on</strong>, because our community deserves to know the details, and hopefully, this will trigger a larger discussion around the integrity of tech journalism.</p><p>Now, at this point, I would really recommend that you read the <a rel="nofollow" target="_blank" href="https://www.wired.com/gallery/best-indoor-air-quality-monitors/">review</a> so that you can <strong>form your own opinion</strong> about it.</p><h2 id="why-did-the-airgradient-one-get-a-not-recommended">Why did the AirGradient One get a “Not Recommended”?</h2><p>The <strong>primary reason</strong> cited for the “Not Recommended” label was a <strong>failing display</strong> on the review unit. Let me be clear: this was a legitimate hardware failure, and we take full responsibility for it. As soon as we learned about the issue, we immediately sent replacement parts and a new unit, including repair instructions, as repairability is one of our core differentiators.</p><p>However, the reviewers logic is difficult to follow when you compare it across products:</p><ul><li><strong>Our monitor</strong>: Downgraded due to a faulty display (a warranty-covered hardware issue)</li><li><strong>Another Monitor</strong>: Recommended, despite having <strong>no display at all</strong></li><li><strong>Another Monitor</strong>: Also recommended, despite <strong>lacking a CO2 sensor</strong>—one of the most critical metrics for assessing indoor air quality and ventilation</li></ul><p>How can a product be penalized for a failing display when another recommended product has no display? How can an indoor monitor without CO2 sensing - essential for understanding indoor air quality - be recommended over one that includes this crucial measurement?</p><p>If the author had reached out regarding the failing display (a single unit from any brand can fail - even those known for high quality) and noted that their first unit failed in the final review, we would understand. But to fail the whole unit on what we know is an isolated issue feels very unfair.</p><div><div><div><p>However, what’s <strong>most shocking</strong> for me is that this review is labelled as “The Best Indoor Air Quality Monitors”, yet it has <strong>basically no testing methodology at all</strong> and is pretty much purely based on the personal preferences of the author.</p></div></div></div><p>The icing on the cake is that I actually corresponded with the author about the issues and sent detailed explanations of our methodology concerns. The responses I got back were limited to brief acknowledgements like “Received. I’m on deadline for three other stories and cannot give you a timeline.”</p><p>This response encapsulates many things wrong with the current state of tech journalism: recommendations that affect <strong>livelihoods and purchasing decisions</strong> are treated as just deadlines to meet, not as professional evaluations that deserve rigorous methodology and meaningful dialogue.</p><p>I wonder how much the author actually cares about the quality and correctness of her reviews.</p><h2 id="when-reviews-fail-everyone-pays">When Reviews Fail, Everyone Pays</h2><p>WIRED’s recommendations don’t just influence individual purchases; they have the power to <strong>shape entire market perceptions</strong>. When they say “Not Recommended,” small companies like ours feel a real reputational and financial impact.</p><p>Now, if this were based on a clear methodology and evaluation, I would probably be the first one who would jump into gear and improve our product (which I have done in the past, e.g. when we had issues with the <a href="https://www.airgradient.com/blog/update-on-pms5003-calibration/">calibration of our PM sensors</a>). But getting ‘Not Recommended’ based on <strong>basically non-existent methodology</strong> leaves me frustrated. How do you argue against personal preference?</p><p><strong>BUT it’s not only us who lose, ultimately it’s the consumer!</strong></p><p>When a major publication abandons objective methodology in favor of subjective impressions, readers get recommendations based on <strong>one person’s preferences</strong> rather than a fair and comprehensive evaluation. They miss out on products that might better serve their actual needs, whether that’s due to repairability, accuracy, connectivity or comprehensive sensor coverage.</p><p>Here’s what any credible product review should include—especially when the publication claims to identify “the best” products:</p><ul><li><strong>Clear Evaluation Criteria</strong>: Define what makes a product good or bad upfront. Is it accuracy? Usability? Value? Readers deserve to know the standards being applied, not discover them through inconsistent conclusions.</li><li><strong>Consistent Methodology</strong>: Apply the same criteria to every product. If display quality matters for one device, it should matter for all devices. If a feature is praised in one product, it shouldn’t be ignored in another.</li><li><strong>Objective Testing</strong>: Use measurable standards wherever possible. For technical products, this means controlled conditions, standardized protocols, and leveraging existing independent testing data rather than relying solely on personal impressions.</li><li><strong>Multiple Data Points</strong>: Test more than one unit when possible, especially if hardware failures occur. A single faulty device shouldn’t define an entire product line’s quality.</li><li><strong>Feature-Based Analysis</strong>: Evaluate products based on their intended use and core capabilities. What problems are they designed to solve? How well do they solve them compared to alternatives?</li><li><strong>Transparent Process</strong>: Explain how conclusions were reached. What was tested? Under what conditions? What weights were given to different criteria?</li></ul><p>Without these foundations, “best of” guides become opinion pieces masquerading as authoritative recommendations—and that’s not fair to either manufacturers or consumers trying to make informed decisions.</p><p>We know that testing air quality monitors is not easy. This is why we’ve outlined proper testing methodology in our guide: <a href="https://www.airgradient.com/blog/how-to-test-an-air-quality-monitor/">How to Test an Air Quality Monitor</a>. From a publication as influential as WIRED, can we not expect a <strong>proper evaluation</strong>?</p><p>Who is the <strong>ultimate loser</strong>? <strong>The consumer</strong>. When accuracy matters for health decisions—understanding air quality affects everything from asthma management to sleep quality—flawed methodology means people miss products that might better serve their actual needs.</p><div><p><img src="https://www.scattered-thoughts.net/images/blog/posts/wired-review/wired-one.webp" alt="Forest in New Zealand." loading="lazy" decoding="async"/></p><div><p>Many thanks to our designer Cid for this nice illustration!</p></div></div><h2 id="why-were-embracing-not-recommended">Why We’re Embracing “Not Recommended”</h2><p>To WIRED: don’t worry, we’re not hiring lawyers or reputation management firms to try burying this review. That’s not who we are. Instead, we’re doing something different—<strong>we’re amplifying it</strong>. We want people to see exactly what passes for “comprehensive” evaluation at your publication.</p><p>Here’s what we stand for: prioritizing sensor accuracy over marketing budgets, building products people can actually repair in a throwaway world, and being transparent about our failures while working to fix them. These are the principles that built our community of 50,000+ users.</p><p>Most companies would be celebrating a “Best Overall” rating and staying quiet about the methodology. But here’s the thing: <strong>even if we’d won, I’d still be writing this article</strong>. Poor methodology doesn’t just hurt the companies that get unfair ratings—it hurts consumers making health decisions based on flawed recommendations, and it hurts the entire industry when subjective preferences masquerade as expert evaluation.</p><blockquote><div><div><div><p>This is <strong>bigger than AirGradient</strong>. When publications with millions of readers abandon rigorous standards, they’re not just affecting one company’s sales and reputation —they’re shaping market perceptions, influencing purchasing decisions that affect people’s health, and setting a precedent that opinion journalism is acceptable where technical expertise should rule.</p></div></div></div></blockquote><p>We remain committed to what we’ve always done: building accurate, repairable, and affordable monitors through independent scientific testing and open-source transparency. We’ll keep standing by our products with comprehensive support, because that’s what our community deserves.</p><p>We’ve escalated our concerns to WIRED’s editorial leadership—not seeking special treatment or demanding apologies, but asking for the <strong>consistent, professional evaluation</strong> that consumers deserve when making decisions about their health and indoor environment. We believe in independent journalism, but independence means nothing without professional standards.</p><p>So yes, we’re <strong>embracing our “Not Recommended” rating</strong>. It’s become a badge of honor—proof that we prioritize substance over relationships, transparency over marketing polish, and community trust over media approval.</p><h2 id="what-this-means-for-all-of-us">What This Means for All of Us</h2><p>To our community: The whole AirGradient team and I appreciate your continued support through this. You’ve stood by us not because of what magazines say, but because you’ve experienced our commitment to accuracy, repairability, and transparency firsthand. That means everything.</p><p>But this experience reinforces something that goes beyond AirGradient: when influential publications abandon rigorous methodology, it creates a <strong>broken ecosystem</strong>. Manufacturers get incentivized to invest in PR relationships instead of product quality. Consumers lose access to reliable information exactly when they need it most—when making decisions about their health and indoor environment. And the companies actually doing the hard work of innovation get drowned out by those with bigger marketing budgets.</p><p>The air quality monitoring space is already confusing enough. People are trying to protect their families from pollution, manage asthma, improve sleep quality, and make informed decisions about their indoor environments. They deserve better than subjective opinion pieces dressed up as authoritative guides.</p><p>Here’s what I’m curious about: <strong>Is this the norm now?</strong> Are you seeing this same pattern across other product categories? When you’re researching purchases—whether it’s air quality monitors, smart home devices, or any technical product—what sources do you actually trust?</p><p>And specifically for situations like this: <strong>How would you want us to handle it?</strong> Should companies stay quiet when review methodology breaks down? Should we be more aggressive in calling this out? Or is transparency and open discussion the right approach?</p><p>I’m asking because this affects all of us. Every time we let poor methodology slide, we’re accepting a world where marketing budgets matter more than product quality, where personal preferences get presented as expert evaluation, and where the companies trying to do right by their communities get penalized for it.</p><p><strong>Drop a comment below</strong> or reach out directly—I read every message, and your perspectives help shape how we navigate these situations. Because at the end of the day, we’re not just building monitors; we’re trying to build a better way of doing business, and that requires all of us pushing for higher standards.</p></div></div></div></div></section></div>
  </body>
</html>
