<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.xoria.org/pipelining/">Original</a>
    <h1>Execution Units Are Often Pipelined</h1>
    
    <div id="readability-page-1" class="page"><article><p>In the context of out-of-order microarchitectures,
I was under the impression that execution units
remain occupied until the µop they’re processing is complete.
This is often not the case.</p><p>As an example, take the <a href="https://dougallj.github.io/applecpu/firestorm.html">Firestorm</a> microarchitecture in the A14 and M1.
It has two integer execution units capable of executing multiplies,
which take three cycles to complete one multiplication.</p><p>Of course, a sequence of dependent instructions like</p><pre><code>benchmark:
	mul	x1, x0, x0 // a
	mul	x2, x1, x1 // b
	mul	x3, x2, x2 // c
	mul	x4, x3, x3 // d
	ret
</code></pre><p>will take 4 × 3 = 12 cycles,
since it can only take advantage of a single execution unit:</p><table><thead><tr><th>cycle</th><th>EU 1</th><th>EU 2</th><th>completed</th></tr></thead><tbody><tr><td>0</td><td><code>[a]</code></td><td><code>[ ]</code></td><td></td></tr><tr><td>1</td><td><code>[a]</code></td><td><code>[ ]</code></td><td></td></tr><tr><td>2</td><td><code>[a]</code></td><td><code>[ ]</code></td><td></td></tr><tr><td>3</td><td><code>[b]</code></td><td><code>[ ]</code></td><td><code>a</code></td></tr><tr><td>4</td><td><code>[b]</code></td><td><code>[ ]</code></td><td><code>a</code></td></tr><tr><td>5</td><td><code>[b]</code></td><td><code>[ ]</code></td><td><code>a</code></td></tr><tr><td>6</td><td><code>[c]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>7</td><td><code>[c]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>8</td><td><code>[c]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>9</td><td><code>[d]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code></td></tr><tr><td>10</td><td><code>[d]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code></td></tr><tr><td>11</td><td><code>[d]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code></td></tr><tr><td>12</td><td><code>[ ]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code>, <code>d</code></td></tr></tbody></table><p>With my original understanding of how execution units work,
a sequence of independent instructions like</p><pre><code>benchmark:
	mul	x1, x0, x0 // a
	mul	x2, x0, x0 // b
	mul	x3, x0, x0 // c
	mul	x4, x0, x0 // d
	ret
</code></pre><p>would take 2 × 3 = 6 cycles:</p><table><thead><tr><th>cycle</th><th>EU 1</th><th>EU 2</th><th>completed</th></tr></thead><tbody><tr><td>0</td><td><code>[a]</code></td><td><code>[b]</code></td><td></td></tr><tr><td>1</td><td><code>[a]</code></td><td><code>[b]</code></td><td></td></tr><tr><td>2</td><td><code>[a]</code></td><td><code>[b]</code></td><td></td></tr><tr><td>3</td><td><code>[c]</code></td><td><code>[d]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>4</td><td><code>[c]</code></td><td><code>[d]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>5</td><td><code>[c]</code></td><td><code>[d]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>6</td><td><code>[ ]</code></td><td><code>[ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code>, <code>d</code></td></tr></tbody></table><p>As it turns out, many execution unit and µop combinations are heavily pipelined.
This means that a µop can be issued to an execution unit
while it’s still busy processing a different µop.
So, on Firestorm that code sequence actually executes more like</p><table><thead><tr><th>cycle</th><th>EU 1</th><th>EU 2</th><th>completed</th></tr></thead><tbody><tr><td>0</td><td><code>[a][ ][ ]</code></td><td><code>[b][ ][ ]</code></td><td></td></tr><tr><td>1</td><td><code>[c][a][ ]</code></td><td><code>[d][b][ ]</code></td><td></td></tr><tr><td>2</td><td><code>[ ][c][a]</code></td><td><code>[ ][d][b]</code></td><td></td></tr><tr><td>3</td><td><code>[ ][ ][c]</code></td><td><code>[ ][ ][d]</code></td><td><code>a</code>, <code>b</code></td></tr><tr><td>4</td><td><code>[ ][ ][ ]</code></td><td><code>[ ][ ][ ]</code></td><td><code>a</code>, <code>b</code>, <code>c</code>, <code>d</code></td></tr></tbody></table><p>taking 4 cycles instead of 6.</p><p>In the limit, where the two execution units are
constantly kept fed with multiplication µops,
my original understanding would have predicted 1.5 cycles/instruction on average,
when they in reality can sustain 0.5 cycles/instruction –
each execution unit can be fed a new multiplication µop every cycle,
and we have two of them.</p><p>Knowing this, I finally get why instruction latency and bandwidth tables
specify reciprocal throughput;
because it’s equivalent to cycles/instruction!</p><p>I put together a <a href="https://github.com/lunacookies/PipeliningBenchmark">GitHub repo</a> where you can see this for yourself.
Make sure to adjust the maximum CPU frequency in <code>Entry Point.c</code>
as appropriate.</p><p>Luna Razzaghipour</p></article></div>
  </body>
</html>
