<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://discourse.llvm.org/t/tpde-llvm-10-20x-faster-llvm-o0-back-end/86664">Original</a>
    <h1>10-20x Faster LLVM -O0 Back-End (2020)</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="text">
              <p>5 years ago, <a href="https://discourse.llvm.org/u/nikic">@nikic</a> <a href="https://www.npopov.com/2020/05/10/Make-LLVM-fast-again.html" rel="noopener nofollow ugc">wrote</a>:</p>
<blockquote>
<p>I can’t say a 10% improvement is making LLVM fast again, we would need a 10x improvement for it to deserve that label.</p>
</blockquote>
<p>We recently open-sourced <a href="https://github.com/tpde2/tpde" rel="noopener nofollow ugc">TPDE</a> and our fast LLVM baseline back-end (TPDE-LLVM), which is 10-20x faster than the LLVM -O0 back-end with similar runtime performance and 10-30% larger code size. We support a typical subset of LLVM-IR and only target x86-64 and AArch64. Posting this here, as this might be interesting for the LLVM community – questions/comments welcome!</p>
<p>Data on SPEC CPU 2017 (x86-64, compile-time speedup and code size relative to LLVM 19 -O0 back-end):</p>
<div>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Comp-Time O0 IR</th>
<th>Code Size O0 IR</th>
<th>Comp-Time O1 IR</th>
<th>Code Size O1 IR</th>
</tr>
</thead>
<tbody>
<tr>
<td>600.perl</td>
<td>11.39x</td>
<td>1.27x</td>
<td>15.06x</td>
<td>0.97x</td>
</tr>
<tr>
<td>602.gcc</td>
<td>12.54x</td>
<td>1.32x</td>
<td>17.55x</td>
<td>1.01x</td>
</tr>
<tr>
<td>605.mcf</td>
<td>9.72x</td>
<td>1.27x</td>
<td>12.47x</td>
<td>0.92x</td>
</tr>
<tr>
<td>620.omnetpp</td>
<td>21.46x</td>
<td>1.24x</td>
<td>26.49x</td>
<td>1.03x</td>
</tr>
<tr>
<td>623.xalanc</td>
<td>18.99x</td>
<td>1.24x</td>
<td>24.80x</td>
<td>0.98x</td>
</tr>
<tr>
<td>625.x264</td>
<td>10.52x</td>
<td>1.26x</td>
<td>15.19x</td>
<td>0.97x</td>
</tr>
<tr>
<td>631.deepsjeng</td>
<td>9.60x</td>
<td>1.25x</td>
<td>17.56x</td>
<td>0.97x</td>
</tr>
<tr>
<td>641.leela</td>
<td>21.44x</td>
<td>1.24x</td>
<td>18.36x</td>
<td>0.95x</td>
</tr>
<tr>
<td>657.xz</td>
<td>10.95x</td>
<td>1.30x</td>
<td>15.15x</td>
<td>0.92x</td>
</tr>
<tr>
<td>geomean</td>
<td>13.34x</td>
<td>1.27x</td>
<td>17.58x</td>
<td>0.97x</td>
</tr>
</tbody>
</table>
</div><p>Results for AArch64 are similar (a bit higher speedups due to GlobalISel). Obviously, on optimized IR LLVM’s optimizing back-ends fare much better in terms of run-time (~2x better) and code size (~2x better), but we don’t intend to compete there.</p>
<h3><a name="p-345975-how-does-it-work-1" href="#p-345975-how-does-it-work-1"></a>How Does It Work?</h3>
<p>See <a href="https://arxiv.org/abs/2505.22610" rel="noopener nofollow ugc">this paper</a>. In essence, we do three passes: one IR cleanup/preparation pass, one analysis pass (loop+liveness), and one codegen pass, which performs lowering, regalloc, and machine code encoding in combination.</p>
<h3><a name="p-345975-features-future-plans-2" href="#p-345975-features-future-plans-2"></a>Features &amp; Future Plans</h3>
<p>Currently, the goal is to support typical Clang O0/O1 IR, so there are <a href="https://docs.tpde.org/tpde-llvm-main.html#autotoc_md92" rel="noopener nofollow ugc">a lot of unsupported features</a>. Flang code often works, but sometimes misses some floating-point operations. Rust code works in principle, but some popular crates use illegal (=not natively supported) vector types, which are not implemented yet. (Legalizing vector types/operations is very annoying.)</p>
<p>Besides some more IR features, our current plans include: DWARF support (we already have some prototype) and better register allocation than “spill everything”. If someone provides sufficient motivation, other big items are non-ELF platforms, non-small-PIC code models, and other targets.</p>
<h3><a name="p-345975-speculatively-answered-questions-3" href="#p-345975-speculatively-answered-questions-3"></a>Speculatively Answered Questions</h3>
<p><strong>How to use TPDE-LLVM?</strong></p>
<p>The LLVM back-end is usable as a library (e.g., for JIT compilation, also usable with ORC JIT), as <code>llc</code>-like tool, and can be integrated in Clang (needs a patch, plugins can’t provide a custom back-end right now). Some more details <a href="https://docs.tpde.org/tpde-llvm-main.html" rel="noopener nofollow ugc">here</a>.</p>
<p><strong>Why not make LLVM faster?</strong></p>
<p>We did, LLVM 18-&gt;20 got <a href="https://llvm.org/devmtg/2025-04/slides/technical_talk/engelke_faster.pdf" rel="noopener nofollow ugc">18% faster on x86-64</a>. I think that another 10-20% might be reasonable to achieve, but I’d expect going beyond that to require deeper changes. Even when doing this, a 10x speedup is <em>unlikely</em> to be achievable.</p>
<p><strong>Which changes to LLVM-IR could allow even faster compilation?</strong></p>
<ul>
<li>No <code>ConstantExpr</code> inside functions. These are hard to handle, so before compilation, we rewrite them to instructions – by iterating over all instruction operands to find such constants (and also walk through constant aggregates).</li>
<li>No arbitrarily-sized struct/array values. The only (IMHO) valid reason for struct/array values inside functions are arguments (e.g. AArch64 HFA/HVA) and multiple return values, but there’s no point in loading a <code>[6000 x {i32, i64}]</code>. (We currently have quadratic runtime in the number of elements per value.)</li>
</ul>
<p>Not directly related to performance, but would make things easier:</p>
<ul>
<li>No direct access to thread-local globals (see e.g. the proposal <a href="https://discourse.llvm.org/t/address-thread-identification-problems-with-coroutine/62015/25">here</a>). We cannot easily generate a function call at arbitrary places, so we rewrite all accesses to use the intrinsic.</li>
<li>No arbitrary bit-width arithmetic. <code>i260</code> doesn’t work well already, yet Clang occasionally generates this (bitfield structs) – we don’t support multi-word integers other than <code>i128</code>.</li>
</ul>
<h3><a name="p-345975-random-fun-facts-4" href="#p-345975-random-fun-facts-4"></a>Random “Fun” Facts</h3>
<ul>
<li>We currently use <a href="https://github.com/tpde2/tpde/blob/f6e87d2e97f49f403c12a27e7cf513a44f0f5dbc/tpde-llvm/src/LLVMAdaptor.hpp#L30-L42" rel="noopener nofollow ugc">4 padding bytes in <code>Instruction</code></a> to store instruction numbers, as there is no field to store auxiliary data.</li>
<li><code>PHINode::getIncomingValForBlock</code> causes quadratic compile-time for blocks with many predecessors. Thus, for blocks with &gt;1k predecessors (happens for auto-generated code), we sort incoming entries by block pointer and use binary search.</li>
<li><code>llvm::successors</code> is slow, so we collect and cache successors once. (This could be changed, <code>SwitchInst</code> should store blocks together instead of interleaved with <code>ConstantInt</code>. Maybe even make <code>SwitchInst</code> store <code>uint64_t</code>s instead of arbitrary-width integer?)</li>
<li>We track the performance of <code>tpde-llc</code> (similar to LLVM c-t-t), but ~90% (77-95%) of the time is spent in bitcode parsing…</li>
</ul>
            </div></div>
  </body>
</html>
