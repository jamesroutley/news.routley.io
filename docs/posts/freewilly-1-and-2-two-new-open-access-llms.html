<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models">Original</a>
    <h1>FreeWilly 1 and 2, two new open-access LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-block-type="2" id="block-92ab10ed08f1f3488e0d"><div>

<div>
  <p>Stability AI and its <a href="https://carper.ai" target="_blank"><span>CarperAI lab</span></a> are proud to announce <a href="https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor" target="_blank"><span>FreeWilly1</span></a> and its successor <a href="https://huggingface.co/stabilityai/FreeWilly2" target="_blank"><span>FreeWilly2</span></a>, two powerful new, open access, Large Language Models (LLMs). Both models demonstrate exceptional reasoning ability across varied benchmarks. FreeWilly1 leverages the original LLaMA 65B foundation model and was carefully fine-tuned with a new synthetically-generated dataset using Supervised Fine-Tune (SFT) in standard Alpaca format. Similarly, FreeWilly2 leverages the LLaMA 2 70B foundation model to reach a performance that compares favorably with GPT-3.5 for some tasks.</p><p>Both models are research experiments, and are released to foster open research under a non-commercial license. While we have conducted internal red-teaming to ensure the model remains polite and harmless, we welcome the community&#39;s feedback and help in further red-teaming.</p><p><strong>Data Generation and Collection</strong></p><p>The training for the FreeWilly models was directly inspired by the methodology pioneered by Microsoft in its paper: &#34;Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” While our data generation process is similar, we differ in our data sources.</p><p>Our variant of the dataset, containing 600,000 data points (roughly 10% of the dataset size the original Orca paper used), was created by prompting language models with high-quality instructions from the following datasets created by <a href="https://huggingface.co/conceptofmind" target="_blank"><span>Enrico Shippole</span></a>:</p><ol data-rte-list="default"><li><p><a href="https://huggingface.co/datasets/conceptofmind/cot_submix_original" target="_blank"><span>COT Submix Original</span></a></p></li><li><p><a href="https://huggingface.co/datasets/conceptofmind/niv2_submix_original" target="_blank"><span>NIV2 Submix Original</span></a></p></li><li><p><a href="https://huggingface.co/datasets/conceptofmind/flan2021_submix_original" target="_blank"><span>FLAN 2021 Submix Original</span></a></p></li><li><p><a href="https://huggingface.co/datasets/conceptofmind/t0_submix_original" target="_blank"><span>T0 Submix Original</span></a></p></li></ol><p>With this approach, we generated 500,000 examples with one simpler LLM model and an additional 100,000 with a more sophisticated LLM model. To ensure fair comparisons, we carefully filtered these datasets and removed examples that originated from evaluation benchmarks. Despite training on one-tenth the sample size of the original Orca paper (significantly reducing the cost and carbon footprint of training the model compared to the original paper), the resulting FreeWilly models demonstrate exceptional performance across various benchmarks – validating our approach to synthetically generated datasets.</p><p><strong>Performance Evaluation</strong></p><p>To internally evaluate these models, we used EleutherAI’s <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank"><span>lm-eval-harness</span></a>, to which we added <a href="https://github.com/dmahan93/lm-evaluation-harness/tree/add-agieval" target="_blank"><span>AGIEval</span></a>.</p><p>Both FreeWilly models excel in many areas, including intricate reasoning, understanding linguistic subtleties, and answering complex questions related to specialized domains, e.g. Law and mathematical problem-solving.</p><p><strong>Open LLM Leaderboard benchmarks:</strong></p><p>These FreeWilly results were evaluated by Stability AI researchers and independently reproduced by Hugging Face on July 21st, 2023, and published in their leaderboard.</p>
</div>



</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1689965544627_21970"><div>

<div>
  <p><strong>Contributing to an open future</strong></p><p>FreeWilly1 and FreeWilly2 set a new standard in the field of open access Large Language Models. They both significantly advance research, enhance natural language understanding, and enable complex tasks. We are excited about the endless possibilities that these models will bring to the AI community, and the new applications they will inspire.</p><p>We would like to express our sincere gratitude to our passionate team of researchers, engineers, and collaborators, whose remarkable efforts and dedication have enabled us to reach this significant milestone.</p><p>Stay tuned for more exciting developments, and begin exploring the incredible potential of FreeWilly today!</p>
</div>



</div></div></div>
  </body>
</html>
