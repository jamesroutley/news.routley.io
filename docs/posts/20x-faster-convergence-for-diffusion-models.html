<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sihyun.me/REPA/">Original</a>
    <h1>20x faster convergence for diffusion models</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    <div id="authors">
        <center>
            
        </center>
        <center>
        <p><span><sup>1</sup>KAIST</span>   
            <span><sup>2</sup>Korea University</span>   
            <span><sup>3</sup>Scaled Foundations</span>   
            <span><sup>4</sup>New York University</span> <br/>
        </p>
        <p><span><sup>*</sup>Equal Advising.</span>   
        </p>
            

        </center>

        
        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/main_qual_12.png" alt="PDF Image"/>
        </p>    
    </div>
    <section id="news">
        <h2>News</h2>
        <hr/>
        <div>
            <p><span> event </span> [Oct 2024] Our project page is released.</p>
        </div>
    </section>

    <section>
        <h2>Overview</h2>
        <hr/>
        <p>
            Generative models based on denoising, such as diffusion models and flow-based models, have been a scalable
            approach in generating high-dimensional visual data. Recent works have started exploring diffusion models as
            representation learners; the idea is that the hidden states of these models can capture meaningful, discriminative features.
        </p>    

        <p>
            We identify that the main challenge in training diffusion models stems from the need to learn a high-quality internal representation.
            In particular, we show: 
        </p>
        <blockquote>    
                <p>
                The performance of generative <b>diffusion models can be improved dramatically</b> when they are supported by an
                <b>external high-quality representation</b> from another model, such as a self-supervised visual encoder.
                </p>
            </blockquote>    
        
        <p>
            Specifically, we introduce <b>REPresentation Alignment (REPA)</b>, a simple regularization technique
            built on recent diffusion transformer architectures.
            In essence, REPA distills the pretrained self-supervised visual representation of a clean image into 
            the diffusion transformer representation of a noisy input. This regularization better aligns 
            the diffusion model representations with the target self-supervised representations.
        </p>

        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/feat_matching_fig.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/teaser_plot_all.png" alt="PDF Image"/>
        </p>    
        <p>
            Notably, model training becomes significantly
            more efficient and effective, and achieves &gt;17.5x faster convergence than the vanilla model.
            In terms of final generation quality, our approach achieves <b>state-of-the-art results of FID=1.42</b>
            using classifier-free guidance with the <a href="https://arxiv.org/abs/2404.07724">guidance interval</a>.

        </p>
    </section>

    <section>
        <h2>Observations</h2>
        <hr/>
        <h3>Alignment behavior for a pretrained SiT model</h3>
        <p>
            We empirically investigate the feature alignment between <a href="https://dinov2.metademolab.com/">DINOv2-g</a>
            and the original <a href="https://scalable-interpolant.github.io/">SiT-XL/2</a> checkpoint
            trained for 7M iterations.
            Similar to prior studies, we first observe that pretrained diffusion models do indeed learn meaningful
            discriminative representations. However, these representations
            are significantly inferior to those produced by DINOv2. Next, we find that the alignment between the 
            representations learned by the diffusion model and those of DINOv2 is still considered weak, 
            which we study by measuring their <a href="https://phillipi.github.io/prh/"><i>representation alignment</i></a>.
            Finally, we observe this alignment between diffusion models and DINOv2 improves consistently with longer
            training and larger models.
        </p>

        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/layerwise_lin_eval_baseline.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/cknna_vanilla_dinov2.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/cknna_progression_vanilla.png" alt="PDF Image"/>
        </p>

        <h3>Bridging the representation gap</h3>

        <p>
        REPA reduces the semantic gap in the representation and better aligns it with the target self-supervised
        representations. Interestingly, with REPA, we observe that sufficient representation alignment
        can be achieved by aligning only the first few transformer blocks. This, in turn, allows the later layers
        of the diffusion transformers to focus on capturing high-frequency details based on the aligned representations,
        further improving generation performance.
        </p>

        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/lin_eval_diff.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/cknna_diff.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/slope_diff.png" alt="PDF Image"/>
        </p>
    </section>

    <section>
        <h2>Results</h2>
        <hr/>
        <h3>REPA improves visual scaling</h3>
        <p>
            We first compare the images generated by two SiT-XL/2 models during the first 400K iterations, with REPA
            applied to one of the models. Both models share the same noise, sampler, and number of sampling steps, 
            and neither uses classifier-free guidance. The model trained with REPA shows much better progression.
        </p>
        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/qual_progression.jpg" alt="PDF Image"/>
        </p>    

        <h3>REPA shows great scalability in various perspectives</h3>
        <p>
            We also examine the scalability of REPA by varying pretrained encoders and diffusion transformer model sizes,
            showing that aligning with better visual representations leads to improved generation and linear probing results.
            REPA also provides more significant speedups in larger models, achieving faster FID-50K improvements compared to 
            vanilla models. Additionally, increasing model size yields faster gains in both generation and linear evaluation.
        </p>
        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/encoder_type.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/scaling_law.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/loglinear_correlation.png" alt="PDF Image"/>
        </p>



        <h3>REPA significantly improves training efficiency and generation quality</h3>
        <p>
             Finally, we compare the FID values between vanilla DiT or SiT models and those trained with REPA. 
             Without classifier-free guidance, REPA achieves FID=7.9 at 400K iterations,
             outperforming the vanilla model&#39;s performance at 7M iterations. Moreover, using classifier-free guidance, 
             SiT-XL/2 with REPA outperforms recent diffusion models with 7× fewer epochs, and achieves state-of-the-art FID=1.42
             with additional guidance scheduling.
        </p>

        <p><img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/tab_wo_cfg.png" alt="PDF Image"/>
            <img src="https://elijer.github.io/garden/devnotes/LeetCode-Journal/assets/tab_w_cfg.png" alt="PDF Image"/>
        </p>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr/>
        <pre><code>@article{yu2024repa,
    title={Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think},
    author={Sihyun Yu and Sangkyung Kwak and Huiwon Jang and Jongheon Jeong and Jonathan Huang and Jinwoo Shin and Saining Xie},
    year={2024},
    journal={arXiv preprint arXiv:2410.06940},
}</code></pre>
    </section>

</div></div>
  </body>
</html>
