<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kxgong.github.io/meta_transformer/">Original</a>
    <h1>Meta-Transformers: Transformers Are Becoming Multi-Modal</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            

                  <p><span><sup>1</sup>Multimedia Lab, The Chinese University of Hong Kong   
                      </span>
                    <span><small></small></span>
                  </p>

                  
        </div>
      </div>
    </div>
  </div>
</section>

<!--
<video poster="" id="tree" autoplay controls muted loop height="100%">

  <source src="static/videos/banner_video.mp4"
  type="video/mp4">
</video>
-->

<!-- Teaser video-->
<section>
  <div>
    <div>
      <p><img src="https://kxgong.github.io/meta_transformer/static/images/teaser.png" alt="Teaser"/></p><h2>
        Unified Multimodal Learning. Meta-Transformer utilizes the same backbone to encode natural languages, images, point clouds, audio, video, infrared, hyperspectral, X-ray, time-series, tabular, and graph data. It reveals the potential of transformer architectures for universal perception. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<section>
<h2>
Modalities
</h2>







</section>



<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            Multimodal learning involves utilizing data from various modalities to improve model capacity. Despite the years of development in this field, it remains challenging to devise a unified framework for processing natural language, 2D images, 3D point clouds, and audio spectrograms due to crucial gaps among these different modalities. This study proposes a novel approach that demonstrates a network with frozen parameters can encode the data from the aforementioned four modalities and achieve favorable performance, resulting in a unified framework called Meta Transformer. Using this framework, the raw input data from various modalities are converted to a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta Transformer is the first framework for unified learning among the four modalities with unpaired data, to the best of our knowledge. We evaluated Meta Transformer on different benchmarks across modalities, such as ImageNet for classification, GLUE for text understanding, ModelNet-40, S3DIS, ShapeNetPart for point cloud, and Speech Commands V2 for speech spectrograms. These results indicate a promising future for developing unified multimodal intelligence with transformers.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section>
  <div>
    <div>
      <div>
        <h2>Video Presentation</h2>
        <p>
                <iframe width="750px" height="415" src="https://www.youtube.com/embed/V8L8xbsTyls" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
              </p>      
      </div>
    </div>
  </div>
</section>




<!-- Video end
<div class="center-div">
</div>
<br>
<table align="center" width="600px">
  <tbody>
      <tr>
          <td>
              <div class="center-div">
                <iframe width="750px" height="415" src="https://www.youtube.com/embed/V8L8xbsTyls" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </div>               
          </td>
      </tr>
      <tr>
  </tbody>
</table> -->

<section>
  <div>
    <div>
      <div>
        <h2>Meta-Transformer</h2>
        <div>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/framework.png" alt="MY ALT TEXT"/></p><p>
            Illustration of Unified Multimodal Learning framework for natural language, images, point clouds, and audio spectrograms. An all-to-one tokenizer is used to convert the raw input data from different modalities to a shared token space. Then, a modality-shared encoder with frozen parameters is used to extract high-level semantic features of the input data. Finally, task-specific heads are used for downstream tasks. This framework enables perceiving different modalities with one shared encoder and without paired data.
          </p>
        </div>
        <h2>Tokenizer</h2>
        <div>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/tokenizer.png" alt="MY ALT TEXT"/></p><p>
            We propose the meta scheme in (a) containing grouping, convolution, and transformation progress. Then (b)-(e) represents the building blocks applied with our meta scheme on texts, images, point clouds, and audio spectrograms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- Result -->
<!-- <section class="section hero is-light"> -->
<section>
  <div>
    <div>
      <div>
        <h2>Experiment</h2>
        <div>
          <p>
            We evaluate Meta-Transformer on a wide range of modalities, including 2D images, natural language, 3D point clouds, audio spectrograms, time-series data, etc.
          </p>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/exp_cmp.png" alt="MY ALT TEXT"/></p><p>
            Compared with current state-of-the-art methods, Meta-Transformer also delivers an outstanding performance.
          </p>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/tab_2.png" alt="MY ALT TEXT"/></p><p>
            Table 1: Experimental results for text understanding on the GLUE benchmark. We compare existing advanced methods from paraphrasing, sentiment, duplication, inference, and answering tasks, and we report the pre-training settings and performances.
          </p>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/tab_3.png" alt="MY ALT TEXT"/></p><p>
            Table 2: Experimental results for image understanding. We conduct experiments in classification, object detection, and instance segmentation tasks on the ImageNet [23], MSCOCO [71], and ADE20K [74] datasets. ∗ denotes zero-shot image classification, † denotes linear probing for image classification, and ‡ indicates the model is pre-trained on ImageNet-22K [23], where Bold and underline indicate best and second best results.
          </p>
        </div>
        <div>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/tab_4.png" alt="MY ALT TEXT"/></p><p>
            Table 3: Experimental results for infrared and hyperspectral image understanding.We conduct experiments in classification tasks on the SYSU-MM01 and Indian Pine datasets. We report Rank-1 (R@1), Top-1 Accuracy scores, and the number of trainable parameters (Params).
          </p>
          <p><img src="https://kxgong.github.io/meta_transformer/static/images/tab_5.png" alt="MY ALT TEXT"/></p><p>
          Table 4: Experimental results for point cloud understanding. We conduct experiments on the ModelNet-40 [25], S3DIS [26], and ShapeNetPart [27] datasets. We compare existing advanced methods from classification, semantic, and object part segmentation tasks, and we report the pretraining modality (Pre-train) and trainable parameters number (Param.) of each method.
          </p>
        </div>

        <p><img src="https://kxgong.github.io/meta_transformer/static/images/tab_time_series.png" alt="MY ALT TEXT"/></p><p>
          Table 6: Time-series Forecasting with Meta-Transformers. Following TimesNet, we report the number of trainable parameters and average performances from 4 different prediction lengths, which is {96, 192, 336, 720}.
        </p>

        
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
 
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
 -->




<!-- 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
 -->


<!-- Video carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
           
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 -->



<!-- Paper poster -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
-->


<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2><p>
      If you find our work useful, please cite our paper. BibTex code is provided below:
      </p><pre><code>@article{zhang2023metatransformer,
        title={Meta-Transformer: A Unified Framework for Multimodal Learning}, 
        author={Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
        year={2023},
        journal={arXiv preprint arXiv:2307.10802},
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
