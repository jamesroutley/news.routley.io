<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dotat.at/@/2024-09-02-ewma.html">Original</a>
    <h1>Exponential Rate Limiting</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>Following my previous post on <a href="https://dotat.at/@/2024-08-30-gcra.html">rate limiting with GCRA, leaky buckets
without the buckets</a>, I reviewed my <a href="https://fanf2.user.srcf.net/hermes/doc/antiforgery/ratelimit.html">old notes on rate limiting
for Exim</a>. I thought I should do a new write-up of the ideas
that I hope will be more broadly interesting.</p>
<p>Exponential rate limiting uses an <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponentially-weighted moving
average</a> to measure the client’s rate. It is motivated by a
shift of perspective:</p>
<ul>
<li>first measure the client’s rate,</li>
<li>then compare it to the limit.</li>
</ul>
<p>Algorithms like GCRA and leaky bucket don’t allow you to separate
these two points because they don’t measure the client’s rate as a
concrete number.</p>
<p>A moving average allows more flexible policy enforcement because the
rate measurement is meaningful even when you don’t apply
back-pressure. For example, it’s useful in a dry run mode, or when
diverting messages to a quarantine.</p>
<p>An exponential rate limiter stores, for each client:</p>
<ul>
<li>last update time</li>
<li>average rate</li>
</ul>
<p>This is a similar amount of space as leaky bucket. GCRA uses less
space because it only needs to store a time.</p>
<p>The main disadvantage is that an exponential rate limiter needs fairly
complicated floating point arithmetic.</p>
<ul>
<li><a href="#configuration-parameters">configuration parameters</a></li>
<li><a href="#algorithm">algorithm</a></li>
<li><a href="#behaviour">behaviour</a></li>
<li><a href="#enforcement">enforcement</a></li>
<li><a href="#rationale">rationale</a>
<ul>
<li><a href="#very-slow-clients">very slow clients</a></li>
<li><a href="#varying-intervals">varying intervals</a></li>
<li><a href="#penalty-time">penalty time</a></li>
<li><a href="#fast-bursts">fast bursts</a></li>
</ul>
</li>
<li><a href="#discussion">discussion</a></li>
</ul>
<h2><a name="configuration-parameters" href="#configuration-parameters">configuration parameters</a></h2>
<p>A rate limiter can have three configuration parameters:</p>
<ul>
<li>a maximum <em>rate</em></li>
<li>a <em>burst</em> size that allows a sender to briefly exceed the rate limit</li>
<li>an averaging <em>period</em> that determines how quickly past behaviour is forgotten</li>
</ul>
<p>In a linear rate limiter like GCRA or leaky bucket, the <em>period</em> is
fixed as <em>burst / rate</em> owing to the way the model works.</p>
<p>An exponential rate limiter has two parameters:</p>
<ul>
<li>a <em>limit</em> which is also the maximum <em>burst</em> size</li>
<li>an averaging <em>period</em></li>
</ul>
<p>The maximum <em>rate</em> is <em>limit / period</em>.</p>
<p>For example, I might set <em>limit</em> = 600 requests per <em>period</em> = 1 hour.
If I want to allow the same long-term average rate, but with a smaller
<em>burst</em> size, I might set <em>limit</em> = 10 requests per <em>period</em> = 1
minute.</p>
<p>Deriving the max <em>rate</em> from the other two parameters makes the
algorithm easy to configure, and it turns out to simplify the
mathematical model very nicely.</p>
<h2><a name="algorithm" href="#algorithm">algorithm</a></h2>
<ul>
<li>
<p>each client has a stored update <em>time</em> and <em>rate</em></p>
</li>
<li>
<p>a request has a <em>cost</em>, which is typically 1 for fixed-cost requests,
or (for example) the request size in bytes when limiting bandwidth</p>
</li>
<li>
<p>when a request arrives, get the client’s details</p>
<pre><code>t_prev = client ? client.time : 0
r_prev = client ? client.rate : 0
</code></pre>
</li>
<li>
<p>calculate the <em>interval</em> since the previous request,
relative to the averaging <em>period</em></p>
<pre><code>interval = (t_now - t_prev) / period
</code></pre>
</li>
<li>
<p>clamp the <em>interval</em> to avoid division by zero</p>
<pre><code>interval = max(interval, 1.0e-10)
</code></pre>
</li>
<li>
<p>the exponential smoothing weight is explained below</p>
<pre><code>alpha = exp(-interval)
</code></pre>
</li>
<li>
<p>the instantaneous rate, measured in <em>cost</em> per <em>period</em></p>
<pre><code>r_inst = cost / interval
</code></pre>
</li>
<li>
<p>the updated average rate is</p>
<pre><code>r_now = (1 - alpha) * r_inst + alpha * r_prev
</code></pre>
</li>
<li>
<p>ensure rare requests are counted in full</p>
<pre><code>r_now = max(r_now, cost)
</code></pre>
</li>
</ul>
<h2><a name="behaviour" href="#behaviour">behaviour</a></h2>
<p>When a client starts making requests very fast, its average rate
(<em>r_prev</em> and <em>r_now</em>) increases by close to the <em>cost</em> each time, so
it will hit the <em>limit</em> after close to <em>limit</em> / <em>cost</em> requests.</p>
<p>When the client’s rate is more modest, or closer to its measured
average, the average changes by a smaller amount for each request.</p>
<p>When the client slows down, its measured rate decays exponentially
towards the new level.</p>
<h2><a name="enforcement" href="#enforcement">enforcement</a></h2>
<p>When a client exceeds its <em>limit</em>, how long must it wait before it can
try again and its request will be allowed?</p>
<pre><code>    t_next = t_now + period * ln(r_now / limit)
</code></pre>
<p>The decision to allow or deny a request is separate from calculating
the client’s average rate. It will typically look like,</p>
<pre><code>    if r_now &gt; limit:
        return DENY(t_next)

    client.time = t_now
    client.rate = r_now
    return ALLOW
</code></pre>
<p>This implements a “leaky” policy that measures the rate of requests
that are allowed, without increasing the client’s rate for requests
that are denied. This is usually the right policy when <code>DENY</code> causes
backpressure and clients are expected to retry denied requests.</p>
<p>You can implement a “strict” policy by updating the client’s stored
rate for both denied and allowed requests. A “strict” policy is often
appropriate when there is no backpressure. I used it when quarantining
email messages from end-users whose accounts might have been
compromised to send spam, or who might have been sending a quarterly
newsletter.</p>
<h2><a name="rationale" href="#rationale">rationale</a></h2>
<p>The next few subsections explain how the algorithm works in more
detail. You don’t need to read them to successfully use exponential
rate limiting.</p>
<h3><a name="very-slow-clients" href="#very-slow-clients">very slow clients</a></h3>
<p>When a client returns after a long gap, the <em>interval</em> is very large,
which means <em>alpha</em> is small, and <em>r_inst</em> is small. As a result
<em>r_now</em> becomes very small.</p>
<p>This is unhelpful in practice: it effectively means the client’s first
request is not counted. A more useful way to handle an isolated
request is to say its rate is the <em>cost</em> of the request per the
<em>period</em>. That way it gets treated like the first request of a fast
burst.</p>
<p>The algorithm implements this logic by ensuring that the average rate
is at least as large as the <em>cost</em> of the current request.</p>
<h3><a name="varying-intervals" href="#varying-intervals">varying intervals</a></h3>
<p>Where does the exponential smoothing weight <code>exp(-interval)</code> come from
in the algorithm above?</p>
<p>We are using the usual formula for an exponentially weighted moving
average,</p>
<pre><code>    r_now = (1 - alpha) * r_inst + alpha * r_prev
</code></pre>
<p>Moving averages are commonly calculated over fixed-size intervals, so
typically <em>alpha</em> is also fixed. The subexpression <code>alpha * r_prev</code>
says how much to forget past behaviour. Each time a fixed-size
interval passes, the old rate gets multiplied by <em>alpha</em> again: that
is, the forgetfulness scales exponentially with time.</p>
<p>In our scenario, we want to update our measurement of the rate of
requests each time a request occurs, at irregular and unpredictable
intervals. So our <em>alpha</em> must vary exponentially with the interval.
We derive it using the time since the previous request as a power of
some <em>base</em>.</p>
<pre><code>    t_delta = t_now - t_prev

    alpha = pow(base, t_delta)
</code></pre>
<p>We set the <em>base</em> using the configured averaging <em>period</em>. I
previously said somewhat vaguely that the <em>period</em> determines how
quickly past behaviour is forgotten. In an exponential rate limiter it
is the time for 63% forgetfulness.</p>
<pre><code>    pow(base, period) == exp(-1.0)

    exp(period * ln(base)) == exp(-1.0)

    ln(base) == -1.0 / period

    pow(base, t_delta) == exp(t_delta * ln(base))
</code></pre>
<p>Therefore,</p>
<pre><code>    alpha = exp(-t_delta / period)
</code></pre>
<h3><a name="penalty-time" href="#penalty-time">penalty time</a></h3>
<p>When a client exceeds its <em>limit</em>, it must <em>wait</em> for some time doing
nothing before its request will be allowed. The <em>wait</em> is derived as
follows:</p>
<pre><code>    limit == (1 - alpha) * 0 + alpha * r_now

    limit / r_now == exp(-wait)

    ln(limit / r_now) == -wait

    wait = ln(r_now / limit)
</code></pre>
<p>This <em>wait</em> is relative to the averaging <em>period</em>, so it gets
multiplied by the period to calculate the next permitted time.</p>
<pre><code>    t_next = t_now + period * wait
</code></pre>
<h3><a name="fast-bursts" href="#fast-bursts">fast bursts</a></h3>
<p>Basing the forgetfulness on <em>e</em> seems somewhat arbitrary: why not make
the forgetfulness 50% (a half life) or 90% instead of 63%?</p>
<p>Another seemingly arbitrary choice is to measure rates in <em>cost</em> per
<em>period</em> instead of per second.</p>
<p>It turns out that these choices fit together neatly so that fast
requests are counted at their full <em>cost</em>, so a client will hit its
<em>limit</em> when expected.</p>
<p>The updated rate is calculated as</p>
<pre><code>    r_now = (1 - alpha) * r_inst + alpha * r_prev
</code></pre>
<p>When the <em>interval</em> is very small, <em>alpha</em> is very nearly 1.0, and as
a result the calculation turns out to be counting approximately
linearly towards the <em>limit</em></p>
<pre><code>    r_now = cost + r_prev
</code></pre>
<p>The second subexpression is obvious but the first one is surprising!</p>
<p>Let’s unpack it.</p>
<pre><code>        (1 - alpha) * r_inst
     == (1 - exp(-interval)) * cost / interval
</code></pre>
<p>Factor out the <em>cost</em>; the surprise is that</p>
<pre><code>   1 ≈≈ (1 - exp(-interval)) / interval
</code></pre>
<p>This property comes from the fact that the gradient of
<i>e<sup>x</sup></i> is 1 when <i>x</i> is 0. To show why this is so,
I need some basic calculus:</p>
<blockquote><p>
    y ≡ f(x) ≡ e<sup>x</sup>
</p><p>
    δy ≡ f(x + δx) − f(x)
</p><p>
    δx ≡ −<tt>interval</tt>
</p></blockquote>
<p>So, when <i>x</i> is zero and <i>δx</i> is small,</p>
<blockquote><table><tbody><tr>
    <td>   </td><td>(1 − e<sup>δx</sup>) / −δx</td>
</tr><tr>
    <td> = </td><td>(e<sup>δx</sup> − 1) / δx</td>
</tr><tr>
    <td> = </td><td>(e<sup>0 + δx</sup> − e<sup>0</sup>) / δx</td>
</tr><tr>
    <td> = </td><td>( f(0 + δx) − f(0) ) / δx</td>
</tr><tr>
    <td> = </td><td>δy / δx</td>
</tr><tr>
    <td> ≈ </td><td>dy / dx</td>
</tr><tr>
    <td> = </td><td>e<sup>x</sup></td>
</tr><tr>
    <td> = </td><td>1</td>
</tr></tbody></table></blockquote>
<h2><a name="discussion" href="#discussion">discussion</a></h2>
<p>I originally developed this algorithm in 2005-2006 to throttle
outgoing email spam floods. It turned out to be easy to use and
produced results that made sense. (More so after I added the slow
client tweak!)</p>
<p>I have gone into some detail to explain how the algorithm is derived
and how it behaves. It was years before I understood some of the
mathematics properly, because I accidentally landed on a sweet spot
through some combination of luck and applied mathematical
supersition – 1/<em>e</em> is more beautiful than 1/2 or 1/10, right?</p>
<p>I don’t know if I reinvented exponential rate limiting, or if there
are other similar algorithms out there. When I was working on it I was
not able to find much literature on exponentially weighted moving
averages with varying intervals, so I laboriously worked it out for
myself.</p>
<p>I would love to hear about any other uses of exponential rate
limiting!</p>

</article></div>
  </body>
</html>
