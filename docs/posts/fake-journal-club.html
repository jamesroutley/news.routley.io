<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gwern.net/Fake-Journal-Club">Original</a>
    <h1>Fake Journal Club</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata">
        <p>Discussion of how to teach active reading and questioning of scientific research. Partially fake research papers may teach a critical attitude. Various ideas for games reviewed.</p>
        
        
      </div><div id="markdownBody">
        <div>
          <blockquote>
            <p>How do researchers transition from uncritically absorbing research papers or arguments to actively grappling with it and questioning it? Most learn this meta-cognitive skill informally or by ad hoc mechanisms like being tutored by a mentor, or watching others critique papers at a ‚Äòjournal club‚Äô. This patchwork may not always work or be the best approach, as it is slow and largely implicit, and similar to calibration training in statistical forecasting, targeted training may be able to teach it rapidly.</p>
            <p>To teach this active reading attitude of not believing everything you read, I borrow the pedagogical strategy of deliberately inserting errors which the student must detect, proposing <em>fake</em> research articles which could be read in a ‚Äòfake journal club‚Äô.</p>
            <p>Faking entire articles is a lot of work and so I look at variations on it. I suggest that NN language models like <a href="https://bellmar.medium.com/docs/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#openai" id="brown-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2005.14165?fallback=original#openai" title="&#39;GPT-3: Language Models are Few-Shot Learners&#39;, Brown¬†et¬†al¬†2020 (Original URL: https://arxiv.org/abs/2005.14165#openai )">GPT-3</a> have gotten good enough to, for short passages, provide a challenge for human readers, and that one could create a fake journal club by having a language model repeatedly complete short passages of research articles (possibly entirely fictional ones).</p>
            <p>This would provide difficult criticism problems with rapid feedback, scalability to arbitrarily many users, and great flexibility in content.</p>
          </blockquote>
        </div>
        <p>Accepting research claims at face-value is the first and lowest level of reading a research paper. As one gains expertise in a field, one must move to a more active form of reading, critically interrogating a paper and its claims: <em>nullius in verba</em>! ‚ÄúWhat is good and bad? What is new or old? What is dubious? Does the results section live up to the abstract‚Äôs claims? What kind of, and how much, data would it need to be trustworthy? Where do the authors most furiously handwave over gaps? Does it make the same errors as everyone else, or are its errors at least entertainingly novel? Can you see any implications that the authors did not? Does it tie into some other result, or some broader paradigm?‚Äù</p>
        <p>So, how do you learn <a href="https://bellmar.medium.com/Research-criticism" id="gwern-research-criticism" title="&#39;How Should We Critique Research?&#39;, Branwen¬†2019">good research criticisms</a>? If the end of the path of learning active reading is deep domain expertise, to the point of being able to know which papers to do forensic statistics on to detect fraud, what is the start of the path? What is the first and smallest possible step one can take?</p>
        <section id="not-learning">
          
          <p>You don‚Äôt learn it from journals; few journals embrace the <a href="https://en.wikipedia.org/wiki/Pottery_Barn_rule" data-link-icon="wikipedia" data-link-icon-type="svg" title="Pottery Barn rule">Pottery Barn rule</a>‚Å†, and the process of getting a criticism published, much less a retraction, would put Kafka to shame. (Journals, like doctors, often prefer to bury their mistakes than publish them.)</p>
          <p>Reading lots of papers is no guarantee; one can spend an arbitrary amount of time doing an activity <a href="https://bellmar.medium.com/notes/Competence" id="gwern-notes-competence" title="&#39;Ordinary Incompetence&#39;, Branwen¬†2021">without improving</a> it if one simply goes through it as a routine, without any attempt at growth or <a href="https://en.wikipedia.org/wiki/Practice_(learning_method)#Deliberate_practice" data-link-icon="wikipedia" data-link-icon-type="svg" title="Practice (learning method) ¬ß Deliberate practice">deliberate practice</a>‚Å†. No matter how many decades you spend commuting, you won‚Äôt become a skilled professional race driver. Similarly, many people can spend decades reading papers and come out the other end still taking every paper at face-value and believing things like ‚Äúa <em>p</em>-value of 0.05 means 95% probability a claim is not due to chance‚Äù or ‚Äúmost published results are true‚Äù or ‚Äúcorrelation = causation when we want it to‚Äù. Citation bias means that a reader will be exposed mostly to cites of papers supporting a claim, leading to an echo chamber of confirmation; one has to actively seek out the gray literature and failures to <a href="https://en.wikipedia.org/wiki/Reproducibility" data-link-icon="wikipedia" data-link-icon-type="svg" title="Reproducibility">replicate</a>‚Å†. Many claims never get ‚Äòdebunked‚Äô or definitively refuted, they just fade away as people stop talking about them, and you don‚Äôt notice an absence if you are simply reading what is put in front of you by the media or journals. (One thinks of the science journalists who breathlessly dash from nutrition study to animal study to pre-clinical drug trial to the latest gap-busting silver bullet, without apparently learning a thing or wondering whatever happened to that thing they reported on a decade ago.)</p>
          <p>Taking notes can help, if you are already actively reading, but note-taking and summarization can often mean simply regurgitating it in slightly different words (note the countless students who transcribe enormously detailed notebooks from lectures, but where it all goes in one ear and out the other).</p>
          <p>It helps if you‚Äôre a jerk who reflexively bridles at being told to believe something by a paper, but that can‚Äôt be taught and might not be a net benefit if it could be taught.</p>
          <p>You can learn it the hard way, by being enthusiastic about a shiny new claim and then watching it crash and burn over the next decade in a <a href="https://bellmar.medium.com/Replication" id="gwern-replication" title="&#39;The Replication Crisis: Flaws in Mainstream Science&#39;, Branwen¬†2010">replication crisis</a>‚Å†, and becoming cynical and jaded about the next new shiny claim; this is not recommended because it takes a while, and one might be unlucky enough to pick one of the new claims which are actually true. It is also difficult to convey the feeling of being burned to a third party‚Äîwhy should they care that something they never heard of turned out to be bunk?</p>
        </section>
        <section id="modeling-mentors">
          
          <p>A better way seems to be mentors/‚Äã<a href="https://nintil.com/bloom-sigma/" id="ric√≥n-2019" data-link-icon="ùìù" data-link-icon-type="text" title="&#39;On Bloom&#39;s two sigma problem: A systematic review of the effectiveness of mastery learning, tutoring, and direct instruction&#39;, Ric√≥n¬†2019">tutors</a> and experts in a field one looks up to who model how to do it for you: you can ask them about a shiny new result, and they will explain the gossip and the ‚Äòbackstage‚Äô‚Äî‚Äúthis is why it‚Äôs bulls‚Äît, because a back of the envelope estimate shows it‚Äôs physically impossible, and further, Smith did it for his thesis a decade ago, better, ruling it out; this new result exists because of a special NIH grant with more cents than sense, and it‚Äôll fail to replicate (and don‚Äôt you dare suggest wasting <em>our</em> funding trying to replicate it! no one is impressed by or cites replication attempts), but by the time the inevitable <a href="https://en.wikipedia.org/wiki/Meta-analysis" data-link-icon="wikipedia" data-link-icon-type="svg" title="Meta-analysis">meta-analysis</a> comes out, Williams will have tenure and no one will care about chewing over stale old controversies. So cite it if a reviewer makes you, but otherwise you should ignore it.‚Äù Or, perhaps ‚Äúit‚Äôs the real deal and you should drop everything to work on this, because this is the ‚ÄòImageNet moment‚Äô everyone has been looking forward to for a decade, and the gold rush is on, boys!‚Äù This can be done in person or remotely, like on <a href="https://en.wikipedia.org/wiki/Twitter" data-link-icon="wikipedia" data-link-icon-type="svg" title="Twitter">Twitter</a> or blogs. (Why can research blogs be so influential, when they are never cited &amp; the people writing them do so to the detriment of their careers, and they are often far from the top of their field‚Äîsimply people who enjoy writing? I suspect much of it is as a publicly-accessible substitute for this backstage access.)</p>
        </section>
        <section id="active-reading">
          
          <p>Good reading of a paper requires an active reading, which is fundamentally <em>predictive</em>. The first step is to always ask ‚Äúwhat do I predict I will read next?‚Äù If I can predict anything and no reported results would ever surprise me, that means I don‚Äôt understand anything about an area. While if I can be surprised but I predict everything a paper does say, what can I learn from it? I apparently already know all the information in it, and wasted my time reading it. What I learn from reading a paper are the parts I can‚Äôt predict: the parts that make me go ‚Äúwhat?‚Äù or ‚Äúwho ordered <em>that</em>?‚Äù or ‚Äúthose maniacs, they actually did it!‚Äù Sometimes, I go ‚Äúthat makes no sense, that contradicts X, Y, and Z, and their effects are larger than anyone else gets‚Äù, and it turns out to be a breakthrough, or, (more often than we wish were the case) I was <a href="https://www.lesswrong.com/posts/5JDkW4MYXit2CquLs/your-strength-as-a-rationalist" data-link-icon="LW" data-link-icon-type="text">confused by fiction</a> because those studies turn out to do something wrong or be unreplicable or outright fraud. The more deep expertise I develop in a field, like the individual authors or gossip about where the bodies are buried or what results usually look like, the better I can predict: ‚Äúyeah, whatever, A always says that; B knows which side of his bread is buttered; C is useless because it‚Äôs just confounded and more precisely estimating a useless number; D is within the usual margins of error for this sort of approach and they shouldn‚Äôt be so excited‚Ä¶‚Äù</p>
          <section id="journal-club">
            <h2><a href="#journal-club" title="Link to section: ¬ß &#39;Journal Club&#39;">Journal Club</a></h2>
            <p><a href="https://en.wikipedia.org/wiki/Seminar" data-link-icon="wikipedia" data-link-icon-type="svg" title="Seminar">Seminars</a>‚Å†, particularly <a href="https://en.wikipedia.org/wiki/Journal_club" data-link-icon="wikipedia" data-link-icon-type="svg" title="Journal club">journal clubs</a> (like the <a href="https://arxiv.org/abs/hep-ph/0204295" id="ioffe-2002" data-link-icon="ùõò" data-link-icon-type="text" title="&#39;Landau&#39;s Theoretical Minimum, Landau&#39;s Seminar, ITEP in the Beginning of the 1950s&#39;, Ioffe¬†2002">Landau seminar</a>), can offer a more scalable version of this. If you go to a journal club, you may see someone criticize a paper in a way you won‚Äôt if you simply read journal articles. On the other hand, passive watching is only a starting point (you still need to actually <em>do the thing</em> at some point), and a journal club might not do much criticism, or bad criticism. (A great journal club might have half the world experts on a topic in it. Or it might have none, and instead they‚Äôre over on Twitter ragging on the paper.) Worse, journal clubs conflate several functions: they exist not just to criticize papers but also to keep a specific group abreast of developments in areas of interest that journal club might select only good, important, papers. They have real work to do, not incidentally teaching undergraduates how to read papers. (They can learn that skill on their own, and if not, plenty more where they came from.) Journal clubs may teach active reading of papers better than most methods, but they are not especially designed to teach this, nor would we expect them to be optimal at it.</p>
          </section>
        </section>
        <section id="meta-cognitive-training-calibration">
          
          <p>An analogous situation is prediction &amp; forecasting, and calibration training (which can feed into the skills of <a href="https://bellmar.medium.com/notes/Fermi" id="gwern-notes-fermi" title="&#39;Fermi Calculation Examples&#39;, Branwen¬†2019">Fermi problems</a> and <a href="https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything" data-link-icon="LW" data-link-icon-type="text">measuring anything</a>‚Å†, see also games like <a href="https://en.wikipedia.org/wiki/Zendo_(game)" data-link-icon="wikipedia" data-link-icon-type="svg" title="Zendo (game)"><em>Zendo</em></a>‚Å†, <a href="https://en.wikipedia.org/wiki/Factorio" data-link-icon="wikipedia" data-link-icon-type="svg" title="Factorio"><em>Factorio</em></a>‚Å†, <a href="https://en.wikipedia.org/wiki/Go_(game)" data-link-icon="wikipedia" data-link-icon-type="svg" title="Go (game)">Go</a>‚Å†, <a href="https://en.wikipedia.org/wiki/Mafia_(party_game)" data-link-icon="wikipedia" data-link-icon-type="svg" title="Mafia (party game)"><em>Mafia</em></a>‚Å†/‚Äã<a href="https://en.wikipedia.org/wiki/Diplomacy_(game)" data-link-icon="wikipedia" data-link-icon-type="svg" title="Diplomacy (game)"><em>Diplomacy</em></a>‚Å†, <a href="https://en.wikipedia.org/wiki/Liar&#39;s_dice" data-link-icon="wikipedia" data-link-icon-type="svg" title="Liar&#39;s dice">Liar‚Äôs dice</a>‚Å†, poker, <a href="https://en.wikipedia.org/wiki/GeoGuessr" data-link-icon="wikipedia" data-link-icon-type="svg" title="GeoGuessr"><em>GeoGuessr</em></a>‚Å†, <a href="https://en.wikipedia.org/wiki/Prediction_markets" data-link-icon="wikipedia" data-link-icon-type="svg" title="Prediction markets">prediction markets</a>‚Å†/‚Äã<a href="https://www.overcomingbias.com/2018/08/my-market-board-game.html" data-link-icon="OB" data-link-icon-type="text"><em>Murder, She Bet</em></a>).</p>
          <p>The bad news about prediction &amp; forecasting is that simple formulas and algorithms, with no knowledge of the real world, can often statistically outpredict experts, who know everything there is to know about the topic, but have not learned what a specific probability <em>feels like</em> or to do simple quantitative checks of their numbers to make sure they don‚Äôt add up to &gt;100% or other silly mistakes like that. I may know nothing whatsoever about North Korea, but if you, a North Korean expert who speaks Korean and has visited the country and studied its bomb program for decades, tell me that they test a hydrogen bomb once every 10 years and you are also 99% certain that they are going to test a hydrogen bomb next year, I should be confused how you can go from 10% ‚Üí 99% and immediately ask what makes you so extraordinarily certain. Perhaps you are actually only &gt;50% sure, and are overconfident (as most people are before training). I can then beat you in producing accurate predictions of North Korea nuclear tests by simply knowing that I know nothing, and avoiding 99% in favor of 50%. And experiments like <a href="https://en.wikipedia.org/wiki/Philip_E._Tetlock" data-link-icon="wikipedia" data-link-icon-type="svg" title="Philip E. Tetlock">Tetlock‚Äôs</a> <a href="https://en.wikipedia.org/wiki/Expert_Political_Judgment" data-link-icon="wikipedia" data-link-icon-type="svg" title="Expert Political Judgment">long-term forecasting experiments</a> or his <a href="https://en.wikipedia.org/wiki/The_Good_Judgment_Project" data-link-icon="wikipedia" data-link-icon-type="svg" title="The Good Judgment Project">Good Judgment Project</a> show that this is entirely possible. The experts do much more poorly than one would expect, because all their knowledge gets mangled on the way out for lacking these meta-cognitive skills.</p>
          <p>One might think that this would be an incurable problem. Perhaps one would need decades more study to hone one‚Äôs probabilities‚Äîwhich would be impossible! (The bomb tests would‚Äôve happened by then, for one thing.) But the good news is that it‚Äôs easy to improve a lot without spending years recording predictions or <a href="https://bellmar.medium.com/Prediction-markets" id="gwern-prediction-markets" title="&#39;Prediction Markets&#39;, Branwen¬†2009">trading on prediction markets</a>‚Å†. It can be fixed quite easily by simply doing <a href="https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises" data-link-icon="LW" data-link-icon-type="text">calibration training</a>‚Å†. That is a fancy name for ‚Äòanswering a bunch of trivia questions quickly with your best probability guess, and seeing what you get wrong, until you know what ‚Äú50% certainty‚Äù <em>feels</em> like or what ‚Äú10% certainty‚Äù <em>feels</em> like, and then going back to real questions with those memories in mind‚Äô. After running through a few hundred trivia questions for an hour, our North Korean expert will now know that ‚ÄòI am 99% confident!‚Äô is actually what his ‚Äò50% confident‚Äô feels like, and can express his informed predictions appropriately. Further, after pondering over tricky trivia questions, he will better understand how one should <a href="https://en.wikipedia.org/wiki/Anchoring" data-link-icon="wikipedia" data-link-icon-type="svg" title="Anchoring">anchor</a> to <a href="https://en.wikipedia.org/wiki/Base_rate" data-link-icon="wikipedia" data-link-icon-type="svg" title="Base rate">base rates</a>‚Å†/‚Äãinformative-<a href="https://en.wikipedia.org/wiki/Prior_probability" data-link-icon="wikipedia" data-link-icon-type="svg" title="Prior probability">priors</a> and how to ask questions ‚Äòsince North Korea only tests once a decade, what evidence do I have that should push it above my default guess of 10%? What unexpected events happened or didn‚Äôt happen?‚Äô</p>
          <p>With calibration fixed, his domain expertise can now be used to its fullest and he can beat the simple formulas with his deeper insight.</p>
        </section>
        <section id="fake-journal-club">
          
          <p>Active readers can do something similar, I believe, by focusing on the active reading task itself without the other elements of regular journal clubs.</p>
          <p>So perhaps we can break off the active-reading chunk and make a specialized <strong>Fake Journal Club</strong> (FJC) which focuses on teaching just that, for people in many areas, without needing to be impossibly expert in every single niche there is or will be?</p>
          <p>I think Fake Journal Club should be possible, because active reading is something that can be done at any level of domain expertise. Even if you do not know much about an area, you should be able to understand if there are logical gaps in an argument or if it is getting the usual sorts of results, and learn more from reading it than a blind acceptance of its claims.</p>
          <p>How?</p>
          <section id="real-papers">
            <h2><a href="#real-papers" title="Link to section: ¬ß &#39;Real Papers&#39;">Real Papers</a></h2>
            <p>Using real science papers is problematic. Trivia questions are super-abundant, extremely short, and no one will know them all, so calibration training can use them for rapid testing &amp; clearcut feedback. Papers are long. How do we give feedback to a reader of a paper on their active reading?</p>
          </section>
          <section id="real-or-fake">
            <h2><a href="#real-or-fake" title="Link to section: ¬ß &#39;Real Or Fake&#39;">Real Or Fake</a></h2>
            <p>Could FJC chose a presenter each time, who must present randomly either a fake or real paper, and the participants guess which, perhaps voting? (An analogy of this would the Los Angeles <a href="https://en.wikipedia.org/wiki/Museum_of_Jurassic_Technology" data-link-icon="wikipedia" data-link-icon-type="svg" title="Museum of Jurassic Technology">Museum of Jurassic Technology</a>‚Å†, which mixes half real-but-strange exhibits and half elaborately-faked exhibits, and is fun to go through trying to guess which.)</p>
            <p>No. This would give participants little feedback (only 1 bit for possibly an hour of work, and one might get runs of reals or fakes depending on the randomization). FJC needs to teach faster than that. Also, this is probably far too easy: any slipup or tiny error in style imitation will give away the game, without requiring any genuine learning or reasoning, just exploiting <a href="https://www.word.golf/">verbal dark knowledge</a>‚Å†. It would be bad if all this work wound up only teaching people about the finer points of <span>L<span>a</span>T<span>e</span>X</span> typography or <a href="https://en.wikipedia.org/wiki/American_Psychological_Association" data-link-icon="wikipedia" data-link-icon-type="svg" title="American Psychological Association">APA</a> citation format.</p>
          </section>
          <section id="real-and-fake">
            <h2><a href="#real-and-fake" title="Link to section: ¬ß &#39;Real And Fake&#39;">Real <em>And</em> Fake</a></h2>
            <p>Could the presenter show both a fake <em>and</em> real paper as a block, simply randomizing the order?</p>
            <p>No. This still too little feedback, although at least now one is guaranteed 1 fake/‚Äãreal comparison per JFC. This is also probably far too effortful for the presenter, who must work extremely hard to make a fake which isn‚Äôt debunked immediately. There is further a difficulty that participants may be able to detect a fake simply because they don‚Äôt recognize it from the news or recognize the real one (even if they never read it, simply reading the title is enough to put it into <a href="https://en.wikipedia.org/wiki/Recognition_memory" data-link-icon="wikipedia" data-link-icon-type="svg" title="Recognition memory">recognition memory</a> for a long time‚Äî‚Äúoh, I feel like I‚Äôve seen that somewhere before‚Ä¶‚Äù).</p>
          </section>
          <section id="part-fake">
            <h2><a href="#part-fake" title="Link to section: ¬ß &#39;Part Fake&#39;">Part Fake</a></h2>
            <p>What if we treat a paper as the block, and falsify <em>inside</em> the paper?</p>
            <p>Now we‚Äôre getting somewhere. Removing or faking parts of a paper is much easier than fabricating entire papers from scratch, and there is a pleasing analogy with pre-registered studies like <a href="https://en.wikipedia.org/wiki/Preregistration_(science)#Registered_reports" data-link-icon="wikipedia" data-link-icon-type="svg" title="Preregistration (science) ¬ß Registered reports">Registered Reports</a> where papers are accepted for publication on the strength of the proposed experiment <em>before</em> the results are known by anyone‚Äîthereby actually enforcing the toy science model we learn in school of ‚ÄúHypothesis ‚Üí Experiment ‚Üí Data ‚Üí Analysis ‚Üí Conclusion‚Äù, which is usually honored in the breach.</p>
            <section id="fake-results-section">
              <h3><a href="#fake-results-section" title="Link to section: ¬ß &#39;Fake Results Section&#39;">Fake ‚ÄúResults‚Äù Section</a></h3>
              <p>Could the presenter could present excerpts from a paper, and randomly write a fake Results section which is the opposite of the real results? Perhaps there could be one fake result, along the lines of <a href="https://www.overcomingbias.com/2008/02/my-favorite-lia.html" data-link-icon="OB" data-link-icon-type="text">‚ÄúMy Favorite Liar‚Äù</a> (a professor who added one deliberate error to every lecture to make the students think).</p>
              <p>This is interesting, but still suffers from the little feedback signal problem and the burden on the presenter of being so good at imitating the style of writing that participants are forced to understand the semantics instead of cheap lazy solutions. Asking them to give a verbal paraphrase is easier but still risks shallow learning of style cues instead of semantics (participants might wind up using side-channels like <a href="https://en.wikipedia.org/wiki/Clever_Hans" data-link-icon="wikipedia" data-link-icon-type="svg" title="Clever Hans">Clever-Hansing</a> the presenter, noting when they are slow because they have to think up a fiction versus when they are fast because they are merely recalling the real answer).</p>
            </section>
            <section id="fake-sentence">
              <h3><a href="#fake-sentence" title="Link to section: ¬ß &#39;Fake Sentence&#39;">Fake Sentence</a></h3>
              <p>OK, perhaps we could take a <a href="https://en.wikipedia.org/wiki/Mad_Libs" data-link-icon="wikipedia" data-link-icon-type="svg" title="Mad Libs">Mad Libs</a> approach, and instead of rewriting the entire text, we instead delete the key numbers and require participants to predict the numbers? Or to choose between 2 sets of fake vs real numbers? (Easy mode: half the numbers are multiplied by a random factor 0.1‚Äì10√ó; medium: 0.5‚Äì2√ó; hard: 0.9‚Äì1.1√ó.)</p>
              <p>This idea of faking smaller parts is getting somewhere. This sort of science Mad Libs closely parallels masking/‚Äãdenoising training in deep learning, which we know works well, and it also addresses some meta-scientific critiques (eg. Paul Meehl) that many researchers do not have good numeracy or informative priors on what point-estimates <em>should</em> be, and rely on crude dichotomizing of = 0 and ‚â† 0, and don‚Äôt know what an informative result looks like at all.</p>
              <p>On the other hand, this doesn‚Äôt seem like it‚Äôd work well in most fields, including deep learning, where the specific numbers are often fairly arbitrary and the threshold changes constantly over time. (If you are reading about a new <a href="https://bellmar.medium.com/docs/www/arxiv.org/2f90212754aa5c9487dcc3552e5d807f87063eca.pdf#google" id="vaswani-et-al-2017" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.03762?fallback=original#google" title="&#39;Attention Is All You Need&#39;, Vaswani¬†et¬†al¬†2017 (Original URL: https://arxiv.org/abs/1706.03762#google )">Transformer</a> for <a href="https://bellmar.medium.com/docs/www/imagenet.stanford.edu/dab99939243996c2b1d002d230efe35a11971385.pdf" data-link-icon="pdf" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://image-net.org/papers/imagenet_cvpr09.pdf" title="(Original URL: https://image-net.org/papers/imagenet_cvpr09.pdf )">ImageNet</a> classification, it is relevant that the new hotness &gt; old hotness, but it is usually not too relevant if that threshold is 87.5% rather than 88.3%, nor would reaching 89.1% accuracy have anyone clutching their chest going ‚Äúbut this changes everything!‚Äù If the number were 10√ó bigger, it would be important, but the usual variation is not.)</p>
              <p>So, maybe specific numbers are a little too little.</p>
            </section>
            <section id="fake-paragraph">
              <h3><a href="#fake-paragraph" title="Link to section: ¬ß &#39;Fake Paragraph&#39;">Fake Paragraph</a></h3>
              <p>To continue the DL analogy, perhaps we can use DL directly, as the generator to our discriminators. Can we do something like my <a href="https://bellmar.medium.com/GPT-3-nonfiction#arxiv-paper" id="gwern-gpt-3-nonfiction-arxiv-paper">GPT-3 Arxiv</a> example where GPT-3 completed a real but obscure paper and I challenged people to detect where the completion began?</p>
              <p>This is starting to be workable: anyone can access GPT-3 and in a few seconds, generate a completion. So it‚Äôs not too burdensome. GPT-3 is such an excellent mimic of style and text that one has to be concentrating to detect the exact transition point: if one waits until the errors have accumulated and the classic ‚Äòmeandering‚Äô text becomes obvious, one may be several paragraphs into the fiction.</p>
              <p>But it still limits how much we learn if we do one completion of the paper. GPT-3 is going to do badly faking the rest of the paper as its errors and forgetting accumulate, and it won‚Äôt be able to fake figures. (We can, however, expect future multimodal systems like <a href="https://bellmar.medium.com/docs/www/arxiv.org/74031be598a9772d87398c8502b6e9263a9333e8.pdf#facebook" id="aghajanyan-et-al-2022" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2201.07520?fallback=original#facebook" title="&#39;CM3: A Causal Masked Multimodal Model of the Internet&#39;, Aghajanyan¬†et¬†al¬†2022 (Original URL: https://arxiv.org/abs/2201.07520#facebook )">CM3</a> to seamlessly fake the figures given an <a href="https://en.wikipedia.org/wiki/ArXiv" data-link-icon="wikipedia" data-link-icon-type="svg" title="ArXiv">Arxiv</a> prompt. Just not yet!)</p>
              <p>Perhaps we can go paragraph by paragraph through the paper? Generate¬†1 paragraph, guess pair, reveal.</p>
              <section id="fake-paragraphs">
                <h4><a href="#fake-paragraphs" title="Link to section: ¬ß &#39;Fake Paragraphs&#39;">Fake Paragraphs</a></h4>
                <p>If pairs are still too much work for too little feedback, we can easily generate more completions and provide, say, 4 choices. Now the user gets dense rapid feedback, with no work from a human presenter.</p>
                <p>To gain variety in completions, we can edit the prompt‚Äînothing says we have to complete using <em>exactly</em> the original text, we can delete sentences or randomize numbers to generate a lot of diversely fake completions, while continuing to show the FJC user the original text. (There are also models like <a href="https://arxiv.org/abs/2203.12990#allen" id="wright-et-al-2022" data-link-icon="ùõò" data-link-icon-type="text" title="Generating Scientific Claims for Zero-Shot Scientific Fact Checking">Wright¬†et¬†al¬†2022</a> for extracting specific claims, which could be used to generate claims &amp; negations.)</p>
                <p>Ideally, with 4 choices, each would be picked 1‚ÅÑ4<sup>th</sup> of the time: if it‚Äôs easy to pick the right one, then the user could be learning more from a harder instance, and if some fakes are picked less than others, they are too easy to detect as being fake. Some fakes will be better or worse than others (or dangerously similar!), so we won‚Äôt see an even distribution of errors over them; so we should drop them and generate new ones. If dissatisfied, we can edit or write them manually, or solicit better ones from users.</p>
                <p>If we find that domain expertise or prior knowledge of papers is a problem, we can edit the prompt further: nothing says that the prompt has to be a real paper in the first place! We can, for example, write our own science paper on, say, the <a href="https://openai.com/blog/better-language-models/" id="gpt-2-blog" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Better Language Models and Their Implications: We&#39;ve trained a large-scale unsupervised language model [GPT-2] which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization‚Äîall without task-specific training&#39;, Radford¬†et¬†al¬†2019">Ovid‚Äôs Unicorn</a>‚Å†, a rare quad-horned silver-white quadruped discovered in a remote valley in the Andes Mountains, who speak English. Since they don‚Äôt exist, we can easily make up whatever scientific findings we want about them, keeping it all self-consistent and logical. A model can then generate fake completions as it goes: perhaps some completions will reference their ‚Äúhorn‚Äù (singular), and others describe them as bovine, or just refer to prior studies on dragons which were, however, not referenced prior.</p>
                <p>If we find that we are getting hung up on fine details of style and formatting, and failing to be adequately abstract, we can be more flexible and ask: does it need to be a <em>paper</em>?</p>
                <p>We could instead complete an <em>outline</em>. (GPT-3 will handle simple indented <a href="https://en.wikipedia.org/wiki/Markdown" data-link-icon="wikipedia" data-link-icon-type="svg" title="Markdown">Markdown</a>-style lists without a problem.) Boil down a paper to an easily-parsed hierarchical outline, and challenge the user with completions of various sub-lists.</p>
              </section>
              <section id="website">
                <h4><a href="#website" title="Link to section: ¬ß &#39;Website&#39;">Website</a></h4>
                <p>This approach lends itself naturally to a scalable human-presenter-free static-website web quiz implementation with papers and a pool of pre-generated completions (no <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" data-link-icon="wikipedia" data-link-icon-type="svg" title="Graphics processing unit">GPU</a> or live OA API access required), collecting statistics on good or bad completions to decide what to remove, with papers from a wide variety of fields, real or fake.</p>
                <p>A simple prototype could be done as a text document with the paragraphs then list of completions, then answer 1 screen down, and the reader grades themselves on the honor system.</p>
              </section>
            </section>
          </section>
        </section>
        <section id="external-links">
          
          <ul>
            <li>
              <a href="https://climateer.substack.com/p/numbers" data-link-icon="substack" data-link-icon-type="svg">‚ÄúEveryone gets numbers wrong, even <em>The New York Times</em>: They‚Äôre shortcuts to understanding, and there are no shortcuts‚Äù</a>
            </li>
          </ul>
        </section>
      </div></div>
  </body>
</html>
