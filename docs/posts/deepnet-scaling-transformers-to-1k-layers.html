<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2203.00555">Original</a>
    <h1>DeepNet: Scaling Transformers to 1k Layers</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    <p>
  
  
  
    
  
  
    
    
  

  [Submitted on 1 Mar 2022]</p>
    
    
      
    
  
    <p><a href="http://thewebivore.com/pdf/2203.00555">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  In this paper, we propose a simple yet effective method to stabilize
extremely deep Transformers. Specifically, we introduce a new normalization
function (DeepNorm) to modify the residual connection in Transformer,
accompanying with theoretically derived initialization. In-depth theoretical
analysis shows that model updates can be bounded in a stable way. The proposed
method combines the best of two worlds, i.e., good performance of Post-LN and
stable training of Pre-LN, making DeepNorm a preferred alternative. We
successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and
feed-forward network sublayers) without difficulty, which is one order of
magnitude deeper than previous deep Transformers. Remarkably, on a multilingual
benchmark with 7,482 translation directions, our 200-layer model with 3.2B
parameters significantly outperforms the 48-layer state-of-the-art model with
12B parameters by 5 BLEU points, which indicates a promising scaling direction.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Shuming Ma [<a href="http://thewebivore.com/show-email/ba917fc0/2203.00555">view email</a>]
      </p></div></div>
  </body>
</html>
