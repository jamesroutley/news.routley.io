<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sealos.io/blog/reduce-container-image-size-case-study">Original</a>
    <h1>We reduced a container image from 800GB to 2GB</h1>
    
    <div id="readability-page-1" class="page"><div><div>
<p>This case study is a real-world story from the Sealos platform engineering team. We believe in transparency, and this is a detailed account of how we diagnosed and resolved a critical production issue, sharing our hands-on experience to help the broader cloud-native community.</p>
<h3 id="tldr"><a data-card="" href="#tldr">TL;DR</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>We tackled critical container image bloat on our Sealos platform, fixing a severe disk space exhaustion issue by shrinking an 800GB, 272-layer image to just 2.05GB. Our solution involved a custom tool, <code>image-manip</code>, to surgically remove files and squash the image layers. This 390:1 reduction not only resolved all production alerts but also provides a powerful strategy to reduce container image size.</p>
<h3 id="1-the-problem-critical-disk-exhaustion-caused-by-container-image-bloat"><a data-card="" href="#1-the-problem-critical-disk-exhaustion-caused-by-container-image-bloat">1. The Problem: Critical Disk Exhaustion Caused by Container Image Bloat</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>It was 2 PM when the PagerDuty alert blared for the fifth time that week: &#34;Disk Usage &gt; 90% on <code>devbox-node-4</code>.&#34; Our Sealos cluster&#39;s development environment node was once again evicting pods, grinding developer productivity to a halt. This was a classic symptom of <a href="https://kubernetes.io/docs/concepts/overview/" rel="noreferrer noopener" target="_blank">Kubernetes</a> disk space exhaustion, but the root cause was elusive. The node was equipped with a hefty 2TB SSD, yet a simple <code>df -h</code> confirmed only 10% of its space remained.</p>
<p>Our initial reaction was to treat the symptom. We expanded the node&#39;s storage to 2.5TB, assuming a transient workload spike. The next day, the alert returned, mocking our efforts. The problem wasn&#39;t a spike; it was a cryptic, relentless consumption of storage stemming from what we would later discover was extreme container image bloat. For a platform promising stable and predictable development environments, this failure was an unacceptable breach of trust.</p>
<h3 id="2-why-it-matters-the-business-context"><a data-card="" href="#2-why-it-matters-the-business-context">2. Why It Matters: The Business Context</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The <strong><a href="https://sealos.io/products/devbox">Sealos devbox feature</a></strong> is a cornerstone of our value proposition: providing developers with isolated, one-click, reproducible cloud-based environments. This persistent disk space exhaustion, a problem often linked to scenarios where a Docker image is too large, wasn&#39;t just a technical nuisance; it was a direct threat to that core promise. Unreliable environments lead to frustrated developers, lost productivity, and ultimately, customer churn. The stability of this single feature was directly tied to user trust and our platform&#39;s reputation in a competitive market. We weren&#39;t just fixing a disk; we were defending our product&#39;s integrity.</p>
<h3 id="3-investigation-pinpointing-the-io-storm-with-iotop-and-du"><a data-card="" href="#3-investigation-pinpointing-the-io-storm-with-iotop-and-du">3. Investigation: Pinpointing the I/O Storm with iotop and du</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Our hands-on investigation began by hunting for the source of the bleeding. The first tool we reached for was <code>iotop</code> to identify any processes with abnormal I/O activity. The culprit was immediately apparent: multiple <strong><a href="https://containerd.io/" rel="noreferrer noopener" target="_blank">containerd</a></strong> processes were writing to disk at a sustained, alarming rate of over 100MB/s. For a container runtime managing mostly idle development environments, this was a massive red flag.</p>
<p><span aria-owns="rmiz-modal-" data-rmiz=""><span data-rmiz-content="not-found"><img alt="Terminal output from the iotop command showing containerd processes with disk write speeds over 100MB/s." loading="lazy" decoding="async" data-nimg="1" sizes="(max-width: 768px) 100vw, (max-width: 1200px) 70vw, 900px" srcset="/_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=640&amp;q=75 640w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=750&amp;q=75 750w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=828&amp;q=75 828w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=3840&amp;q=75 3840w" src="https://sealos.io/_next/image?url=.%2Fimages%2Fcontainerd-high-disk-io-iotop.png&amp;w=3840&amp;q=75"/></span></span><span>Terminal output from the iotop command showing containerd processes with disk write speeds over 100MB/s.</span></p>
<p>This pointed to a problem within the containers themselves. We began hunting for the largest offenders within containerd&#39;s storage directory, using <code>du</code> to scan the overlayfs snapshots.</p>
<figure tabindex="0"></figure>
<p>The output was not what we expected. Instead of a mix of large user files, a single filename appeared repeatedly, each instance a monstrous 11GB.</p>
<figure tabindex="0"></figure>
<p>The file <code>/var/log/btmp</code> is a standard Linux system file that records failed login attempts. On a healthy system, it measures in kilobytes. An 11GB <code>btmp</code> file is unheard of. We inspected the contents of one of these files using the <code>last</code> command.</p>
<figure tabindex="0"></figure>
<p>The terminal was flooded with a scrolling wall of failed SSH login attempts, timestamped at a rate of dozens per second. This was clear evidence of a persistent, months-long brute-force attack. Our system had been dutifully recording every single failed attempt.</p>
<h3 id="4-root-cause-how-overlayfs-copy-on-write-amplified-a-brute-force-attack"><a data-card="" href="#4-root-cause-how-overlayfs-copy-on-write-amplified-a-brute-force-attack">4. Root Cause: How OverlayFS Copy-on-Write Amplified a Brute-Force Attack</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The discovery of the brute-force attack was only the first layer of the problem. Why did it cause such catastrophic disk usage? This analysis revealed the root cause of our container image bloat: a perfect storm created by the intersection of container image architecture and a series of security oversights.</p>
<p><strong>Primary Technical Contradiction: Copy-on-Write vs. Log Files</strong></p>
<p>The core of the issue was a disastrous interaction between <strong><a href="https://www.kernel.org/doc/html/latest/filesystems/overlayfs.html" rel="noreferrer noopener" target="_blank">OverlayFS</a></strong>&#39;s Copy-on-Write (CoW) mechanism and the ever-growing <code>btmp</code> file, a textbook example of poor <strong>OverlayFS copy-on-write performance</strong> when handling large, frequently modified files. The problematic user image had an astonishing <strong>272 layers</strong>, each representing a <code>commit</code> operation.</p>
<p><span aria-owns="rmiz-modal-" data-rmiz=""><span data-rmiz-content="not-found"><img alt="Terminal screenshot showing a single container image composed of 272 layers, indicating image bloat." loading="lazy" decoding="async" data-nimg="1" sizes="(max-width: 768px) 100vw, (max-width: 1200px) 70vw, 900px" srcset="/_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=640&amp;q=75 640w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=750&amp;q=75 750w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=828&amp;q=75 828w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=3840&amp;q=75 3840w" src="https://sealos.io/_next/image?url=.%2Fimages%2Fbloated-container-image-272-layers.png&amp;w=3840&amp;q=75"/></span></span><span>Terminal screenshot showing a single container image composed of 272 layers, indicating image bloat.</span></p>
<p>Here&#39;s how the disaster unfolded:</p>
<ol>
<li>A user&#39;s container is under a brute-force attack, and <code>/var/log/btmp</code> grows to 11GB.</li>
<li>The user performs a <code>commit</code>, creating a new image layer.</li>
<li>A single new failed login is appended to <code>/var/log/btmp</code>.</li>
<li>Because of CoW, OverlayFS doesn&#39;t just write the new line. It copies the <em>entire 11GB file</em> into the new, upper layer.</li>
<li>This process repeated 271 times.</li>
</ol>
<p>Even if the user deleted the <code>btmp</code> file in the newest layer, the 271 copies of the 11GB file would remain immutably stored in the layers underneath. The disk space was fundamentally unrecoverable through standard container operations.</p>
<p><strong>Compounding Factors: The Swiss Cheese Model</strong></p>
<p>This technical failure was enabled by three distinct oversights in our platform&#39;s design:</p>
<ul>
<li><strong>Defense #1 (Missing): A Cap on Image Layer Growth.</strong> We had no safeguards to prevent an image&#39;s layer count from ballooning. This allowed the CoW problem to compound exponentially.</li>
<li><strong>Defense #2 (Missing): Secure Base Image Configuration.</strong> Our early <code>devbox</code> base images prioritized ease-of-use, leaving SSH password authentication enabled and exposed to the public internet without rate-limiting tools like <code>fail2ban</code>.</li>
<li><strong>Defense #3 (Missing): Log Rotation.</strong> Believing containers to be ephemeral, we omitted a standard <code>logrotate</code> configuration for system logs like <code>btmp</code>. This fatal assumption allowed a single log file to grow without bounds.</li>
</ul>
<h3 id="5-solution-building-a-custom-oci-tool-to-squash-272-image-layers"><a data-card="" href="#5-solution-building-a-custom-oci-tool-to-squash-272-image-layers">5. Solution: Building a Custom OCI Tool to Squash 272 Image Layers</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Standard <code>docker</code> commands were insufficient; we had to perform surgical <strong><a href="https://github.com/opencontainers/image-spec" rel="noreferrer noopener" target="_blank">OCI image</a></strong> manipulation on its immutable history. This required building our own specialized tooling and a dedicated, high-performance processing environment to truly reduce the container image size.</p>
<p><strong>Architecture Rework: The <code>image-manip</code> Scalpel</strong></p>
<p>We developed a new CLI tool, <code>image-manip</code>, to treat OCI images as manipulable data structures. For this task, we leveraged two of its core functions:</p>
<ol>
<li><strong><code>image-manip remove /var/log/btmp &lt;image&gt;</code></strong>: This command adds a new top layer containing an OverlayFS &#34;whiteout&#34; file. This special marker instructs the container runtime to ignore the <code>btmp</code> file in all underlying layers, effectively deleting it from the merged view without altering the original layers.</li>
<li><strong><code>image-manip squash</code></strong>: This is the key to reclaiming disk space and the core of our strategy to squash the image layers. The tool creates a temporary container, applies all 272 layers in sequence to an empty root filesystem, and then exports the final, merged filesystem as a single new layer. This flattens the image&#39;s bloated history into a lean, optimized final state.</li>
</ol>
<p><strong>Tool Innovation: The High-Performance Operating Room</strong></p>
<p>Performing these intensive operations on production nodes was not an option. We built dedicated <code>devbox-image-squash-server</code> nodes in three regions using <code>ecs.c7a.2xlarge</code> instances (8-core CPU, 16GB RAM). To handle the I/O storm, we configured a striped <strong><a href="https://www.redhat.com/sysadmin/lvm-logical-volume-management" rel="noreferrer noopener" target="_blank">LVM (Logical Volume Management)</a></strong> volume across two 1TB ESSD cloud disks.</p>
<figure tabindex="0"></figure>
<p>A <strong><a href="https://fio.readthedocs.io/en/latest/" rel="noreferrer noopener" target="_blank"><code>fio</code></a></strong> benchmark confirmed our setup could handle the load, achieving 90.1k random write IOPS.</p>
<figure tabindex="0"></figure>
<p>Finally, we fine-tuned the OS to give <code>containerd</code> maximum I/O priority.</p>
<figure tabindex="0"></figure>
<p>At 10:00 AM on September 11, we began the procedure on the most critical image: 800GB spread across 272 layers. The <code>remove</code> operation, which adds a &#34;whiteout&#34; layer, was nearly instantaneous:</p>
<figure tabindex="0"></figure>
<p>The <code>squash</code> operation was the main event. After an hour of intense processing, the logs delivered the news we were hoping for.</p>
<figure tabindex="0"></figure>
<h3 id="6-validation-the-proof-is-in-the-data"><a data-card="" href="#6-validation-the-proof-is-in-the-data">6. Validation: The Proof Is in the Data</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The new, squashed image was a mere <strong>2.05GB</strong>. We had achieved a staggering 390:1 compression ratio.</p>
<p>After pushing the optimized image, we restarted the user&#39;s <code>devbox</code>. It started successfully. A quick check confirmed the operation&#39;s success: the user&#39;s environment was perfectly intact, and the <code>btmp</code> monster was gone.</p>
<figure tabindex="0"></figure>
<p>The quantitative impact on the platform was dramatic and immediate. The data unequivocally demonstrates the success of our approach to optimize a container image, leading to massive savings in storage, cost, and developer time.</p>
<div><table><thead><tr><th>Metric</th><th>Before Fix</th><th>After Fix</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>Disk Space Alerts (30 days)</strong></td><td>23</td><td><strong>0</strong></td><td><strong>100% Reduction</strong></td></tr><tr><td><strong>Avg. Node Disk I/O</strong></td><td>120 MB/s</td><td>26 MB/s</td><td><strong>78% Decrease</strong></td></tr><tr><td><strong>Avg. Container Image Pull Time</strong></td><td>75 seconds</td><td>26 seconds</td><td><strong>65% Faster</strong></td></tr><tr><td><strong>Max Container Image Size</strong></td><td>800 GB</td><td>2.05 GB</td><td><strong>390x Smaller</strong></td></tr><tr><td><strong>Estimated Storage Cost</strong></td><td>~$520/cluster/mo</td><td>~$70/cluster/mo</td><td><strong>$450/mo Savings</strong></td></tr></tbody></table></div>
<hr/>
<h3 id="7-lessons-learned--next-steps"><a data-card="" href="#7-lessons-learned--next-steps">7. Lessons Learned &amp; Next Steps</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-label="Link to section"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>This incident was a painful but invaluable lesson in the hidden complexities of containerized systems. Our solution was effective, but it was a reactive, manual procedureâ€”a complex surgery to fix a preventable disease. Here are our key takeaways and future preventative measures, framed as a quick FAQ.</p>
<p><strong>Q: What was the primary cause of the extreme container image bloat?</strong></p>
<p><strong>A:</strong> The primary cause was the interaction between OverlayFS&#39;s Copy-on-Write (CoW) mechanism and a large, frequently updated log file (<code>/var/log/btmp</code>). Each minor update caused the entire 11GB file to be copied into a new image layer, a process that repeated over 270 times, compounding the storage consumption.</p>
<p><strong>Q: Why couldn&#39;t you just delete the file with a standard <code>docker commit</code>?</strong></p>
<p><strong>A:</strong> Deleting a file in a new layer only adds a &#34;whiteout&#34; marker that hides the file from the final view. The original 271 copies of the 11GB file would remain immutably stored in the underlying layers, continuing to consume disk space. A full layer squash was necessary to create a new, clean filesystem and truly reclaim the space.</p>
<p><strong>Q: What is the key lesson for other platform engineers from this experience?</strong></p>
<p><strong>A:</strong> The key insight is to treat container images not as opaque black boxes, but as structured, manipulable archives. Deeply understanding the underlying technology, like the OCI image specification, allows for advanced optimization and troubleshooting that goes far beyond standard tooling. This knowledge is essential for preventing issues like <strong>Kubernetes disk space exhaustion</strong> before they start.</p>
<p>Our immediate next step is to move from firefighting to fire prevention. We have already implemented automated monitoring that triggers an alert if any user image exceeds 50 layers or 10GB in size. Furthermore, all new <code>devbox</code> base images now ship with password authentication disabled by default and a properly configured <code>logrotate</code> service.</p><section aria-labelledby="related-articles-heading"><header><h2 id="related-articles-heading">Related Articles</h2></header></section></div></div></div>
  </body>
</html>
