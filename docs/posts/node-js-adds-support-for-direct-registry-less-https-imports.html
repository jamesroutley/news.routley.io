<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/nodejs/node/pull/36328">Original</a>
    <h1>Node.js adds support for direct registry-less HTTPS imports</h1>
    
    <div id="readability-page-1" class="page"><div disabled="" sortable="">
<div>
          <p dir="auto">Hey all, so I brought up some points recently on Twitter related to this proposal, so I thought I&#39;d re-articulate them here. Keep in mind I&#39;ve never contributed to Node.js, this is just my opinion as a long-time user and a contributor to Deno.</p>
<h2 dir="auto">TL;DR:</h2>
<p dir="auto">Importing from npm isn&#39;t inherently more secure than importing from a raw URL, so let&#39;s work to make the whole ecosystem more secure, regardless of how packages are imported.</p>
<h2 dir="auto">URL Imports Aren&#39;t Inherently Less Secure</h2>
<p dir="auto">First I want to address the biggest concern that always comes up when talking about URL imports: &#34;It&#39;s insecure.&#34; The argument is usually summed up similarly to what <a data-hovercard-type="user" data-hovercard-url="/users/jasnell/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/jasnell">@jasnell</a> said earlier in this thread: &#34;I could fairly easily write a script that returns one bit of javascript during development but replaces that with a malicious script when accessed from a production server, [...]&#34;</p>
<p dir="auto">It&#39;s true, but it implies that the current way of importing is somehow more secure, and that it is more secure by virtue of not explicitely using a URL. However, packages hosted on npm are vulnerable to this problem as well. Not even a month ago, there was <a href="https://www.zdnet.com/article/malicious-npm-package-opens-backdoors-on-programmers-computers/" rel="nofollow">another report of malicious code</a> being distributed in an npm package.</p>
<p dir="auto">We&#39;ve become accustomed to assume that central registries are somehow more secure, but it&#39;s simply not the case. Packages are still fetched from a remote source and malicious code can still make its way into npm. Once we accept that reality,  URL imports become much less scary (or central registries become much more scary, depending on how you look at it).</p>
<p dir="auto">The concerns raised here are all very valid; We just have to remember that they are issues that also affect the current import model, and so I don&#39;t it&#39;s fair to block this proposal on the grounds that it&#39;s &#34;less secure&#34;.</p>
<p dir="auto">I would even argue that URL imports have the potential to be <em>more</em> secure than traditional imports through npm, because it leverages two important aspects of the web: Authority and Visibility. It gives end-users the possibility to verify that they are imported from a trusted domain name and that that domain name has a valid SSL certificates that verifies that it is indeed that domain. It also allows to verify the whois information associated with that domain, potentially being much more explicit about when a domain or a package changes ownership. (As an aside: wouldn&#39;t you rather import express directly from <a href="https://expressjs.org" rel="nofollow">https://expressjs.org</a> than <a href="https://npmjs.org" rel="nofollow">https://npmjs.org</a> ?)</p>
<p dir="auto">Those are all things that end users can implement themselves and include in their CI/CD pipelines and compliance tools. They&#39;ve been the backbone of the internet for a long time. Browsers trust them. Users trust them. Why couldn&#39;t we?</p>
<hr/>
<h2 dir="auto">Feedback On The Concerns Above</h2>
<p dir="auto">The other part of <a data-hovercard-type="user" data-hovercard-url="/users/jasnell/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/jasnell">@jasnell</a> comment - &#34;[...] without the developer ever knowing that there&#39;s been a change.&#34; is very important; I think if URL imports are to go forward, they should be restricted to tagged versions, no &#34;latest&#34; allowed. deno.land/x uses the <code>@</code> symbol to identify versions. While we don&#39;t support semver type versioning like package.json does, it&#39;s probably possible to implement in order to support the same version limiting already available in Node.</p>
<p dir="auto">Local caching and interop with <code>require(&#34;some-local-package&#34;)</code> are also two good points. Now, this might just be me being naive about the complexity of the node_modules system, but we could simply cache url-imported modules in node_modules: It would make the old style of imports compatible with url imports and solve the issue of the local cache. Sure it wouldn&#39;t be compliant with http cache headers, but then again, node_modules already isn&#39;t and that&#39;s never been a problem in the past. In fact, I think there&#39;s a larger argument to be made here that being compliant with http cache headers don&#39;t make much sense on the server side.</p>
<p dir="auto">Similarly, I agree with <a data-hovercard-type="user" data-hovercard-url="/users/jasnell/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/jasnell">@jasnell</a> with regards to the cookies support; I don&#39;t think it makes much sense here.</p>
<p dir="auto">I&#39;m less certain about redirects, they can be helpful for resolving versions when using tags like <code>^1.6</code> or something. Maybe restrict them to the same origin?</p>
<p dir="auto">I don&#39;t see a reason to <em>not</em> include an allow/block function for specific origins, though I think we should also add a recommendation in the docs for operators to include those in firewall rules (hey if we&#39;re gonna take advantage of web mechanisms, let&#39;s go all the way!)</p>
<p dir="auto">Now for a big one: <strong>integrity checks</strong>. Honestly, I don&#39;t know how much we can do there. Ultimately we can include checksum verification to make sure that the content the user receives is the same one the server said it was sending and that it wasn&#39;t tempered with. Beyond that, I don&#39;t there&#39;s much else that can be done. As I said earlier, it&#39;s already possible for legitimate users to upload malicious code to a legitimate platform. The best anyone can do here is certify that the content wasn&#39;t messed with by a man-in-the-middle.</p>
      </div>
</div></div>
  </body>
</html>
