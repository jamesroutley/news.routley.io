<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.causal.app/blog/scaling">Original</a>
    <h1>Scaling our Spreadsheet Engine from Thousands to Billions of Cells</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><div id="content"><p>Causal is a spreadsheet built for the 21st century to help people work better with numbers. Behind Causal‚Äôs innocent web UI is a complex calculation engine ‚Äî¬†an interpreter that executes formulas on an in-memory, multidimensional database. The engine sends the result from evaluating expressions like Price * Units to the browser. The engine calculates the result for each dimension such as time, product name, country e.g. what the revenue was for a single product, during February ‚Äò22, in Australia.</p><p>In the early days of Causal, the calculation engine ran<em> </em>in Javascript in <em>the browser</em>, but that only scaled to 10,000s of cells. So we moved the calculation engine <em>out</em> of the browser to a Node.js service, getting us to acceptable performance for low 100,000s of cells. In its latest and current iteration, we moved the calculation engine to Go, getting us to 1,000,000s of cells.</p><p>But every time we scale up by an order of magnitude, our customers find new use-cases that require yet another order of magnitude more cells!</p><p>With no more ‚Äúcheap tricks‚Äù of switching the run-time again, how can we scale the calculation engine 100x, from millions to <em>billions</em> of cells?</p><p>In summary: by moving from maps to arrays. üòÖ That may seem like an awfully pedestrian observation, but it certainly wasn‚Äôt obvious to us at the outset that this was the crux of the problem!¬†</p><p>We want to take you along our little journey of what to do once you‚Äôve reached a dead-end with the profiler. Instead, we‚Äôll be approaching the problem from first principles with back-of-the envelope calculations and writing simple programs to get a feel for the performance of various data structures. Causal isn‚Äôt quite at billions of cells yet, but we‚Äôre rapidly making our way there!</p><h3>Optimizing beyond the profiler dead-end</h3><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/62c439204caf55db770d399c_SkOrgttmk55tZeydFINTMGWE6SDnGLqJm9u8UM1BkSICZUigxu_tNJHZ8bqcYYiQcy7ZYLwStGAUREdBZwbA5OEmBFmfyyTGw83ywet1etztQmLgR2TvTSVpqrCX4nNG7AfKLXQDg_8vYWBGXxk.png" alt=""/></p><figcaption>Profile from the calculation engine that feels difficult to action</figcaption></figure><p>What does it look like to reach a <em>dead-end</em> with a profiler? When you run a profiler for the first time, you‚Äôll often get something useful: your program‚Äôs spending 20% of time in an auxiliary function log_and_send_metrics()that you know <em>reasonably</em> shouldn‚Äôt take 20% of time.</p><p>You peek at the function, see that it‚Äôs doing a ridiculous amount of string allocations, UDP-jiggling, and blocking the computing thread‚Ä¶ You play this fun and rewarding profile whack-a-mole for a while, getting big and small increments here and there.</p><p>But at some point, your profile starts to look a bit like the above: There‚Äôs no longer anything that stands out to you as <em>grossly</em> against what‚Äôs <em>reasonable.</em> No longer any pesky log_and_send_metrics() eating double-digit percentages of your precious runtime.</p><p>The constraints move to your own calibration of what % is reasonable in the profile: It‚Äôs spending time in the GC, time allocating objects, a bit of time accessing hash maps, ‚Ä¶ Isn‚Äôt that all <em>reasonable</em>? How can we possibly know whether 5.3% of time scanning objects for the GC is <em>reasonable</em>? Even if we did optimize our memory allocations to get that number to 3%, that‚Äôs a puny incremental gain‚Ä¶ It‚Äôs not going to get us to billions of cells! Should we switch to a non-GC‚Äôed language? Rust?! At a certain point, you‚Äôll go mad trying to turn a profile into a performance roadmap.</p><p>When analyzing a system top-down with a profiler, it‚Äôs easy to miss the forest for the trees. It helps to take a step back, and analyze the problem from first principles.¬†</p><p>We sat down and thought about fundamentally, what is a calculation engine? With some back-of-the-envelope calculations, what‚Äôs the upper bookend of how many cells we could reasonably expect the Calculation engine to support?</p><p>In my experience, first-principle thinking is <em>required</em> to break out of iterative improvement and make order of magnitude improvements. A profiler can‚Äôt be your only performance tool.</p><h3>Approaching the calculation engine from first principles</h3><p>To understand, we have to explain two concepts from Causal that help keep your spreadsheet organized: dimensions and variables.</p><p>We might have a variable &#34;Sales&#39;‚Äù that is broken down by the dimensions &#34;Product&#34; and &#34;Country&#34;. To appreciate how easy it is to build a giant model, if we have 100s of months, 10,000s of products, 10s of countries, and 100 variables we‚Äôve already created a model with 1B+ cells. In Causal ‚ÄúSales‚Äù looks like this:</p><p>‚Äç</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/62c439208c4cef29dfe9c924_h6l0EigghOUAE7KEKJ7GQHferBkXvLJxvuqxc-_dNIhoH9RUWZ-c5q8r8HXeA4kzJUTnsyylZVcqw4Ck2yyffotwiN2FdZVVr8IhWoH7--TDZx0u663k_Vby6dIhpvSfzSXkw7MZHbWxxKP4FA.png" alt=""/></p><figcaption>Sales model in Causal</figcaption></figure><p>In a first iteration we might represent Sales and its cells with a map. This seems innocent enough. Especially when you‚Äôre coming from an original implementation in Javascript, hastily ported to Go. As we‚Äôll learn in this blog post, there are several performance problems with this data structure, but we‚Äôll take it step by step:</p><div><pre><code>sales := make(map[int]*Cell)</code>
</pre></div><p>The integer index would be the <em>dimension index </em>to reference a specific cell. It is the index representing the specific dimension combination we‚Äôre interested in. For example, for Sales[Toy-A][Canada] the index would be 0 because Toy-A is the 0th Product Name and Canada is the 0th Country. For Sales[Toy-A][United Kingdom] it would be 1 (0th Toy, 1st Country), for Sales[Toy-C][India] it would be 3 * 3 = 9.¬†</p><p>An ostensible benefit of the map structure is that if a lot of cells are 0, then we don‚Äôt have to store those cells at all. In other words, this data structure seems useful for <em>sparse</em> models.</p><p>But to make the spreadsheet come alive, we to calculate formulas such as Net Profit = Sales * Profit. This simple equation shows the power of Causal‚Äôs dimensional calculations, as this will calculate each cell‚Äôs unique net profit!</p><p>Now that we have a simple mental model of how Causal‚Äôs calculation engine works, we can start reasoning about its performance from first principles.¬†</p><p>If we multiply two variables of 1B cells of 64 bit floating points each (<a href="#">~8 GiB memory</a>) into a third variable, then we have to traverse at least ~24 GiB of memory. If we naively assume this is sequential access (which hashmap access <em>isn‚Äôt</em>) and we have SIMD and multi-threading, we can process that memory at a rate of 30ms / 1 GiB, or ~700ms total (and <em>half</em> that time if we were willing to drop to 32-bit floating points and forgo some precision!).</p><p>So from first-principles, it seems <em>possible </em>to do calculations of billions of cells in less than a second. Of course, there‚Äôs far more complexity below the surface as we execute the many types of formulas, and computations on dimensions. But there‚Äôs reason for optimism! We will carry through this example of multiplying variables for Net Profit as it serves as a good proxy for the performance we can expect on large models, where typically you‚Äôll have fewer, smaller variables.</p><p>In the remainder of this post, we will try to close the gap between smaller Go prototypes and the napkin math. That should serve as evidence of what performance work to focus on in the 30,000+ line of code engine.</p><h3>Iteration 1: map[int]*Cell, 30m cells in ~6s ‚ùå¬†</h3><p>In Causal‚Äôs calculation engine each Cell in the map was initially ~88 bytes to store various information about the cell such as the formula, dependencies, and other references. We start our investigation by implementing this basic data-structure in Go.</p><p>With 10M cell variables, for a total of 30M cells, it takes almost 6s to compute the Net Profit = Sales * Profit calculation. These numbers from our prototype doesn‚Äôt include all the other overhead that naturally accompanies running in a larger code-base, that‚Äôs far more feature-complete. In the real engine, this takes a few times longer.</p><p>We want to be able to do <em>billions </em>in seconds with plenty of wiggle-room for necessary overhead, so 10s of millions in seconds won‚Äôt fly. We have to do better. We know from our napkin math, that we <em>should </em>be able to.</p><div><pre><code>$ go build main.go &amp;&amp; hyperfine ./main
Benchmark 1: ./napkin
  Time (mean ¬± œÉ):      5.828 s ¬±  0.032 s    [User: 10.543 s, System: 0.984 s]
  Range (min ... max):    5.791 s ...  5.881 s    10 runs</code>
</pre>

<pre><code>package main

import (
        &#34;math/rand&#34;
)

type Cell88 struct {
        padding [80]byte // just to simulate what would be real stuff
        value   float64
}

func main() {
        pointerMapIntegerIndex(10_000_000) // 3 variables = 30M total
}

func pointerMapIntegerIndex(nCells int) {
        one := make(map[int]*Cell88, nCells)
        two := make(map[int]*Cell88, nCells)
        res := make(map[int]*Cell88, nCells)

        rand := rand.New(rand.NewSource(0xCA0541))

        for i := 0; i &lt; nCells; i++ {
                one[i] = &amp;Cell88{value: rand.Float64()}
                two[i] = &amp;Cell88{value: rand.Float64()}
        }

        for i := 0; i &lt; nCells; i++ {
                res[i] = &amp;Cell88{value: one[i].value * two[i].value}
        }
}</code>
</pre></div><h3>Iteration 2: []Cell, 30m cells in ~400ms üòç</h3><p>In our napkin math, we assumed <em>sequential </em>memory access. But hashmaps don‚Äôt do sequential memory access. Perhaps this is a far larger offender than our profile above might seemingly suggest?</p><p>Well, how do hashmaps work? You hash a key to find the <em>bucket</em> that this key/value pair is stored in. In that bucket, you insert the key and value. When the average size of the buckets grows to around ~6.5 entries, the number of buckets will double and all the entries will get re-shuffled (fairly expensive, and a good size to pre-size your maps). The re-sizing occurs to about equality on a lot of keys in ever-increasing buckets.</p><p>‚Äç</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/62c439204d425cf630fd8a5b_ZQtWdDDKbcYRusK8ilA5_W_uST_YSJDhJUj-LVVpmdUozJG1fPLCFiLn46_SZFHXwLRqE5GeaShSXhFVRZauCKPVtbHjlHSrjoj-Ryy9iI_O0cgIqe1XlWPTzDIrngRnHSGOIToXzWFqd26pioU.png" alt=""/></p><figcaption>Inner workings of a hashmap</figcaption></figure><p>Let‚Äôs think about the performance implications of this from the ground up. Every time we look up a cell from its integer index, the operations we have to perform (and their performance, according to the <a href="https://github.com/sirupsen/napkin-math">napkin math reference</a>):</p><ol role="list"><li><strong>Hash</strong> the integer index to a hashed value: 25ns¬†</li><li><strong>Mask</strong> the hashed value to map it to a bucket: 1-5ns</li><li><strong>Random memory read</strong> to map the bucket to a pointer to the bucket‚Äôs address: 1ns (because it‚Äôll be in the cache)</li><li><strong>Random memory read</strong> to read the bucket: 50ns</li><li><strong>Equality</strong> operations on up to 6-7 entries in the bucket to locate the right key: 1-10ns</li><li><strong>Random memory read</strong> to follow and read the *Cell pointer: 50ns</li></ol><p>Most of this goes out the wash, by far the most expensive are these random memory reads that the map entails. Let‚Äôs say ~100ns per look-up, and we have ~30M of them, that‚Äôs ~3 seconds in hash lookups alone. That lines up with the performance we‚Äôre seeing. Fundamentally, it really seems like trouble to get to billions of cells with a map.</p><p>There‚Äôs another problem with our data structure in addition to all the pointer-chasing leading to slow random memory reads: the size of the cell. Each cell is 88 bytes. When a CPU reads memory, it fetches one <em>cache line</em> of 64 bytes at a time. In this case, the entire 88 byte cell doesn&#39;t fit in a single cache line. 88 bytes spans two cache lines, with 128 - 88 = 40 bytes of wasteful fetching of our precious memory bottleneck!</p><p>If those 40 bytes belonged to the next cell, that‚Äôs not a big deal, since we‚Äôre about to use them anyway. However, in this random-memory-read heavy world of using a hashmap that stores pointers, we can‚Äôt trust that cells will be adjacent. This is enormously wasteful for our precious memory bandwidth.</p><p>In the <a href="https://github.com/sirupsen/napkin-math">napkin math reference</a>, random memory reads are <em>~50x slower than sequential access</em>. A huge reason for this is that the CPU‚Äôs memory prefetcher cannot predict memory access. Accessing memory is one of the slowest things a CPU does, and if it can‚Äôt preload cache lines, we‚Äôre spending <em>a lot </em>of time stalled on memory.</p><p>Could we give up the map? We mentioned earlier that a nice property of the map is that it allows us to build sparse models with lots of empty cells. For example, cohort models tend to have half of their cells empty. But perhaps half of the cells being empty is not quite enough to qualify as ‚Äòsparse‚Äô?</p><p>We could consider mapping the index for the cells into a large, pre-allocated array. Then cell access would be just a <em>single </em>random-read of 50ns! In fact, it‚Äôs even better than that: In this particular Net Profit, all the memory access is sequential. This means that the CPU can be smart and prefetch memory because it can reasonably predict what we‚Äôll access next. For a single thread, we know we can do about <a href="https://github.com/sirupsen/napkin-math#numbers">1 GiB/100ms</a>. This is about 30M * 88 bytes ~= 2.5 GiB, so it should take somewhere in the ballpark of 250-300ms. Consider also that the allocations themselves on the first few lines take a bit of time.</p><div><pre><code>func arrayCellValues(nCells int) {
        one := make([]Cell88, nCells)
        two := make([]Cell88, nCells)
        res := make([]Cell88, nCells)

        rand := rand.New(rand.NewSource(0xCA0541))

        for i := 0; i &lt; nCells; i++ {
                one[i].value = rand.Float64()
                two[i].value = rand.Float64()
        }

        for i := 0; i &lt; nCells; i++ {
                res[i].value = one[i].value * two[i].value
        }
}</code>
</pre>

<pre><code>napkin:go2 $ go build main.go &amp;&amp;  hyperfine ./main
Benchmark 1: ./main
  Time (mean ¬± œÉ):     346.4 ms ¬±  21.1 ms    [User: 177.7 ms, System: 171.1 ms]
  Range (min ... max):   332.5 ms ... 404.4 ms    10 runs</code>
</pre></div><p>That‚Äôs great! And it tracks our expectations from our napkin math well (the extra overhead is partially from the random number generator).</p><h3>Iteration 3: Threading, 250ms ü§î</h3><p>Generally, we expect threading to speed things up substantially as we‚Äôre able to utilize more cores. However, in this case, we‚Äôre memory bound, not computationally bound. We‚Äôre just doing simple calculations between the cells, which is generally the case in real Causal models. Multiplying numbers takes single-digit cycles, fetching memory takes double to triple-digit number of cycles. Compute bound workloads scale well with cores. Memory bound workloads act differently when scaled up.</p><p>If we look at raw memory bandwidth numbers in the <a href="https://github.com/sirupsen/napkin-math#numbers">napkin math reference</a>, a 3x speed-up in a memory-bound workload seems to be our ceiling. In other words, if you‚Äôre memory bound, you only need about ~3-4 cores to exhaust memory bandwidth. More won‚Äôt help much. But they do help, because a single thread <a href="https://news.ycombinator.com/item?id=16174813">cannot exhaust memory bandwidth on most CPUs</a>.</p><p>When <a href="https://gist.github.com/sirupsen/d413b130d0f45d0d35d0bc85b9071abb">implemented</a> however, we only get a 0.6x speedup (400ms ‚Üí 250ms), and not a 3x speed-up (130ms)? I am frankly not sure how to explain this ~120ms gap. If anyone has a theory, <a href="mailto:lukas@causal.app">we‚Äôd love to hear it</a>!</p><p>Either way, we definitely seem to be memory bound now. Then there‚Äôs only two ways forward: (1) Get more memory bandwidth on a different machine, or (2) Reduce the amount of memory we‚Äôre using. Let‚Äôs try to find some more brrr with (2).</p><h3>Iteration 4: Smaller Cells, 88 bytes ‚Üí 32 bytes, 70ms üòç</h3><p>If we were able to cut the cell size 3x from 88 bytes to 32 bytes, we‚Äôd expect the performance to roughly 3x as well! In our simulation tool, we‚Äôll reduce the size of the cell:</p><div><pre><code>type Cell32 struct {
    padding [24]byte
    value   float64
}</code>
</pre></div><p>Indeed, with the threading on top, <a href="https://gist.github.com/sirupsen/d413b130d0f45d0d35d0bc85b9071abb">this gets us to ~70ms</a> which is just around a 3x improvement!</p><p>In fact, what <em>is </em>even in that cell struct? The cell stores things like formulas, but for many cells, we don‚Äôt <em>actually</em> need the formula stored with the cell. For most cells in Causal, the formula is the same as the <em>previous</em> cell. I won‚Äôt show the original struct, because it‚Äôs confusing, but there are other pointers, e.g. to the parent variable. By more carefully writing the calculation engine‚Äôs interpreter to keep track of the context, we should be able to remove various pointers to e.g. the parent variable. Often, structs get expanded with cruft as a quick way to break through some logic barrier, rather than carefully executing the surrounding context to provide this information on the stack.¬†</p><p>As a general pattern, we can reduce the size of the cell by switching from an <em>array of structs </em>design to a <em>struct of arrays </em>design, in other words, if we‚Äôre in a cell with index 328, and need the formula for the cell, we could look up index 328 in a formula array. These are called <em>parallel arrays</em>. Even if we access a different formula for every single cell the CPU is smart enough to detect that it‚Äôs another sequential access. This is generally much faster than using pointers.</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/62c43920f23265c9ade897e1_T4czwNktnIV3b6AeBliU9JwheL6nfA1i5yWencyuFFNNeoYLgQuRTBUtHM0gCoL9A-8g3jSyHD1NK-4ODUrhaPRPXDv21TNS6h5WbEwMwSNdVH84AoeBk5KoGBjogqZdwIInW-ELiKA5v5pwqMs.png" alt=""/></p><figcaption>Array of structs to struct of arrays</figcaption></figure><p>None of this is particularly hard to do, but it wasn‚Äôt until now that we realized how paramount this was to the engine‚Äôs performance! Unfortunately, the profiler isn&#39;t yet helpful enough to tell you that reducing the size of a struct below that 64-byte threshold can lead to non-linear performance increases. You need to know to use tools like <a href="https://linux.die.net/man/1/pahole">pahole(1)</a> for that.</p><h3>Iteration 5: []float64 w/ Parallel Arrays, 20ms ü§§</h3><p>If we want to find the absolute speed-limit for Causal‚Äôs performance then, we‚Äôd want to imagine that the Cell is just:</p><div><pre><code>type Cell8 struct {
    value   float64
}</code>
</pre></div><p>That‚Äôs a total memory usage of¬† 30 * 8 byte = 228 MiB which we can read at <a href="https://github.com/sirupsen/napkin-math#numbers">35 Œºs / 1 MiB</a> in a threaded program, so ~8ms. We won‚Äôt get much faster than this, since we also inevitably have to spend time allocating the memory.</p><p>When <a href="https://gist.github.com/sirupsen/d413b130d0f45d0d35d0bc85b9071abb">implemented</a>, the raw floats take ~20ms (consider that we have to allocate the memory too) for our 30M cells.</p><p>Let‚Äôs scale it up. For <a href="https://gist.github.com/sirupsen/d413b130d0f45d0d35d0bc85b9071abb#file-simulator-go-L38">1B cells</a>, this takes ~3.5s. That‚Äôs pretty good! Especially considering that the Calculation engine already has a lot of caching already to ensure we don‚Äôt have to re-evaluate every cell in the sheet. But, we want to make sure that the worst-case of evaluating the entire sheet performs well, and we have some space for inevitable overhead.</p><p>Our initial napkin math suggested we could get to ~700ms for 3B cells, so there‚Äôs a bit of a gap. We get to ~2.4s for 1B cells by <a href="https://gist.github.com/sirupsen/d413b130d0f45d0d35d0bc85b9071abb#file-simulator-go-L133">moving allocations into the threads that actually need them</a>, closing the gap further would take some more investigation. However, localizing allocations start to get into a territory of what would be quite hard to implement generically in reality‚Äîso we‚Äôll stop around here until we have the luxury of this problem being the bottleneck. Plenty of work to make all these transitions in a big, production code-base!</p><h3>Iteration N: SIMD, compression, GPU ‚Ä¶¬†</h3><p>That said, there are <em>lots</em> of optimizations we can do. Go‚Äôs compiler currently doesn‚Äôt do SIMD, which allows us to get <em>even more</em> memory bandwidth. Another path for optimization that‚Äôs common for number-heavy programs is to encode the numbers, e.g. delta-encoding. Because we‚Äôre constrained by memory bandwidth more than compute, counter-intuitively, compression can make the program <em>faster</em>. Since the CPU is stalled for tons of cycles while waiting for memory access, we can use these extra cycles to do simple arithmetic to decompress.</p><p>Another trend from the AI-community when it comes to number-crunching too is to leverage GPUs. These have <em>enormous </em>memory bandwidth. However, we can create serious bottlenecks when it comes to moving memory back and forth between the CPU and GPU. We‚Äôd have to learn what kinds of models would take advantage of this, we have little experience with GPUs as a team‚Äîbut we may be able to utilize lots of existing ND-array implementations used for training neural nets. This would come with significant complexity‚Äîbut also serious performance improvements for large models.</p><p>Either way there‚Äôs <em>plenty </em>of work to get to the faster, simpler design described above in the code-base. This would be further out, but makes us excited about the engineering ahead of us!</p><h3>Conclusion</h3><p>Profiling had become a dead-end to make the calculation engine faster, so we needed a different approach. Rethinking the core data structure from first principles, and understanding exactly why each part of the current data structure and access patterns was slow got us out of disappointing, iterative single-digit percentage performance improvements, and unlocked order of magnitude improvements. This way of thinking about designing software is often referred to as data-oriented engineering, and <a href="https://media.handmade-seattle.com/practical-data-oriented-design/">this talk by Andrew Kelly</a>, the author of the Zig compiler, is an excellent primer that was inspirational to the team.</p><p>With these results, we were able to build a technical roadmap for incrementally moving the engine towards a more data-oriented design. The reality is <em>far </em>more complicated, as the calculation engine is north of 40K lines of code. But this investigation gave us confidence in the effort required to change the core of how the engine works, and the performance improvements that will come over time!</p><p>The biggest performance take-aways for us were:</p><ul role="list"><li>When you‚Äôre stuck with performance on profilers, start thinking about the problem from first principles</li><li>Use indices, not pointers when possible</li><li>Use array of structs when you access almost everything all the time, use struct of arrays when you don‚Äôt</li><li>Use arrays instead of maps when possible; the data needs to be <em>very </em>sparse for the memory savings to be worth it</li><li>Memory bandwidth is precious, and you can‚Äôt just parallelize your way out of it!</li></ul><p>Causal doesn‚Äôt smoothly support 1 billion cells yet, but we feel confident in our ability to iterate our way there. Since starting this work, our small team has already improved performance more than 3x on real models. If you‚Äôre interested in working on this with us, and help us get to 10s of billions of cells, you should consider joining the Causal team ‚Äî email <a href="mailto:lukas@causal.app">lukas@causal.app</a>!</p></div></div></div></div></div></div>
  </body>
</html>
