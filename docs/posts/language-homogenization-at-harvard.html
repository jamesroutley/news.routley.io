<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://inteoryx.com/htmls/LanguageHomog.html">Original</a>
    <h1>Language homogenization at Harvard</h1>
    
    <div id="readability-page-1" class="page"><div id="whole">
        
        <div>
            
            <p>On December 20, 2021 I read an <a href="https://cspicenter.org/reports/increasing-politicization-and-homogeneity-in-scientific-funding-an-analysis-of-nsf-grants-1990-2020/">interesting paper from CSPI</a> written by Leif Rasmussen.  My main takeaway was that among National Science Foundation grant award abstracts there is what Rasmussen calls &#34;Constriction of the space of ideas.&#34;  In other words, the language of NSF grant abstracts is becoming increasingly similar.</p>
            <p>The constriction is demonstrated by a decrease in cosine similarity among word embeddings and word frequency vectors between the years 1990 and 2020.  Now, I know that sentence was perfectly clear, but, just for fun, let&#39;s break that down a little.</p>
            <h2>Background</h2>
            <p>Suppose we were going to rate words on a scale of one to ten in two dimensions.  First, we are going to rate how old fashioned a word is.  Second, we will rate how funny the word seems to us.  For each word we will get a pair of numbers.</p>
            <p>For example, if we wanted to rate &#34;poppycock&#34; we might say that it&#39;s a 9 for &#34;old fashioned&#34; and an 8 for &#34;Funny&#34;.  Poppycock would be 9, 8.  Another word, like snail, isn&#39;t especially old fashioned or funny.  Though, the word has been around a while - we could call it a 5, 2.
            </p>
            <p>Now that we can turn any word into a pair of numbers we can also turn a sequence of words, called a &#34;document&#34;, into a pair of numbers.  Just convert all the words in the document into pairs of numbers and then find the average of those values.</p>
            <p><i>My snail speaks poppycock.</i> = 7, 4</p>
            <p>We can treat the pair of numbers as a coordinate in two dimensional space.  We can imagine an arrow drawn from the origin (0, 0) with an arrow head at our coordinate.  A document, which could be anything from a single word, to a simple sentence, or even a full length novel, can be an arrow in 2-d space.  Two documents become two arrows.
            </p>
            <p><img src="https://inteoryx.com/images/ExampleVector.jpg" alt="Example of two documents in 2-d space as arrows." onclick="window.open(this.src)"/></p><p>We can then find the <a href="https://en.wikipedia.org/wiki/Cosi ne_similarity">cosine similarity</a> which is the cosine of the angle between the two arrows.  The cosine similarity gives us a measure, between zero and one, of how similar the two vectors are.</p>
            <p>In our example the cosine similarity between our two vectors is 0.987.  That tells us that our documents are similarly composed of old timey silly words.  Which is true.</p>
            <p>Cosine difference is just 1 - cosine similarity.  It&#39;s a measure of how different two vectors are.  Smaller difference and the vectors are more similar.</p>
            <p>Our sentences aren&#39;t really that similar though.  They only appear to be similar because we are just evaluating them on two dimensions - old-timeyness and silliness.  What if we were to evaluate the sentences on 96 dimensions?  Or 300?  As we expand the number of dimensions we are evaluating we expand our description of words and the extent to which we can tell whether words, or documents, are similar or different.</p>
            <p>Evaluating every word on 300 dimensions would be tedious.  Luckily machine learning models can now do that for us.  A model can process a bunch of text and learn to represent words as vectors.  Once that is in place, we can do the same evaluation from our example, just in 300 dimensions instead of 2 dimensions - the math is the same.</p>
            <p>Now that we understand the vocabulary, let&#39;s review what the CSPI paper did.  </p>
            <ol>
                <li>Get all of the accepted grant abstracts between 1990 and 2020.</li>
                <li>Turn each of these documents into a vector representing the average vector of all the word vectors of each word in the document.</li>
                <li>Go year by year and calculate the average cosine distance between grant abstracts.</li>
                <li>Finally, plot the cosine distance over time to reveal a decline.</li>
            </ol>
            <p><img src="https://cspicenter.org/wp-content/uploads/2021/11/Screen-Shot-2021-11-16-at-23.16.21-1-1200x901.png" alt="CSPI graph illustrating decline in average cosine similarity for word embeddings" onclick="window.open(this.src)"/></p><p>What that graph shows is a decline in the cosine distance.  The abstracts are getting increasingly similar.  Or, as Rasmussen put it - &#34;Constriction of the space of ideas.&#34;</p>
            <h2>Language at The Crimson</h2>
            <p>Since reading the CSPI paper I&#39;ve been on the lookout for a dataset to which I could apply a similar analysis.  I found one in Harvard&#39;s <a href="https://www.thecrimson.com/">The Crimson</a> which is the university&#39;s daily newspaper that has <a href="https://www.thecrimson.com/sitemap/">an archive</a> going back to before 1900.</p>
            <p>I scraped all of the articles between 1900 and 2020 to create my dataset.  Dataset available as a torrent <span onclick="document.getElementById(&#39;magnetLink&#39;).classList.remove(&#39;hidden&#39;);">here</span><span id="magnetLink"> magnet:?xt=urn:btih:a5455b4aee06e0397c1d2b31a031fd2d3cde9247&amp;dn=CrimsonArticles.zip</span>.</p>
            <p>I then used <a href="https://spacy.io/">spaCy&#39;s</a> pretrained &#34;en_core_web_sm&#34; model for word embeddings to compute an annual average cosine distance and plot it.  What I found, will shock you...</p>
            <p><img src="https://inteoryx.com/images/AverageCosineDistance.png" onclick="window.open(this.src)"/></p><p>Okay, maybe it won&#39;t shock you.  But, what I found is similar to Rasmussen&#39;s result.  A steady decline in average annual cosine distance between articles.  Each dot shows an annual average cosine distance for a year between 1900 and 2020 and the solid line represents a regression plot illustrating the trend in cosine distance.</p>
            <p>Can we say anything about what causes this?  Not definitively, no.  Rasmussen connects the decline in lexical diversity with an increase in diversity words - equity, diversity, inclusion, gender, marginalize, underrepresented, and disparity.</p>
            <p>Rasmussen does not say exactly this, but my sense from reading his paper is that (he thinks that) NSF grant applicants are becoming more focused on the political notion of diversity and that focus is constricting their ideas making their writing more and more similar to other applicants over time.</p>
            <p>We can illustrate this idea with The Crimson dataset by adding a line to our graph that represents the percentage of news articles in a year containing at least one of our diversity words.</p>
            <p><img src="https://inteoryx.com/images/DistanceAndDiversity.png" onclick="window.open(this.src)"/></p><p>The right hand y-axis shows the percentage of articles for a given year that contain at least one diversity word.</p>
            <p>On the one hand, distance and diversity words are inversely correlated.  The correlation between the two is -0.68.  Diversity words go up as distance goes down.  </p>
            <p>On the other hand, that&#39;s not too surprising.  The diversity words are new (new-ish).  Anything else newish would be inversely correlated with cosine distance too.  For example, I repeated the same experiment with &#34;tech&#34; words like Google, YouTube, iPhone, browser, and so on.  Those tech words correlated at -0.6 with cosine distance.</p>
            <p>Another argument against connecting distance and diversity is that distance is on a long running decline from 1900 even for the first four decades while diversity words were basically flat.  When diversity words pop in the 90&#39;s there isn&#39;t an immediate reaction in cosine distance, it&#39;s only about a decade later, in 2000, that cosine distance takes a steep drop.</p>
            <p>I don&#39;t really have a better idea than Rasmussen.  I&#39;m certainly sympathetic to the notion that, if 20% of your news articles are at least mentioning diversity, then that is an inherent focus (or constriction) of your idea-space.  But, I don&#39;t really see it in the graph.</p>
            <p>I&#39;m using a different dataset than Rasmussen - he looked at NSF applications and I looked at The Crimson articles.  But, in both datasets we see the same trend.  A decline in lexical diversity as measured by cosine distance of document vectors from word embeddings AND an increase in documents using diversity words.  It&#39;s interesting, even though I don&#39;t really know what is going with this.</p>
            
            
        </div>
        
        
        
    </div></div>
  </body>
</html>
