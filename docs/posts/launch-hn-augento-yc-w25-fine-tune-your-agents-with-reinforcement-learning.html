<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=43537505">Original</a>
    <h1>Launch HN: Augento (YC W25) – Fine-tune your agents with reinforcement learning</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hi HN, we’re the cofounders of Augento (<a href="https://augento.ai/">https://augento.ai/</a>). We’re building Deepseek R1-like fine-tuning as a service. You connect your agent, tell us when it’s right or wrong, and we deliver an LLM optimized for that agent. There’s a demo video <a href="https://www.youtube.com/watch?v=j5RQaTdRrKE" rel="nofollow">https://www.youtube.com/watch?v=j5RQaTdRrKE</a>, and our docs are at <a href="https://docs.augento.ai/">https://docs.augento.ai/</a>. It’s open for anyone to use at <a href="https://augento.ai">https://augento.ai</a>.</p><p>Agents fail all the time, especially when you try to use them for something actually useful. Current solution approaches suck: prompting has intrinsic limits and supervised fine-tuning requires big explicit datasets that are hard to collect.</p><p>Two months ago, the DeepSeek R1 paper outlined a way to post-train LLMs with (almost) pure reinforcement learning. We took up their research and built a fine-tuning platform around that.</p><p>You let us intercept your agent&#39;s data flow, and we deliver you a fine-tuned open-source model, that is trained on the agent&#39;s specific task. Instead of providing big datasets of explicit fine-tuning samples, you provide a reward function, judging the model&#39;s outputs.</p><p>Here are examples of what this can be used for:</p><p>Coding Agent: We fine-tuned a coding agent that was constantly making syntax errors and failed to handle semantic edge cases properly. By providing a reward function that evaluated code against the compiler, the agent learned not to produce these errors. The fine-tuned model reduced critical bugs by 40% with just 20 training samples.</p><p>MCP Tool Specialization: Imagine you have a custom set of internal tools using the MCP protocol, but your agent keeps selecting the wrong tool or passing incompatible parameters. You could fine-tune with a reward function that scores tool selection and parameter matching.</p><p>Browser Agent Navigation: If you&#39;re building a browser agent that struggles with complex web UIs or specific sites, you could fine-tune it to better understand UI elements and navigation patterns. With a reward function that scores successful task completion (like &#34;find the best price for this product&#34; or &#34;complete this multi-step form&#34;), you could train an agent that better identifies clickable elements, understands form validation errors, and navigates through complex SPAs without getting stuck.</p><p>VLA Robot Control: If you&#39;re using vision-language models to control robotic arms or other hardware, you could fine-tune for your specific actuator setup. With a reward function based on high-level task completion, you could train a Vision-Langauge-Action (VLA) model that translates natural language commands like &#34;move the red block behind the blue cylinder&#34; into actuator controls for your specific hardware.</p><p>As you see from these examples, the current paradigm is best suited for &#34;verifiable domains”, where it is possible to give an explicit function judging the model’s outputs. However, up next, we will also support an &#34;alignment mode&#34;, where you don&#39;t have to provide a reward function but provide high-level feedback on past failure runs of your agent. Just tag where things went wrong, and we&#39;ll handle the rest. This makes it even easier to improve your agents without needing to write formal reward functions.</p><p>Our platform is not itself open source, but it fine-tunes open-source language models. I.e. it is an alternative to the reinforcement fine-tuning API from OpenAI, but with Qwen, LLama, Deepseek, etc., and more customizability on the reward model. We charge users for the training and for their inference/interaction with the model later on ($0 monthly flat fee + training cost + inference cost).</p><p>The platform is self-serving and open to use at <a href="https://augento.ai/dashboard">https://augento.ai/dashboard</a>. We’ll give you $20 in training credits, which should be enough for connecting your agent and delivering some observable improvement on your use case.</p><p>We’d love to hear your thoughts and feedback!</p></div></td></div></div>
  </body>
</html>
