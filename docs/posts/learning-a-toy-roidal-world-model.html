<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://irvin.quest/toroidal-world/">Original</a>
    <h1>Learning a Toy-roidal World Model</h1>
    
    <div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/Article"><header><p>January 27, 2026</p></header><section itemprop="articleBody"><p>I decided to try out a transformer based architecture after my <a href="https://grid.space/minimum-viable-world/">initial explorations</a> into world modelling with a U-Net based diffusion approach. The new architecture combined a vision transformer-based masked autoencoder (MAE) with a transformer-based predictor. The predictor interleaved encoded frames with encoded actions to predict the next frame in latent space, which the MAE decoder then reconstructed. You can see an initial version of this <a href="https://github.com/ih/developmental-robot-movement/commit/e5d6a40d18e88975ade5d7a8f35ecc140461a2d3#diff-9764b72c2d921e14e0a42829f23e8eff03c53bd9f361b15b84f5ea69896c3eee">here</a>. I spent a few months trying to get something working on the data from my JetBot, but didn’t see much progress. Despite receiving a sequence of frames and actions, the model ignored the action data; it simply used the last frame as its prediction for the next. My <a href="https://docs.google.com/document/d/e/2PACX-1vTdVqbwuou38bDDVrR4LonrjLcE2SXu6SYXUpeU9nmfKAc9raojYJW40eqHxlj8fqNR1FU9o24JCzPX/pub">log</a> up to Day 43 covers what I tried (more recently I found some evidence the JetBot data quality might not be very good, but that’s a different story). I decided to simplify the problem and ended up with a dot on a screen that would either move to the right or not based on a binary action. The dot would wrap around to the other side of the screen as it continued so I called this the toroidal dot world.</p>
<p><img src="https://grid.space/8caba09455a8e2f3cb5246b926edef60/toroidal-dot.gif" alt="toroidal dot"/></p>
<p>I started by collecting and training on data of a dot at a fixed y position moving across the screen. The model struggled, consistently predicting either a black frame or simply repeating the previous frame, depending on whether I prioritized latent feature loss or pixel-level loss. The predictor wasn’t really working, but the autoencoder seemed to do a good job at reconstructing images so I thought I’d try recasting the problem to just use an autoencoder. Rather than treating prediction as a sequence of discrete images and actions, I combined everything into a single image—a ‘canvas.’ In this format, frames are concatenated side-by-side, separated by colored bars that encode the preceding action.</p>
<p><span>
      <a href="https://grid.space/static/4260cddf8c658b8d497591301be557a7/5faa8/canvas-example.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="canvas example" title="canvas example" src="https://grid.space/static/4260cddf8c658b8d497591301be557a7/f058b/canvas-example.png" srcset="/static/4260cddf8c658b8d497591301be557a7/c26ae/canvas-example.png 158w,
/static/4260cddf8c658b8d497591301be557a7/6bdcf/canvas-example.png 315w,
/static/4260cddf8c658b8d497591301be557a7/f058b/canvas-example.png 630w,
/static/4260cddf8c658b8d497591301be557a7/40601/canvas-example.png 945w,
/static/4260cddf8c658b8d497591301be557a7/78612/canvas-example.png 1260w,
/static/4260cddf8c658b8d497591301be557a7/5faa8/canvas-example.png 1444w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy" decoding="async"/>
  </a>
    </span></p>
<p>This reframed prediction as an inpainting task: masking the last frame in the canvas and asking the model to fill it in. This allowed me to simplify the model to just the MAE, adapted to process canvases (<a href="https://github.com/ih/developmental-robot-movement/commit/4aad90081cc3a57663943d3964c27cc7a2ac034b">code here</a>).</p>
<p>This approach didn’t work initially. The inpainting of the last part of the canvas would be mostly black. After experimenting with a loss function that combined standard mean squared error (MSE) and a focal weighted MSE I began to see some progress. The focal weighted MSE applies exponential weighting to errors, placing extra emphasis on areas where the prediction error is high. This hybrid loss started giving predictions of multiple dots roughly where the true dot might be.</p>
<p><span>
      <a href="https://grid.space/static/3a64f41f94b0230877909e853636857b/5b2ff/ghosting.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="ghosting" title="ghosting" src="https://grid.space/static/3a64f41f94b0230877909e853636857b/f058b/ghosting.png" srcset="/static/3a64f41f94b0230877909e853636857b/c26ae/ghosting.png 158w,
/static/3a64f41f94b0230877909e853636857b/6bdcf/ghosting.png 315w,
/static/3a64f41f94b0230877909e853636857b/f058b/ghosting.png 630w,
/static/3a64f41f94b0230877909e853636857b/40601/ghosting.png 945w,
/static/3a64f41f94b0230877909e853636857b/78612/ghosting.png 1260w,
/static/3a64f41f94b0230877909e853636857b/5b2ff/ghosting.png 2190w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy" decoding="async"/>
  </a>
    </span></p>
<p>After tuning hyperparameters and speeding up the training pipeline so I could iterate faster, I trained a model that makes accurate action-conditioned predictions. I then made the toroidal world slightly harder by randomizing the dot’s starting y-position; the model still performed well on a held-out validation set (Days 66-73).</p>
<p><span>
      <a href="https://grid.space/static/ea75dbf5de6e46322b12b644d4a4668d/12f7a/counterfactual-inference-success.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="canvas example" title="canvas example" src="https://grid.space/static/ea75dbf5de6e46322b12b644d4a4668d/f058b/counterfactual-inference-success.png" srcset="/static/ea75dbf5de6e46322b12b644d4a4668d/c26ae/counterfactual-inference-success.png 158w,
/static/ea75dbf5de6e46322b12b644d4a4668d/6bdcf/counterfactual-inference-success.png 315w,
/static/ea75dbf5de6e46322b12b644d4a4668d/f058b/counterfactual-inference-success.png 630w,
/static/ea75dbf5de6e46322b12b644d4a4668d/40601/counterfactual-inference-success.png 945w,
/static/ea75dbf5de6e46322b12b644d4a4668d/78612/counterfactual-inference-success.png 1260w,
/static/ea75dbf5de6e46322b12b644d4a4668d/12f7a/counterfactual-inference-success.png 3678w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy" decoding="async"/>
  </a>
    </span></p>
<p>I’m now applying the same idea to real robot data. I’m starting with the LeRobot SO-101 arm, focusing on learning an action-conditioned model of a single joint before scaling up to more complex motion.</p></section><hr/></article></div>
  </body>
</html>
