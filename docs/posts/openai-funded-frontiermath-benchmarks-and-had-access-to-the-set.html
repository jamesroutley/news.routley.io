<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform">Original</a>
    <h1>OpenAI funded FrontierMath Benchmarks and had access to the set</h1>
    
    <div id="readability-page-1" class="page"><div><p><span><span><div><div><div><div><div id="FR5bGBmCkcoGniY9m"><div><div><div><div><div><p><strong>FrontierMath was funded by OpenAI.</strong><span data-footnote-reference="" data-footnote-index="1" data-footnote-id="myj4sfqnu7c" role="doc-noteref" id="fnrefmyj4sfqnu7c"><sup><a href="#fnmyj4sfqnu7c">[1]</a></sup></span></p><p>The communication about this has been non-transparent, and many people, including contractors working on this dataset, have not been aware of this connection. <i>Thanks to </i><a href="https://www.lesswrong.com/users/7vik?from=post_header"><i>7vik</i></a><i> for their contribution to this post.</i></p><p>Before Dec 20th (the day OpenAI announced o3) there was no public communication about OpenAI funding this benchmark. Previous Arxiv versions v1-v4 do not acknowledge OpenAI for their support. This support was made public on Dec 20th.<span data-footnote-reference="" data-footnote-index="1" data-footnote-id="myj4sfqnu7c" role="doc-noteref" id="fnrefmyj4sfqnu7c"><sup><a href="#fnmyj4sfqnu7c">[1]</a></sup></span></p><p>Because the Arxiv version mentioning OpenAI contribution came out right after o3 announcement, I&#39;d guess Epoch AI had some agreement with OpenAI to not mention it publicly until then.</p><p>The mathematicians creating the problems for FrontierMath were not (actively)<span data-footnote-reference="" data-footnote-index="2" data-footnote-id="7ud6hc06t7a" role="doc-noteref" id="fnref7ud6hc06t7a"><sup><a href="#fn7ud6hc06t7a">[2]</a></sup></span> communicated to about funding from OpenAI. The contractors were instructed to be secure about the exercises and their solutions, including not using Overleaf or Colab or emailing about the problems, and signing NDAs, &#34;to ensure the questions remain confidential&#34; and to avoid leakage. The contractors were also not communicated to about OpenAI funding on December 20th. I believe there were named authors of the paper that had no idea about OpenAI funding.</p><p>I believe the impression for most people, and for most contractors, was &#34;<i>This benchmark’s questions and answers will be kept fully private, and the benchmark will only be run by Epoch. Short of the companies fishing out the questions from API logs (which seems quite unlikely), this shouldn’t be a problem.</i>&#34;<span data-footnote-reference="" data-footnote-index="3" data-footnote-id="mx3of151t1" role="doc-noteref" id="fnrefmx3of151t1"><sup><a href="#fnmx3of151t1">[3]</a></sup></span></p><p>Now Epoch AI or OpenAI don&#39;t say publicly that OpenAI has access to the exercises or answers or solutions. I have heard second-hand that OpenAI does have access to exercises and answers and that they use them for validation. I am not aware of an agreement between Epoch AI and OpenAI that prohibits using this dataset for training if they wanted to, and have slight evidence against such an agreement existing.</p><p>In my view Epoch AI should have disclosed OpenAI funding, and contractors should have transparent information about the potential of their work being used for capabilities, when choosing whether to work on a benchmark.</p><ol data-footnote-section="" role="doc-endnotes"><li data-footnote-item="" data-footnote-index="1" data-footnote-id="myj4sfqnu7c" role="doc-endnote" id="fnmyj4sfqnu7c"><span data-footnote-back-link="" data-footnote-id="myj4sfqnu7c"><sup><strong><a href="#fnrefmyj4sfqnu7c">^</a></strong></sup></span><p><a href="https://arxiv.org/pdf/2411.04872">Arxiv v5</a> (Dec 20th version) &#34;We gratefully acknowledge OpenAI for their support in creating the benchmark.&#34;</p></li><li data-footnote-item="" data-footnote-index="2" data-footnote-id="7ud6hc06t7a" role="doc-endnote" id="fn7ud6hc06t7a"><span data-footnote-back-link="" data-footnote-id="7ud6hc06t7a"><sup><strong><a href="#fnref7ud6hc06t7a">^</a></strong></sup></span><p>I do not know if they have disclosed it in neutral questions about who is funding this.</p></li><li data-footnote-item="" data-footnote-index="3" data-footnote-id="mx3of151t1" role="doc-endnote" id="fnmx3of151t1"><span data-footnote-back-link="" data-footnote-id="mx3of151t1"><sup><strong><a href="#fnrefmx3of151t1">^</a></strong></sup></span><p>This is from a <a href="https://news.ycombinator.com/item?id=42094546">comment</a> by a non-Epoch AI person on HackerNews from two months ago. Another example: Ars Technica writes &#34;FrontierMath&#39;s difficult questions remain unpublished so that AI companies can&#39;t train against it.&#34; in a <a href="https://arstechnica.com/ai/2024/11/new-secret-math-benchmark-stumps-ai-models-and-phds-alike/">news article</a> from November.</p></li></ol></div></div></div></div></div><div><div><div id="veedfswdCYKZEhptz"><div><div><div><div><div><p>Tamay from Epoch AI here.</p><p>We made a mistake in not being more transparent about OpenAI&#39;s involvement. We were restricted from disclosing the partnership until around the time o3 launched, and in hindsight we should have negotiated harder for the ability to be transparent to the benchmark contributors as soon as possible. Our contract specifically prevented us from disclosing information about the funding source and the fact that OpenAI has data access to much but not all of the dataset. We own this error and are committed to doing better in the future.</p><p>For future collaborations, we will strive to improve transparency wherever possible, ensuring contributors have clearer information about funding sources, data access, and usage purposes at the outset. While we did communicate that we received lab funding to some mathematicians, we didn&#39;t do this systematically and did not name the lab we worked with. This inconsistent communication was a mistake. We should have pushed harder for the ability to be transparent about this partnership from the start, particularly with the mathematicians creating the problems.</p><p>Getting permission to disclose OpenAI&#39;s involvement only around the o3 launch wasn&#39;t good enough. Our mathematicians deserved to know who might have access to their work. Even though we were contractually limited in what we could say, we should have made transparency with our contributors a non-negotiable part of our agreement with OpenAI.</p><p>Regarding training usage: We acknowledge that OpenAI does have access to a large fraction of FrontierMath problems and solutions, with the exception of a unseen-by-OpenAI hold-out set that enables us to independently verify model capabilities. However, we have a verbal agreement that these materials will not be used in model training. </p><p>Relevant OpenAI employees’ <a href="https://x.com/__nmca__/status/1870170112290107540"><u>public communications</u></a> have described FrontierMath as a &#39;strongly held out&#39; evaluation set. While this public positioning aligns with our understanding, I would also emphasize more broadly that labs benefit greatly from having truly uncontaminated test sets.</p><p>OpenAI has also been fully supportive of our decision to maintain a separate, unseen holdout set—an extra safeguard to prevent overfitting and ensure accurate progress measurement. From day one, FrontierMath was conceived and presented as an evaluation tool, and we believe these arrangements reflect that purpose. </p><p>[Edit: Clarified OpenAI&#39;s data access - they do not have access to a separate holdout set that serves as an additional safeguard for independent verification.]</p></div></div></div></div></div><div><div><div id="6seHG3rtAz2nfMZgk"><div><div><div><div><div><blockquote><p>we have a verbal agreement that these materials will not be used in model training</p></blockquote><p>Get that agreement in writing.</p><p>I am happy to bet 1:1 OpenAI will refuse to make an agreement in writing to not use the problems/the answers for testing.</p><p>You have done work that contributes to AI capabilities, and you have misled mathematicians who contributed to that work about its nature.</p></div></div></div></div></div><div><div><div id="cB9dZ3iHHoTYrBGdA"><div><div><div><div><div><blockquote>
<p>Get that agreement in writing.</p>
</blockquote>
<p>I&#39;m not sure that would be particularly reassuring to me (writing as one of the contributors). First, how would one check that the agreement had been adhered to (maybe it&#39;s possible, I don&#39;t know)? Second, people in my experience often don&#39;t notice they are training on data (as mentioned in a post above by ozziegooen).</p>
</div></div></div></div></div><div><div><div id="sbSyyYWTTRzX2hEb4"><div><div><div><div><div><p>I agree entirely that it would not be very reassuring, for the reasons you explained. But I would still consider it a mildly interesting signal to see if OpenAI would be willing to provide such an agreement in writing, and maybe make a public statement on the precise way they used the data so far.</p>
<p>Also: if they make a legally binding commitment, and then later evidence shows up that they violated the terms of this agreement (e.g. via whistleblowers), I do think that this is a bigger legal risk for them than breeching some fuzzy verbal agreement.</p>
</div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="BAGhuuCgwqm6aAcaK"><div><div><div><div><div><p>I found this extra information very useful, thanks for revealing what you did.</p><p>Hiding this information seems very similar to lying to the public. So at very least, from what I&#39;ve seen, I don&#39;t feel like we have many reasons to trust their communications - especially their &#34;<i>tweets from various employees.</i>&#34;</p></div></div></div></div></div></div></div><div><div id="F5EtJmiappsmoRv3z"><div><div><div><div><div><p>I think you should publicly commit to:</p>
<ul>
<li>full transparency about any funding from for profit organisations, including nonprofit organizations affiliated with for profit</li>
<li>no access to the benchmarks to any company</li>
<li>no NDAs around this stuff</li>
</ul>
<p>If you currently have any of these with the computer use benchmark in development, you should seriously try to get out of those contractual obligations if there are any.</p>
<p>Ideally, you commit to these in a legally binding way, which would make it non-negotiable in any negotiation, and make you more credible to outsiders.</p>
</div></div></div></div></div></div></div><div><div id="8m6o7EGwbwXt9QtnF"><div><div><div><div><div><blockquote><p>However, we have a verbal agreement that these materials will not be used in model training. </p></blockquote><p>If by this you mean &#34;OpenAI will not train on this data&#34;, that doesn&#39;t address the vast majority of the concern. If OpenAI is evaluating the model against the data, they will be able to more effectively optimize for capabilities advancement, and that&#39;s a betrayal of the trust of the people who worked on this with the understanding that it will be used only outside of the research loop to check for dangerous advancements. And, particularly, not to make those dangerous advancements come sooner by giving OpenAI another number to optimize for.</p><p>If you mean OpenAI will not be internally evaluating models on this to improve and test the training process, please state this clearly in writing (and maybe explain why they got privileged access to the data despite being prohibited from the obvious use of that data).</p></div></div></div></div></div></div></div><div><div id="hAXDfbS42tAup9ECi"><div><div><div><div><div><p>Thank you for the clarification! What I would be curious about: you write</p>
<blockquote>
<p>OpenAI does have access to a large fraction of FrontierMath problems and solutions</p>
</blockquote>
<p>Does this include the detailed solution write-up (mathematical arguments, in LaTeX) or just the final answer (numerical result of the question / Python script verifying the correctness of the AI response)?</p>
</div></div></div></div></div></div></div><div><div id="ijCynzN5SGCqWPfM4"><div><div><div><div><div><blockquote><p>We acknowledge that OpenAI does have access to a large fraction of FrontierMath problems and solutions, with the exception of a unseen-by-OpenAI hold-out set that enables us to independently verify model capabilities.</p></blockquote><p>Can you say exactly how large of a fraction is the set that OpenAI has access to, and how much is the hold-out set?</p></div></div></div></div></div><div><div><div id="jDg9M9EJXJwyRkFWa"><div><div><div><div><div><p>Not Tamay, but from <a href="https://www.reddit.com/r/singularity/comments/1i4n0r5/this_is_so_disappointing_epoch_ai_the_startup/">elliotglazer on Reddit</a><span data-footnote-reference="" data-footnote-index="1" data-footnote-id="hveboetm5yp" role="doc-noteref" id="fnrefhveboetm5yp"><sup><a href="#fnhveboetm5yp">[1]</a></sup></span> (14h ago): &#34;<i>Epoch&#39;s lead mathematician here. Yes, OAI funded this and has the dataset, which allowed them to evaluate o3 in-house. We haven&#39;t yet independently verified their 25% claim. To do so, we&#39;re <strong>currently developing</strong> a hold-out dataset and will be able to test their model without them having any prior exposure to these problems.</i></p><p><i>My personal opinion is that OAI&#39;s score is legit (i.e., they didn&#39;t train on the dataset), and that they have no incentive to lie about internal benchmarking performances. However, we can&#39;t vouch for them until our independent evaluation is complete.</i>&#34;</p><p><strong>Currently developing</strong> a hold-out dataset gives a different impression than </p><p>&#34;<i>We acknowledge that OpenAI does have access to a large fraction of FrontierMath problems and solutions, with the exception of a unseen-by-OpenAI hold-out set that enables us to independently verify model capabilities</i>&#34; and &#34;they do not have access to a separate holdout set that serves as an additional safeguard for independent verification.&#34; </p><ol data-footnote-section="" role="doc-endnotes"><li data-footnote-item="" data-footnote-index="1" data-footnote-id="hveboetm5yp" role="doc-endnote" id="fnhveboetm5yp"><span data-footnote-back-link="" data-footnote-id="hveboetm5yp"><sup><strong><a href="#fnrefhveboetm5yp">^</a></strong></sup></span><p>Emphasis mine. He also references &#34;the holdout set we are developing&#34; on <a href="https://x.com/ElliotGlazer/status/1880812021966602665">Twitter</a>.</p></li></ol></div></div></div></div></div></div></div></div></div></div><div><div id="NeHPmbku7icMCCPng"><div><div><div><div><div><p>Just to confirm, you will be benchmarking models other than OpenAI models using this dataset and you aren&#39;t contractually prevented from doing this right?</p>
<p>(The original <a href="https://epoch.ai/frontiermath/the-benchmark">blog post</a> cites scores of models from multiple developers, so I assume so.)</p>
</div></div></div></div></div></div></div><div><div id="AFzEF3AgCNwbb4F8z"><div><div><div><div><div><p>How much funding did OpenAI provide EpochAI?</p><p>Or, how much funding do you expext to receive in total from OpenAI for FrontierMath if you haven&#39;t received all funding yet?</p></div></div></div></div></div></div></div><div><div id="jrP6tHw697utAH42Z"><div><div><div><div><p>Creating further even harder datasets could plausibly accelerate OpenAI&#39;s progress. I read on twitter that people are working on an even harder dataset now. I would not give them access to this, they may break their promise not to train on this if it allows them to accelerate progress. This is extremely valuable training data that you have handed to them.</p></div></div></div></div></div></div><div><div id="99viLxAwhfFeArNqZ"><div><div><div><div><p>This is extremely informative, especially the bit about the holdout set. I think it&#39;d reassure a lot of people about the FrontierMath&#39;s validity to know more here. Have you used it to assess any of OpenAI&#39;s models? If so, how, and what were the results?</p></div></div></div></div></div></div></div></div></div><div><div id="2nFtnDNir9YDHCux7"><div><div><div><div><div><p>It&#39;s probably worth them mentioning for completeness that Nat Friedman funded an earlier version of the dataset too. (I was advising at that time and provided the main recommendation that it needs to be research-level because they were focusing on Olympiad level.)</p>
<p>Also can confirm they aren&#39;t giving access to the mathematicians&#39; questions to AI companies other than OpenAI like xAI.</p>
</div></div></div></div></div></div></div><div><div id="dkFefbTRv4YFTbXzy"><div><div><div><div><p>EpochAI is also working on a &#34;<a href="https://x.com/tamaybes/status/1876692639363612919">next-generation computer-use benchmark</a>&#34;. I wonder who is involved in funding that. It could be OpenAI given recent rumors they are planning to release a computer-use model early this year.</p></div></div></div></div><div><div><div id="azPs8PL5uzSLrAHsg"><div><div><div><div><p>Having hopefully learned from our mistakes regarding FrontierMath, we intend to be more transparent to collaborators for this new benchmark. However, at this stage of development, the benchmark has not reached a point where any major public disclosures are necessary.</p></div></div></div></div><div><div><div id="C6wCR5dpa2MnB69GC"><div><div><div><div><p>Well, I&#39;d sure like to know whether you are planning to give the dataset to OpenAI or any other frontier companies! It might influence my opinion of whether this work is net positive or net negative.</p></div></div></div></div><div><div><div id="N3X6xRubZewA6uN6J"><div><div><div><div><p>I can&#39;t make any confident claims or promises in my personal capacity right now, but my best guess is that we will make sure this new benchmark stays entirely private and under Epoch&#39;s control, to the extent this is feasible for us. However, I want to emphasize that by saying this, I&#39;m not making a public commitment on behalf of Epoch.</p></div></div></div></div><div><div><div id="G372watzSruhgXvvY"><div><div><div><div><div><blockquote>
<p>to the extent this is feasible for us</p>
</blockquote>
<p>Was [keeping FrontierMath entirely private and under Epoch&#39;s control] feasible for Epoch in the same sense of &#34;feasible&#34; you are using here?</p>
</div></div></div></div></div><div><div><div id="MiafM9XKsQHXmGv3D"><div><div><div><div><div><p>I&#39;m not completely sure, since I was not personally involved in the relevant negotiations for FrontierMath. However, what I can say is that Tamay already indicated that Epoch should have tried harder to obtain different contract terms that enabled us to have greater transparency. I don&#39;t think it makes sense for him to say that unless he believes it was feasible to have achieved a different outcome.</p><p>Also, I want to clarify that this new benchmark is separate from FrontierMath and we are under different constraints with regards to it.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="7NJimpub6yccwdFaY"><div><div><div><div><div><p>Thanks for posting this!</p><p>I have to admit, the quote here doesn&#39;t seem to clearly support your title -- I think &#34;support in creating the benchmark&#34; could mean lots of different things, only some of which are funding. Is there something I&#39;m missing here?</p><p>Regardless, I agree that FrontierMath should make clear what the extent was of their collaboration with OpenAI. Obviously the details here are material to the validity of their benchmark.</p></div></div></div></div></div></div></div><div><div id="Cd7FN7kGtD4qEGLwD"><div><div><div><div><p>Why do you consider it unlikely that companies could (or would) fish out the questions from API-logs?</p></div></div></div></div><div><div><div id="JZgFjftuaovk7fGAd"><div><div><div><div><div><p>That was a quote from a commenter in Hacker news, not my view. I reference the comment as something I thought a lot of people&#39;s impression was pre- Dec 20th. You may be right that maybe most people didn&#39;t have the impression that it&#39;s unlikely, or that maybe they didn&#39;t have a reason to think that. I don&#39;t really know.</p><p>Thanks, I&#39;ll put the quote in italics so it&#39;s clearer.</p></div></div></div></div></div></div></div></div></div></div><div><div id="qzC7vKPgaWquD92nE"><div><div><div><div><p>Hey everyone, could you spell out to me what&#39;s the issue here? I read a lot of comments that basically assume &#34;x and y are really bad&#34; but never spell it out. So, is the problem that:</p></div></div></div></div><div><div><div id="pRAENni5XckfArEpW"><div><div><div><div><div><p>Really high quality high-difficulty benchmarks are much more scarce and important for capabilities advancing than just training data. Having an apparently x-risk focused org do a benchmark implying it&#39;s for evaluating danger from highly capable models in a way which the capabilities orgs can&#39;t use to test their models, then having it turn out that&#39;s secretly funded by OpenAI with OpenAI getting access to most of the data is very sketchy.</p><p>Some people who contributed questions likely thought they would be reducing x-risk by helping build bright line warning signs. Their work being available to OpenAI will mostly have increased x-risk by giving the capabilities people an unusually important number-goes-up to optimize for, bringing timelines to dangerous systems closer. That&#39;s a betrayal of trust, and Epoch should do some serious soul searching about taking money to do harmful things.</p></div></div></div></div></div><div><div><div id="dj9PvDQp6m4BJJbWK"><div><div><div><div><p>If the funding didn&#39;t come from OpenAI, would OpenAI still be able to use that benchmark? Like, I&#39;d imagine Epoch would still use that to evaluate where current models are at. I think this might be my point of confusion. Maybe the answer is &#34;not as much for it to be as useful to them&#34;?</p></div></div></div></div><div><div><div id="DgcBS67ee6u2sbCps"><div><div><div><div><div><p>Evaluation on demand because they can run them intensely lets them test small models for architecture improvements. This is where the vast majority of the capability gain is.</p><p>Getting an evaluation of each final model is going to be way less useful for the research cycle, as it only gives a final score, not a metric which is part of the feedback loop.</p></div></div></div></div></div></div></div><div><div id="4ix2t7tBrvL79bBaB"><div><div><div><div><div><p>Yes, that answer matches my understanding of the concern. If the vast majority of the dataset was private to Epoch, OpenAI they could occasionally submit their solution (probably via API) to Epoch to grade, but wouldn’t be able to use the dataset with high frequency as evaluation in many experiments.</p></div></div></div></div></div><div><div><div id="DSEp7xKeX6ts5R9aE"><div><div><div><div><p>I&#39;m guessing you view having better understanding of what&#39;s coming as very high value, enough that burning some runway is acceptable? I could see that model (though put &lt;15% on it), but I think this is at least not good integrity wise to have put on the appearance of doing just the good for x-risk part and not sharing it as an optimizable benchmark, while being funded by and giving the data to people who will use it for capability advancements. </p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div><div id="qHGqKcFrGWmaNgAfn"><div><div><div><div><p>this doesn&#39;t seem like a huge deal</p></div></div></div></div></div></div></div></div></div></div></div></div></span></span></p></div></div>
  </body>
</html>
