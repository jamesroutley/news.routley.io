<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/toredash/automatic-zone-placement">Original</a>
    <h1>Automatic K8s pod placement to match external service zones</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">In Kubernetes, it is not possible to schedule Pods for optimal network performance when Pods use resources outside of the cluster.</p>
<p dir="auto">This project offers a solution by making Kubernetes-aware of network topology outside the cluster. A lightweight <strong>lookup service</strong> provides an endpoint to resolve an external resource&#39;s domain name to an IP address and maps it to a known network zone (like an AWS Availability Zone). This data is used with a mutating webhook to inject node affinity rules into Pods at creation time. This ensures Pods are scheduled in the same zone as the external resources they depend on, optimizing for low-latency communication. The mechanism is generic and works for any external resource, on-prem or in the cloud, as long as its FQDN resolves to a single IP in a known subnet.</p>
<p dir="auto">This approach can yield significant performance improvements; a simple <code>pgbench</code> benchmark demonstrates a ~175% to ~375% improvement in <a href="https://en.wikipedia.org/wiki/Transactions_per_second" rel="nofollow">TPS</a>. Any workload that is latency-sensitive can benefit from this.</p>
<p dir="auto"><em>Note: This is not a fix for placing related workloads running in the same cluster relatively near each other. That is a problem already solved in Kubernetes with affinity rules and smart use of label selectors.</em></p>

<p dir="auto">The basic problem is that the scheduler in Kubernetes is unaware of the network topology it is running on, so node assignment stays random. For instance, assume your RDS Writer Instance is running in AZ C, Kubernetes is not able to determine that it should (ideally) schedule your SQL client Pod in AZ C:</p>
<section data-identity="0b2a0560-fbad-4e39-84cc-2d14a7e7bb21" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div data-json="{&#34;data&#34;:&#34;---\nconfig:\n  layout: fixed\n---\nflowchart TD\n subgraph subGraph0[\&#34;Kubernetes Control Plane\&#34;]\n        K8sAPI[\&#34;K8s API Server\&#34;]\n  end\n subgraph subGraph1[\&#34;Availability Zone a\&#34;]\n        Pod[\&#34;SQL Client Pod\n        I should be in zone C üôÅ\&#34;]\n        NodeA[\&#34;K8s Node\&#34;]\n  end\n subgraph subGraph2[\&#34;Availability Zone b\&#34;]\n        NodeB[\&#34;K8s Node\&#34;]\n  end\n subgraph subGraph3[\&#34;Availability Zone c\&#34;]\n        RDS[(\&#34;RDS Writer Instance\&#34;)]\n        NodeC[\&#34;K8s Node\&#34;]\n  end\n subgraph subGraph4[\&#34;AWS Region (eu-central-1)\&#34;]\n        subGraph1\n        subGraph2\n        subGraph3\n  end\n    K8sAPI -- schedules pod by random (unaware of RDS location) --&amp;gt; Pod\n    Pod -- \&#34;High Latency / Cross-AZ Traffic\&#34; --&amp;gt; RDS\n    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff\n    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff\n    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff\n&#34;}" data-plain="---
config:
  layout: fixed
---
flowchart TD
 subgraph subGraph0[&#34;Kubernetes Control Plane&#34;]
        K8sAPI[&#34;K8s API Server&#34;]
  end
 subgraph subGraph1[&#34;Availability Zone a&#34;]
        Pod[&#34;SQL Client Pod
        I should be in zone C üôÅ&#34;]
        NodeA[&#34;K8s Node&#34;]
  end
 subgraph subGraph2[&#34;Availability Zone b&#34;]
        NodeB[&#34;K8s Node&#34;]
  end
 subgraph subGraph3[&#34;Availability Zone c&#34;]
        RDS[(&#34;RDS Writer Instance&#34;)]
        NodeC[&#34;K8s Node&#34;]
  end
 subgraph subGraph4[&#34;AWS Region (eu-central-1)&#34;]
        subGraph1
        subGraph2
        subGraph3
  end
    K8sAPI -- schedules pod by random (unaware of RDS location) --&gt; Pod
    Pod -- &#34;High Latency / Cross-AZ Traffic&#34; --&gt; RDS
    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff
    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff
    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff
" dir="auto">
    <div dir="auto">
      <pre lang="mermaid" aria-label="Raw mermaid code">---
config:
  layout: fixed
---
flowchart TD
 subgraph subGraph0[&#34;Kubernetes Control Plane&#34;]
        K8sAPI[&#34;K8s API Server&#34;]
  end
 subgraph subGraph1[&#34;Availability Zone a&#34;]
        Pod[&#34;SQL Client Pod
        I should be in zone C üôÅ&#34;]
        NodeA[&#34;K8s Node&#34;]
  end
 subgraph subGraph2[&#34;Availability Zone b&#34;]
        NodeB[&#34;K8s Node&#34;]
  end
 subgraph subGraph3[&#34;Availability Zone c&#34;]
        RDS[(&#34;RDS Writer Instance&#34;)]
        NodeC[&#34;K8s Node&#34;]
  end
 subgraph subGraph4[&#34;AWS Region (eu-central-1)&#34;]
        subGraph1
        subGraph2
        subGraph3
  end
    K8sAPI -- schedules pod by random (unaware of RDS location) --&gt; Pod
    Pod -- &#34;High Latency / Cross-AZ Traffic&#34; --&gt; RDS
    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff
    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff
    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff
</pre>
    </div>
  </div>
  <span role="presentation">
    <span data-view-component="true">
  <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" aria-hidden="true" data-view-component="true">
    <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
    <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>    <span>Loading</span>
</span>
  </span>
</section>


<p dir="auto">By making network topology information available for the scheduler, we can allow it to make informed decisions. The best case for our workload is to be placed in the same AZ as the RDS instance it is communicating with:</p>
<section data-identity="0511758e-2849-4308-bddd-a0b83e100b73" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div data-json="{&#34;data&#34;:&#34;graph TD\n    subgraph \&#34;Kubernetes Control Plane\&#34;\n        K8sAPI[\&#34;K8s API Server\&#34;]\n    end\n\n    subgraph \&#34;AWS Region (eu-central-1)\&#34;\n        subgraph \&#34;Solution Components\&#34;\n            Kyverno[\&#34;Kyverno Policy\n            (Mutating Webhook)\&#34;]\n            Lookup[\&#34;Lookup Service\&#34;]\n            Kyverno -- \&#34;gets location data\&#34; --&amp;gt; Lookup\n        end\n\n        subgraph \&#34;Availability Zone a\&#34;\n            NodeA[\&#34;K8s Node\&#34;]\n        end\n        subgraph \&#34;Availability Zone b\&#34;\n            NodeB[\&#34;K8s Node\&#34;]\n        end\n        subgraph \&#34;Availability Zone c\&#34;\n            Pod[\&#34;SQL Client Pod\n            Minimal latency üòÄ\&#34;]\n            RDS[(\&#34;RDS Instance\&#34;)]\n            NodeC[\&#34;K8s Node\&#34;]\n        end\n    end\n\n    K8sAPI -- \&#34;admission controller,\n    triggers webhook on Pod creation\&#34; --&amp;gt; Kyverno\n    Lookup -. \&#34;perform informed / optimal placement of Pod\&#34; .-&amp;gt; Pod\n    Pod -- \&#34;Low Latency\&#34; --&amp;gt; RDS\n\n    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff\n    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff\n    style Kyverno fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff\n    style Lookup fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff\n    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff\n&#34;}" data-plain="graph TD
    subgraph &#34;Kubernetes Control Plane&#34;
        K8sAPI[&#34;K8s API Server&#34;]
    end

    subgraph &#34;AWS Region (eu-central-1)&#34;
        subgraph &#34;Solution Components&#34;
            Kyverno[&#34;Kyverno Policy
            (Mutating Webhook)&#34;]
            Lookup[&#34;Lookup Service&#34;]
            Kyverno -- &#34;gets location data&#34; --&gt; Lookup
        end

        subgraph &#34;Availability Zone a&#34;
            NodeA[&#34;K8s Node&#34;]
        end
        subgraph &#34;Availability Zone b&#34;
            NodeB[&#34;K8s Node&#34;]
        end
        subgraph &#34;Availability Zone c&#34;
            Pod[&#34;SQL Client Pod
            Minimal latency üòÄ&#34;]
            RDS[(&#34;RDS Instance&#34;)]
            NodeC[&#34;K8s Node&#34;]
        end
    end

    K8sAPI -- &#34;admission controller,
    triggers webhook on Pod creation&#34; --&gt; Kyverno
    Lookup -. &#34;perform informed / optimal placement of Pod&#34; .-&gt; Pod
    Pod -- &#34;Low Latency&#34; --&gt; RDS

    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff
    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff
    style Kyverno fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff
    style Lookup fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff
    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff
" dir="auto">
    <div dir="auto">
      <pre lang="mermaid" aria-label="Raw mermaid code">graph TD
    subgraph &#34;Kubernetes Control Plane&#34;
        K8sAPI[&#34;K8s API Server&#34;]
    end

    subgraph &#34;AWS Region (eu-central-1)&#34;
        subgraph &#34;Solution Components&#34;
            Kyverno[&#34;Kyverno Policy
            (Mutating Webhook)&#34;]
            Lookup[&#34;Lookup Service&#34;]
            Kyverno -- &#34;gets location data&#34; --&gt; Lookup
        end

        subgraph &#34;Availability Zone a&#34;
            NodeA[&#34;K8s Node&#34;]
        end
        subgraph &#34;Availability Zone b&#34;
            NodeB[&#34;K8s Node&#34;]
        end
        subgraph &#34;Availability Zone c&#34;
            Pod[&#34;SQL Client Pod
            Minimal latency üòÄ&#34;]
            RDS[(&#34;RDS Instance&#34;)]
            NodeC[&#34;K8s Node&#34;]
        end
    end

    K8sAPI -- &#34;admission controller,
    triggers webhook on Pod creation&#34; --&gt; Kyverno
    Lookup -. &#34;perform informed / optimal placement of Pod&#34; .-&gt; Pod
    Pod -- &#34;Low Latency&#34; --&gt; RDS

    style Pod fill:#FF9900,stroke:#333,stroke-width:2px,color:#fff
    style RDS fill:#5A30B5,stroke:#333,stroke-width:2px,color:#fff
    style Kyverno fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff
    style Lookup fill:#2771e2,stroke:#333,stroke-width:2px,color:#fff
    style K8sAPI fill:#D92A2A,stroke:#333,stroke-width:2px,color:#fff
</pre>
    </div>
  </div>
  <span role="presentation">
    <span data-view-component="true">
  <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" aria-hidden="true" data-view-component="true">
    <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
    <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>    <span>Loading</span>
</span>
  </span>
</section>

<p dir="auto">The solution has two main components:</p>
<ol dir="auto">
<li><strong>A Lookup Service:</strong> An API endpoint that can determine which zone a particular resource is located in. This endpoint accepts a domain name, resolves it to an IP, and maps that IP to a specific availability zone based on a predefined list of network CIDR ranges.</li>
<li><strong>A Mutating Webhook:</strong> Using a policy engine like Kyverno, a webhook intercepts Pod creation requests. If a Pod is annotated correctly, the webhook calls the lookup service to find the optimal zone and injects a <code>nodeAffinity</code> rule into the Pod&#39;s specification before it&#39;s scheduled.</li>
</ol>
<p dir="auto">Manually setting a Node Affinity rule is a temporary fix, but it fails when the external resource&#39;s location changes. For example, in a multi-node RDS cluster, the writer instance can be relocated during maintenance events, making static affinity rules unreliable.</p>

<p dir="auto">For this to work, you need the following present in your Kubernetes environment:</p>
<ul dir="auto">
<li>Kyverno (or OPA, or create your own mutating policies to work in a similar fashion)</li>
<li>A workload in Kubernetes that communicates with an external resource via FQDN</li>
<li>The FQDN needs to return a <em>single</em> A record for the external resource</li>
<li>Zone information about your subnets</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 1: Gather Zone Information</h3><a id="user-content-step-1-gather-zone-information" aria-label="Permalink: Step 1: Gather Zone Information" href="#step-1-gather-zone-information"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To gather zone information, use this command and bring the output into the <code>SUBNETS_DATA</code> array in <code>resources/server.py</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ aws ec2 describe-subnets --query &#39;Subnets[*].{CIDRBlock:CidrBlock, AvailabilityZone:AvailabilityZone, AvailabilityZoneId:AvailabilityZoneId}&#39; --output json | jq &#39;.[]&#39; -c | sort | uniq | sed &#39;s/$/,/g&#39; | sed &#39;$s/\,$//g&#39;
{&#34;CIDRBlock&#34;:&#34;10.0.192.0/20&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1b&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az3&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.0.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1b&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az3&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.128.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1a&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az2&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.160.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1c&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az1&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.32.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1a&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az2&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.64.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1c&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az1&#34;},
{&#34;CIDRBlock&#34;:&#34;192.168.96.0/19&#34;,&#34;AvailabilityZone&#34;:&#34;eu-central-1b&#34;,&#34;AvailabilityZoneId&#34;:&#34;euc1-az3&#34;}"><pre>$ aws ec2 describe-subnets --query <span><span>&#39;</span>Subnets[*].{CIDRBlock:CidrBlock, AvailabilityZone:AvailabilityZone, AvailabilityZoneId:AvailabilityZoneId}<span>&#39;</span></span> --output json <span>|</span> jq <span><span>&#39;</span>.[]<span>&#39;</span></span> -c <span>|</span> sort <span>|</span> uniq <span>|</span> sed <span><span>&#39;</span>s/$/,/g<span>&#39;</span></span> <span>|</span> sed <span><span>&#39;</span>$s/\,$//g<span>&#39;</span></span>
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>10.0.192.0/20<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1b<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az3<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.0.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1b<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az3<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.128.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1a<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az2<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.160.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1c<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az1<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.32.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1a<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az2<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.64.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1c<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az1<span>&#34;</span></span>},
{<span><span>&#34;</span>CIDRBlock<span>&#34;</span></span>:<span><span>&#34;</span>192.168.96.0/19<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZone<span>&#34;</span></span>:<span><span>&#34;</span>eu-central-1b<span>&#34;</span></span>,<span><span>&#34;</span>AvailabilityZoneId<span>&#34;</span></span>:<span><span>&#34;</span>euc1-az3<span>&#34;</span></span>}</pre></div>
<p dir="auto"><em><strong>Note:</strong> Having multiple AWS Accounts, VPC peerings or Transit Gateways? Map them as well. This will enable you to optimize Pod placement for even more external resources.</em></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 2: Deploy the Lookup Service</h3><a id="user-content-step-2-deploy-the-lookup-service" aria-label="Permalink: Step 2: Deploy the Lookup Service" href="#step-2-deploy-the-lookup-service"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The lookup service is available at <code>resources/server.py</code>, implemented in Python with no dependencies. <code>resources/automatic-zone-lookup.yaml</code> deploys a Deployment with using an alpine container with the Python code injected as a ConfigMap, with a Service called <code>automatic-zone-placement</code> which Kyverno can utilize.</p>
<p dir="auto">Deploy the service:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cd resources/
$ kubectl create configmap --from-file=server.py -n kyverno automatic-zone-placement
configmap/automatic-zone-placement created
$ kubectl create -f automatic-zone-lookup.yaml -n kyverno
service/automatic-zone-placement created
deployment.apps/automatic-zone-placement created"><pre>$ <span>cd</span> resources/
$ kubectl create configmap --from-file=server.py -n kyverno automatic-zone-placement
configmap/automatic-zone-placement created
$ kubectl create -f automatic-zone-lookup.yaml -n kyverno
service/automatic-zone-placement created
deployment.apps/automatic-zone-placement created</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 3: Deploy the Mutating Policy</h3><a id="user-content-step-3-deploy-the-mutating-policy" aria-label="Permalink: Step 3: Deploy the Mutating Policy" href="#step-3-deploy-the-mutating-policy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">With the lookup service running, you can apply the Kyverno <code>ClusterPolicy</code> located in <code>resources/kyverno_clusterpolicy.yaml</code>. This policy will look for <code>Pod.CREATE</code> operations. If a Pod has the annotation <code>automatic-zone-placement</code>, it will call the lookup service to determine the optimal zone.</p>
<p dir="auto">Apply the policy:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ kubectl apply -f kyverno_clusterpolicy.yaml
clusterpolicy.kyverno.io/automatic-zone-placement created"><pre>$ kubectl apply -f kyverno_clusterpolicy.yaml
clusterpolicy.kyverno.io/automatic-zone-placement created</pre></div>
<p dir="auto"><em>Note: You need to make your own considerations on how you want to the mutation for your Pods to look like. A simpler nodeSelector would also work.</em></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 4: Test the Solution</h3><a id="user-content-step-4-test-the-solution" aria-label="Permalink: Step 4: Test the Solution" href="#step-4-test-the-solution"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Deploy a pod with the required annotation to see the mutation in action:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo &#39;---
apiVersion: v1
kind: Pod
metadata:
  name: pgbench-with-annotation
  annotations:
    automatic-zone-placement: my-rds-instance.cluster-c7eeqk68ktn1.eu-central-1.rds.amazonaws.com
spec:
  containers:
  - name: postgres
    image: postgres:15-bullseye
    command:
    - &#34;/bin/bash&#34;
    - &#34;-c&#34;
    - |
      echo &#34;SELECT 1;&#34; &gt; /tmp/sql
      export PGPASSWORD=postgresqlpassword
      while true; do
        pgbench -h my-rds-instance.cluster-c7eeqk68ktn1.eu-central-1.rds.amazonaws.com -U postgresql -r -f /tmp/sql -T 9 -n postgres 2&gt;&amp;1 | grep -e tps -e latency
      done&#39; | kubectl apply -f -
pod/pgbench-with-annotation created"><pre>$ <span>echo</span> <span><span>&#39;</span>---</span>
<span>apiVersion: v1</span>
<span>kind: Pod</span>
<span>metadata:</span>
<span>  name: pgbench-with-annotation</span>
<span>  annotations:</span>
<span>    automatic-zone-placement: my-rds-instance.cluster-c7eeqk68ktn1.eu-central-1.rds.amazonaws.com</span>
<span>spec:</span>
<span>  containers:</span>
<span>  - name: postgres</span>
<span>    image: postgres:15-bullseye</span>
<span>    command:</span>
<span>    - &#34;/bin/bash&#34;</span>
<span>    - &#34;-c&#34;</span>
<span>    - |</span>
<span>      echo &#34;SELECT 1;&#34; &gt; /tmp/sql</span>
<span>      export PGPASSWORD=postgresqlpassword</span>
<span>      while true; do</span>
<span>        pgbench -h my-rds-instance.cluster-c7eeqk68ktn1.eu-central-1.rds.amazonaws.com -U postgresql -r -f /tmp/sql -T 9 -n postgres 2&gt;&amp;1 | grep -e tps -e latency</span>
<span>      done<span>&#39;</span></span> <span>|</span> kubectl apply -f -
pod/pgbench-with-annotation created</pre></div>
<p dir="auto">We know this Pod should ideally be placed in zone <code>eu-central-1b</code>. Let&#39;s verify that a <code>nodeAffinity</code> was injected and the correct value was set:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ kubectl get pod pgbench-with-annotation -o json | jq &#39;.spec.affinity&#39;
{
  &#34;nodeAffinity&#34;: {
    &#34;preferredDuringSchedulingIgnoredDuringExecution&#34;: [
      {
        &#34;preference&#34;: {
          &#34;matchExpressions&#34;: [
            {
              &#34;key&#34;: &#34;topology.kubernetes.io/zone&#34;,
              &#34;operator&#34;: &#34;In&#34;,
              &#34;values&#34;: [
                &#34;eu-central-1b&#34;
              ]
            }
          ]
        },
        &#34;weight&#34;: 100
      }
    ]
  }
}"><pre>$ kubectl get pod pgbench-with-annotation -o json <span>|</span> jq <span><span>&#39;</span>.spec.affinity<span>&#39;</span></span>
{
  <span><span>&#34;</span>nodeAffinity<span>&#34;</span></span>: {
    <span><span>&#34;</span>preferredDuringSchedulingIgnoredDuringExecution<span>&#34;</span></span>: [
      {
        <span><span>&#34;</span>preference<span>&#34;</span></span>: {
          <span><span>&#34;</span>matchExpressions<span>&#34;</span></span>: [
            {
              <span><span>&#34;</span>key<span>&#34;</span></span>: <span><span>&#34;</span>topology.kubernetes.io/zone<span>&#34;</span></span>,
              <span><span>&#34;</span>operator<span>&#34;</span></span>: <span><span>&#34;</span>In<span>&#34;</span></span>,
              <span><span>&#34;</span>values<span>&#34;</span></span>: [
                <span><span>&#34;</span>eu-central-1b<span>&#34;</span></span>
              ]
            }
          ]
        },
        <span><span>&#34;</span>weight<span>&#34;</span></span>: 100
      }
    ]
  }
}</pre></div>
<p dir="auto">It worked! We see that the <code>nodeAffinity</code> field was added by the Kyverno policy.</p>

<p dir="auto">Running <code>pgbench</code> with a simple <code>SELECT 1;</code> query in a loop, it is easy to spot the performance difference. Below is an output from a Deployment with 3 replicas, one Pod per zone, <em>without</em> the automatic placement annotation. Can you spot which Pod is running in the same AZ as its RDS instance?</p>
<div data-snippet-clipboard-copy-content="pgbench-6544568f8b-72kx5 latency average = 0.147 ms
pgbench-6544568f8b-72kx5 tps = 6793.317768 (without initial connection time)
pgbench-6544568f8b-7h4dk latency average = 0.704 ms
pgbench-6544568f8b-7h4dk tps = 1420.275917 (without initial connection time)
pgbench-6544568f8b-25vsx latency average = 0.404 ms
pgbench-6544568f8b-25vsx tps = 2473.397971 (without initial connection time)"><pre><code>pgbench-6544568f8b-72kx5 latency average = 0.147 ms
pgbench-6544568f8b-72kx5 tps = 6793.317768 (without initial connection time)
pgbench-6544568f8b-7h4dk latency average = 0.704 ms
pgbench-6544568f8b-7h4dk tps = 1420.275917 (without initial connection time)
pgbench-6544568f8b-25vsx latency average = 0.404 ms
pgbench-6544568f8b-25vsx tps = 2473.397971 (without initial connection time)
</code></pre></div>
<p dir="auto">That&#39;s a 175% to 375% performance improvement. For anyone working with networking, this isn&#39;t a surprise. Lower latency means improved performance.</p>
<p dir="auto">AWS publish intra-zone latency metrics for each zone in all regions via their <a href="https://eu-central-1.console.aws.amazon.com/nip/" rel="nofollow">Network Manager &gt; Infrastructure Performance</a> page. At the time of writing, the following latency metrics was:</p>
<div data-snippet-clipboard-copy-content="Cross-AZ latency:
euc1-az1 to euc1-az2: 0.656 ms
euc1-az2 to euc1-az3: 0.548 ms
euc1-az3 to euc1-az1: 0.458 ms

Inter-AZ latency:
euc1-az1 to euc1-az1 0.104 ms
euc1-az2 to euc1-az2 0.119 ms
euc1-az3 to euc1-az3 0.107 ms"><pre><code>Cross-AZ latency:
euc1-az1 to euc1-az2: 0.656 ms
euc1-az2 to euc1-az3: 0.548 ms
euc1-az3 to euc1-az1: 0.458 ms

Inter-AZ latency:
euc1-az1 to euc1-az1 0.104 ms
euc1-az2 to euc1-az2 0.119 ms
euc1-az3 to euc1-az3 0.107 ms
</code></pre></div>
<p dir="auto">The above data shows that within the same region, one should expect a latency of ~0.100 ms. But, if your application has to traverse to another zone in the same region, the latency increases to ~0.450-0.650 ms. This increase in cross-AZ latency matches roughly the performance gains shown previously.</p>
<p dir="auto">So the math checks out :)</p>

<p dir="auto">The Python script has example CIDR ranges and can be run locally for testing.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ python3 ./resources/server.py &amp;
[...]
$ curl localhost:8080/192.168.0.1.nip.io | jq
{
  &#34;zone&#34;: &#34;eu-central-1b&#34;,
  &#34;zoneId&#34;: &#34;euc1-az3&#34;
}
$ curl localhost:8080/192.168.32.1.nip.io | jq
{
  &#34;zone&#34;: &#34;eu-central-1a&#34;,
  &#34;zoneId&#34;: &#34;euc1-az2&#34;
}"><pre>$ python3 ./resources/server.py <span>&amp;</span>
[...]
$ curl localhost:8080/192.168.0.1.nip.io <span>|</span> jq
{
  <span><span>&#34;</span>zone<span>&#34;</span></span>: <span><span>&#34;</span>eu-central-1b<span>&#34;</span></span>,
  <span><span>&#34;</span>zoneId<span>&#34;</span></span>: <span><span>&#34;</span>euc1-az3<span>&#34;</span></span>
}
$ curl localhost:8080/192.168.32.1.nip.io <span>|</span> jq
{
  <span><span>&#34;</span>zone<span>&#34;</span></span>: <span><span>&#34;</span>eu-central-1a<span>&#34;</span></span>,
  <span><span>&#34;</span>zoneId<span>&#34;</span></span>: <span><span>&#34;</span>euc1-az2<span>&#34;</span></span>
}</pre></div>
<p dir="auto"><em>Note: Beware that your local DNS resolver might implement DNS rebinding protection, which may result in failure to resolve local and private IP addresses. Using nip.io won&#39;t work in that case.</em></p>

<p dir="auto">This solution shows how one can increase performance by ~175%-375%, by adding additional meta-data for the kube-scheduler to use to make informative Pod placement. OK, thats not really correct, but it sounds better. What this solution really does is ensuring <em>consistent</em> and <em>best</em> performance by placing the Pods in the correct availbility zone.</p>
<p dir="auto">I think this approach can be very valuable for those who require low latency or want to reduce cross-AZ data transfer cost. I wouldn&#39;t be surprised if this approach could be used for on-prem as well, as long as there is sufficient meta-data available.</p>
<p dir="auto">Got a physical database in Rack A, Row 2 in DC 1? <em>Most likely</em> the network performance is best at the rack level, so you could have a lookup policy that would return that the Pod placement should be in Rack A if possible, then Row 2 and definitly in DC 1.</p>
<p dir="auto">This could be used with strechted Kubernetes clusters with AWS Outpost as well. Of course I would like my developers to care more about the placement of the resources they connect to, but few do.</p>
<p dir="auto">I&#39;m working on a solution when the external service is multi-AZ capable, then placement of the Pod is not important, but traffic routing is. Stay tuned.</p>

<details>
<summary>What was the motivation for this project?</summary>
<p dir="auto">I was working on a platform team that got a rather special support request: &#34;Our application is <em>slower</em> in production, help?&#34;. It turned out that their production workload was (by chance) deployed in another zone than their RDS instance. Apparently, cross-AZ latency is actually relatively large compared to same-zone latency. Hence this solution.</p>
</details>
<details>
<summary>Why not just use static node affinity rules?</summary>
<p dir="auto">While node affinity rules could solve this problem manually, they lack the ability to automatically adapt when external resources move. For example, RDS instances &#34;frequently&#34; move between availability zones during maintenance events, patching, or instance resizing, which would require constant manual updates to affinity rules.</p>
</details>
<details>
<summary>Why do you include both `zone` and `zoneId`?</summary>
<p dir="auto">Because your <code>eu-central-1a</code> is probably different from my <code>eu-central-1a</code>. AWS maps physical Availability Zones randomly to the AZ names for each AWS account. The Availability Zone ID (<code>zoneId</code>) is the same for everyone. See the <a href="https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html" rel="nofollow">AWS documentation</a> for more details.</p>
</details>
<details>
<summary>Does this work in GCP or Azure?</summary>
<p dir="auto">Kinda. In AWS, subnets are zonal resources, making it easy to map an IP to a zone. In GCP and Azure, subnets are regional resources. This means you would have to query the cloud provider API to determine which zone an IP address is currently located in, which adds significant complexity (permissions, rate limiting, etc.).</p>
</details>
<details>
<summary>Have you considered alternative solutions?</summary>
<p dir="auto">Yes:</p>
<ul dir="auto">
<li><strong>Modifying manifests in the CI/CD pipeline:</strong> Adds complexity and requires re-triggering pipelines when the resource location changes.</li>
<li><strong>A sidecar container:</strong> Adds complexity, and the Pod would need permissions to query the cloud API.</li>
<li><strong>A custom controller:</strong> Same issues with permissions and complexity.</li>
<li><strong>Setting affinity on the Deployment/StatefulSet:</strong> Could work with a background job in Kyverno. The environment is mainly spot instances, so Pods are already frequently re-scheduled. Also determined that we would like to avoid any sync issues with ArgoCD, just to keep it simple.</li>
</ul>
</details>

<ul dir="auto">
<li><strong>Support for multiple A-records:</strong> The current solution does not work for endpoints that return multiple A records (e.g., an AWS Application Load Balancer). I&#39;m working on a solution for this, so if your interested let me know.</li>
</ul>
</article></div></div>
  </body>
</html>
