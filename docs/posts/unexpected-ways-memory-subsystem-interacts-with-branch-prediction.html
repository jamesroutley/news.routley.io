<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://johnnysswlab.com/unexpected-ways-memory-subsystem-interacts-with-branch-prediction/">Original</a>
    <h1>Unexpected ways memory subsystem interacts with branch prediction</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>We at <strong>Johnny’s Software Lab LLC</strong> are experts in performance. If performance is in any way concern in your software project, feel free to <a href="https://johnnysswlab.com/contact/">contact us</a>.</em></p><p>This is the 15th post in the series about memory subsystem optimizations. You can also explore other posts in the same series <a href="https://johnnysswlab.com/category/performance/low-level-performance/memory-performance/" target="_blank" rel="noreferrer noopener">here</a>.</p><p>In this post we will explore how branch prediction interacts with the memory subsystem. We assume you know what branch prediction is and also understand the memory subsystem on modern processors.</p><h2>Branch Prediction and Memory Subsystem</h2><h3>A Quick Introduction to Branch Prediction and Speculative Execution</h3><p>Branch prediction circuitry is part of many modern CPUs and it is used to speed up computation when the CPU doesn’t have enough work. In essence, the CPU tries to predict the outcome of a conditional branch, before the condition of the branch is evaluated. We call this event <em>speculation on the outcome of the branch</em>, and the instructions executed after this branch are executed <em>speculatively</em>, meaning their results will get discarded if prediction turned out to be wrong. Let’s look at the example code:</p><pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">for (size_t i = 0; i &lt; n; i++) {
   if (a[i] &gt;= 0) {
       positive++;
   } else {
       negative++;
   }
}</pre><p>In the above example, the result of the condition <code>if (a[i] &gt;= 0)</code> can be speculated, if operand <code>a[i]</code> is not available. Let’s assume that the CPU has predicted that the outcome of this branch will be true. It then starts to execute <code>positive++</code>. Later, the value of <code>a[i]</code> becomes available. If the branch outcome was correctly speculated, then the CPU has done some useful work and nothing is wasted. However, if the branch outcome was mispredicted, the CPU reverts the speculatively executed instructions and start executing the else branch.</p><p>Alternative to this would be to wait for the operand <code>a[i]</code> to become available and only then decide which instructions to execute – those in <code>if</code> or <code>else</code> branch. However, in most cases this approach would be slower, for obvious reasons.</p><h3>Details of Branch Prediction</h3><p>When a branch prediction is successful, this is an overall win for the performance. So, in the case of branches that are easily to predict<sup><a href="#footnote_0_3658" id="identifier_0_3658" title="Many branches are easy to predict, because they mostly evaluate to true, false, or some predictable pattern">1</a></sup>, there is nothing we can do to increase software efficiency or memory subsystem efficiency. But what happens when a branch is mispredicted?</p><p>When a branch is mispredicted, it means that instructions that were speculatively executed are not needed and their results should be discarded. Among them, load and store instructions, even when speculatively executed, spend memory subsystem resources. And because of misprediction, these resources were wasted.</p><p>If our goal is to decrease the pressure on the memory subsystem, then <a href="https://johnnysswlab.com/frugal-programming-saving-memory-subsystem-bandwidth/#cache-pollution-due-to-speculation" target="_blank" rel="noreferrer noopener">writing branchless code would be the answer</a>. However, our principal goal is software that runs faster, not software that utilizes the least amount of memory bandwidth. And to make software faster, we need to understand how branch prediction and memory subsystem resources interact.</p><h3>Interaction Between Speculation and Memory Subsystem</h3><p>When analyzing the performance of branch prediction, we need to have three things in mind:</p><ul><li>Speculation is generally useful, because it allows the CPU to do potentially useful work. Very often, the reason why the CPU needs to speculate is because an operand hasn’t been read from the memory subsystem – speculation is a way to move forward because of the bottleneck in the memory subsystem.</li><li>If a branch is mispredicted, there is a penalty to pay – the results of speculatively executed instructions needs to be reverted. The price depends on the CPU, but is not too high and it is typically around 10-20 cycles.</li><li>Speculation can increase the required memory subsystem bandwidth, because some data that is fetched will not be used. On the up side, it allows some loads/stores to be issued earlier, which is very important for high-latency loads (coming from the memory or larger data caches).</li></ul><p>The alternative to branchful code is branchless code, where both the bodies of <code>if</code> and <code>else</code> statements are evaluated, and the correct result selected based on the condition. This guarantees no branch misprediction penalty, but has some disadvantages:</p><ul><li>Evaluating bodies of both <code>if</code> and <code>else</code> statements is more expensive.</li><li>The CPU cannot speculate. In practice, this means some instructions will get issued later then with branchful versions.</li></ul><p>As it is more apparent now, the question of performance and branch prediction is complex, because it is a compromise between three factors: branch misprediction penalty, load/store latency and memory subsystem bandwidth. In practice, however, the software performance in the presence of branches looks like this:</p><ul><li>If the data is mostly coming from fast caches, like L1 or possibly L2 caches, then branchless version will typically be faster. Loads from L1 and L2 caches have low latencies; in this case the misprediction penalty will be higher than the latency saved because loads were issued earlier.</li><li>If the data is mostly coming from slower caches of memory, then branchful version will typically be faster. These loads have higher latencies and issuing them earlier, even when they are not needed, will have a positive effect on performance.</li></ul><p><i>Like what you are reading? Follow us on <a rel="noreferrer noopener" href="https://www.linkedin.com/company/johnysswlab" target="_blank">LinkedIn </a>, <a rel="noreferrer noopener" href="https://twitter.com/johnnysswlab" target="_blank">Twitter</a> or <a rel="noreferrer noopener" href="https://mastodon.online/@johnnysswlab" target="_blank">Mastodon</a> and get notified as soon as new content becomes available.</i></p><h2>Techniques</h2><p>To make software faster, you would need to select a branchless or a branchful implementation of the algorithm, as described earlier. However, implementing branchless code in C/C++ turns out to be not that simple!</p><p>When it comes the branches, the compiler is free to emit or not to emit branches according to its own heuristics. In practice, most of the time it will emit branchful code, but you would need to check this in the compiler’s assembly report to make sure. That being said, implementing branchless version can be a challenge. In this section we present two techniques on how to implement branchless algorithms.</p><h3>Branchless using assembly or compiler intrinsics</h3><p>The only 100% certain way to guarantee that a piece of code is written as branchless code is by writing it in assembly or compiler intrinsics. Going into details about how to write assembly is beyond the scope of this post, but for the reference, here are the two versions of binary search we used later in testing, branchful <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2023-12-branches-memory/binary_search.cpp#L26" target="_blank" rel="noreferrer noopener">original </a>and branchless with <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2023-12-branches-memory/binary_search.cpp#L35">conditional moves</a>.</p><p>On X86-64, if your code is working with floats or doubles, you can use compiler intrinsics that you would normally using for vectorization. Compiler intrinsics for floats and doubles exist for non-vectorized code as well, since floating-point processing on modern X86-64 processors is done in vector registers. Here is an example of branchless quicksort we used later in testing written using compiler intrinsics: <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2022-01-sort/quicksort.h#L22" target="_blank" rel="noreferrer noopener">original code</a>, <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2022-01-sort/quicksort.h#L43" target="_blank" rel="noreferrer noopener">branchless code</a>.</p><h3>Branchless using arithmetics</h3><p>You can use arithmetics to go branchless. Take the following code segment as an example:</p><pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">if (a[i] &gt; 0) {
   cnt++;
}</pre><p>Rewriting using arithmetic takes advantage of the fact that the expression <code>a[i] &gt; 0</code> has an arithmetic value 1 if true and 0 if false. So the whole expression can be rewritten as:</p><pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">cnt += a[i]</pre><p>For a more complicated example of using arithmetics to implement a branchless version of binary search, take a look at here: <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2023-12-branches-memory/binary_search.cpp#L26" target="_blank" rel="noreferrer noopener">original</a>, <a href="https://github.com/ibogosavljevic/johnysswlab/blob/master/2023-12-branches-memory/binary_search.cpp#L53" target="_blank" rel="noreferrer noopener">branchless using arithmetics</a>.</p><h2>Experiments</h2><p>Here we present two experiments to measure how speculation and memory subsystem interact. All the experiments were executed on Intel(R) Core(TM) i5-10210U system. Each experiment was executed five times, and we report an average result. Standard deviation was small. The source code for all experiments is available <a href="https://github.com/ibogosavljevic/johnysswlab/tree/master/2023-12-branches-memory" target="_blank" rel="noreferrer noopener">here</a>.</p><h3>Branchless Binary Search</h3><p>For the first experiment, we measure the performance of three versions of binary search: simple (branchful), branchless with conditional moves and branchless with arithmetics. The source code of the binary search algorithm is this:</p><pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">int binary_search(int* array, int number_of_elements, int key) {
    int low = 0, high = number_of_elements-1, mid;
    while(low &lt;= high) {
        mid = (low + high)/2;

        if(array[mid] &lt; key)
            low = mid + 1; 
        else if(array[mid] == key)
            return mid;
        else
            high = mid-1;
    }
    return -1;
}</pre><p>In the branchful version, the values for <code>low</code> and <code>high</code> are speculatively calculated, even before <code>array[mid]</code> has been fetched from the memory subsystem. But these speculations are correct only 50% of the time, so this code suffers from branch mispredictions.</p><p>Let’s measure the performance of binary search on arrays of various sizes. In all cases we perform 4M lookups.</p><figure><table><thead><tr><th>Array Size (in elements)</th><th>Original</th><th>Conditional Moves</th><th>Arithmetics</th></tr></thead><tbody><tr><td>4 K</td><td>Runtime: 0.22 s</td><td>Runtime: 0.14 s</td><td>Runtime: 0.19 s</td></tr><tr><td>16 K</td><td>Runtime: 0.26 s</td><td>Runtime: 0.19 s</td><td>Runtime: 0.24 s</td></tr><tr><td>64 K</td><td>Runtime: 0.32 s</td><td>Runtime: 0.24 s</td><td>Runtime: 0.31</td></tr><tr><td>256 K</td><td>Runtime: 0.43 s</td><td>Runtime: 0.39 s</td><td>Runtime: 0.47 s</td></tr><tr><td>1 M</td><td>Runtime: 0.56 s</td><td>Runtime: 0.59 s</td><td>Runtime: 0.70 s</td></tr><tr><td>4 M</td><td>Runtime: 1.127 s</td><td>Runtime: 1.48 s</td><td>Runtime: 1.59 s</td></tr><tr><td>16 M</td><td>Runtime: 1.65 s</td><td>Runtime:  2.75 s</td><td>Runtime: 2.90 s</td></tr></tbody></table></figure><p>If you observe the table above you will notice:</p><ul><li>The conditional move version is faster than the branchful version until the array size 256 K elements. At the size of 1 M, the branchful version is faster.</li><li>The conditional move version executes about two times more instructions than the branchful version.</li><li>The arithmetic version executes executes about 2.5 times more instructions than the branchful version.</li><li>The branchful version loads more data from the memory than the other two versions. For the largest array size, it transfers 2.5 more data then the other versions.</li></ul><p>There is a price to branch misprediction, but there is also the price of not issuing loads soon enough. For smaller array sizes, branchless version is faster, but for larger array size, branchful version is faster.</p><p><i>Like what you are reading? Follow us on <a rel="noreferrer noopener" href="https://www.linkedin.com/company/johnysswlab" target="_blank">LinkedIn </a>, <a rel="noreferrer noopener" href="https://twitter.com/johnnysswlab" target="_blank">Twitter</a> or <a rel="noreferrer noopener" href="https://mastodon.online/@johnnysswlab" target="_blank">Mastodon</a> and get notified as soon as new content becomes available.</i></p><h3>Branchless Quicksort and Heapsort</h3><p>For the second experiment, we pick the Quicksort sorting algorithm<sup><a href="#footnote_1_3658" id="identifier_1_3658" title="This measurement has already been done in the post Why is quicksort faster than heapsort? And how to make them faster?">2</a></sup>. The key component of quicksort is the data partitioning. For partitioning, we pick a pivot element randomly from the array. Next, the data in the array is partitioned into two halves: the left half where all elements smaller than the pivot are and the right half with the remaining. The partitioning algorithm looks like this:</p><pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">static int partition(std::vector&lt;float&gt;&amp; vector, int low, int high) {
    float pivot = vector[high];

    int i = (low - 1);

    for (int j = low; j &lt; high; j++) {
        if (vector[j] &lt;= pivot) {
            i++;
            std::swap(vector[i], vector[j]);
        }
    }
    i = i + 1;
    std::swap(vector[i], vector[high]);
    return i;
}</pre><p>The critical branch is on line 7. This branch is difficult to predict. The CPU speculatively executes data swap, which consists of two memory loads and two memory stores. The access to data is sequential, which means that the algorithm optimally uses the available memory bandwidth.</p><p>We implemented the same algorithm using compiler intrinsics to make it branchless. Here is the runtime for sorting an array of 32 M floats:</p><figure><table><thead><tr><th>Metric</th><th>Quicksort branchful</th><th>Quicksort branchless</th></tr></thead><tbody><tr><td>Runtime</td><td>2.94 s</td><td>1.69 s</td></tr><tr><td>Instructions</td><td>8.81 billion</td><td>14.7 billion</td></tr><tr><td>Cycles</td><td>11.4 billion</td><td>6.5 billion</td></tr><tr><td>CPI</td><td>1.3</td><td>0.44</td></tr><tr><td>Memory Data Volume</td><td>5.23 GB</td><td>4.21 GB</td></tr></tbody></table><figcaption>Quicksort original vs branchless</figcaption></figure><p>Going branchless has actually made sorting faster, although it was executing almost two times more instructions! The algorithm is accessing data sequentially, which means that the data prefetcher is able to quickly deliver the data to the CPU.</p><p>In this case, the price of speculating wrongly is high compared to latency of loads, and the branchless version is version. Branchless version also fetches less data from the memory subsystem.</p><p>Let’s contrast this with heapsort implementation. The heapsort implementation builds an in-array heap. The algorithm looks like this:</p><pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">static void heapify(std::vector&lt;float&gt;&amp; vec, int n, int i) {
    int largest = i;

    int start = 2 * i + 1;
    int end = std::min(start + 2, n);

    for (int j = start; j &lt; end; j++) {
        if (vec[j] &gt; vec[largest]) {
            largest = j;
        }
    }

    if (largest != i) {
        std::swap(vec[i], vec[largest]);

        heapify_k(vec, n, largest);
    }
}</pre><p>The loop on lines 7-11 is short and has at most two iterations. Then the function <code>heapify</code> calls itself. This algorithm creates a heap, by building in-array tree. If the current node of the tree is <code>i</code>, the left child is at position <code>2 * i + 1</code> and the right child is at position <code>2 * i + 2</code>. Performing a search on such a tree results, at least from the hardware perspective, with a large number of random memory accesses.</p><p>Here are the runtimes of heapsort, both branchless and branchfull version on the same array of 32M floats:</p><figure><table><thead><tr><th></th><th>Heapsort branchful</th><th>Heapsort branchless</th></tr></thead><tbody><tr><td>Runtime</td><td>12.6 s</td><td>29.1 s</td></tr><tr><td>Instructions</td><td>8.94 billion</td><td>35.5 billion</td></tr><tr><td>Cycles</td><td>41.6 billion</td><td>73.5 billion</td></tr><tr><td>CPI</td><td>1.06</td><td>2.07</td></tr><tr><td>Memory Data Volume</td><td>69.3 GB</td><td>79.3 GB</td></tr></tbody></table><figcaption>Heapsort original vs branchless</figcaption></figure><p>If we look at these numbers, we see the complete opposite to Quicksort algorithm. The branchful versionis two times faster than the branchless version. By its nature, heapsort performs a lot of random memory accesses. On such a large array, random memory accesses are mostly served from the memory, not the caches. They have a long latency and ideally they should be issued as soon as possible.</p><p><i>Like what you are reading? Follow us on <a rel="noreferrer noopener" href="https://www.linkedin.com/company/johnysswlab" target="_blank">LinkedIn </a>, <a rel="noreferrer noopener" href="https://twitter.com/johnnysswlab" target="_blank">Twitter</a> or <a rel="noreferrer noopener" href="https://mastodon.online/@johnnysswlab" target="_blank">Mastodon</a> and get notified as soon as new content becomes available.</i></p></div></div>
  </body>
</html>
