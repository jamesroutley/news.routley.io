<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.shruggingface.com/blog/how-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog">Original</a>
    <h1>I used Stable Diffusion and Dreambooth to create an art portrait of my dog</h1>
    
    <div id="readability-page-1" class="page"><div><article><header><time>April 16, 2023</time></header><section><h3>Introduction</h3>
<p>When I first started playing with Stable Diffusion text-to-image generation, in August 2022, my immediate reaction was, &#34;ZOMG! I need to make art prints for my art wall!&#34;. Only to then immediately face-plant because vanilla Stable Diffusion is quite challenging to tame. If you are trying to reproduce a specific subject, you need to utilize additional strategies and techniques, none of which existed at the time.</p>
<p>In the following months, several new community projects have emerged that aim to give the AI artist full creative control over the visual outputs they are trying to bring to life. One such technique is <a href="https://huggingface.co/blog/lora">LoRA (Low-Rank Adaptation)</a>. I explored using LoRA in my posts about <a href="https://www.shruggingface.com/blog/self-portraits-with-stable-diffusion-and-lora">Making Self Portraits with Stable Diffusion</a> and <a href="https://www.shruggingface.com/blog/blending-artist-styles-together-with-stable-diffusion-and-lora">Blending Custom Artist Styles</a>.</p>
<p>An even more popular technique is <a href="https://dreambooth.github.io/">Dreambooth</a>, and that is what we will focus on for the remainder of this blog post. I will walk through my entire workflow/process for bringing Stable Diffusion to life as a high-quality framed art print. We‚Äôll touch on making art with Dreambooth, Stable Diffusion, Outpainting, Inpainting, Upscaling, preparing for print with Photoshop, and finally printing on fine-art paper with an Epson XP-15000 printer.</p>
<p>So without further ado, let‚Äôs dive in!</p>
<h3>What is Dreambooth?</h3>
<p>Dreambooth is a fine-tuning technique for text-to-image diffusion AI models. Basically, that just means that you can ‚Äúfine-tune‚Äù the already capable open source Stable Diffusion model to produce reliable and consistent images of subjects and styles which you define.</p>
<figure><img alt="Diagram of how Dreambooth works from a high level." loading="lazy" width="1589" height="593" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled.webp&amp;w=3840&amp;q=75"/><figcaption><em>Diagram of how Dreambooth works from a high level.</em></figcaption></figure>
<p>If you are interested in this sort of thing, I highly recommend reading through the Dreambooth paper, which you can find here <a href="https://arxiv.org/abs/2208.12242">https://arxiv.org/abs/2208.12242</a>; while there are some technical sections, they also include many image examples which helps you build an intuition about what is possible. I found the Dreambooth paper to be hugely inspirational, and it actually led me to making this art project and writing this blog post; perhaps mostly because there are a ton of dog photo examples üòÖ. I‚Äôm including a few images from their paper below.</p>
<figure><img alt="It‚Äôs like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you‚Ä¶ - https://dreambooth.github.io/" loading="lazy" width="1650" height="462" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25201.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25201.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25201.webp&amp;w=3840&amp;q=75"/><figcaption><em>It‚Äôs like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you‚Ä¶ - <a href="https://dreambooth.github.io/">https://dreambooth.github.io/</a></em></figcaption></figure>
<figure><img alt="5-dog photos in, endless generated images out. The dataset for these lives here: https://github.com/google/dreambooth/tree/main/dataset/dog5" loading="lazy" width="1233" height="602" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25202.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25202.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FUntitled%25202.webp&amp;w=3840&amp;q=75"/><figcaption><em>5-dog photos in, endless generated images out. The dataset for these lives here: <a href="https://github.com/google/dreambooth/tree/main/dataset/dog5">https://github.com/google/dreambooth/tree/main/dataset/dog5</a></em></figcaption></figure>
<h3>How To Train Your Own Dreambooth Model with Replicate</h3>
<p>For this project and post, we‚Äôre going to train a Dreambooth model on photos of my best friend, üßÄ¬†Queso.</p>
<p>Queso is a very photogenic and cuddly English Cream Golden Retriever, and he is the goodest boy that‚Äôs ever existed, which makes him the perfect subject for training a custom Dreambooth model!</p>
<img alt="Quesito Bonito, you are my favrito." loading="lazy" width="960" height="1280" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FIMG_5672_Large.webp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FIMG_5672_Large.webp&amp;w=1920&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FIMG_5672_Large.webp&amp;w=1920&amp;q=75"/>
<h4>Building an Image Training Set</h4>
<p>The first thing you need when training a custom Dreambooth model is a ‚Äúhigh quality‚Äù image training set. I put high quality in quotes because I‚Äôve seen pretty good results with less-than-ideal images in the past. However, the common practice is to select several images of your subject in a variety of poses, environments, and lighting conditions. The more variety (in poses, environments, and lighting) you have of your subject, the more generalized and flexible your fine-tuned Dreambooth model will be.</p>
<p>In the paper, they use 3-5 photos for training Dreambooth models; but in the community, it‚Äôs common to use more. So in my case, I gathered 40 photos of Queso in various poses, lighting, and environment.</p>
<p>I chose to cut out the backgrounds of my images since some of them were in very similar environments, and in early tests, I found that those background elements began to show up in my generated images. This is very optional, and I don‚Äôt recommend it unless you run into issues. I was able to do this pretty quickly in Photoshop with the <code>Object Selection</code> tool; quickly selecting Queso, inverting the selection, and deleting the background.</p>
<p>Once I had all my images, I created a <code>.zip</code> file, and uploaded it to s3, where I could reference it by URL. This is important, becasue we will pass this zip file url into the Dreambooth training job in the next step.</p>
<p>Below is an image grid of my Queso training set. Isn‚Äôt he the best boy you‚Äôve ever seen?</p>
<figure><img alt="40 photos of Queso, with the backgrounds removed." loading="lazy" width="1180" height="1076" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-training-image-grid.webp&amp;w=1200&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-training-image-grid.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-training-image-grid.webp&amp;w=3840&amp;q=75"/><figcaption><em>40 photos of Queso, with the backgrounds removed.</em></figcaption></figure>
<h4>Running the Dreambooth Training on Replicate</h4>
<p>For our Dreambooth training adventures, I chose to use <a href="https://replicate.com/">Replicate</a> (as I did in my last few posts). Replicate is nice for projects like this because it minimizes the pain of fumbling with cloud GPUs and manually getting everything installed and set up. You just send an HTTP request without having to think about GPUs or terminating instances when you‚Äôre done. Replicate has a semi-documented Dreambooth training API, which is described in this <a href="https://replicate.com/blog/dreambooth-api">blog post</a>.</p>
<p>If you are adventurous and just want to dive into the deep end I would suggest trying out this set of fast-stable-diffusion google colab notebook by <code>@TheLastBen</code>: <a href="https://github.com/TheLastBen/fast-stable-diffusion">https://github.com/TheLastBen/fast-stable-diffusion</a>. They‚Äôve got a notebook for training Dreambooth models and quickly spinning up the <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111 Stable Diffusion web interface</a>.</p>
<p>Following the <a href="https://replicate.com/blog/dreambooth-api">Replicate Dreambooth documentation blog post</a>, I made a quick one-off bash script with some hard-coded inputs.</p>
<p>Below I‚Äôve included my <code>queso-1.5.sh</code> bash script, which is just copy-pasted from the Replicate blog. Below that, I‚Äôve included a breakdown of the various parameters I‚Äôm using. If you‚Äôre interested in more advanced training parameters, you can find the detailed per-parameter documentation here: <a href="https://replicate.com/replicate/dreambooth/api">https://replicate.com/replicate/dreambooth/api</a></p>
<p>This script takes ~30-40 minutes to run from start to finish, so you might want to take a break and go for a walk with a furry friend. Unfortunately, more training steps mean more time training and 4000 steps is a lot.</p>
<p>You‚Äôll notice there‚Äôs a <code>model</code> field in the JSON request body. Once the training job is complete, a private replicate model will be created at a URL like <a href="https://replicate.com/jakedahn/queso-1-5"><code>https://replicate.com/jakedahn/queso-1-5</code></a> (I left mine private, so it will return a 404). Once this model is created, you‚Äôll be able to generate images via the Replicate web UI, or via the Replicate API.</p>
<pre><code><span>#!/bin/bash</span>

curl -X POST \
    -H <span>&#34;Authorization: Token <span>$REPLICATE_API_TOKEN</span>&#34;</span> \
    -H <span>&#34;Content-Type: application/json&#34;</span> \
    -d <span>&#39;{
            &#34;input&#34;: {
                &#34;instance_prompt&#34;: &#34;a photo of a qdg dog&#34;,
                &#34;class_prompt&#34;: &#34;photograph of a golden retriever dog, 4k hd, high detail photograph, sharp lens, realistic, highly detailed, fur&#34;,
                &#34;instance_data&#34;: &#34;https://shruggyface.s3-us-west-2.amazonaws.com/queso-2023-transparent-all.zip&#34;,
                &#34;max_train_steps&#34;: 4000
            },
            &#34;model&#34;: &#34;jakedahn/queso-1-5&#34;, # The dreambooth model will be added to your Replicate account at this URL. Replace &#34;jakedahn&#34; with your username...
            &#34;trainer_version&#34;: &#34;cd3f925f7ab21afaef7d45224790eedbb837eeac40d22e8fefe015489ab644aa&#34;,
            &#34;webhook_completed&#34;: &#34;https://abc123.m.pipedream.net/queso-1-5&#34;
        }&#39;</span> \
    https://dreambooth-api-experimental.replicate.com/v1/trainings
</code></pre>
<p>Then I ran it like this:</p>
<pre><code>REPLICATE_API_TOKEN=your-token-here ./queso-1-5.sh
</code></pre>
<h4>Breaking down the inputs</h4>
<p>The inputs in this script define <em><strong>how</strong></em> the Dreambooth model is trained, and they are important.</p>
<ul>
<li><code>instance_prompt</code> : The Instance Prompt is sort of like an example prompt that you would use if you wanted to get an image of your model subject. The suggested format is <code>a [identifier] [class noun]</code>.  Interestingly, you want the <code>identifier</code> to be a unique ‚Äútoken‚Äù. That means it should be 3-4 letters, and it should not be a word. I‚Äôve heard some folks have specific tokens that work better for them, I chose <code>qdg</code>.</li>
<li><code>class_prompt</code>: When training Dreambooth models, you need to provide additional ‚ÄúRegularization Images,‚Äù which help to prevent extreme overfitting. Without these images, every image generated will just be trying to recreate the exact images in your training set. By giving the training set additional ‚Äúsimilar‚Äù images, in our case, more photos of golden retrievers, the output model will be more flexible and give better results in more scenarios. By default, Replicate will generate 50 images using your class prompt; I suggest experimenting with more.</li>
<li><code>instance_data</code>: This is a zip file containing all of your training images. Replicate has an API for uploading this file to their servers, but it‚Äôs kind of complicated/involved, so I just self-hosted my file on s3, where I could easily reuse it for future projects.</li>
<li><code>max_train_steps</code>: This is the number of training steps. Higher is better, sort of. I‚Äôve heard multiple conflicting things about this value, but the most consistent thing seems to be ‚Äú100 steps for every training image‚Äù. So since I have 40 images, I used 4000 steps. In previous training runs, I was getting great results with 40 images and 3000 steps ‚Äî so this is something you‚Äôll want to experiment with on your own.</li>
<li><code>trainer_version</code>: The trainer version is important! There are a handful of options that you may want to experiment with.<!-- -->
<ul>
<li>If you want to use Stable Diffusion v1.5, use <code>cd3f925f7ab21afaef7d45224790eedbb837eeac40d22e8fefe015489ab644aa</code></li>
<li>If you want to use Stable Diffusion v2.1, use <code>d5e058608f43886b9620a8fbb1501853b8cbae4f45c857a014011c86ee614ffb</code></li>
</ul>
</li>
<li><code>webhook_completed</code>: Training a dreambooth model takes some time, and getting a notification when it is complete is nice. I use a requestbin from <a href="https://pipedream.com">https://pipedream.com</a> for this webhook url, which gives a simple UI for exploring the data that is sent to the webhook endpoint:</li>
</ul>
<figure><img alt="Screenshot of the Pipedream.com requestbin" loading="lazy" width="1319" height="728" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FScreenshot_2023-04-14_at_2.39.38_PM.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FScreenshot_2023-04-14_at_2.39.38_PM.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2FScreenshot_2023-04-14_at_2.39.38_PM.webp&amp;w=3840&amp;q=75"/><figcaption><em>Screenshot of the Pipedream.com requestbin</em></figcaption></figure>
<h3>Generating Images</h3>
<p>Great! If you have been following along thus far, you should have your very own custom Dreambooth model! Next is the fun part: generating a ridiculous amount of images of your furry friend.</p>
<p>First, we need to write a handful of prompts, and then we can generate hundreds or thousands of images üò±.</p>
<p>Seeing as I am quite possibly the world&#39;s worst prompt engineer, I went the easy route and went for an hour-long stroll down the infinite scroll of <a href="https://lexica.art">Lexica</a>. Lexica is a massive collection of AI-generated imagery, all shared with their prompts. After a while, I picked ten images I thought were cool from the search term <code>dog portrait</code>, and copied their prompts. You can see this vie</p>
<p>I am quite possibly the world&#39;s worst prompt engineer.</p>
<p>I collected the following prompts and replaced the dog breeds with my token <code>qdg</code>:</p>
<pre><code>PROMPTS = [
    <span>&#34;Adorably cute qdg dog portrait, artstation winner by Victo Ngai, Kilian Eng and by Jake Parker, vibrant colors, winning-award masterpiece, fantastically gaudy, aesthetic octane render, 8K HD Resolution&#34;</span>,
    <span>&#34;Incredibly cute golden retriever qdg dog portrait, artstation winner by Victo Ngai, Kilian Eng and by Jake Parker, vibrant colors, winning-award masterpiece, fantastically gaudy, aesthetic octane render, 8K HD Resolution&#34;</span>,
    <span>&#34;a high quality painting of a very cute golden retriever qdg dog puppy, friendly, curious expression. painting by artgerm and greg rutkowski and alphonse mucha &#34;</span>,
    <span>&#34;magnificent qdg dog portrait masterpiece work of art. oil on canvas. Digitally painted. Realistic. 3D. 8k. UHD.&#34;</span>,
    <span>&#34;intricate five star qdg dog facial portrait by casey weldon, oil on canvas, hdr, high detail, photo realistic, hyperrealism, matte finish, high contrast, 3 d depth, centered, masterpiece, vivid and vibrant colors, enhanced light effect, enhanced eye detail, artstationhd &#34;</span>,
    <span>&#34;a portrait of a qdg dog in a scenic environment by mary beale and rembrandt, royal, noble, baroque art, trending on artstation &#34;</span>,
    <span>&#34;a painted portrait of a qdg dog with brown fur, no white fur, wearing a sea captain&#39;s uniform and hat, sea in background, oil painting by thomas gainsborough, elegant, highly detailed, anthro, anthropomorphic dog, epic fantasy art, trending on artstation, photorealistic, photoshop, behance winner &#34;</span>,
    <span>&#34;qdg dog guarding her home, dramatic sunset lighting, mat painting, highly detailed, &#34;</span>,
    <span>&#34;qdg dog, realistic shaded lighting poster by ilya kuvshinov katsuhiro otomo, magali villeneuve, artgerm, jeremy lipkin and michael garmash and rob rey &#34;</span>,
    <span>&#34;a painting of a qdg dog dog, greg rutkowski, cinematic lighting, hyper realistic painting&#34;</span>,
]
</code></pre>
<p>Then I wrote a super quick/bad Python script that iterates through each of these prompts ten times, generating a total of 100 images. I did this many times‚Ä¶ I never get sick of looking at AI generated dog art.</p>
<pre><code><span>import</span> os
<span>import</span> urllib
<span>import</span> random
<span>import</span> replicate

USERNAME = <span>&#39;jakedahn&#39;</span>
MODEL_NAME = <span>&#39;queso-1.5&#39;</span>
MODEL_SLUG = <span>f&#39;<span>{USERNAME}</span>/<span>{MODEL_NAME}</span>&#39;</span>


model = replicate.models.get(MODEL_SLUG)

version = model.versions.<span>list</span>()[<span>0</span>] 

<span>def</span> <span>download_prompt</span>(<span>prompt, negative_prompt=NEGATIVE_PROMPT, num_outputs=<span>1</span></span>):
    <span>print</span>(<span>&#34;=====================================================================&#34;</span>)
    <span>print</span>(<span>&#34;prompt:&#34;</span>, prompt)
    <span>print</span>(<span>&#34;negative_prompt:&#34;</span>, negative_prompt)
    <span>print</span>(<span>&#34;num_outputs:&#34;</span>, num_outputs)
    <span>print</span>(<span>&#34;=====================================================================&#34;</span>)
    image_urls = version.predict(
        prompt=prompt,
        width=<span>512</span>,
        height=<span>512</span>,
        negative_prompt=negative_prompt,
        num_outputs=num_outputs,
    )
    <span>for</span> url <span>in</span> image_urls:
        img_id = url.split(<span>&#34;/&#34;</span>)[<span>4</span>][:<span>6</span>]
        prompt = prompt.replace(<span>&#34; &#34;</span>, <span>&#34;-&#34;</span>).replace(<span>&#34;,&#34;</span>, <span>&#34;&#34;</span>).replace(<span>&#34;.&#34;</span>, <span>&#34;-&#34;</span>)
        out_file = <span>f&#34;data/<span>{MODEL_NAME}</span>/<span>{img_id}</span>--<span>{prompt}</span>&#34;</span>[:<span>200</span>]
        out_file = out_file + <span>&#34;.jpg&#34;</span>

        
        os.makedirs(os.path.dirname(out_file), exist_ok=<span>True</span>)

        <span>print</span>(<span>&#34;Downloading to&#34;</span>, out_file)
        urllib.request.urlretrieve(url, out_file)
    <span>print</span>(<span>&#34;=====================================================================&#34;</span>)

NEGATIVE_PROMPT = <span>&#34;cartoon, blurry, deformed, watermark, dark lighting, image caption, caption, text, cropped, low quality, low resolution, malformed, messy, blurry, watermark&#34;</span>


PROMPTS = [
    <span>&#34;Adorably cute qdg dog portrait, artstation winner by Victo Ngai, Kilian Eng and by Jake Parker, vibrant colors, winning-award masterpiece, fantastically gaudy, aesthetic octane render, 8K HD Resolution&#34;</span>,
    <span>&#34;Incredibly cute golden retriever qdg dog portrait, artstation winner by Victo Ngai, Kilian Eng and by Jake Parker, vibrant colors, winning-award masterpiece, fantastically gaudy, aesthetic octane render, 8K HD Resolution&#34;</span>,
    <span>&#34;a high quality painting of a very cute golden retriever qdg dog puppy, friendly, curious expression. painting by artgerm and greg rutkowski and alphonse mucha &#34;</span>,
    <span>&#34;magnificent qdg dog portrait masterpiece work of art. oil on canvas. Digitally painted. Realistic. 3D. 8k. UHD.&#34;</span>,
    <span>&#34;intricate five star qdg dog facial portrait by casey weldon, oil on canvas, hdr, high detail, photo realistic, hyperrealism, matte finish, high contrast, 3 d depth, centered, masterpiece, vivid and vibrant colors, enhanced light effect, enhanced eye detail, artstationhd &#34;</span>,
    <span>&#34;a portrait of a qdg dog in a scenic environment by mary beale and rembrandt, royal, noble, baroque art, trending on artstation &#34;</span>,
    <span>&#34;a painted portrait of a qdg dog with brown fur, no white fur, wearing a sea captain&#39;s uniform and hat, sea in background, oil painting by thomas gainsborough, elegant, highly detailed, anthro, anthropomorphic dog, epic fantasy art, trending on artstation, photorealistic, photoshop, behance winner &#34;</span>,
    <span>&#34;qdg dog guarding her home, dramatic sunset lighting, mat painting, highly detailed, &#34;</span>,
    <span>&#34;qdg dog, realistic shaded lighting poster by ilya kuvshinov katsuhiro otomo, magali villeneuve, artgerm, jeremy lipkin and michael garmash and rob rey &#34;</span>,
    <span>&#34;a painting of a qdg dog dog, greg rutkowski, cinematic lighting, hyper realistic painting&#34;</span>,
]


random.shuffle(PROMPTS)


<span>for</span> i <span>in</span> <span>range</span>(<span>10</span>):
    <span>for</span> prompt <span>in</span> PROMPTS:
        download_prompt(prompt)
</code></pre>
<p>After running this script many times, I generated at least 1000 images. I would say ~20% were nonsense, and about 80% were cute, funny, or accurate. These are some of my favorites:</p>
<figure><img alt="Screenshot of the Pipedream.com requestbin" loading="lazy" width="1656" height="1481" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-gen-grid.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-gen-grid.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fqueso-gen-grid.webp&amp;w=3840&amp;q=75"/><figcaption><em>Cherry-picked favorite images</em></figcaption></figure>
<h3>Finding THE ONE</h3>
<p>Eventually, after generating hundreds of artificial Quesos, I landed on this one. I love the color palette, which is vibrant and contrasty. I love the texture and all of the fine lines and details. It also captures Queso‚Äôs eyes pretty well, which is ultimately what sold me. Every time I look at it, I think, ‚Äúdang, that‚Äôs Queso!‚Äù</p>
<figure><img alt="Screenshot of the Pipedream.com requestbin" loading="lazy" width="512" height="512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F7150-1.webp&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F7150-1.webp&amp;w=1080&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F7150-1.webp&amp;w=1080&amp;q=75"/><figcaption><em>This. This is the one.</em></figcaption></figure>
<p>Now, the end goal for this art project was to end up with a high-quality art print that I could frame and put on my art wall. While cool, this image wouldn‚Äôt make for a very good art print; the awkward crop at the top and bottom limits its potential. Also, if I were to print <code>512x512px</code> at <code>300dpi</code>, this image would only be <code>1.7x1.7‚Äù</code> on paper. I‚Äôm targeting <code>11x17‚Äù</code>.</p>
<p>So the next step in this project was to fix the awkward cropping. Boy, it‚Äôs a real bummer that we can‚Äôt just add new pixels to the top and bottom.</p>
<p>Just kidding! We can! That‚Äôs where <a href="https://arxiv.org/abs/2104.00675">Outpainting</a> comes in.</p>
<h3>Outpainting</h3>
<p>Outpainting is a technique where you can generate new pixels that seamlessly extend an image&#39;s existing bounds. This means we can just generate new pixels on the top and bottom of our image, to get a complete artistic rendition of Queso. As far as I understand, and I may be wrong ü§∑‚Äç‚ôÇÔ∏è, <a href="https://openai.com/blog/dall-e-introducing-outpainting">outpainting for diffusion models was first implemented by OpenAI</a>.</p>
<p>They have an excellent example in their announcement blog post, which I‚Äôve included here.</p>
<video src="https://assets-shruggingface-com.s3-us-west-2.amazonaws.com/assets/how-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog/girl-with-a-pearl-earring-bts-cropped.mp4" controls="" loop="" autoplay="" muted=""></video>
<p><em>Original:¬†Girl with a Pearl Earring¬†by Johannes Vermeer Outpainting: August Kamp √ó DALL¬∑E, taken from <a href="https://openai.com/blog/dall-e-introducing-outpainting">https://openai.com/blog/dall-e-introducing-outpainting</a></em></p>
<p>I haven‚Äôt used <a href="https://openai.com/product/dall-e-2">Dall-E 2</a> much, so I wanted to give it a try and see how it did with outpainting. In my opinion, the OpenAI outpainting user experience and interface is the best I‚Äôve tried; but I wasn‚Äôt a huge fan of the generated pixel results. Here I‚Äôve got a little video snippet where I added pixels to the top of my image, but all of the results were a little too cartoony for my liking; they also made it look like Queso was wearing a tiara headband.</p>
<p>‚ùå¬†Strike 1</p>
<video src="https://assets-shruggingface-com.s3-us-west-2.amazonaws.com/assets/how-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog/openai-outpainting.mp4" controls="" loop="" autoplay="" muted=""></video>
<p>Then I tried using the <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111 Stable Diffusion WebUI</a> from the notebooks I mentioned above <a href="https://github.com/TheLastBen/fast-stable-diffusion">https://github.com/TheLastBen/fast-stable-diffusion</a>. The Automatic1111 UI is the most full-featured and extensible UI in the community, so I figured outpainting would Just Work‚Ñ¢Ô∏è. I was wrong üòê. it seemed to take the top and bottom-most row of pixels and extend them down from 512px tall to 1344px tall.</p>
<p>‚ùå¬†Strike 2!</p>
<figure><img alt="Screenshot of broken outpainting functionality in the AUTOMATIC1111 UI." loading="lazy" width="1611" height="1203" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fa111-outpainting0.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fa111-outpainting0.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fa111-outpainting0.webp&amp;w=3840&amp;q=75"/><figcaption><em>Screenshot of broken outpainting functionality in the AUTOMATIC1111 UI.</em></figcaption></figure>
<p>Finally, I tried using the <a href="https://apps.apple.com/us/app/draw-things-ai-generation/id6444050820">Draw Things Mac App</a>. I actually really like the Draw Things app. It does much of what Automatic1111 does but has a better UI and runs locally for free on an M1/M2 Macbook Pro. However, I couldn‚Äôt get the outpainting UI to actually work üòê. So I ultimately settled on using <code>img2img</code> of my <code>512x512px</code> image to a <code>768x1152px</code> image.</p>
<p>You may notice that the starting <code>512x512px</code> image here is slightly different than the ones above. That‚Äôs because I got excited and started playing with inpainting (which I‚Äôll cover in more detail next) before expanding the image. Don‚Äôt worry ‚Äòbout it!</p>
<figure><img alt="Screenshot of DrawThings.app img2img resize." loading="lazy" width="1611" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fimg2img-outpainting.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fimg2img-outpainting.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fimg2img-outpainting.webp&amp;w=3840&amp;q=75"/><figcaption><em>Screenshot of DrawThings.app img2img resize.</em></figcaption></figure>
<figure><img alt="DrawThings.app img2img resize results. 768x1152px." loading="lazy" width="1611" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupsized-queso.webp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupsized-queso.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupsized-queso.webp&amp;w=3840&amp;q=75"/><figcaption><em>DrawThings.app img2img resize results. 768x1152px.</em></figcaption></figure>
<p>This worked pretty well! I was kind of surprised because I thought img2img would start generating weirdness in the unfilled space, but it did the right thing here.</p>
<p>You will notice that the hair detail in the forehead and the neck medallion circle are a little wobbly and un-detailed. I‚Äôm also not a huge fan of how the bottom looks like flowers. How in the world are we going to change that?</p>
<p>Inpainting, of course!</p>
<h3>Inpainting</h3>
<p>Inpainting is a technique where you mask out (paint over) a specific selection of your image and replace it with newly generated pixels. This is sort of similar and related to outpainting, but it‚Äôs used for fixing issues and adding details.</p>
<p>The inpainting step is where I spent most of my time on this project. Slowly and iteratively selecting small chunks, generating new imagery for those chunks, and inching towards something better. This is the life of an artist, materializing a vision from an idea. When I paint in real life (IRL), I do this exact same practice with acrylic paints on canvas. Slowly bringing sections of the painting to life.</p>
<p>I tried the DrawThings app again for inpainting, but it wasn‚Äôt working correctly. So I returned to using the Automatic1111 UI for inpainting, which worked very well.</p>
<p>Below is a screenshot of what the inpainting process looks like. On the left, you paint over a bunch of places you want to modify/change, and then you generate many new iterations at once. I love doing this because it lets you test tens of directions in a few seconds instead of painstakingly testing tens of directions over the course of hours.</p>
<figure><img alt="Example of Upscaling in the AUTOMATIC1111 UI. The left shows the options used, the right shows the result." loading="lazy" width="2025" height="1285" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Finpainting-example.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Finpainting-example.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Finpainting-example.webp&amp;w=3840&amp;q=75"/><figcaption><em>Example of Upscaling in the AUTOMATIC1111 UI. The left shows the options used, the right shows the result.</em></figcaption></figure>
<p>I recorded a little video of me scrolling back and forth through inpainting iterations for Queso‚Äôs ears. I can‚Äôt help but feel like this ‚Äúgenerate 20 things, pick 1‚Äù is the new foundational workflow for artists in this new AI-capable world.</p>
<video src="https://assets-shruggingface-com.s3-us-west-2.amazonaws.com/assets/how-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog/inpainting.mp4" controls="" loop="" autoplay="" muted=""></video>
<h3>Final Result</h3>
<p>After a few hours of playing with inpainting, I got to a place I was happy to call finished.</p>
<p>I made a neat video showing the before/after visualization of this AI-based post-processing journey. I love how the cartoony pieces of the original image were replaced with more fur-like elements; while keeping a consistent coloring scheme. I love it!</p>
<video src="https://assets-shruggingface-com.s3-us-west-2.amazonaws.com/assets/how-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog/comparison.mp4" controls="" loop="" autoplay="" muted=""></video>
<p>But this image is only 640x1280px; it needs to be much larger. EMBIGGEN!</p>
<p>That‚Äôs where Upscaling comes in.</p>
<h3>Upscaling</h3>
<p>Upscaling is just a fancy way to ‚Äúblow up‚Äù an image. Taking a small image and making it much larger. This is super helpful for high-quality prints, where you print at a <code>300dpi</code> resolution ‚Äî that‚Äôs a lot of pixels! Sometimes, it feels like I‚Äôm clicking the ‚ÄúENHANCE‚Äù button ü§Ø</p>
<p>I like to use the <code>Real-ESRGAN 4x+</code> upscaling model. In the screenshot below, you can see I 6x‚Äôd the image. To do this, you‚Äôll need a GPU with a lot of VRAM. I ran this in google colab on an Nvidia A100 with 40GB VRAM. When I‚Äôm not running in a colab notebook, I will usually use this Replicate model <a href="https://replicate.com/nightmareai/real-esrgan">https://replicate.com/nightmareai/real-esrgan</a></p>
<figure><img alt="Example of Inpainting. The black on the left side was me selecting areas to re-generate. The right shows the results." loading="lazy" width="2027" height="1228" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-fin.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-fin.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-fin.webp&amp;w=3840&amp;q=75"/><figcaption><em>Example of Upscaling. The black on the left side was me selecting areas to re-generate. The right shows the results.</em></figcaption></figure>
<p>I tried a handful of settings, but the settings in the image above are what I ended with. Interestingly, not all settings are made equal! In the following image, I display three upscaled images side by side. Each of them has varying degrees of compression and loss of detail. They are especially noticeable around the eyes, nose, and fur.</p>
<figure><img alt="Side-by-side comparison of 3 upscaler results, with varied settings." loading="lazy" width="2037" height="939" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-comps.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-comps.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fupscale-comps.webp&amp;w=3840&amp;q=75"/><figcaption><em>Side-by-side comparison of 3 upscaler results, with varied settings.</em></figcaption></figure>
<p>After upscaling, the Queso portrait was <code>2888x3835px</code>, which is <code>9.63x12.78‚Äù</code> at <code>300dpi</code>. However, I‚Äôm targeting a visible space of <code>11x17‚Äù</code> and printing on <code>13x19‚Äù</code> paper.</p>
<p>So the final step is to prepare everything for print.</p>
<h3>Preparing for Print</h3>
<p>In the images above, you may have noticed weird compression artifacts and textures in the green gradient background. I want just to delete all of that and place it on a solid gradient.</p>
<p>First, I used the <code>Object Selection</code> tool to quickly select Queso, then I inverted the selection and deleted the background.</p>
<figure><img alt="Using the `Object Selection` tool to remove the messy green background behind Queso." loading="lazy" width="2056" height="1329" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fphotoshop-crop-2.webp&amp;w=3840&amp;q=75 1x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fphotoshop-crop-2.webp&amp;w=3840&amp;q=75"/><figcaption><em>Using the <code>Object Selection</code> tool to remove the messy green background behind Queso.</em></figcaption></figure>
<p>Then I could resize and position Queso until he was perfectly positioned for the framing I had in mind.</p>
<p>I sent this final image to the printer, but I‚Äôve scaled it down to ~50% size for this blog post. At full resolution, it is <code>13x19‚Äù @ 300dpi</code>, which is <code>3900x5700px</code>.</p>
<figure><img alt="Final Image, which I printed." loading="lazy" width="876" height="1280" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F2022-3-12-queso-print2_Large.webp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F2022-3-12-queso-print2_Large.webp&amp;w=1920&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2F2022-3-12-queso-print2_Large.webp&amp;w=1920&amp;q=75"/><figcaption><em>Final Image, which I printed.</em></figcaption></figure>
<h3>Printing</h3>
<p>To print, I used an <a href="https://www.amazon.com/Expression-XP-15000-Wireless-Wide-Format-Replenishment/dp/B076PLTQQT">Epson XP-15000</a>, the best home printer I‚Äôve ever used. It‚Äôs ~$400 on Amazon, and the ink is expensive, but the results are professional grade.</p>
<p>Another critical aspect of printing a high-quality art print that is intended to be framed is the paper. Epson has super high-quality <a href="https://www.amazon.com/dp/B00007DTD3/ref=twister_B07BVD8DV6">Velvet Fine Art Paper</a>, which the printer is calibrated to print on. The results are stunning.</p>
<figure><img alt="Print-in-progress interior view of the Epson XP-15000, making the Queso magic happen." loading="lazy" width="2016" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp2.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp2.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp2.webp&amp;w=3840&amp;q=75"/><figcaption><em>Print-in-progress interior view of the Epson XP-15000, making the Queso magic happen.</em></figcaption></figure>
<figure><img alt="Nearly complete print-in-progress. Can you feel the excitement?" loading="lazy" width="2016" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp3.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp3.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fp3.webp&amp;w=3840&amp;q=75"/><figcaption><em>Nearly complete print-in-progress. Can you feel the excitement?</em></figcaption></figure>
<h3>Closing Thoughts</h3>
<p>This project is something I‚Äôve wanted to do since the day I first toyed with Stable Diffusion. The pace at which the community is developing new tools and techniques is astounding.</p>
<p>I still feel like there is a lot of missing creative control while making art with Stable Diffusion, but the future is bright! Stable Diffusion hasn‚Äôt even been out for a full year yet, and it&#39;s becoming increasingly obvious to me, through projects like this, that it‚Äôs going to be a game-changer for artists of all backgrounds.</p>
<p>The more I play with it, the more excited I get about it. I can&#39;t wait to see what the future holds for Stable Diffusion, and the world of text-to-image generative art.</p>
<p>Below I‚Äôve included expanded photos of my art wall, including my bestest pal üßÄ Queso ü§ó. The photos don‚Äôt do it justice. The actual detail in print is better than I would have guessed possible.</p>
<figure><img alt="Close-up shot of the Queso print in a 11x17‚Äù Ikea frame." loading="lazy" width="2016" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Ffinal-frame.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Ffinal-frame.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Ffinal-frame.webp&amp;w=3840&amp;q=75"/><figcaption><em>Close-up shot of the Queso print in a 11x17‚Äù Ikea frame.</em></figcaption></figure>
<figure><img alt="Wider view of where Queso fits in with the rest of the art wall." loading="lazy" width="2016" height="1512" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fartwall.webp&amp;w=2048&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fartwall.webp&amp;w=3840&amp;q=75 2x" src="https://www.shruggingface.com/_next/image?url=https%3A%2F%2Fassets-shruggingface-com.s3-us-west-2.amazonaws.com%2Fassets%2Fhow-i-used-stable-diffusion-and-dreambooth-to-create-a-painted-portrait-of-my-dog%2Fartwall.webp&amp;w=3840&amp;q=75"/><figcaption><em>Wider view of where Queso fits in with the rest of the art wall.</em></figcaption></figure></section></article></div></div>
  </body>
</html>
