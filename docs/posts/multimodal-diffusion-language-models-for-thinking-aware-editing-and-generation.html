<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/tyfeld/MMaDA-Parallel">Original</a>
    <h1>Multimodal Diffusion Language Models for Thinking-Aware Editing and Generation</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/tyfeld/MMaDA-Parallel/blob/main/assets/demo.gif"><img src="https://decomposition.al/tyfeld/MMaDA-Parallel/raw/main/assets/demo.gif" alt="Parallel Generation Demo" data-animated-image=""/></a></p><p dir="auto">Demo: Parallel text-image generation in action.</p>
</div>
<p dir="auto">While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation.
To systematically analyze this issue, we propose <strong>ParaBench</strong>, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image.
To resolve this, we propose a parallel multimodal diffusion framework that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. This model, <strong>MMaDA-Parallel</strong>, is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (<strong>ParaRL</strong>), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our approach significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in <strong>Output Alignment</strong> on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis.</p>
<div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/tyfeld/MMaDA-Parallel/blob/main/assets/method.png"><img src="https://decomposition.al/tyfeld/MMaDA-Parallel/raw/main/assets/method.png"/></a></p><p dir="auto">Architecture of MMaDA-Parallel. During Training, image and text responses are masked and predicted in parallel with a uniform mask predictor. During Sampling, the model performs parallel decoding to generate both image and text responses jointly, enabling continuous cross-modal interaction. </p>
</div>

<div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/tyfeld/MMaDA-Parallel/blob/main/assets/lumina_01.png"><img src="https://decomposition.al/tyfeld/MMaDA-Parallel/raw/main/assets/lumina_01.png" alt="Main Results"/></a></p><p dir="auto">Qualitative comparison. </p>
</div>
<div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/tyfeld/MMaDA-Parallel/blob/main/assets/mainresults.png"><img src="https://decomposition.al/tyfeld/MMaDA-Parallel/raw/main/assets/mainresults.png" alt="Main Results"/></a></p><p dir="auto">Quantitative Results on ParaBench.</p>
</div>

<ul dir="auto">
<li><strong>[2025-11-11]</strong> We release our codes and models for <a href="https://arxiv.org/abs/2511.09611" rel="nofollow">MMaDA-Parallel</a>, with two released 8B models <a href="https://huggingface.co/tyfeld/MMaDA-Parallel-A" rel="nofollow">MMaDA-Parallel-A</a> and <a href="https://huggingface.co/tyfeld/MMaDA-Parallel-M" rel="nofollow">MMaDA-Parallel-M</a>.</li>
<li><strong>[2025-11-10]</strong> We release our <a href="https://arxiv.org/abs/2511.09611" rel="nofollow">research paper</a> for Parallel Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation.</li>
</ul>

<p dir="auto"><strong>Note:</strong> Our model has been successfully validated on synthetic datasets focusing on <strong>environments, still life, architecture, and natural landscapes.</strong> Its performance on out-of-distribution inputs—such as human faces or real-world photographic imagery—has not yet been fully explored. We are actively expanding our training corpus to include more diverse datasets.</p>

<p dir="auto">First, start with a torch environment with torch 2.3.1 or higher version, then install the following dependencies:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
<p dir="auto">We provide two varients of MMaDA-Parallel with different tokenizers. MMaDA-Parallel-A is trained with tokenizer Amused-VQ, and MMaDA-Parallel-M is trained with tokenizer Magvitv2.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">2. Experiencing Parallel Gen with MMaDA-Parallel-A</h3><a id="user-content-2-experiencing-parallel-gen-with-mmada-parallel-a" aria-label="Permalink: 2. Experiencing Parallel Gen with MMaDA-Parallel-A" href="#2-experiencing-parallel-gen-with-mmada-parallel-a"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can directly use the local gradio app to experience the parallel generation with MMaDA-Parallel-A:</p>

<p dir="auto">Or you can use the inference script to generate the parallel generation results:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd MMaDA-Parallel-A
python inference.py \
    --checkpoint tyfeld/MMaDA-Parallel-A \
    --vae_ckpt tyfeld/MMaDA-Parallel-A \
    --prompt &#34;Replace the laptops with futuristic transparent tablets displaying holographic screens, and change the drink to a cup of glowing blue energy drink.&#34; \
    --image_path examples/image.png \
    --height 512 \
    --width 512 \
    --timesteps 64 \
    --text_steps 128 \
    --text_gen_length 256 \
    --text_block_length 32 \
    --cfg_scale 0 \
    --cfg_img 4.0 \
    --temperature 1.0 \
    --text_temperature 0 \
    --seed 42 \
    --output_dir output/results_interleave"><pre><span>cd</span> MMaDA-Parallel-A
python inference.py \
    --checkpoint tyfeld/MMaDA-Parallel-A \
    --vae_ckpt tyfeld/MMaDA-Parallel-A \
    --prompt <span><span>&#34;</span>Replace the laptops with futuristic transparent tablets displaying holographic screens, and change the drink to a cup of glowing blue energy drink.<span>&#34;</span></span> \
    --image_path examples/image.png \
    --height 512 \
    --width 512 \
    --timesteps 64 \
    --text_steps 128 \
    --text_gen_length 256 \
    --text_block_length 32 \
    --cfg_scale 0 \
    --cfg_img 4.0 \
    --temperature 1.0 \
    --text_temperature 0 \
    --seed 42 \
    --output_dir output/results_interleave</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">3. Parallel Gen with MMaDA-Parallel-M</h3><a id="user-content-3-parallel-gen-with-mmada-parallel-m" aria-label="Permalink: 3. Parallel Gen with MMaDA-Parallel-M" href="#3-parallel-gen-with-mmada-parallel-m"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="cd MMaDA-Parallel-M
python inference.py interleave_root=./interleave_validation  "><pre><span>cd</span> MMaDA-Parallel-M
python inference.py interleave_root=./interleave_validation  </pre></div>


<ul>
<li> Release the MMaDA-Parallel code and paper.</li>
<li> Evaluation on ParaBench code.</li>
<li> Refine MMaDA-Parallel-M and update the corresponding checkpoint.</li>
<li> Training code for SFT and ParaRL.</li>
</ul>


<div data-snippet-clipboard-copy-content="@article{tian2025mmadaparallel,
  title={MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation},
  author={Tian, Ye and Yang, Ling and Yang, Jiongfan and Wang, Anran and Tian, Yu and Zheng, Jiani and Wang, Haochen and Teng, Zhiyang and Wang, Zhuochen and Wang, Yinjie and Tong, Yunhai and Wang, Mengdi and Li, Xiangtai},
  journal={arXiv preprint arXiv:2511.09611},
  year={2025}
}"><pre><code>@article{tian2025mmadaparallel,
  title={MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation},
  author={Tian, Ye and Yang, Ling and Yang, Jiongfan and Wang, Anran and Tian, Yu and Zheng, Jiani and Wang, Haochen and Teng, Zhiyang and Wang, Zhuochen and Wang, Yinjie and Tong, Yunhai and Wang, Mengdi and Li, Xiangtai},
  journal={arXiv preprint arXiv:2511.09611},
  year={2025}
}
</code></pre></div>

<p dir="auto">This work is heavily based on <a href="https://github.com/Gen-Verse/MMaDA">MMaDA</a> and <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO">Lumina-DiMOO</a>. Thanks to all the authors for their great work.</p>
</article></div></div>
  </body>
</html>
