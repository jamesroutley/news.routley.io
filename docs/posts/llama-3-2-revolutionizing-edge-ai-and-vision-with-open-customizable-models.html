<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1">Original</a>
    <h1>Llama 3.2: Revolutionizing edge AI and vision with open, customizable models</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>We’ve been excited by the <a href="https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/" target="_blank" data-lnfb-mode="ie"><u>impact the Llama 3.1 herd of models have made</u></a> in the two months since we announced them, including the <a href="https://www.meta.ai/?utm_source=llama_meta_site&amp;utm_medium=web&amp;utm_content=Llama_nav&amp;utm_campaign=July_moment" target="_blank" data-lnfb-mode="ie"><u>405B</u></a>—the first open frontier-level AI model. While these models are incredibly powerful, we recognize that building with them requires significant compute resources and expertise. We’ve also heard from developers who don’t have access to these resources and still want the opportunity to build with Llama. As Meta Founder and CEO Mark Zuckerberg shared today at Connect, they won’t have to wait any longer. Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B) and lightweight, text-only models (1B and 3B) that fit onto select edge and mobile devices.</p><p>It’s only been a year and a half since we first announced Llama, and we’ve made incredible progress in such a short amount of time. This year, <a href="https://ai.meta.com/blog/llama-usage-doubled-may-through-july-2024/" target="_blank" data-lnfb-mode="ie"><u>Llama has achieved 10x growth</u></a> and become the standard for responsible innovation. Llama also continues to lead on openness, modifiability, and cost efficiency, and it’s competitive with closed models—even leading in some areas. We believe that openness drives innovation and is the right path forward, which is why we continue to share our research and collaborate with our partners and the developer community.</p><p>We’re making Llama 3.2 models available for download on <a href="https://llama.meta.com/" target="_blank" data-lnfb-mode="ie"><u>llama.com</u></a> and <a href="https://huggingface.co/meta-llama" target="_blank" data-lnfb-mode="ie"><u>Hugging Face</u></a>, as well as available for immediate development on our broad ecosystem of partner platforms. Partners are an important part of this work, and we’ve worked with over 25 companies, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, and Snowflake, to enable services on day one. For the Llama 3.2 release, we’re also working with on-device partners Arm, MediaTek, and Qualcomm to offer a broad range of services at launch. Starting today, we’re also making <a href="https://github.com/meta-llama/llama-stack" target="_blank" data-lnfb-mode="ie"><u>Llama Stack</u></a> available to the community. More details on the latest release, including information on the <a href="https://euneedsai.com/" target="_blank" data-lnfb-mode="ie"><u>multimodal availability</u></a> in Europe, can be found in <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md" target="_blank" data-lnfb-mode="ie"><u>our acceptable use policy</u></a>.</p></div><p>Meet Llama 3.2</p></div><div><div><p>The two largest models of the Llama 3.2 collection, 11B and 90B, support image reasoning use cases, such as document-level understanding including charts and graphs, captioning of images, and visual grounding tasks such as directionally pinpointing objects in images based on natural language descriptions. For example, a person could ask a question about which month in the previous year their small business had the best sales, and Llama 3.2 can then reason based on an available graph and quickly provide the answer. In another example, the model could reason with a map and help answer questions such as when a hike might become steeper or the distance of a particular trail marked on the map. The 11B and 90B models can also bridge the gap between vision and language by extracting details from an image, understanding the scene, and then crafting a sentence or two that could be used as an image caption to help tell the story.</p><p>The lightweight 1B and 3B models are highly capable with multilingual text generation and tool calling abilities. These models empower developers to build personalized, on-device agentic applications with strong privacy where data never leaves the device. For example, such an application could help summarize the last 10 messages received, extract action items, and leverage tool calling to directly send calendar invites for follow-up meetings.</p><p>Running these models locally comes with two major advantages. First, prompts and responses can feel instantaneous, since processing is done locally. Second, running models locally maintains privacy by not sending data such as messages and calendar information to the cloud, making the overall application more private. Since processing is handled locally, the application can clearly control which queries stay on the device and which may need to be processed by a larger model in the cloud.</p></div><p>Model evaluations</p><div><p>Our evaluation suggests that the Llama 3.2 vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks. The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as following instructions, summarization, prompt rewriting, and tool-use, while the 1B is competitive with Gemma.</p><p>We evaluated performance on over 150 benchmark datasets that span a wide range of languages. For the vision LLMs, we evaluated performance on benchmarks for image understanding and visual reasoning.</p><br/></div><p><img src="https://scontent-ams4-1.xx.fbcdn.net/v/t39.2365-6/461288018_1255239495501495_271827633811450582_n.png?_nc_cat=102&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=UI8ViTCexcwQ7kNvgH6f8Ls&amp;_nc_ht=scontent-ams4-1.xx&amp;_nc_gid=AulsFpBJ2f3qGMR_wVx31PR&amp;oh=00_AYCJ6LZXdNgaXgvgPi1fgE9VIYdNGf1kvaZE7JIjWWvR7A&amp;oe=670F54ED" alt="" id="u_0_5_YM"/></p><p><img src="https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/461157789_931406385491961_1692349435372036848_n.png?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=cgqzw-5Xp68Q7kNvgFpgWGw&amp;_nc_ht=scontent-ams2-1.xx&amp;_nc_gid=AulsFpBJ2f3qGMR_wVx31PR&amp;oh=00_AYAVXImYVZSm0zZJvSvm1Y2D5B6x90ghsR7IcV1AGs_MvQ&amp;oe=670F6C58" alt="" id="u_0_6_75"/></p><p>Vision models</p><div><p>As the first Llama models to support vision tasks, the 11B and 90B models required an entirely new model architecture that supports image reasoning.</p><p>To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.</p><p>Our training pipeline consists of multiple stages, starting from pretrained Llama 3.1 text models. First, we add image adapters and encoders, then pretrain on large-scale noisy (image, text) pair data. Next, we train on medium-scale high quality in-domain and knowledge-enhanced (image, text) pair data.</p><p>In post-training, we use a similar recipe as the text models by doing several rounds of alignment on supervised fine-tuning, rejection sampling, and direct preference optimization. We leverage synthetic data generation by using the Llama 3.1 model to filter and augment question and answers on top of in-domain images, and use a reward model to rank all the candidate answers to provide high quality fine-tuning data. We also add safety mitigation data to produce a model with a high level of safety while retaining helpfulness of the mode</p><p>The end result is a set of models that can take in both image and text prompts, and deeply understand and reason on the combination. This is another step toward Llama models having even richer agentic capabilities.</p><br/></div><p>Lightweight models</p><div><p>As we talked about with Llama 3.1, powerful teacher models can be leveraged to create smaller models that have improved performance. We used two methods—pruning and distillation—on the 1B and 3B models, making them the first highly capable lightweight Llama models that can fit on devices efficiently.</p><p>Pruning enabled us to reduce the size of extant models in the Llama herd while recovering as much knowledge and performance as possible. For the 1B and 3B models, we took the approach of using structured pruning in a single shot manner from the Llama 3.1 8B. This involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network.</p><p>Knowledge distillation uses a larger network to impart knowledge on a smaller network, with the idea that a smaller model can achieve better performance using a teacher than it could from scratch. For the 1B and 3B in Llama 3.2, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance.</p><br/></div><p><img src="https://scontent-ams4-1.xx.fbcdn.net/v/t39.2365-6/461209081_511117684875670_45564063096782202_n.png?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ieNr4u425SwQ7kNvgF2okbm&amp;_nc_ht=scontent-ams4-1.xx&amp;_nc_gid=AulsFpBJ2f3qGMR_wVx31PR&amp;oh=00_AYDZ0E5FeaxNKUVu-i4FY2GkcSWpm7_phqVb6XUNPbMZ7A&amp;oe=670F545E" alt="" id="u_0_a_IW"/></p><div><p>In post-training, we use a similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involves supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).</p><p>In post-training, we scale context length support to 128K tokens, while maintaining the same quality as the pre-trained model. We also engage in synthetic data generation that goes through careful data processing and filtering to ensure high quality. We carefully blend the data to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.</p><p>To enable the community to innovate on these models, we worked closely with Qualcomm and Mediatek, the top two mobile system on a chip (SoC) companies in the world, and Arm, who provides the foundational compute platform for <a href="https://www.arm.com/company" target="_blank" data-lnfb-mode="ie"><u>99</u></a><a href="https://www.arm.com/company" target="_blank" data-lnfb-mode="ie">%</a> of mobile devices. The weights being released today are based on BFloat16 numerics. Our teams are actively exploring quantized variants that will run even faster, and we hope to share more on that soon.</p><br/></div><div><p>This demo is based on an unreleased quantized model.</p></div><div><p>This demo is based on an unreleased quantized model.</p></div><div><p>Llama Stack distributions</p><div><p>In July, we released a <a href="https://github.com/meta-llama/llama-stack/issues/6" target="_blank" data-lnfb-mode="ie"><u>request for comment</u></a> on the Llama Stack API, a standardized interface for canonical toolchain components (fine-tuning, synthetic data generation) to customize Llama models and build agentic applications. The engagement has been great.</p><p>Since then, we have been working hard to make the API real. We built a reference implementation of the APIs for inference, tool use, and RAG. In addition, we have been working with partners to adapt them to become providers for the APIs. Finally, we have introduced Llama Stack Distribution as a way to package multiple API Providers that work well together to provide a single endpoint for developers. We are now sharing with the community a simplified and consistent experience that will enable them to work with Llama models in multiple environments, including on-prem, cloud, single-node, and on-device.</p><br/></div><p><img src="https://scontent-ams4-1.xx.fbcdn.net/v/t39.2365-6/460924239_3402957093334534_4357083070437107157_n.png?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=zKTWaIWC3AIQ7kNvgG30nYZ&amp;_nc_ht=scontent-ams4-1.xx&amp;_nc_gid=AulsFpBJ2f3qGMR_wVx31PR&amp;oh=00_AYArq-kAhDT2GC9c9xlw5HnqMah0aLOcrZw3HxRTjZWfbQ&amp;oe=670F6EE0" alt="" id="u_0_k_/V"/></p><div><div><p>The full set of releases includes:</p><ol><li>Llama CLI (command line interface) to build, configure, and run Llama Stack distributions</li><li>Client code in multiple languages, including python, node, kotlin, and swift</li><li>Docker containers for Llama Stack Distribution Server and Agents API Provider</li><li>Multiple distributions<ol><li>Single-node Llama Stack Distribution via Meta internal implementation and Ollama</li><li>Cloud Llama Stack distributions via AWS, Databricks, Fireworks, and Together</li><li>On-device Llama Stack Distribution on iOS implemented via PyTorch ExecuTorch</li><li>On-prem Llama Stack Distribution supported by Dell</li></ol></li></ol><p>We look forward to working with developers and partners to simplify all aspects of building with Llama models and welcome feedback.</p><br/></div><p><img src="https://scontent-ams2-1.xx.fbcdn.net/v/t39.2365-6/460942153_931942502081982_4461283719059292584_n.png?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=yPZiTRUpQyIQ7kNvgF3aR3D&amp;_nc_ht=scontent-ams2-1.xx&amp;_nc_gid=AulsFpBJ2f3qGMR_wVx31PR&amp;oh=00_AYA3w1xK-UiJG1nS9ZDChXblCfPLsB_95ec_UhhB3MT3ig&amp;oe=670F44D5" alt="" id="u_0_l_U7"/></p><div><p>System level safety</p><div><p>Taking an open approach has many benefits. It helps ensure that more people around the world can access the opportunities that AI provides, guards against concentrating power in the hands of a small few, and deploys technology more equitably and safely across society. As we continue to innovate, we also want to make sure we’re empowering developers to build safe and responsible systems.</p><p>Building on our previous release and continuous effort to support responsible innovation, today we’re adding new updates to our family of safeguards:</p><ul><li>First, we’re releasing Llama Guard 3 11B Vision, which is designed to support Llama 3.2’s new image understanding capability and filter text+image input prompts or text output responses to these prompts.</li><li>Second, as we released 1B and 3B Llama models to be used in more constrained environments like on-device, we also optimized Llama Guard to drastically reduce its deployment cost. Llama Guard 3 1B is based on the Llama 3.2 1B model and has been pruned and quantized bringing its size from 2,858 MB down to 438 MB, making it more efficient than ever to deploy.</li></ul><p>These new solutions are integrated into our reference implementations, demos, and applications and are ready for the open source community to use on day one.</p><br/></div><div><p>Try Llama 3.2 today</p><div><p>Llama 3.2 is poised to reach more people than ever before and enable exciting new use cases. We believe sharing these models with the open source community isn’t enough. We want to make sure developers also have the tools they need to build with Llama responsibly. As part of our continued responsible release efforts, we’re offering developers new <a href="https://ai.meta.com/blog/responsible-ai-connect-2024/" target="_blank" data-lnfb-mode="ie"><u>tools and resources</u></a>, and as always, we’ll update best practices in our <a href="https://ai.meta.com/static-resource/responsible-use-guide/" target="_blank" data-lnfb-mode="ie"><u>Responsible Use Guide</u></a>.</p><p>We continue to share the latest advancements in the Llama ecosystem because we believe openness drives innovation and is good for developers, Meta, and the world. We’re excited to continue the conversations we’re having with our partners and the open source community, and as always, we can’t wait to see what the community builds using Llama 3.2 and Llama Stack.</p><p><i>This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, AMD, Arm, AWS, Cloudflare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM watsonx, Infosys, Intel, Kaggle, Lenovo, LMSYS, MediaTek, Microsoft Azure, NVIDIA, OctoAI, Ollama, Oracle Cloud, PwC, Qualcomm, Sarvam AI, Scale AI, Snowflake, Together AI, and UC Berkeley - vLLM Project.</i></p><br/></div><a href="https://www.llama.com/" data-ms-clickable="true" data-ms="{&#34;creative&#34;:&#34;click_external&#34;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_p_kf"><div><p>Learn more on the Llama website</p><svg viewBox="0 0 36 36" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.746 10H26V21.254L24.0301 21.2735L24.0297 13.377L11.4067 26L10 24.5933L22.6039 11.9894L14.746 11.9894V10Z" fill="CurrentColor"></path></svg></div></a></div></div></div></div></div></div>
  </body>
</html>
