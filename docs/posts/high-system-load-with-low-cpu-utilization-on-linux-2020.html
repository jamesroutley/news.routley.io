<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tanelpoder.com/posts/high-system-load-low-cpu-utilization-on-linux/">Original</a>
    <h1>High System Load with Low CPU Utilization on Linux? (2020)</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>In this post I will show you how to break down Linux <strong>system load</strong> by the load contributor or reason. You can drill down into the <em>“linux system load in thousands”</em> and <em>“high system load, but low CPU utilization”</em> problem patterns too.</p>
<ol>
<li><a href="#introduction---terminology">Introduction - terminology</a></li>
<li><a href="#troubleshooting-high-system-load-on-linux">Troubleshooting high system load on Linux</a></li>
<li><a href="#drilling-down-deeper---wchan">Drilling down deeper - WCHAN</a></li>
<li><a href="#drilling-down-deeper---kernel-stack">Drilling down deeper - kernel stack</a></li>
<li><a href="#how-to-troubleshoot-past-problems">How to troubleshoot past problems</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#further-reading">Further reading</a></li>
</ol>
<h3 id="introduction---terminology">Introduction - Terminology</h3>
<ul>
<li>The <em>system load</em> metric aims to represent the system “resource demand” as just a single number. On classic Unixes, it only counts the demand for CPU (threads in Runnable state)</li>
<li>The <em>unit</em> of system load metric is “number of processes/threads” (or <em>tasks</em> as the scheduling unit is called on Linux). The load <em>average</em> is an average number of threads over a time period (last 1,5,15 mins) that <em>“compete for CPU”</em> on classic unixes or <em>“either compete for CPU or wait in an uninterruptible sleep state”</em> on Linux</li>
<li><strong>Runnable</strong> state means “not blocked by anything”, ready to run on CPU. The thread is either currently running on CPU or waiting in the CPU runqueue for the OS scheduler to put it onto CPU</li>
<li>On Linux, the system load includes threads <em>both</em> in Runnable (R) <em>and</em> in <strong>Uninterruptible</strong> sleep (D) states (typically disk I/O, but not always)</li>
</ul>
<p>So, on Linux, an absurdly high load figure can be caused by having lots of threads in Uninterruptible sleep (D) state, in addition to CPU demand.</p>
<h3 id="troubleshooting-high-system-load-on-linux">Troubleshooting high system load on Linux</h3>
<p>Here’s one example from a Linux database server with 32 CPUs:</p>
<p><img src="https://tanelpoder.com/files/images/linux-load.png" alt="High system load on Linux"/>
<em>The system load, incorrectly labeled as “runnable processes” by the above monitoring tool, jumped to over 3000!</em></p>
<p>Let’s confirm this with standard OS level commands, to avoid getting misled by potential GUI magic by the monitoring tool:</p>
<pre>[tanel@linux01 ~]$ <strong>w</strong>
 11:49:29 up 13 days, 13:55,  2 users,  load average: <mark>3446.04</mark>, 1242.09, 450.47
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
tanel    pts/0    192.168.0.159    Thu14   21:09m  0.05s  0.05s -bash
tanel    pts/1    192.168.0.159    11:46    1.00s  0.36s  0.23s w
</pre>
<p>Does this mean that we have a huge demand for CPU time? Must have <em>lots</em> of threads in the CPU runqueues, right?</p>
<pre>[tanel@linux01 ~]$ <strong>sar -u 5</strong>
11:58:51 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
11:58:56 AM     all     <mark>36.64      0.00      4.42</mark>     17.17      0.00     41.77
11:59:01 AM     all     41.04      0.00      3.72     26.67      0.00     28.57
11:59:06 AM     all     35.38      0.00      2.95     28.39      0.00     33.27
</pre>
<p>But the CPUs are well over 50% idle! CPU utilization is around 40-45% when adding <code>%user</code>, <code>%nice</code> and <code>%system</code> together. <code>%iowait</code> means that the CPU is idle, it just happens to have a synchronous I/O submitted by a thread on it before becoming idle.</p>
<p>So, we don’t seem to have a CPU oversubscription scenario in this case. Is there a way to systematically drill down deeper by measuring what (and who) is contributing to this load then?</p>
<p>Yes, and it’s super simple. Remember, the <em>current</em> system load is just the number of threads (called tasks) on Linux that are either in <code>R</code> or <code>D</code> state. We can just run <code>ps</code> to list the current number of threads in these states:</p>
<pre>[tanel@linux01 ~]$ <strong>ps -eo s,user | grep ^[RD] | sort | uniq -c | sort -nbr | head -20</strong>
   <mark>3045 D root</mark>
     20 R oracle
      3 R root
      1 R tanel
</pre>
<p>In the above command, <code>ps -eo s,user</code> will list the <em>thread state</em> field first and any other fields of interest (like username), later. The <code>grep ^[RD]</code> filters out any threads in various “idle” and “sleeping” states that don’t contribute to Linux load (S,T,t,Z,I etc).</p>
<p>Indeed, in addition to total 24 threads in Runnable state (R), it looks like there’s over 3000 threads in Uninterruptible Sleep (D) state that typically (but not always) indicates sleeping due to synchronous disk IO. They are all owned by <code>root</code>. Is there some daemon that has gone crazy and has all these active processes/threads trying to do IO?</p>
<p>Let’s add one more column to <code>ps</code> to list the command line/program name too:</p>
<pre>[tanel@linux01 ~]$ <strong>ps -eo s,user,cmd | grep ^[RD] | sort | uniq -c | sort -nbr | head -20</strong>
     15 R oracle   oracleLIN19C (LOCAL=NO)
      3 D oracle   oracleLIN19C (LOCAL=NO)
      1 R tanel    ps -eo s,user,cmd
      1 R root     xcapture -o /backup/tanel/0xtools/xcaplog -c exe,cmdline,kstack
      <mark>1</mark> D root     <mark>[kworker/6:99]</mark>
      1 D root     [kworker/6:98]
      1 D root     [kworker/6:97]
      1 D root     [kworker/6:96]
      1 D root     [kworker/6:95]
      1 D root     [kworker/6:94]
      1 D root     [kworker/6:93]
      1 D root     [kworker/6:92]
      1 D root     [kworker/6:91]
      1 D root     [kworker/6:90]
      1 D root     [kworker/6:9]
      1 D root     [kworker/6:89]
      1 D root     [kworker/6:88]
      1 D root     [kworker/6:87]
      1 D root     [kworker/6:86]
      1 D root     [kworker/6:85]
</pre>
<p>But now “root” seems to be gone from the top and we see only some Oracle processes near the top, with relatively little R/D activity. My command has a <code>head -20</code> filter in the end to avoid printing out thousands of lines of output when most of the <code>ps</code> output lines are unique, that is the case here with all the individual <code>kworker</code> threads with unique names. There are thousands of them, each contributing just “1 thread with this name” to the load summary.</p>
<p>If you don’t want to start mucking around with further awk/sed commands to group the ps output better, you can use my <a href="https://tanelpoder.com/psnapper/">pSnapper from 0x.tools</a> that does the work for you. Also, it samples thread states multiple times and prints a breakdown of activity averages (to avoid getting misled by a single “unlucky” sample):</p>
<pre>[tanel@linux01 ~]$ <strong>psn</strong>

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/stat for 5 seconds... finished.


=== Active Threads ================================================

 samples | avg_threads | comm             | state                  
-------------------------------------------------------------------
   10628 |     <mark>3542.67</mark> | (kworker/*:*)    | Disk (Uninterruptible) 
      37 |       12.33 | (oracle_*_l)     | Running (ON CPU)       
      17 |        5.67 | (oracle_*_l)     | Disk (Uninterruptible) 
       2 |        0.67 | (xcapture)       | Running (ON CPU)       
       1 |        0.33 | (ora_lg*_xe)     | Disk (Uninterruptible) 
       1 |        0.33 | (ora_lgwr_lin*)  | Disk (Uninterruptible) 
       1 |        0.33 | (ora_lgwr_lin*c) | Disk (Uninterruptible) 


samples: 3 (expected: 100)
total processes: 10470, threads: 11530
runtime: 6.13, measure time: 6.03
</pre>
<p>By default, pSnapper replaces any digits in the task’s <code>comm</code> field before aggregating (the <code>comm2</code> field would leave them intact). Now it’s easy to see that our extreme system load spike was caused by a large number of <code>kworker</code> <em>kernel threads</em> (with “root” as process owner). So this is not about some userland daemon running under root, but a kernel problem.</p>
<h3 id="drilling-down-deeper---wchan">Drilling down deeper - WCHAN</h3>
<p>I’ll drill down deeper into this with another instance of the same problem (on the same machine). System load is in hundreds this time:</p>
<pre>[tanel@linux01 ~]$ w
 13:47:03 up 7 days, 15:53,  3 users,  load average: <mark>496.62, 698.40, 440.26</mark>
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
tanel    pts/0    192.168.0.159    13:36    7:03   0.06s  0.06s -bash
tanel    pts/1    192.168.0.159    13:41    7.00s  0.32s  0.23s w
tanel    pts/2    192.168.0.159    13:42    3:03   0.23s  0.02s sshd: tanel [priv]  
</pre>
<p>Let’s break the demand down by <code>comm</code> and <code>state</code> fields again, but I’ll also add the current system call and kernel wait location (<code>wchan</code>) to the breakdown. With these extra fields, I should run pSnapper with <code>sudo</code> as modern Linux kernel versions tend to block access to (or hide values in) some fields, when running as non-root:</p>
<pre>[tanel@linux01 ~]$ sudo psn <strong>-G syscall,wchan</strong>

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stat, wchan for 5 seconds... finished.


=== Active Threads ==========================================================================================

 samples | avg_threads | comm             | state                  | syscall         | wchan                 
-------------------------------------------------------------------------------------------------------------
     511 |      255.50 | <mark>(kworker/*:*)</mark>    | Disk (Uninterruptible) | <mark>[kernel_thread]</mark> | <mark>blkdev_issue_flush</mark> 
     506 |      253.00 | <mark>(oracle_*_l)</mark>     | Disk (Uninterruptible) | <mark>pread64</mark>         | <mark>do_blockdev_direct_IO</mark> 
      28 |       14.00 | (oracle_*_l)     | Running (ON CPU)       | [running]       | 0                     
       1 |        0.50 | (collectl)       | Running (ON CPU)       | [running]       | 0                     
       1 |        0.50 | (mysqld)         | Running (ON CPU)       | [running]       | 0                     
       1 |        0.50 | (ora_lgwr_lin*c) | Disk (Uninterruptible) | io_submit       | inode_dio_wait        
       1 |        0.50 | (oracle_*_l)     | Disk (Uninterruptible) | pread64         | 0                     
       1 |        0.50 | (oracle_*_l)     | Running (ON CPU)       | [running]       | SYSC_semtimedop       
       1 |        0.50 | (oracle_*_l)     | Running (ON CPU)       | [running]       | read_events           
       1 |        0.50 | (oracle_*_l)     | Running (ON CPU)       | read            | 0                     
       1 |        0.50 | (oracle_*_l)     | Running (ON CPU)       | semtimedop      | SYSC_semtimedop       
</pre>
<p><em>You may need to scroll right to see the full output.</em></p>
<p>In the above breakdown of current system load, close to half of activity was caused by kernel <code>kworker</code> threads that were currently sleeping in <code>blkdev_issue_flush</code> kernel function responsible for an internal <code>fsync</code> to ensure that the writes get persisted to storage. The remaining “close to half” active threads were by oracle processes, waiting in a synchronous <code>pread64</code> system call, in <code>do_blockdev_direct_IO</code> kernel function.</p>
<p>From the “Running (ON CPU)” lines you see that there was some CPU usage too, but doesn’t seem to be anywhere near to the hundreds of threads in I/O sleeps.</p>
<p>While doing these tests, I ran an Oracle benchmark with 1000 concurrent connections (that were sometimes idle), so the 253 sessions waiting in the synchronous <code>pread64</code> system calls can be easily explained. Synchronous single block reads are done for index tree walking &amp; index-based table block access, for example. But why do we see so many <code>kworker</code> kernel threads waiting for I/O too?</p>
<p>The answer is asynchronous I/O and I/Os done against higher level block devices (like the device-mapper <code>dm</code> devices for LVM and <code>md</code> devices for software RAID). With asynchronous I/O, the thread <em>completing</em> an I/O request in kernel memory structures is different from the (application) thread submitting the I/O. That’s where the kernel <code>kworker</code> threads come in and the story gets more complex with LVMs/dm/md devices (as there are multiple layers of I/O queues on the request path).</p>
<p>So you could say that the 253 thread where the oracle processes were sleeping within the <code>pread64</code> syscall were the synchronous reads and the 255.5 kernel threads (without a system call as kernel code doesn’t need system calls to enter kernel mode) are due to asynchronous I/O.</p>
<blockquote>
<p>Note that while synchronous I/O waits like <code>pread64</code> will contribute to Linux system load because they end up in D state, the asynchronous I/O completion check (and IO reaping) system call <code>io_getevents</code> ends up in S state (Sleeping), if it’s instructed to wait by the application. So, only synchronous I/O operations (by the application or kernel threads) contribute to Linux system load!</p>
<p>Additionally, the <code>io_submit</code> system call for asynchronous submission of I/O requests may itself get blocked before a successful IO submit. This can happen if the I/O queue is already full of non-complete, non-reaped I/O requests or there’s a “roadblock” somewhere earlier on the path to the block device (like a filesystem layer or LVM issue). In such case an <code>io_submit</code> call itself would get stuck (despite the supposed asynchronous nature of I/O) and the thread issuing the I/O ends up waiting in D state, despite the I/O not having even been sent out to the hardware device yet.</p>
</blockquote>
<p>There’s at least one Linux kernel bug causing trouble in the touchpoint of kworkers and dm/md devices in high-throughput systems, but I’ll leave it to a next post.</p>
<p>You don’t have to guess where the bottleneck resides, just dig deeper using what pSnapper offers. One typical question is that which file(s) or devices are we waiting for the most? Let’s add <code>filename</code> (or <code>filenamesum</code> that consolidates filenames with digits in them into one) into the breakdown:</p>
<pre>[tanel@linux01 ~]$ sudo psn -G syscall,filenamesum

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stat for 5 seconds... finished.


=== Active Threads =======================================================================================================

 samples | avg_threads | comm            | state                  | syscall         | filenamesum                         
--------------------------------------------------------------------------------------------------------------------------
    2027 |      506.75 | (kworker/*:*)   | Disk (Uninterruptible) | [kernel_thread] |                                     
    1963 |      490.75 | (oracle_*_l)    | Disk (Uninterruptible) | pread64         | <mark>/data/oracle/LIN*C/soe_bigfile.dbf</mark>
      87 |       21.75 | (oracle_*_l)    | Running (ON CPU)       | [running]       |                                     
      13 |        3.25 | (kworker/*:*)   | Running (ON CPU)       | [running]       |                                     
       4 |        1.00 | (oracle_*_l)    | Running (ON CPU)       | read            | socket:[*]                          
       2 |        0.50 | (collectl)      | Running (ON CPU)       | [running]       |                                     
       1 |        0.25 | (java)          | Running (ON CPU)       | futex           |                                     
       1 |        0.25 | (ora_ckpt_xe)   | Disk (Uninterruptible) | pread64         | /data/oracle/XE/control*.ctl        
       1 |        0.25 | (ora_m*_linprd) | Running (ON CPU)       | [running]       |                                     
       1 |        0.25 | (ora_m*_lintes) | Running (ON CPU)       | [running]       |                                     

</pre>
<p>Apparently the system load has increased by now (we have over 1000 active threads in R/D state). Most of the synchronous read waits witnessed are against <em>/data/oracle/LIN*C/soe_bigfile.dbf</em> file (by oracle user). The kworker threads don’t show any filenames for their I/Os as pSnapper gets the filename from the system call arguments (and resolves the file descriptor to a filename, where possible) — but kernel threads don’t need system calls as they are already operating deep in the kernel, always in kernel mode. Nevertheless, this field is useful in many application troubleshooting scenarios.</p>
<h3 id="drilling-down-deeper---kernel-stack">Drilling down deeper - kernel stack</h3>
<p>Let’s dig even deeper. You’ll need to scroll right to see the full picture, I’ve highlighted some things in all the way to the right. We can sample the kernel stack of a thread too (kernel threads and also userspace application threads, when they happen to be executing kernel code):</p>
<pre>[tanel@linux01 ~]$ sudo psn -p -G syscall,wchan,<strong>kstack</strong>

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/wchan, stack, syscall, stat for 5 seconds... finished.


=== Active Threads =======================================================================================================================================================================================================================================================================================================================================================================================

 samples | avg_threads | comm          | state                  | syscall         | wchan                        | kstack                                                                                                                                                                                                                                                                                 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
     281 |      140.50 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | blkdev_issue_flush           | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;generic_write_sync()-&gt;xfs_file_fsync()-&gt;xfs_blkdev_issue_flush()-&gt;blkdev_issue_flush()                                                                          
     211 |      105.50 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | <mark>call_rwsem_down_read_failed</mark>  | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;generic_write_sync()-&gt;xfs_file_fsync()-&gt;<mark>xfs_ilock()-&gt;call_rwsem_down_read_failed()</mark>                                                                            
     169 |       84.50 | (oracle_*_li) | Disk (Uninterruptible) | pread64         | <mark>call_rwsem_down_write_failed</mark> | system_call_fastpath()-&gt;SyS_pread64()-&gt;vfs_read()-&gt;do_sync_read()-&gt;xfs_file_aio_read()-&gt;xfs_file_dio_aio_read()-&gt;<mark>touch_atime()-&gt;update_time()-&gt;xfs_vn_update_time()-&gt;xfs_ilock()-&gt;call_rwsem_down_write_failed()</mark>                                                                       
      64 |       32.00 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | <mark>xfs_log_force_lsn</mark>            | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;generic_write_sync()-&gt;<mark>xfs_file_fsync()-&gt;xfs_log_force_lsn()</mark>                                                                                                     
      24 |       12.00 | (oracle_*_li) | Disk (Uninterruptible) | pread64         | call_rwsem_down_read_failed  | system_call_fastpath()-&gt;SyS_pread64()-&gt;vfs_read()-&gt;do_sync_read()-&gt;xfs_file_aio_read()-&gt;xfs_file_dio_aio_read()-&gt;__blockdev_direct_IO()-&gt;do_blockdev_direct_IO()-&gt;xfs_get_blocks_direct()-&gt;__xfs_get_blocks()-&gt;xfs_ilock_data_map_shared()-&gt;xfs_ilock()-&gt;call_rwsem_down_read_failed() 
       5 |        2.50 | (oracle_*_li) | Disk (Uninterruptible) | pread64         | do_blockdev_direct_IO        | system_call_fastpath()-&gt;SyS_pread64()-&gt;vfs_read()-&gt;do_sync_read()-&gt;xfs_file_aio_read()-&gt;xfs_file_dio_aio_read()-&gt;__blockdev_direct_IO()-&gt;do_blockdev_direct_IO()                                                                                                                       
       3 |        1.50 | (oracle_*_li) | Running (ON CPU)       | [running]       | 0                            | system_call_fastpath()-&gt;SyS_pread64()-&gt;vfs_read()-&gt;do_sync_read()-&gt;xfs_file_aio_read()-&gt;xfs_file_dio_aio_read()-&gt;__blockdev_direct_IO()-&gt;do_blockdev_direct_IO()                                                                                                                       
       2 |        1.00 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | call_rwsem_down_write_failed | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;xfs_end_io_direct_write()-&gt;xfs_iomap_write_unwritten()-&gt;xfs_ilock()-&gt;call_rwsem_down_write_failed()                                                             
       2 |        1.00 | (kworker/*:*) | Running (ON CPU)       | [running]       | 0                            | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;generic_write_sync()-&gt;xfs_file_fsync()-&gt;xfs_blkdev_issue_flush()-&gt;blkdev_issue_flush()                                                                          
       2 |        1.00 | (oracle_*_li) | Disk (Uninterruptible) | io_submit       | call_rwsem_down_write_failed | system_call_fastpath()-&gt;SyS_io_submit()-&gt;do_io_submit()-&gt;xfs_file_aio_read()-&gt;xfs_file_dio_aio_read()-&gt;touch_atime()-&gt;update_time()-&gt;xfs_vn_update_time()-&gt;xfs_ilock()-&gt;call_rwsem_down_write_failed()                                                                                 
       1 |        0.50 | (java)        | Running (ON CPU)       | futex           | futex_wait_queue_me          | system_call_fastpath()-&gt;SyS_futex()-&gt;do_futex()-&gt;futex_wait()-&gt;futex_wait_queue_me()                                                                                                                                                                                                   
       1 |        0.50 | (ksoftirqd/*) | Running (ON CPU)       | [running]       | 0                            | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;smpboot_thread_fn()                                                                                                                                                                                                                           
       1 |        0.50 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | worker_thread                | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()                                                                                                                                                                                                                               
       1 |        0.50 | (kworker/*:*) | Disk (Uninterruptible) | [kernel_thread] | worker_thread                | ret_from_fork_nospec_begin()-&gt;kthread()-&gt;worker_thread()-&gt;process_one_work()-&gt;dio_aio_complete_work()-&gt;dio_complete()-&gt;generic_write_sync()-&gt;xfs_file_fsync()-&gt;xfs_blkdev_issue_flush()-&gt;blkdev_issue_flush()                                                                          
       1 |        0.50 | (ora_lg*_xe)  | Disk (Uninterruptible) | io_submit       | inode_dio_wait               | system_call_fastpath()-&gt;SyS_io_submit()-&gt;do_io_submit()-&gt;xfs_file_aio_write()-&gt;xfs_file_dio_aio_write()-&gt;inode_dio_wait()                                                                                                                                                              
       1 |        0.50 | (oracle_*_li) | Disk (Uninterruptible) | [running]       | 0                            | -                                                                                                                                                                                                                                                                                      
</pre>
<p>Looks like a different hiccup has happened in my benchmark system now, additional WCHAN (kernel sleep location) values have popped up in the report: <code>call_rwsem_down_*_failed</code> by both Oracle and kworker threads and <code>xfs_log_force_lsn</code> waits by 32 kworker threads. <code>rwsem</code> stands for “reader-writer semaphore” that is essentially a low level lock. So, a large part of our system load (D state waits) are caused by some sort of <em>locking</em> in the kernel now and not by waiting for hardware I/O completion.</p>
<p>If you scroll all the way right and follow the kernel function call chain, it becomes (somewhat) evident that we are waiting for XFS inode locks when accessing (both reading and writing) files. Additionally, when searching what the <code>xfs_log_force_lsn</code> function does, you’d see that this is an XFS journal write that persists XFS metadata updates to disk so that you wouldn’t end up with a broken filesystem in case of a crash. <a href="https://www.kernel.org/doc/Documentation/filesystems/xfs-delayed-logging-design.txt">XFS delayed logging</a> must be ordered and checkpoints atomic, so there may be cases where one XFS-related kworker on one CPU will block other kworkers (that have assumed the same role) on remaining CPUs. For example, if the XFS log/checkpoint write is too slow for some reason. It’s probably not a coincidence that pSnapper shows exactly 32 threads waiting in <code>xfs_log_force_lsn</code> function on my 32 CPU system.</p>
<p>Why do we even have noticeable XFS metadata generation? Unlike ZFS, XFS does not log actual data in the journal, just the changed file metadata. Every time you write to a file, some metadata must be logged (last file modification timestamp) and even reads can cause metadata to be generated (filesystem mount options <code>noatime</code> and to lesser extent <code>relatime</code> avoid metadata generation on reads).</p>
<p>So in addition to the 32 kernel threads waiting for XFS log sync completion, we have hundreds of concurrent application processes (Oracle) and different kernel kworker threads that apparently contend for XFS inode lock (where the <code>xfs_ilock</code> kernel function seen in stack). All this lock contention and sleeping will contribute to system load as the threads will be in R or D state.</p>
<p>With any lock contention, one reasonable question is <em>“why hasn’t the lock holder released it yet?”</em>, in other words, what is the lock holder itself doing for so long? This could be explained by slow I/O into the XFS filesystem journal, where the slow XFS log sync prevents everyone else from generating more XFS metadata (buffers are full), <em>including the thread that may already hold an inode lock of the “hot” file</em> because it wants to change its last change timestamp. And everyone else will wait!</p>
<p>So, the top symptom will point towards an inode lock contention/semaphore problem, while a deeper analysis will show that slow XFS journal writes, possibly experienced just by one thread, are the root cause. There are good performance reasons to put the filesystem journal to a separate block device even in the days of fast SSDs. I have just connected a few dots here thanks to previously troubleshooting such problems, but in order to be completely sytematic, kernel tracing or <code>xfs_stats</code> sampling would be needed for showing the relationship with XFS log sync waits and all the inode semaphore waits. I’ll leave this to a future blog entry.</p>
<h3 id="how-to-troubleshoot-past-problems">How to troubleshoot past problems?</h3>
<p>Running <code>ps</code> or <code>psn</code> will only help you troubleshoot currently ongoing problems. If you’ve been paying attention to my <a href="https://0x.tools">Always-on Profiling of Production Systems</a> posts, you know that I’ve published an open source super-efficient /proc sampler tool <code>xcapture</code> that can save a log of sampled “active thread states” into CSV files. I haven’t written fancier tools for analyzing the output yet, but on command line you can just run “SQL” against the CSV using standard Linux tools:</p>
<pre>$ <strong>grep &#34;^2020-11-20 11:47&#34; 2020-11-20.11.csv | \
            awk -F, &#39;{ printf(&#34;%-2s %-10s %-15s %s\n&#34;, $5, $4, $7, $8) }&#39; | \
            sort | uniq -c | sort -nbr</strong>

  32165 D  root       [no_syscall]    blkdev_issue_flush
   5933 D  oracle     pread64         do_blockdev_direct_IO
   1001 R  oracle     [running]       0
     20 R  root       [running]       0
     16 R  oracle     futex           futex_wait_queue_me
     12 D  oracle     [running]       0
      8 R  oracle     read            sk_wait_data
      5 D  oracle     io_submit       inode_dio_wait
      4 R  oracle     pread64         do_blockdev_direct_IO
      2 R  tanel      [running]       0
      2 R  oracle     semtimedop      SYSC_semtimedop
      2 D  root       [running]       0
      2 D  root       [no_syscall]    msleep
      1 R  root       [running]       worker_thread
      1 R  oracle     [running]       do_blockdev_direct_IO
      1 R  oracle     nanosleep       hrtimer_nanosleep
      1 R  oracle     io_getevents    read_events
      1 D  root       [no_syscall]    xfs_log_force
      1 D  root       [no_syscall]    worker_thread
      1 D  root       [no_syscall]    hub_event
      1 D  oracle     [running]       read_events
      1 D  oracle     pwrite64        wait_on_page_bit
      1 D  oracle     pwrite64        blkdev_issue_flush
      1 D  oracle     io_getevents    read_events
</pre>
<p>In the above example, I just zoomed in to one minute of time of interest (11:47) with <code>grep</code> and then just used <code>awk</code>, <code>sort</code> and <code>uniq</code> for projecting fields of interest and a group by + order by top activity. Note that I didn’t have 32165 threads active here, I’d need to divide this figure with 60 as I’m zooming in to a whole minute (sampling happens once per second) to get the <em>average active threads</em> in R/D states.</p>
<h3 id="summary">Summary</h3>
<p>The main point of this article was to demonstrate that high system load on Linux doesn’t come only from CPU demand, but also from disk I/O demand — and more specifically number of threads that end up in the <em>uninterruptible sleep</em> state <strong>D</strong> for whatever reason. Sometimes this reason is synchronous disk I/O, but sometimes the threads don’t even get that far when they hit some kernel-level bottleneck even before getting to submit the block I/O to the hardware device. Luckily with tools like <a href="https://tanelpoder.com/psnapper">pSnapper</a> (or just <code>ps</code> with the right arguments), it is possible to drill down pretty deep from userspace, without having to resort to kernel tracing.</p>
<h3 id="further-reading">Further Reading</h3>
<p>Brendan Gregg did some <a href="http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html">archaeological investigation</a> into the origins of Linux load accounting differences. In short, the developer felt that it wouldn’t be right for the system load to drop, in case there’s an I/O bottleneck due to things like swapping. When you hit a sudden I/O bottleneck, then CPU utilization typically drops as your code waits more and it felt unreasonable to the developer that the “system load” drops lower while the system gets less work done. In other words, on Linux the load metric tries to account <em>system load</em>, not just CPU load only. However, as I’ve shown above, there are quite a few reasons (like asynch I/O activity not contributing to load) why the single-number <em>system load metric</em> alone won’t tell you the complete picture of your actual system load.</p>
<h3 id="next-articles">Next articles</h3>
<p>Here’s a list of blog entries that I think of writing next (I’ll update these with links once done):</p>
<ul>
<li>Application I/O waits much longer than block I/O latency in iostat?</li>
<li>Troubleshooting too many (thousands of) kworker threads</li>
<li>Pressure Stall Information (PSI) as the new Linux load metric, how useful is it?</li>
</ul>

</div><p><span>
    <a href="https://tanelpoder.com/about/"><img src="https://tanelpoder.com/files/images/tanelpoder_small.png"/></a>
  </span>
  <span>
    <div>
    <ol>
    	<li><em>Check out my 2022 online training classes in a <strong>new format</strong>!</em></li><em>
      <li>
    	  <em><a href="https://tanelpoder.com/contact/">Get weekly updates</a> by email or follow Social/RSS</em>
    	</li>
    </em></ol></div><em>
  </em></span><em>
</em></p></div>
  </body>
</html>
