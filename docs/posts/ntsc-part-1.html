<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://marginalfutility.net/2024/07/14/ntsc-pt-1/">Original</a>
    <h1>NTSC Part 1</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
  
  <p><span>14 Jul 2024</span></p><p>When I came to the Recurse Center this year, I expected to work on something completely unrelated to video. I’d wanted to branch out into front- and back-end web development, learn some dev ops, and get into topics like distributed systems and CRDTs. But on the first day, I met the amazing Valadaptive, whose project <a href="https://github.com/valadaptive">NTSC-RS</a> is a toolkit for building vintage image filters. It must have been destiny.</p>

<p>For context, my work at <a href="https://1se.co/">1 Second Everyday</a> revolves entirely around video. It’s something that I’ve taken a deep interest in, to the point of trying to build a naive camcorder filter multiple times in the past, even reaching out to Apple engineers for help thinking about the problem. I’ve scoured the web and <a href="https://www.shadertoy.com/">Shadertoy</a> but I’ve never been happy with the results I’ve been able to produce. Little did I know, I just needed a little NTSC.</p>

<h2 id="what-is-ntsc-video">What is NTSC Video?</h2>

<p><img src="http://marginalfutility.net/2024/07/14/assets/smpte-colors.png" alt="SMPTE color bar test pattern"/></p>

<p>Short for <a href="https://en.wikipedia.org/wiki/NTSC">National Television Standards Committee</a>, NTSC was the original standard for analog TV in the US, and evolved in 1953 to support color TV. TVs and VHS players and cameras in the Americas and elsewhere used the <a href="https://en.wikipedia.org/wiki/NTSC#SMPTE_C">NTSC color system</a>, encoded to YIQ (Luminance, In-Phase, Quadrature,) until the rise of digital technologies in the 1990s. Backward compatibility with black-and-white sets was maintained by transmitting the black-and-white luminance data on a separate subcarrier from the chroma (color) channels, much in the same way stereo FM radio works.</p>

<p>In the end, this YIQ model would be the key to the whole project. The basic architecture of the project is to:</p>

<ol>
  <li>Decode an input frame</li>
  <li>Convert the RGB data to YIQ</li>
  <li>Perform some operations on the YIQ data</li>
  <li>Convert back to RGB</li>
  <li>Render</li>
</ol>

<p>Step 3 is where the real work of the filter would happen, by applying blurs, highpass and lowpass filters, noise, etc. to one of the three YIQ channels. This would be how to achieve effects like color bleed, luminance noise, or bleeding from luma into chroma (and vice versa.)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h2 id="porting-to-swift-and-core-image">Porting to Swift and Core Image</h2>

<p>The goal with my project was to port the existing Rust code to Swift and Core Image, moving from the CPU to the GPU in order to have something performant enough for live video recording or for filtering live video playback. Along the way I could run the Rust code, writing tests on both platforms and stepping through them in the debugger to make sure I was on the right track.</p>

<p>All of the image processing I’d done up to this point used Apple’s Core Image framework, which provides a ton of builtin filters that can be composed together in a performant way. When you need custom filters you can write a <a href="https://developer.apple.com/documentation/coreimage/cicolorkernel#"><code>CIColorKernel</code></a> in Metal using the Metal Shader Language (a dialect of C++.) I assumed that this is the approach I’d use to write my NTSC code. It was not to be.</p>

<p>The big thing I’d overlooked is that in the RGB color model, channel values are bounded by 0. You can’t have an R, G, or B value that’s less than pure black. But YIQ isn’t bounded in this way, and negative values are commonplace. My plan had been to store YIQ images as regular CIImages, but the zero lower bound made this impossible. Enter Metal.</p>



<p>Metal is Apple’s graphics programming framework, designed as a modern low-level replacement for OpenGL on Apple’s platforms. As I mentioned above, Core Image kernels are written in Metal already. I just needed to make sure that I’d be able to store negative values in my “pixel data,” then I could simply store Y, I, and Q values where R, G, and B ones would ordinarily be.</p>

<p>When writing image processing code in Metal, the two main components are textures and shaders. Textures can be thought of as multidimensional arrays of vectors, and for our purposes are the backing stores for our images. Shaders are programs written to be executed on the GPU, and the ones we care about (fragment shaders) are designed to be run once per pixel. In this way, you could say that CIColorKernel is itself a special kind of fragment shader.</p>

<p>Unlike regular Core Image code, which is essentially functional (input image in, output image out,) Metal shaders will take one or more input textures as arguments and write to an output texture. Here’s some sample code to give you an idea</p>

<div><div><pre><code><span>let</span> <span>encoder</span><span>:</span> <span>MTLComputeCommandEncoder</span> <span>=</span> <span>...</span>
<span>encoder</span><span>.</span><span>setTexture</span><span>(</span><span>input</span><span>,</span> <span>index</span><span>:</span> <span>0</span><span>)</span>
<span>encoder</span><span>.</span><span>setTexture</span><span>(</span><span>output</span><span>,</span> <span>index</span><span>:</span> <span>1</span><span>)</span>
<span>var</span> <span>min</span><span>:</span> <span>Float16</span> <span>=</span> <span>min</span>
<span>encoder</span><span>.</span><span>setBytes</span><span>(</span><span>&amp;</span><span>min</span><span>,</span> <span>length</span><span>:</span> <span>MemoryLayout</span><span>&lt;</span><span>Float16</span><span>&gt;.</span><span>size</span><span>,</span> <span>index</span><span>:</span> <span>0</span><span>)</span>
<span>var</span> <span>max</span><span>:</span> <span>Float16</span> <span>=</span> <span>max</span>
<span>encoder</span><span>.</span><span>setBytes</span><span>(</span><span>&amp;</span><span>max</span><span>,</span> <span>length</span><span>:</span> <span>MemoryLayout</span><span>&lt;</span><span>Float16</span><span>&gt;.</span><span>size</span><span>,</span> <span>index</span><span>:</span> <span>1</span><span>)</span>
</code></pre></div></div>

<p>You can see that we’re setting textures and values on the encoder, assigning indices to each one. This is how we’ll be able to access them on the Metal side.</p>

<pre><code>kernel void mix
(
 texture2d&lt;half, access::read&gt; input [[texture(0)]],
 texture2d&lt;half, access::write&gt; out [[texture(1)]],
 constant half &amp;min [[buffer(0)]],
 constant half &amp;max [[buffer(1)]],
 uint2 gid [[thread_position_in_grid]]
 ) {
    half4 px = input.read(gid);
    half4 mixed = mix(min, max, px);
    out.write(mixed, gid);
}
</code></pre>

<p>The line <code>kernel void mix</code> declares a Metal kernel (shader) whose return type is <code>void</code> and name is <code>mix</code>. We have access to the two textures and two values that we set in the Swift code, taking care to make sure the indices match up (note that 16-bit floating point numbers are called <code>half</code>s in Metal but they’re identical to Swift’s <code>Float16</code> type. Ditto <code>float</code> and <code>Float</code>.) The last three lines read a pixel from the <code>input</code> texture using <code>gid</code> (the current XY coordinate,) call the <code>mix</code> <a href="https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf">function in Metal</a> (different from our kernel with the same name) using the pixel data and our <code>min</code> and <code>max</code> arguments, and write the new pixel back out to <code>out</code>. Finally, we can use an <code>MTKView</code> to get this texture data onscreen.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p>

<h2 id="boilerplate">Boilerplate</h2>

<p>“Sure,” you might ask, “but how do I actually get the GPU to run this code?” Generally, there’s some boilerplate that we need to do every frame:</p>

<ol>
  <li>Get a command buffer</li>
  <li>For each function we want to call, encode it and its textures and data to the buffer</li>
  <li>Commit the buffer (and optionally wait for it to finish executing)</li>
</ol>

<h3 id="1-getting-command-buffers">1. Getting Command Buffers</h3>

<p>You get these from a <code>MTLCommandQueue</code>. You only ever need one queue so you’ll want to create it and hold onto it. You do this using your <code>MTLDevice</code>, which itself is the root-level object for interacting with Metal. Generally, you’ll get access to a device instance by calling <code>MTLCreateSystemDefaultDevice</code>.</p>

<div><div><pre><code><span>class</span> <span>MyClass</span> <span>{</span>
    <span>private</span> <span>let</span> <span>device</span><span>:</span> <span>MTLDevice</span>
    <span>private</span> <span>let</span> <span>commandQueue</span><span>:</span> <span>MTLCommandQueue</span>
    <span>init</span><span>?()</span> <span>{</span>
        <span>guard</span> <span>let</span> <span>device</span> <span>=</span> <span>MTLCreateSystemDefaultDevice</span><span>()</span> <span>else</span> <span>{</span>
            <span>return</span> <span>nil</span>
        <span>}</span>
        <span>self</span><span>.</span><span>device</span> <span>=</span> <span>device</span>
        <span>guard</span> <span>let</span> <span>commandQueue</span> <span>=</span> <span>device</span><span>.</span><span>makeCommandQueue</span><span>()</span> <span>else</span> <span>{</span>
            <span>return</span> <span>nil</span>
        <span>}</span>
        <span>self</span><span>.</span><span>commandQueue</span> <span>=</span> <span>commandQueue</span>
    <span>}</span>
<span>}</span>

<span>extension</span> <span>MyClass</span><span>:</span> <span>MTKViewDelegate</span> <span>{</span>
    <span>func</span> <span>draw</span><span>(</span><span>in</span> <span>view</span><span>:</span> <span>MTKView</span><span>)</span> <span>{</span>
        <span>guard</span> <span>let</span> <span>commandBuffer</span> <span>=</span> <span>commandQueue</span><span>.</span><span>makeCommandBuffer</span><span>()</span> <span>else</span> <span>{</span>
            <span>return</span>
        <span>}</span>
        <span>...</span>
    <span>}</span>
<span>}</span>

</code></pre></div></div>

<p>As shown above, you’ll usually want to generate a command buffer in response to some event, say an <a href="https://developer.apple.com/documentation/metalkit/mtkviewdelegate/draw(in:)"><code>MTKViewDelegate</code> callback</a> or <a href="https://developer.apple.com/documentation/avfoundation/avvideocompositing/1388894-startrequest"><code>AVVideoCompositing.startRequest(_:)</code></a>.</p>

<h3 id="2-encoding-function-calls">2. Encoding Function Calls</h3>

<p>The basic pattern is going to look like this</p>

<div><div><pre><code><span>// Get a command encoder</span>
<span>let</span> <span>encoder</span><span>:</span> <span>MTLComputeCommandEncoder</span> <span>=</span> <span>commandBuffer</span><span>.</span><span>makeComputeCommandEncoder</span><span>()</span><span>!</span>

<span>// Set up the pipeline state (i.e., encode a reference to your function)</span>
<span>let</span> <span>library</span><span>:</span> <span>MTLLibrary</span> <span>=</span> <span>device</span><span>.</span><span>makeDefaultLibrary</span><span>()</span><span>!</span>
<span>let</span> <span>fn</span><span>:</span> <span>MTLFunction</span> <span>=</span> <span>library</span><span>.</span><span>makeFunction</span><span>(</span><span>name</span><span>:</span> <span>&#34;mix&#34;</span><span>)</span><span>!</span>
<span>let</span> <span>pipelineState</span><span>:</span> <span>MTLComputePipelineState</span> <span>=</span> <span>device</span><span>.</span><span>makeComputePipelineState</span><span>(</span><span>function</span><span>:</span> <span>fn</span><span>)</span><span>!</span>
<span>encoder</span><span>.</span><span>setComputePipelineState</span><span>(</span><span>pipelineState</span><span>)</span>

<span>// Encode references to your textures and parameters</span>
<span>encoder</span><span>.</span><span>setTexture</span><span>(</span><span>...</span><span>)</span>
<span>encoder</span><span>.</span><span>setBytes</span><span>(</span><span>...</span><span>)</span>

<span>// Dispatch threads (describe how you want the GPU to process the request)</span>
<span>encoder</span><span>.</span><span>dispatchThreads</span><span>(</span><span>...</span><span>)</span>
<span>encoder</span><span>.</span><span>endEncoding</span><span>()</span>
</code></pre></div></div>

<p>A couple of points to keep in mind:</p>

<ol>
  <li>You want to instantiate your library once and hold a reference to it</li>
  <li>Your pipeline states are expensive to create and should be cached and reused</li>
  <li>We covered setting textures and bytes above. <a href="https://developer.apple.com/documentation/metal/mtlbuffer#"><code>MTLBuffer</code>s</a> behave the same if you need to use those.</li>
  <li>If you don’t call <code>dispatchThreads(_:threadsPerThreadgroup:)</code> your function won’t actually be invoked.</li>
  <li>You need to remember to call <code>endEncoding</code>, otherwise you’ll get a crash when you start trying to encode your next command.</li>
</ol>

<p>I’m not really sure about the science of <code>dispatchThreads</code>, but the approach I’ve been taking is to use:</p>

<p>a) MTLSize(width: textureWidth, height: textureHeight, depth: 1) for <code>threadsPerGrid</code>
b) MTLSize(width: 8, height: 8, depth: 1) for <code>threadsPerThreadgroup</code></p>

<p>I don’t really have anything to add here other than that this method details how you want Metal to apportion resources to run your function. The first argument represents the total number of elements (pixels) that need to be processed, and the second is how big you want your threadgroups to be (how much parallelism you want.)</p>

<h3 id="3-committing">3. Committing</h3>

<div><div><pre><code><span>let</span> <span>buffer</span><span>:</span> <span>MTLCommandBuffer</span> <span>=</span> <span>...</span>
<span>...</span>

<span>buffer</span><span>.</span><span>commit</span><span>()</span>
<span>buffer</span><span>.</span><span>waitUntilCompleted</span><span>()</span>
</code></pre></div></div>

<p>In our case, we want to wait until the buffer has been processed so that we can use our final texture to render a <code>CIImage</code> or whatever but that’s pretty much it!</p>

<h2 id="recap">Recap</h2>

<p>Image processing with Metal isn’t so tricky. Now that we know how to set up a Metal pipeline, encode functions and data, and get our code to run on the GPU, we can build arbitrary image processing tools outside of what’s achievable with Core Image alone. To recap:</p>

<ol>
  <li>Some objects are long-lived: MTLDevice, MTLCommandQueue, and MTLLibrary, as well as a pool of MTLTextures (covered in the next post)</li>
  <li>Every time we render a frame, we need to encode all of the functions that we want to call, along with references to textures and any data that the functions need in order to run. We do this with a fresh command encoder for each function invocation. The functions will run in the order they’ve been added to the command buffer.</li>
  <li>Part of encoding a function invocation is making sure you’ve set up the correct pipeline state, dispatched threads, and ended encoding.</li>
</ol>

<p>In the next post I’ll detail how to integrate a Metal pipeline with video streams, show you some glue code to make everything a little less verbose, and take a step back to look at the trip a command buffer takes through your image pipeline.</p>



</div>



    </div></div>
  </body>
</html>
