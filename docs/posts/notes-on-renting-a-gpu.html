<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stpn.bearblog.dev/notes-on-renting-a-gpu/">Original</a>
    <h1>notes on renting a gpu</h1>
    
    <div id="readability-page-1" class="page"><div>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-06-01T19:30Z">
                    01 Jun, 2025
                </time>
            </i>
        </p>
    

    <p>This past week I attempted to reproduce the results from the <a href="https://openai.com/index/deep-double-descent/">2019 deep double descent</a> paper and found myself in need of a beefier GPU.</p>
<p>The details of the matter don&#39;t matter too much here, but the paper uses the resnet18 model from 2015 so the gpu requirements aren&#39;t too crazy. My 2024 m4 pro macbook can run this model locally at ~10s per training iteration.</p>
<p>However, the paper wants 4000 iterations (or &#34;epochs&#34;) which would take 11 hours total. From there, it additionally wants 64 model sizes with different 3 configurations to compare between. Even cutting that down to just 7 model sizes (size=1, 2, ...64), that&#39;s over 9 days of non-stop training time for my poor macbook.</p>
<p>I thought &#34;renting a GPU&#34; would be a complicated endeavor but it turns out to be mostly the same as spinning up an EC2 machine. Here, I&#39;ll document some things I learned about how GPU workloads scale and some practical notes for doing training runs on them.</p>
<h2 id="getting-a-machine-running">getting a machine running</h2><p>I made accounts on both <a href="https://vast.ai">vast.ai</a> and <a href="https://lambda.ai">lambda labs</a>, having taken recommendations from <a href="https://arena-chapter0-fundamentals.streamlit.app">ARENA</a>. Lambda feels like a more enterprise solution (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>$</mi></mrow></math>) whereas vast.ai seems like airbnb&#39;ing GPUs from randos. Overall, vast.ai has both more and cheaper options.</p>
<h2 id="selecting-a-machine">selecting a machine</h2><p>There are a lot of knobs to decide what machine to get. I ended up renting probably 10-15 machines while I figured out what was going on.</p>
<h3 id="network">network</h3><p>Machines with ~100mbps up/down both took longer to bootup initially and to download pytorch and friends which got frustrating pretty quickly.</p>
<h3 id="memory-and-cpu">memory and cpu</h3><p>Resnet18 is pretty small (0.5-1GB per process depending on model size), but I ideally wanted many copies running in parallel. That meant my bottleneck was going to be on compute, not memory or memory bandwidth.</p>
<p>If I&#39;m understanding correctly, more expensive cards (A100, etc.) could offer more memory/parallelism but can actually run at a slower clock speed. My training process is bottlenecked by the serial nature of training, so that extra parallelism would go to waste.</p>
<p>I ended up going with 2x Nvidia 4090 cards which was overkill on memory (24GB), but could train an epoch in a reasonable amount of time.</p>
<h3 id="sm-and-pytorch-compatibility">sm and pytorch compatibility</h3><p>Once I figured out I wanted a fast, cheap card, I tried a 5090 (successor to the 4090, released Jan 2025), but as of writing in June 2025, stable pytorch doesn&#39;t support its sm version (think: something like the <a href="https://en.wikipedia.org/wiki/Instruction_set_architecture">ISA for the gpu</a>). The 5090 requires v9.0+ but pytorch is at v8.x. This ended up being hugely frustrating for someone not well-versed in python package management and I ended up giving up after wrestling <code>uv</code> and <code>pip</code> for an hour.</p>
<h2 id="other-considerations">other considerations</h2><p>I didn&#39;t pay much attention to what CPU/memory bandwidth machines had because they weren&#39;t bottlenecks for my model runs, but I imagine they can factor in (dataset loading etc, especially for large models) depending on the workload.</p>
<p>I was also kind of shocked at the price. A 4090 will run you <a href="https://www.newegg.com/p/pl?d=4090">$4.7k</a>, but renting one is ~$.27/hr. That&#39;s nearly 2 years of compute before you get to break even.</p>
<p>For my purposes, 7 model sizes x 3 configs x 4000 epochs ended up taking about ~20-22 hours for a full run over 2x 4090s, costing ~$13. Not bad! The deep double descent paper doesn&#39;t disclose its training time for comparison.</p>
<p>After you actually boot a machine up, it&#39;s more similar than not to any other VM. You can ssh in, git pull your notebook down, and get things running. As a vscode enjoyer, the <a href="https://code.visualstudio.com/docs/remote/ssh">remote ssh</a> workflow works great.</p>
<p>I looked at a lot of <code>nvidia-smi</code> (system management interface) which shows the current GPU % utilization, memory usage, and power draw which was super helpful for experimenting with how to run my workload best. Later, I switched to <a href="https://github.com/XuehaiPan/nvitop"><code>nvitop</code></a> which has the same information but with nicer visualizations and updates over time.</p>
<p>Coming from a background of mostly running CPU-compute systems, I was surprised to find that context switching seems relatively cheap on GPUs. Intuitively, this kind of makes sense since CPUs specialize in a lot of hardware (pipelining, branch prediction, etc.) to make serialized programs fast, while GPUs end up with specializations to fan-out compute and doing things in parallel.</p>
<p>To illustrate this point, at first I thought I wanted many GPUs running a single process instead of many processes on a single GPU, but that turned out to not matter much. More runs on a single GPU turned out to scale linear-ish-ly and didn&#39;t have obvious contention problems.</p>
<p>My initial training script would execute only one model config at a time. I didn&#39;t really want to manually manage 21 different scripts on the remote machine so I ended up using the python <code>multiprocessing</code> module with something like:</p>
<div><pre><span></span><span>random_start</span> <span>=</span> <span>random</span><span>.</span><span>randint</span><span>(</span><span>0</span><span>,</span> <span>gpu_count</span> <span>-</span> <span>1</span><span>)</span>
<span>k_set</span> <span>=</span> <span>[</span><span>2</span><span>**</span><span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>7</span><span>)]</span>
<span>jobs</span> <span>=</span> <span>[</span>
    <span>TrainingArgs</span><span>(</span>
        <span>model_args</span><span>=</span><span>ModelArgs</span><span>(</span><span>k</span><span>=</span><span>k</span><span>),</span>
        <span># ...other training args</span>
        <span>rank</span><span>=</span><span>(</span><span>i</span> <span>+</span> <span>random_start</span><span>)</span><span>%</span><span>gpu_count</span><span>,</span>
    <span>)</span>
    <span>for</span> <span>i</span><span>,</span> <span>k</span> <span>in</span> <span>enumerate</span><span>(</span><span>k_set</span><span>)</span>
<span>]</span>

<span>with</span> <span>mp</span><span>.</span><span>Pool</span><span>(</span><span>processes</span><span>=</span><span>gpu_count</span> <span>*</span> <span>args</span><span>.</span><span>jobs_per_gpu</span><span>)</span> <span>as</span> <span>pool</span><span>:</span>
    <span>pool</span><span>.</span><span>map</span><span>(</span><span>train</span><span>,</span> <span>jobs</span><span>)</span>
</pre></div>
<p>As a caveat, this code assigns jobs to GPUs up front (via &#34;rank&#34;), but I found that <code>pool.map</code> doesn&#39;t seem to actually respect that ordering for starting jobs, so if you do run a configuration where not all the jobs are executed from the start, this might try to schedule all the jobs that are on the same GPU for some reason. I briefly looked at fixing this but gave up since I wanted all the jobs running at once anyways.</p>
<p>I also had some trouble with separate <code>tqdm</code> progress bars clobbering each other. You can mitigate this to some degree with the <a href="https://tqdm.github.io/docs/tqdm/#tqdm-objects">position</a> argument, but it only helps so much since the separate subprocesses can&#39;t coordinate writes to stdout.</p>
<h2 id="gpu-compute-is-more-familiar-than-not">gpu compute is more familiar than not</h2><p>That&#39;s about it! For whatever reason, I thought &#34;connecting to a GPU&#34; would be a very annoying task of connecting my local pytorch over the network and doing some driver voodoo, but it turned out to be pretty tame experience.</p>


    

    
        

        
            


        
    


  </div></div>
  </body>
</html>
