<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html">Original</a>
    <h1>ChromeOS will soon be developed on large portions of the Android stack</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
	

<p> Originally posted 2024-06-11</p>
<p> Tagged: <a href="https://www.moderndescartes.com/essays/tags/software_engineering">software engineering</a>, <a href="https://www.moderndescartes.com/essays/tags/statistics">statistics</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr/>

<p>I once worked with an algorithm whose runtime scaled roughly linearly
with the number of rows of data. To date, the largest job we’d ever
attempted had taken a few hours. One day, we tried to run a job that was
only four times as large as our previous largest job. We figured we’d
just leave it running for a day and it would be complete. Instead, due
to frequent job preemption and other flakiness, the job took almost two
weeks to complete! Each time, the easiest path forward was to restart
and pray for good luck. I joked that we’d somehow discovered an
algorithm with exponential runtime.</p>
<p>When I went back and did the math, it turns out we did, in fact, have
exponential runtime!</p>
<p>If flakiness is proportional (with rate <span>\(p\)</span>) to the base flake-free runtime of the
job, then the flake equation says that total runtime grows exponentially
with job size.</p>
<p><span>\[O(f(n)) \rightarrow O(f(n)\cdot
e^{p\cdot f(n)})\]</span></p>
<p>When the expected number of flakes is less than one, we’re in the
flat part of the exponential term, and flakiness is an occasional
nuisance. However, as the expected number of flakes exceeds one per run,
the likelihood of job success drops exponentially. If you retry the job
until it completes, you can expect to wait some time that is exponential
in the job size.</p>
<p>The derivation is pretty simple. The expected number of flakes is
<span>\(O(p\cdot f(n))\)</span>, and then the
probability that you complete the job with zero flakes is, by the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson
distribution</a>, <span>\(O(e^{-p\cdot
f(n)})\)</span>. Inverting this probability, we see that on average, we
must repeat the job <span>\(O(e^{p\cdot
f(n)})\)</span> times before a job completes without flaking.</p>
<h2 id="a-grand-unified-theory-of-bug-fixes">A grand unified theory of
bug fixes</h2>
<p>Flakiness is not just bad luck. It is a failure to design for the
realities of the production environment. And yet we only have limited
engineering hours to fix all possible scenarios. Flakes are omnipresent,
passing through our software like neutrinos through the Earth, and we
cannot and should not try to fix them all. The only folks who actually
worry about literal cosmic rays, are NASA, who must deal with elevated
radiation in space with extended mission times, precisely the two terms
that show up in the flake equation’s exponential term.</p>
<p>The flake equation presents a sharp decision boundary: any specific
flake will exist in one of two regimes: occasionally annoying, or
oppressively showstopping. As you scale up software usage, flakes flip
from the former regime to the latter, giving you the shortest of warning
periods during the transition. In my experience, great engineers can
sniff out early hints of flakiness and fix them while the exponential is
still in the somewhat flat parts of its growth curve, allowing teams to
scale smoothly.</p>
<p>Because of the exponential growing penalty associated with these
flakes, there is no ignoring flakes - as soon as a flake mode starts to
rear its head, you will not be able to scale more than 2x past your
current level without seeing that flake grow into a reliable job-killing
monster.</p>
<h2 id="flake-prone-contexts">Flake-prone contexts</h2>
<p>Software scaling can happen in many forms: in job size, in job
volume, in heterogeneity of deployment environments, in the diversity of
user journeys, and so on. Each of these scale ups presents an
opportunity for an previously innocuous flake mode to go exponential.
You will encounter increasingly obscure modes of flakiness, and your
software’s growth rate will be capped at the rate at which you can fix
these showstoppers.</p>
<p>We can see the flake equation most clearly in software whose usage
grows continuously in multiple dimensions.</p>
<h3 id="continuous-integration">Continuous integration</h3>
<p>One haven for flakiness is in continuous integration and testing,
where developer activity generates both increased volume and
heterogeneity of tests. CI is particularly pernicious because there is
often no recourse to a failed CI run other than to simply rerun the
entire test suite! Software teams adopt the following mitigations to
stave off the inevitability of flaky CI:</p>
<ul>
<li>reduction in scope of how many tests need to run with each
change</li>
<li>discipline in not writing inherently flaky tests (no random numbers,
no network connections, no complex containers/servers etc.)</li>
<li>granular retries of failed tests, with tests marked pass if the
second try passes.</li>
</ul>
<p>Test-level retries seems to irritate some folks who think that we
should focus our efforts on reducing flaky tests in the first place, but
I will point out that retrying is dead simple to activate and reduces
the flake equation penalty from <span>\(O(e^{p\cdot
f(n)})\)</span> to <span>\(O(e^{p^2\cdot
f(n)})\)</span>, while fixing flaky tests consumes developer time linear
to each fixed test. It allows teams to exponentially flatten the global
failure rate while continuing to derive value from individual flaky
tests. (Presumably the flaky tests still generate some value, otherwise
they ought to be deleted entirely!)</p>
<h3 id="large-language-model-training">Large Language Model
Training</h3>
<p>Another contemporary haven for flakes is LLM training runs. LLM
scaling implies scaling up in volume and variety of data, quantity of
hardware being orchestrated, and duration of training. As training runs
scale from GPT2 to GPT3 to GPT4 to GPT5 sized models, increasingly
obscure failure modes are encountered. To give an example of a
GPT3-scale flake, if the average lifetime of a GPU is 5 years, then if
you run a cluster of 500 GPUs for a week, well.. that’s about 10 years
of GPU time. Two of the GPUs will die on average per run, and how will
the code respond to that?</p>
<p>For LLMs, checkpointing is the canonical solution to flakiness. It’s
a powerful solution because it covers a broad class of flake modes,
whereas rearchitecting to detect and recover from failed GPUs, irregular
network connectivity, etc. is far harder and requires deep engineering
expertise for each flake mode fixed.</p>
<h4 id="checkpointing-optimization">Checkpointing optimization</h4>
<p>Some fun math, feel free to skip: if there are <span>\(\lambda\)</span> expected flakes per run, the
whole job costs <span>\(N\)</span>, rate of flakes
is <span>\(p\)</span> (with <span>\(Np = \lambda\)</span>), and checkpointing costs
<span>\(C\)</span> per checkpoint save, then what is
the optimal frequency of checkpointing?</p>
<p>Let’s assume all progress since the last checkpoint is lost if a
flake occurs, and that the checkpointing operation itself is flake-free.
Then, we can track the total cost of a job as the cost to reach each
checkpoint. Call the number of checkpoints <span>\(f\)</span>. The cost to reach each checkpoint is
<span>\(\frac{N}{f}e^{\lambda/f}\)</span>, and the
total cost is <span>\(Ne^{\lambda/f} +
Cf\)</span>.</p>
<p>We can take the derivative w.r.t. <span>\(f\)</span> to find the minimum of this function to
be the solution of the following equation:</p>
<p><span>\[-\frac{N\lambda}{f^2} * e^{\lambda/f} +
C = 0\]</span></p>
<p>Rearranging, we get the following equation:</p>
<p><span>\[f^2e^f =
\lambda^2e^{\lambda}\frac{N}{C\lambda}\]</span></p>
<p>As an approximation to the solution, let’s guess that <span>\(f = \lambda\)</span> (i.e. if there are 5 expected
flakes per run, then take 5 checkpoints). In that scenario, we’re left
with the equation <span>\(C\lambda = N\)</span>. So
in other words, if there are 5 flakes, and we checkpoint 5 times, and
the cost of one checkpoint is roughly 1/5th the cost of the whole job,
then this is the optimal frequency of checkpointing. If checkpointing is
cheaper than that, then we should checkpoint more frequently than the
expected number of flakes, and vice versa.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It is highly counterintuitive to me that the retry penalty of
flakiness is exponential, rather than linear. I think the reason is that
we so rarely manage to glimpse and understand this transitionary phase
from mostly flat to exponential blowup. Either the job is only mildly
flaky/annoying (in which case the Taylor approximation to the
exponential is, in fact, linear), or it is so annoying that it already
got fixed.</p>
<p>There are contexts in which we live this transitionary phase for
years: sometimes company growth is <em>just</em> slow enough that the
exponential growth term comes on gradually over one or two years. In
these scenarios, the metaphorical frog is boiled, and only an outside
perspective can comprehend the stupidly low productivity. From the
inside, it just feels like a bad case of tech debt. While the “tech
debt” diagnosis is trivially true, the underlying insight is that
incremental tech debt gets added together, and then exponentiated - so
that each marginal unit of debt is responsible for another
multiplicative slowdown for the whole team! As companies scale,
investments into technical excellence and developer productivity is what
forestalls the tyranny of the flake equation.</p>


    </div>
</div></div>
  </body>
</html>
