<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2404.01332">Original</a>
    <h1>Explaining Large Language Models Decisions Using Shapley Values</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2404.01332"
        dc:identifier="/abs/2404.01332"
        dc:title="Explaining Large Language Models Decisions Using Shapley Values"
        trackback:ping="/trackback/2404.01332" />
    </rdf:RDF>
--><div id="abs-outer">

  <div>
    

    <p><strong>arXiv:2404.01332</strong> (cs)
    </p>

<div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2404.01332">View PDF</a></p><blockquote>
            <span>Abstract:</span>The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model&#39;s output. Through two applications - a discrete choice experiment and an investigation of cognitive biases - we demonstrate how the Shapley value method can uncover what we term &#34;token noise&#34; effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation. Our model-agnostic approach extends its utility to proprietary LLMs, providing a valuable tool for practitioners and researchers to strategically optimize prompts and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving LLM responses before relying on them as substitutes for human subjects in survey settings. We emphasize the importance of researchers reporting results conditioned on specific prompt templates and exercising caution when drawing parallels between human behavior and LLMs.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div>
    <div>
      <h2>Submission history</h2><p> From: Behnam Mohammadi [<a href="https://arxiv.org/show-email/52ec1f13/2404.01332" rel="nofollow">view email</a>]      </p></div>
  </div>
  <!--end leftcolumn-->
<div>    <div>
      <p><a name="other"></a>
      <span>Full-text links:</span></p><h2>Access Paper:</h2>
      <ul>
  <li><a href="https://arxiv.org/pdf/2404.01332" aria-describedby="download-button-info" accesskey="f">View PDF</a></li><li><a href="https://arxiv.org/format/2404.01332">Other Formats</a></li></ul>
      
    </div>
    <!--end full-text-->    <div><p>
    Current browse context: </p><p>cs.CL</p>

  

    </div>
      

<p>

    <span id="bib-cite-trigger">export BibTeX citation</span>
    
</p>

<div>
  <p><h3>Bookmark</h3></p><p><a href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2404.01332&amp;description=Explaining Large Language Models Decisions Using Shapley Values" title="Bookmark on BibSonomy">
    <img src="https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png" alt="BibSonomy logo"/>
  </a>
  <a href="https://reddit.com/submit?url=https://arxiv.org/abs/2404.01332&amp;title=Explaining Large Language Models Decisions Using Shapley Values" title="Bookmark on Reddit">
    <img src="https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png" alt="Reddit logo"/>
  </a>
</p></div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      
      <div>
        <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
      </div>
        
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p>
      <p>
      <label for="tabfour">Related Papers</label></p>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
        <div>
          <div>
            
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv&#39;s community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  
  
</div>
      </div>
    </div></div>
  </body>
</html>
