<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/bentoml/OpenLLM">Original</a>
    <h1>OpenLLM</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bentoml/OpenLLM/blob/main/assets/main-banner.png"><img src="https://github.com/bentoml/OpenLLM/raw/main/assets/main-banner.png" alt="Banner for OpenLLM"/></a></p>

<h2 tabindex="-1" dir="auto"><a id="user-content--introduction" aria-hidden="true" href="#-introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="book" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png">📖</g-emoji> Introduction</h2>
<p dir="auto">With OpenLLM, you can run inference with any open-source large-language models,
deploy to the cloud or on-premises, and build powerful AI apps.</p>
<p dir="auto"><g-emoji alias="steam_locomotive" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f682.png">🚂</g-emoji> <strong>State-of-the-art LLMs</strong>: built-in supports a wide range of open-source LLMs
and model runtime, including StableLM, Falcon, Dolly, Flan-T5, ChatGLM,
StarCoder and more.</p>
<p dir="auto"><g-emoji alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png">🔥</g-emoji> <strong>Flexible APIs</strong>: serve LLMs over RESTful API or gRPC with one command,
query via WebUI, CLI, our Python/Javascript client, or any HTTP client.</p>
<p dir="auto"><g-emoji alias="chains" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/26d3.png">⛓️</g-emoji> <strong>Freedom To Build</strong>: First-class support for LangChain, BentoML and
HuggingFace that allows you to easily create your own AI apps by composing LLMs
with other models and services.</p>
<p dir="auto"><g-emoji alias="dart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png">🎯</g-emoji> <strong>Streamline Deployment</strong>: Automatically generate your LLM server Docker
Images or deploy as serverless endpoint via
<a href="https://l.bentoml.com/bento-cloud" rel="nofollow"><g-emoji alias="cloud" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2601.png">☁️</g-emoji> BentoCloud</a>.</p>
<p dir="auto"><g-emoji alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png">🤖️</g-emoji> <strong>Bring your own LLM</strong>: Fine-tune any LLM to suit your needs with
<code>LLM.tuning()</code>. (Coming soon)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/bentoml/OpenLLM/blob/main/assets/output.gif"><img src="https://github.com/bentoml/OpenLLM/raw/main/assets/output.gif" alt="Gif showing OpenLLM Intro" data-animated-image=""/></a>
<br/></p>
<h2 tabindex="-1" dir="auto"><a id="user-content--getting-started" aria-hidden="true" href="#-getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>🏃‍ Getting Started</h2>
<p dir="auto">To use OpenLLM, you need to have Python 3.8 (or newer) and <code>pip</code> installed on
your system. We highly recommend using a Virtual Environment to prevent package
conflicts.</p>
<p dir="auto">You can install OpenLLM using pip as follows:</p>

<p dir="auto">To verify if it&#39;s installed correctly, run:</p>
<div data-snippet-clipboard-copy-content="$ openllm -h

Usage: openllm [OPTIONS] COMMAND [ARGS]...

   ██████╗ ██████╗ ███████╗███╗   ██╗██╗     ██╗     ███╗   ███╗
  ██╔═══██╗██╔══██╗██╔════╝████╗  ██║██║     ██║     ████╗ ████║
  ██║   ██║██████╔╝█████╗  ██╔██╗ ██║██║     ██║     ██╔████╔██║
  ██║   ██║██╔═══╝ ██╔══╝  ██║╚██╗██║██║     ██║     ██║╚██╔╝██║
  ╚██████╔╝██║     ███████╗██║ ╚████║███████╗███████╗██║ ╚═╝ ██║
   ╚═════╝ ╚═╝     ╚══════╝╚═╝  ╚═══╝╚══════╝╚══════╝╚═╝     ╚═╝

  An open platform for operating large language models in production.
  Fine-tune, serve, deploy, and monitor any LLMs with ease."><pre><code>$ openllm -h

Usage: openllm [OPTIONS] COMMAND [ARGS]...

   ██████╗ ██████╗ ███████╗███╗   ██╗██╗     ██╗     ███╗   ███╗
  ██╔═══██╗██╔══██╗██╔════╝████╗  ██║██║     ██║     ████╗ ████║
  ██║   ██║██████╔╝█████╗  ██╔██╗ ██║██║     ██║     ██╔████╔██║
  ██║   ██║██╔═══╝ ██╔══╝  ██║╚██╗██║██║     ██║     ██║╚██╔╝██║
  ╚██████╔╝██║     ███████╗██║ ╚████║███████╗███████╗██║ ╚═╝ ██║
   ╚═════╝ ╚═╝     ╚══════╝╚═╝  ╚═══╝╚══════╝╚══════╝╚═╝     ╚═╝

  An open platform for operating large language models in production.
  Fine-tune, serve, deploy, and monitor any LLMs with ease.
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-starting-an-llm-server" aria-hidden="true" href="#starting-an-llm-server"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Starting an LLM Server</h3>
<p dir="auto">To start an LLM server, use <code>openllm start</code>. For example, to start a <code>dolly-v2</code>
server:</p>

<p dir="auto">Following this, a Web UI will be accessible at <a href="http://localhost:3000" rel="nofollow">http://localhost:3000</a> where you
can experiment with the endpoints and sample input prompts.</p>
<p dir="auto">OpenLLM provides a built-in Python client, allowing you to interact with the
model. In a different terminal window or a Jupyter notebook, create a client to
start interacting with the model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="&gt;&gt;&gt; import openllm
&gt;&gt;&gt; client = openllm.client.HTTPClient(&#39;http://localhost:3000&#39;)
&gt;&gt;&gt; client.query(&#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;)"><pre><span>&gt;&gt;</span><span>&gt;</span> <span>import</span> <span>openllm</span>
<span>&gt;&gt;</span><span>&gt;</span> <span>client</span> <span>=</span> <span>openllm</span>.<span>client</span>.<span>HTTPClient</span>(<span>&#39;http://localhost:3000&#39;</span>)
<span>&gt;&gt;</span><span>&gt;</span> <span>client</span>.<span>query</span>(<span>&#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;</span>)</pre></div>
<p dir="auto">You can also use the <code>openllm query</code> command to query the model from the
terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENLLM_ENDPOINT=http://localhost:3000
openllm query &#39;Explain to me the difference between &#34;further&#34; and &#34;farther&#34;&#39;"><pre><span>export</span> OPENLLM_ENDPOINT=http://localhost:3000
openllm query <span><span>&#39;</span>Explain to me the difference between &#34;further&#34; and &#34;farther&#34;<span>&#39;</span></span></pre></div>
<p dir="auto">Visit <code>http://localhost:3000/docs.json</code> for OpenLLM&#39;s API specification.</p>
<p dir="auto">Users can also specify different variants of the model to be served, by
providing the <code>--model-id</code> argument, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="openllm start flan-t5 --model-id google/flan-t5-large"><pre>openllm start flan-t5 --model-id google/flan-t5-large</pre></div>
<p dir="auto">Use the <code>openllm models</code> command to see the list of models and their variants
supported in OpenLLM.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--supported-models" aria-hidden="true" href="#-supported-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="jigsaw" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f9e9.png">🧩</g-emoji> Supported Models</h2>
<p dir="auto">The following models are currently supported in OpenLLM. By default, OpenLLM
doesn&#39;t include dependencies to run all models. The extra model-specific
dependencies can be installed with the instructions below:</p>

<table>
<tbody><tr>
<th>Model</th>
<th>CPU</th>
<th>GPU</th>
<th>Installation</th>
<th>Model Ids</th>
</tr>
<tr>
<td><a href="https://huggingface.co/docs/transformers/model_doc/flan-t5" rel="nofollow">flan-t5</a></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &#34;openllm[flan-t5]&#34;"><pre>pip install <span><span>&#34;</span>openllm[flan-t5]<span>&#34;</span></span></pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/google/flan-t5-small" rel="nofollow"><code>google/flan-t5-small</code></a></li>
<li><a href="https://huggingface.co/google/flan-t5-base" rel="nofollow"><code>google/flan-t5-base</code></a></li>
<li><a href="https://huggingface.co/google/flan-t5-large" rel="nofollow"><code>google/flan-t5-large</code></a></li>
<li><a href="https://huggingface.co/google/flan-t5-xl" rel="nofollow"><code>google/flan-t5-xl</code></a></li>
<li><a href="https://huggingface.co/google/flan-t5-xxl" rel="nofollow"><code>google/flan-t5-xxl</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://github.com/databrickslabs/dolly">dolly-v2</a></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install openllm"><pre>pip install openllm</pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/databricks/dolly-v2-3b" rel="nofollow"><code>databricks/dolly-v2-3b</code></a></li>
<li><a href="https://huggingface.co/databricks/dolly-v2-7b" rel="nofollow"><code>databricks/dolly-v2-7b</code></a></li>
<li><a href="https://huggingface.co/databricks/dolly-v2-12b" rel="nofollow"><code>databricks/dolly-v2-12b</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://github.com/THUDM/ChatGLM-6B">chatglm</a></td>
<td><g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &#34;openllm[chatglm]&#34;"><pre>pip install <span><span>&#34;</span>openllm[chatglm]<span>&#34;</span></span></pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/thudm/chatglm-6b" rel="nofollow"><code>thudm/chatglm-6b</code></a></li>
<li><a href="https://huggingface.co/thudm/chatglm-6b-int8" rel="nofollow"><code>thudm/chatglm-6b-int8</code></a></li>
<li><a href="https://huggingface.co/thudm/chatglm-6b-int4" rel="nofollow"><code>thudm/chatglm-6b-int4</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://github.com/bigcode-project/starcoder">starcoder</a></td>
<td><g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &#34;openllm[starcoder]&#34;"><pre>pip install <span><span>&#34;</span>openllm[starcoder]<span>&#34;</span></span></pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/bigcode/starcoder" rel="nofollow"><code>bigcode/starcoder</code></a></li>
<li><a href="https://huggingface.co/bigcode/starcoderbase" rel="nofollow"><code>bigcode/starcoderbase</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://falconllm.tii.ae/" rel="nofollow">falcon</a></td>
<td><g-emoji alias="x" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png">❌</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &#34;openllm[falcon]&#34;"><pre>pip install <span><span>&#34;</span>openllm[falcon]<span>&#34;</span></span></pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/tiiuae/falcon-7b" rel="nofollow"><code>tiiuae/falcon-7b</code></a></li>
<li><a href="https://huggingface.co/tiiuae/falcon-40b" rel="nofollow"><code>tiiuae/falcon-40b</code></a></li>
<li><a href="https://huggingface.co/tiiuae/falcon-7b-instruct" rel="nofollow"><code>tiiuae/falcon-7b-instruct</code></a></li>
<li><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" rel="nofollow"><code>tiiuae/falcon-40b-instruct</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://github.com/Stability-AI/StableLM">stablelm</a></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install openllm"><pre>pip install openllm</pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b" rel="nofollow"><code>stabilityai/stablelm-tuned-alpha-3b</code></a></li>
<li><a href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b" rel="nofollow"><code>stabilityai/stablelm-tuned-alpha-7b</code></a></li>
<li><a href="https://huggingface.co/stabilityai/stablelm-base-alpha-3b" rel="nofollow"><code>stabilityai/stablelm-base-alpha-3b</code></a></li>
<li><a href="https://huggingface.co/stabilityai/stablelm-base-alpha-7b" rel="nofollow"><code>stabilityai/stablelm-base-alpha-7b</code></a></li></ul>
</td>
</tr>
<tr>
<td><a href="https://huggingface.co/docs/transformers/model_doc/opt" rel="nofollow">opt</a></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td><g-emoji alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">✅</g-emoji></td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="pip install openllm"><pre>pip install openllm</pre></div>
</td>
<td>
<ul dir="auto"><li><a href="https://huggingface.co/facebook/opt-125m" rel="nofollow"><code>facebook/opt-125m</code></a></li>
<li><a href="https://huggingface.co/facebook/opt-350m" rel="nofollow"><code>facebook/opt-350m</code></a></li>
<li><a href="https://huggingface.co/facebook/opt-1.3b" rel="nofollow"><code>facebook/opt-1.3b</code></a></li>
<li><a href="https://huggingface.co/facebook/opt-2.7b" rel="nofollow"><code>facebook/opt-2.7b</code></a></li>
<li><a href="https://huggingface.co/facebook/opt-6.7b" rel="nofollow"><code>facebook/opt-6.7b</code></a></li>
<li><a href="https://huggingface.co/facebook/opt-66b" rel="nofollow"><code>facebook/opt-66b</code></a></li></ul>
</td>
</tr>
</tbody></table>

<h3 tabindex="-1" dir="auto"><a id="user-content-runtime-implementations-experimental" aria-hidden="true" href="#runtime-implementations-experimental"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Runtime Implementations (Experimental)</h3>
<p dir="auto">Different LLMs may have multiple runtime implementations. For instance, they
might use Pytorch (<code>pt</code>), Tensorflow (<code>tf</code>), or Flax (<code>flax</code>).</p>
<p dir="auto">If you wish to specify a particular runtime for a model, you can do so by
setting the <code>OPENLLM_{MODEL_NAME}_FRAMEWORK={runtime}</code> environment variable
before running <code>openllm start</code>.</p>
<p dir="auto">For example, if you want to use the Tensorflow (<code>tf</code>) implementation for the
<code>flan-t5</code> model, you can use the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="OPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5"><pre>OPENLLM_FLAN_T5_FRAMEWORK=tf openllm start flan-t5</pre></div>
<blockquote>
<p dir="auto"><span><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</span> For GPU support on Flax, refers to
<a href="https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier">Jax&#39;s installation</a>
to make sure that you have Jax support for the corresponding CUDA version.</p>
</blockquote>
<h3 tabindex="-1" dir="auto"><a id="user-content-integrating-a-new-model" aria-hidden="true" href="#integrating-a-new-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Integrating a New Model</h3>
<p dir="auto">OpenLLM encourages contributions by welcoming users to incorporate their custom
LLMs into the ecosystem. Check out
<a href="https://github.com/bentoml/OpenLLM/blob/main/ADDING_NEW_MODEL.md">Adding a New Model Guide</a>
to see how you can do it yourself.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-️-integrations" aria-hidden="true" href="#️-integrations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="gear" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2699.png">⚙️</g-emoji> Integrations</h2>
<p dir="auto">OpenLLM is not just a standalone product; it&#39;s a building block designed to
easily integrate with other powerful tools. We currently offer integration with
<a href="https://github.com/bentoml/BentoML">BentoML</a> and
<a href="https://github.com/hwchase17/langchain">LangChain</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-bentoml" aria-hidden="true" href="#bentoml"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>BentoML</h3>
<p dir="auto">OpenLLM models can be integrated as a
<a href="https://docs.bentoml.org/en/latest/concepts/runner.html" rel="nofollow">Runner</a> in your
BentoML service. These runners have a <code>generate</code> method that takes a string as a
prompt and returns a corresponding output string. This will allow you to plug
and play any OpenLLM models with your existing ML workflow.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import bentoml
import openllm

model = &#34;dolly-v2&#34;

llm_config = openllm.AutoConfig.for_model(model)
llm_runner = openllm.Runner(model, llm_config=llm_config)

svc = bentoml.Service(
    name=f&#34;llm-dolly-v2-service&#34;, runners=[llm_runner]
)

@svc.api(input=Text(), output=Text())
async def prompt(input_text: str) -&gt; str:
    answer = await llm_runner.generate(input_text)
    return answer"><pre><span>import</span> <span>bentoml</span>
<span>import</span> <span>openllm</span>

<span>model</span> <span>=</span> <span>&#34;dolly-v2&#34;</span>

<span>llm_config</span> <span>=</span> <span>openllm</span>.<span>AutoConfig</span>.<span>for_model</span>(<span>model</span>)
<span>llm_runner</span> <span>=</span> <span>openllm</span>.<span>Runner</span>(<span>model</span>, <span>llm_config</span><span>=</span><span>llm_config</span>)

<span>svc</span> <span>=</span> <span>bentoml</span>.<span>Service</span>(
    <span>name</span><span>=</span><span>f&#34;llm-dolly-v2-service&#34;</span>, <span>runners</span><span>=</span>[<span>llm_runner</span>]
)

<span>@<span>svc</span>.<span>api</span>(<span>input</span><span>=</span><span>Text</span>(), <span>output</span><span>=</span><span>Text</span>())</span>
<span>async</span> <span>def</span> <span>prompt</span>(<span>input_text</span>: <span>str</span>) <span>-&gt;</span> <span>str</span>:
    <span>answer</span> <span>=</span> <span>await</span> <span>llm_runner</span>.<span>generate</span>(<span>input_text</span>)
    <span>return</span> <span>answer</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-huggingface-agents" aria-hidden="true" href="#huggingface-agents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>HuggingFace Agents</h3>
<p dir="auto">OpenLLM seamlessly integrates with HuggingFace Agents.</p>
<blockquote>
<p dir="auto"><span><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</span> The HuggingFace Agent is still at experimental stage. It is
recommended to OpenLLM with <code>pip install &#39;openllm[nightly]&#39;</code> to get the latest
API update for HuggingFace agent.</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="import transformers

agent = transformers.HfAgent(&#34;http://localhost:3000/hf/agent&#34;)  # URL that runs the OpenLLM server

agent.run(&#34;Is the following `text` positive or negative?&#34;, text=&#34;I don&#39;t like how this models is generate inputs&#34;)"><pre><span>import</span> <span>transformers</span>

<span>agent</span> <span>=</span> <span>transformers</span>.<span>HfAgent</span>(<span>&#34;http://localhost:3000/hf/agent&#34;</span>)  <span># URL that runs the OpenLLM server</span>

<span>agent</span>.<span>run</span>(<span>&#34;Is the following `text` positive or negative?&#34;</span>, <span>text</span><span>=</span><span>&#34;I don&#39;t like how this models is generate inputs&#34;</span>)</pre></div>
<blockquote>
<p dir="auto"><span><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</span> Only <code>starcoder</code> is currently supported with Agent integration. The
example aboved was also ran with four T4s on EC2 <code>g4dn.12xlarge</code></p>
</blockquote>
<p dir="auto">If you want to use OpenLLM client to ask questions to the running agent, you can
also do so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import openllm

client = openllm.client.HTTPClient(&#34;http://localhost:3000&#34;)

client.ask_agent(
    task=&#34;Is the following `text` positive or negative?&#34;,
    text=&#34;What are you thinking about?&#34;,
)"><pre><span>import</span> <span>openllm</span>

<span>client</span> <span>=</span> <span>openllm</span>.<span>client</span>.<span>HTTPClient</span>(<span>&#34;http://localhost:3000&#34;</span>)

<span>client</span>.<span>ask_agent</span>(
    <span>task</span><span>=</span><span>&#34;Is the following `text` positive or negative?&#34;</span>,
    <span>text</span><span>=</span><span>&#34;What are you thinking about?&#34;</span>,
)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-langchain-coming-soon" aria-hidden="true" href="#langchain-coming-soon"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LangChain (<g-emoji alias="hourglass_flowing_sand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/23f3.png">⏳</g-emoji>Coming Soon!)</h3>
<p dir="auto">In future LangChain releases, you&#39;ll be able to effortlessly invoke OpenLLM
models, like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain.llms import OpenLLM
llm = OpenLLM.for_model(model_name=&#39;flan-t5&#39;)
llm(&#34;What is the difference between a duck and a goose?&#34;)"><pre><span>from</span> <span>langchain</span>.<span>llms</span> <span>import</span> <span>OpenLLM</span>
<span>llm</span> <span>=</span> <span>OpenLLM</span>.<span>for_model</span>(<span>model_name</span><span>=</span><span>&#39;flan-t5&#39;</span>)
<span>llm</span>(<span>&#34;What is the difference between a duck and a goose?&#34;</span>)</pre></div>
<p dir="auto">if you have an OpenLLM server deployed elsewhere, you can connect to it by
specifying its URL:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain.llms import OpenLLM
llm = OpenLLM.for_model(server_url=&#39;http://localhost:8000&#39;, server_type=&#39;http&#39;)
llm(&#34;What is the difference between a duck and a goose?&#34;)"><pre><span>from</span> <span>langchain</span>.<span>llms</span> <span>import</span> <span>OpenLLM</span>
<span>llm</span> <span>=</span> <span>OpenLLM</span>.<span>for_model</span>(<span>server_url</span><span>=</span><span>&#39;http://localhost:8000&#39;</span>, <span>server_type</span><span>=</span><span>&#39;http&#39;</span>)
<span>llm</span>(<span>&#34;What is the difference between a duck and a goose?&#34;</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content--deploying-to-production" aria-hidden="true" href="#-deploying-to-production"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji> Deploying to Production</h2>
<p dir="auto">To deploy your LLMs into production:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Building a Bento</strong>: With OpenLLM, you can easily build a Bento for a
specific model, like <code>dolly-v2</code>, using the <code>build</code> command.:</p>

<p dir="auto">A
<a href="https://docs.bentoml.org/en/latest/concepts/bento.html#what-is-a-bento" rel="nofollow">Bento</a>,
in BentoML, is the unit of distribution. It packages your program&#39;s source
code, models, files, artifacts, and dependencies.</p>
</li>
<li>
<p dir="auto"><strong>Containerize your Bento</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="bentoml containerize &lt;name:version&gt;"><pre>bentoml containerize <span>&lt;</span>name:version<span>&gt;</span></pre></div>
<p dir="auto">BentoML offers a comprehensive set of options for deploying and hosting
online ML services in production. To learn more, check out the
<a href="https://docs.bentoml.org/en/latest/concepts/deploy.html" rel="nofollow">Deploying a Bento</a>
guide.</p>
</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content--telemetry" aria-hidden="true" href="#-telemetry"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="grapes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f347.png">🍇</g-emoji> Telemetry</h2>
<p dir="auto">OpenLLM collects usage data to enhance user experience and improve the product.
We only report OpenLLM&#39;s internal API calls and ensure maximum privacy by
excluding sensitive information. We will never collect user code, model data, or
stack traces. For usage tracking, check out the
<a href="https://github.com/bentoml/OpenLLM/blob/main/src/openllm/utils/analytics.py">code</a>.</p>
<p dir="auto">You can opt-out of usage tracking by using the <code>--do-not-track</code> CLI option:</p>
<div dir="auto" data-snippet-clipboard-copy-content="openllm [command] --do-not-track"><pre>openllm [command] --do-not-track</pre></div>
<p dir="auto">Or by setting environment variable <code>OPENLLM_DO_NOT_TRACK=True</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENLLM_DO_NOT_TRACK=True"><pre><span>export</span> OPENLLM_DO_NOT_TRACK=True</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content--community" aria-hidden="true" href="#-community"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="busts_in_silhouette" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f465.png">👥</g-emoji> Community</h2>
<p dir="auto">Engage with like-minded individuals passionate about LLMs, AI, and more on our
<a href="https://l.bentoml.com/join-openllm-discord" rel="nofollow">Discord</a>!</p>
<p dir="auto">OpenLLM is actively maintained by the BentoML team. Feel free to reach out and
join us in our pursuit to make LLMs more accessible and easy-to-use<g-emoji alias="point_right" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f449.png">👉</g-emoji>
<a href="https://l.bentoml.com/join-slack" rel="nofollow">Join our Slack community!</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content--contributing" aria-hidden="true" href="#-contributing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><g-emoji alias="gift" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f381.png">🎁</g-emoji> Contributing</h2>
<p dir="auto">We welcome contributions! If you&#39;re interested in enhancing OpenLLM&#39;s
capabilities or have any questions, don&#39;t hesitate to reach out in our
<a href="https://l.bentoml.com/join-openllm-discord" rel="nofollow">discord channel</a>.</p>
<p dir="auto">Checkout our
<a href="https://github.com/bentoml/OpenLLM/blob/main/DEVELOPMENT.md">Developer Guide</a>
if you wish to contribute to OpenLLM&#39;s codebase.</p>
</article>
          </div></div>
  </body>
</html>
