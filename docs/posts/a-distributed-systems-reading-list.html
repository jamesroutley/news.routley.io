<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ferd.ca/a-distributed-systems-reading-list.html">Original</a>
    <h1>A distributed systems reading list</h1>
    
    <div id="readability-page-1" class="page"><article>
        
            <span>2024/02/07</span>
        
        
        
<p>This document contains various resources and quick definition of a lot of background information behind distributed systems. It is not complete, even though it is kinda sorta detailed. I had written it some time in 2019 when coworkers at the time had asked for a list of references, and I put together what I thought was a decent overview of the basics of distributed systems literature and concepts.</p>

<p>Since I was asked for resources again recently, I decided to pop this text into my blog. I have verified the links again and replaced those that broke with archive links or other ones, but have not sought alternative sources when the old links worked, nor taken the time to add any extra content for new material that may have been published since then.</p>

<p>It is meant to be used as a quick reference to understand various distsys discussions, and to discover the overall space and possibilities that are around this environment.</p>

<h3>Foundational theory</h3>


<p>This is information providing the basics of all the distsys theory. Most of the papers or resources you read will make references to some of these concepts, so explaining them makes sense.</p>

<h4>Models</h4>


<h5>In a Nutshell</h5>


<p>There are three model types used by computer scientists doing distributed system theory:</p>

<ol>
<li>synchronous models</li>
<li>semi-synchronous models</li>
<li>asynchronous models</li>
</ol>
<p>A <strong>synchronous model</strong> means that each message sent within the system has a known <em>upper bound</em> on communications (max delay between a message being sent and received) and the processing speed between nodes or agents. This means that you can <em>know</em> for sure that after a period of time, a message was missed. This model is applicable in rare cases, such as hardware signals, and is mostly beginner mode for distributed system proofs.</p>

<p>An <strong>asynchronous model</strong> means that you have no upper bound. It is legit for agents and nodes to process and delay things indefinitely. You can never assume that a &#34;lost&#34; message you haven&#39;t seen for the last 15 years won&#39;t just happen to be delivered tomorrow. The other node can also be stuck in a GC loop that lasts 500 centuries, that&#39;s good.</p>

<p>Proving something works on asynchronous model means it works with all other types. This is expert mode for proofs and is even trickier than real world implementations to make work in most cases.</p>

<p>The <strong>Semi-synchronous models</strong> are the cheat mode for real world. There <em>are</em> upper-bounds to the communication mechanisms and nodes everywhere, but they are often configurable and unspecified. This is what lets a protocol designer go &#34;you know what, we&#39;re gonna stick a <em>ping</em> message in there, and if you miss too many of them we consider you&#39;re dead.&#34;</p>

<p>You can&#39;t assume all messages are delivered reliably, but you give yourself a chance to say &#34;now that&#39;s enough, I won&#39;t wait here forever.&#34;</p>

<p>Protocols like Raft, Paxos, and ZAB (quorum protocols behind etcd, Chubby, and ZooKeeper respectively) all fit this category.</p>

<h4>Theoretical Failure Modes</h4>


<p>The way failures happen and are detected is important to a bunch of algorithms. The following are the most commonly used ones:</p>

<ol>
<li>Fail-stop failures</li>
<li>Crash failures</li>
<li>Omission failures</li>
<li>Performance failures</li>
<li>Byzantine failures</li>
</ol>
<p>First, <strong>Fail-stop failures</strong> mean that if a node encounters a problem, everyone can know about it and detect it, and can restore state from stable storage. This is easy mode for theory and protocols, but super hard to achieve in practice (and in some cases impossible)</p>

<p><strong>Crash failures</strong> mean that if a node or agent has a problem, it crashes and then never comes back. You are either correct or late forever. This is actually easier to design around than fail-stop in theory (but a huge pain to operate because redundancy is the name of the game, forever).</p>

<p><strong>Omission failures</strong> imply that you give correct results that respect the protocol or never answer.</p>

<p><strong>Performance failures</strong> assumes that while you respect the protocol in terms of the content of messages you send, you will also possibly send results late.</p>

<p><strong>Byzantine failures</strong> means that anything can go wrong (including people willingly trying to break you protocol with bad software pretending to be good software). There&#39;s a special class of <strong>authentication-detectable byzantine failures</strong> which at least put the constraint that you can&#39;t forge other messages from other nodes, but that is an optional thing. Byzantine modes <a href="http://scholar.harvard.edu/files/mickens/files/thesaddestmoment.pdf">are the worst</a>.</p>

<p>By default, most distributed system theory assumes that there are no bad actors or agents that are corrupted and willingly trying to break stuff, and byzantine failures are left up to blockchains and some forms of package management.</p>

<p>Most modern papers and stuff will try and stick with either crash or fail-stop failures since they tend to be practical.</p>

<p>See <a href="https://ti.tuwien.ac.at/cps/teaching/courses/dependable_systems-ss08/dcs_slides/dcs-2007-p5.pdf">this typical distsys intro slide deck</a> for more details.</p>

<h4>Consensus</h4>


<p>This is one of the core problems in distributed systems: how can all the nodes or agents in a system agree on one value? The reason it&#39;s so important is that if you can agree on <em>just one value</em>, you can then do a lot of things.</p>

<p>The most common example of picking a single very useful value is <em>the name of an elected leader</em> that enforces decisions, just so you can stop having to build more consensuses because holy crap consensuses are painful.</p>

<p>Variations exist on what exactly is a consensus, including does everyone agree fully? (strong) or just a majority? (t-resilient) and asking the same question in various synchronicity or failure models.</p>

<p>Note that while classic protocols like Paxos use a leader to ensure consistency and speed up execution while remaining consistent, a bunch of systems will forgo these requirements.</p>

<h4>FLP Result</h4>


<h5>In A Nutshell</h5>


<p>Stands for <em>Fischer-Lynch-Patterson</em>, the authors of a 1985 paper that states that proper consensus where <em>all</em> participants agree on a value is unsolvable in a purely asynchronous model (even though it is in a synchronous model) as long as any kind of failure is possible, even if they&#39;re just delays.</p>

<p>It&#39;s one of the most influential papers in the arena because it triggered a lot of other work for other academics to define what exactly is going on in distributed systems.</p>

<h5>Detailed reading</h5>


<ul>
<li><a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">original paper</a></li>
<li><p><a href="https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility/">blog post review</a> (<a href="https://web.archive.org/web/20230603122927/https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility/">archive</a>)</p></li>
</ul>
<h4>Fault Detection</h4>


<p>Following FLP results, which showed that failure detection was kind of super-critical to making things work, a lot of computer science folks started working on what exactly it means to detect failures.</p>

<p>This stuff is hard and often much less impressive than we&#39;d hope for it to be. There are <em>strong</em> and <em>weak</em> fault detectors. The former implies all faulty processes are eventually identified by all non-faulty ones, and the latter that only some non-faulty processes find out about faulty ones.</p>

<p>Then there are degrees of accuracy:</p>

<ol>
<li>Nobody who has not crashed is suspected of being crashed</li>
<li>It&#39;s possible that a non-faulty process is never suspected at all</li>
<li>You can be confused because there&#39;s chaos but at some point non-faulty processes stop being suspected of being bad</li>
<li>At some point there&#39;s at least one non-faulty process that is not suspected</li>
</ol>
<p>You can possibly realize that a strong and fully accurate detector (said to be <em>perfect</em>) kind of implies that you get a consensus, and since consensus is not really doable in a fully asynchronous system model with failures, then there are hard limits to things you can detect reliably.</p>

<p>This is often why <em>semi-synchronous</em> system models make sense: if you treat delays greater than <em>T</em> to be a failure, then you can start doing adequate failure detection.</p>

<p><a href="https://www.inf.ed.ac.uk/teaching/courses/ds/slides1415/failure.pdf">See this slide deck for a decent intro</a></p>

<h4>CAP Theorem</h4>


<p>The CAP theorem was for a long while just a conjecture, but has been proven in the early 2000s, leading to a lot of eventually consistent databases.</p>

<h5>In A Nutshell</h5>


<p>There are three properties to a distributed system:</p>

<ul>
<li><strong>C</strong>onsistency: any time you write to a system and read back from it, you get the value you wrote or a fresher one back.</li>
<li><strong>A</strong>vailability: every request results in a response (including both reads and writes)</li>
<li><strong>P</strong>artition tolerance: the network can lose messages</li>
</ul>
<p>In theory, you can get a system that is both available and consistent, but only under synchronous models on a perfect network. Those don&#39;t really exist so in practice <strong>P</strong> is always there.</p>

<p>What the CAP theorem states is essentially that <em>given</em> <strong>P</strong>, you have to choose either <strong>A</strong> (keep accepting writes and potentially corrupt data) <em>or</em> <strong>C</strong> (stop accepting writes to save the data, and go down).</p>

<h5>Refinements</h5>


<p>CAP is a bit strict in what you get in practice. Not all partitions in a network are equivalent, and not all consistency levels are the same.</p>

<p>Two of the most common approaches to add some flexibility to the CAP theorem are the <em>Yield/Harvest</em> models and <em>PACELC</em>.</p>

<p><em>Yield/Harvest</em> essentially says that you can think of the system differently: <em>yield</em> is your ability to fulfill requests (as in uptime), and <em>harvest</em> is the fraction of all the potential data you can actually return. Search engines are a common example here, where they will increase their <em>yield</em> and answer more often by reducing their <em>harvest</em> when they ignore some search results to respond faster if at all.</p>

<p><em>PACELC</em> adds the idea that eventually-consistent databases are overly strict. In case of network <strong>P</strong>artitioning you have to choose between <strong>A</strong>vailability or <strong>C</strong>onsistency, but <strong>E</strong>lse --when the system is running normally--one has to choose between <strong>L</strong>atency and <strong>C</strong>onsistency. The idea is that you can decide to degrade your consistency for availability (but only when you really need to), or you could decide to always forego consistency because you gotta go fast.</p>

<p>It is important to note that you <em>cannot</em> beat the CAP theorem (as long as you respect the models under which it was proven), and anyone claiming to do so is often a snake oil salesman.</p>

<h5>Resources</h5>


<ul>
<li><a href="https://mwhittaker.github.io/blog/an_illustrated_proof_of_the_cap_theorem/">CAP visual proof</a></li>
<li><a href="https://codahale.com/you-cant-sacrifice-partition-tolerance/">You can&#39;t sacrifice partition tolerance</a></li>
<li><a href="https://en.wikipedia.org/wiki/PACELC_theorem">PACELC</a></li>
</ul>
<p>There&#39;s been countless rehashes of the CAP theorem and various discussions over the years; the results are mathematically proven even if many keep trying to make the argument that they&#39;re so reliable it doesn&#39;t matter.</p>

<h4>Message Passing Definitions</h4>


<p>Messages can be sent zero or more times, in various orderings. Some terms are introduced to define what they are:</p>

<ul>
<li><em>unicast</em> means that the message is sent to one entity only</li>
<li><em>anycast</em> means that the message is sent to any valid entity</li>
<li><em>broadcast</em> means that a message is sent to all valid entities</li>
<li><em>atomic broadcast</em> or <em>total order broadcast</em> means that all the non-faulty actors in a system receive the same messages in the same order, whichever that order is</li>
<li><em>gossip</em> stands for the family of protocols where messages are forwarded between peers with the hope that eventually everyone gets all the messages</li>
<li><em>at least once delivery</em> means that each message will be sent once or more; listeners are to expect to see all messages, but possibly more than once</li>
<li><em>at most once delivery</em> means that each sender will only send the message one time. It&#39;s possible that listeners never see it.</li>
<li><em>exactly once delivery</em> means that each message is guaranteed to be sent and seen only once. This is a nice theoretical objective but quite impossible in real systems. It ends up being simulated through other means (combining <em>atomic broadcast</em> with specific flags and ordering guarantees, for example)</li>
</ul>
<p>Regarding ordering:</p>

<ul>
<li><em>total order</em> means that all messages have just one strict ordering and way to compare them, much like 3 is always greater than 2.</li>
<li><em>partial order</em> means that some messages can compare with some messages, but not necessarily all of them. For example, I could decide that all the updates to the key <code>k1</code> can be in a total order regarding each other, but independent from updates to the key <code>k2</code>. There is therefore a <em>partial</em> order between all updates across all keys, since <code>k1</code> updates bear no information relative to the <code>k2</code> updates.</li>
<li><em>causal order</em> means that all messages that depend on other messages are received after these (you can&#39;t learn of a user&#39;s avatar before you learn about that user). It is a specific type of partial order.</li>
</ul>
<p>There isn&#39;t a &#34;best&#34; ordering, each provides different possibilities and comes with different costs, optimizations, and related failure modes.</p>

<h5>Idempotence</h5>


<p>Idempotence is important enough to warrant its own entry. Idempotence means that when messages are seen more than once, resent or replayed, they don&#39;t impact the system differently than if they were sent just once.</p>

<p>Common strategies is for each message to be able to refer to previously seen messages so that you define an ordering that will prevent replaying older messages, setting unique IDs (such as transaction IDs) coupled with a store that will prevent replaying transactions, and so on.</p>

<p>See <a href="https://queue.acm.org/detail.cfm?id=2187821">Idempotence is not a medical condition</a> for a great read on it, with various related strategies.</p>

<h4>State Machine Replication</h4>


<p>This is a theoretical model by which, given the same sequences of states and the same operations applied to them (disregarding all kinds of non-determinism), all state machines will end up with the same result.</p>

<p>This model ends up being critical to most reliable systems out there, which tend to all try to replay all events to all subsystems in the same order, ensuring predictable data sets in all places.</p>

<p>This is generally done by picking a leader; all writes are done through the leader, and all the followers get a consistent replicated state of the system, allowing them to eventually become leaders or to fan-out their state to other actors.</p>

<h4>State-Based Replication</h4>


<p>State-based replication can be conceptually simpler to state-machine replication, with the idea that if you only replicate the state, you get the state at the end!</p>

<p>The problem is that it is extremely hard to make this fast and efficient. If your state is terabytes large, you don&#39;t want to re-send it on every operation. Common approaches will include splitting, hashing, and bucketing of data to detect changes and only send the changed bits (think of <code>rsync</code>), <a href="https://en.wikipedia.org/wiki/Merkle_tree">merkle trees</a> to detect changes, or the idea of a <code>patch</code> to source code.</p>

<h3>Practical Matters</h3>


<p>Here are a bunch of resources worth digging into for various system design elements.</p>

<h4>End-to-End Argument in System Design</h4>


<p>Foundational practical aspect of system design for distributed systems:</p>

<ul>
<li>a message that is sent is not a message that is necessarily received by the other party</li>
<li>a message that is received by the other party is not necessarily a message that is actually read by the other party</li>
<li>a message that is read by the other party is not necessarily a message that has been acted on by the other party</li>
</ul>
<p>The conclusion is that if you want anything to be reliable, you <em>need</em> an end-to-end acknowledgement, usually written by the application layer.</p>

<ul>
<li><a href="https://blog.acolyer.org/2014/11/14/end-to-end-arguments-in-system-design/">Overview by the morning paper</a></li>
<li><a href="http://web.mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf">Actual paper</a></li>
<li><a href="https://en.wikipedia.org/wiki/End-to-end_principle">Wikipedia page</a></li>
</ul>
<p>These ideas are behind the design of TCP as a protocol, but the authors also note that it wouldn&#39;t be sufficient to leave it at the protocol, the application layer must be involved.</p>

<h4>Fallacies of Distributed Computing</h4>


<p>The fallacies are:</p>

<ul>
<li>The network is reliable</li>
<li>Latency is zero</li>
<li>Bandwidth is infinite</li>
<li>The network is secure</li>
<li>Topology doesn&#39;t change</li>
<li>There is one administrator</li>
<li>Transport cost is zero</li>
<li>The network is homogeneous</li>
</ul>
<p>Partial explanations on the <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">Wiki page</a> or full ones in <a href="https://www.researchgate.net/publication/322500050_Fallacies_of_Distributed_Computing_Explained">the paper</a>.</p>

<h4>Common Practical Failure Modes</h4>


<p>In practice, when you switch from Computer Science to Engineering, the types of faults you will find are a bit more varied, but can map to any of the theoretical models.</p>

<p>This section is an informal list of common sources of issues in a system. See also <a href="https://ferd.ca/beating-the-cap-theorem-checklist.html">the CAP theorem checklist</a> for other common cases.</p>

<h5>Netsplit</h5>


<p>Some nodes can talk to each other, but some nodes are unreachable to others. A common example is that a US-based network can communicate fine internally, and so could a EU-based network, but both would be unable to speak to each-other</p>

<h5>Asymmetric Netsplit</h5>


<p>Communication between groups of nodes is not symmetric. For example, imagine that the US network can send messages to the EU network, but the EU network cannot respond back.</p>

<p>This is a rarer mode when using TCP (although it has happened before), and a potentially common one when using UDP.</p>

<h5>Split Brain</h5>


<p>The way a lot of systems deal with failures is to keep a majority going. A split brain is what happens when both sides of a netsplit think they are the leader, and starts making conflicting decisions.</p>

<h5>Timeouts</h5>


<p>Timeouts are particularly tricky because they are non-deterministic. They can only be <em>observed</em> from one end, and you never know if a timeout that is ultimately interpreted as a failure was actually a failure, or just a delay due to networking, hardware, or GC pauses.</p>

<p>There are times where retransmissions are not safe if the message has already been seen (i.e. it is not idempotent), and timeouts essentially make it impossible to know if retransmission is safe to try: was the message acted on, dropped, or is it still in transit or in a buffer somewhere?</p>

<h5>Missing Messages due to Ordering</h5>


<p>Generally, using TCP and crashes will tend to mean that few messages get missed across systems, but frequent cases can include:</p>

<ul>
<li>The node has gone down (or the software crashed) for a few seconds during which it missed a message that won&#39;t be repeated</li>
<li><p>The updates are received transitively across various nodes. For example, a message published by service <code>A</code> on a bus (whether Kafka or RMQ) can end up read, transformed or acted on and re-published by service <code>B</code>, and there is a possibility that service <code>C</code> will read <code>B</code>&#39;s update before <code>A</code>&#39;s, causing issues in causality</p></li>
</ul>
<h5>Clock Drift</h5>


<p>Not all clocks on all systems are synchronized properly (even using <a href="https://en.wikipedia.org/wiki/Network_Time_Protocol">NTP</a>) and will go at different speeds.</p>

<p>Using a timestamp to sort through events is almost guaranteed to be a source of bugs, even moreso if the timestamps come from multiple computers.</p>

<h5>The Client is Part of the System</h5>


<p>A very common pitfall is to forget that the client that participates in a distributed system is part of it. Consistency on the server-side will not necessarily be worth much if the client can&#39;t make sense of the events or data it receives.</p>

<p>This is particularly insidious for database clients that do a non-idempotent transactions, time out, and have no way to know if they can try it again.</p>

<h5>Restoring from multiple backups</h5>


<p>A single backup is kind of easy to handle. Multiple backups run into a problem called <a href="https://www.cs.cornell.edu/courses/cs5414/2010fa/publications/BM93.pdf">consistent cuts</a> (<a href="https://blog.mattchung.me/2021/02/13/distributed-system-snapshots-consistent-vs-inconsistent-cuts/">high level view</a>) and distributed snapshots, which means that not all the backups are taken at the same time, and this introduces inconsistencies that can be construed as corrupting data.</p>

<p>The good news is there&#39;s no great solution and everyone suffers the same.</p>

<h4>Consistency Models</h4>


<p>There are dozens different levels of consistency, all of which are documented on <a href="https://en.wikipedia.org/wiki/Consistency_model">Wikipedia</a>, by <a href="http://www.vldb.org/pvldb/vol7/p181-bailis.pdf">Peter Bailis&#39; paper on the topic</a>, or overviewed by <a href="https://aphyr.com/posts/313-strong-consistency-models">Kyle Kingsbury post on them</a></p>

<ul>
<li><em>Linearizability</em> means each operation appears atomic and could not have been impacted by another one, as if they all ran just one at a time. The order is known and deterministic, and a read that started after a given write had started will be able to see that data.</li>
<li><em>Serializability</em> means that while all operations appear to be atomic, it makes no guarantee about <em>which</em> order they would have happened in. It means that some operations <em>might</em> start after another one and complete before it, and as long as the isolation is well-maintained, that isn&#39;t a problem.</li>
<li><em>Sequential consistency</em> means that even if operations might have taken place out-of-order, they will appear as if they all happened in order</li>
<li><em>Causal Consistency</em> means that only operations that have a logical dependency on each other need to be ordered amongst each other</li>
<li><em>Read-committed</em> consistency means that any operation that has been committed is available for further reads in the system</li>
<li><em>Repeatable reads</em> means that within a transaction, reading the same value multiple times always yields the same result</li>
<li><em>Read-your-writes</em> consistency means that any write you have completed must be readable <em>by the same client</em> subsequently</li>
<li><em>Eventual Consistency</em> is a kind of special family of consistency measures that say that the system can be inconsistent as long as it eventually becomes consistent again. <em>Causal consistency</em> is an example of eventual consistency.</li>
<li><em>Strong Eventual Consistency</em> is like <em>eventual consistency</em> but demands that no conflicts can happen between concurrent updates. This is usually the land of <em>CRDTs</em>.</li>
</ul>
<p>Note that while these definitions have clear semantics that academics tend to respect, they are not adopted uniformly or respected in various projects&#39; or vendors&#39; documentation in the industry.</p>

<h4>Database Transaction Scopes</h4>


<p>By default, most people assume database transactions are linearizable, and they tend not to be because that&#39;s way too slow as a default.</p>

<p>Each database might have different semantics, so the following links may cover the most major ones.</p>

<ul>
<li><a href="https://www.postgresql.org/docs/9.1/transaction-iso.html">PostgreSQL</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html">MySQL</a> (depends on the storage engine used)</li>
<li><a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql?view=sql-server-2017">Transact-SQL</a> (most of Microsoft&#39;s products)</li>
<li><a href="https://docs.oracle.com/cd/E25178_01/server.1111/e25789/consist.htm">Oracle</a></li>
</ul>
<p>Be aware that while the PostgreSQL documentation is likely the clearest and most easy to understand one to introduce the topic, various vendors can assign different meanings to the same standard transaction scopes.</p>

<h4>Logical Clocks</h4>


<p>Those are data structures that allow to create either total or partial orderings between messages or state transitions.</p>

<p>Most common ones are:</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Lamport_timestamps">Lamport timestamps</a>, which are just a counter. They allow the silent undetected crushing of conflicting data</li>
<li><a href="https://en.wikipedia.org/wiki/Vector_clock">Vector Clocks</a>, which contain a counter per node, incremented on each message seen. They can detect conflicts in data and on operations.</li>
<li><a href="https://en.wikipedia.org/wiki/Version_vector">Version Vectors</a> are like vector clocks, but only change the counters on state variations rather than all event seens</li>
<li><a href="https://github.com/ricardobcl/Dotted-Version-Vectors">Dotted Version Vectors</a> are fancy version vectors that allow tracking conflicts that would be perceived by the client talking to a server.</li>
<li><p><a href="https://github.com/ricardobcl/Interval-Tree-Clocks">Interval Tree Clocks</a> attempts to fix the issues of other clock types by requiring less space to store node-specific information and allowing a kind of built-in garbage collection. It also has <a href="http://gsd.di.uminho.pt/members/cbm/ps/itc2008.pdf">one of the nicest papers ever</a>.</p></li>
</ul>
<h4>CRDTs</h4>


<p>CRDTs essentially are data structures that restrict operations that can be done such that they can never conflict, no matter which order they are done in or how concurrently this takes place.</p>

<p>Think of it as the specification on how someone would write a distributed redis that was never wrong, but only left maths behind.</p>

<p>This is still an active area of research and countless papers and variations are always coming out.</p>

<ul>
<li><a href="https://hal.inria.fr/inria-00609399v1/document">The big original complete paper</a></li>
<li><a href="https://medium.com/@istanbul_techie/a-look-at-conflict-free-replicated-data-types-crdt-221a5f629e7e">A decent intro</a></li>
<li><a href="https://medium.com/@amberovsky/crdt-conflict-free-replicated-data-types-b4bfc8459d26">Another intro</a></li>
<li><p><a href="https://blog.acolyer.org/2019/03/11/efficient-synchronisation-of-state-based-crdts/">Challenges in synchronizing CRDTs in practice</a></p></li>
</ul>
<h3>Other interesting material</h3>


<ul>
<li><a href="https://aphyr.com/tags/jepsen">Databases Reviews by Kyle Kingsbury</a></li>
<li><a href="https://dancres.github.io/Pages/">This list of material is great</a></li>
<li><a href="https://www.ijcsmc.com/docs/papers/June2014/V3I6201466.pdf">Leader Election Algorithms</a></li>
<li><a href="https://raft.github.io/">Raft</a> (easy protocol intro)</li>
<li><a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">Paxos made simple</a></li>
<li><a href="https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf">Paxos</a> (the part-time parliament)</li>
<li><a href="https://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf">Paxos made Live</a> (google experience report in making paxos work)</li>
<li><a href="https://en.wikipedia.org/wiki/Paxos_" title="computer_science">Paxos variations</a></li>
<li><a href="https://www.cs.cornell.edu/courses/cs6452/2012sp/papers/zab-ieee.pdf">ZAB</a> (the zookeeper algorithm)</li>
<li><a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">The Dynamo paper</a></li>
</ul>
<p>The bible for putting all of these views together is <a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/">Designing Data-Intensive Applications</a> by Martin Kleppmann. Be advised however that everyone I know who absolutely loves this book are people who had a good foundation in distributed systems from reading a bunch of papers, and greatly appreciated having it all put in one place. Most people I&#39;ve seen read it in book clubs with the aim get better at distributed systems still found it challenging and confusing at times, and benefitted from having someone around to whom they could ask questions in order to bridge some gaps. It is still the clearest source I can imagine for everything in one place.</p>

    </article></div>
  </body>
</html>
