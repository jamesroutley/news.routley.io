<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thomasbroadley.com/blog/new-metr-blog-post-on-monitorability/">Original</a>
    <h1>New METR blog post on monitorability</h1>
    
    <div id="readability-page-1" class="page"><section>
        <p>
          Last June or July, I switched from working primarily on METR&#39;s AI
          evaluations infrastructure to working on a particular research
          project. Last week, my team published a blog post explaining our
          research progress so far! You can read it here:
          <a href="https://metr.org/blog/2026-01-19-early-work-on-monitorability-evaluations/">https://metr.org/blog/2026-01-19-early-work-on-monitorability-evaluations/</a>
        </p>
        <p>
          Our goal is to evaluate the monitorability of AI systems. METR wants
          to be able to measure if AIs are capable of tampering with our
          evaluations of them. If they are, then they could deceive METR into
          thinking they&#39;re less capable than they actually are. This blog post
          is a step in that direction (a direction that a lot of other research
          teams are also stepping in, e.g.
          <a href="https://openai.com/index/evaluating-chain-of-thought-monitorability/">OpenAI&#39;s safety team</a>).
        </p>
        <p>
          In our evaluations, we tell an agent to complete a side task while
          pretending to complete a different main task. We took main tasks from
          METR&#39;s <a href="https://arxiv.org/abs/2503.17354">HCAST</a> benchmark.
          Then, we ask a monitor AI to review the agent&#39;s actions (and,
          optionally, chain of thought) and give a probability that the agent
          was in fact trying to complete a side task. We also ask the monitor to
          review transcripts of agents completing just the main task.
        </p>
        <p>
          To translate suspicion probabilities to a binary caught/uncaught
          score, we calculate the threshold, between 0 and 1, that 5% of the
          main-task-only run suspicion probabilities are below. (That is, it&#39;s
          the threshold that gives a 5% false positive rate on main-task-only
          runs.) Then, we count side-task runs as caught if the monitor returns
          a suspicion above the threshold, and uncaught if below the threshold.
        </p>
        <p>
          We do this for a bunch of agents and monitors and see some interesting
          trends:
        </p>
        <p>
          <img src="https://thomasbroadley.com/blog/new-metr-blog-post-on-monitorability/percent_uncaught_heatmap_excluding_visible.jpg" alt="A heatmap of percent uncaught for different combinations of agents and monitors."/>
        </p>
        <p>
          Agents with a higher time horizon are better at evading monitors.
          Monitors with a higher time horizon are better at catching agents.
        </p>
        <p>
          We also find that giving the monitor chain-of-thought access helps a
          lot when monitoring GPT-5, but much less when monitoring Claude Sonnet
          4.5:
        </p>
        <p>
          <img src="https://thomasbroadley.com/blog/new-metr-blog-post-on-monitorability/compare_visible_reasoning.jpg" alt="A bar graph showing average monitor catch rate against GPT-5 high and Claude Sonnet 4.5, broken down by reasoning visibility."/>
        </p>
        <p>
          Both agents and monitors are pretty bad at these tasks. Agents don&#39;t
          generally get that creative in trying to evade monitors. Monitors
          sometimes miss blatant side task actions, and are quite sensitive to
          prompting. We tried a few different monitor prompts before settling on
          one we liked.
        </p>
        <p>
          We collected most of these results last fall. Since then, we&#39;ve
          collected some more interesting data, so my team plans to publish
          another research update on
          <a href="https://metr.org/notes/">https://metr.org/notes/</a> in a
          couple of weeks!
        </p>
      </section></div>
  </body>
</html>
