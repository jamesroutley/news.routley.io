<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://burn.dev/blog/sota-multiplatform-matmul/">Original</a>
    <h1>Multiplatform Matrix Multiplication Kernels</h1>
    
    <div id="readability-page-1" class="page"><div><!--$--> <div data-astro-cid-gx3kfwto=""> <p data-astro-cid-gx3kfwto="">
Few algorithmic problems are as central to modern computing as matrix
          multiplication. It is fundamental to AI, forming the basis of fully
          connected layers used throughout neural networks. In transformer
          architectures, <i data-astro-cid-gx3kfwto="">most of the computation</i> is spent performing matrix
          multiplication. And since compute largely determines capability, faster
          matrix multiplication algorithms directly translate into more powerful
          models <span>[<!--$--><a href="#reference-1">1</a><!--/-->]</span>.
</p><p data-astro-cid-gx3kfwto="">
NVIDIA probably deserves much of the credit for making matrix
          multiplication fast. Their early focus on building GPUs for gaming
          drove major improvements in general-purpose linear algebra. As deep
          learning took over, they introduced Tensor Cores — specialized units
          designed to accelerate matrix multiplication in AI models.
</p><p data-astro-cid-gx3kfwto="">
With hardware evolving to support these new workloads, accelerators
          have become increasingly complex, and writing high-performance code
          for them remains challenging. To ease this burden, hardware vendors
          like NVIDIA, AMD and Intel provide off-the-shelf matrix multiplication
          kernels, typically integrated into tensor libraries. However, the
          quality and performance of these kernels vary. While NVIDIA’s
          implementations, through libraries like cuBLAS <span>[<!--$--><a href="#reference-2">2</a><!--/-->]</span> and cuDNN <span>[<!--$--><a href="#reference-3">3</a><!--/-->]</span> , are highly optimized,
          a key limitation remains: as precompiled binaries, they lack flexibility
          and extensibility.
</p><p data-astro-cid-gx3kfwto="">
In modern AI workloads, the primary bottleneck is no longer
          computation. It&#39;s data movement. Moving data from memory to registers
          often costs more than performing the actual calculations. The best way
          to optimize for this is quite obvious: minimize data movement. A
          powerful way to achieve this is to fuse multiple kernels or algorithms
          into a single kernel <span>[<!--$--><a href="#reference-4">4</a><!--/-->]</span>. For
          matrix multiplication, it means executing element-wise operations on
          the values calculated <i data-astro-cid-gx3kfwto="">just before</i> writing them to global memory.
          Pre-built matrix multiplication kernels don&#39;t support this kind of composition,
          making custom kernel implementations necessary.
</p><p data-astro-cid-gx3kfwto="">
This is why NVIDIA created CUTLASS <span>[<!--$--><a href="#reference-5">5</a><!--/-->]</span>: a set of C++ templates that help developers tailor
          matrix-multiplication kernels to their needs. However, it remains very
          intricate to use and, of course, only works on NVIDIA GPUs.
</p><p data-astro-cid-gx3kfwto="">
As a result, using CUTLASS to optimize a model often comes at the cost
          of portability. But wouldn&#39;t it be better to optimize models across
          platforms? That’s why we built CubeCL <span>[<!--$--><a href="#reference-6">6</a><!--/-->]</span>, and on top of it, a matrix multiplication kernel engine <span>[<!--$--><a href="#reference-7">7</a><!--/-->]</span> that searches for and generates optimized kernels for any GPU and even
          CPUs.
</p> <h3 data-astro-cid-gx3kfwto="">Not Just an Algorithm</h3> <p data-astro-cid-gx3kfwto="">
One of the main challenges with matrix multiplication is that the
          input shapes strongly influence which algorithm performs best. As a
          result, no single algorithm is optimal in all cases; different shapes
          require different strategies.
</p><p data-astro-cid-gx3kfwto="">
To address this, we built an engine that actually makes it very easy
          to create different algorithms that are totally configurable. At its
          core is a multi-level matrix multiplication architecture that we&#39;ll
          explore in this blog.
</p><p data-astro-cid-gx3kfwto="">
...Or, skip the theory and <u data-astro-cid-gx3kfwto=""><a href="#benchmarks" data-astro-cid-gx3kfwto="">jump straight to benchmarks</a></u>.
</p> <h2 data-astro-cid-gx3kfwto="">A Bit on the Hardware First</h2> <p data-astro-cid-gx3kfwto="">
Before diving into the solution, let&#39;s briefly review the architecture
          of modern GPUs. A GPU consists of multiple streaming multiprocessors
          (SMs), each capable of independently executing parts of a program in
          parallel. Each SM has a fixed set of resources shared across multiple
          concurrent executions.
</p><p data-astro-cid-gx3kfwto="">There are three main levels of execution granularity:</p> <ol data-astro-cid-gx3kfwto=""> <li data-astro-cid-gx3kfwto=""> <b data-astro-cid-gx3kfwto="">Unit</b> (<i data-astro-cid-gx3kfwto="">thread</i> in CUDA, <i data-astro-cid-gx3kfwto="">invocation</i> in Vulkan/Wgpu):
            the smallest execution entity performing computations.
</li><li data-astro-cid-gx3kfwto=""> <b data-astro-cid-gx3kfwto="">Plane</b> (<i data-astro-cid-gx3kfwto="">warp</i> in CUDA, <i data-astro-cid-gx3kfwto="">subgroup</i> in Vulkan/Wgpu): a
            group of (typically 32) units executing in lockstep and able to share
            data efficiently through registers.
</li><li data-astro-cid-gx3kfwto=""> <b data-astro-cid-gx3kfwto="">Cube</b> (<i data-astro-cid-gx3kfwto="">thread block</i> in CUDA, <i data-astro-cid-gx3kfwto="">workgroup</i> in Vulkan/Wgpu):
            a group of units that execute on the same SM, sharing memory and able
            to synchronize.
</li> </ol> </div> <figure data-astro-cid-gx3kfwto=""> <img src="https://loeber.substack.com/_astro/matmul_problem.CVEuJXYp.jpg" alt="Matmul Problem" data-astro-cid-gx3kfwto=""/> <figcaption data-astro-cid-gx3kfwto=""> <b data-astro-cid-gx3kfwto="">Matmul Problem</b>. Representation of an (m, n, k) matrix
          multiplication problem in row-major layout. For every pair (i, j) in
          the output, we must compute the accumulated sum over k&#39; in [0, k) of
          Lhs(i, k&#39;) × Rhs(k&#39;, j). In this blog post, we will use a (384, 224,
          64)-Matmul as an example.
</figcaption> </figure> <p data-astro-cid-gx3kfwto="">
Let&#39;s set aside batching and focus on the core problem of computing the
        matrix product Lhs × Rhs with shapes [m, k] × [k, n], which we&#39;ll refer
        to as an (m, n, k)-Matmul.
</p><p data-astro-cid-gx3kfwto="">
A naive CPU implementation would iterate over the m and n dimensions,
        computing dot products along k for each (m, n) pair. As illustrated in
        the above figure, each output element results from a dot product (or
        inner product) involving k multiply-add operations. This process is
        repeated m × n times—once for each output element.
</p><p data-astro-cid-gx3kfwto="">
The same result can also be computed as a sum of k outer products. An
        outer product is what you get when you multiply a column vector by a row
        vector — it gives you all combinations of their elements, filling an
        entire matrix. This approach requires maintaining all m × n intermediate
        results (accumulators) at once, but avoids reloading the same data
        multiple times. Quite interestingly, we will actually use both
        strategies, depending on the level of tiling.
</p> <h3 data-astro-cid-gx3kfwto="">Complexity</h3> <p data-astro-cid-gx3kfwto="">
A matrix multiplication consists of m × n dot products, each involving
        vectors of length k. Each dot product requires k multiply-and-add
        operations, which modern hardware can often fuse into single
        instructions. Although the Strassen algorithm can theoretically reduce
        the number of operations, its practical implementation usually results
        in fewer fused multiply-add instructions, making it less efficient in
        practice <span>[<!--$--><a href="#reference-8">8</a><!--/-->]</span>. Therefore, we
        consider 2 × b × m × n × k as the fundamental operation count. Dividing
        this operation count by execution time yields the achieved number of
        TFLOPs, a measure we will use in the benchmarks section to compare our
        algorithms.
</p><p data-astro-cid-gx3kfwto="">
This means that given optimal hardware instruction usage, there exists a
        theoretical performance ceiling. Our goal in optimizing Matmul at the
        software level is to approach this ceiling as closely as possible. In
        other words, matrix multiplication is compute-bound, and our challenge
        lies in minimizing or hiding global memory access latency.
</p><p data-astro-cid-gx3kfwto="">
As we&#39;ve discussed in our previous explorations of quantization
<span>[<!--$--><a href="#reference-9">9</a><!--/-->]</span> and fusion techniques <span>[<!--$--><a href="#reference-4">4</a><!--/-->]</span>, global memory access almost always represents the primary
        performance bottleneck in GPU computing. While several optimization
        strategies exist - including memory coalescing, vectorization through
        SIMD loads, and efficient unit grouping with planes - implementing these
        optimizations requires careful consideration. Simon Böhm&#39;s excellent
        analysis of CUDA matrix multiplication <span>[<!--$--><a href="#reference-10">10</a><!--/-->]</span>
provides a thorough progression through some of these optimization techniques.
        However, without disciplined, continuous refactoring, you&#39;ll likely find
        yourself maintaining a monolithic, overwhelmingly complex kernel. This reality
        led us to develop a more structured approach using CubeCL.
</p> <h2 data-astro-cid-gx3kfwto="">Four Levels of Abstractions</h2> <p data-astro-cid-gx3kfwto="">
CubeCL&#39;s Rust-based approach to GPU programming leverages traits and
        generics to encode multiple matrix multiplication implementations
        through composable components, using abstractions that incur zero
        runtime cost. The architecture divides matrix multiplication into four
        distinct levels, each handling progressively larger problem scales using
        a divide-and-conquer strategy. This hierarchical structure makes it
        possible to split a single Matmul into smaller, more manageable Matmuls,
        optimizing data locality and movement at each level:
</p>  <ol data-astro-cid-gx3kfwto=""> <li data-astro-cid-gx3kfwto=""> <u data-astro-cid-gx3kfwto=""><a href="#tile_matmul" data-astro-cid-gx3kfwto="">Tile Matmul</a></u>: Interfaces directly
          with hardware capabilities
</li><li data-astro-cid-gx3kfwto=""> <u data-astro-cid-gx3kfwto=""><a href="#stage_matmul" data-astro-cid-gx3kfwto="">Stage Matmul</a></u>: Manages shared memory
          operations for local computations
</li><li data-astro-cid-gx3kfwto=""> <u data-astro-cid-gx3kfwto=""><a href="#global_matmul" data-astro-cid-gx3kfwto="">Global Matmul</a></u>: Accumulates results
          from a series of Stage Matmuls over arbitrary k dimensions
</li><li data-astro-cid-gx3kfwto=""> <u data-astro-cid-gx3kfwto=""><a href="#batch_matmul" data-astro-cid-gx3kfwto="">Batch Matmul</a></u>: Orchestrates multiple
          Global Matmuls across the entire computation
</li> </ol> <!--/--><!--$--><!--/--></div></div>
  </body>
</html>
