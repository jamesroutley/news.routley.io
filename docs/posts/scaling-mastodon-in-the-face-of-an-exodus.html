<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nora.codes/post/scaling-mastodon-in-the-face-of-an-exodus/">Original</a>
    <h1>Scaling Mastodon in the face of an exodus</h1>
    
    <div id="readability-page-1" class="page"><article>
<blockquote>
<p>TL;DR: Mastodon’s Sidekiq deferred execution jobs are the limiting factor for
scaling federated traffic in a single-server or small cluster deployment.
Sidekiq performance scales poorly under a single process model, and can be
limited by database performance in a deployment of the default Dockerized
configuration.</p>
<p><strong>If you are an embattled instance admin, go <a href="#lessons-learned">here</a>.</strong></p>
</blockquote>
<p>I recently moved to a well-established Mastodon
(<a href="https://github.com/hometown-fork/hometown">Hometown</a> fork)
server called <a href="https://weirder.earth">weirder.earth</a>,
a wonderful community with several very dedicated moderators
and longstanding links to other respected servers in the fediverse.
Overall, it’s been lovely; the community is great,
the Hometown fork has some nice ease-of-use features,
and the moderators tend to squash spam and bigotry pretty quickly.</p>
<p>Then Elon Musk bought Twitter.</p>
<h2 id="the-setup">The Setup</h2>
<p>Just as tens of thousands of people<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> fled Twitter to join the fediverse,
Weirder Earth had a storage migration pending.
It did not go well, for reasons unrelated to this post,
and the instance was down for several hours.</p>
<p>Normally, this would not be a problem; ActivityPub<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>,
the protocol that powers Mastodon and other fediverse software,
is very resilient to outages,
and most implementations have plenty of built-in mechanisms like exponential
backoff and so forth to get their neighbors up to date when they come back online.
This particular outage, however, coincided with a huge amount of traffic,
so when the server did come back online,
there were many tens of thousands of messages waiting for it.
Each of these messages necessitates a Sidekiq task which integrates it into the
database, users’ timelines and notification lists, and so forth.</p>
<p>Sidekiq did not succeed in handling this load.</p>
<figure>
    <a href="https://nora.codes/images/sidekiq.png"><img src="https://nora.codes/images/sidekiq.png" alt="the past six months of Sidekiq data. Most of it is steady, maybe a slowdecline, and labeled “In which Elon Musk has not yet bought Twitter”. The lastlittle bit is labeled “OH SHII-” and shows instance load spiking massively."/></a>
A screenshot from the Weirder Earth Sidekiq dashboard, showing processed and
failed jobs over time. Created and edited by <a href="https://weirder.earth/@packbat">Packbats</a>.
</figure>

<p>Users began to experience multi-hour delays in incoming federation,
though outgoing federation was unaffected.
To users of other instances, nothing was wrong;
weirder.earth posts showed up within seconds, or about two minutes at the worst.
Even to weirder.earth users, the web interface worked perfectly well;
it’s just that there was little new content to populate it.</p>
<p>The Sidekiq queue grew and grew over time,
eventually reaching a peak of over 200,000 queued jobs.
In conversation with the admin of another instance of a roughly similar size,
I learned that processing a 6-digit number of jobs is a day is only a fairly
recent phenomenon.
So, this was an issue.</p>
<h2 id="the-cause">The Cause</h2>
<p>I became involved with this situation during the initial outage,
helping with Docker problems, alongside another user, the Packbats,<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>
and another instance admin.
As the Sidekiq queue grew and grew, myself and two of the instance’s admins
looked into every possible cause,
mostly sifting through documentation,
since it was not an area of expertise for any of us.</p>
<p>It was immediately clear that at least part of the problem was Sidekiq’s
inability to successfully use the 8GB of memory and 4 CPU cores available
to the server;
the RSS of the single Sidekiq process was around 350 MB,
and the load factor was averaging around 3 – not even fully utilizing the 4
available cores.</p>
<p>So, we started increasing the number of workers available to Sidekiq.
This was done by increasing <code>MAX_THREADS</code> and passing the <code>-c</code> (“concurrency”)
argument to Sidekiq.
(We later learned that <code>DB_POOL</code> is more appropriate; see below.)
We increased first to 15, then to 25, and finally to 50, which is the
mythical “stable limit” of Sidekiq according to several StackOverflow posts.</p>
<p>It helped, but the queue depth was still growing, so we increased our database
container’s max connections to 200 (by adding -c ‘max_connections = 200’ to the
invocation in docker-compose.yml) and increased to 150 Sidekiq threads.
Again, this helped, but overnight the queue continued to grow.</p>
<p>At this point, I realized that the bottleneck was not in Sidekiq anymore;
individual jobs had gone from completing in seconds to tens of seconds and even
sometimes over a minute.
One of the instance admins pulled up PgHero and we realized that, indeed,
the database was being incredibly conservative with memory.
By default, it was using only 128MB for shared buffers, which is dramatically
low on a server with 4GB of memory and a workload like Mastodon,
where a huge volume of traffic is just fetching the same few posts and
conversations over and over again.
So, we increased that to the aggressive value of 2GB,
and again saw an improvement in performance.
Rather than growing, the queue was now stable, at 194,000 events and a latent
time of about 14 hours.</p>
<p>We had conquered the first derivative.</p>
<h2 id="the-solution">The Solution</h2>
<p>Ultimately, the real solution as a combination of three things:</p>
<ul>
<li>Increasing the memory available to the database for shared buffers</li>
<li>Increasing the number of threads available to Sidekiq
(and, accordingly, the database connection pool size and maximum number of
connections via <code>DB_POOL</code> and <code>max_connections</code>)</li>
<li>Splitting Sidekiq’s queues off into their own processes.</li>
</ul>
<p>See, Mastodon uses 5 Sidekiq queues: <code>default</code>, <code>push</code>, <code>pull</code>, <code>mailers</code>, and
<code>scheduler</code>.
For us, push was conquered early on,
and mailers and scheduler were never an issue,
having very low volume.
<code>pull</code>, for getting incoming federation events, and <code>default</code>, for building
notifications and timelines, were the real trouble.
So, we split them into their own Sidekiq processes.</p>
<p>We had gone up to 200 <code>max_connections</code> and a concurrency of 150, so I
guesstimated that the following split would work well:</p>
<ul>
<li>a single container/process for the <code>push</code>, <code>mailers</code>, and <code>scheduler</code> queues
with 25 threads</li>
<li>a container/process each for <code>pull</code> and <code>default</code> with 60 threads each</li>
</ul>
<p>This worked great for <code>pull</code>. We went from holding steady to dropping about 500 events every
minute, and rapidly cleared the entire pull queue – but the default queue was
still holding on. <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>This was harder to diagnose, but eventually we realized that the Sidekiq
instance working on the default queue wasn’t at its full potential.
Looking at the PgHero connection status dashboard,
we could see that many of our 200 connections were held by inactive or
low-activity Sidekiq processes with names like <code>sidekiq 6.2.1 mastodon [0 of 25 busy]</code> while the active ones were only holding a few connections.
But that should be okay, right? 60 + 60 + 25 is much less than 200,
so there should be plenty of connections to go around.</p>
<p>As it turned out, Puma was also configured to use lots of database connections
(<code>MAX_THREADS = 150</code>), so it was hogging many that it didn’t need.
We sorted that out, refined our Postgres config to allow even more connections,
split the Sidekiq processes up even more,
with fallbacks and various different ordered combinations of queues.
We all went to sleep and woke up to an empty queue!</p>
<p>We’ve been experimenting and optimizing since, but the server has been
coping with the load handily since.</p>
<h2 id="lessons-learned">Lessons Learned</h2>

<p>The default Mastodon configuration is broken.
It’s fine for a tiny instance on a tiny server, but once you start to grow,
you must scale it up. Here’s what we did:</p>
<h3 id="increase-your-databases-resources">Increase Your Database’s Resources</h3>
<p>By default, especially in the default Docker setup,
Postgres is not using any appreciable percentage of your memory.
I suggest using <a href="https://pgtune.leopard.in.ua/">PgTune</a>,
choosing an “online transaction processing” or OLTP workload,
setting about half of your server’s memory, and applying whatever it says.</p>
<p>In Docker, it’s easiest to apply this through adding <code>-c &#34;option=value&#34;</code>
arguments to the invocation in <code>docker-compose.yml</code>;
this can also work if you’re invoking Postgres through a SystemD unit.
Or, you can use <code>postgresql.conf</code>. Don’t forget to restart the database.</p>
<h3 id="split-your-sidekiq-queues">Split Your Sidekiq Queues</h3>
<p>Sidekiq can scale to as many as hundreds of threads on a single process,
but using multiple processes is much more efficient.
<strong>Before increasing concurrency</strong> (that’s the next step!),
split out the queues into different processes.</p>
<p>All the queues in Mastodon can be split among multiple processes,
<strong>except for <code>scheduler</code></strong>, which must only ever be on a single process.
I recommend having a single Sidekiq process each for the <code>mailers</code> and <code>scheduler</code>
queues, and then processes that prioritize <code>pull</code>, <code>push</code>, and <code>default</code>.
You can specify queues with the <code>-q</code> option in <code>docker-compose.yml</code> or your
SystemD unit files.</p>
<p>Specifying multiple <code>-q</code> options gives Sidekiq a fallback order for working on
jobs, so for instance <code>-q default -q pull</code> means that when all <code>default</code> jobs
are done, that process will start working on <code>pull</code> jobs.
Make sure that there is at least one process prioritizing every queue,
or else processing that queue could stall entirely!</p>
<p>Because the queues you want to prioritize are <code>default</code>, <code>push</code>, and <code>pull</code>,
it might be sensible to make processes with all the permutations of
<code>default</code>, <code>push</code>, and <code>pull</code>:</p>
<ul>
<li><code>default</code>, <code>push</code>, <code>pull</code></li>
<li><code>default</code>, <code>pull</code>, <code>push</code></li>
<li><code>pull</code>, <code>default</code>, <code>push</code></li>
<li><code>pull</code>, <code>push</code>, <code>default</code> *</li>
<li><code>push</code>, <code>default</code>, <code>pull</code></li>
<li><code>push</code>, <code>pull</code>, <code>default</code> *</li>
</ul>
<p>However, in reality, <code>push</code> is generally much lower volume than the others,
so it makes more sense to drop the number of queues to four, removing the
fourth and final orderings (marked with *).</p>
<p>To do this in Docker, simply copy and paste the <code>sidekiq</code> service definition,
change the name, and add arguments to the invocation.</p>
<details><summary>For instance, we might go from this default definition: </summary>
<div><pre tabindex="0"><code data-lang="yml"><span>sidekiq</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span></code></pre></div></details>
<details><summary>To these definitions: </summary>
<div><pre tabindex="0"><code data-lang="yml"><span>sidekiq-mailers</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q mailers -q scheduler<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span><span>
</span><span></span><span>sidekiq-scheduler</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q scheduler<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span><span>
</span><span></span><span>sidekiq-default-push-pull</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q default -q push -q pull<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span><span>
</span><span></span><span>sidekiq-default-pull-push</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q default -q pull -q push<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span><span>
</span><span></span><span>sidekiq-pull-default-push</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q pull -q default -q push<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span><span>
</span><span></span><span>sidekiq-push-default-pull</span>:<span>
</span><span>  </span><span>build</span>:<span> </span>.<span>
</span><span>  </span><span>image</span>:<span> </span>tootsuite/mastodon:v3.4.6<span>
</span><span>  </span><span>restart</span>:<span> </span>always<span>
</span><span>  </span><span>env_file</span>:<span> </span>.env.production<span>
</span><span>  </span><span>command</span>:<span> </span>bundle exec sidekiq -q push -q default -q pull<span>
</span><span>  </span><span>depends_on</span>:<span>
</span><span>    </span>- db<span>
</span><span>    </span>- redis<span>
</span><span>  </span><span>networks</span>:<span>
</span><span>    </span>- external_network<span>
</span><span>    </span>- internal_network<span>
</span><span>  </span><span>volumes</span>:<span>
</span><span>    </span>- ./public/system:/mastodon/public/system<span>
</span><span>  </span><span>healthcheck</span>:<span>
</span><span>    </span><span>test</span>:<span> </span>[<span>&#39;CMD-SHELL&#39;</span>,<span> </span><span>&#34;ps aux | grep &#39;[s]idekiq\ 6&#39; || false&#34;</span>]<span>
</span></code></pre></div></details>
<h3 id="fully-utilize-your-database">Fully Utilize Your Database</h3>
<p>Now that it’s got more to work with,
take note of the <code>max_connections</code> parameter you set for Postgres.
Ideally, those connections would be mostly, but not completely, utilized.
<strong>Database contention will slow you down.</strong></p>
<p>The <code>DB_POOL</code> variable controls how many database connections a Ruby on
Rails process will use.
(<code>MAX_THREADS</code> controls this for Puma, the server used in <code>web</code>.)
In Mastodon, the relevant Rails processes are in the
Docker-Compose services <code>web</code>, <code>streaming</code>, and whatever Sidekiq services you’ve
configured.
In <code>docker-compose.yml</code>, you can change these variable by adding a section like
the following to each relevant service:</p>
<div><pre tabindex="0"><code data-lang="yml"><span>environment</span>:<span>
</span><span>  </span>- DB_POOL=value<span>
</span></code></pre></div><p>On <code>sidekiq</code> services, also add the <code>-c</code> option to the <code>sidekiq</code> invocation,
with the same value as the <code>DB_POOL</code> variable.</p>
<div><pre tabindex="0"><code data-lang="yml"><span>command</span>:<span> </span>bundle exec sidekiq -q default -q push -q pull -c 25<span>
</span><span></span><span>environment</span>:<span>
</span><span>  </span>- DB_POOL=25<span>
</span></code></pre></div><p>In addition, the <code>web</code> service takes a variable called <code>WEB_CONCURRENCY</code> to
control how many processes it runs.
Similarly, <code>streaming</code> has <code>STREAMING_CLUSTER_NUM</code> to control the number
of processes.</p>
<p><strong>The sum of <code>MAX_THREADS</code> times <code>WEB_CONCURRENCY</code> in <code>web</code>,
<code>STREAMING_CLUSTER_NUM</code> times <code>DB_POOL</code> in <code>streaming</code>,
and all the <code>sidekiq</code> <code>DB_POOL</code> variables, must be less than <code>max_connections</code>
in your Postgres config.</strong>
If it’s more, you’ll experience database contention.</p>
<p>In the example above,
assuming the rest of the configuration is default and you have 200 database
connections available, I’d set the following:</p>
<ul>
<li><code>web</code>: <code>MAX_THREADS = 10</code>, <code>WEB_CONCURRENCY=3</code> for 30 connections</li>
<li><code>streaming</code>: <code>STREAMING_CLUSTER_NUM = 3</code>, <code>DB_POOL = 15</code> for 45 connections</li>
<li><code>sidekiq-default-push-pull</code>: <code>DB_POOL = 25</code>, <code>-c 25</code> for 25 connections</li>
<li><code>sidekiq-default-pull-push</code>: <code>DB_POOL = 25</code>, <code>-c 25</code> for 25 connections</li>
<li><code>sidekiq-pull-default-push</code>: <code>DB_POOL = 25</code>, <code>-c 25</code> for 25 connections</li>
<li><code>sidekiq-push-default-pull</code>: <code>DB_POOL = 25</code>, <code>-c 25</code> for 25 connections</li>
<li><code>sidekiq-push-scheduler</code>: <code>DB_POOL = 5</code>, <code>-c 5</code> for 5 connections</li>
<li><code>sidekiq-push-mailers</code>: <code>DB_POOL = 5</code>, <code>-c 5</code> for 5 connections</li>
</ul>
<p>For a sum of 185 connections.
This means there will be 15 loose database connections for things like migrations
and manually connecting to the database to do queries and maintenance.</p>
<h2 id="into-the-future">Into the Future</h2>
<p>These numbers may not work perfectly for you, and you may have to tweak things
as workloads change.</p>
<p>There is more work left to do,
and we’ll only know if this is successful once the next big wave hits,
but it’s definitely better than it was before.
We also have not figured out how to monitor the instance for future events,
though the top candidate is, of course, Prometheus and Grafana.</p>
<p>Good luck.
May your timelines run smoothly and your processes never be OOM killed.</p>


</article></div>
  </body>
</html>
