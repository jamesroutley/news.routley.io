<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.paulengstler.com/invisible-stitch/">Original</a>
    <h1>Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting</h1>
    
    <div id="readability-page-1" class="page"><p><video autoplay="" loop="" muted="" playsinline=""><source src="videos/paper_480p.mp4" type="video/mp4"/>Your browser does not support the video tag.</video></p><div><h2 id="dataset">Building 3D Scenes With Depth Inpainting</h2><p>To hallucinate scenes beyond known regions and lift images generated by 2D-based models into three dimensions, current 3D scene generation methods rely on monocular depth estimation networks. For this task, it is crucial to <i>seamlessly</i> integrate the newly hallucinated regions into the existing scene representation. Simple global scale-and-shift operations to the predicted depth map, as used by previous methods, might lead to discontinuities between the scene and its hallucinated extension. We introduce a <b>depth completion network</b> that is able to smoothly extrapolate the existing scene depth based on an input image.</p><div><p><img src="https://research.paulengstler.com/invisible-stitch/images/paper_projection_figure_stacked.jpg" alt="3D scene generation method figure"/></p><p><a target="_blank" href="https://research.paulengstler.com/invisible-stitch/images/paper_projection_figure_stacked.png">View a larger version<!-- --> of this figure<!-- --> <svg viewBox="0 0 24 24" focusable="false"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><path d="M15 3h6v6"></path><path d="M10 14L21 3"></path></g></svg></a></p><p><span>Overview of our 3D scene generation method. </span>Starting from an input image <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">I_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>I</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, we project it to a point cloud based on a depth map predicted by a depth estimation network <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span></span>. To extend the scene, we render it from a new view point and query a generative model <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span></span> to hallucinate beyond the scene&#39;s boundary. Now, we condition <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span></span> on the depth of the existing scene and the image of the scene extended by <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span></span> to produce a geometrically consistent depth map to project the hallucinated points. This process may be repeated until a 360-degree scene has been generated.</p></div><p>The depth completion network learns to inpaint masked depth map regions by being conditioned on an image and the depth of known regions. We use masks that represent typical occlusion patterns generated by view point changes. To retain the model&#39;s ability to predict depth if no sparse depth is available, the sparse depth input is occasionally dropped.</p><div><p><img src="https://research.paulengstler.com/invisible-stitch/images/paper_training_procedure.jpg" alt="Training procedure figure"/></p><p><a target="_blank" href="https://research.paulengstler.com/invisible-stitch/images/paper_training_procedure.png">View a larger version<!-- --> of this figure<!-- --> <svg viewBox="0 0 24 24" focusable="false"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><path d="M15 3h6v6"></path><path d="M10 14L21 3"></path></g></svg></a></p><p><span>Overview of our training procedure. </span>In this compact training scheme, a depth completion network <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span></span> is learned by jointly training depth inpainting as well as depth prediction without a sparse depth input (the ratio being determined by the task probability <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span></span></span></span></span>).A teacher network <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">g_T</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>g</span><span><span><span><span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> is utilized to generate a pseudo ground-truth depth map <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span> for a given image <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span></span></span></span></span>. This depth map is then masked with a random mask <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>M</span></span></span></span></span>, to obtain a sparse depth input <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>D</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>D</span></span><span><span></span><span><span>~</span></span></span></span></span></span></span></span></span></span></span>.</p></div><h2>360-Degree Scene Results</h2><div><div><div tabindex="0" role="tabpanel" aria-labelledby="tabs-:R3pt9f6:--tab-0" id="tabs-:R3pt9f6:--tabpanel-0"><p><video width="360" height="240" autoplay="" loop="" muted="" playsinline=""><source src="videos/demo_zion_360_rgb.mp4" type="video/mp4"/>Your browser does not support the video tag.</video><video width="360" height="240" autoplay="" loop="" muted="" playsinline=""><source src="videos/demo_zion_360.mp4" type="video/mp4"/>Your browser does not support the video tag.</video></p><p>&#34;a view of Zion National Park&#34;</p></div></div></div><h2>Evaluating Scene Geometry</h2><p>Within the fully generative task of scene generation, evaluating the geometric properties of generated scenes is difficult due to the lack of ground-truth data. Most existing work resorts to image-text similarity scores, which only measures the global semantic alignment of the generation with a text description. To evaluate the geometric consistency and quality of the depth predictions used to build the scene, we propose a new evaluation benchmark. This benchmark quantifies the depth-reconstruction quality on a partial scene with known ground truth depth.</p><div><p><img src="https://research.paulengstler.com/invisible-stitch/images/paper_sce_figure.jpg" alt="Scene evaluation approach figure"/></p><p><a target="_blank" href="https://research.paulengstler.com/invisible-stitch/images/paper_sce_figure.png">View a larger version<!-- --> of this figure<!-- --> <svg viewBox="0 0 24 24" focusable="false"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><path d="M15 3h6v6"></path><path d="M10 14L21 3"></path></g></svg></a></p><p><span>Overview of our scene consistency evaluation approach. </span>Assume a scene is described by a set of views <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mtext> </mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{v_1, v_2, \dots\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>v</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>}</span></span></span></span></span> with associated images, depth maps, and camera poses, where the overlap of two views is described by a function <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(v_i, v_j)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϕ</span><span>(</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span>. For a given view pair <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v_i, v_j)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span> with <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>≥</mo><mi>τ</mi></mrow><annotation encoding="application/x-tex">\phi(v_i, v_j) \geq \tau</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϕ</span><span>(</span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>≥</span><span></span></span><span><span></span><span>τ</span></span></span></span></span>, we generate a representation, e.g., a point cloud, from the ground-truth (GT) data for <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. Then, we render the representation from the view point of <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">v_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. We feed the corresponding ground-truth image and the representation&#39;s depth into the model under consideration to extrapolate the missing depth. Finally, we calculate the mean absolute error between the result and the ground-truth depth for <span data-testid="react-katex"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">v_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>, only considering those regions that were extrapolated.</p></div><p>In both, a real-world and a photorealistic setting, our inpainting model produces predictions that are more faithful to the ground-truth than prior methods.</p><h2>Abstract</h2><p>3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models. Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry.  These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation. These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt. In this work, we make two fundamental contributions to the field of 3D scene generation. First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene. We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene. Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene.</p></div></div>
  </body>
</html>
