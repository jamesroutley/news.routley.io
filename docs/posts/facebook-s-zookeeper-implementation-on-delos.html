<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://engineering.fb.com/2022/06/08/developer-tools/zelos/">Original</a>
    <h1>Facebook&#39;s Zookeeper Implementation on Delos</h1>
    
    <div id="readability-page-1" class="page"><div>

		<p><span>Within large-scale services, durable storage, distributed leases, and coordination primitives such as distributed locks, semaphores, and events should be strongly consistent</span><span>. At Meta, we have historically used Apache ZooKeeper as a centralized service for these primitives</span><span>.</span></p>
<p><span>However, as Meta’s workload has scaled, we’ve found ourselves pushing the limits of ZooKeeper’s capabilities</span><span>. </span><span>Modifying and tuning ZooKeeper for performance has become a significant pain point. ZooKeeper, a single, tightly integrated monolithic system, couples much of the application state with its consensus protocol, ZooKeeper atomic broadcast (ZAB). Consequently, extending ZooKeeper to work better at scale has proved extremely difficult despite several ambitious initiatives, including </span><a href="https://www.facebook.com/watch/?v=2984998221531756&amp;t=1816"><span>native transparent sharding support</span></a><span>, weaker consistency models,</span><a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1416"> <span>persistent watch protocol</span></a><span>, and</span><a href="https://www.facebook.com/watch/?v=2984998221531756&amp;t=1047"> <span>server-side synchronization primitives</span></a><span>. This inability to safely improve and scale compelled us to pose the question:</span></p>
<p><span>Can we construct a more modular, extensible, and performant variant of ZooKeeper?</span></p>
<p><span>This led us to construct ZooKeeper on</span><a href="https://engineering.fb.com/2019/06/06/data-center-engineering/delos/"> <span>Delos</span></a><span>, aka Zelos, which will eventually replace all ZooKeeper clusters in our fleet.</span></p>
<p><span>Delos makes building strongly consistent distributed applications simple by abstracting away many of the challenges in distributed consensus and state management. It also provides a clean log and a database-based abstraction for application development. Furthermore, as Delos cleanly separates application logic from consensus, it naturally allows the system to grow and evolve.</span></p>
<p><span>However, ZooKeeper doesn’t naturally map itself to the Delos abstractions. For instance, ZooKeeper requires the notion of session management, a notion not present in Delos. Additionally, ZooKeeper provides stronger-than-linearizable semantics within a session (and weaker semantics outside of a session). Further complicating things, ZooKeeper has many uses at Meta, so we need a feature-compatible implementation that can support all legacy use cases and transparently migrate from legacy ZooKeeper to our new Delos-based implementation.</span></p>
<p><span>Building Zelos and migrating a complex legacy distributed system like ZooKeeper into our state-of-the-art Delos platform meant we had to solve these challenges.</span></p>
<h2><span>The Delos distributed system platform</span></h2>
<p><img src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?w=1024" alt="Zelos" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png 2560w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-1.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Delos’s goal is to abstract away all the common issues that arise with distributed consensus and provide a simple interface to build distributed applications. This requires Delos applications to worry only about their application logic, and to get Delos’s strong consistency (linearizability) and high availability “for free” when they’re written within the Delos framework. As such, the Delos platform manages numerous problems: state distribution and consensus, failure detection, leader election, distributed state management, ensemble membership management, and recovery from machine faults.</span></p>
<p><span>Delos achieves this by abstracting an application into a finite state machine (FSM) replicated across the nodes in the system, often called replicas. A linearizable distributed shared log maintained by the Delos system reflects the state transitions. The replicas of the ensemble then learn these state machine updates in order and apply the updates to their local storage.</span></p>
<p><span>As these updates are applied deterministically on all replicas, they guarantee consistency across the replicated state machine. To provide linearizability for reads, a replica first syncs its local storage up to the tail of the shared log and then services the read from its local state. In this way, Delos promises linearizable reads and writes without knowledge of the application’s business logic.</span></p>
<p><span>Many applications written on Delos share similar functionality, such as write batching. Delos provides the abstraction of state machine replication engines, or SMREngines. An application selects a stack of these engines based on the features it requires. Each proposal is propagated down the engine stack before it reaches the shared log. The engines may modify the entry as needed for the engine’s logic. Conversely, when a replica learns an entry from the shared log, it gets applied up the stack (in the reverse order of append) so that the engines may transform the entry as needed.</span></p>
<p><span>It’s worth noting that the separation of consensus and business logic gives Delos great power in both expanding its capabilities to meet our scale needs and adapting to future changes in distributed consensus technologies. For instance, Delos can dynamically change its shared log implementation without downtime to the underlying application. So if a newer, faster shared log implementation becomes available, Delos can immediately provide that benefit to all applications with no application involvement.</span></p>
<h2><span>The Delos and ZooKeeper impedance mismatch</span></h2>
<p><span>At a high level, ZooKeeper maintains an application state that roughly parallels a filesystem. Its znodes act as both files and directories, in that they both store data and hold directory-like links to other znodes. They can also be accessed with a filesystem-like path. This portion of ZooKeeper’s logic can be easily transitioned to Delos since it can be directly represented with a distributed state machine. However, many of the behaviors inside of ZooKeeper that are tightly coupled with its consensus protocol fail to map well to Delos. Foremost among them are ZooKeeper’s session primitives.</span></p>
<p><span>In ZooKeeper, all clients initiate communication with the ZooKeeper ensemble through a (global) session. Sessions provide both failure detection and consistency guarantees within the ZooKeeper model. Throughout the lifetime of a session, a client must continuously heartbeat the server to keep its session alive. This allows the server to track live sessions and to keep the session-specific state alive.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?w=1024" alt="Zelos" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png 2560w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-2.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The basis of Delos’s consistency boils down to its linearizable shared log. This guarantees a single linearizable order of all operations, typically sufficient for distributed system needs. However, ZooKeeper provides very strong total ordering within a session and weaker semantics between sessions. As a result, Delos’s linearizable model lacks sufficiency for session operations, as Delos allows proposals to its shared log to be reordered before they reach the log.</span></p>
<p><span>Additionally, in some circumstances, ZooKeeper will offer infallible operations (e.g., if A and B are issued to a session, B cannot logically progress unless A succeeds). Delos allows operations to abort for a wide range of reasons, such as network failures or system reconfigurations. Finally, Delos doesn’t provide any real-time primitives, which are required for session lifetime management and used in ZooKeeper’s leader election and failure detection primitives.</span></p>
<p><span>To achieve our goal of mapping ZooKeeper to Delos in Zelos, we had to overcome significant impedance mismatches in three primary areas:</span></p>
<ol>
<li><span> Sessions and strong ordering within a session</span></li>
<li><span> Session-based leases and real-time contracts</span></li>
<li><span> Transparent migration of ZooKeeper-based applications to our Zelos architecture</span></li>
</ol>
<h2><span>Mapping ZooKeeper sessions to Delos</span></h2>
<h2><span><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png?w=975" alt="" width="975" height="456" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png 975w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png?resize=916,428 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png?resize=768,359 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-3.png?resize=192,90 192w" sizes="(max-width: 992px) 100vw, 62vw"/> </span></h2>
<h3><span>Session as ordering primitive</span></h3>
<p><span>The shared log provided by Delos maintains a linearizable order of log entries once an entry is appended to the log. However, append operations can fail or be reordered while being sent to the log (e.g., from a network issue or a dynamic change of Delos’s configuration). This reordering may break a ZooKeeper session’s total ordering or infallibility properties.</span></p>
<p><span>Naively, we could force a session to issue only one command at a time to provide a totally ordered session, but this would be prohibitively slow. Instead, we see that Delos will only rarely reorder appends. So, for most operations issued by a Zelos node, Delos provides total ordering within a session, and only rarely will it break this guarantee.</span></p>
<p><span>Using this observation, we propose speculative execution. We speculatively send commands ahead to Delos’s shared log, assuming they will not get reordered. In the rare event that a command is reordered, Zelos will detect the reordering when reading from the log and abort the reordered events. It will then reissue the commands pessimistically. This will guarantee session ordering while preserving parallel append dispatch in the common case.</span></p>
<p><span>The SessionOrderingEngine within Delos encapsulates this behavior. It provides an infallible stream over the shared log. It does this by tagging each proposal with its node ID and monotonically increasing proposal ID before sending it to the shared log. Once the shared log replicates the proposal, the replicas will apply it and reject any proposal found out of order. The replica that sent the proposal will also learn about this and mark the session as broken.</span></p>
<p><span>When the session is broken, the SessionOrderingEngine stops issuing new proposals and waits for any previously appended proposals to apply. Once all the in-flight proposals are applied, it resends any it had to abort due to speculative execution.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png?w=975" alt="" width="975" height="596" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png 975w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png?resize=916,560 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png?resize=768,469 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png?resize=96,59 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-image-4.png?resize=192,117 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Up to now, we’ve described how the SessionOrderingEngine orders writes. However, ZooKeeper semantics require both reads and writes to be totally ordered within the session. To order reads within Zelos, we block a session’s read until any preceding write is locally applied. Once the write has completed, any reads immediately following that write may proceed. In ZooKeeper and Zelos, reads are only ordered with respect to their session. </span><span>Consequently, they don’t require a full sync with the shared log but only knowledge of the last write to that session.</span></p>
<p><span>We encapsulate this logic within a Zelos layer known as the RequestProcessor, which intercepts all read and write requests and orders them properly with respect to a session. Using lightweight logical snapshots available in Delos, Zelos can dramatically improve read performance by doing only minimal work in the critical path. The RequestProcessor captures a logical snapshot before executing the write. This allows it to perform all dependent reads out of order.</span></p>
<h3><span>Session as a lease</span></h3>
<p><span>When a client connects to ZooKeeper for the first time, it establishes a season that it keeps alive by sending periodic heartbeats. Each server is responsible for tracking the last heartbeat it received from every client. ZooKeeper’s leader aggregates this information across the ensemble by periodically contacting all servers. This allows the leader to detect any client that has stopped heartbeating and expire the corresponding session. ZooKeeper also allows for the creation of one or more “ephemeral” nodes, which are automatically deleted when the session that created them is closed. Other clients can “watch” for deletion of the ephemeral nodes created by a particular client to learn about its failure. This mechanism underpins various distributed coordination primitives that ZooKeeper offers.</span></p>
<p><span>This protocol allows ZooKeeper to detect client failure as long as the leader itself doesn’t fail. To handle this instance, the set of sessions in the system is replicated using the underlying consensus protocol. Upon leader failure, the system will elect a new leader, which will seamlessly take over session management for the ensemble.</span></p>
<p><span>Zelos is unable to directly adopt ZooKeeper’s session management mechanism because it relies on the concept of an ensemble leader, which is a notion that isn’t present in Delos. In Zelos, clients connect directly to replicas, and replicas send messages directly to the shared log. This led us to develop a two-level session management solution, where a client establishes a session with any replica within the ensemble. Once that session is established, the client will continue to communicate with that replica, and it will track the client’s session locally. We call this functionality the Local Session Manager (LSM). Within the LSM, the replica can track client health through client heartbeating and expire sessions just as the ZooKeeper’s leader implementation would.</span></p>
<p><span>However, this adds a new failure state. If the replica fails, then its LSM also fails, leaving live clients without a session manager. To handle this, we require an additional replicated Global Session Manager (GSM)</span> <span>layer, a distributed state machine present on each replica. We make session creation and expiration replicated via this state machine as they are in ZooKeeper. In addition, GSM tracks the health of our LSMs and their connected replicas. Just as clients heartbeat replicas, LSMs heartbeat through the shared log, and the GSM monitors these heartbeats. In this way, Zelos’s session management is more efficient than ZooKeeper’s — a single distributed LSM heartbeat represents all sessions serviced by the LSM, while a ZooKeeper leader needs to track heartbeat per session. If an LSM fails, the clients will detect this via heartbeats between the client and LSM. A client may then choose a new replica’s LSM, which will take over the ownership of that session. To manage this transfer, we also ensure that any changes to LSM session ownership are replicated via GSM. </span></p>
<p><span>There is one final edge case worth noting: If a client and an LSM fail simultaneously, then the GSM will detect both that the LSM has failed and that the session hasn’t transferred to a new LSM within the session’s preconfigured session timeout. Once these conditions are detected, the GSM will expire the session. In this way, Zelos replicates ZooKeeper’s leader-based session management without relying on consensus-based leader election mechanisms.</span></p>
<p><span>The heartbeat protocol used by the GSM is useful for failure detection, however it depends on the notion of time, a well-known challenge in a distributed state machine. While applying a heartbeat log entry, if a replica were to read the system clock, it would highly likely read a different real-time clock value than other replicas in the system. This could result in a divergence of state, where one replica believes another has failed but the other replicas in the system do not, and could result in undesired behavior.</span></p>
<p><span>To resolve this, GSM uses a distributed time protocol that approximates real time. We call this protocol TimeKeeper. In building TimeKeeper, we didn’t want to manage the task of having to closely synchronize system clocks (e.g., with a TrueTime-like protocol). Instead, we developed a simpler protocol that allows arbitrary clock skew (deviation from real time) but assumes clock drift to be small over the time interval comparable to maximum session timeout. </span></p>
<p><span>The goal of TimeKeeper is to track LSM failures, which in Zelos is too large of a time duration without an LSM heartbeat. To facilitate this, TimeKeeper leverages Delos’s shared log. Each LSM heartbeat is part of a log message and therefore associated with a log position. TimeKeeper protocol measures elapsed time since the last LSM heartbeat by associating a “tick” count between each heartbeating LSM and each TimeKeeper. If this tick count ever surpasses the LSM timeout time, it is declared dead. To associate this tick count, each TimeKeeper replica sends a tick message with its last learned log position at some defined interval. The TimeKeeper protocol tracks both the latest received heartbeat, and the number of tick counts from each TimeKeeper since that heartbeat. If any of the TimeKeeper nodes has ever sent more than the allowed number of ticks (based on the tick interval and timeout duration), then the LSM is considered as failed.</span></p>
<p><span>Since building a GSM</span> <span>encapsulates generally useful but nontrivial logic, we have built it as an SMREngine within Delos that can be used by any other Delos application that needs a highly scalable distributed leasing system. </span></p>
<p><span>Despite the complexity of distributed session management, by leveraging Delos’s consensus and the engine stack framework, our actual production-proven implementation is only a few hundred lines of code.</span></p>
<h3><span>Transparent migration of workloads to Zelos</span><span> </span></h3>
<p><span>There are infrastructure systems at Meta that use ZooKeeper that would cause outages if they were to suffer any significant downtime. This means that in order to use Zelos in production, we needed a migration plan with no client-observable downtime.</span></p>
<p><span>To facilitate this, we leveraged the semantics of ZooKeeper, which allows a session to disconnect for a variety of reasons, such as leader election. By spoofing a ZooKeeper leader election to the client, we could disconnect from a ZooKeeper ensemble and reconnect to a Zelos replica without violating any client-visible ZooKeeper semantics.</span></p>
<p><span>However, for this to work, we must guarantee that at the time of transition, we atomically transition all clients and that the Zelos ensemble the client will connect to has the exact same client-visible state as the ZooKeeper ensemble that it’s replacing.</span><span> </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?w=1024" alt="Zelos" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png 2560w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-5.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/> <img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?w=1024" alt="Zelos" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png 2560w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2022/06/Zelos-6.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>To atomically manage this transition of state, we set up the new Zelos ensemble as a follower of the ZooKeeper ensemble. While in follower mode, the Zelos ensemble does not service a Delos workload but instead observes ZooKeeper’s ZAB-replicated consensus protocol and applies all the ZooKeeper state updates directly to the Zelos on-disk state.</span></p>
<p><span>We then define a new, special node in the ZooKeeper tree that we call the barrier node. We based the atomic switch and the illusion we create for the client on this barrier node. The barrier node serves two purposes. First, it notifies the Zelos ensemble that it’s time to transition and converts the ensemble from a follower of ZooKeeper to the leader of its own consensus protocol. </span></p>
<p><span>Second, the creation of the barrier node is a trigger to transfer our client. We created a new client that wrapped the ZooKeeper client and the Zelos client and knew what the barrier node meant. On startup, it connects to ZooKeeper and creates a watch on the existence of this barrier node. Once the barrier node is observed by the client, it disconnects from the ZooKeeper ensemble, pretending a “leader election” is occurring. It then connects to the Zelos ensemble and confirms that the ensemble has observed the creation of the barrier node. If the Zelos ensemble has observed the barrier node, its state is at least as up to date as the ZooKeeper ensemble at the time of migration, and can serve the same traffic the ZooKeeper ensemble would.</span></p>
<p><span>We have used this process to successfully migrate 50 percent of our ZooKeeper workloads to Zelos with zero downtime across various use cases. </span></p>
<h2><span>Next steps for Zelos </span></h2>
<p><span>Building a ZooKeeper API using Delos gives us the flexibility to change the semantics of ZooKeeper without doing a major redesign. For example, while migrating use cases, we realized that most of them really do not care about the ordering semantics provided by the session. For such use cases, we could significantly improve the throughput of the ensemble. We did so by removing the session-ordered engine from the stack and highly simplifying the request processor.</span></p>
<p><span>Many of our internal teams that use ZooKeeper have grown out of a single ensemble and have had to split their use case into multiple ensembles and maintain their own mapping of ensembles. We plan to extend the Delos platform to provide building blocks for sharding, allowing a single, logical Zelos namespace to map to many physical ensembles. This mechanism will be common across all Delos-powered apps, so Zelos can take advantage of it out of the box. While this work will be challenging, we are most excited about the potential here. </span></p>

		
	</div></div>
  </body>
</html>
