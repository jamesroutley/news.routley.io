<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://suhith.mataroa.blog/blog/intuition-behind-the-cuda-implementation-of-a-blelloch-scan/">Original</a>
    <h1>CUDA Blelloch Scan Intuition</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
            <p>This is a brief summary of a Blelloch scan along with my understanding of how to code the trickiest elements of it.</p>
<p><em>If there are any issues or mistakes do let me know, I&#39;m still learning so it&#39;s entirely possible! You can open an issue <a href="https://github.com/suhithr/cuda-parallel-prefix-sum/issues">here</a>.</em></p>
<p>It helps if you have a basic understanding of the CUDA programming model along with what threads, blocks, and shared memory are. If you don&#39;t <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">NVIDIA&#39;s CUDA Programming Guide</a> could be a good starting point.</p>
<p>A Blelloch scan is a parallel algorithm that constructs an exclusive prefix sum of an array.
<img alt="exclusive_prefix_sum_diagram.png" src="https://suhith.mataroa.blog/images/eb1db120.png"/></p>
<p>The algorithm describes a way to do this and make use of the highly parallel execution environment offered by a GPU.</p>
<p>It consists of 2 stages, an <em>upsweep</em> and a <em>downsweep</em>.</p>
<p><img alt="39fig03.jpg" src="https://suhith.mataroa.blog/images/b66ae3af.jpeg"/>
<em>Upsweep Phase - Image Credit <a href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">1</a></em></p>
<p>In the upsweep stage we construct a binary tree of the input elements adding pairs together to determine the sum of all values in the array.</p>
<p><img alt="39fig04.jpg" src="https://suhith.mataroa.blog/images/f2679bd2.jpeg"/>
<em>Downsweep Phase - Image Credit <a href="https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda">2</a></em></p>
<p>In the downsweep phase we construct the tree in reverse. We first replace the &#34;root&#34; of the tree with 0.
Then for each level of the tree a node passes it&#39;s element to it&#39;s left child and accumulates the previous value of the left element &amp; it&#39;s right element in it&#39;s right child.</p>
<p>For me, once I understood this, translating this into a CUDA kernel was another story altogether. So here we are, where I&#39;d like to share some of the code &amp; intuition behind it. </p>
<h2 id="upsweep-intuition">Upsweep Intuition</h2>
<h3 id="so-how-many-threads-do-we-need">So how many threads do we need?</h3>
<p>Looking at the bottom/widest portion of the tree we need <code>num_elements / 2</code> threads at most, and this reduces by half every iteration until the top of the tree is created with a single thread.</p>
<h3 id="how-do-we-map-threads-to-a-pair-of-elements">How do we map threads to a pair of elements?</h3>
<p><em>First the elements start out just 1 index apart, then 2, then 4, then 8, and so on.</em></p>
<p>Additionally we want to map a <strong>continuous</strong> set of threads (denoted by their indices) to a <strong>discontinuous</strong> set of elements which are stride width apart.</p>
<p><img alt="threads to index mapping iteration 1.png" src="https://suhith.mataroa.blog/images/9535b7c3.png"/>
<em>First iteration</em></p>
<p><img alt="threads to index mapping iteration 2.png" src="https://suhith.mataroa.blog/images/6160f722.png"/>
<em>Second iteration</em></p>
<p>We can multiply <code>2 * local_tid</code> to skip over all the pairs of element that the <code>thread_idx &lt; local_tid</code> have already claimed. This offset is multiplied by <code>stride</code> to take into account that for each iteration, the threads are claiming a section of the input <code>stride</code> elements apart, so skipping over <code>2 * local_tid</code> isn&#39;t sufficient. Thus for each of these we need to skip over <code>stride * 2 * local_tid</code>.</p>
<p>We can use <code>left = ((2 * local_tid + 1)*stride ) - 1</code> </p>
<p>and <code>right = left + stride = ((2 * local_tid + 2)*stride) - 1 = ((2 * local_tid + 1 + 1)*stride) - 1</code>.</p>
<p>The <code>-1</code> is because these calculations need to fit an array that is <code>0-indexed</code> and all of these offsets produce pairs that are 1 index off, even when the <code>thread_id</code> used is 0 indexed.</p>
<p>The code of the upsweep might look something like this:</p>
<div><pre><span></span><span>    </span><span>int</span><span> local_tid </span><span>=</span><span> threadIdx.x;</span>
<span>    </span><span>int</span><span> max_elems </span><span>=</span><span> blockDim.x;</span>
<span>    </span><span>int</span><span> stride </span><span>=</span><span> </span><span>1</span><span>; </span><span>// how far apart a pair of elements is</span>

<span>    </span><span>// d is really the number of threads that will be active in each stage</span>
<span>    </span><span>for</span><span> (</span><span>int</span><span> d </span><span>=</span><span> max_elems </span><span>&gt;&gt;</span><span> </span><span>1</span><span>; d </span><span>&gt;</span><span> </span><span>0</span><span>; d</span><span>&gt;&gt;=</span><span> </span><span>1</span><span>, stride </span><span>&lt;&lt;=</span><span> </span><span>1</span><span>)</span>
<span>    {</span>
<span>        </span><span>if</span><span> (local_tid </span><span>&lt;</span><span> d)</span>
<span>        {</span>
<span>            </span><span>/* map our threads to a pair of elements */</span>
<span>            </span><span>int</span><span> left </span><span>=</span><span> stride </span><span>*</span><span> (</span><span>2</span><span> </span><span>*</span><span> local_tid </span><span>+</span><span> </span><span>1</span><span>) </span><span>-</span><span> </span><span>1</span><span>;</span>
<span>            </span><span>int</span><span> right </span><span>=</span><span> left </span><span>+</span><span> stride;</span>
<span>            sharedMem[right] </span><span>+=</span><span> sharedMem[left]</span>
<span>        }</span>
<span>        __syncthreads();</span>
<span>    }</span>
</pre></div>

<h2 id="downsweep-intution">Downsweep Intution</h2>
<h3 id="how-many-threads-do-we-need">How many threads do we need?</h3>
<p>Starting with a single thread, we double the number of threads until we reach <code>maxiumum_elements/2</code>. We can use a similar loop as we used in the upsweep, just start from <code>d = 1</code> and double it until we reach <code>max_elems / 2</code>.</p>
<h3 id="how-do-we-map-these-elements">How do we map these elements?</h3>
<p>Again we can take inspiration from the tree that we build, that grew upwards. Here we just grow in the opposite direction. So we can use the exact same offset calculation, but the difference is that we start from a stride value of <code>max_elems / 2</code>. This may not be obvious but we can make use of the fact that we are <strong>interacting with the exact same elements as the upsweep</strong>. Just in a reversed order.</p>
<div><pre><span></span><span>    stride </span><span>=</span><span> max_elems </span><span>&gt;&gt;</span><span> </span><span>1</span><span>;</span>
<span>    </span><span>for</span><span> (</span><span>int</span><span> d </span><span>=</span><span> </span><span>1</span><span>; d </span><span>&lt;</span><span> </span><span>=</span><span>max_elems </span><span>&lt;&lt;</span><span> </span><span>1</span><span>; d </span><span>&lt;&lt;=</span><span> </span><span>1</span><span>, stride </span><span>&gt;&gt;=</span><span> </span><span>1</span><span>)</span>
<span>    {</span>
<span>        </span><span>if</span><span> (local_tid </span><span>&lt;</span><span> d)</span>
<span>        {</span>
<span>            </span><span>int</span><span> left </span><span>=</span><span> stride </span><span>*</span><span> (</span><span>2</span><span> </span><span>*</span><span> local_tid </span><span>+</span><span> </span><span>1</span><span>) </span><span>-</span><span> </span><span>1</span><span>;</span>
<span>            </span><span>int</span><span> right </span><span>=</span><span> left </span><span>+</span><span> stride;</span>

<span>            </span><span>int</span><span> temp </span><span>=</span><span> sharedMem[left];</span>
<span>            sharedMem[left] </span><span>=</span><span> sharedMem[right];</span>
<span>            sharedMem[right] </span><span>=</span><span> temp </span><span>+</span><span> sharedMem[right];</span>

<span>        }</span>
<span>        __syncthreads();</span>
<span>    }</span>
</pre></div>

<h2 id="some-questions-i-had-about-edge-cases">Some questions I had about edge cases:</h2>
<p>Whenever I read about an algorithm like this I&#39;m not convinced that it works unless I think through some weird edge cases to see for myself that it&#39;s robust. Here are a couple that worried me when I was writing it.</p>

<p><em>Here we assume we are using a shared memory space equal to the number of elements.</em> We actually cannot, because when we test the extreme values of the algorithm (with the guards in place, ie: local_tid &lt; d) then we only ever index into correct values.</p>
<h3 id="what-happens-if-our-array-is-smaller-than-the-size-of-the-block">What happens if our array is smaller than the size of the block?</h3>
<p>By filling the remaining elements in shared memory with zeros, our algorithm is unaffected. Look here to see the a diagram demonstrating this on the upsweep.</p>
<p><img alt="if_array_smaller_than_block.png" src="https://suhith.mataroa.blog/images/27ec3ed4.png"/></p>
<p>You can find my repository implementing the entire Blelloch Scan <a href="https://github.com/suhithr/cuda-parallel-prefix-sum">here</a>.</p>
        </div></div>
  </body>
</html>
