<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ardentperf.com/2021/07/06/paranoid-sql-execution-on-postgresql/">Original</a>
    <h1>Paranoid SQL Execution on Postgres</h1>
    
    <div id="readability-page-1" class="page"><div>
				
<p>Suppose that you want to be completely over-the-top paranoid about making sure that when you execute some particular SQL statement on your Postgres database, you’re doing it in the safest and least risky way?</p>



<p>For example, suppose it’s the production database behind your successful startup’s main commercial website. If anything even causes queries to block/pause for a few minutes then people will quickly be tweeting about how they can’t place orders and it hurt both your company’s revenue and reputation.</p>



<p>You know that it’s really important to save regular snapshots and keep a history of important metrics. You’re writing some of your own code to capture a few specific stats like the physical size of your most important application tables or maybe the number of outstanding orders over time. Maybe you’re writing some java code that gets scheduled by quartz, or maybe some python code that you’ll run with cron.</p>



<p>Or another situation might be that you’re planning to make an update to your schema – adding a new table, adding a new column to an existing table, modifying a constraint, etc.  You plan to execute this change as an online operation during the weekly period of lowest activity on the system – maybe it’s very late Monday night, if you’re in an industry that’s busiest over weekends.</p>



<p>How can you make sure that your SQL is executed on the database in the safest possible way?</p>



<p>Here are a few ideas I’ve come up with:</p>



<ul><li>Setting <code><strong>connect_timeout</strong></code> to something short, for example 2 seconds.</li><li>Setting <code><strong>lock_timeout</strong></code> to something appropriate. For example, 2ms on queries that shouldn’t be doing any locking. <em>(I’ve seen entire systems brown-out because a “quick” DDL had to get in line behind an app transaction, and then all the new app transactions piled up behind the DDL that was waiting!)</em></li><li>Setting <code><strong>statement_timeout</strong></code> to something reasonable for the query you’re running – thus putting an upper bound on execution time.</li><li>Using an appropriate <strong>client-side timeout</strong>, for cases when the server fails to kill the query using <code>statement_timeout</code>.  For example, in Java the Statement class has native support for this.</li><li>When writing SQL, <strong>fully qualify names</strong> of tables and functions with the schema/namespace.  <em>(This can be a security feature; I have heard of attacks where someone manages to change the search_path for connections.)</em></li><li>Check at least one <strong>explain plan</strong> and make sure it’s doing what you would expect it to be doing, and that it seems likely to be the most efficient way to get the information you need.</li><li>Don’t use system views that join in unneeded data sources; go direct to needed <strong>raw relation or a raw function</strong>.</li><li>Access <strong>each data source exactly once</strong>, never more than once. In that single pass, get all data that will be needed. Analytic or window functions are very useful for avoiding self-joins.</li><li>Restrict the user to <strong>minimum needed privileges</strong>. For example, the <code>pg_read_all_stats</code> role on an otherwise unprivileged user might be useful.</li><li>Make sure your code has <strong>back-off logic</strong> for retries when failures or unexpected results are encountered.</li><li><strong>Prevent connection pile-ups</strong> resulting from database slowdowns or hangs. For example, by using a dedicated client-side connection pool with dynamic sizing entirely disabled or with a small max pool size.</li><li>Run the query against a <strong>physical replica/hot standby</strong> (e.g. pulling a metric for the physical size of important tables) <strong>or logical copy</strong> (e.g. any query against application data), instead of running the query against the primary production database.  <em>(However, note that when <code>hot_standby_feedback</code> is enabled, long-running transactions on the PostgreSQL hot standby can still impact the primary system.)</em></li><li>For all DDL, carefully <strong>check the level of locking</strong> that it will require and test to get a feel for possible execution time. Watch out for table rewrites. Many DDLs that used to require a rewrite no longer do in current versions of PostgreSQL, but there are still a few out there. ALTER TABLE statements must be evaluated very carefully. Frankly ALTER TABLE is a bit notorious for being unclear about which incantations cause table rewrites and which ones don’t. <em>(I have a friend who just tests every specific ALTER TABLE operation first on an empty table and then checks if pg_class changes show that a rewrite happened.)</em></li></ul>







<p>What am I missing?  What other ideas are out there for executing SQL in Postgres with a “paranoid” level of safety?</p>



<hr/>



<p><em>Note: see also <a href="https://ardentperf.com/2018/11/08/column-and-table-redefinition-with-minimal-locking/">Column And Table Redefinition With Minimal Locking</a></em></p>

				

				
							</div><div id="entry-author-info">
				<p><img alt="" src="https://2.gravatar.com/avatar/5bc73f6d08274ed75832c381e74c49c9?s=60&amp;d=https%3A%2F%2F2.gravatar.com%2Favatar%2Fad516503a11cd5ca435acc9bb6523536%3Fs%3D60&amp;r=G" height="60" width="60"/>				</p><!-- #author-avatar -->
				<!-- #author-description -->
			</div></div>
  </body>
</html>
