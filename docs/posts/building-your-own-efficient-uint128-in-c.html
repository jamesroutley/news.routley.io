<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://solidean.com/blog/2026/building-your-own-u128/">Original</a>
    <h1>Building Your Own Efficient uint128 in C&#43;&#43;</h1>
    
    <div id="readability-page-1" class="page"><div>
        
<section>
    <div>
        <article>
            

            <div>
                <div>
    <p><strong>TL;DR:</strong></p><div>
        <p>We build a minimal <code>u128</code> as two <code>u64</code> limbs and implement arithmetic using carry, borrow, and multiply intrinsics that map directly to x64 instructions.
The generated code is on par with builtin <code>__uint128_t</code> for addition, subtraction, multiplication, and comparison.
This is unsigned-only, x64-focused, and intentionally narrow in scope.
The result is a solid foundation for exact, fixed-width arithmetic with a focus on good codegen and predictability, not abstraction.</p>
<p>Full code and compiler output: <a href="https://godbolt.org/z/K6dn3s91Y">https://godbolt.org/z/K6dn3s91Y</a></p>

    </div>
</div><h2 id="scope">Scope</h2>
<p>We take the smallest reasonable definition of a 128-bit integer, two 64-bit words, and turn it into a usable arithmetic type whose generated code is indistinguishable from a builtin <code>__uint128_t</code>.</p>
<p>This post is explicitly not about dynamically-sized big integer arithmetic.
It is about being explicit with range bounds and letting the compiler emit the exact instructions we want.
The scope is deliberately limited: unsigned arithmetic, fixed width, modern x64, with Clang and GCC as the primary targets and notes for MSVC where it differs.</p>
<h2 id="why-fixed-width-big-integers">Why fixed-width big integers</h2>
<p>In many domains, especially geometry and numerics, we do not need arbitrary precision.
We need enough precision to be exact for known bounds, and we need the cost to be predictable.</p>
<p>Dynamic big integer libraries solve a different problem.
They are flexible and general, but they pay for that generality in memory traffic, branches, and indirection.
If your values fit into a fixed number of bits and you know that ahead of time, fixed-width arithmetic is usually the better trade.
(In fact, our high-performance exact mesh booleans are completely built on this: <a href="https://solidean.com/docs/concepts/exact-arithmetic/">Exact Arithmetic in Solidean</a>)</p>
<p>A 128-bit integer is the gateway drug to fixed-width arithmetic.
It is the smallest width that is no longer builtin, while still mapping cleanly to the underlying hardware.
Once the carry and multiply patterns are explicit at 128 bits, extending them to 192 or 256 bits is straightforward.
In production, we use 256-bit integers in our hot paths and go up to 564 bits for certain edge cases.</p>
<h2 id="representation">Representation</h2>
<p>We represent a 128-bit unsigned integer as two 64-bit limbs.
You can literally think of this as writing the <code>u128</code> as a 2-digit number in base \(2^{64}\).
Because it&#39;s unsigned, we don&#39;t need to think about two&#39;s complement.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>#include </span><span>&lt;cstdint&gt;
</span><span>#include </span><span>&lt;immintrin.h&gt; </span><span>// _addcarry_u64, _subborrow_u64, _mulx_u64
</span><span>
</span><span>// this is the type used in the intrinsics signature
</span><span>// (and uint64_t is unsigned long, not unsigned long long...)
</span><span>using </span><span>u64 </span><span>= </span><span>unsigned long long</span><span>;
</span><span>
</span><span>struct </span><span>u128
</span><span>{
</span><span>    u64 low </span><span>= </span><span>0</span><span>;
</span><span>    u64 high </span><span>= </span><span>0</span><span>;
</span><span>}</span><span>;
</span></code></pre>
<h2 id="addition-with-carry">Addition, with carry</h2>
<p>We start off easy.
Addition is simply done using long addition on our base \(2^{64}\) digits.
The intrinsic <code>_addcarry_u64</code> corresponds to the x64 instruction <code>adc</code> and is exactly what we need:
Given two <code>u64</code> summands and an input carry (0 or 1), we get the <code>u64</code> result (via a slightly cumbersome output parameter) and a new carry.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator+</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 r</span><span>;
</span><span>    </span><span>unsigned char</span><span> c </span><span>= </span><span>_addcarry_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span>  b.low</span><span>,  &amp;</span><span>r.low)</span><span>;
</span><span>    (</span><span>void</span><span>)</span><span>_addcarry_u64</span><span>(c</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>r.high)</span><span>;
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>The generated assembly is exactly what you would write by hand.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>+</span><span>(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>add     </span><span>rax</span><span>, </span><span>rdx
</span><span>        </span><span>adc     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>ret
</span></code></pre>
<p>The moves are just calling convention noise.
The core is an <code>add</code> (because the first addition has no input carry) followed by an <code>adc</code>.
This is identical to what the compiler emits for <code>__uint128_t</code> addition.</p>
<p>A small but important point is that intrinsics are preferable to inline assembly here.
They act like specialized IR operations.
The compiler understands their semantics, can schedule around them, and can still optimize aggressively.
Inline assembly is more of a black box and much easier to get wrong or inhibit optimizations.</p>
<h2 id="subtraction-same-story-inverted">Subtraction: same story, inverted</h2>
<p>Subtraction mirrors addition almost perfectly.
Instead of a carry, we track a borrow.
On x64, this is <code>sbb</code>, subtract with borrow.
The corresponding intrinsic is <code>_subborrow_u64</code>.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator-</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 r</span><span>;
</span><span>    </span><span>unsigned char</span><span> c </span><span>= </span><span>_subborrow_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span>  b.low</span><span>,  &amp;</span><span>r.low)</span><span>;
</span><span>    (</span><span>void</span><span>)</span><span>_subborrow_u64</span><span>(c</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>r.high)</span><span>;
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>And again, the assembly is exactly what we want.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>-</span><span>(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>sub     </span><span>rax</span><span>, </span><span>rdx
</span><span>        </span><span>sbb     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>ret
</span></code></pre>
<p>At this point, addition and subtraction are basically solved.
There is no hidden cost and no abstraction penalty.
It&#39;s also easy to see how this scales to larger integer types with one extra instruction per <code>u64</code> &#34;digit&#34;.</p>
<h2 id="multiplication-regrouping-our-u64-digits">Multiplication: regrouping our <code>u64</code> digits</h2>
<p>Multiplication is where things get more interesting.
A 128-bit by 128-bit multiply produces a 256-bit result but we want only the lower 128 bit for our result.
Same story with <code>u64 * u64</code> really, which produces a 128 bit result in theory, but you usually only use the lower <code>u64</code>.
Speaking of which, all modern 64-bit architectures give you access to fast <code>u64 * u64 -&gt; u128</code> instructions.
With BMI2, this is exposed as <code>_mulx_u64</code>.
On MSVC, the equivalent is <code>_umul128</code>.
This is our building block for large multiplication.</p>
<p>You can derive the code from writing the <code>u128 * u128</code> as 2-digit long multiplication <code>(u64, u64) * (u64, u64)</code> and then look sharply at what sums up to which digit.</p>
<p>That&#39;s how I do it on paper, but here we can also choose an algebraic route.
Write the numbers as:
$$
(a.\text{low} + 2^{64} \cdot a.\text{high}) \cdot (b.\text{low} + 2^{64} \cdot b.\text{high})
$$
Expanding this gives four terms. Of those, only three contribute to the low 128 bits:</p>
<ul>
<li>\(a_\text{low} \cdot b_\text{low}\) contributes both low and high parts</li>
<li>\(a_\text{low} \cdot b_\text{high}\) contributes to bits 64..127</li>
<li>\(a_\text{high} \cdot b_\text{low}\) contributes to bits 64..127</li>
<li>\(a_\text{high} \cdot b_\text{high}\) contributes only above bit 128 and can be discarded</li>
</ul>
<p>This works for larger integers as well, though summing up intermediate terms can produce carries for &#34;higher digits&#34;.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>operator*</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    </span><span>// we want the low 128 bits of:
</span><span>    </span><span>// (a.low + 2^64 a.high) * (b.low + 2^64 b.high)
</span><span>    </span><span>//
</span><span>    </span><span>// r.low  = lo64(a.low * b.low)
</span><span>    </span><span>// r.high = hi64(a.low * b.low)
</span><span>    </span><span>//        + lo64(a.low * b.high)
</span><span>    </span><span>//        + lo64(a.high * b.low)        (mod 2^64)
</span><span>
</span><span>    </span><span>// NOTE (MSVC): for multiply, you can use _umul128(a, b, &amp;hi) instead of _mulx_u64.
</span><span>    </span><span>//              Clang/GCC: _mulx_u64 is BMI2 and needs -mbmi2.
</span><span>
</span><span>    u128 r</span><span>;
</span><span>
</span><span>    u64 p0_hi</span><span>;
</span><span>    r.low </span><span>= </span><span>_mulx_u64</span><span>(a.low</span><span>,</span><span> b.low</span><span>, &amp;</span><span>p0_hi)</span><span>;
</span><span>
</span><span>    </span><span>// cross terms: only the low 64 bits contribute to r.high
</span><span>    u64 t1_hi</span><span>;
</span><span>    u64 t1_lo </span><span>= </span><span>_mulx_u64</span><span>(a.low</span><span>,</span><span>  b.high</span><span>, &amp;</span><span>t1_hi)</span><span>;
</span><span>
</span><span>    u64 t2_hi</span><span>;
</span><span>    u64 t2_lo </span><span>= </span><span>_mulx_u64</span><span>(a.high</span><span>,</span><span> b.low</span><span>,  &amp;</span><span>t2_hi)</span><span>;
</span><span>
</span><span>    </span><span>// simply add is sufficient: carries would land in bit 128 and are discarded
</span><span>    r.high </span><span>=</span><span> p0_hi </span><span>+</span><span> t1_lo </span><span>+</span><span> t2_lo</span><span>;
</span><span>
</span><span>    </span><span>return</span><span> r</span><span>;
</span><span>}
</span></code></pre>
<p>Note that we do not need carry handling there.
Any carry out of bit 127 would land in bit 128, which we are discarding anyway.</p>
<p>The compiler output reflects this reasoning.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator</span><span>*</span><span>(u128</span><span>, </span><span>u128):
</span><span>        mulx    </span><span>r8</span><span>, </span><span>rax</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rcx</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>r8
</span><span>        </span><span>ret
</span></code></pre>
<p>The compiler chooses slightly different instructions and registers for the builtin <code>__uint128_t</code> multiplication but is otherwise identical as well.</p>
<h2 id="equality">Equality</h2>
<p>Now an easy one.
<code>u128</code> equality is simple structural equality.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator==</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    </span><span>return</span><span> a.low </span><span>==</span><span> b.low </span><span>&amp;&amp;</span><span> a.high </span><span>==</span><span> b.high</span><span>;
</span><span>}
</span></code></pre>
<p>The generated assembly is worth a quick look.</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator==(u128</span><span>, </span><span>u128):
</span><span>        </span><span>xor     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>xor     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>or      </span><span>rsi</span><span>, </span><span>rdi
</span><span>        </span><span>sete    </span><span>al
</span><span>        </span><span>ret
</span></code></pre>
<p>Instead of branching (as you might expect from the short-circuiting of <code>&amp;&amp;</code>), the compiler XORs the corresponding limbs.
XOR produces zero if and only if the inputs are equal.
ORing the results combines the checks.
If the final value is zero, both limbs were equal.</p>
<p>This pattern continues for larger integers, though we might see branching due to short-circuiting at some point.
(We could of course just use the XOR approach in <code>operator==</code> but it is distinctly less readable.)</p>
<h2 id="comparison-borrow-beats-branching">Comparison: borrow beats branching</h2>
<p>The straightforward way to compare two 128-bit integers is to compare the high parts first and then the low parts.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator&lt;</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>)
</span><span>{
</span><span>    </span><span>if </span><span>(a.high </span><span>!=</span><span> b.high) </span><span>return</span><span> a.high </span><span>&lt;</span><span> b.high</span><span>;
</span><span>    </span><span>return</span><span> a.low </span><span>&lt;</span><span> b.low</span><span>;
</span><span>}
</span></code></pre>
<p>This is correct, but the codegen is not great:</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator&lt;(u128</span><span>, </span><span>u128):
</span><span>        </span><span>xor     </span><span>r8d</span><span>, </span><span>r8d
</span><span>        </span><span>cmp     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>setb    </span><span>r8b
</span><span>        </span><span>xor     </span><span>eax</span><span>, </span><span>eax
</span><span>        </span><span>cmp     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>setb    </span><span>al
</span><span>        </span><span>cmove   </span><span>eax</span><span>, </span><span>r8d
</span><span>        </span><span>ret
</span></code></pre>
<p>Works, but heavier than necessary.
We can do better by leaning on the hardware borrow flag.</p>
<p>Unsigned comparison <code>a &lt; b</code> is equivalent to checking whether <code>a - b</code> produces a borrow:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>operator&lt;</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u64 dont_care</span><span>;
</span><span>
</span><span>    </span><span>// compute borrow from (a.low - b.low). If a.low &lt; b.low =&gt; borrow = 1.
</span><span>    </span><span>unsigned char</span><span> borrow </span><span>= </span><span>_subborrow_u64</span><span>(</span><span>0</span><span>,</span><span> a.low</span><span>,</span><span> b.low</span><span>, &amp;</span><span>dont_care)</span><span>;
</span><span>
</span><span>    </span><span>// now subtract highs with that borrow
</span><span>    </span><span>// final borrow tells us if a &lt; b in 128-bit unsigned.
</span><span>    borrow </span><span>= </span><span>_subborrow_u64</span><span>(borrow</span><span>,</span><span> a.high</span><span>,</span><span> b.high</span><span>, &amp;</span><span>dont_care)</span><span>;
</span><span>
</span><span>    </span><span>return</span><span> borrow </span><span>!= </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>The resulting assembly is minimal:</p>
<pre data-lang="asm"><code data-lang="asm"><span>operator&lt;(u128</span><span>, </span><span>u128):
</span><span>        </span><span>cmp     </span><span>rdi</span><span>, </span><span>rdx
</span><span>        </span><span>sbb     </span><span>rsi</span><span>, </span><span>rcx
</span><span>        </span><span>setb    </span><span>al
</span><span>        </span><span>ret
</span></code></pre>
<p>One compare, one subtract with borrow, and a flag check.
This is exactly the kind of codegen we want in hot code.</p>
<h2 id="a-small-use-site">A small use site</h2>
<p>To make sure everything composes properly, let&#39;s build a slightly larger function.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>u128 </span><span>demo_u128</span><span>(u128 </span><span>a,</span><span> u128 </span><span>b</span><span>) 
</span><span>{
</span><span>    u128 x </span><span>=</span><span> a </span><span>+</span><span> b</span><span>;
</span><span>    u128 y </span><span>=</span><span> a </span><span>*</span><span> b</span><span>;
</span><span>    </span><span>return</span><span> x </span><span>&lt;</span><span> y
</span><span>            </span><span>?</span><span> y </span><span>-</span><span> x
</span><span>            </span><span>:</span><span> x </span><span>-</span><span> y</span><span>;
</span><span>}
</span></code></pre>
<p>All operators inline cleanly:</p>
<pre data-lang="asm"><code data-lang="asm"><span>demo_u128(u128</span><span>, </span><span>u128):
</span><span>        </span><span>mov     </span><span>r8</span><span>, </span><span>rdi
</span><span>        </span><span>add     </span><span>r8</span><span>, </span><span>rdx
</span><span>        </span><span>mov     </span><span>r9</span><span>, </span><span>rsi
</span><span>        </span><span>adc     </span><span>r9</span><span>, </span><span>rcx
</span><span>        mulx    </span><span>rax</span><span>, </span><span>r10</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rcx</span><span>, </span><span>rdi
</span><span>        </span><span>imul    </span><span>rdx</span><span>, </span><span>rsi
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>add     </span><span>rdx</span><span>, </span><span>rax
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>r8
</span><span>        </span><span>sub     </span><span>rax</span><span>, </span><span>r10
</span><span>        </span><span>mov     </span><span>rcx</span><span>, </span><span>r9
</span><span>        </span><span>sbb     </span><span>rcx</span><span>, </span><span>rdx
</span><span>        </span><span>jae     </span><span>.LBB13_2
</span><span>        </span><span>sub     </span><span>r10</span><span>, </span><span>r8
</span><span>        </span><span>sbb     </span><span>rdx</span><span>, </span><span>r9
</span><span>        </span><span>mov     </span><span>rax</span><span>, </span><span>r10
</span><span>        </span><span>mov     </span><span>rcx</span><span>, </span><span>rdx
</span><span>.LBB13_2:
</span><span>        </span><span>mov     </span><span>rdx</span><span>, </span><span>rcx
</span><span>        </span><span>ret
</span></code></pre>
<p>There is a branch for the ternary operator while the builtin version uses conditional moves instead.
Which is better depends on data patterns but could go either way.
I consider this basically as good as it gets.</p>
<h2 id="platform-notes">Platform notes</h2>
<p>The examples shown are for x64 with Clang or GCC.</p>
<p>On MSVC, <code>_addcarry_u64</code> and <code>_subborrow_u64</code> work the same way.
For multiplication, <code>_umul128</code> replaces <code>_mulx_u64</code>.</p>
<p>On AArch64, the same approach applies using <code>adds</code> and <code>adcs</code> instructions for addition, <code>subs</code> and <code>sbcs</code> for subtraction, and <code>mul + umulh</code> for the low + high half of a 64-bit multiply.
The patterns carry over directly, even though the intrinsics differ slightly (and multiplication is split into two parts).</p>
<h2 id="outlook">Outlook</h2>
<p>This <code>u128</code> is the easiest large integer you can write.
Our goal is best performance, so we made sure that codegen is reasonably identical to the builtin <code>__uint128_t</code>.</p>
<p>From here, the same patterns extend naturally to signed variants, widening multiplies such as u128 times u128 to u256, and chains of fixed-width integers like i192 or i256.
More importantly, the same reasoning applies when you design predicate-specific arithmetic that only computes what is actually needed.</p>
<p>A lot of our performance really boils down to these types:
No <code>BigInteger</code> tax, no floating predicates (that need a few kB of stack space for the worst case).
Just a lot of straightline integer code and some light static branching.</p>
<h2 id="addendum-2026-01-24">Addendum 2026-01-24</h2>
<p>Some notes based on reader feedback.</p>
<p><strong>On PowerPC:</strong>
I don&#39;t know much about PowerPC or the availability of intrinsics, but the instructions you need are definitely there.
For add/sub with carry, there&#39;s <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-addc-add-carrying-instruction">addc</a> / <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-adde-ae-add-extended-instruction">adde</a> for addition and <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-subfc-sf-subtract-from-carrying-instruction">subfc</a> / <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-subfe-sfe-subtract-from-extended-instruction">subfe</a> for subtraction with borrow.
For wide multiplication, <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-mulhdu-multiply-high-double-word-unsigned-instruction">mulhdu</a> gives you the high half of a 64-bit multiply (and <a href="https://www.ibm.com/docs/en/aix/7.2.0?topic=set-mulhd-multiply-high-double-word-instruction">mulhd</a> for signed).</p>
<p><strong>On GCC codegen with intrinsics:</strong>
Someone pointed out that GCC doesn&#39;t always handle the intrinsic-based addition optimally.
It sometimes <a href="https://godbolt.org/z/f6aqMcfcW">moves the result through the stack</a> before setting it in the final registers.
This is related to writing directly to the result struct.
If you write to &#34;real&#34; scalars first, the <a href="https://godbolt.org/z/bo9x4951v">codegen is optimal again</a>.
A weird one.</p>
<p><strong>On division:</strong>
There is no neat codegen for division.
Even the builtin <a href="https://godbolt.org/z/rrMo5deqz">delegates to a library call</a>.
The naive but practical approach is binary long division, which finishes in up to 128 steps.
Either branchless with fixed runtime or with a loop that searches for the next set bit.
Either way it&#39;s a bit of work.
Our exact predicates are always formulated in a division-free way simply because division would be expensive.</p>
<p><strong>On <code>_BitInt(N)</code>:</strong>
The upcoming <code>_BitInt(N)</code> type does work, but with caveats.
For B = 128, you get the normal codegen.
For <a href="https://godbolt.org/z/j9fd5EW3n">B &gt; 128</a>, GCC calls into a function where the bit size is a runtime parameter.
So yes, they would work, but performance will be subpar for larger widths.
Clang generates properly inlined assembly.</p>

            </div>

            
        </article>
    </div>
</section>

    </div></div>
  </body>
</html>
