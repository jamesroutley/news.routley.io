<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sometechblog.com/posts/try-gemini-api-with-openai-fallback/">Original</a>
    <h1>Use the Gemini API with OpenAI Fallback in TypeScript</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      <main id="main" tabindex="-1">
    

    <article>
        
        

  
  




  
  





        <div>
            <p>If you want to use Gemini’s public API, but at the same time have a safe fallback in case you have exhausted the rate limits, you can use the <a href="https://github.com/openai/openai-node">OpenAI TS/JS library</a> and a few helper functions. In my particular case I needed a type-safe solution for a <a href="https://www.chartmaker.io/">chartmaker app</a> with a fallback since Gemini’s <code>gemini-2.5-pro-exp-03-25</code> model is restricted to 20 request/min.</p>
<p>First, you need to define which models you want to use so that they appear as autosuggest when you use the helper functions:</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>type</span> Model <span>=</span> ChatCompletionParseParams[<span>&#39;model&#39;</span>] <span>|</span> <span>&#39;gemini-2.5-pro-exp-03-25&#39;</span> <span>|</span> <span>&#39;gemini-2.0-flash&#39;</span>;
</span></span></code></pre></div><p>The helper function requires one argument; an array of 2 configuration objects for the desired AI queries (in principle, you can add as many as you want, or choose other AIs that are compatible with the OpenAI library):</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>export</span> <span>const</span> getCompletion <span>=</span> <span>async</span> (
</span></span><span><span>  options<span>:</span> [
</span></span><span><span>    Omit&lt;<span>ChatCompletionParseParams</span><span>,</span> <span>&#39;</span>model<span>&#39;</span>&gt; <span>&amp;</span> { model: <span>Model</span> },
</span></span><span><span>    Omit&lt;<span>ChatCompletionParseParams</span><span>,</span> <span>&#39;</span>model<span>&#39;</span>&gt; <span>&amp;</span> { model: <span>Model</span> },
</span></span><span><span>  ],
</span></span><span><span>) <span>=&gt;</span> {
</span></span><span><span>  <span>try</span> {
</span></span><span><span>    <span>const</span> isGemini <span>=</span> options[<span>0</span>].model.includes(<span>&#39;gemini&#39;</span>);
</span></span><span><span>    <span>const</span> openai <span>=</span> <span>new</span> OpenAI(
</span></span><span><span>      isGemini
</span></span><span><span>        <span>?</span> {
</span></span><span><span>            apiKey: <span>process.env.GEMINI_API_KEY</span>,
</span></span><span><span>            baseURL<span>:</span> <span>&#39;https://generativelanguage.googleapis.com/v1beta/openai/&#39;</span>,
</span></span><span><span>          }
</span></span><span><span>        <span>:</span> { apiKey: <span>process.env.OPENAI_API_KEY</span> },
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>await</span> openai.chat.completions.create(options[<span>0</span>]);
</span></span><span><span>  } <span>catch</span> (error) {
</span></span><span><span>    console.log(<span>`Failed completion for first model (</span><span>${</span>options[<span>0</span>].model<span>}</span><span>)`</span>, error);
</span></span><span><span>
</span></span><span><span>    <span>const</span> isGemini <span>=</span> options[<span>1</span>].model.includes(<span>&#39;gemini&#39;</span>);
</span></span><span><span>    <span>const</span> openai <span>=</span> <span>new</span> OpenAI(
</span></span><span><span>      isGemini
</span></span><span><span>        <span>?</span> {
</span></span><span><span>            apiKey: <span>process.env.GEMINI_API_KEY</span>,
</span></span><span><span>            baseURL<span>:</span> <span>&#39;https://generativelanguage.googleapis.com/v1beta/openai/&#39;</span>,
</span></span><span><span>          }
</span></span><span><span>        <span>:</span> { apiKey: <span>process.env.OPENAI_API_KEY</span> },
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>await</span> openai.chat.completions.create(options[<span>1</span>]);
</span></span><span><span>  }
</span></span><span><span>};
</span></span></code></pre></div><p>The help function can be used in the following ways:</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span>
</span></span><span><span><span>const</span> messages <span>=</span> [{ role<span>:</span> <span>&#39;user&#39;</span>, content<span>:</span> <span>&#39;Tell a short joke.&#39;</span> }];
</span></span><span><span><span>const</span> completion <span>=</span> <span>await</span> getCompletion([
</span></span><span><span>  { model<span>:</span> <span>&#39;gemini-2.0-flash&#39;</span>, messages },
</span></span><span><span>  { model<span>:</span> <span>&#39;gpt-3.5-turbo&#39;</span>, messages },
</span></span><span><span>]);
</span></span><span><span>
</span></span><span><span>console.log(completion);
</span></span><span><span><span>// {
</span></span></span><span><span><span>//   &#34;choices&#34;: [
</span></span></span><span><span><span>//     {
</span></span></span><span><span><span>//       &#34;finish_reason&#34;: &#34;stop&#34;,
</span></span></span><span><span><span>//       &#34;index&#34;: 0,
</span></span></span><span><span><span>//       &#34;message&#34;: {
</span></span></span><span><span><span>//         &#34;content&#34;: &#34;Why don&#39;t scientists trust atoms?\n\nBecause they make up everything!\n&#34;,
</span></span></span><span><span><span>//         &#34;role&#34;: &#34;assistant&#34;
</span></span></span><span><span><span>//       }
</span></span></span><span><span><span>//     }
</span></span></span><span><span><span>//   ],
</span></span></span><span><span><span>//   &#34;created&#34;: 1743757243,
</span></span></span><span><span><span>//   &#34;model&#34;: &#34;gemini-2.0-flash&#34;,
</span></span></span><span><span><span>//   &#34;object&#34;: &#34;chat.completion&#34;,
</span></span></span><span><span><span>//   &#34;usage&#34;: {
</span></span></span><span><span><span>//     &#34;completion_tokens&#34;: 16,
</span></span></span><span><span><span>//     &#34;prompt_tokens&#34;: 5,
</span></span></span><span><span><span>//     &#34;total_tokens&#34;: 21
</span></span></span><span><span><span>//   }
</span></span></span><span><span><span>// }
</span></span></span></code></pre></div><p>You can also create a helper function for type-safe structured output:</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>export</span> <span>const</span> getJSONCompletion <span>=</span> <span>async</span> &lt;<span>T</span>&gt;(
</span></span><span><span>  options<span>:</span> [
</span></span><span><span>    Omit&lt;<span>ChatCompletionParseParams</span><span>,</span> <span>&#39;</span>model<span>&#39;</span>&gt; <span>&amp;</span> { model: <span>Model</span> },
</span></span><span><span>    Omit&lt;<span>ChatCompletionParseParams</span><span>,</span> <span>&#39;</span>model<span>&#39;</span>&gt; <span>&amp;</span> { model: <span>Model</span> },
</span></span><span><span>  ],
</span></span><span><span>)<span>:</span> Promise&lt;<span>ParsedChatCompletion</span><span>&lt;</span>T&gt; <span>&amp;</span> { _request_id?: <span>string</span> <span>|</span> <span>null</span> <span>|</span> <span>undefined</span> }<span>&gt;</span> <span>=&gt;</span> {
</span></span><span><span>  <span>try</span> {
</span></span><span><span>    <span>const</span> isGemini <span>=</span> options[<span>0</span>].model.includes(<span>&#39;gemini&#39;</span>);
</span></span><span><span>    <span>const</span> openai <span>=</span> <span>new</span> OpenAI(
</span></span><span><span>      isGemini
</span></span><span><span>        <span>?</span> {
</span></span><span><span>            apiKey: <span>process.env.GEMINI_API_KEY</span>,
</span></span><span><span>            baseURL<span>:</span> <span>&#39;https://generativelanguage.googleapis.com/v1beta/openai/&#39;</span>,
</span></span><span><span>          }
</span></span><span><span>        <span>:</span> { apiKey: <span>process.env.OPENAI_API_KEY</span> },
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>await</span> openai.beta.chat.completions.parse({ ...options[<span>0</span>] });
</span></span><span><span>  } <span>catch</span> (error) {
</span></span><span><span>    console.log(<span>&#39;Failed completion for first model&#39;</span>, error);
</span></span><span><span>
</span></span><span><span>    <span>const</span> isGemini <span>=</span> options[<span>1</span>].model.includes(<span>&#39;gemini&#39;</span>);
</span></span><span><span>    <span>const</span> openai <span>=</span> <span>new</span> OpenAI(
</span></span><span><span>      isGemini
</span></span><span><span>        <span>?</span> {
</span></span><span><span>            apiKey: <span>process.env.GEMINI_API_KEY</span>,
</span></span><span><span>            baseURL<span>:</span> <span>&#39;https://generativelanguage.googleapis.com/v1beta/openai/&#39;</span>,
</span></span><span><span>          }
</span></span><span><span>        <span>:</span> { apiKey: <span>process.env.OPENAI_API_KEY</span> },
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>await</span> openai.beta.chat.completions.parse({ ...options[<span>1</span>] });
</span></span><span><span>  }
</span></span><span><span>};
</span></span></code></pre></div><p>It can be used in the following way:</p>
<div><pre tabindex="0"><code data-lang="ts"><span><span><span>import</span> z <span>from</span> <span>&#39;zod&#39;</span>;
</span></span><span><span>
</span></span><span><span><span>//... Omitted for brevity
</span></span></span><span><span><span></span>
</span></span><span><span><span>const</span> messages <span>=</span> [{ role<span>:</span> <span>&#34;user&#34;</span>, content<span>:</span> <span>&#34;Your instructions...&#34;</span>}] satisfies ChatCompletionMessageParam[];
</span></span><span><span><span>const</span> format <span>=</span> z.<span>object</span>({ customizations: <span>z.array</span>(z.<span>string</span>()) });
</span></span><span><span><span>const</span> responseFormat <span>=</span> zodResponseFormat(format, <span>&#39;chart-customizations&#39;</span>);
</span></span><span><span>
</span></span><span><span><span>const</span> completion <span>=</span> <span>await</span> getJSONCompletion&lt;<span>z.infer</span><span>&lt;</span>typeof format&gt;<span>&gt;</span>(
</span></span><span><span>  [
</span></span><span><span>    { model<span>:</span> <span>&#39;gemini-2.5-pro-exp-03-25&#39;</span>, response_format: <span>responseFormat</span>, messages, temperature: <span>0</span> },
</span></span><span><span>    { model<span>:</span> <span>&#39;o3-mini-2025-01-31&#39;</span>, reasoning_effort<span>:</span> <span>&#39;high&#39;</span>, response_format: <span>responseFormat</span>, messages },
</span></span><span><span>  ],
</span></span><span><span>);
</span></span><span><span>
</span></span><span><span><span>const</span> customizationsArr <span>=</span> completion.choices[<span>0</span>].message.parsed<span>?</span>.customizations;
</span></span></code></pre></div>
        </div>

    </article>

    
    

    
        
        
    

    

    
        








    

    

    
</main>
       
 
    </div></div>
  </body>
</html>
