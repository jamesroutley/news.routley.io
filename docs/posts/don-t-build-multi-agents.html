<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cognition.ai/blog/dont-build-multi-agents">Original</a>
    <h1>Don&#39;t Build Multi-Agents</h1>
    
    <div id="readability-page-1" class="page"><div id="blog-post__body"> <h2><strong>Principles of Context Engineering</strong></h2><p>We’ll work our way up to the following principles:</p><ol><li>Share context</li><li>Actions carry implicit decisions</li></ol><p><strong>Why think about principles?</strong></p><p>HTML was introduced in 1993. In 2013, Facebook released React to the world. It is now 2025 and React (and its descendants) dominates the way developers build sites and apps. Why? Because React is not just a scaffold for writing code. It is a philosophy. By using React, you embrace building applications with a pattern of reactivity and modularity, which people now accept to be a standard requirement, but this was not always obvious to early web developers.</p><p>In the age of LLMs and building AI Agents, it feels like we’re still playing with raw HTML &amp; CSS and figuring out how to fit these together to make a good experience. No single approach to building agents has become the standard yet, besides some of the absolute basics.</p><blockquote>In some cases, libraries such as <a href="https://github.com/openai/swarm">https://github.com/openai/swarm</a> by OpenAI and <a href="https://github.com/microsoft/autogen">https://github.com/microsoft/autogen</a> by Microsoft actively push concepts which I believe to be the wrong way of building agents. Namely, using multi-agent architectures, and I’ll explain why.</blockquote><p>That said, if you’re new to agent-building, there are lots of resources on how to set up the basic scaffolding [<a href="https://www.anthropic.com/engineering/building-effective-agents">1</a>] [<a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">2</a>]. But when it comes to building serious production applications, it&#39;s a different story.</p><h2><strong>A Theory of Building Long-running Agents</strong></h2><p>Let’s start with reliability. When agents have to actually be reliable while running for long periods of time and maintain coherent conversations, there are certain things you must do to contain the potential for compounding errors. Otherwise, if you’re not careful, things fall apart quickly. At the core of reliability is Context Engineering.</p><p><em>Context Engineering</em></p><p>In 2025, the models out there are extremely intelligent. But even the smartest human won’t be able to do their job effectively without the context of what they’re being asked to do. “Prompt engineering” was coined as a term for the effort needing to write your task in the ideal format for a LLM chatbot. “Context engineering” is the next level of this. It is about doing this automatically in a dynamic system. It takes more nuance and is effectively the #1 job of engineers building AI agents.</p><p>Take an example of a common type of agent. This agent</p><ol><li>breaks its work down into multiple parts</li><li>starts subagents to work on those parts</li><li>combines those results in the end</li></ol><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/721e44474051c62156e15b5ffb1a249c996f0607-1404x1228.png" alt="" loading="lazy"/></a></p><p>This is a tempting architecture, especially if you work in a domain of tasks with several parallel components to it. However, it is very fragile. The key failure point is this:</p><blockquote>Suppose your <strong>Task</strong> is “build a Flappy Bird clone”. This gets divided into <strong>Subtask 1</strong> “build a moving game background with green pipes and hit boxes” and <strong>Subtask 2</strong> “build a bird that you can move up and down”.</blockquote><p>This may seem contrived, but most real-world tasks have many layers of nuance that all have the potential to be miscommunicated. You might think that a simple solution would be to just copy over the original task as context to the subagents as well. That way, they don’t misunderstand their subtask. But remember that in a real production system, the conversation is most likely multi-turn, the agent probably had to make some tool calls to decide how to break down the task, and any number of details could have consequences on the interpretation of the task.</p><blockquote><em>Principle 1</em></blockquote><p>Let’s take another revision at our agent, this time making sure each agent has the context of the previous agents.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/e3bdf57c10a9b6c4531b93a10fb79a712464c712-1408x1232.png" alt="" loading="lazy"/></a></p><p>Unfortunately, we aren’t quite out of the woods. When you give your agent the same Flappy Bird cloning task, this time, you might end up with a bird and background with completely different visual styles. Subagent 1 and subagent 2 cannot not see what the other was doing and so their work ends up being inconsistent with each other.</p><p>The actions subagent 1 took and the actions subagent 2 took were based on conflicting assumptions not prescribed upfront.</p><blockquote><em>Principle 2</em></blockquote><p>I would argue that Principles 1 &amp; 2 are so critical, and so rarely worth violating, that you should by default rule out any agent architectures that don’t abide by then. You might think this is constraining, but there is actually a wide space of different architectures you could still explore for your agent.</p><p>The simplest way to follow the principles is to just use a single-threaded linear agent:</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/06f64ae3557594588f702b2608d43564edc98c3d-1404x1230.png" alt="" loading="lazy"/></a></p><p>Here, the context is continuous. However, you might run into issues for very large tasks with so many subparts that context windows start to overflow.</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/4a36b048810fb2cba4ee4055ed2d3c80f188befc-1394x1218.png" alt="" loading="lazy"/></a></p><p>To be honest, the simple architecture will get you very far, but for those who have truly long-duration tasks, and are willing to put in the effort, you can do even better. There are several ways you could solve this, but today I will present just one:</p><p><a href="https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png" target="_blank"><img src="https://cdn.sanity.io/images/2mc9cv2v/production/836a7407ddf3dfacc0715c0502b4f3ffc7388829-1406x1230.png" alt="" loading="lazy"/></a></p><p>In this world, we introduce a new LLM model whose key purpose is to compress a history of actions &amp; conversation into key details, events, and decisions. This is <em>hard to get right.</em> It takes investment into figuring out what ends up being the key information and creating a system that is good at this. Depending on the domain, you might even consider fine-tuning a smaller model (this is in fact something we’ve done at Cognition).</p><p>The benefit you get is an agent that is effective at longer contexts. You will still eventually hit a limit though. For the avid reader, I encourage you to think of better ways to manage arbitrarily long contexts. It ends up being quite a deep rabbit hole!</p><h2><strong>Applying the Principles</strong></h2><p>If you’re an agent-builder, ensure your agent’s every action is informed by the context of all relevant decisions made by other parts of the system. Ideally, every action would just see everything else. Unfortunately, this is not always possible due to limited context windows and practical tradeoffs, and you may need to decide what level of complexity you are willing to take on for the level of reliability you aim for.</p><p>As you think about architecting your agents to avoid conflicting decision-making, here are some real-world examples to ponder:</p><p><strong>Multi-Agents</strong></p><p>If we really want to get parallelism out of our system, you might think to let the decision makers “talk” to each other and work things out.</p><p>This is what us humans do when we disagree (in an ideal world). If Engineer A’s code causes a merge conflict with Engineer B, the correct protocol is to talk out the differences and reach a consensus. However, agents today are not quite able to engage in this style of long-context proactive discourse with much more reliability than you would get with a single agent. Humans are quite efficient at communicating our most important knowledge to one another, but this efficiency takes nontrivial intelligence.</p><p>Since not long after the launch of ChatGPT, people have been exploring the idea of multiple agents interacting with one another to achieve goals [<a href="https://arxiv.org/abs/2304.03442">3</a>][<a href="https://github.com/FoundationAgents/MetaGPT">4</a>]. While I’m optimistic about the long-term possibilities of agents collaborating with one another, it is evident that in 2025, running multiple agents in collaboration only results in fragile systems. The decision-making ends up being too dispersed and context isn’t able to be shared thoroughly enough between the agents. At the moment, I don’t see anyone putting a dedicated effort to solving this difficult cross-agent context-passing problem. I personally think it will come for free as we make our single-threaded agents even better at communicating with humans. When this day comes, it will unlock much greater amounts of parallelism and efficiency.</p><p><strong>Toward a More General Theory</strong></p><p>These observations on context engineering are just the start to what we might someday consider the standard principles of building agents. And there are many more challenges and techniques not discussed here. At Cognition, agent building is a key frontier we think about. We build our internal tools and frameworks around these principles we repeatedly find ourselves relearning as a way to enforce these ideas. But our theories are likely not perfect, and we expect things to change as the field advances, so some flexibility and humility is required as well.</p><p>We welcome you to try our work at <a href="http://app.devin.ai">app.devin.ai</a>. And if you would enjoy discovering some of these agent-building principles with us, reach out to <a href="mailto:walden@cognition.ai">walden@cognition.ai</a></p> </div></div>
  </body>
</html>
