<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://medium.com/in-the-hudl/operation-jumbo-drop-how-sending-large-packets-broke-our-aws-network-ff5041fc7a09">Original</a>
    <h1>Operation Jumbo Drop: How sending large packets broke our AWS network</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><p id="e0e6">Hudl recently ran into some really strange networking failures in our testing environment — we tracked them down to a pretty obscure network setting in AWS EC2.</p><p id="1121">If you’re interested in Chef or Linux debugging, AWS networking troubleshooting, or just love a good rabbit hole, read on.</p><p id="5755">Every weekend at Hudl we run a process to restore a sanitised version of our production databases to our staging environment, to give our developers realistic data to test with before going to production.</p><p id="39ea">This Monday we woke up to a Slack room full of errors from the restoration process, telling us that the Chef run which configures the databases had failed on every single EC2 instance.</p><p id="d514">The error we were presented with wasn’t super helpful, “EOFError, end of file reached.”</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*fFKxf3vNH0LNFxya" width="700" height="236" role="presentation"/></p></div></figure><p id="c4ac">It’s unclear what file it’s talking about. The recipe referenced does write multiple configuration files out, but they haven’t changed for years and it’s unclear why this would suddenly break.</p><p id="9db6">When we logged on to the server ourselves and ran <code>chef-client</code>, we were able to replicate the exact error. So at least we had a starting point.</p><p id="1040">Our first suspicions were that the access keys the server was using to talk to the Chef Server were invalid. We’d recently rotated the Chef keys associated with this process and could have broken them.</p><p id="8523">It seemed unlikely as <code>chef-client</code> had clearly been able to pull cookbooks down and start running, but it was the only thing we’d changed recently.</p><p id="5edc"><code>Knife</code> is a CLI that administrators can use to interact with the Chef server. In theory it uses the same API calls as <code>chef-client</code>, so we attempted to use <code>knife</code> to communicate with the Chef server to see if the key was valid.</p><pre><span id="5a69">knife node list -c /etc/chef/client.rb</span></pre><p id="8ad0">We were able to retrieve a list of all our nodes registered with Chef and the keys seemed fine.</p><p id="25e5">Running Chef again, we noticed that it hung for around two minutes before printing the <code>EOFError</code>, and if you killed the <code>chef-client</code> run with <code>ctrl-c</code>, the stack trace indicated it was stuck trying to write a node attribute back to the Chef Server.</p><pre><span id="f3b1">5: node.normal[:aws][:ebs_volume] = Hash[node[:volumes].map { |v| [</span></pre><p id="b4a8">We repeated this a few times and it was always consistently stuck here.</p><p id="ffed">Was this some sort of networking issue talking to the Chef Server?</p><p id="8eaa">It seemed weird we’d have a networking issue talking to the Chef Server. We’d successfully pulled down all the cookbooks from it, and had been able to talk to the Chef Server with <code>knife</code>.</p><p id="53fb">We decided to use <code>knife</code> to try and save a node attribute; this was, after all, what the <code>chef-client</code> seemed to get stuck on.</p><p id="7392">We ran:</p><pre><span id="e64b">EDITOR=vim knife node edit &lt;NODE_NAME&gt; -c /etc/chef/client.rb</span></pre><p id="5eaa">And edited a random attribute, which worked fine.</p><p id="43a0">We were slightly confused as to why <code>chef-client</code> was failing to save a node attribute to the server, as we could do this using <code>knife</code>. We decided to run <code>chef-client</code> with debug logs enabled, hoping that it would give us some context around the error.</p><pre><span id="3efc">chef-client -l debug</span><span id="ac11"><strong>[2022–02–28T11:18:53+00:00] DEBUG: Initiating POST to </strong><a href="https://chef12-server.app.hudl.com/organizations/hudl/reports/nodes/" rel="noopener ugc nofollow" target="_blank"><strong>https://chef12-server.app.hudl.com/organizations/hudl/reports/nodes/</strong></a><strong>&lt;node name&gt;/runs/&lt;run_id&gt;</strong></span><span id="1cca">[2022–02–28T11:18:53+00:00] DEBUG: Content-Length: 301</span></pre><p id="a42c">The logs seem consistent with the stack trace we got before. The HTTP POST is presumably the update to the node attribute.</p><p id="a228">We’d come as far as we could on this server, and decided to see if the logs on the other side of the HTTP connection had any more information.</p><p id="c0b7">The Nginx log on the Chef Server had more information on the request. It had returned a HTTP 408, which is a <a href="https://http.cat/408" rel="noopener ugc nofollow" target="_blank">timeout</a>.</p><pre><span id="9a15">172.27.75.108 — — [28/Feb/2022:11:57:20 +0000] “PUT /organizations/hudl/nodes/&lt;node name&gt; HTTP/1.1” 408 “60.058” 0 “-” “Chef Client/12.14.89 (ruby-2.3.1-p112; ohai-8.20.0; x86_64-linux; +https://chef.io)&#34; “-” “-” “-” “12.14.89” “algorithm=sha1;version=1.1;” “&lt;node name&gt;” “2022–02–28T11:55:20Z” “&lt;random id&gt;” 1164</span></pre><p id="f826">We now seemed to be getting somewhere — but what on the Chef Server was causing the requests to timeout?</p><p id="1900">The Chef Server runs a lot of different processes, from Rabbit, Solr and Redis, to the actual Chef Server processes itself. This makes it hard to understand the “health” of the Chef Server, so we had to check each process was running and read the logs from them one by one.</p><p id="e44b">There were a few red herrings in the log files, errors that are apparently normal, but overall nothing seemed obviously broken.</p><p id="6eb4">Confused about what was happening, we started to look at if it was just our newly restored databases or a bigger problem.</p><p id="7f8a">All our production servers could still run Chef fine, but we could replicate the same error on all our servers inside our staging VPC.</p><p id="82d1">Even more confused, we turned to Google to see if others had this oddly specific error where some servers couldn’t write to the Chef Server.</p><p id="bfef">It turns out others had experienced almost the exact same error: <a href="https://github.com/chef/chef/issues/1937" rel="noopener ugc nofollow" target="_blank">https://github.com/chef/chef/issues/1937</a>.</p><p id="182e">Worryingly, the issue seemed to be related to low level networking settings: <a href="https://github.com/chef/chef/issues/1937#issuecomment-53661631" rel="noopener ugc nofollow" target="_blank">https://github.com/chef/chef/issues/1937#issuecomment-53661631</a>.</p><p id="2a13">It seemed like a large MTU setting could cause servers to be unable to send data to the Chef Server if the requests were over a certain size in some environments.</p><p id="c4a5">The <a href="https://www.cloudflare.com/en-gb/learning/network-layer/what-is-mtu/" rel="noopener ugc nofollow" target="_blank">MTU</a> setting on a network interface specifies the maximum size of the packets the interface sends.</p><p id="0003">Due to the overhead of things like headers on the packets, it’s more efficient to send fewer but larger packets.</p><p id="c51c">But the internet is littered with an assortment of networking devices, some of which can only handle smaller packets.</p><p id="e33c">This means you may be forced to send smaller packets to ensure all the devices they pass through can handle them.</p><p id="e7fa">A generally accepted MTU is <a href="https://serverfault.com/a/325987" rel="noopener ugc nofollow" target="_blank">1500</a>. Some networks such as inside AWS VPC support MTUs of <code>9001</code> (referred to as Jumbo Frames), and it appears the AMIs AWS provides launch EC2 instances with the MTU set to <code>9001</code>, which is what our EC2 instances have.</p><p id="3f93">This issue seemed plausible for explaining why our tests worked with <code>knife</code> but the <code>chef-client</code> run still failed. We were still slightly sceptical and wanted to try and confirm this was the issue.</p><p id="a51f">Both our Chef Server and Database had MTUs of <code>9001</code>.</p><pre><span id="9f95">[ec2-user@<strong>mongo</strong> ~]$ ifconfig</span><span id="9b19">[root@<strong>chef-server</strong> ~]# ifconfig</span></pre><p id="bb7e">On the Chef Server we ran <code>netcat</code> in listening mode, printing out the number of bytes it received.</p><pre><span id="8dff">[root@chef-server ~]# nc -l 9234 | wc — bytes</span></pre><p id="b41c">On the MongoServer we used <code>dd</code> to generate fixed size amounts of data and send it to our <code>netcat</code> process.</p><pre><span id="3e1a">ec2-user@t-augmentation-mongo-rs1-use1d-520nmi ~]$ dd if=/dev/urandom count=1 bs=9002 | nc chef12-server.app.hudl.com 9234</span></pre><p id="7b6b">We couldn’t replicate the exact findings in the Github issue thread — packets of size <code>9002</code>, <code>1500</code> and all sizes in between worked fine.</p><p id="4719">We randomly tried some larger sizes, and <code>20,000</code> byte requests seemed to replicate the issue as they were never received by the Chef Server.</p><p id="6480">Some binary searching led us to discover that <code>10,497</code> bytes was the tipping point. Any requests equal to or larger than this failed, but smaller than this worked.</p><p id="7310">It still isn’t clear why <code>10,497</code> is the tipping point, and not <code>9002</code>, but presumably it’s related to headers on the packets or something like that.</p><p id="1cd1">Curious about what was actually happening, we used <code>tcpdump</code> to capture the traffic on the database instance (we tried on the Chef Server, but installing <code>tcpdump</code> required updating a bunch of other packages that we weren’t sure were safe to upgrade), and the results were interesting.</p><p id="9b4b">Working requests were unexciting. The TCP connection was established, the packets sent, and the connection torn down.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*p4jxu7buXRJhILuA" width="700" height="169" role="presentation"/></p></div></figure><p id="38be">On the failed requests, the TCP connection was established and then after sending the first packet of data, no ACK is ever received and it just goes into a constant TCP retry loop.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/0*y-R9rota3vWCCZKQ" width="700" height="180" role="presentation"/></p></div></figure><p id="c25d">So the size of the packet does seem to affect if it reaches the destination.</p><p id="0247">Now we lowered the MTU on the database server from <code>9001</code> to <code>1500</code> and repeated the tests to see if it made any difference.</p><p id="2eb8">It fixed the issue! Our <code>chef-client</code> run succeeded and by repeating the test with <code>netcat</code>, we confirmed data of all sizes made it to the server.</p><p id="a9e6">We’d narrowed down the issue but more questions still remained:</p><ol><li id="915c">What caused this to break? It worked fine last week.</li><li id="bf5d">How were we going to fix this? Do we really have to go and change our MTU everywhere in staging? Why does it still work in Prod?</li></ol><p id="0313">Some Googling suggested that <code>tracepath</code> might be able to show us what the MTU was on the path between our database and Chef Server.</p><pre><span id="4943">[root@t-augmentation-mongo-rs1-use1d-520nmi ~]# tracepath 10.110.78.16</span><span id="89ac">1?: [LOCALHOST] pmtu 1500</span><span id="5160">Resume: pmtu 1500 hops 2 back 1</span></pre><p id="6834">As expected it confirms <code>1500</code>.</p><p id="d271">To experiment we set the database servers MTU back to <code>9100</code> and ran the tool again.</p><pre><span id="0730">[root@t-augmentation-mongo-rs1-use1d-520nmi ~]# tracepath 10.110.78.16</span><span id="8d84">1?: [LOCALHOST] pmtu 9100</span><span id="9e96">Too many hops: pmtu 9100</span></pre><p id="3dc0">This time it didn’t seem to work. We were under the impression <code>tracepath</code> would try different MTUs until it found one that worked, but it just seems broken.</p><p id="9451">We played around with <code>tracepath</code> some more and were never able to get it work, unless we manually lowered the servers MTU to <code>1500</code>.</p><p id="36a8">This wasn’t getting us any closer to a fix though, so next we turned our attention to Path MTU Discovery.</p><p id="b4f6">Path MTU Discovery is supposed to be a way for servers with differing MTUs to negotiate a mutually compatible MTU setting for their packets.</p><p id="3109">If a device between the servers, or the server itself, receives a message that is too large for it to process, it drops the packet and sends back an ICMP Fragmentation Needed packet. This signals to the client that it must reduce its MTU and send the packet again.</p><p id="9d8e">Why wasn’t this working for us, and allowing our servers to negotiate a compatible MTU?</p><p id="2394">We suspected that maybe the security groups were blocking the ICMP messages. As an experiment we altered the security groups for both the Chef Server and Database to allow ICMP packets in from anywhere, but it made no difference.</p><p id="71ac">Running out of ideas, we turned our attention to try and work out why the Prod servers could reach the Chef Server but the staging servers could not. What were the differences in their networks?</p><p id="1629">Our Chef Server lives in its own VPC for “internal tools,” whilst Prod and Staging are separated into their own VPCs.</p><p id="f8e6">During this investigation, we discovered that just before the weekend, the staging VPC had been altered by another team to send its packets to the internal tools VPC via a Transit Gateway. However, the return packets still went down a VPC Peering connection.</p><p id="087c">The Prod VPC was untouched, and used a separate proxy for communicating with the Chef Server.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*CdLkVjEMxljUpVOPSkcUrQ.jpeg" width="700" height="525" role="presentation"/></p></div></figure><p id="df0a">This seemed like it must be the issue, but it was still unclear. Why had we configured our Transit Gateway incorrectly?</p><p id="7d40">Reading the AWS Documentation on MTUs, there’s a small note about MTUs and TransitGateways</p><blockquote><p id="71e0">“For more information about supported MTU sizes for transit gateways, see MTU in Amazon VPC Transit Gateways.”</p></blockquote><p id="ab6b">Buried in the “Quotas” section of the MTU docs are these useful points:</p><blockquote><p id="8533">A transit gateway supports an MTU of 8500 bytes for traffic between VPCs, Direct Connect gateway, and peering attachments. Traffic over VPN connections can have an MTU of 1500 bytes.</p><p id="503f">When migrating from VPC peering to use a transit gateway, an MTU size mismatch between VPC peering and the transit gateway might result in some asymmetric traffic packets dropping. Update both VPCs at the same time to avoid jumbo packets dropping due to a size mismatch.</p><p id="ece6">Packets with a size larger than 8500 bytes that arrive at the transit gateway are dropped.</p><p id="b831">The transit gateway does not generate the FRAG_NEEDED for ICMPv4 packet, or the Packet Too Big (PTB) for ICMPv6 packet. Therefore, the Path MTU Discovery (PMTUD) is not supported.</p></blockquote><p id="ef89">We had essentially just reverse engineered all of this information.</p><p id="4097">Transit Gateways don’t support MTUs of <code>9001</code>, they don’t support Path MTU Discovery, and asymmetric paths can cause issues, all of which we’d been experiencing.</p><p id="c2e3">Some more Googling revealed this fantastic blog post, where they’d experienced the exact same issue we had: <a rel="noopener" href="https://medium.com/seek-blog/beware-of-transit-gateways-bearing-large-packets-77702c4c1b20">https://medium.com/seek-blog/beware-of-transit-gateways-bearing-large-packets-77702c4c1b20</a>.</p><p id="f259">We won’t repeat all of what they’ve explained excellently. Go and read that post for the full technical details on what went wrong here and then come back.</p><p id="08b7">The short version is when using TCP connections:</p><ul><li id="4114">Networking devices such as TransitGateways can rewrite the value of a field in the TCP Header during connection establishment, called the Maximum Segment Size</li><li id="42df">This field tells the receiver the maximum size of the packets it should send in response</li><li id="d7b2">Packets sent from the database to the Chef Server would have had the MSS changed to <code>8500</code> as they passed through the TransitGateway</li><li id="d886">Packets sent from the Chef Server to the database would NOT have the MSS changed to <code>8500</code> as they don’t pass through the TransitGateway, so the database server doesn’t understand it shouldn’t send packets greater than <code>8500</code> bytes</li><li id="b5d6">The initial connection was able to be established via these asymmetric paths as the SYN / SYN ACK packets are naturally smaller than <code>8500</code> bytes</li></ul><p id="92d1">We altered the Route Tables to send the traffic in both directions through the TransitGateway and everything started working again with the MTU still set to <code>9001</code>.</p><p id="977f">We took packet captures again, whilst sending <code>10,497</code> byte packets from the database to the Chef Server to verify the MSS is being altered with the fix in place.</p><p id="5ccc">Let’s look at what we have from the database client <strong>before</strong> the fix.</p><p id="1ee8">On the TCP SYN packet the Database sent to the Chef Server the MSS is <code>8961</code>.</p><figure><p><img alt="" src="https://miro.medium.com/max/1266/1*NCMQHdC3DqGEberzqWggjw.png" width="633" height="172" role="presentation"/></p></figure><p id="28a8">On the TPC SYN ACK, the Chef server sent back to the Database the MSS is <code>8961</code>.</p><figure><p><img alt="" src="https://miro.medium.com/max/1290/1*MSa0bUxXQ5fQwJkRn2Br_g.png" width="645" height="188" role="presentation"/></p></figure><p id="8942">And now compare that to the databases perspective <strong>after</strong> the issue is fixed.</p><p id="72a7">The SYN packet the database sends to the Chef Server still has a MSS of <code>8961</code>.</p><figure><p><img alt="" src="https://miro.medium.com/max/1266/1*NCMQHdC3DqGEberzqWggjw.png" width="633" height="172" role="presentation"/></p></figure><p id="0ac2">The SYN ACK we get back from the Chef Server to the database has a smaller MSS of <code>8460</code>.</p><figure><p><img alt="" src="https://miro.medium.com/max/1256/1*yCsJaKC83qwvKxXKSy4mog.png" width="628" height="160" role="presentation"/></p></figure><p id="d9c9">There we have it — verification that the MSS was not getting set to the smaller maximum supported value when we had the issue.</p><p id="6363">After the fix, when traffic in both directions was going through the TransitGateway, the TransitGateway alters the MSS to the correct lower value so the clients can send appropriately sized packets through it.</p><p id="bb87">This investigation took around a day and a half to complete, and took us deep into the intricacies of how networking inside AWS works.</p><p id="ffa3">Without the Github issue and blog post from others experiencing similar issues, it may have taken significantly longer to fix this.</p><p id="bc23">We hope this post might similarly prove useful as we highlighted some networking debugging tools and techniques you can use to troubleshoot issues.</p><p id="6db6">It was an interesting dive into an area we don’t normally have to touch. Most importantly, we got everything working again.</p></div></div></section></div>
  </body>
</html>
