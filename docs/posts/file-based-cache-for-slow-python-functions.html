<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://docs.sweep.dev/blogs/file-cache">Original</a>
    <h1>Show HN: File-based cache for slow Python functions</h1>
    
    <div id="readability-page-1" class="page"><article><main><div><p>ðŸ“š Blogs</p><svg fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path></svg><p>ðŸ“‚ A better Python cache for slow function calls</p></div>
<p><strong>William Zeng</strong> - March 18th, 2024</p>
<hr/>
<p>We wrote a file cache - it&#39;s like Python&#39;s <code dir="ltr">lru_cache</code>, but it stores the values in files instead of in memory. This has saved us hours of time running our LLM benchmarks, and we&#39;d like to share it as its own Python module. Thanks to <a href="https://github.com/lukejagg" target="_blank" rel="noreferrer">Luke Jaggernauth<span> (opens in a new tab)</span></a> (former Sweep engineer) for building the initial version of this!</p>
<p>Here&#39;s the link:
<a href="https://github.com/sweepai/sweep/blob/main/docs/public/file_cache.py" target="_blank" rel="noreferrer">https://github.com/sweepai/sweep/blob/main/docs/public/file_cache.py<span> (opens in a new tab)</span></a>. To use it, simply add the <code dir="ltr">file_cache</code> decorator to your function. Here&#39;s an example:</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>import</span><span> time</span></span>
<span><span>from</span><span> file_cache </span><span>import</span><span> file_cache</span></span>
<span> </span>
<span><span>@file_cache</span><span>()</span></span>
<span><span>def</span><span> </span><span>slow_function</span><span>(</span><span>x</span><span>,</span><span> </span><span>y</span><span>):</span></span>
<span><span>    time</span><span>.</span><span>sleep</span><span>(</span><span>30</span><span>)</span></span>
<span><span>    </span><span>return</span><span> x </span><span>+</span><span> y</span></span>
<span> </span>
<span><span>print</span><span>(</span><span>slow_function</span><span>(</span><span>1</span><span>, </span><span>2</span><span>))</span><span> </span><span># -&gt; 3, takes 30 seconds</span></span>
<span><span>print</span><span>(</span><span>slow_function</span><span>(</span><span>1</span><span>, </span><span>2</span><span>))</span><span> </span><span># -&gt; 3, takes 0 seconds</span></span></code></pre></div>
<h2>Background<a href="#background" id="background" aria-label="Permalink for this section"></a></h2>
<p>We spend a lot of time prompt engineering our agents at Sweep. Our agents take a set of input strings, formats them as a prompt, then sends the prompt and any other information off to the LLM. We chain multiple agents to turn a GitHub issue to a pull request.
For example, to modify code we&#39;ll input the old code, any relevant context, and instructions then output the new code.</p>
<p><img src="https://docs.sweep.dev/assets/multi_llm_step.png" alt="Multiple llm steps being chained"/></p>
<p>A typical improvement involves tweaking a small part of our pipeline (like improving our planning algorithm), then running the entire pipeline again.
We use pdb (python&#39;s native debugger) to set breakpoints and inspect the state of our prompts, input values, and parsing logic.
For example, we can check whether a certain string matches a regex:</p>
<div><pre data-language="shell" data-theme="default"><code dir="ltr" data-language="shell" data-theme="default"><span><span>(</span><span>Pdb</span><span>) print(</span><span>todays_date</span><span>)</span></span>
<span><span>&#39;2024-03-14&#39;</span></span>
<span><span>(</span><span>Pdb</span><span>) re.match(</span><span>r</span><span>&#34;^\d{4}-\d{2}-\d{2}$&#34;</span><span>,</span><span> </span><span>todays_date</span><span>)</span></span>
<span><span>&lt;</span><span>re.Match object; span</span><span>=</span><span>(</span><span>0</span><span>,</span><span> </span><span>10</span><span>)</span><span>,</span><span> </span><span>match=</span><span>&#39;2024-03-14&#39;</span><span>&gt;</span></span></code></pre></div>
<p>This lets us debug at runtime with the actual data.</p>
<h2>Cached pdb<a href="#cached-pdb" id="cached-pdb" aria-label="Permalink for this section"></a></h2>
<p>pdb works great, but we have to wait for the entire pipeline to run again.
Imagine a version of pdb that not only interrupted execution, but also cached the entire program state up to that point.
Our time to hit that same bug could be cut from 10 minutes to 15 seconds (a 40x improvement).</p>
<p>We didn&#39;t build this, but we think our file_cache works just as well.</p>
<p>LLM calls are slow but their inputs and outputs are easy to cache, saving a lot of time. We can use the input prompt/string as the cache key, and the output string as the cache value.</p>
<h3>What&#39;s different from lru_cache?<a href="#whats-different-from-lru_cache" id="whats-different-from-lru_cache" aria-label="Permalink for this section"></a></h3>
<p>lru_cache is great for memoizing repeated function calls, but it doesn&#39;t support two key features.</p>
<ol>
<li>
<p>we need to persist the cache between runs.</p>
<ul>
<li>lru_cache stores the results in-memory, which means that the next time you run the program the cache will be empty. file_cache stores the results on disk. We also considered using Redis, but writing to disk is easier to set up/manage.</li>
</ul>
</li>
<li>
<p>lru_cache doesn&#39;t support ignoring arguments that invalidate the cache.</p>
<ul>
<li>We&#39;ll use a custom <code dir="ltr">chat_logger</code> which stores the chats for visualization. It contains the current timestamp <code dir="ltr">chat_logger.expiration</code>, which will invalidate the cache if it&#39;s serialized.</li>
<li>To counteract this we added ignored parameters, used like this: <code dir="ltr">file_cache(ignore_params=[&#34;chat_logger&#34;])</code>. This removes <code dir="ltr">chat_logger</code> from the cache key construction and prevents bad invalidation due to the constantly changing <code dir="ltr">expiration</code>.</li>
</ul>
</li>
</ol>
<h2>Implementation<a href="#implementation" id="implementation" aria-label="Permalink for this section"></a></h2>
<p>Our two main methods are <code dir="ltr">recursive_hash</code> and <code dir="ltr">file_cache</code>.</p>
<h3>recursive_hash<a href="#recursive_hash" id="recursive_hash" aria-label="Permalink for this section"></a></h3>
<p>We want to stably hash objects, and this is <a href="https://death.andgravity.com/stable-hashing" target="_blank" rel="noreferrer">not natively supported in python<span> (opens in a new tab)</span></a>.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>import</span><span> hashlib</span></span>
<span><span>from</span><span> cache </span><span>import</span><span> recursive_hash</span></span>
<span> </span>
<span> </span>
<span><span>class</span><span> </span><span>Obj</span><span>:</span></span>
<span><span>    </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> </span><span>name</span><span>):</span></span>
<span><span>        self</span><span>.</span><span>name </span><span>=</span><span> name</span></span>
<span> </span>
<span><span>obj </span><span>=</span><span> </span><span>Obj</span><span>(</span><span>&#34;test&#34;</span><span>)</span></span>
<span><span>print</span><span>(</span><span>recursive_hash</span><span>(obj))</span><span> </span><span># -&gt; this works fine</span></span>
<span><span>try</span><span>:</span></span>
<span><span>    hashlib</span><span>.</span><span>md5</span><span>(obj).</span><span>hexdigest</span><span>()</span></span>
<span><span>except</span><span> </span><span>Exception</span><span> </span><span>as</span><span> e</span><span>:</span></span>
<span><span>    </span><span>print</span><span>(e)</span><span> </span><span># -&gt; this doesn&#39;t work</span></span></code></pre></div>
<p>hashlib.md5 alone doesn&#39;t work for objects, giving us the error: <code dir="ltr">TypeError: object supporting the buffer API required</code>.
We use recursive_hash, which works for arbitrary python objects.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>def</span><span> </span><span>recursive_hash</span><span>(</span><span>value</span><span>,</span><span> </span><span>depth</span><span>=</span><span>0</span><span>,</span><span> </span><span>ignore_params</span><span>=</span><span>[]):</span></span>
<span><span>    </span><span>&#34;&#34;&#34;Hash primitives recursively with maximum depth.&#34;&#34;&#34;</span></span>
<span><span>    </span><span>if</span><span> depth </span><span>&gt;</span><span> MAX_DEPTH</span><span>:</span></span>
<span><span>        </span><span>return</span><span> hashlib</span><span>.</span><span>md5</span><span>(</span><span>&#34;max_depth_reached&#34;</span><span>.</span><span>encode</span><span>()).</span><span>hexdigest</span><span>()</span></span>
<span> </span>
<span><span>    </span><span>if</span><span> </span><span>isinstance</span><span>(value, (</span><span>int</span><span>, </span><span>float</span><span>, </span><span>str</span><span>, </span><span>bool</span><span>, </span><span>bytes</span><span>)):</span></span>
<span><span>        </span><span>return</span><span> hashlib</span><span>.</span><span>md5</span><span>(</span><span>str</span><span>(value).</span><span>encode</span><span>()).</span><span>hexdigest</span><span>()</span></span>
<span><span>    </span><span>elif</span><span> </span><span>isinstance</span><span>(value, (</span><span>list</span><span>, </span><span>tuple</span><span>)):</span></span>
<span><span>        </span><span>return</span><span> hashlib</span><span>.</span><span>md5</span><span>(</span></span>
<span><span>            </span><span>&#34;&#34;</span><span>.</span><span>join</span><span>(</span></span>
<span><span>                [</span><span data-rehype-pretty-code-wrapper="true"><span>recursive_hash</span><span>(item, depth </span><span>+</span><span> </span><span>1</span><span>, ignore_params)</span></span><span> </span><span>for</span><span> item </span><span>in</span><span> value]</span></span>
<span><span>            ).</span><span>encode</span><span>()</span></span>
<span><span>        ).</span><span>hexdigest</span><span>()</span></span>
<span><span>    </span><span>elif</span><span> </span><span>isinstance</span><span>(value, </span><span>dict</span><span>):</span></span>
<span><span>        </span><span>return</span><span> hashlib</span><span>.</span><span>md5</span><span>(</span></span>
<span><span>            </span><span>&#34;&#34;</span><span>.</span><span>join</span><span>(</span></span>
<span><span>                [</span></span>
<span><span>                    </span><span data-rehype-pretty-code-wrapper="true"><span>recursive_hash</span><span>(key, depth </span><span>+</span><span> </span><span>1</span><span>, ignore_params)</span></span></span>
<span><span>                    </span><span data-rehype-pretty-code-wrapper="true"><span>+</span><span> </span><span>recursive_hash</span><span>(val, depth </span><span>+</span><span> </span><span>1</span><span>, ignore_params)</span></span></span>
<span><span>                    </span><span>for</span><span> key, val </span><span>in</span><span> value.</span><span>items</span><span>()</span></span>
<span><span>                    </span><span>if</span><span> key </span><span>not</span><span> </span><span>in</span><span> ignore_params</span></span>
<span><span>                ]</span></span>
<span><span>            ).</span><span>encode</span><span>()</span></span>
<span><span>        ).</span><span>hexdigest</span><span>()</span></span>
<span><span>    </span><span>elif</span><span> </span><span>hasattr</span><span>(value, </span><span>&#34;__dict__&#34;</span><span>)</span><span> </span><span>and</span><span> value</span><span>.</span><span>__class__</span><span>.</span><span>__name__</span><span> </span><span>not</span><span> </span><span>in</span><span> ignore_params</span><span>:</span></span>
<span><span>        </span><span>return</span><span> </span><span data-rehype-pretty-code-wrapper="true"><span>recursive_hash</span><span>(value.</span><span>__dict__</span><span>, depth </span><span>+</span><span> </span><span>1</span><span>, ignore_params)</span></span></span>
<span><span>    </span><span>else</span><span>:</span></span>
<span><span>        </span><span>return</span><span> hashlib</span><span>.</span><span>md5</span><span>(</span><span>&#34;unknown&#34;</span><span>.</span><span>encode</span><span>()).</span><span>hexdigest</span><span>()</span></span></code></pre></div>
<h3>file_cache<a href="#file_cache" id="file_cache" aria-label="Permalink for this section"></a></h3>
<p>file_cache is a decorator that handles the caching logic for us.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>@file_cache</span><span>()</span></span>
<span><span>def</span><span> </span><span>search_codebase</span><span>(</span></span>
<span><span>    </span><span>cloned_github_repo</span><span>,</span></span>
<span><span>    </span><span>query</span><span>,</span></span>
<span><span>):</span></span>
<span><span>    </span><span># ... take a long time ...</span></span>
<span><span>    </span><span># ... llm agent logic to search through the codebase ...</span></span>
<span><span>    </span><span>return</span><span> top_results</span></span></code></pre></div>
<p>Without the cache, searching through the codebase using our LLM agent to get <code dir="ltr">top_results</code> takes 5 minutes - way too long if we&#39;re not actually testing it. Instead with file_cache, we just need to wait for deserialization of the pickled object - basically instantaneous for search results.</p>
<h4>Wrapper<a href="#wrapper" id="wrapper" aria-label="Permalink for this section"></a></h4>
<p>First we store our cache in <code dir="ltr">/tmp/file_cache</code>. This lets us remove the cache by simply deleting the directory (running <code dir="ltr">rm -rf /tmp/file_cache</code>).
We can also selectively remove function calls using <code dir="ltr">rm -rf /tmp/file_cache/search_codebase*</code>.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>def</span><span> </span><span>wrapper</span><span>(</span><span>*</span><span>args</span><span>,</span><span> </span><span>**</span><span>kwargs</span><span>):</span></span>
<span><span>    cache_dir </span><span>=</span><span> </span><span>&#34;/tmp/file_cache&#34;</span></span>
<span><span>    os</span><span>.</span><span>makedirs</span><span>(cache_dir, exist_ok</span><span>=</span><span>True</span><span>)</span></span></code></pre></div>
<p>Then we can create a cache key.</p>
<h4>Cache Key Creation / Miss Conditions<a href="#cache-key-creation--miss-conditions" id="cache-key-creation--miss-conditions" aria-label="Permalink for this section"></a></h4>
<p>We have another problem - we want to miss our cache under two conditions:</p>
<ol>
<li>The arguments to the function change - handled by <code dir="ltr">recursive_hash</code></li>
<li>The code changes</li>
</ol>
<p>To handle 2. we used <code dir="ltr">inspect.getsource(func)</code> to add the function&#39;s source code to the hash, correctly missing the cache when the the code changes.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>    args_names </span><span>=</span><span> func</span><span>.</span><span>__code__</span><span>.</span><span>co_varnames</span><span>[:</span><span> func</span><span>.</span><span>__code__</span><span>.</span><span>co_argcount</span><span>]</span></span>
<span><span>    args_dict </span><span>=</span><span> </span><span>dict</span><span>(</span><span>zip</span><span>(args_names, args))</span></span>
<span> </span>
<span><span>    </span><span># Remove ignored params</span></span>
<span><span>    kwargs_clone </span><span>=</span><span> kwargs</span><span>.</span><span>copy</span><span>()</span></span>
<span><span>    </span><span>for</span><span> param </span><span>in</span><span> ignore_params</span><span>:</span></span>
<span><span>        args_dict</span><span>.</span><span>pop</span><span>(param, </span><span>None</span><span>)</span></span>
<span><span>        kwargs_clone</span><span>.</span><span>pop</span><span>(param, </span><span>None</span><span>)</span></span>
<span> </span>
<span><span>    </span><span># Create hash based on argument names, argument values, and function source code</span></span>
<span><span>    arg_hash </span><span>=</span><span> (</span></span>
<span><span>        </span><span>recursive_hash</span><span>(args_dict, ignore_params</span><span>=</span><span>ignore_params)</span></span>
<span><span>        </span><span>+</span><span> </span><span>recursive_hash</span><span>(kwargs_clone, ignore_params</span><span>=</span><span>ignore_params)</span></span>
<span><span>        </span><span>+</span><span> </span><span data-rehype-pretty-code-wrapper="true"><span>hash_code</span><span>(inspect.</span><span>getsource</span><span>(func))</span></span></span>
<span><span>    )</span></span>
<span><span>    cache_file </span><span>=</span><span> os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span></span>
<span><span>        cache_dir, </span><span>f</span><span>&#34;</span><span>{</span><span>func.</span><span>__module__}</span><span>_</span><span>{</span><span>func.</span><span>__name__}</span><span>_</span><span>{</span><span>arg_hash</span><span>}</span><span>.pickle&#34;</span></span>
<span><span>    )</span></span></code></pre></div>
<h4>Cache hits and misses<a href="#cache-hits-and-misses" id="cache-hits-and-misses" aria-label="Permalink for this section"></a></h4>
<p>Finally we check cache key existence and write to the cache in the case of a cache miss.</p>
<div><pre data-language="python" data-theme="default"><code dir="ltr" data-language="python" data-theme="default"><span><span>    </span><span>try</span><span>:</span></span>
<span><span>        </span><span># If cache exists, load and return it</span></span>
<span><span>        </span><span>if</span><span> os</span><span>.</span><span>path</span><span>.</span><span>exists</span><span>(cache_file):</span></span>
<span><span>            </span><span>if</span><span> verbose</span><span>:</span></span>
<span><span>                </span><span>print</span><span>(</span><span>&#34;Used cache for function: &#34;</span><span> </span><span>+</span><span> func.</span><span>__name__</span><span>)</span></span>
<span><span>            </span><span>with</span><span> </span><span>open</span><span>(cache_file, </span><span>&#34;rb&#34;</span><span>)</span><span> </span><span>as</span><span> f</span><span>:</span></span>
<span><span>                </span><span>return</span><span> pickle</span><span>.</span><span>load</span><span>(f)</span></span>
<span><span>    </span><span>except</span><span> </span><span>Exception</span><span>:</span></span>
<span><span>        logger</span><span>.</span><span>info</span><span>(</span><span>&#34;Unpickling failed&#34;</span><span>)</span></span>
<span> </span>
<span><span>    </span><span># Otherwise, call the function and save its result to the cache</span></span>
<span><span>    result </span><span>=</span><span> </span><span>func</span><span>(</span><span>*</span><span>args, </span><span>**</span><span>kwargs)</span></span>
<span><span>    </span><span>try</span><span>:</span></span>
<span><span>        </span><span>with</span><span> </span><span>open</span><span>(cache_file, </span><span>&#34;wb&#34;</span><span>)</span><span> </span><span>as</span><span> f</span><span>:</span></span>
<span><span>            pickle</span><span>.</span><span>dump</span><span>(result, f)</span></span>
<span><span>    </span><span>except</span><span> </span><span>Exception</span><span> </span><span>as</span><span> e</span><span>:</span></span>
<span><span>        logger</span><span>.</span><span>info</span><span>(</span><span>f</span><span>&#34;Pickling failed: </span><span>{</span><span>e</span><span>}</span><span>&#34;</span><span>)</span></span>
<span><span>    </span><span>return</span><span> result</span></span></code></pre></div>
<h2>Conclusion<a href="#conclusion" id="conclusion" aria-label="Permalink for this section"></a></h2>
<p>We hope this code is useful to you. We&#39;ve found it to be a massive time saver when debugging LLM calls.
We&#39;d love to hear your feedback and contributions at <a href="https://github.com/sweepai/sweep" target="_blank" rel="noreferrer">https://github.com/sweepai/sweep<span> (opens in a new tab)</span></a>!</p></main></article></div>
  </body>
</html>
