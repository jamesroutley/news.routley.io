<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://addxorrol.blogspot.com/2025/07/a-non-anthropomorphized-view-of-llms.html">Original</a>
    <h1>A non-anthropomorphized view of LLMs</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-1029035411984375975" itemprop="description articleBody">
<p>In many discussions where questions of &#34;alignment&#34; or &#34;AI safety&#34; crop up, I am baffled by seriously intelligent people imbuing almost magical human-like powers to something that - in my mind - is just MatMul with interspersed nonlinearities.</p><h3>The space of words</h3><p>The tokenization and embedding step maps individual words (or tokens) to some \(\mathbb{R}^n\) vectors. So let us imagine for a second that we have \(\mathbb{R}^n\) in front of us. A piece of text is then a path through this space - going from word to word to word, tracing a (possibly convoluted) line.</p><p>Imagine now that you label each of the &#34;words&#34; that form the path with a number: The last word with 1, counting forward until you hit the first word or the maximum context length \(c\). If you&#39;ve ever played the game &#34;Snake&#34;, picture something similar, but played in very high-dimensional space - you&#39;re moving forward through space with the tail getting truncated off.</p><p>The LLM takes your previous path into account, calculates probabilities for the next point to go to, and then makes a random pick into the next point according to these probabilities. An LLM instantiated with a fixed random seed is a mapping of the form \((\mathbb{R}^n)^c \mapsto (\mathbb{R}^n)^c\).</p><p>In my mind, the paths generated by these mappings look a lot like <a href="https://en.m.wikipedia.org/wiki/Attractor">strange attractors</a> in dynamical systems - complicated, convoluted paths that are structured-ish.</p><h3>Learning the mapping</h3><p>We obtain this mapping by training it to mimic human text. For this, we use approximately all human writing we can obtain, plus corpora written by human experts on a particular topic, plus some automatically generated pieces of text in domains where we can automatically generate and validate them.</p><p><h3>Paths to avoid</h3></p><div><p>There are certain language sequences we wish to avoid - because the sequences these models generate try to mimic human speech in all it&#39;s empirical structure, but we feel that some of the things that humans have empirically written are very undesirable to be generated. We also feel that a variety of other paths should ideally not be generated, if - when interpreted by either humans or other computer systems - undesirable results arise.</p></div><div><p>Alignment and safety for LLMs mean that <b>we should be able to quantify and bound the probability with which certain undesirable sequences are generated.</b> The trouble is that we largely fail at describing &#34;undesirable&#34; except by example, which makes calculating bounds difficult.</p></div><h3>The surprising utility of LLMs</h3><div><p>LLMs solve a large number of problems that could previously not be solved algorithmically. NLP (as the field was a few years ago) has largely been solved.</p></div><p>We&#39;re on a pretty steep improvement curve, so I expect the number of currently-intractable problems that these models can solve to keep increasing for a while.</p><h3>Where anthropomorphization loses me</h3><p>The moment that people ascribe properties such as &#34;consciousness&#34; or &#34;ethics&#34; or &#34;values&#34; or &#34;morals&#34; to these learnt mappings is where I tend to get lost. We are speaking about a big recurrence equation that produces a new word, and that stops producing words if we don&#39;t crank the shaft.</p><p>To me, wondering if this contraption will &#34;wake up&#34; is similarly bewildering as if I was to ask a computational meteorologist if he isn&#39;t afraid of his meteorological numerical calculation will &#34;wake up&#34;.</p><p>I am baffled that the AI discussions seem to never move away from treating a function to generate sequences of words as something that resembles a human. Statements such as &#34;an AI agent could become an insider threat so it needs monitoring&#34; are simultaneously unsurprising (you have a randomized sequence generator fed into your shell, literally <b>anything</b> can happen!) and baffling (you talk as if you believe the dice you play with had a mind of their own and could decide to conspire against you).</p><p>Instead of saying &#34;we cannot ensure that no harmful sequences will be generated by our function, partially because we don&#39;t know how to specify and enumerate harmful sequences&#34;, we talk about &#34;behaviors&#34;, &#34;ethical constraints&#34;, and &#34;harmful actions in pursuit of their goals&#34;. All of these are anthropocentric concepts that - in my mind - do not apply to functions or other mathematical objects. And using them muddles the discussion, and our thinking about what we&#39;re doing when we create, analyze, deploy and monitor LLMs.</p><p>This muddles the public discussion. We have many historical examples of humanity ascribing bad random events to &#34;the wrath of god(s)&#34; (earthquakes, famines, etc.), &#34;evil spirits&#34; and so forth. The fact that intelligent highly educated researchers talk about these mathematical objects in anthropomorphic terms makes the technology seem mysterious, scary, and magical.</p><div><p>We should think in terms of &#34;this is a function to generate sequences&#34; and &#34;by providing prefixes we can steer the sequence generation around in the space of words and change the probabilities for output sequences&#34;. And for every possible undesirable output sequence of a length smaller than \(c\), we can pick a context that maximizes the probability of this undesirable output sequence.</p></div><h3>Why many AI luminaries tend to anthropomorphize</h3><p>Perhaps I am fighting windmills, or rather a self-selection bias: A fair number of current AI luminaries have self-selected by their belief that they might be the ones getting to AGI - &#34;creating a god&#34; so to speak, the creation of something like life, as good as or better than humans. You are more likely to choose this career path if you believe that it is feasible, and that current approaches might get you there. Possibly I am asking people to &#34;please let go of the belief that you based your life around&#34; when I am asking for an end to anthropomorphization of LLMs, which won&#39;t fly.</p><h3>Why I think human consciousness isn&#39;t comparable to an LLM</h3><p>The following is uncomfortably philosophical, but: In my worldview, humans are dramatically different things than a function \((\mathbb{R}^n)^c \mapsto (\mathbb{R}^n)^c\). For hundreds of millions of years, nature generated new versions, and only a small number of these versions survived. Human thought is a poorly-understood process, involving enormously many neurons, extremely high-bandwidth input, an extremely complicated cocktail of hormones, constant monitoring of energy levels, and millions of years of harsh selection pressure.</p><p>We understand essentially nothing about it. In contrast to an LLM, given a human and a sequence of words, I cannot begin putting a probability on &#34;will this human generate this sequence&#34;. </p><p>To repeat myself: To me, considering that any human concept such as ethics, will to survive, or fear, apply to an LLM appears similarly strange as if we were discussing the feelings of a numerical meteorology simulation.</p><p><h3>The real issues</h3></p><p>The function class represented by modern LLMs are very useful. Even if we never get anywhere close to AGI and just deploy the current state of technology everywhere where it might be useful, we will get a dramatically different world. LLMs might end up being similarly impactful as electrification.</p><div><p>My grandfather lived from 1904 to 1981, a period which encompassed moving from gas lamps to electric, the replacement of horse carriages by cars, nuclear power, transistors, all the way to computers. It also spanned two world wars, the rise of Communism and Stalinism, almost the entire lifetime of the USSR and GDR etc. The world on his birth looked nothing like the world when he died.</p></div>

</div></div>
  </body>
</html>
