<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/mozilla-ai/any-llm">Original</a>
    <h1>Show HN: Any-LLM â€“ Lightweight router to access any LLM Provider</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <img src="https://blog.divyendusingh.com/mozilla-ai/any-llm/raw/main/docs/images/any-llm-logo-mark.png" width="20%" alt="Project logo"/>
  </picture></themed-picture>
</p>




<p dir="auto"><code>any-llm</code> offers:</p>
<ul dir="auto">
<li><strong>Simple, unified interface</strong> - one function for all providers, switch models with just a string change</li>
<li><strong>Developer friendly</strong> - full type hints for better IDE support and clear, actionable error messages</li>
<li><strong>Leverages official provider SDKs</strong> when available, reducing maintenance burden and ensuring compatibility</li>
<li><strong>Stays framework-agnostic</strong> so it can be used across different projects and use cases</li>
<li><strong>Actively maintained</strong> - we use this in our own product (<a href="https://github.com/mozilla-ai/any-agent">any-agent</a>) ensuring continued support</li>
<li><strong>No Proxy or Gateway server required</strong> so you don&#39;t need to deal with setting up any other service to talk to whichever LLM provider you need.</li>
</ul>

<p dir="auto">The landscape of LLM provider interfaces presents a fragmented ecosystem with several challenges that <code>any-llm</code> aims to address:</p>
<p dir="auto"><strong>The Challenge with API Standardization:</strong></p>
<p dir="auto">While the OpenAI API has become the de facto standard for LLM provider interfaces, providers implement slight variations. Some providers are fully OpenAI-compatible, while others may have different parameter names, response formats, or feature sets. This creates a need for light wrappers that can gracefully handle these differences while maintaining a consistent interface.</p>
<p dir="auto"><strong>Existing Solutions and Their Limitations:</strong></p>
<ul dir="auto">
<li><strong><a href="https://github.com/BerriAI/litellm">LiteLLM</a></strong>: While popular, it reimplements provider interfaces rather than leveraging official SDKs, which can lead to compatibility issues and unexpected behavior modifications</li>
<li><strong><a href="https://github.com/andrewyng/aisuite/issues">AISuite</a></strong>: Offers a clean, modular approach but lacks active maintenance, comprehensive testing, and modern Python typing standards.</li>
<li><strong><a href="https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models">Framework-specific solutions</a></strong>: Some agent frameworks either depend on LiteLLM or implement their own provider integrations, creating fragmentation</li>
<li><strong><a href="https://openrouter.ai/" rel="nofollow">Proxy Only Solutions</a></strong>: solutions like <a href="https://openrouter.ai/" rel="nofollow">OpenRouter</a> and <a href="https://github.com/Portkey-AI/portkey-python-sdk">Portkey</a> require a hosted proxy to serve as the interface between your code and the LLM provider.</li>
</ul>


<ul dir="auto">
<li>Python 3.11 or newer</li>
<li>API_KEYS to access to whichever LLM you choose to use.</li>
</ul>

<p dir="auto">In your pip install, include the <a href="https://blog.divyendusingh.com/mozilla-ai/any-llm/blob/main/docs/providers.md">supported providers</a> that you plan on using, or use the <code>all</code> option if you want to install support for all <code>any-llm</code> supported providers.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install &#39;any-llm-sdk[mistral,ollama]&#39;"><pre>pip install <span><span>&#39;</span>any-llm-sdk[mistral,ollama]<span>&#39;</span></span></pre></div>
<p dir="auto">Make sure you have the appropriate API key environment variable set for your provider. Alternatively,
you could use the <code>api_key</code> parameter when making a completion call instead of setting an environment variable.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export MISTRAL_API_KEY=&#34;YOUR_KEY_HERE&#34;  # or OPENAI_API_KEY, etc"><pre><span>export</span> MISTRAL_API_KEY=<span><span>&#34;</span>YOUR_KEY_HERE<span>&#34;</span></span>  <span><span>#</span> or OPENAI_API_KEY, etc</span></pre></div>

<p dir="auto">The provider_id key of the model should be specified according the <a href="https://blog.divyendusingh.com/mozilla-ai/any-llm/blob/main/docs/providers.md">provider ids supported by any-llm</a>.
The <code>model_id</code> portion is passed directly to the provider internals: to understand what model ids are available for a provider,
you will need to refer to the provider documentation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from any_llm import completion
import os

# Make sure you have the appropriate environment variable set
assert os.environ.get(&#39;MISTRAL_API_KEY&#39;)
# Basic completion
response = completion(
    model=&#34;mistral/mistral-small-latest&#34;, # &lt;provider_id&gt;/&lt;model_id&gt;
    messages=[{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello!&#34;}]
)
print(response.choices[0].message.content)"><pre><span>from</span> <span>any_llm</span> <span>import</span> <span>completion</span>
<span>import</span> <span>os</span>

<span># Make sure you have the appropriate environment variable set</span>
<span>assert</span> <span>os</span>.<span>environ</span>.<span>get</span>(<span>&#39;MISTRAL_API_KEY&#39;</span>)
<span># Basic completion</span>
<span>response</span> <span>=</span> <span>completion</span>(
    <span>model</span><span>=</span><span>&#34;mistral/mistral-small-latest&#34;</span>, <span># &lt;provider_id&gt;/&lt;model_id&gt;</span>
    <span>messages</span><span>=</span>[{<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Hello!&#34;</span>}]
)
<span>print</span>(<span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>)</pre></div>
</article></div></div>
  </body>
</html>
