<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://duckdb.org/2021/12/03/duck-arrow.html">Original</a>
    <h1>DuckDB quacks Arrow: A zero-copy data integration between Arrow and DuckDB</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<div>
		
			<div>
				<div>
					
						<div>
							<p><span>2021-12-03</span><span>Pedro Holanda and Jonathan Keane</span></p>
							
							<p><em>TLDR: The zero-copy integration between DuckDB and Apache Arrow allows for rapid analysis of larger than memory datasets in Python and R using either SQL or relational APIs.</em></p>

<p>This post is a collaboration with and cross-posted on <a href="https://arrow.apache.org/blog/2021/12/03/arrow-duckdb/">the Arrow blog</a>.</p>

<p>Part of <a href="https://arrow.apache.org">Apache Arrow</a> is an in-memory data format optimized for analytical libraries. Like Pandas and R Dataframes, it uses a columnar data model. But the Arrow project contains more than just the format: The Arrow C++ library, which is accessible in Python, R, and Ruby via bindings, has additional features that allow you to compute efficiently on datasets. These additional features are on top of the implementation of the in-memory format described above. The datasets may span multiple files in Parquet, CSV, or other formats, and files may even be on remote or cloud storage like HDFS or Amazon S3. The Arrow C++ query engine supports the streaming of query results, has an efficient implementation of complex data types (e.g., Lists, Structs, Maps), and can perform important scan optimizations like Projection and Filter Pushdown.</p>

<p><a href="https://www.duckdb.org">DuckDB</a> is a new analytical data management system that is designed to run complex SQL queries within other processes. DuckDB has bindings for R and Python, among others. DuckDB can query Arrow datasets directly and stream query results back to Arrow. This integration allows users to query Arrow data using DuckDB’s SQL Interface and API, while taking advantage of DuckDB’s parallel vectorized execution engine, without requiring any extra data copying. Additionally, this integration takes full advantage of Arrow’s predicate and filter pushdown while scanning datasets.</p>

<p>This integration is unique because it uses zero-copy streaming of data between DuckDB and Arrow and vice versa so that you can compose a query using both together. This results in three main benefits:</p>

<ol>
  <li><strong>Larger Than Memory Analysis:</strong> Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.</li>
  <li><strong>Complex Data Types:</strong> DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.</li>
  <li><strong>Advanced Optimizer:</strong> DuckDB’s state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.</li>
</ol>

<p>For those that are just interested in benchmarks, you can jump ahead <a href="#Benchmark Comparison">benchmark section below</a>.</p>
      <h2 id="quick-tour">
        
        <a href="#quick-tour">Quick Tour</a>
        
      </h2>
    
<p>Before diving into the details of the integration, in this section we provide a quick motivating example of how powerful and simple to use is the DuckDB-Arrow integration. With a few lines of code, you can already start querying Arrow datasets. Say you want to analyze the infamous <a href="https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page">NYC Taxi Dataset</a> and figure out if groups tip more or less than single riders.</p>
      <h3 id="r">
        
        <a href="#r">R</a>
        
      </h3>
    
<p>Both Arrow and DuckDB support dplyr pipelines for people more comfortable with using dplyr for their data analysis. The Arrow package includes two helper functions that allow us to pass data back and forth between Arrow and DuckDB (<code>to_duckdb()</code> and <code>to_arrow()</code>).
This is especially useful in cases where something is supported in one of Arrow or DuckDB but not the other. For example, if you find a complex dplyr pipeline where the SQL translation doesn’t work with DuckDB, use <code>to_arrow()</code> before the pipeline to use the Arrow engine. Or, if you have a function (e.g., windowed aggregates) that aren’t yet implemented in Arrow, use <code>to_duckdb()</code> to use the DuckDB engine. All while not paying any cost to (re)serialize the data when you pass it back and forth!</p>

<div><div><pre><code><span>library</span><span>(</span><span>duckdb</span><span>)</span><span>
</span><span>library</span><span>(</span><span>arrow</span><span>)</span><span>
</span><span>library</span><span>(</span><span>dplyr</span><span>)</span><span>

</span><span># Open dataset using year,month folder partition</span><span>
</span><span>ds</span><span> </span><span>&lt;-</span><span> </span><span>arrow</span><span>::</span><span>open_dataset</span><span>(</span><span>&#34;nyc-taxi&#34;</span><span>,</span><span> </span><span>partitioning</span><span> </span><span>=</span><span> </span><span>c</span><span>(</span><span>&#34;year&#34;</span><span>,</span><span> </span><span>&#34;month&#34;</span><span>))</span><span>

</span><span>ds</span><span> </span><span>%&gt;%</span><span>
  </span><span># Look only at 2015 on, where the number of passenger is positive, the trip distance is</span><span>
  </span><span># greater than a quarter mile, and where the fare amount is positive</span><span>
  </span><span>filter</span><span>(</span><span>year</span><span> </span><span>&gt;</span><span> </span><span>2014</span><span> </span><span>&amp;</span><span> </span><span>passenger_count</span><span> </span><span>&gt;</span><span> </span><span>0</span><span> </span><span>&amp;</span><span> </span><span>trip_distance</span><span> </span><span>&gt;</span><span> </span><span>0.25</span><span> </span><span>&amp;</span><span> </span><span>fare_amount</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span><span> </span><span>%&gt;%</span><span>
  </span><span># Pass off to DuckDB</span><span>
  </span><span>to_duckdb</span><span>()</span><span> </span><span>%&gt;%</span><span>
  </span><span>group_by</span><span>(</span><span>passenger_count</span><span>)</span><span> </span><span>%&gt;%</span><span>
  </span><span>mutate</span><span>(</span><span>tip_pct</span><span> </span><span>=</span><span> </span><span>tip_amount</span><span> </span><span>/</span><span> </span><span>fare_amount</span><span>)</span><span> </span><span>%&gt;%</span><span>
  </span><span>summarise</span><span>(</span><span>
    </span><span>fare_amount</span><span> </span><span>=</span><span> </span><span>mean</span><span>(</span><span>fare_amount</span><span>,</span><span> </span><span>na.rm</span><span> </span><span>=</span><span> </span><span>TRUE</span><span>),</span><span>
    </span><span>tip_amount</span><span> </span><span>=</span><span> </span><span>mean</span><span>(</span><span>tip_amount</span><span>,</span><span> </span><span>na.rm</span><span> </span><span>=</span><span> </span><span>TRUE</span><span>),</span><span>
    </span><span>tip_pct</span><span> </span><span>=</span><span> </span><span>mean</span><span>(</span><span>tip_pct</span><span>,</span><span> </span><span>na.rm</span><span> </span><span>=</span><span> </span><span>TRUE</span><span>)</span><span>
  </span><span>)</span><span> </span><span>%&gt;%</span><span>
  </span><span>arrange</span><span>(</span><span>passenger_count</span><span>)</span><span> </span><span>%&gt;%</span><span>
  </span><span>collect</span><span>()</span><span>
</span></code></pre></div></div>
      <h3 id="python">
        
        <a href="#python">Python</a>
        
      </h3>
    
<p>The workflow in Python is as simple as it is in R. In this example we use DuckDB’s Relational API.</p>

<div><div><pre><code><span>import</span> <span>duckdb</span>
<span>import</span> <span>pyarrow</span> <span>as</span> <span>pa</span>
<span>import</span> <span>pyarrow.dataset</span> <span>as</span> <span>ds</span>

<span># Open dataset using year,month folder partition
</span><span>nyc</span> <span>=</span> <span>ds</span><span>.</span><span>dataset</span><span>(</span><span>&#39;nyc-taxi/&#39;</span><span>,</span> <span>partitioning</span><span>=</span><span>[</span><span>&#34;year&#34;</span><span>,</span> <span>&#34;month&#34;</span><span>])</span>

<span># We transform the nyc dataset into a DuckDB relation
</span><span>nyc</span> <span>=</span> <span>duckdb</span><span>.</span><span>arrow</span><span>(</span><span>nyc</span><span>)</span>

<span># Run same query again
</span><span>nyc</span><span>.</span><span>filter</span><span>(</span><span>&#34;year &gt; 2014 &amp; passenger_count &gt; 0 &amp; trip_distance &gt; 0.25 &amp; fare_amount &gt; 0&#34;</span><span>)</span>
    <span>.</span><span>aggregate</span><span>(</span><span>&#34;SELECT AVG(fare_amount), AVG(tip_amount), AVG(tip_amount / fare_amount) as tip_pct&#34;</span><span>,</span><span>&#34;passenger_count&#34;</span><span>).</span><span>arrow</span><span>()</span>
</code></pre></div></div>
      <h2 id="duckdb-and-arrow-the-basics">
        
        <a href="#duckdb-and-arrow-the-basics">DuckDB and Arrow: The Basics</a>
        
      </h2>
    

<p>In this section, we will look at some basic examples of the code needed to read and output Arrow tables in both Python and R.</p>
      <h4 id="setup">
        
        <a href="#setup">Setup</a>
        
      </h4>
    

<p>First we need to install DuckDB and Arrow. The installation process for both libraries in Python and R is shown below.</p>
<div><div><pre><code><span># Python Install</span>
pip <span>install </span>duckdb
pip <span>install </span>pyarrow
</code></pre></div></div>

<div><div><pre><code><span># R Install</span><span>
</span><span>install.packages</span><span>(</span><span>&#34;duckdb&#34;</span><span>)</span><span>
</span><span>install.packages</span><span>(</span><span>&#34;arrow&#34;</span><span>)</span><span>
</span></code></pre></div></div>

<p>To execute the sample-examples in this section, we need to download the following custom parquet files:</p>
<ul>
  <li>https://github.com/duckdb/duckdb-web/blob/master/_posts/data/integers.parquet?raw=true</li>
  <li>https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet</li>
</ul>
      <h4 id="python-1">
        
        <a href="#python-1">Python</a>
        
      </h4>
    

<p>There are two ways in Python of querying data from Arrow:</p>
<ol>
  <li>Through the Relational API</li>
</ol>

<div><div><pre><code><span># Reads Parquet File to an Arrow Table
</span><span>arrow_table</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;integers.parquet&#39;</span><span>)</span>

<span># Transforms Arrow Table -&gt; DuckDB Relation
</span><span>rel_from_arrow</span> <span>=</span> <span>duckdb</span><span>.</span><span>arrow</span><span>(</span><span>arrow_table</span><span>)</span>

<span># we can run a SQL query on this and print the result
</span><span>print</span><span>(</span><span>rel_from_arrow</span><span>.</span><span>query</span><span>(</span><span>&#39;arrow_table&#39;</span><span>,</span> <span>&#39;SELECT SUM(data) FROM arrow_table WHERE data &gt; 50&#39;</span><span>).</span><span>fetchone</span><span>())</span>

<span># Transforms DuckDB Relation -&gt; Arrow Table
</span><span>arrow_table_from_duckdb</span> <span>=</span> <span>rel_from_arrow</span><span>.</span><span>arrow</span><span>()</span>
</code></pre></div></div>

<ol>
  <li>By using replacement scans and querying the object directly with SQL:</li>
</ol>

<div><div><pre><code><span># Reads Parquet File to an Arrow Table
</span><span>arrow_table</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;integers.parquet&#39;</span><span>)</span>

<span># Gets Database Connection
</span><span>con</span> <span>=</span> <span>duckdb</span><span>.</span><span>connect</span><span>()</span>

<span># we can run a SQL query on this and print the result
</span><span>print</span><span>(</span><span>con</span><span>.</span><span>execute</span><span>(</span><span>&#39;SELECT SUM(data) FROM arrow_table WHERE data &gt; 50&#39;</span><span>).</span><span>fetchone</span><span>())</span>

<span># Transforms Query Result from DuckDB to Arrow Table
# We can directly read the arrow object through DuckDB&#39;s replacement scans.
</span><span>con</span><span>.</span><span>execute</span><span>(</span><span>&#34;SELECT * FROM arrow_table&#34;</span><span>).</span><span>fetch_arrow_table</span><span>()</span>
</code></pre></div></div>

<p>It is possible to transform both DuckDB Relations and Query Results back to Arrow.</p>
      <h4 id="r-1">
        
        <a href="#r-1">R</a>
        
      </h4>
    

<p>In R, you can interact with Arrow data in DuckDB by registering the table as a view (an alternative is to use dplyr as shown above).</p>
<div><div><pre><code><span>library</span><span>(</span><span>duckdb</span><span>)</span><span>
</span><span>library</span><span>(</span><span>arrow</span><span>)</span><span>
</span><span>library</span><span>(</span><span>dplyr</span><span>)</span><span>

</span><span># Reads Parquet File to an Arrow Table</span><span>
</span><span>arrow_table</span><span> </span><span>&lt;-</span><span> </span><span>arrow</span><span>::</span><span>read_parquet</span><span>(</span><span>&#34;integers.parquet&#34;</span><span>,</span><span> </span><span>as_data_frame</span><span> </span><span>=</span><span> </span><span>FALSE</span><span>)</span><span>

</span><span># Gets Database Connection</span><span>
</span><span>con</span><span> </span><span>&lt;-</span><span> </span><span>dbConnect</span><span>(</span><span>duckdb</span><span>::</span><span>duckdb</span><span>())</span><span>

</span><span># Registers arrow table as a DuckDB view</span><span>
</span><span>arrow</span><span>::</span><span>to_duckdb</span><span>(</span><span>arrow_table</span><span>,</span><span> </span><span>table_name</span><span> </span><span>=</span><span> </span><span>&#34;arrow_table&#34;</span><span>,</span><span> </span><span>con</span><span> </span><span>=</span><span> </span><span>con</span><span>)</span><span>

</span><span># we can run a SQL query on this and print the result</span><span>
</span><span>print</span><span>(</span><span>dbGetQuery</span><span>(</span><span>con</span><span>,</span><span> </span><span>&#34;SELECT SUM(data) FROM arrow_table WHERE data &gt; 50&#34;</span><span>))</span><span>

</span><span># Transforms Query Result from DuckDB to Arrow Table</span><span>
</span><span>result</span><span> </span><span>&lt;-</span><span> </span><span>dbSendQuery</span><span>(</span><span>con</span><span>,</span><span> </span><span>&#34;SELECT * FROM arrow_table&#34;</span><span>)</span><span>
</span></code></pre></div></div>
      <h3 id="streaming-data-fromto-arrow">
        
        <a href="#streaming-data-fromto-arrow">Streaming Data from/to Arrow</a>
        
      </h3>
    
<p>In the previous section, we depicted how to interact with Arrow tables. However, Arrow also allows users to interact with the data in a streaming fashion. Either consuming it (e.g., from an Arrow Dataset) or producing it (e.g., returning a RecordBatchReader). And of course, DuckDB is able to consume Datasets and produce RecordBatchReaders. This example uses the NYC Taxi Dataset, stored in Parquet files partitioned by year and month, which we can download through the Arrow R package:</p>
<div><div><pre><code><span>arrow</span><span>::</span><span>copy_files</span><span>(</span><span>&#34;s3://ursa-labs-taxi-data&#34;</span><span>,</span><span> </span><span>&#34;nyc-taxi&#34;</span><span>)</span><span>
</span></code></pre></div></div>
      <h4 id="python-2">
        
        <a href="#python-2">Python</a>
        
      </h4>
    
<div><div><pre><code><span># Reads dataset partitioning it in year/month folder
</span><span>nyc_dataset</span> <span>=</span> <span>ds</span><span>.</span><span>dataset</span><span>(</span><span>&#39;nyc-taxi/&#39;</span><span>,</span> <span>partitioning</span><span>=</span><span>[</span><span>&#34;year&#34;</span><span>,</span> <span>&#34;month&#34;</span><span>])</span>

<span># Gets Database Connection
</span><span>con</span> <span>=</span> <span>duckdb</span><span>.</span><span>connect</span><span>()</span>

<span>query</span> <span>=</span> <span>con</span><span>.</span><span>execute</span><span>(</span><span>&#34;SELECT * FROM nyc_dataset&#34;</span><span>)</span>
<span># DuckDB&#39;s queries can now produce a Record Batch Reader
</span><span>record_batch_reader</span> <span>=</span> <span>query</span><span>.</span><span>fetch_record_batch</span><span>()</span>
<span># Which means we can stream the whole query per batch.
# This retrieves the first batch
</span><span>chunk</span> <span>=</span> <span>record_batch_reader</span><span>.</span><span>read_next_batch</span><span>()</span>
</code></pre></div></div>
      <h4 id="r-2">
        
        <a href="#r-2">R</a>
        
      </h4>
    
<div><div><pre><code><span># Reads dataset partitioning it in year/month folder</span><span>
</span><span>nyc_dataset</span><span> </span><span>=</span><span> </span><span>open_dataset</span><span>(</span><span>&#34;nyc-taxi/&#34;</span><span>,</span><span> </span><span>partitioning</span><span> </span><span>=</span><span> </span><span>c</span><span>(</span><span>&#34;year&#34;</span><span>,</span><span> </span><span>&#34;month&#34;</span><span>))</span><span>

</span><span># Gets Database Connection</span><span>
</span><span>con</span><span> </span><span>&lt;-</span><span> </span><span>dbConnect</span><span>(</span><span>duckdb</span><span>::</span><span>duckdb</span><span>())</span><span>

</span><span># We can use the same function as before to register our arrow dataset</span><span>
</span><span>duckdb</span><span>::</span><span>duckdb_register_arrow</span><span>(</span><span>con</span><span>,</span><span> </span><span>&#34;nyc&#34;</span><span>,</span><span> </span><span>nyc_dataset</span><span>)</span><span>

</span><span>res</span><span> </span><span>&lt;-</span><span> </span><span>dbSendQuery</span><span>(</span><span>con</span><span>,</span><span> </span><span>&#34;SELECT * FROM nyc&#34;</span><span>,</span><span> </span><span>arrow</span><span> </span><span>=</span><span> </span><span>TRUE</span><span>)</span><span>
</span><span># DuckDB&#39;s queries can now produce a Record Batch Reader</span><span>
</span><span>record_batch_reader</span><span> </span><span>&lt;-</span><span> </span><span>duckdb</span><span>::</span><span>duckdb_fetch_record_batch</span><span>(</span><span>res</span><span>)</span><span>

</span><span># Which means we can stream the whole query per batch.</span><span>
</span><span># This retrieves the first batch</span><span>
</span><span>cur_batch</span><span> </span><span>&lt;-</span><span> </span><span>record_batch_reader</span><span>$</span><span>read_next_batch</span><span>()</span><span>
</span></code></pre></div></div>

<p>The preceding R code shows in low-level detail how the data is streaming. We provide the helper <code>to_arrow()</code> in the Arrow package which is a wrapper around this that makes it easy to incorporate this streaming into a dplyr pipeline. <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>
      <h2 id="benchmark-comparison">
        
        <a href="#benchmark-comparison">Benchmark Comparison</a>
        
      </h2>
    

<p>Here we demonstrate in a simple benchmark the performance difference between querying Arrow datasets with DuckDB and querying Arrow datasets with Pandas.
For both the Projection and Filter pushdown comparison, we will use Arrow tables. That is due to Pandas not being capable of consuming Arrow stream objects.</p>

<p>For the NYC Taxi benchmarks, we used the <a href="https://www.monetdb.org/wiki/Scilens-configuration-standard">scilens diamonds configuration</a> and for the TPC-H benchmarks, we used an m1 MacBook Pro. In both cases, parallelism in DuckDB was used (which is now on by default).</p>

<p>For the comparison with Pandas, note that DuckDB runs in parallel, while pandas only support single-threaded execution. Besides that, one should note that we are comparing automatic optimizations. DuckDB’s query optimizer can automatically push down filters and projections. This automatic optimization is not supported in pandas, but it is possible for users to manually perform some of these predicate and filter pushdowns by manually specifying them them in the <code>read_parquet()</code> call.</p>
      <h3 id="projection-pushdown">
        
        <a href="#projection-pushdown">Projection Pushdown</a>
        
      </h3>
    

<p>In this example we run a simple aggregation on two columns of our lineitem table.</p>

<div><div><pre><code><span># DuckDB
</span><span>lineitem</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;lineitemsf1.snappy.parquet&#39;</span><span>)</span>
<span>con</span> <span>=</span> <span>duckdb</span><span>.</span><span>connect</span><span>()</span>

<span># Transforms Query Result from DuckDB to Arrow Table
</span><span>con</span><span>.</span><span>execute</span><span>(</span><span>&#34;&#34;&#34;SELECT sum(l_extendedprice * l_discount) AS revenue
                FROM
                lineitem;&#34;&#34;&#34;</span><span>).</span><span>fetch_arrow_table</span><span>()</span>

</code></pre></div></div>

<div><div><pre><code><span># Pandas
</span><span>arrow_table</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;lineitemsf1.snappy.parquet&#39;</span><span>)</span>

<span># Converts an Arrow table to a Dataframe
</span><span>df</span> <span>=</span> <span>arrow_table</span><span>.</span><span>to_pandas</span><span>()</span>

<span># Runs aggregation
</span><span>res</span> <span>=</span>  <span>pd</span><span>.</span><span>DataFrame</span><span>({</span><span>&#39;sum&#39;</span><span>:</span> <span>[(</span><span>df</span><span>.</span><span>l_extendedprice</span> <span>*</span> <span>df</span><span>.</span><span>l_discount</span><span>).</span><span>sum</span><span>()]})</span>

<span># Creates an Arrow Table from a Dataframe
</span><span>new_table</span> <span>=</span> <span>pa</span><span>.</span><span>Table</span><span>.</span><span>from_pandas</span><span>(</span><span>res</span><span>)</span>

</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td>0.19</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td>2.13</td>
    </tr>
  </tbody>
</table>

<p>The lineitem table is composed of 16 columns, however, to execute this query only two columns <code>l_extendedprice</code> and  *  <code>l_discount</code> are necessary. Since DuckDB can push down the projection of these columns, it is capable of executing this query about one order of magnitude faster than Pandas.</p>
      <h3 id="filter-pushdown">
        
        <a href="#filter-pushdown">Filter Pushdown</a>
        
      </h3>
    

<p>For our filter pushdown we repeat the same aggregation used in the previous section, but add filters on 4 more columns.</p>

<div><div><pre><code><span># DuckDB
</span><span>lineitem</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;lineitemsf1.snappy.parquet&#39;</span><span>)</span>

<span># Get database connection
</span><span>con</span> <span>=</span> <span>duckdb</span><span>.</span><span>connect</span><span>()</span>

<span># Transforms Query Result from DuckDB to Arrow Table
</span><span>con</span><span>.</span><span>execute</span><span>(</span><span>&#34;&#34;&#34;SELECT sum(l_extendedprice * l_discount) AS revenue
        FROM
            lineitem
        WHERE
            l_shipdate &gt;= CAST(&#39;1994-01-01&#39; AS date)
            AND l_shipdate &lt; CAST(&#39;1995-01-01&#39; AS date)
            AND l_discount BETWEEN 0.05
            AND 0.07
            AND l_quantity &lt; 24; &#34;&#34;&#34;</span><span>).</span><span>fetch_arrow_table</span><span>()</span>

</code></pre></div></div>

<div><div><pre><code><span># Pandas
</span><span>arrow_table</span> <span>=</span> <span>pq</span><span>.</span><span>read_table</span><span>(</span><span>&#39;lineitemsf1.snappy.parquet&#39;</span><span>)</span>

<span>df</span> <span>=</span> <span>arrow_table</span><span>.</span><span>to_pandas</span><span>()</span>
<span>filtered_df</span> <span>=</span> <span>lineitem</span><span>[</span>
        <span>(</span><span>lineitem</span><span>.</span><span>l_shipdate</span> <span>&gt;=</span> <span>&#34;1994-01-01&#34;</span><span>)</span> <span>&amp;</span>
        <span>(</span><span>lineitem</span><span>.</span><span>l_shipdate</span> <span>&lt;</span> <span>&#34;1995-01-01&#34;</span><span>)</span> <span>&amp;</span>
        <span>(</span><span>lineitem</span><span>.</span><span>l_discount</span> <span>&gt;=</span> <span>0.05</span><span>)</span> <span>&amp;</span>
        <span>(</span><span>lineitem</span><span>.</span><span>l_discount</span> <span>&lt;=</span> <span>0.07</span><span>)</span> <span>&amp;</span>
        <span>(</span><span>lineitem</span><span>.</span><span>l_quantity</span> <span>&lt;</span> <span>24</span><span>)]</span>

<span>res</span> <span>=</span>  <span>pd</span><span>.</span><span>DataFrame</span><span>({</span><span>&#39;sum&#39;</span><span>:</span> <span>[(</span><span>filtered_df</span><span>.</span><span>l_extendedprice</span> <span>*</span> <span>filtered_df</span><span>.</span><span>l_discount</span><span>).</span><span>sum</span><span>()]})</span>
<span>new_table</span> <span>=</span> <span>pa</span><span>.</span><span>Table</span><span>.</span><span>from_pandas</span><span>(</span><span>res</span><span>)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td>0.04</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td>2.29</td>
    </tr>
  </tbody>
</table>

<p>The difference now between DuckDB and Pandas is more drastic, being two orders of magnitude faster than Pandas. Again, since both the filter and projection are pushed down to Arrow, DuckDB reads less data than Pandas, which can’t automatically perform this optimization.</p>
      <h3 id="streaming">
        
        <a href="#streaming">Streaming</a>
        
      </h3>
    

<p>As demonstrated before, DuckDB is capable of consuming and producing Arrow data in a streaming fashion. In this section we run a simple benchmark, to showcase the benefits in speed and memory usage when comparing it to full materialization and Pandas. This example uses the full NYC taxi dataset which you can download</p>

<div><div><pre><code><span># DuckDB
# Open dataset using year,month folder partition
</span><span>nyc</span> <span>=</span> <span>ds</span><span>.</span><span>dataset</span><span>(</span><span>&#39;nyc-taxi/&#39;</span><span>,</span> <span>partitioning</span><span>=</span><span>[</span><span>&#34;year&#34;</span><span>,</span> <span>&#34;month&#34;</span><span>])</span>

<span># Get database connection
</span><span>con</span> <span>=</span> <span>duckdb</span><span>.</span><span>connect</span><span>()</span>

<span># Run query that selects part of the data
</span><span>query</span> <span>=</span> <span>con</span><span>.</span><span>execute</span><span>(</span><span>&#34;SELECT total_amount, passenger_count,year FROM nyc where total_amount &gt; 100 and year &gt; 2014&#34;</span><span>)</span>

<span># Create Record Batch Reader from Query Result.
# &#34;fetch_record_batch()&#34; also accepts an extra parameter related to the desired produced chunk size.
</span><span>record_batch_reader</span> <span>=</span> <span>query</span><span>.</span><span>fetch_record_batch</span><span>()</span>

<span># Retrieve all batch chunks
</span><span>chunk</span> <span>=</span> <span>record_batch_reader</span><span>.</span><span>read_next_batch</span><span>()</span>
<span>while</span> <span>len</span><span>(</span><span>chunk</span><span>)</span> <span>&gt;</span> <span>0</span><span>:</span>
    <span>chunk</span> <span>=</span> <span>record_batch_reader</span><span>.</span><span>read_next_batch</span><span>()</span>
</code></pre></div></div>

<div><div><pre><code><span># Pandas
# We must exclude one of the columns of the NYC dataset due to an unimplemented cast in Arrow.
</span><span>working_columns</span> <span>=</span> <span>[</span><span>&#34;vendor_id&#34;</span><span>,</span><span>&#34;pickup_at&#34;</span><span>,</span><span>&#34;dropoff_at&#34;</span><span>,</span><span>&#34;passenger_count&#34;</span><span>,</span><span>&#34;trip_distance&#34;</span><span>,</span><span>&#34;pickup_longitude&#34;</span><span>,</span>
    <span>&#34;pickup_latitude&#34;</span><span>,</span><span>&#34;store_and_fwd_flag&#34;</span><span>,</span><span>&#34;dropoff_longitude&#34;</span><span>,</span><span>&#34;dropoff_latitude&#34;</span><span>,</span><span>&#34;payment_type&#34;</span><span>,</span>
    <span>&#34;fare_amount&#34;</span><span>,</span><span>&#34;extra&#34;</span><span>,</span><span>&#34;mta_tax&#34;</span><span>,</span><span>&#34;tip_amount&#34;</span><span>,</span><span>&#34;tolls_amount&#34;</span><span>,</span><span>&#34;total_amount&#34;</span><span>,</span><span>&#34;year&#34;</span><span>,</span> <span>&#34;month&#34;</span><span>]</span>

<span># Open dataset using year,month folder partition
</span><span>nyc_dataset</span> <span>=</span> <span>ds</span><span>.</span><span>dataset</span><span>(</span><span>dir</span><span>,</span> <span>partitioning</span><span>=</span><span>[</span><span>&#34;year&#34;</span><span>,</span> <span>&#34;month&#34;</span><span>])</span>
<span># Generate a scanner to skip problematic column
</span><span>dataset_scanner</span> <span>=</span> <span>nyc_dataset</span><span>.</span><span>scanner</span><span>(</span><span>columns</span><span>=</span><span>working_columns</span><span>)</span>

<span># Materialize dataset to an Arrow Table
</span><span>nyc_table</span> <span>=</span> <span>dataset_scanner</span><span>.</span><span>to_table</span><span>()</span>

<span># Generate Dataframe from Arow Table
</span><span>nyc_df</span> <span>=</span> <span>nyc_table</span><span>.</span><span>to_pandas</span><span>()</span>

<span># Apply Filter
</span><span>filtered_df</span> <span>=</span> <span>nyc_df</span><span>[</span>
    <span>(</span><span>nyc_df</span><span>.</span><span>total_amount</span> <span>&gt;</span> <span>100</span><span>)</span> <span>&amp;</span>
    <span>(</span><span>nyc_df</span><span>.</span><span>year</span> <span>&gt;</span><span>2014</span><span>)]</span>

<span># Apply Projection
</span><span>res</span> <span>=</span> <span>filtered_df</span><span>[[</span><span>&#34;total_amount&#34;</span><span>,</span> <span>&#34;passenger_count&#34;</span><span>,</span><span>&#34;year&#34;</span><span>]]</span>

<span># Transform Result back to an Arrow Table
</span><span>new_table</span> <span>=</span> <span>pa</span><span>.</span><span>Table</span><span>.</span><span>from_pandas</span><span>(</span><span>res</span><span>)</span>
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Time (s)</th>
      <th>Peak Memory Usage (GBs)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DuckDB</td>
      <td>0.05</td>
      <td>0.3</td>
    </tr>
    <tr>
      <td>Pandas</td>
      <td>146.91</td>
      <td>248</td>
    </tr>
  </tbody>
</table>

<p>The difference in times between DuckDB and Pandas is a combination of all the integration benefits we explored in this article. In DuckDB the filter pushdown is applied to perform partition elimination (i.e., we skip reading the Parquet files where the year is &lt;= 2014). The filter pushdown is also used to eliminate unrelated row_groups (i.e., row groups where the total amount is always &lt;= 100). Due to our projection pushdown, Arrow only has to read the columns of interest from the Parquet files, which allows it to read only 4 out of 20 columns. On the other hand, Pandas is not capable of automatically pushing down any of these optimizations, which means that the full dataset must be read. <strong>This results in the 4 orders of magnitude difference in query execution time.</strong></p>

<p>In the table above, we also depict the comparison of peak memory usage between DuckDB (Streaming) and Pandas (Fully-Materializing).  In DuckDB, we only need to load the row-group of interest into memory. Hence our memory usage is low. We also have constant memory usage since we only have to keep one of these row groups in-memory at a time. Pandas, on the other hand, has to fully materialize all Parquet files when executing the query. Because of this, we see a constant steep increase in its memory consumption. <strong>The total difference in memory consumption of the two solutions is around 3 orders of magnitude.</strong></p>
      <h2 id="conclusion-and-feedback">
        
        <a href="#conclusion-and-feedback">Conclusion and Feedback</a>
        
      </h2>
    
<p>In this blog post, we mainly showcased how to execute queries on Arrow datasets with DuckDB. There are additional libraries that can also consume the Arrow format but they have different purposes and capabilities. As always, we are happy to hear if you want to see benchmarks with different tools for a post in the future! Feel free to drop us an <a href="https://duckdb.org/cdn-cgi/l/email-protection#b7c7d2d3c5d8f7d3c2d4dcd3d5dbd6d5c499d4d8da8cddd8d9f7c1d8dbc3c5d8d9d3d6c3d699d4d8da">email</a> or share your thoughts directly in the Hacker News post.</p>

<p>Last but not least, if you encounter any problems when using our integration, please open an issue in in either <a href="https://github.com/duckdb/duckdb/issues">DuckDB’s - issue tracker</a>  or <a href="https://issues.apache.org/jira/projects/ARROW/">Arrow’s - issue tracker</a>, depending on which library has a problem.</p>



							<p><a href="https://duckdb.org/news/"><span>←</span> back to news archive</a>
						</p></div>
					
				</div>
			</div>
			
			
		
		
	</div></div></div>
  </body>
</html>
