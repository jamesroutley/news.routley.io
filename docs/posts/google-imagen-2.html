<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://deepmind.google/technologies/imagen-2/">Original</a>
    <h1>Google Imagen 2</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  
    
  
  
  
    

    
    
      
        <div>
          
            
            
              
              <div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="992" height="992" srcset="https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w992-rw 1x, https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w1984-rw 2x"/><source media="(min-width: 1024px)" type="image/webp" width="736" height="736" srcset="https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w736-rw 1x, https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w1472-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="1024" height="1024" srcset="https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w1024-rw 1x, https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w2048-rw 2x"/><source type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w600-rw 1x, https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w1200-rw 2x"/>
      <img alt="A collage of images generated by Imagen" height="736" src="https://lh3.googleusercontent.com/NUrdyTei3-YvxScO7Ur9Dtz-t4liu-9oHQmqC3xlonGQEkZ3e_U_y5IPdpakZWsKk7rGL0xP8hJhDkSlCmIerQAVcS-7jyMd_5OSZ1XViB9V89gEjg=w736" width="736"/>
    </picture>
    
  <div>
        <p>Technology</p>
        
        <p data-block-key="x6ctw">Our most advanced text-to-image technology</p>

        
        
      </div>
    </div>
            
          
            
            
              
              <div>
  <p data-block-key="xzqsf">Imagen 2 is our most advanced text-to-image diffusion technology, delivering high-quality, photorealistic outputs that are closely aligned and consistent with the user’s prompt. It can generate more lifelike images by using the natural distribution of its training data, instead of adopting a pre-programmed style.</p><p data-block-key="dbvt7">Imagen 2’s powerful text-to-image technology is available for developers and Cloud customers via the <a href="https://cloud.google.com/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available" rel="noopener" target="_blank">Imagen API in Google Cloud Vertex AI</a>.</p><p data-block-key="u0i">The Google Arts and Culture team is also deploying our Imagen 2 technology in their <a href="https://artsandculture.google.com/experiment/2QFyx9MN7Inxtg" rel="noopener" target="_blank">Cultural Icons</a> experiment, allowing users to explore, learn and test their cultural knowledge with the help of Google AI.</p>
</div>
            
          
            
            
              
              

<div id="carousel-2dbae751-b94b-4125-b2e3-b564c3e2f521">
  
  <div>
    <div>
          

<figure aria-label="Slide 1" id="carousel-image-ba06b29b-4aa4-4ddb-80b9-d075a70ce393">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h600-rw 1x, https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h520-rw 1x, https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h300-rw 1x, https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h600-rw 2x"/>
      <img alt="A shot of a 32-year-old female, up and coming conservationist in a jungle; athletic with short, curly hair and a warm smile" height="600" loading="lazy" src="https://lh3.googleusercontent.com/I9eBUMXSb_2as6wVlTgcXJSSloPArudkM5Cu0bc00EqBGyvMh2Ce402ciE57f2nT8i3FIPKBlHnvmtGCTPM1bg8pOKRXmB17ZE01jGZycabRpvSfbg=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="8w7w3">Prompt: A shot of a 32-year-old female, up and coming conservationist in a jungle; athletic with short, curly hair and a warm smile</p></figcaption>
</figure>
        
          

<figure aria-label="Slide 2" id="carousel-image-27e77efc-baa1-451e-a0af-54aa6456bbd0">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h600-rw 1x, https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h520-rw 1x, https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h300-rw 1x, https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h600-rw 2x"/>
      <img alt="A jellyfish on a dark blue background" height="600" loading="lazy" src="https://lh3.googleusercontent.com/irRwQJu9MW8P5hOrV9feo_wigQblxyiCR7oddo_EDomL_GXX5l-5DqzCfNuYVdiSGvL4bzqoqNVpt3kcZeEwdR5HAuWx-QRbb3ZMRY--Nth0tm2tLw=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="8w7w3">Prompt: A jellyfish on a dark blue background</p></figcaption>
</figure>
        
          

<figure aria-label="Slide 3" id="carousel-image-19ca9543-53e6-4390-b000-96ae3ff7cecf">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h600-rw 1x, https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h520-rw 1x, https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h300-rw 1x, https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h600-rw 2x"/>
      <img alt="Small canvas oil painting of an orange on a chopping board. Light is passing through orange segments, casting an orange light across part of the chopping board. There is a blue and white cloth in the background. Caustics, bounce light, expressive brush strokes" height="600" loading="lazy" src="https://lh3.googleusercontent.com/ZwJ0QRTYzUIq3qlPcvN3hypd_eFtwFMv9-L8PX_MhfJzYGcX0sdncoWs7V7-PfaCsd8RRoDtf6sEXRvvoTsqV_XI00YlLLvb2kDPuVXy27_uBkMfYg=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="8w7w3">Prompt: Small canvas oil painting of an orange on a chopping board. Light is passing through orange segments, casting an orange light across part of the chopping board. There is a blue and white cloth in the background. Caustics, bounce light, expressive brush strokes</p></figcaption>
</figure>
        </div>
  </div>
  
</div>
            
          
            
            
              
              <div>
  <h2 data-block-key="xzqsf">Improved image-caption understanding</h2><p data-block-key="bjf92">Text-to-image models learn to generate images that match a user’s prompt from details in their training datasets’ images and captions. But the quality of detail and accuracy in these pairings can vary widely for each image and caption.</p><p data-block-key="6uveq">To help create higher-quality and more accurate images that better align to a user’s prompt, further description was added to image captions in Imagen 2’s training dataset, helping Imagen 2 learn different captioning styles and generalize to better understand a broad range of user prompts.</p><p data-block-key="35cmb">These enhanced image-caption pairings help Imagen 2 better understand the relationship between images and words — increasing its understanding of context and nuance.</p><p data-block-key="8oivh">Here are examples of Imagen 2’s prompt understanding:</p>
</div>
            
          
            
            
              
              

<div id="carousel-b2ca4b01-158f-4c17-935f-b70c919a94cc">
  
  <div>
    <div>
          

<figure aria-label="Slide 1" id="carousel-image-0d1ea3c9-54ad-42f7-9ec9-659999d356f9">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h600-rw 1x, https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h520-rw 1x, https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h300-rw 1x, https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h600-rw 2x"/>
      <img alt="AI Image generated from prompt &#34;Soft purl the streams the birds renew their notes and through the air their mingled music floats&#34; (A Hymn to the Evening by Phillis Wheatley)" height="600" loading="lazy" src="https://lh3.googleusercontent.com/LYjZSwmz1R3i4YAlTT8wDbQebsvp6E88wFML1tCmleZjs-2160GBnbJDWq_CD2AbUmlFpzc4aGpI5pl4JpzTInMHMUkU42hWo9oCRJSl7D5xAKBUuJE=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="ufy5j"><i>Prompt: “Soft purl the streams, the birds renew their notes, And through the air their mingled music floats.” (</i>A Hymn to the Evening by Phillis Wheatley<i>)</i></p></figcaption>
</figure>
        
          

<figure aria-label="Slide 2" id="carousel-image-378d2b14-cf7f-46a9-ae51-9fbf585cc1be">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h600-rw 1x, https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h520-rw 1x, https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h300-rw 1x, https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h600-rw 2x"/>
      <img alt="AI generated image of a painted underwater scene." height="600" loading="lazy" src="https://lh3.googleusercontent.com/S2jdmKaiItO1hHHG7EDNWZaR7yptbdySsEjPBv_S3QmN40d5c9sSMt0f80CU2uFhAbP788vJq9UKGRcjENhIBnv39cihhL1HaX9-iaxakjxAYubCRsc=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="ufy5j">Prompt: “<i>Consider the subtleness of the sea; how its most dreaded creatures glide under water, unapparent for the most part, and treacherously hidden beneath the loveliest tints of azure</i>.&#34; (Moby-Dick by Herman Melville)</p></figcaption>
</figure>
        
          

<figure aria-label="Slide 3" id="carousel-image-3eaa9bb0-1cd7-4892-ad3c-2c0f10e9e3b4">
  
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="600" height="600" srcset="https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h600-rw 1x, https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h1200-rw 2x"/><source media="(min-width: 600px)" type="image/webp" width="520" height="520" srcset="https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h520-rw 1x, https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h1040-rw 2x"/><source type="image/webp" width="300" height="300" srcset="https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h300-rw 1x, https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h600-rw 2x"/>
      <img alt="AI generated photo-realistic image of a singing robin" height="600" loading="lazy" src="https://lh3.googleusercontent.com/eAOVatYEvNbrH4W31iA5M1GJ8-piO7LrOtAMF4NYQOIVVssiRq8g9ovi3T2LUpbwHgnm9faGPRraYTmjckdfFbHuJ26D7K3pzxjygX3Thj399kXLww=h600" width="600"/>
    </picture>
    
  
  <figcaption><p data-block-key="ufy5j">Prompt: ”<i>The robin flew from his swinging spray of ivy on to the top of the wall and he opened his beak and sang a loud, lovely trill, merely to show off. Nothing in the world is quite as adorably lovely as a robin when he shows off - and they are nearly always doing it.</i>&#34; (The Secret Garden by Frances Hodgson Burnett)</p></figcaption>
</figure>
        </div>
  </div>
  
</div>
            
          
            
            
              
              <div>
  <h2 data-block-key="xzqsf">More realistic image generation</h2><p data-block-key="dg5h2">Imagen 2’s dataset and model advances have delivered improvements in many of the areas that text-to-image tools often struggle with, including rendering realistic hands and human faces and keeping images free of distracting visual artifacts.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="4p2sb">Examples of Imagen 2 generating realistic hands and human faces.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <p data-block-key="xzqsf">We trained a specialized image aesthetics model based on human preferences for qualities like good lighting, framing, exposure, sharpness, and more. Each image was given an aesthetics score which helped condition Imagen 2 to give more weight to images in its training dataset that align with qualities humans prefer. This technique improves Imagen 2’s ability to generate higher-quality images.</p>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="ijnnk">AI-generated images using the prompt “Flower”, with lower aesthetics scores (left) to higher scores (right).</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="xzqsf">Fluid style conditioning</h2><p data-block-key="6r57v">Imagen 2’s diffusion-based techniques provide a high degree of flexibility, making it easier to control and adjust the style of an image. By providing reference style images in combination with a text prompt, we can condition Imagen 2 to generate new imagery that follows the same style.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="4p2sb">A visualization of how Imagen 2 makes it easier to control the output style by using reference images alongside a text prompt.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="xzqsf">Advanced inpainting and outpainting</h2><p data-block-key="34etu">Imagen 2 also enables image editing capabilities like ‘inpainting’ and ‘outpainting’. By providing a reference image and an image mask, users can generate new content directly into the original image with a technique called inpainting, or extend the original image beyond its borders with outpainting. This technology is planned for Google Cloud’s Vertex AI in the new year.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="oyz8d">Imagen 2 can generate new content directly into the original image with inpainting.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="ic1x8">Imagen 2 can extend the original image beyond its borders with outpainting.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="xzqsf">Responsible by design</h2><p data-block-key="32ffu">To help mitigate the potential risks and challenges of our text-to-image generative technology, we set robust guardrails in place, from design and development to deployment in our products.</p><p data-block-key="7ol74">Imagen 2 is integrated with <a href="https://deepmind.google/technologies/synthid/" rel="noopener" target="_blank">SynthID</a>, our cutting-edge toolkit for watermarking and identifying AI-generated content, enabling allowlisted Google Cloud customers to add an imperceptible digital watermark directly into the pixels of the image, without compromising image quality. This allows the watermark to remain detectable by SynthID, even after applying modifications like filters, cropping, or saving with lossy compression schemes.</p><p data-block-key="c8efg">Before we release capabilities to users, we conduct robust safety testing to minimize the risk of harm. From the outset, we invested in training data safety for Imagen 2, and added technical guardrails to limit problematic outputs like violent, offensive, or sexually explicit content. We apply safety checks to training data, input prompts, and system-generated outputs at generation time. For example, we’re applying comprehensive safety filters to avoid generating potentially problematic content, such as images of named individuals. As we are expanding the capabilities and launches of Imagen 2, we are also continuously evaluating them for safety.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              


            
          
            
            
              
              <div>
  <h2 data-block-key="lfcd9">Acknowledgements</h2><p data-block-key="dsld">This work was made possible by key research and engineering contributions from:</p><p data-block-key="ece29">Aäron van den Oord, Ali Razavi, Benigno Uria, Çağlar Ünlü, Charlie Nash, Chris Wolff, Conor Durkan, David Ding, Dawid Górny, Evgeny Gladchenko, Felix Riedel, Hang Qi, Jacob Kelly, Jakob Bauer, Jeff Donahue, Junlin Zhang, Mateusz Malinowski, Mikołaj Bińkowski, Pauline Luc, Robert Riachi, Robin Strudel, Sander Dieleman, Tobenna Peter Igwe, Yaroslav Ganin, Zach Eaton-Rosen.</p><p data-block-key="cjj9">Thanks to: Ben Bariach, Dawn Bloxwich, Ed Hirst, Elspeth White, Gemma Jennings, Jenny Brennan, Komal Singh, Luis C. Cobo, Miaosen Wang, Nick Pezzotti, Nicole Brichtova, Nidhi Vyas, Nina Anderson, Norman Casagrande, Sasha Brown, Sven Gowal, Tulsee Doshi, Will Hawkins, Yelin Kim, Zahra Ahmed for driving delivery; Douglas Eck, Nando de Freitas, Oriol Vinyals, Eli Collins, Demis Hassabis for their advice.</p><p data-block-key="6ucbn">Thanks also to many others who contributed across Google DeepMind, including our partners in Google.</p>
</div>
            
          
            
            
              
              



  
    
  

            
          
            
            
              
              
            
          
        </div>
      
    

    
  
  

  

  </article>

    </div></div>
  </body>
</html>
