<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://falconllm.tii.ae/">Original</a>
    <h1>Falcon LLM – A 40B Model</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    <section id="overview">
        <div>



            
            <div>
                <p>
                    <h5> Falcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM – a 40B model. </h5>



                </p>
                <p>

                    <h5> The model uses only 75 percent of GPT-3’s training compute, 40 percent of Chinchilla’s, and 80 percent of PaLM-62B’s. </h5>

                </p>

            </div>
            <div>
                <div>
                    <p><img src="https://falconllm.tii.ae/assets/images/graph01.webp" width="384" height="268" alt=""/>
                    </p>
                </div>
                <div>
                    <p><img src="https://falconllm.tii.ae/assets/images/graph02.webp" width="384" height="268" alt=""/>
                    </p>
                </div>
            </div>


            <div>
                <p>
                    <h4>How was Falcon LLM developed?</h4>
                </p>
                <div id="accordionExample">
                    <div>
                        
                        <div id="collapseOne" aria-labelledby="headingOne" data-bs-parent="#accordionExample">
                            <div>
                                <ul>
                                    <li>Falcon was built using custom tooling and leverages a unique data pipeline that can extract high-quality content out of web data and use it for training a custom codebase, independent from the works of NVIDIA, Microsoft, or HuggingFace.</li>
                                    <li>A particular focus was put on data quality at scale. LLMs are notoriously sensitive to the quality of their training data, so significant care was taken in building a data pipeline that would both scale to tens of thousands of CPU cores for fast processing, and that would extract high-quality content from the web using extensive filtering and deduplication.</li>
                                    <li>The architecture of Falcon was optimized for performance and efficiency. Combining high-quality data with these optimizations, Falcon significantly outperforms GPT-3 for only 75% of the training compute budget—and requires a fifth of the compute at inference time.</li>
                                    <li>Falcon matches the performance of state-of-the-art LLMs from DeepMind, Google, and Anthropic.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div>
                        
                        <div id="collapseTwo" aria-labelledby="headingTwo" data-bs-parent="#accordionExample">
                            <div>
                                <ul>
                                    <li>Falcon is a 40 billion parameters autoregressive decoder-only model trained on 1 trillion tokens. It was trained on 384 GPUs on AWS over the course of two months. </li>
                                    <li>Pretraining data was collected from public crawls of the web to build the pretraining dataset of Falcon. Using dumps from CommonCrawl, after significant filtering (to remove machine generated text and adult content) and deduplication, a pretraining dataset of nearly five trillion tokens was assembled. </li>
                                    <li>To broaden Falcon abilities, this dataset was then extended with a few curated sources such as research papers and conversations from social media. </li>
                                    <li>Finally, Falcon’s performance was validated against open-source benchmarks such as EAI Harness, HELM, and BigBench.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div>
                        
                        <div id="collapseThree" aria-labelledby="headingThree" data-bs-parent="#accordionExample">
                            <div>
                                <ul>
                                    <li>Generate creative text and solve complex problems.</li>
                                    <li>Used in chatbots, customer service operations, virtual assistants, language translation, content generation, and sentiment analysis.</li>
                                    <li>Broad use cases are foreseen by Falcon, although we are most excited about applications to reduce and automate “repetitive” work. </li>
                                    <li>Falcon will help Emirati companies and startups become more efficient, streamlining internal processes and giving back time for employees to focus on what matters.</li>
                                    <li>At an individual level, chatbots embedding Falcon will be able to assist users in their daily lives.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="open-source">
        <div>
            <div>
                <p>
                    <h2>Falcon 40B is Open Sourced</h2>
                </p>
                <div>
                    <div>
                        <h5>
                            Technology Innovation Institute has publicly released the model’s weights for research and commercial use.
                        </h5>

                        <h5>
                          For researchers and developers this will make Falcon 40B, 7B more accessible, with it being based on released under the Apache License Version 2.0 (<a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank">https://www.apache.org/licenses/LICENSE-2.0</a>).
                        </h5>
                    </div>
                </div>
            </div>

            <div>
                <div>
                    
                    <div>
                        <div>


                            <h5>Open sourcing technology promotes collaboration and drives innovation by allowing a global community of developers to share their expertise and contribute to the growth and enhancement of the software. It also promotes transparency, enabling users to inspect and verify the code for security and reliability. Technology Innovation Institute hopes to advance the knowledge and research in LLMs in a safe and transparent manner leading to more uses of AI for good.
                            </h5>

                            <p><a href="https://huggingface.co/tiiuae" target="_blank">
                                    <span>
                                        <p><span>Access Falcon LLM</span>
                                            <span>Access Falcon LLM</span>
                                        </p>
                                        
                                    </span>
                                </a>
                        </p></div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section id="call-cases">
        <div>
            <div>
                <p>
                    <h2>Call for Use Cases</h2>
                </p>
                <div>
                    <p>
                        <h5>
                        Technology Innovation Institute is calling for proposals from the global research community and SME entrepreneurs to submit use cases for Falcon LLM. Exceptional use case proposals will receive &#34;Training Compute Power&#34; as an investment in addition to TII is offering further commercialization opportunities.
                        </h5>

                    </p>
                </div>
            </div>

            <div>
                <div>
                    <div>
                        <p>
                            <h4>Do you have a bright idea for how to use Falcon LLM?</h4>
                        </p>
                    </div>
                    
                </div>
            </div>
        </div>
    </section>


    <section id="papers">
        <div>
            
            
            <div>
                <div>
                    <p> Our experts regularly share our work through a range of publications, including books, journal articles, patents, presentations and white papers </p>
                    <p> Falcon is a new state-of-the-art Large Language Model. Falcon was built from scratch in Abu Dhabi, using custom-built tooling for data pre-processing and model training. Falcon significantly outperforms GPT-3 at a fraction of the cost and matches the performance of similarly sized LLMs from DeepMind (Chinchilla), Google (PaLM-62B), and Anthropic. </p>
                </div>
            </div>
        </div>
        
    </section>

    <section id="projects">
        <div>
            
            <div>
                <div>
                    <h2>Projects</h2>
                    <p>Falcon Scientific Overview</p>
                </div>
            </div>
        </div>
    </section>


    <section id="open-source">
        <div>

            <div>
                <p>

                    <h3>“Falcon Chatbot” Coming soon.</h3>
                </p>
            </div>
        </div>
    </section>


    <section id="news">
        <div>
            
            <div>
                <p>
                    <h2>In the news</h2>
                    <h5>Stay up-to-date on the latest headlines with our daily news roundup. </h5>
                </p>
            </div>

             
            
        </div>
    </section>
</div></div>
  </body>
</html>
