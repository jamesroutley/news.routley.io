<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2309.07062">Original</a>
    <h1>Large Language Models for Compiler Optimization</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cummins%2C+C">Chris Cummins</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Seeker%2C+V">Volker Seeker</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grubisic%2C+D">Dejan Grubisic</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Elhoushi%2C+M">Mostafa Elhoushi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang%2C+Y">Youwei Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roziere%2C+B">Baptiste Roziere</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gehring%2C+J">Jonas Gehring</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gloeckle%2C+F">Fabian Gloeckle</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hazelwood%2C+K">Kim Hazelwood</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Synnaeve%2C+G">Gabriel Synnaeve</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leather%2C+H">Hugh Leather</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2309.07062">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  We explore the novel application of Large Language Models to code
optimization. We present a 7B-parameter transformer model trained from scratch
to optimize LLVM assembly for code size. The model takes as input unoptimized
assembly and outputs a list of compiler options to best optimize the program.
Crucially, during training, we ask the model to predict the instruction counts
before and after optimization, and the optimized code itself. These auxiliary
learning tasks significantly improve the optimization performance of the model
and improve the model&#39;s depth of understanding.
</blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Chris Cummins [<a href="https://arxiv.org/show-email/fc152431/2309.07062">view email</a>]
      </p></div></div>
  </body>
</html>
