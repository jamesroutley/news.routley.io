<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lxe/llavavision">Original</a>
    <h1>Show HN: LLaVaVision: An AI &#34;Be My Eyes&#34;-like web app with a llama.cpp backend</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/lxe/llavavision/blob/main/screenshot.gif"><img src="https://github.com/lxe/llavavision/raw/main/screenshot.gif" alt="Screenshot" data-animated-image=""/></a></p>
<p dir="auto">A simple &#34;Be My Eyes&#34; web app with a llama.cpp/llava backend created in about an hour using ChatGPT, Copilot, and some minor help from me, <a href="https://twitter.com/lxe" rel="nofollow">@lxe</a>. It describes what it sees using <a href="https://huggingface.co/SkunkworksAI/BakLLaVA-1" rel="nofollow">SkunkworksAI BakLLaVA-1</a> model via <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> and narrates the text using <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API" rel="nofollow">Web Speech API</a>.</p>
<p dir="auto">Inspired by <a href="https://github.com/Fuzzy-Search/realtime-bakllava">Fuzzy-Search/realtime-bakllava</a>.</p>
<h3 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">You will need a machine with about ~5 GB of RAM/VRAM for the q4_k version.</p>
<h4 tabindex="-1" id="user-content-set-up-the-llamacpp-server" dir="auto"><a href="#set-up-the-llamacpp-server">Set up the llama.cpp server<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<p dir="auto">(Optional) Install the CUDA toolkit:</p>
<div data-snippet-clipboard-copy-content="sudo apt install nvidia-cuda-toolkit"><pre><code>sudo apt install nvidia-cuda-toolkit
</code></pre></div>
<p dir="auto">Build llama.cpp (build instructions for various platforms at <a href="https://github.com/ggerganov/llama.cpp#build">llama.cpp build</a>):</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake .. -DLLAMA_CUBLAS=ON # Remove the flag if CUDA is unavailable
cmake --build . --config Release"><pre><code>git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake .. -DLLAMA_CUBLAS=ON # Remove the flag if CUDA is unavailable
cmake --build . --config Release
</code></pre></div>
<p dir="auto">Download the models from <a href="https://huggingface.co/mys/ggml_bakllava-1/tree/main" rel="nofollow">ggml_bakllava-1</a>:</p>
<div data-snippet-clipboard-copy-content="wget https://huggingface.co/mys/ggml_bakllava-1/resolve/main/mmproj-model-f16.gguf
wget https://huggingface.co/mys/ggml_bakllava-1/resolve/main/ggml-model-q4_k.gguf # Choose another quant if preferred"><pre><code>wget https://huggingface.co/mys/ggml_bakllava-1/resolve/main/mmproj-model-f16.gguf
wget https://huggingface.co/mys/ggml_bakllava-1/resolve/main/ggml-model-q4_k.gguf # Choose another quant if preferred
</code></pre></div>
<p dir="auto">Start the server (server options detailed <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">here</a>):</p>
<div data-snippet-clipboard-copy-content="./bin/server -m ggml-model-q4_k.gguf --mmproj mmproj-model-f16.gguf -ngl 35 -ts 100,0 # For GPU-only, single GPU
# ./bin/server -m ggml-model-q4_k.gguf --mmproj mmproj-model-f16.gguf # For CPU"><pre><code>./bin/server -m ggml-model-q4_k.gguf --mmproj mmproj-model-f16.gguf -ngl 35 -ts 100,0 # For GPU-only, single GPU
# ./bin/server -m ggml-model-q4_k.gguf --mmproj mmproj-model-f16.gguf # For CPU
</code></pre></div>
<h4 tabindex="-1" id="user-content-launch-llavavision" dir="auto"><a href="#launch-llavavision">Launch LLaVaVision<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<p dir="auto">Clone and set up the environment:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/lxe/llavavision
cd llavavision
python3 -m venv venv 
. ./venv/bin/activate
pip install -r requirements.txt"><pre><code>git clone https://github.com/lxe/llavavision
cd llavavision
python3 -m venv venv 
. ./venv/bin/activate
pip install -r requirements.txt
</code></pre></div>
<p dir="auto">Create dummy certificates and start the server. HTTPS is required for mobile video functionality:</p>
<div data-snippet-clipboard-copy-content="openssl req -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out cert.pem -keyout key.pem
flask run --host=0.0.0.0 --key key.pem --cert cert.pem --debug"><pre><code>openssl req -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out cert.pem -keyout key.pem
flask run --host=0.0.0.0 --key key.pem --cert cert.pem --debug
</code></pre></div>
<p dir="auto">Access <a href="https://your-machine-ip:5000" rel="nofollow">https://your-machine-ip:5000</a> from your mobile device. Optionally, start a local tunnel with ngrok or localtunnel:</p>
<div data-snippet-clipboard-copy-content="npx localtunnel --local-https --allow-invalid-cert --port 5000"><pre><code>npx localtunnel --local-https --allow-invalid-cert --port 5000
</code></pre></div>
<h3 tabindex="-1" id="user-content-acknowledgements-and-inspiration" dir="auto"><a href="#acknowledgements-and-inspiration">Acknowledgements and Inspiration<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<ul dir="auto">
<li><a href="https://github.com/Fuzzy-Search/realtime-bakllava">Fuzzy-Search/realtime-bakllava</a></li>
<li><a href="https://github.com/ggerganov/llama.cpp/issues/3332" data-hovercard-type="issue" data-hovercard-url="/ggerganov/llama.cpp/issues/3332/hovercard">Multimodal LLama.cpp</a></li>
<li><a href="https://llava-vl.github.io/" rel="nofollow">llava-vl.github.io</a></li>
<li><a href="https://huggingface.co/SkunkworksAI/BakLLaVA-1" rel="nofollow">SkunkworksAI/BakLLaVA-1</a></li>
</ul>
</article>
          </div></div>
  </body>
</html>
