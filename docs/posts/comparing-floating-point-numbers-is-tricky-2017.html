<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bitbashing.io/comparing-floats.html">Original</a>
    <h1>Comparing Floating-Point Numbers Is Tricky (2017)</h1>
    
    <div id="readability-page-1" class="page"><article>
    <!-- https://katex.org/docs/browser.html -->






<!--
Let's try a bit of a deeper dive.
I hope this is, in some sense, a spiritual successor to some of Bruce Dawson's
work, especially "Comparing Floating Point Numbers, 2012 Editon".
(https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)
His excellent series of blog posts on the topic are what got me interested.
The title is, of course, lifted from Ulrich Drepper's fantastic writeup on OS
synchronization primitives, "Futexes Are Tricky".
-->

<p>Floating-point math is fraught with subtle gotchas,
and comparing values properly is no exception.
Here we discuss common pitfalls,
examine some possible solutions,
and try to beat Boost.</p>

<h2 id="things-you-probably-know-about-floats">Things you probably know about floats</h2>

<p>If you need to represent a non-integer in a mainstream programming
language, you’ll probably end up using IEEE 754 floating-point values.
Since their standardization in 1985, they’ve become ubiquitous.
Nearly all modern CPUs—and many microprocessors—contain special hardware
(called <em>floating-point units</em>, or FPUs) to handle them.</p>

<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/IEEE_754_Double_Floating_Point_Format.svg" alt="The layout of a 64-bit IEEE 754 float"/>
<figcaption>
(from <a href="https://commons.wikimedia.org/wiki/File:IEEE_754_Double_Floating_Point_Format.svg">Wikipedia</a>)
</figcaption>
</figure>

<p>Each float consists of a sign bit, some bits representing an exponent,
and bits representing a fraction, also called the <em>mantissa</em>.
Under most circumstances, the value of a float is:</p>

<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>s</mi></msup><mo>×</mo><mn>1.</mn><mi>m</mi><mo>×</mo><msup><mn>2</mn><mrow><mi>e</mi><mo>−</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">(-1)^s \times 1.m \times 2^{e - c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>−</span><span>1</span><span><span>)</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span></span></span></span></span><span></span><span>×</span><span></span></span><span><span></span><span>1.</span><span>m</span><span></span><span>×</span><span></span></span><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span><span>e</span><span>−</span><span>c</span></span></span></span></span></span></span></span></span></span></span></span></span>

<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>s</span></span></span></span>  is our sign bit, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span></span></span></span> is some fraction represented by the mantissa bits,
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>e</span></span></span></span> is an unsigned integer represented by the exponent bits,
and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>c</span></span></span></span> is half the maximum value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>e</span></span></span></span>, i.e.,
127 for a 32-bit float and 1023 for a 64-bit float.</p>

<p>There are also some special cases.
For example, when all exponent bits are zero, the formula changes to:</p>

<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>s</mi></msup><mo>×</mo><mn>0.</mn><mi>m</mi><mo>×</mo><msup><mn>2</mn><mrow><mo>−</mo><mi>c</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(-1)^s \times 0.m \times 2^{-c + 1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>−</span><span>1</span><span><span>)</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span></span></span></span></span><span></span><span>×</span><span></span></span><span><span></span><span>0.</span><span>m</span><span></span><span>×</span><span></span></span><span><span></span><span><span>2</span><span><span><span><span><span><span></span><span><span><span>−</span><span>c</span><span>+</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span></span>

<p>Note the lack of an implicit 1 preceding the mantissa—this allows us to store
small values close to zero, called <em>denormal</em> or <em>subnormal</em> values.
And when all exponent bits are one, certain mantissa values represent
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">+\infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>+</span><span>∞</span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>−</span><span>∞</span></span></span></span>, and “Not a Number” (NaN), the result of undefined
or unrepresentable operations such as dividing by zero.</p>

<p>We’ll make two observations that will prove themselves useful shortly:</p>

<ol>
  <li>
    <p>Floats cannot store arbitrary real numbers,
or even arbitrary rational numbers.
They can only store numbers representable by the equations shown before.
For example, if I declare some variable,</p>

    

    <p><code>f</code> becomes 0.100000001490116119384765625, the closest 32-bit float
value to 0.1.</p>
  </li>
  <li>
    <p>Since the equations are exponential, the distance on the number line
between adjacent values increases (exponentially!) as you move away
from zero.
The distance between 1.0 and the next possible value is about
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.19</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.19 \times 10^{-7}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1.19</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>−</span><span>7</span></span></span></span></span></span></span></span></span></span></span></span>, but the distance between adjacent floats near
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6.022</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>23</mn></msup></mrow><annotation encoding="application/x-tex">6.022 \times 10^{23}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>6.022</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>23</span></span></span></span></span></span></span></span></span></span></span></span> is roughly <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3.6</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>16</mn></msup></mrow><annotation encoding="application/x-tex">3.6 \times 10^{16}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3.6</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>16</span></span></span></span></span></span></span></span></span></span></span></span>.
This will prove to be our greatest challenge: when comparing floats,
we want to handle inputs close to zero as well as we handle ones
close to the Avogadro constant.</p>
  </li>
</ol>

<h2 id="what-is-equality">What is equality?</h2>

<p>Since the result of every floating-point operation must be rounded to the
nearest possible value, math doesn’t behave like it does with real numbers.
Depending on your hardware, compiler, and compiler flags,
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.1</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">0.1 \times 10</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>0.1</span><span></span><span>×</span><span></span></span><span><span></span><span>10</span></span></span></span> may produce a different result than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></msubsup><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\sum_{n=1}^{10} 0.1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>n</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span>10</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>0.1</span></span></span></span>.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>
Whenever we compare calculated values to each other,
we should provide some leeway to account for this.
Comparing their exact values with <code>==</code> won’t cut it.</p>

<p>Instead,
we should consider two distinct values <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span> equal if
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>a</mi><mo>−</mo><mi>b</mi><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">|a-b| \leq \epsilon</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∣</span><span>a</span><span></span><span>−</span><span></span></span><span><span></span><span>b</span><span>∣</span><span></span><span>≤</span><span></span></span><span><span></span><span>ϵ</span></span></span></span> for some sufficiently small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϵ</span></span></span></span>.
As luck would have it, the C standard library contains a <code>FLT_EPSILON</code>.
Let’s try it out!</p>

<div><div><pre><code><span>bool</span> <span>almostEqual</span><span>(</span><span>float</span> <span>a</span><span>,</span> <span>float</span> <span>b</span><span>)</span>
<span>{</span>
    <span>return</span> <span>fabs</span><span>(</span><span>a</span> <span>-</span> <span>b</span><span>)</span> <span>&lt;=</span> <span>FLT_EPSILON</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>We would hope that we’re done here, but we would be wrong.
A look at the language standards reveals that
<code>FLT_EPSILON</code> is equal to
the difference between 1.0 and the value that follows it.
But as we noted before, float values aren’t equidistant!
For values smaller than 1, <code>FLT_EPSILON</code> quickly becomes too large to be useful.
For values greater than 2, <code>FLT_EPSILON</code> is smaller than the
distance between adjacent values, so
<code><span>fabs</span><span>(</span><span>a</span> <span>-</span> <span>b</span><span>)</span> <span>&lt;=</span> <span>FLT_EPSILON</span></code>
will always be false.</p>

<p>To address these problems,
what if we scaled <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϵ</span></span></span></span> proportionally to our inputs?</p>

<div><div><pre><code><span>bool</span> <span>relativelyEqual</span><span>(</span><span>float</span> <span>a</span><span>,</span> <span>float</span> <span>b</span><span>,</span>
    <span>float</span> <span>maxRelativeDiff</span> <span>=</span> <span>FLT_EPSILON</span><span>)</span>
<span>{</span>
    <span>const</span> <span>float</span> <span>difference</span> <span>=</span> <span>fabs</span><span>(</span><span>a</span> <span>-</span> <span>b</span><span>);</span>

    <span>// Scale to the largest value.</span>
    <span>a</span> <span>=</span> <span>fabs</span><span>(</span><span>a</span><span>);</span>
    <span>b</span> <span>=</span> <span>fabs</span><span>(</span><span>b</span><span>);</span>
    <span>const</span> <span>float</span> <span>scaledEpsilon</span> <span>=</span>
        <span>maxRelativeDiff</span> <span>*</span> <span>max</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>);</span>

    <span>return</span> <span>difference</span> <span>&lt;=</span> <span>scaledEpsilon</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This works better than our initial solution,
but it’s not immediately obvious what values of
<code>maxRelativeDiff</code> we might want for different cases.
The fact that we scale it by arbitrary inputs also means it can fall prey
to the same rounding we’re worried about in the first place.</p>

<h2 id="what-about-boost">What about Boost?</h2>

<p>Boost, the popular collection of C++ libraries,
provides functions for similar purposes.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>
After removing template boilerplate and edge case handling for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\pm\infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>±</span><span>∞</span></span></span></span> and NaNs,
they resemble:</p>

<div><div><pre><code><span>float</span> <span>relative_difference</span><span>(</span><span>float</span> <span>a</span><span>,</span> <span>float</span> <span>b</span><span>)</span>
<span>{</span>
    <span>return</span> <span>fabs</span><span>((</span><span>a</span> <span>-</span> <span>b</span><span>)</span> <span>/</span> <span>min</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>));</span>
<span>}</span>

<span>float</span> <span>epsilon_difference</span><span>(</span><span>float</span> <span>a</span><span>,</span> <span>float</span> <span>b</span><span>)</span>
<span>{</span>
    <span>return</span> <span>relative_difference</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>/</span>
           <span>FLT_EPSILON</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Unfortunately, these functions don’t seem to solve
our problems.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>
Since the division in <code>relative_difference</code> often makes its result
quite small,<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>
how do we know what a good threshold might be?
By dividing that result by <code>FLT_EPSILON</code>,
<code>epsilon_difference</code> attempts to give an easier value to reason about.
But we just saw the dangers of <code>FLT_EPSILON</code>!
This scheme becomes increasingly questionable as inputs move away from one.</p>

<h2 id="what-about-ulps">What about ULPs?</h2>

<p>It would be nice to define comparisons in terms of
something more concrete than arbitrary thresholds.
Ideally, we would like to know the number of possible floating-point
values—sometimes called <em>units of least precision</em>, or ULPs—between
inputs.
If I have some value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span>, and another value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span> is only two or three ULPs away,
we can probably consider them equal, assuming some rounding error.
Most importantly, this is true regardless of the distance between
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span> on the number line.</p>

<p>Boost offers a function called <code>float_distance</code> to get the distance
between values in ULPs,
but it’s about an order of magnitude slower than the approaches
discussed so far.
With some bit-fiddling, we can do better.</p>

<p>Consider some positive float <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span></span></span></span> where every mantissa
bit is one.
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mn>1</mn><mtext> ULP</mtext></mrow><annotation encoding="application/x-tex">x + 1\text{ ULP}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span><span></span><span>+</span><span></span></span><span><span></span><span>1</span><span><span> ULP</span></span></span></span></span> must use the next largest exponent,
and all its mantissa bits must be zero.
As an example, consider 1.99999988 and 2:</p>



<table>
  <thead>
    <tr>
      <th>Value</th>
      <th>Bits</th>
      <th>Exponent</th>
      <th>Mantissa bits</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1.99999988</td>
      <td><code>0x3FFFFFFF</code></td>
      <td>127</td>
      <td><code>0x7FFFFF</code></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>2.0</td>
      <td><code>0x40000000</code></td>
      <td>128</td>
      <td><code>0x000000</code></td>
    </tr>
  </tbody>
</table>

<p>The property holds for denormals, even though they have a different
value equation.
Consider the largest denormal value and the smallest normal one:</p>

<table>
  <thead>
    <tr>
      <th>Value</th>
      <th>Bits</th>
      <th>Exponent</th>
      <th>Mantissa bits</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.1754942</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>38</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.1754942 \times 10^{-38}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1.1754942</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>−</span><span>38</span></span></span></span></span></span></span></span></span></span></span></span></td>
      <td><code>0x007FFFFF</code></td>
      <td>-126</td>
      <td><code>0x7FFFFF</code></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.17549435</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>38</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.17549435 \times 10^{-38}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1.17549435</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>−</span><span>38</span></span></span></span></span></span></span></span></span></span></span></span></td>
      <td><code>0x00800000</code></td>
      <td>-126</td>
      <td><code>0x000000</code></td>
    </tr>
  </tbody>
</table>

<p>Notice an interesting corollary:
adjacent floats (of the same sign)
have adjacent integer values when reinterpreted as such.
This reinterpretation is sometimes called <em>type punning</em>,
and we can use it to calculate the distance between values in ULPs.</p>

<p>Traditionally in C and C++, one used a union trick:</p>

<div><div><pre><code><span>union</span> <span>FloatPun</span> <span>{</span>
    <span>float</span> <span>f</span><span>;</span>
    <span>int32_t</span> <span>i</span><span>;</span>
<span>};</span>

<span>FloatPun</span> <span>fp</span><span>;</span>
<span>fp</span><span>.</span><span>f</span> <span>=</span> <span>25.624</span><span>f</span><span>;</span>
<span>// Read the same value as an integer.</span>
<span>printf</span><span>(</span><span>&#34;%x&#34;</span><span>,</span> <span>fp</span><span>.</span><span>i</span><span>);</span>
</code></pre></div></div>

<p>This still works in C,
but can run afoul of strict aliasing rules in C++.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup>
A better approach is to use <code>memcpy</code>.
Given the usual use of the function, one might assume that it would be less
efficient, but</p>

<div><div><pre><code><span>int32_t</span> <span>floatToInt</span><span>(</span><span>float</span> <span>f</span><span>)</span>
<span>{</span>
    <span>int32_t</span> <span>r</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>r</span><span>,</span> <span>&amp;</span><span>f</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>compiles to a single instruction that moves the value from a floating-point
register to an integer one.
This is exactly what we want.</p>

<p>With that problem solved, calculating the ULPs between values
becomes quite straightforward:</p>

<div><div><pre><code><span>int32_t</span> <span>ulpsDistance</span><span>(</span><span>const</span> <span>float</span> <span>a</span><span>,</span> <span>const</span> <span>float</span> <span>b</span><span>)</span>
<span>{</span>
    <span>// Save work if the floats are equal.</span>
    <span>// Also handles +0 == -0</span>
    <span>if</span> <span>(</span><span>a</span> <span>==</span> <span>b</span><span>)</span> <span>return</span> <span>0</span><span>;</span>

    <span>const</span> <span>auto</span> <span>max</span> <span>=</span>
        <span>std</span><span>::</span><span>numeric_limits</span><span>&lt;</span><span>int32_t</span><span>&gt;::</span><span>max</span><span>();</span>

    <span>// Max distance for NaN</span>
    <span>if</span> <span>(</span><span>isnan</span><span>(</span><span>a</span><span>)</span> <span>||</span> <span>isnan</span><span>(</span><span>b</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>// If one&#39;s infinite and they&#39;re not equal, max distance.</span>
    <span>if</span> <span>(</span><span>isinf</span><span>(</span><span>a</span><span>)</span> <span>||</span> <span>isinf</span><span>(</span><span>b</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>int32_t</span> <span>ia</span><span>,</span> <span>ib</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>ia</span><span>,</span> <span>&amp;</span><span>a</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>ib</span><span>,</span> <span>&amp;</span><span>b</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>

    <span>// Don&#39;t compare differently-signed floats.</span>
    <span>if</span> <span>((</span><span>ia</span> <span>&lt;</span> <span>0</span><span>)</span> <span>!=</span> <span>(</span><span>ib</span> <span>&lt;</span> <span>0</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>// Return the absolute value of the distance in ULPs.</span>
    <span>int32_t</span> <span>distance</span> <span>=</span> <span>ia</span> <span>-</span> <span>ib</span><span>;</span>
    <span>if</span> <span>(</span><span>distance</span> <span>&lt;</span> <span>0</span><span>)</span> <span>distance</span> <span>=</span> <span>-</span><span>distance</span><span>;</span>
    <span>return</span> <span>distance</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This code is quite portable—it only assumes that the platform supports
32-bit integers and that floats are stored in accordance with
IEEE 754.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>
We avoid comparing differently-signed values for a few reasons:</p>

<ol>
  <li>
    <p>ULPs are the wrong tool to compare values near or across zero,
as we’ll see below.</p>
  </li>
  <li>
    <p>Almost all modern CPUs use <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a>
arithmetic, while floats use
<a href="https://en.wikipedia.org/wiki/Signed_number_representations#Signed_magnitude_representation">signed magnitude</a>.
Converting one format to the other in order to meaningfully add or subtract
differently-signed values requires some extra work.
For the same reason, the sign of our result might not be what we expect,
so we take its absolute value.
We only care about the distance between our two inputs.</p>
  </li>
  <li>
    <p>If the subtraction overflows or underflows, we get undefined behavior
with signed integers and modular arithmetic with unsigned ones.
Neither is desirable here.</p>
  </li>
</ol>

<p>We calculate the absolute value ourselves instead of using <code>std::abs</code>
for two reaons.
First, the integer versions of <code>std::abs</code> only take types—such as
<code><span>int</span></code>, <code><span>long</span></code>, and <code><span>long</span> <span>long</span></code>—whose
sizes are platform-specific.
We want to avoid assumptions about implicit conversions
between those types and <code><span>int32_t</span></code>.<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>
The second is a strange pitfall related to the placement of
<code>std::abs</code> overloads in the C++ standard library.
If you include <code>&lt;cmath&gt;</code> but not <code>&lt;cstdlib&gt;</code>,
only the floating-point versions of <code>std::abs</code> are provided.
Several toolchains I tested then promote the <code><span>int32_t</span></code>
value to a <code><span>double</span></code>,
even if your target only has
a 32-bit FPU and must emulate <code><span>double</span></code>
using integer registers.
(As one might guess, this is <em>terrible</em> for performance.)
Warning flags such as <code>-Wconversion</code> can help us notice this happening,
or we can just avoid all these gotchas by calculating the absolute value directly.
At any rate, this is a trivial detail.</p>

<h2 id="no-silver-bullets">No silver bullets</h2>

<p>Relative epsilons—including ULPs-based ones—don’t make sense around zero.
The exponential nature of floats means that many more values are
gathered there than anywhere else on the number line.
Despite being a fairly small value in the context of many calculations,
0.1 is over one billion ULPs away from zero!
Consequently, fixed epsilons are probably the best
choice when you expect the results to be small.
What particular <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϵ</span></span></span></span> you want is entirely dependent on the calculations
performed.</p>

<p>Armed with this knowledge,
you may be tempted to write some end-all comparison function along the lines of:</p>

<div><div><pre><code><span>bool</span> <span>nearlyEqual</span><span>(</span><span>float</span> <span>a</span><span>,</span> <span>float</span> <span>b</span><span>,</span>
        <span>float</span> <span>fixedEpsilon</span><span>,</span> <span>int</span> <span>ulpsEpsilon</span><span>)</span>
<span>{</span>
    <span>// Handle the near-zero case.</span>
    <span>const</span> <span>float</span> <span>difference</span> <span>=</span> <span>fabs</span><span>(</span><span>a</span> <span>-</span> <span>b</span><span>);</span>
    <span>if</span> <span>(</span><span>difference</span> <span>&lt;=</span> <span>fixedEpsilon</span><span>)</span> <span>return</span> <span>true</span><span>;</span>

    <span>return</span> <span>ulpsDistance</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>&lt;=</span> <span>ulpsEpsilon</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>But using it meaningfully is difficult without understanding the theory
we’ve discussed.</p>

<h2 id="brief-aside-other-ulps-based-functions">Brief aside: Other ULPs-based functions</h2>

<p>We can use the same techniques to write other useful functions,
such as one that increments a float by some number of ULPs.
Boost offers a similar family of functions
(<code>float_next</code>, <code>float_advance</code>, etc.),
but like <code>float_distance</code>, they pay a performance cost to avoid
type punning.</p>

<p>One would hope we could simply get our ULPs, perform our addition,
and pun the result back, e.g.,</p>

<div><div><pre><code><span>float</span> <span>ulpsIncrement</span><span>(</span><span>float</span> <span>f</span><span>,</span> <span>int32_t</span> <span>ulps</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>isnan</span><span>(</span><span>f</span><span>)</span> <span>||</span> <span>isinf</span><span>(</span><span>f</span><span>))</span> <span>return</span> <span>f</span><span>;</span>
    <span>int32_t</span> <span>i</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>i</span><span>,</span> <span>&amp;</span><span>f</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>i</span> <span>+=</span> <span>ulps</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>f</span><span>,</span> <span>&amp;</span><span>i</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>return</span> <span>f</span><span>;</span>
<span>}</span>
</code></pre></div></div>
<p>This naïve solution works for positive values,
but on most hardware, “incrementing” a negative float by a positive number of
ULPs will move us away from zero!
This is probably not what we want.
We mentioned before that floats use a signed magnitude scheme,
whereas most CPUs use two’s complement.
So, to operate on negative values, we need to convert from the former
to the CPU’s native integer format.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" rel="footnote">8</a></sup></p>

<div><div><pre><code><span>static</span> <span>const</span> <span>int32_t</span> <span>int32SignBit</span> <span>=</span> <span>(</span><span>int32_t</span><span>)</span><span>1</span> <span>&lt;&lt;</span> <span>31</span><span>;</span>

<span>int32_t</span> <span>floatToNativeSignedUlps</span><span>(</span><span>float</span> <span>f</span><span>)</span>
<span>{</span>
    <span>int32_t</span> <span>i</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>i</span><span>,</span> <span>&amp;</span><span>f</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>

    <span>// Positive values are the same in both</span>
    <span>// two&#39;s complement and signed magnitude.</span>
    <span>// For negative values, remove the sign bit</span>
    <span>// and negate the result (subtract from 0).</span>
    <span>return</span> <span>i</span> <span>&gt;=</span> <span>0</span> <span>?</span> <span>i</span> <span>:</span> <span>-</span><span>(</span><span>i</span> <span>&amp;</span> <span>~</span><span>int32SignBit</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>After operating on the ULPs,
we must convert back to signed magnitude:</p>

<div><div><pre><code><span>float</span> <span>nativeSignedUlpsToFloat</span><span>(</span><span>int32_t</span> <span>ulps</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>ulps</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
        <span>ulps</span> <span>=</span> <span>-</span><span>ulps</span><span>;</span>
        <span>ulps</span> <span>|=</span> <span>int32SignBit</span><span>;</span>
    <span>}</span>
    <span>float</span> <span>f</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>f</span><span>,</span> <span>&amp;</span><span>ulps</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>return</span> <span>f</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>With those functions defined,
we can return to our goal:</p>

<div><div><pre><code><span>float</span> <span>ulpsIncrement</span><span>(</span><span>float</span> <span>f</span><span>,</span> <span>int32_t</span> <span>ulps</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>isnan</span><span>(</span><span>f</span><span>)</span> <span>||</span> <span>isinf</span><span>(</span><span>f</span><span>))</span> <span>return</span> <span>f</span><span>;</span>
    <span>int32_t</span> <span>i</span> <span>=</span> <span>floatToNativeSignedUlps</span><span>(</span><span>f</span><span>);</span>
    <span>i</span> <span>+=</span> <span>ulps</span><span>;</span>
    <span>return</span> <span>nativeSignedUlpsToFloat</span><span>(</span><span>i</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<h2 id="takeaways">Takeaways</h2>

<p>When comparing floating-point values, remember:</p>

<ul>
  <li>
    <p><code>FLT_EPSILON</code>… isn’t float epsilon,
except in the ranges [-2, -1] and [1, 2].
The distance between adjacent values depends on the values in question.</p>
  </li>
  <li>
    <p>When comparing to some known value—especially zero or values near it—use
a fixed <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ϵ</span></span></span></span> that makes sense for your calculations.</p>
  </li>
  <li>
    <p>When comparing non-zero values, some ULPs-based comparison is probably the
best choice.</p>
  </li>
  <li>
    <p>When values could be anywhere on the number line,
some hybrid of the two is needed.
Choose epsilons carefully based on expected outputs.</p>
  </li>
</ul>

<h2 id="acknowledgments">Acknowledgments</h2>

<p>Much of this was adapted from Bruce Dawson’s <em>fantastic</em>
exploration of the topic on his blog,
<a href="https://randomascii.wordpress.com">Random ASCII</a>.
Thanks also to
coworkers Evan Thompson and Matt Drees for their input.</p>

<h2 id="afterward-performance-concerns">Afterward: performance concerns</h2>

<p>The relatively poor performance of <code>boost::float_distance</code>
was a large motivation for implementing
our own <code>ulpsDistance</code>.
For the sake of completeness, the following is a benchmark
(using <a href="https://github.com/google/benchmark">Google’s benchmark library</a>)
comparing the two with a handful of inputs.</p>

<hr/>

<div><div><pre><code><span>#include</span> <span>&lt;cstring&gt;</span><span> // For memcpy</span><span>
#include</span> <span>&lt;limits&gt;</span><span> // for numeric_limits&lt;float&gt;::infinity</span><span>
#include</span> <span>&lt;random&gt;</span><span>
</span>
<span>#include</span> <span>&lt;benchmark/benchmark.h&gt;</span><span>
#include</span> <span>&lt;boost/math/special_functions/next.hpp&gt;</span><span>
#include</span> <span>&lt;boost/math/special_functions/relative_difference.hpp&gt;</span><span>
</span>
<span>using</span> <span>namespace</span> <span>std</span><span>;</span>
<span>using</span> <span>namespace</span> <span>boost</span><span>::</span><span>math</span><span>;</span>

<span>std</span><span>::</span><span>pair</span><span>&lt;</span><span>float</span><span>,</span> <span>float</span><span>&gt;</span> <span>pickInput</span><span>()</span>
<span>{</span>
    <span>static</span> <span>auto</span> <span>re</span> <span>=</span> <span>mt19937</span><span>(</span><span>random_device</span><span>()());</span>
    <span>static</span> <span>auto</span> <span>coinFlip</span> <span>=</span> <span>bernoulli_distribution</span><span>(</span><span>0.5</span><span>);</span>
    <span>static</span> <span>auto</span> <span>inputPicker</span> <span>=</span> <span>uniform_int_distribution</span><span>&lt;</span><span>int</span><span>&gt;</span><span>(</span><span>1</span><span>,</span> <span>10</span><span>);</span>

    <span>const</span> <span>float</span> <span>infinity</span> <span>=</span> <span>numeric_limits</span><span>&lt;</span><span>float</span><span>&gt;::</span><span>infinity</span><span>();</span>

    <span>switch</span><span>(</span><span>inputPicker</span><span>(</span><span>re</span><span>))</span> <span>{</span>
        <span>// Let&#39;s say there&#39;s a 5% chance our values are denormal.</span>
        <span>// (This is probably more pessimal than our actual data.)</span>
        <span>case</span> <span>1</span><span>:</span>
            <span>if</span> <span>(</span><span>coinFlip</span><span>(</span><span>re</span><span>))</span> <span>return</span> <span>{</span><span>1e-38</span><span>f</span><span>,</span> <span>float_advance</span><span>(</span><span>1e-38</span><span>f</span><span>,</span> <span>3</span><span>)};</span>
            <span>// Intentional fall-through</span>

        <span>// Let&#39;s throw in some huge numbers</span>
        <span>case</span> <span>2</span><span>:</span>
        <span>case</span> <span>3</span><span>:</span>
        <span>case</span> <span>4</span><span>:</span>
        <span>case</span> <span>5</span><span>:</span>
            <span>return</span> <span>{</span><span>6.022e23</span><span>f</span><span>,</span> <span>2.998e8</span><span>f</span><span>};</span>
            <span>break</span><span>;</span>

        <span>// And so not-so-huge ones.</span>
        <span>case</span> <span>6</span><span>:</span>
        <span>case</span> <span>7</span><span>:</span>
        <span>case</span> <span>8</span><span>:</span>
        <span>case</span> <span>9</span><span>:</span>
            <span>return</span> <span>{</span><span>1.0</span><span>f</span><span>,</span> <span>11.0</span><span>f</span><span>};</span>

        <span>// Let&#39;s say there&#39;s a 5% chance we have NaNs</span>
        <span>// and another 5% chance they&#39;re infinity</span>
        <span>case</span> <span>10</span><span>:</span>
            <span>if</span> <span>(</span><span>coinFlip</span><span>(</span><span>re</span><span>))</span> <span>return</span> <span>{</span><span>42</span><span>,</span> <span>numeric_limits</span><span>&lt;</span><span>float</span><span>&gt;::</span><span>quiet_NaN</span><span>()};</span>
            <span>else</span> <span>return</span> <span>{</span><span>42</span><span>,</span> <span>infinity</span><span>};</span>

        <span>default:</span> <span>assert</span><span>(</span><span>0</span><span>);</span>
    <span>}</span>
<span>}</span>

<span>__attribute__</span><span>((</span><span>noinline</span><span>))</span> <span>// For visibility when benchmarking</span>
<span>int32_t</span> <span>ulpsDistance</span><span>(</span><span>const</span> <span>float</span> <span>a</span><span>,</span> <span>const</span> <span>float</span> <span>b</span><span>)</span>
<span>{</span>
    <span>// We can skip all the following work if they&#39;re equal.</span>
    <span>if</span> <span>(</span><span>a</span> <span>==</span> <span>b</span><span>)</span> <span>return</span> <span>0</span><span>;</span>

    <span>const</span> <span>auto</span> <span>max</span> <span>=</span> <span>numeric_limits</span><span>&lt;</span><span>int32_t</span><span>&gt;::</span><span>max</span><span>();</span>

    <span>// We first check if the values are NaN.</span>
    <span>// If this is the case, they&#39;re inherently unequal;</span>
    <span>// return the maximum distance between the two.</span>
    <span>if</span> <span>(</span><span>isnan</span><span>(</span><span>a</span><span>)</span> <span>||</span> <span>isnan</span><span>(</span><span>b</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>// If one&#39;s infinite, and they&#39;re not equal,</span>
    <span>// return the max distance between the two.</span>
    <span>if</span> <span>(</span><span>isinf</span><span>(</span><span>a</span><span>)</span> <span>||</span> <span>isinf</span><span>(</span><span>b</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>// At this point we know that the floating-point values aren&#39;t equal and</span>
    <span>// aren&#39;t special values (infinity/NaN).</span>
    <span>// Because of how IEEE754 floats are laid out</span>
    <span>// (sign bit, then exponent, then mantissa), we can examine the bits</span>
    <span>// as if they were integers to get the distance between them in units</span>
    <span>// of least precision (ULPs).</span>
    <span>static_assert</span><span>(</span><span>sizeof</span><span>(</span><span>float</span><span>)</span> <span>==</span> <span>sizeof</span><span>(</span><span>int32_t</span><span>),</span> <span>&#34;What size is float?&#34;</span><span>);</span>

    <span>// memcpy to get around the strict aliasing rule.</span>
    <span>// The compiler knows what we&#39;re doing and will just transfer the float</span>
    <span>// values into integer registers.</span>
    <span>int32_t</span> <span>ia</span><span>,</span> <span>ib</span><span>;</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>ia</span><span>,</span> <span>&amp;</span><span>a</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>
    <span>memcpy</span><span>(</span><span>&amp;</span><span>ib</span><span>,</span> <span>&amp;</span><span>b</span><span>,</span> <span>sizeof</span><span>(</span><span>float</span><span>));</span>

    <span>// If the signs of the two values aren&#39;t the same,</span>
    <span>// return the maximum distance between the two.</span>
    <span>// This is done to avoid integer overflow, and because the bit layout of</span>
    <span>// floats is closer to sign-magnitude than it is to two&#39;s complement.</span>
    <span>// This *also* means that if you&#39;re checking if a value is close to zero,</span>
    <span>// you should probably just use a fixed epsilon instead of this function.</span>
    <span>if</span> <span>((</span><span>ia</span> <span>&lt;</span> <span>0</span><span>)</span> <span>!=</span> <span>(</span><span>ib</span> <span>&lt;</span> <span>0</span><span>))</span> <span>return</span> <span>max</span><span>;</span>

    <span>// If we&#39;ve satisfied all our caveats above, just subtract the values.</span>
    <span>// The result is the distance between the values in ULPs.</span>
    <span>int32_t</span> <span>distance</span> <span>=</span> <span>ia</span> <span>-</span> <span>ib</span><span>;</span>
    <span>if</span> <span>(</span><span>distance</span> <span>&lt;</span> <span>0</span><span>)</span> <span>distance</span> <span>=</span> <span>-</span><span>distance</span><span>;</span>
    <span>return</span> <span>distance</span><span>;</span>
<span>}</span>

<span>void</span> <span>benchFloatDistance</span><span>(</span><span>benchmark</span><span>::</span><span>State</span><span>&amp;</span> <span>state</span><span>)</span>
<span>{</span>
    <span>while</span> <span>(</span><span>state</span><span>.</span><span>KeepRunning</span><span>())</span> <span>{</span>
        <span>state</span><span>.</span><span>PauseTiming</span><span>();</span>
        <span>float</span> <span>a</span><span>,</span> <span>b</span><span>;</span>
        <span>std</span><span>::</span><span>tie</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>=</span> <span>pickInput</span><span>();</span>
        <span>state</span><span>.</span><span>ResumeTiming</span><span>();</span>
        <span>// float_distance can&#39;t handle NaN and Infs.</span>
        <span>if</span> <span>(</span><span>!</span><span>isnan</span><span>(</span><span>a</span><span>)</span> <span>&amp;&amp;</span> <span>!</span><span>isnan</span><span>(</span><span>b</span><span>)</span> <span>&amp;&amp;</span> <span>!</span><span>isinf</span><span>(</span><span>a</span><span>)</span> <span>&amp;&amp;</span> <span>!</span><span>isinf</span><span>(</span><span>b</span><span>))</span> <span>{</span>
            <span>benchmark</span><span>::</span><span>DoNotOptimize</span><span>(</span><span>float_distance</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>));</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
<span>BENCHMARK</span><span>(</span><span>benchFloatDistance</span><span>);</span>

<span>void</span> <span>benchUlps</span><span>(</span><span>benchmark</span><span>::</span><span>State</span><span>&amp;</span> <span>state</span><span>)</span>
<span>{</span>
    <span>while</span> <span>(</span><span>state</span><span>.</span><span>KeepRunning</span><span>())</span> <span>{</span>
        <span>state</span><span>.</span><span>PauseTiming</span><span>();</span>
        <span>float</span> <span>a</span><span>,</span> <span>b</span><span>;</span>
        <span>std</span><span>::</span><span>tie</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>=</span> <span>pickInput</span><span>();</span>
        <span>state</span><span>.</span><span>ResumeTiming</span><span>();</span>
        <span>benchmark</span><span>::</span><span>DoNotOptimize</span><span>(</span><span>ulpsDistance</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>));</span>
    <span>}</span>
<span>}</span>
<span>BENCHMARK</span><span>(</span><span>benchUlps</span><span>);</span>


<span>BENCHMARK_MAIN</span><span>();</span>
</code></pre></div></div>

<hr/>

<p>On my laptop (an Intel Core i7 Skylake), I get:</p>

<div><div><pre><code>Benchmark                   Time           CPU Iterations
---------------------------------------------------------
benchFloatDistance        717 ns        836 ns     850424
benchUlps                 157 ns        176 ns    3914780
</code></pre></div></div>

<p>And on an ARMv6 board we use at work for embedded Linux platforms, I get:</p>

<div><div><pre><code>Benchmark                   Time           CPU Iterations
---------------------------------------------------------
benchFloatDistance      43674 ns      42609 ns      16646
benchUlps                4748 ns       4602 ns     151382
</code></pre></div></div>

<p>Actual timing values obviously depend on the type of inputs,
the uniformity of the inputs (which influences branch prediction),
and <em>many</em> other factors,
but our function seems to outperform Boost alternatives in the general case.</p>

<hr/>

<p>For the typographically inclined, this is available as a
<a href="https://assets.bitbashing.io/papers/floats.pdf">PDF</a>
and its
<a href="https://assets.bitbashing.io/papers/floats.tar.xz"><span>L<sup>a</sup>T<sub>e</sub>X</span>
 source</a>.</p>

<hr/>



  </article></div>
  </body>
</html>
