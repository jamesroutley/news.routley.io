<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eli.thegreenplace.net/2025/consistent-hashing/">Original</a>
    <h1>Consistent hashing</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                <p>This post is an introduction to <a href="https://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a>,
an algorithm for designing a hash table such that only a small portion of
keys has to be recomputed when the table&#39;s size changes.</p>
<div id="motivating-use-case">
<h2>Motivating use case</h2>
<p>Suppose we&#39;re designing a <a href="https://eli.thegreenplace.net/2022/go-and-proxy-servers-part-1-http-proxies/">caching web proxy</a>,
but the expected storage demands are higher than what a single machine can
handle. So we distribute the cache across multiple machines. How do we do that?
Given a URL, how do we make sure that we can easily find out which server we
should approach for a potentially cached version <a href="#footnote-1" id="footnote-reference-1">[1]</a>?</p>
<p>An approach that immediately comes to mind is <em>hashing</em>. Let&#39;s calculate a
numeric hash of the URL and distribute it evenly between N nodes (that&#39;s what
we&#39;ll call the servers in this post):</p>
<div><pre><span></span>hash := calculateHashFunction(url)
nodeId := hash % N
</pre></div>
<p>This process works but turns out to have serious downsides in real-world
applications.</p>
</div>
<div id="the-problem-with-the-naive-hashing-approach">
<h2>The problem with the naive hashing approach</h2>
<p>Consider our caching use case again; in a realistic application at &#34;internet
scale&#34;, one of the assumptions we made implicitly doesn&#39;t hold - the cache
nodes are not static. New nodes are added to the system if the load is
high (or if new machines come into service); existing nodes can crash or
be taken offline for maintenance. In other words, the number <tt>N</tt> in our
application is not a constant.</p>
<p>The problem may be apparent now; to demonstrate it directly, let&#39;s take an
actual implementation of <tt>hashItem</tt> using Go&#39;s <tt>md5</tt> package <a href="#footnote-2" id="footnote-reference-2">[2]</a>:</p>
<div><pre><span></span><span>// hashItem computes the slot an item hashes to, given a total number of slots.</span><span></span>
<span>func</span><span> </span><span>hashItem</span><span>(</span><span>item</span><span> </span><span>string</span><span>,</span><span> </span><span>nslots</span><span> </span><span>uint64</span><span>)</span><span> </span><span>uint64</span><span> </span><span>{</span><span></span>
<span>  </span><span>digest</span><span> </span><span>:=</span><span> </span><span>md5</span><span>.</span><span>Sum</span><span>([]</span><span>byte</span><span>(</span><span>item</span><span>))</span><span></span>
<span>  </span><span>digestHigh</span><span> </span><span>:=</span><span> </span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint64</span><span>(</span><span>digest</span><span>[</span><span>8</span><span>:</span><span>16</span><span>])</span><span></span>
<span>  </span><span>digestLow</span><span> </span><span>:=</span><span> </span><span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint64</span><span>(</span><span>digest</span><span>[:</span><span>8</span><span>])</span><span></span>
<span>  </span><span>return</span><span> </span><span>(</span><span>digestHigh</span><span> </span><span>^</span><span> </span><span>digestLow</span><span>)</span><span> </span><span>%</span><span> </span><span>nslots</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>The terminology is slightly adjusted:</p>
<ul>
<li>Instead of <tt>url</tt>, we&#39;ll just refer to a generic <tt>item</tt></li>
<li>&#34;Slot&#34; is a common concept in hash tables: our <tt>hashItem</tt> computes a slot
number for an item, given the total number of available slots</li>
</ul>
<p>Let&#39;s say we started with 32 slots, and we hashed the strings <tt>&#34;hello&#34;</tt>,
<tt>&#34;consistent&#34;</tt> and <tt>&#34;marmot&#34;</tt>. We get these slots:</p>
<div><pre><span></span>hello       (n=32): 4
consistent  (n=32): 14
marmot      (n=32): 5
</pre></div>
<p>Now suppose that another node is added, and the total <tt>nslots</tt> grows to 33.
Hashing our items again:</p>
<div><pre><span></span>hello       (n=33): 23
consistent  (n=33): 18
marmot      (n=33): 31
</pre></div>
<p>All the slots changed!</p>
<p>This is a significant problem with the naive hashing approach. Whenever
<tt>nslots</tt> changes, we get completely different slots for pretty much any item.
In a realistic application it means that whenever a new node joins or leaves
our caching cluster, there will be a flood of cache misses on every query until
the new cluster settles. And node changes sometimes occur at the most
incovenient times; imagine that the load is spiking (maybe a site was mentioned
in a high-profile news outlet, or there&#39;s a live event streaming) and new
nodes are added to handle it. This isn&#39;t a great time to temporarily lose all
caching!</p>
</div>
<div id="consistent-hashing-1">
<h2>Consistent hashing</h2>
<p>The consistent hashing algorithm solves the problem
in an elegant way. The key idea is to map both nodes and items onto
an interval, and then an item belongs to a node closest to it. Concretely,
we take the unit circle <a href="#footnote-3" id="footnote-reference-3">[3]</a>, and map nodes and items to angles on this circle.
Here&#39;s an example that explains how this method works in more detail:</p>
<p><img alt="Circle showing consistent hashing in action" src="https://eli.thegreenplace.net/images/2025/consistent-hashing-circle.png"/></p><p>This shows five nodes: N1 through N5, and three items: Ix, Iy, Iz.
Initially, we add the nodes: using a hashing operation we map them onto the
circle (details later). Then, as items come in, we determine which node they
belong to, as follows:</p>
<ul>
<li>Use the same hashing operation to find the item&#39;s location on the circle</li>
<li>The node this item belongs to is the closest one, in the clockwise direction</li>
</ul>
<p>In our diagram, Ix is mapped to N1, Iy to N2, and Iz to N3. So far so good, but
the benefit of this approach becomes apparent when the nodes change. In our
diagram, suppose N3 is removed. Then Iz will map to N5.
<strong>The mapping of the other items doesn&#39;t change!</strong></p>
<p>Adding nodes has a similar outcome. If a new node N6 is added and it hashes to a
position between Iy and N2 on the circle, from that moment Iy will be mapped to
N6, but the other items keep their mapping.</p>
<p>Suppose we have a total of <em>M</em> items that we need to distribute across <em>N</em>
nodes. Using the naive hashing approach, whenever we add or remove a node, all
<em>M</em> items change their mapping. On the other hand, with consistent hashing only
about  need to change. This is a huge difference.</p>
<p>The original consistent hashing paper (see <a href="#footnote-1" id="footnote-reference-4">[1]</a>) calls this the <em>monotonicity
property</em> of the algorithm:</p>
<blockquote>
If items are initially assigned to a set of buckets , and then
some new buckets are added to form , then an item may move from
an old bucket to a new bucket, but not from one old bucket to another.</blockquote>
</div>
<div id="implementing-consistent-hashing">
<h2>Implementing consistent hashing</h2>
<p>Implementing the consistent hashing algorithm as described above is fairly easy.
The most critical part of the implementation is finding which node an item maps
to - this involves some kind of search. The original consistent hashing paper
suggests using a balanced binary tree for the search; the implementation I&#39;m
demonstrating here uses a slightly different but equivalent approach:
binary search in a linear array of node positions (slots) <a href="#footnote-4" id="footnote-reference-5">[4]</a>.</p>
<p>First, some practical considerations:</p>
<ul>
<li>Theoretically, the unit circle can be seen as the continuous range
<tt>[0, 1)</tt>. In programming we much prefer the discrete domain, however,
so we&#39;re going to &#34;quantize&#34; this range to <tt>[0, ringSize)</tt>, where
<tt>ringSize</tt> is some suitably large number that avoids collisions.</li>
<li>Looking at the circle diagram above, imagine that 0 degrees is the &#34;north&#34;
direction (12 o&#39;clock), and angles increase clockwise. In our discrete
domain, 12 o&#39;clock is 0, 3 o&#39;clock is <tt>ringSize/4</tt>, and so on.</li>
</ul>
<p>When a node is added to the consistent hash, its location is found by applying
a hash function like <tt>hashItem</tt> as described above, with
<tt>nslots=ringSize</tt>. The nodes are stored using a pair of data structures,
as follows; this example uses the approximate locations of the nodes N1
through N5 in the circle diagram above (assume <tt>ringSize=1024</tt> here):</p>
<p><img alt="Nodes and slots arrays for the has shown above" src="https://eli.thegreenplace.net/images/2025/nodes-slots.png"/></p><p>The positions of the nodes on the circle are stored in <tt>slots</tt>, which is
sorted. <tt>nodes</tt> holds the corresponding node names. For each <tt>i</tt>,
<tt>nodes[i]</tt> is at position <tt>slots[i]</tt> on the circle.</p>
<p>Here&#39;s the <tt>ConsistentHasher</tt> data structure in Go:</p>
<div><pre><span></span><span>type</span><span> </span><span>ConsistentHasher</span><span> </span><span>struct</span><span> </span><span>{</span><span></span>
<span>  </span><span>// nodes is a list of nodes in the hash ring; it&#39;s sorted in the same order</span><span></span>
<span>  </span><span>// as slots: for each i, the node at index slots[i] is nodes[i].</span><span></span>
<span>  </span><span>nodes</span><span> </span><span>[]</span><span>string</span><span></span>

<span>  </span><span>// slots is a sorted slice of node indices.</span><span></span>
<span>  </span><span>slots</span><span> </span><span>[]</span><span>uint64</span><span></span>

<span>  </span><span>ringSize</span><span> </span><span>uint64</span><span></span>
<span>}</span><span></span>

<span>// NewConsistentHasher creates a new consistent hasher with a given maximal</span><span></span>
<span>// ring size.</span><span></span>
<span>func</span><span> </span><span>NewConsistentHasher</span><span>(</span><span>ringSize</span><span> </span><span>uint64</span><span>)</span><span> </span><span>*</span><span>ConsistentHasher</span><span> </span><span>{</span><span></span>
<span>  </span><span>return</span><span> </span><span>&amp;</span><span>ConsistentHasher</span><span>{</span><span></span>
<span>    </span><span>ringSize</span><span>:</span><span> </span><span>ringSize</span><span>,</span><span></span>
<span>  </span><span>}</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>And this is how finding which node a given item maps to is implemented:</p>
<div><pre><span></span><span>// FindNodeFor finds the node an item hashes to. It&#39;s an error to call this</span><span></span>
<span>// method if the hasher doesn&#39;t have any nodes.</span><span></span>
<span>func</span><span> </span><span>(</span><span>ch</span><span> </span><span>*</span><span>ConsistentHasher</span><span>)</span><span> </span><span>FindNodeFor</span><span>(</span><span>item</span><span> </span><span>string</span><span>)</span><span> </span><span>string</span><span> </span><span>{</span><span></span>
<span>  </span><span>if</span><span> </span><span>len</span><span>(</span><span>ch</span><span>.</span><span>nodes</span><span>)</span><span> </span><span>==</span><span> </span><span>0</span><span> </span><span>{</span><span></span>
<span>    </span><span>panic</span><span>(</span><span>&#34;FindNodeFor called when ConsistentHasher has no nodes&#34;</span><span>)</span><span></span>
<span>  </span><span>}</span><span></span>
<span>  </span><span>ih</span><span> </span><span>:=</span><span> </span><span>hashItem</span><span>(</span><span>item</span><span>,</span><span> </span><span>ch</span><span>.</span><span>ringSize</span><span>)</span><span></span>

<span>  </span><span>// Since ch.slots is a sorted list of all the node indices for our nodes, a</span><span></span>
<span>  </span><span>// binary search is what we need here. ih is mapped to the node that has the</span><span></span>
<span>  </span><span>// same or the next larger node index. slices.BinarySearch does exactly this,</span><span></span>
<span>  </span><span>// by returning the index where the value would be inserted.</span><span></span>
<span>  </span><span>slotIndex</span><span>,</span><span> </span><span>_</span><span> </span><span>:=</span><span> </span><span>slices</span><span>.</span><span>BinarySearch</span><span>(</span><span>ch</span><span>.</span><span>slots</span><span>,</span><span> </span><span>ih</span><span>)</span><span></span>

<span>  </span><span>// When the returned index is len(slots), it means the search wrapped</span><span></span>
<span>  </span><span>// around.</span><span></span>
<span>  </span><span>if</span><span> </span><span>slotIndex</span><span> </span><span>==</span><span> </span><span>len</span><span>(</span><span>ch</span><span>.</span><span>slots</span><span>)</span><span> </span><span>{</span><span></span>
<span>    </span><span>slotIndex</span><span> </span><span>=</span><span> </span><span>0</span><span></span>
<span>  </span><span>}</span><span></span>

<span>  </span><span>return</span><span> </span><span>ch</span><span>.</span><span>nodes</span><span>[</span><span>slotIndex</span><span>]</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>The key here is the binary search invocation. Adding and removing nodes is done
similarly using binary search - see <a href="https://github.com/eliben/code-for-blog/tree/main/2025/consistent-hashing">the full code</a>.</p>
</div>
<div id="better-item-distribution-with-virtual-nodes">
<h2>Better item distribution with virtual nodes</h2>
<p>A common issue that comes up in the implementation of consistent hashing is
unbalanced distribution of items across the different nodes. With 
items and a total of  nodes, the <em>average</em> distribution will be
about  per node, but in practice it won&#39;t be very balanced -
some nodes will have many more items assigned to them than others
(see the Appendix for more details).</p>
<p>In a real application, this may mean that some cache servers will be much busier
than others, which is a bad thing as far as capacity planning and efficient use
of HW. Luckily, there&#39;s an elegant tweak to the consistent hashing algorithm
that significantly mitigates the problem: virtual nodes.</p>
<p>Instead of mapping each node to a single location on the circle, we&#39;ll map
it to <em>V</em> locations instead. There are several ways to do this - the
simplest is just to tweak the node name in some way. For example, when
<tt>AddNode</tt> is called to add <tt>node</tt>, it will run:</p>
<div><pre><span></span><span>for</span><span> </span><span>i</span><span> </span><span>:=</span><span> </span><span>range</span><span> </span><span>V</span><span> </span><span>{</span><span></span>
<span>  </span><span>vnodeName</span><span> </span><span>=</span><span> </span><span>fmt</span><span>.</span><span>Sprintf</span><span>(</span><span>&#34;%v@%v&#34;</span><span>,</span><span> </span><span>node</span><span>,</span><span> </span><span>i</span><span>)</span><span></span>

<span>  </span><span>// ... now add vnodeName to the nodes/slots slices</span><span></span>
<span>}</span><span></span>
</pre></div>
<p>Then, when looking up an item we&#39;ll run into one of the virtual nodes, decode
the node&#39;s name from it (in our example just strip the <tt>@&lt;number&gt;</tt> suffix)
and return that. Implementing node removal is similarly simple.</p>
<p>The idea is that given a node named <tt>foo</tt>, the virtual node names
<tt>foo@0</tt>, <tt>foo@1</tt>, <tt>foo@2</tt> etc. will be spread all around the circle
and not cluster in a single place. See the Appendix for a calculation of how
this affects the final distribution.</p>
<p>The source code for this post includes a <tt>ConsistentHasherV</tt> type that
is very similar to <tt>ConsistentHasher</tt>, except that it implements the virtual
node strategy. The user interface remains exactly the same - it&#39;s only the
internal implementation that changes slightly.</p>
</div>
<div id="code">
<h2>Code</h2>
<p>The full source code for this post is <a href="https://github.com/eliben/code-for-blog/tree/main/2025/consistent-hashing">on GitHub</a>.</p>
</div>
<div id="appendix">
<h2>Appendix</h2>
<p>The quality of the hash function is very important for good shuffling of nodes
on the circle, but even if we take a perfect hash function that produces
uniformly distributed values, the outcome is likely to be sub-optimal for our
needs.</p>
<p>Let&#39;s say we select  points on the unit circle, uniformly in the range
<tt>[0, 1)</tt>. If we sort the points by angle, the gaps between neighboring angles
are the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>.
These follow the <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>
with parameters <tt>(1, <span>N-1)</span></tt>, which has a mean of  and a
variance of .</p>
<p>This is quite significant. Consider a circle with 20 nodes. The standard
deviation of the distribution is the square root of the variance; substituting
, we get:</p>
<p>With 20 nodes uniformly distributed around a circle, we can expect an average of
18 degrees distance between two nodes. A standard deviation of 0.048 means 17
degrees, which is comparable to the average!</p>
<p>We can also do a realistic example to demonstrate this. Let&#39;s generate 20 random
angles on a circle, and show how the node distribution looks:</p>
<p><img alt="Circle with randomly distributed points" src="https://eli.thegreenplace.net/images/2025/circle-random-distribution.png"/></p><p>In this particular sample, the average angle between two adjacent nodes is
18 degrees (as expected). The smallest angle is just 1.04 degrees, while the
largest one is 42 degrees. This means that some nodes will get 40x as many
items assigned to them as others!</p>
<p>It&#39;s easy to see how virtual nodes help; imagine that each server maps to
some number of randomly distributed nodes on the circle; some of these will
be farther than others from their closest neighbor, but the average will have
much less variety. Mathematically, given a set of <em>n</em> uniformly distributed
random variables with variance <em>v</em>, the variance of their average is
.</p>
<p>As a concrete experiment, I ran a similar simulation to the one above, but
with 10 virtual nodes per node. We&#39;ll consider the total portion of the circle
mapped to a node when it maps to either of its virtual nodes. While the
average remains 18 degrees, the variance is reduced drastically - the smallest
one is 11 degrees and the largest 26.</p>
<p>You can find the code for
these experiments in the <tt>demo.go</tt> file of the source code repository.</p>
<hr/>




</div>

            </div></div>
  </body>
</html>
