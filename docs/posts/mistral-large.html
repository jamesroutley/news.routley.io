<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mistral.ai/news/mistral-large/">Original</a>
    <h1>Mistral Large</h1>
    
    <div id="readability-page-1" class="page"><div><p>We are releasing <em>Mistral Large</em>, our latest and most advanced language
model. <em>Mistral Large</em> is available through la Plateforme. We are also making
it available through Azure, our first distribution partner.</p><h4 id="mistral-large-our-new-flagship-model">Mistral Large, our new flagship model</h4><p>Mistral Large is our new cutting-edge text generation model. It reaches
top-tier reasoning capabilities. It can be used for complex multilingual
reasoning tasks, including text understanding, transformation, and code
generation.</p><p>Mistral Large achieves strong results on commonly used benchmarks,
making it the world&#39;s second-ranked model generally available through
an API (next to GPT-4) [see
below for details on bencharks].</p><p><img src="https://mistral.ai/images/news/mistral-large/mistral-large-bar-plot.png" alt="Detailed benchmarks" width="100%"/></p><p>Figure 1: Comparison of GPT-4, Mistral Large (pre-trained), Claude 2, Gemini Pro 1.0, GPT 3.5 and LLaMA 2 70B on MMLU (Measuring massive multitask language
understanding).</p><p>Mistral Large comes with new capabilities and strengths:</p><ul><li><p>It is <strong>natively fluent in English, French, Spanish, German, and
Italian,</strong> with a nuanced understanding of grammar and cultural
context.</p></li><li><p>Its <strong>32K tokens context window</strong> allows precise information recall
from large documents.</p></li><li><p>Its <strong>precise instruction-following</strong> enables developers to design
their <strong>moderation policies</strong> – we used it to set up the system-level moderation of le Chat.</p></li><li><p><strong>It is natively capable of function calling.</strong> This, along with
constrained output mode, implemented on la Plateforme, enables
application development and tech stack modernisation at scale.</p></li></ul><h4 id="partnering-with-microsoft-to-provide-our-models-on-azure"><strong>Partnering with Microsoft to provide our models on Azure</strong></h4><p>At Mistral, our mission is to make frontier AI ubiquitous. This is why
we’re announcing today that we’re bringing our open and commercial
models to Azure. Microsoft’s trust in our model is a step forward in
our journey! Our models are now available through:</p><ol><li><p><strong>La Plateforme</strong>: safely hosted on Mistral’s infrastructure in
Europe, this access point enables developers to create applications
and services across our comprehensive range of models.</p></li><li><p><strong>Azure:</strong> Mistral Large is available through <a href="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistal-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai-models/ba-p/4066996">Azure AI Studio and Azure
Machine Learning</a>, with as seamless a user experience as our APIs. Beta customers
have used it with <a href="https://mistral.ai/business/">significant success</a>.</p></li><li><p><strong>Self-deployment</strong>: our models can be deployed on your environment
for the most sensitive use cases with access to our model weights;
Read success stories on this kind of deployment, and <a href="https://mistral.ai/contact/">contact our team</a> for further details.</p></li></ol><h4 id="mistral-large-capacities"><strong>Mistral Large capacities</strong></h4><p>We compare Mistral Large&#39;s performance to the top-leading LLM models on
commonly used benchmarks.</p><p><strong>Reasoning and knowledge</strong></p><p>Mistral Large shows powerful reasoning capabilities. In the following figure, we report the performance of the pretrained models on standard benchmarks.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-reasoning-table.svg" alt="Detailed benchmarks" width="100%"/></p><p>Figure 2: Performance on widespread common sense, reasoning and
knowledge benchmarks of the top-leading LLM models on the market: MMLU
(Measuring massive multitask language in understanding), HellaSwag
(10-shot), Wino Grande (5-shot), Arc Challenge (5-shot), Arc Challenge
(25-shot), TriviaQA (5-shot) and TruthfulQA.</p><p><strong>Multi-lingual capacities</strong></p><p>Mistral Large has native multi-lingual capacities. It strongly
outperforms LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU benchmarks
in French, German, Spanish and Italian.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-multilingual-table.svg" alt="Detailed benchmarks" width="100%"/></p><p>Figure 3: Comparison of Mistral Large, Mixtral 8x7B and LLaMA 2 70B on
HellaSwag, Arc Challenge and MMLU in French, German, Spanish and
Italian.</p><p><strong>Maths &amp; Coding</strong></p><p>Mistral Large shows top performance in coding and math tasks.
In the table below, we report the performance across a suite of
popular benchmarks to evaluate the coding and math performance for some
of the top-leading LLM models.</p><p><img src="https://mistral.ai/images/news/mistral-large/large-bench-coding-maths-table.svg" alt="Detailed benchmarks" width="100%"/></p><p>Figure 4: Performance on popular coding and math benchmarks of the leading LLM models on the market: HumanEval pass@1, MBPP pass@1, Math maj@4, GSM8K maj@8
(8-shot) and GSM8K maj@1 (5 shot).</p><h3 id="a-new-mistral-small-optimised-for-low-latency-workloads"><strong>A new Mistral Small, optimised for low latency workloads</strong></h3><p>Alongside Mistral Large, we’re releasing a new optimised model, Mistral
Small, optimised for latency and cost. Mistral Small outperforms Mixtral 8x7B and has
lower latency, which makes it a refined intermediary
solution between our open-weight offering and our flagship model.</p><p>Mistral Small benefits from the same innovation as Mistral Large
regarding RAG-enablement and function calling.</p><p>We’re simplifying our endpoint offering to provide the following:</p><ul><li><p>Open-weight endpoints with competitive pricing. This comprises
<code>open-mistral-7B</code> and <code>open-mixtral-8x7b</code>.</p></li><li><p>New optimised model endpoints, <code>mistral-small-2402</code> and <code>mistral-large-2402</code>.
We’re maintaining <code>mistral-medium</code>, which we are not updating today.</p></li></ul><p>Our <a href="https://docs.mistral.ai/platform/endpoints/">benchmarks</a> give a comprehensive view of performance/cost tradeoffs.</p><p>Beyond the new model offering, we’re allowing organisation management
multi-currency pricing and have updated service tiers on la Plateforme.
We have also made a lot of progress in
reducing the latency of all our endpoints.</p><h3 id="json-format-and-function-calling"><strong>JSON format and function calling</strong></h3><p>JSON format mode forces the language model output to be valid JSON. This
functionality enables developers to interact with our models more
naturally to extract information in a structured format that can be
easily used in the remainder of their pipelines.</p><p>Function calling lets developers interface Mistral endpoints with a
set of their own tools, enabling more complex interactions with internal
code, APIs or databases. You will learn more in our <a href="https://docs.mistral.ai/guides/function-calling">function
calling guide</a>.</p><p>Function calling and JSON format are only available on mistral-small and
mistral-large. We will be adding formatting to all endpoints shortly, as
well as enabling more fine-grained format definitions.</p><p><strong>Try Mistral Large and Mistral Small today</strong></p><p>Mistral Large is available on La Plateforme and Azure as of today.
Mistral Large is also exposed on our beta assistant demonstrator, <a href="https://chat.mistral.ai/">le
Chat</a>. As always, we’re eager to have your feedback!</p></div></div>
  </body>
</html>
