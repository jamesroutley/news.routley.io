<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://utcc.utoronto.ca/~cks/space/blog/web/SingleProcessServingAppeal">Original</a>
    <h1>The appeal of serving your web pages with a single process</h1>
    
    <div id="readability-page-1" class="page"><div><h2>The appeal of serving your web pages with a single process</h2>

	<p><small>April 17, 2025</small></p>
</div><div><p>As I slowly work on updating the software behind this blog to deal
with <a href="https://utcc.utoronto.ca/~cks/space/blog/web/SyndicationFeedsRatelimitedNow">the unfortunate realities of the modern web</a> (<a href="https://utcc.utoronto.ca/~cks/space/blog/web/OldBrowsersCrawlerProblem">also</a>),
I&#39;ve found myself thinking (more than once) how much simpler my
life would be if I was serving everything through a single process,
instead of my eccentric, more or less stateless CGI-based approach.
The simple great thing about doing everything through a single
process (with threads, goroutines, or whatever inside it for
concurrency) is that you have all the shared state you could ever
want, and that shared state makes it so easy to do so many things.</p>

<p>Do you have people hitting one URL too often from a single IP
address? That&#39;s easy to detect, track, and return <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429">HTTP 429</a>
responses for until they cool down. Do you have an IP making too
many requests across your entire site? You can track that sort of
volume information. There&#39;s all sorts of potential bad stuff that
it&#39;s at least easier to detect when you have easy shared global
state. And the other side of this is that it&#39;s also relatively
easy to add simple brute force caching in a single process with
global state.</p>

<p>(Of course you have some practical concerns about memory and CPU
usage, depending on how much stuff you&#39;re keeping track of and for
how long.)</p>

<p>You can do a certain amount of this detection with a separate
&#39;database&#39; process of some sort (or a database file, like sqlite),
and there&#39;s various specialized software that will let you keep
this sort of data in memory (instead of on disk) and interact with
it easily. But this is an extra layer or two of overhead over
simply updating things in your own process, especially if you have
to set up things like a database schema for what you&#39;re tracking
or caching.</p>

<p>(It&#39;s my view that ease of implementation is especially useful when
you&#39;re not sure what sort of anti-abuse measures are going to be
useful. The easier it is to implement something and at least get
logs of what and how much it would have done, the more you&#39;re going
to try and the more likely you are to hit on something that works
for you.)</p>

<p>Unfortunately it seems like we&#39;re only going to need more of this
kind of thing in our immediate future. I don&#39;t expect the level of
crawling and abuse to go down any time soon; if anything, I expect
it to keep going up, especially as more and more websites move
behind effective but heavyweight precautions and the crawlers turn
more of their attention to the rest of us.</p>
</div></div>
  </body>
</html>
