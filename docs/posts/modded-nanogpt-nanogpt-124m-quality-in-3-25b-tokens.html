<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/KellerJordan/modded-nanogpt">Original</a>
    <h1>Modded-NanoGPT: NanoGPT (124M) quality in 3.25B tokens</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This is a variant of the <a href="https://github.com/karpathy/llm.c/blob/7b929300217ff1a974b63791a228928b39b26409/train_gpt2.py">PyTorch GPT-2 trainer</a> from
Andrej Karpathy&#39;s <a href="https://github.com/karpathy/llm.c">llm.c</a> repo. It:</p>
<ul dir="auto">
<li>Trains 3x more efficiently (taking only 3.15B tokens instead of 10B to reach the same validation loss).</li>
<li>Has shorter code (524 lines instead of 860).</li>
<li>Implements architectural modernizations (rotary embeddings, RMSNorm, ReLU^2).</li>
<li>Implements a new optimizer (Muon - Momentum Orthogonalized by Newton-schulz).</li>
</ul>
<p dir="auto">To execute the training, run the following three commands on an 8xA100 or 8xH100 node.
They complete in &lt;45min on an 8xH100 with decent internet connection.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt
python data/cached_fineweb10B.py 35 # downloads the first 3.5B tokens
./run.sh"><pre>pip install -r requirements.txt
python data/cached_fineweb10B.py 35 <span><span>#</span> downloads the first 3.5B tokens</span>
./run.sh</pre></div>
<p dir="auto">This will train a 124M-parameter transformer for 6000 steps on 3.15B tokens of Fineweb [1], achieving ~3.275 validation loss.
For comparison, the default llm.c PyTorch trainer yields <a href="https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">&gt;3.28 validation loss after training for 10B tokens</a>.</p>
<hr/>

<p dir="auto">Figure 1. Proposed optimizer vs. a well-tuned AdamW.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/KellerJordan/modded-nanogpt/blob/master/img/fig_optimizer.png"><img src="https://github.com/KellerJordan/modded-nanogpt/raw/master/img/fig_optimizer.png" alt=""/></a></p>
<hr/>

<p dir="auto">For this training scenario, the proposed optimizer has the following properties:</p>
<ul dir="auto">
<li>Half the memory usage of Adam</li>
<li>1.5x faster training</li>
<li>&lt;9% wallclock overhead (which can be further brought down by distributing the overhead; it&#39;s currently performed redundantly on all 8 GPUs)</li>
</ul>
<p dir="auto">The optimizer is defined as follows:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/KellerJordan/modded-nanogpt/blob/master/img/algo_optimizer.png"><img src="https://github.com/KellerJordan/modded-nanogpt/raw/master/img/algo_optimizer.png" alt=""/></a></p>
<p dir="auto">Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3]:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@torch.compile
def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() / (G.norm() + eps)
    if G.size(0) &gt; G.size(1):
        X = X.T 
    for _ in range(steps):
        A = X @ X.T 
        B = A @ X 
        X = a * X + b * B + c * A @ B 
    if G.size(0) &gt; G.size(1):
        X = X.T 
    return X.to(G.dtype)"><pre><span>@<span>torch</span>.<span>compile</span></span>
<span>def</span> <span>zeroth_power_via_newtonschulz5</span>(<span>G</span>, <span>steps</span><span>=</span><span>5</span>, <span>eps</span><span>=</span><span>1e-7</span>):
    <span>assert</span> <span>len</span>(<span>G</span>.<span>shape</span>) <span>==</span> <span>2</span>
    <span>a</span>, <span>b</span>, <span>c</span> <span>=</span> (<span>3.4445</span>, <span>-</span><span>4.7750</span>,  <span>2.0315</span>)
    <span>X</span> <span>=</span> <span>G</span>.<span>bfloat16</span>() <span>/</span> (<span>G</span>.<span>norm</span>() <span>+</span> <span>eps</span>)
    <span>if</span> <span>G</span>.<span>size</span>(<span>0</span>) <span>&gt;</span> <span>G</span>.<span>size</span>(<span>1</span>):
        <span>X</span> <span>=</span> <span>X</span>.<span>T</span> 
    <span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>steps</span>):
        <span>A</span> <span>=</span> <span>X</span> @ <span>X</span>.<span>T</span> 
        <span>B</span> <span>=</span> <span>A</span> @ <span>X</span> 
        <span>X</span> <span>=</span> <span>a</span> <span>*</span> <span>X</span> <span>+</span> <span>b</span> <span>*</span> <span>B</span> <span>+</span> <span>c</span> <span>*</span> <span>A</span> @ <span>B</span> 
    <span>if</span> <span>G</span>.<span>size</span>(<span>0</span>) <span>&gt;</span> <span>G</span>.<span>size</span>(<span>1</span>):
        <span>X</span> <span>=</span> <span>X</span>.<span>T</span> 
    <span>return</span> <span>X</span>.<span>to</span>(<span>G</span>.<span>dtype</span>)</pre></div>

<p dir="auto">Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of <a href="https://github.com/KellerJordan/cifar10-airbench">CIFAR-10 speedrunning</a>.
In particular, we experimentally obtained the following practices:</p>
<ul dir="auto">
<li>Using Nesterov momentum inside the update, with orthogonalization applied after momentum.</li>
<li>Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.</li>
<li>Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations.</li>
<li>Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often compute the preconditioners via inverse-pth-roots in fp32 or fp64).</li>
</ul>
<p dir="auto">Our use of a Newton-Schulz iteration for orthogonalization traces to <a href="https://arxiv.org/abs/2409.20325" rel="nofollow">Bernstein &amp; Newhouse (2024)</a>,
who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation.
In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the
orthogonalization method for this optimizer.
If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful.
Bernstein &amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm,
and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent.
The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs
compared to Shampoo.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Other general differences between this codebase and NanoGPT</h2><a id="user-content-other-general-differences-between-this-codebase-and-nanogpt" aria-label="Permalink: Other general differences between this codebase and NanoGPT" href="#other-general-differences-between-this-codebase-and-nanogpt"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To simplify the code, some features have been removed, including text generation.
And to obtain a training speed improvement, we have diverged from being a strict reproduction of the GPT-2 paper.</p>
<p dir="auto">The speedup is due to the following changes:</p>
<ul dir="auto">
<li>Increased learning rate by 3x</li>
<li>Switched to trapezoidal learning rate schedule following [7]</li>
<li>Switched to rotary embeddings and ReLU^2 activation</li>
<li>Removed the special initialization for linear layers before residuals. Instead, just scale down the output of the attention block by a fixed scalar.</li>
<li>Removed all affine scale and bias parameters from the architecture, and switched to RMSNorm (actually this causes a slight slowdown, and I just did it to reduce code complexity)</li>
<li>Switched from AdamW to new optimizer, and removed learning rate warmup</li>
</ul>
<hr/>

<ol dir="auto">
<li><a href="https://arxiv.org/abs/2406.17557" rel="nofollow">Penedo, Guilherme, et al. &#34;The fineweb datasets: Decanting the web for the finest text data at scale.&#34; arXiv preprint arXiv:2406.17557 (2024).</a></li>
<li>Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics, 2008. Equation 5.22.</li>
<li>Günther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57–59, 1933.</li>
<li><a href="https://arxiv.org/abs/2409.20325" rel="nofollow">Jeremy Bernstein and Laker Newhouse. &#34;Old Optimizer, New Norm: An Anthology.&#34; arxiv preprint arXiv:2409.20325 (2024).</a></li>
<li><a href="https://arxiv.org/abs/1802.09568" rel="nofollow">Vineet Gupta, Tomer Koren, and Yoram Singer. &#34;Shampoo: Preconditioned stochastic tensor optimization.&#34; International Conference on Machine Learning. PMLR, 2018.</a></li>
<li><a href="https://arxiv.org/abs/2002.09018" rel="nofollow">Anil, Rohan, et al. &#34;Scalable second order optimization for deep learning.&#34; arXiv preprint arXiv:2002.09018 (2020).</a></li>
<li><a href="https://arxiv.org/abs/2405.18392" rel="nofollow">Hägele, Alexander, et al. &#34;Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations.&#34; arXiv preprint arXiv:2405.18392 (2024).</a></li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/KellerJordan/modded-nanogpt/blob/master/img/dofa.jpg"><img src="https://github.com/KellerJordan/modded-nanogpt/raw/master/img/dofa.jpg" alt="itsover_wereback"/></a></p>
</article></div></div>
  </body>
</html>
