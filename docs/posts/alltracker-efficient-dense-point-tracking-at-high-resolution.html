<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alltracker.github.io/">Original</a>
    <h1>AllTracker: Efficient Dense Point Tracking at High Resolution</h1>
    
    <div id="readability-page-1" class="page"><div>

      <!-- The Grid -->
      <div>

	<!-- paper container -->	  
	<div>
	  
	  
	  <hr/>

          <div>
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/172_rate2_24.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
	      </div>
	  <hr/>
	  <p>
	    <h3>Overview</h3>
	  </p>
	  <p>
	    We introduce AllTracker: a model that estimates long-range point tracks by
	    way of estimating the flow field between a query frame and every other frame of
	    a video. Unlike existing point tracking methods, our approach delivers
	    high-resolution and dense (all-pixel) correspondence fields, which can be
	    visualized as flow maps. Unlike existing optical flow methods, our approach
	    corresponds one frame to hundreds of subsequent frames, rather than just the
	    next frame. We develop a new architecture for this task, blending techniques
	    from existing work in optical flow and point tracking: the model performs
	    iterative inference on low-resolution grids of correspondence estimates,
	    propagating information spatially via 2D convolution layers, and propagating
	    information temporally via pixel-aligned attention layers. The model is fast
	    and parameter-efficient (16 million parameters), and delivers state-of-the-art
	    point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on
	    a 40G GPU). A benefit of our design is that we can train on a wider set of
	    datasets, and we find that doing so is crucial for top performance. We provide
	    an extensive ablation study on our architecture details and training recipe,
	    making it clear which details matter most. Our code and model weights are
	    available.
	  </p>
          <table>
	    <tbody><tr>
	      <td>
		<!-- <div align="center">DINO</div> -->
		<!-- <img style="width:100%;max-width:100%" alt="" src="images/camel_mask.gif"> -->
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/212_rate4_30.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
		
	      </td>
	      <td>
		<!-- <div align="center">RAFT</div> -->
		<!-- <img style="width:100%;max-width:100%" alt="" src="images/fish_mask.gif"> -->
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/monkey_rate4_30.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
		
	      </td>
	      <td>
		<!-- <img style="width:100%;max-width:100%" alt="" src="images/dancer_mask.gif"> -->
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/211_rate4_30.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
		
	      </td>
	    </tr>
	  </tbody></table>
	  <hr/>
	  
	  


	  <!--
	  <div class="w3-center">
	    <h2>Video</h2>
	  </div>
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <iframe width="800" height="600" src="https://www.youtube.com/embed/Jg2f5fkgxZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	  </div>
	  <hr>
	  -->

	  <p>
	    <h3>Native high-resolution dense tracking</h3>
	  </p>
	  <p>Most point trackers operate at either 256x256 or 384x512, and run out of memory at higher resolutions. Our method can operate at 768x1024 on a 40G GPU, and our accuracy reliably scales with resolution. Most point trackers also only track sparse points, while we track every pixel. We only subsample our output for the purpose of clear visualization.</p>
          <table>
	    <tbody><tr>
	      <td>
		<p>No subsampling</p>
	      </td>
	      <td>
		<p>Subsampling rate 2</p>
	      </td>
	      <td>
		<p>Subsampling rate 4</p>
	      </td>
	      <td>
		<p>Subsampling rate 8</p>
	      </td>
	    </tr>
	  </tbody></table>
	  <div>
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/167_stack.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
	      </div>
	  <!-- <p>Our method works even at resolutions like 768x1024 (>700k points). Our experiments show that higher resolutions yield better performance for AllTracker, which is not true for all baselines. </p> -->
	  <hr/>

	  <!-- <div class="w3-center"> -->
	  <!--   <h3>State-of-the-art accuracy</h3> -->
	  <!-- </div> -->
	  <!-- We evaluate on a total of nine publicly-available benchmarks with point annotations, covering a wide set of domains -->
	  <!-- that include animals, YouTube -->
	  <!-- videos, -->
	  <!-- surveillance camera recordings, egocentric recordings, and robotics data. -->
	  <!-- AllTracker outperforms all other models on average, with the -->
	  <!-- closest competitor being CoTracker3, which relies on a much more expensive training setup (real-video bootstrapping).  -->
	  <!-- <table> -->
	  <!--   <tr style="padding:0px"> -->
	  <!--     <td style="width:100%;vertical-align:middle"> -->
	  <!-- 	<img src="images/table1.png" style="width:100%"> -->
	  <!--     </td> -->
	  <!--   </tr> -->
	  <!-- </table> -->
	  <!-- <hr> -->

	  
	  <!-- <div class="w3-center"> -->
	  <!--   <h3>Accuracy scales with resolution</h3> -->
	  <!-- </div> -->
	  <!-- Most point trackers operate at either 256x256 or 384x512, and run out of memory at higher resolutions. Our method scales to 768x1024 on a 40G GPU, and our accuracy reliably gets better at higher resolutions. -->
	  <!-- <table> -->
	  <!--   <tr style="padding:0px"> -->
	  <!--     <td style="width:100%;vertical-align:middle"> -->
	  <!-- 	<img src="images/table2.png" style="width:100%"> -->
	  <!--     </td> -->
	  <!--   </tr> -->
	  <!-- </table> -->
	  
	  <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center"> -->
	  <!--   <iframe width="800" height="600" src="https://www.youtube.com/embed/Jg2f5fkgxZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
	  <!-- </div> -->
	  <!-- <hr> -->

	  <p>
	    <h3>Long-range optical flow, visibility, and confidence, in sliding window</h3>
	  </p>
	  <p>AllTracker works by estimating optical flow between a &#34;query frame&#34; and every other frame of a video, using a sliding-window strategy. We also output visibility and confidence. In relation to other point trackers, this flow formulation is our key novelty, and we make sparse methods (of similar speed and accuracy) redundant. In relation to other optical flow models, the key novelty of our approach is that we solve a window of flow problems simultaneously, instead of frame-by-frame; the information shared within and across windows unlocks the capability to resolve flows across wide time intervals.</p>
          <table>
	    <tbody><tr>
	      <td>
		<p>Input video</p>
	      </td>
	      <td>
		<p>Optical flow</p>
	      </td>
	      <td>
		<p>Visibility</p>
	      </td>
	      <td>
		<p>Confidence</p>
	      </td>
	    </tr>
	  </tbody></table>
	  <div>
		<video controls="" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/000_stack.mp4" type="video/mp4"/>Sorry, your browser doesn&#39;t support embedded videos.
		</video>
	      </div>
	  <hr/>
	  
	  
	  

	  <div>
	    <p><h2>Paper</h2></p>
	    
	    <p><a href="https://arxiv.org/abs/2506.07310"><img src="https://alltracker.github.io/images/alltracker_paperimage.png"/></a>
	    </p>
	    <div>
	      <p>
		Adam W. Harley, Yang You, Xinglong Sun, Yang Zheng, Nikhil Raghuraman, Yunqi Gu, Sheldon Liang, Wen-Hsuan Chu, Achal Dave, Pavel Tokmakov, Suya You, Rares Ambrus, Katerina Fragkiadaki, Leonidas J. Guibas.
		<i>AllTracker: Efficient Dense Point Tracking at High Resolution.</i>
		arXiv 2025.
	      </p>
	      <h3><a href="https://arxiv.org/pdf/2506.07310.pdf">[pdf]</a>â€ƒ<a href="https://alltracker.github.io/bib.txt">[bibtex]</a></h3>
	    </div>
	    
	  </div>
	  <hr/>
	  
	  <!-- end paper container -->

	</div><!-- End Grid -->
      </div><!-- End Page Container -->

  

</div></div>
  </body>
</html>
