<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/May/18/cli-tools-for-llms/">Original</a>
    <h1>CLI tools for working with ChatGPT and other LLMs</h1>
    
    <div id="readability-page-1" class="page"><div>




<p>I’ve been building out a small suite of command-line tools for working with ChatGPT, GPT-4 and potentially other language models in the future.</p>
<p>The three tools I’ve built so far are:</p>
<ul>
<li>
<strong><a href="https://github.com/simonw/llm">llm</a></strong>—a command-line tool for sending prompts to the OpenAI APIs, outputting the response and logging the results to a SQLite database. I introduced that <a href="https://simonwillison.net/2023/Apr/4/llm/">a few weeks ago</a>.</li>
<li>
<strong><a href="https://github.com/simonw/ttok">ttok</a></strong>—a tool for counting and truncating text based on tokens</li>
<li>
<strong><a href="https://github.com/simonw/strip-tags">strip-tags</a></strong>—a tool for stripping HTML tags from text, and optionally outputting a subset of the page based on CSS selectors</li>
</ul>
<p>The idea with these tools is to support working with language model prompts using Unix pipes.</p>
<p>You can install the three like this:</p>
<div><pre>pipx install llm
pipx install ttok
pipx install strip-tags</pre></div>
<p>Or use <code>pip</code> if you haven’t adopted <a href="https://pypa.github.io/pipx/">pipx</a> yet.</p>
<p><code>llm</code> depends on an OpenAI API key in the <code>OPENAI_API_KEY</code> environment variable or a <code>~/.openai-api-key.txt</code> text file. The other tools don’t require any configuration.</p>
<p>Now let’s use them to summarize the homepage of the New York Times:</p>
<div><pre>curl -s https://www.nytimes.com/ \
  <span>|</span> strip-tags .story-wrapper \
  <span>|</span> ttok -t 4000 \
  <span>|</span> llm --system <span><span>&#39;</span>summary bullet points<span>&#39;</span></span> -s</pre></div>
<p>Here’s what that command outputs when you run it in the terminal:</p>
<p><img src="https://static.simonwillison.net/static/2023/llm-nytimes.gif" alt="Animated output from running that command: 1. Senator Dianne Feinstein suffered complications from encephalitis during her recent bout with shingles, which has raised concerns about her health among some of her allies. 2. Investors, economists, and executives are preparing contingency plans in case of a possible United States debt default, but the timeline for when the government will run out of cash is uncertain. 3. The Pentagon has freed up an additional $3 billion for Ukraine through an accounting mistake, relieving pressure on the Biden administration to ask Congress for more money for weapon supplies. 4. Explosions damaged a Russian-controlled freight train in Crimea, and the railway operator has suggested that it may have been an act of sabotage, but there is no confirmation yet from Ukrainian authorities. 5. Group of Seven leaders are expected to celebrate the success of a novel effort to stabilize global oil markets and punish Russia through an untested oil price cap."/></p>
<p>Let’s break that down.</p>
<ul>
<li>
<code>curl -s https://www.nytimes.com/</code> uses <code>curl</code> to retrieve the HTML for the New York Times homepage—the <code>-s</code> option prevents it from outputting any progress information.</li>
<li>
<code>strip-tags .story-wrapper</code> accepts HTML to standard input, finds just the areas of that page identified by the CSS selector <code>.story-wrapper</code>, then outputs the text for those areas with all HTML tags removed.</li>
<li>
<code>ttok -t 4000</code> accepts text to standard input, tokenizes it using the default tokenizer for the <code>gpt-3.5-turbo</code> model, truncates to the first 4,000 tokens and outputs those tokens converted back to text.</li>
<li>
<code>llm --system &#39;summary bullet points&#39; -s</code> accepts the text to standard input as the user prompt, adds a system prompt of “summary bullet points”, then the <code>-s</code> option tells the tool to stream the results to the terminal as they are returned, rather than waiting for the full response before outputting anything.</li>
</ul>
<h3>It’s all about the tokens</h3>
<p>I built <code>strip-tags</code> and <code>ttok</code> this morning because I needed better ways to work with tokens.</p>
<p>LLMs such as ChatGPT and GPT-4 work with tokens, not characters.</p>
<p>This is an implementation detail, but it’s one that you can’t avoid for two reasons:</p>
<ol>
<li>APIs have token limits. If you try and send more than the limit you’ll get an error message like this one: “This model’s maximum context length is 4097 tokens. However, your messages resulted in 116142 tokens. Please reduce the length of the messages.”</li>
<li>Tokens are how pricing works. <code>gpt-3.5-turbo</code> (the model used by ChatGPT, and the default model used by the <code>llm</code> command) costs $0.002 / 1,000 tokens. GPT-4 is $0.03 / 1,000 tokens of input and $0.06 / 1,000 for output.</li>
</ol>
<p>Being able to keep track of token counts is really important.</p>
<p>But tokens are actually really hard to count! The rule of thumb is roughly 0.75 * number-of-words, but you can get an exact count by running the same tokenizer that the model uses on your own machine.</p>
<p>OpenAI’s <a href="https://github.com/openai/tiktoken">tiktoken</a> library (documented <a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">in this notebook</a>) is the best way to do this.</p>
<p>My <code>ttok</code> tool is a <a href="https://github.com/simonw/ttok/blob/0.1/ttok/cli.py">very thin wrapper</a> around that library. It can do three different things:</p>
<ul>
<li>Count tokens</li>
<li>Truncate text to a desired number of tokens</li>
<li>Show you the tokens</li>
</ul>
<p>Here’s a quick example showing all three of those in action:</p>
<div><pre>$ <span>echo</span> <span><span>&#39;</span>Here is some text<span>&#39;</span></span> <span>|</span> ttok
5
$ <span>echo</span> <span><span>&#39;</span>Here is some text<span>&#39;</span></span> <span>|</span> ttok --truncate 2
Here is
$ <span>echo</span> <span><span>&#39;</span>Here is some text<span>&#39;</span></span> <span>|</span> ttok --tokens    
8586 374 1063 1495 198</pre></div>
<p>My <a href="https://observablehq.com/@simonw/gpt-3-token-encoder-decoder">GPT-3 token encoder and decoder</a> Observable notebook provides an interface for exploring how these tokens work in more detail.</p>
<h3>Stripping tags from HTML</h3>
<p>HTML tags take up a lot of tokens, and usually aren’t relevant to the prompt you are sending to the model.</p>
<p>My new <code>strip-tags</code> command strips those tags out.</p>
<p>Here’s an example showing quite how much of a difference that can make:</p>
<div><pre>$ curl -s https://simonwillison.net/ <span>|</span> ttok
21543
$ curl -s https://simonwillison.net/ <span>|</span> strip-tags <span>|</span> ttok
9688</pre></div>
<p>For my blog’s homepage, stripping tags reduces the token count by more than half!</p>
<p>The above is still too many tokens to send to the API.</p>
<p>We could truncate them, like this:</p>
<div><pre>$ curl -s https://simonwillison.net/ \
  <span>|</span> strip-tags <span>|</span> ttok --truncate 4000 \
  <span>|</span> llm --system <span><span>&#39;</span>turn this into a bad poem<span>&#39;</span></span> -s</pre></div>
<p>Which outputs:</p>
<pre><code>download-esm,

A tool to download ECMAScript modules.

Get your packages straight from CDN,

No need for build scripts, let that burden end.

All dependencies will be fetched,

Import statements will be re-writched.

Works like a charm, simple and sleek,

JavaScript just got a whole lot more chic.
</code></pre>
<p>But often it’s only specific parts of a page that we care about. The <code>strip-tags</code> command takes an optional list of CSS selectors as arguments—if provided, only those parts of the page will be output.</p>
<p>That’s how the New York Times example works above. Compare the following:</p>
<div><pre>$ curl -s https://www.nytimes.com/ <span>|</span> ttok             
210544
$ curl -s https://www.nytimes.com/ <span>|</span> strip-tags <span>|</span> ttok
115117
$ curl -s https://www.nytimes.com/ <span>|</span> strip-tags .story-wrapper <span>|</span> ttok
2165</pre></div>
<p>By selecting just the text from within the <code>&lt;section class=&#34;story-wrapper&#34;&gt;</code> elements we can trim the whole page down to just the headlines and summaries of each of the main articles on the page.</p>
<h3>Future plans</h3>
<p>I’m really enjoying being able to use the terminal to interact with LLMs in this way. Having a quick way to pipe content to a model opens up all kinds of fun opportunities.</p>
<p>Want a quick explanation of how some code works using GPT-4? Try this:</p>
<pre><code>cat ttok/cli.py | llm --system &#39;Explain this code&#39; -s --gpt4
</code></pre>
<p>(<a href="https://gist.github.com/simonw/a06e091310ceee0b9d5146722279c93c">Output here</a>).</p>
<p>I’ve been having fun piping my <a href="https://shot-scraper.datasette.io/">shot-scraper tool</a> into it too, which goes a step further than <code>strip-tags</code> in providing a full headless browser.</p>
<p>Here’s an example that uses the <a href="https://til.simonwillison.net/shot-scraper/readability">Readability recipe from this TIL</a> to extract the main article content, then further strips HTML tags from it and pipes it into the <code>llm</code> command:</p>
<div><pre>shot-scraper javascript https://www.theguardian.com/uk-news/2023/may/18/rmt-to-hold-rail-strike-across-england-on-eve-of-fa-cup-final <span><span>&#34;</span></span>
<span>async () =&gt; {</span>
<span>    const readability = await import(&#39;https://cdn.skypack.dev/@mozilla/readability&#39;);</span>
<span>    return (new readability.Readability(document)).parse().content;</span>
<span>}<span>&#34;</span></span> <span>|</span> strip-tags <span>|</span> llm --system summarize</pre></div>
<p>In terms of next steps, the thing I’m most excited about is teaching that <code>llm</code> command how to talk to other models—initially Claude and PaLM2 via APIs, but I’d love to get it working against locally hosted models running on things like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> as well.</p>




</div></div>
  </body>
</html>
