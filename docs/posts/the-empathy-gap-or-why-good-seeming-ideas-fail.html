<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nathanieltravis.com/2021/12/16/why-good-research-ideas-fail/">Original</a>
    <h1>The empathy gap, or why good-seeming ideas fail</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main" role="main">

			
<article id="post-493">

	

	
	<div>
		
<p>Lessons from Google and My Time Working with Great Researchers</p>







<figure><img src="https://twin-cities.umn.edu/sites/twin-cities.umn.edu/files/media/A-question-mark-inside-a-light-bulb.jpg" alt=""/></figure>







<p>One day in my life as a machine learning researcher, I had a new idea, and it felt like a good idea. I had a rush of excitement, but then… some hesitation. As always, I knew that having an idea that <em>feels good</em> is different from having an idea that’s <em>actually good</em>. </p>



<p>The ultimate test of whether an idea is <em>actually good</em> is to see if it works in the real world. Testing in the real world, though, requires careful implementation and experimentation. It takes time. So in the not-so-off chance that I was hallucinating, I wanted to pitch my idea to a colleague first and see what they had to say.</p>



<p>I went to my colleague and friend, Ram (like the animal but pronounced with an “ah”). Ram is a smart guy. The kind of guy you can tell a crazy, half-baked, hyper-technical idea to and he’ll immediately know what you mean even better than you know what you mean. The kind of guy that slays illogical arguments the way Achilles slayed Trojans.</p>



<p>If there was an obvious flaw in my idea, Ram would no doubt find it and save me some time and effort. I told him my idea and, to my delight, he said “sounds like a good idea to me.” After that, I wasted no time. I ran to my desk, eagerly coded it up, and began running tests, watching as the tensorboard graphs updated minute-by-minute…</p>



<p>It didn’t work.</p>



<p>I mean, it didn’t <em>completely</em> not work, but any improvements were so miniscule and inconsistent that it may as well have completely not worked.</p>



<p>In the field of machine learning (and possibly every field of science and technology), this type of failure—where an idea that seems intuitively good to you, and even to other experts, is not actually good—is incredibly common. I’d go so far as to estimate the failure rate of good-seeming ideas to be at least 90%. There are perhaps ML gods who are an exception to the rule, but for mere mortals and even demigods, the rule rings true. And given that most ideas can take a lot of time and effort to test—days, weeks, even months for exceptionally ambitious ideas—this is an exhausting statistic.</p>



<p>Which brings us to the questions I want to ask today: (1) <em>why is the failure rate of good-seeming ideas so high?</em> and (2) <em>what should we do about it?</em></p>



<p>To begin to answer (1), it’s clear that the problem lies with our intuition. More specifically, I’d claim that it often lies in our attempt and inability to empathize with the model. It’s an <em>empathy gap</em>.</p>



<p>Empathy—the ability to understand other humans from their perspective—is a human superpower. It’s a superpower that works in many realms of life, so we try to use it in many realms of life, even highly technical, highly non-human realms. With an ML model, we try to get a feel for how this model “thinks,” what this model “wants.” We’re tempted to personify a model, to think of a neural network like a human brain. But of course…a model is not a person or a human brain. Building a better ML model is more like building a better frog than building a better human.</p>



<p>Sure, we likely understand a thing or two about frogs. We watch nature documentaries.</p>



<p><em>“Watch as the exquisite tree frog of South America leaps from branch to branch, searching for its mate…”</em></p>



<p>But even if we’ve seen a few leaping tree frogs, we still don’t know <em>that much</em> about frogs. If we want to build a better frog, we need to understand every detail of the frogs that already exist. Their habitats, behaviors, genotypes, phenotypes, predators, prey, adaptations, and quirks. Their detailed history and evolution.</p>



<p>In the field of machine learning, most of the big breakthroughs have at least one historical great of the field involved in some fashion. There are many explanations for why this could be the case. One explanation I believe is important, though, is the fact that the historical greats of the field are, by definition, historians of the field. And it helps to be a historian of the field.</p>



<p>The current state-of-the-art in machine learning is an amalgamation of semi-arbitrary constructs: batch norm, dot product attention, dropout, the adam optimizer, xavier weight initialization, input augmentation, adversarial regularization. The details boggle the mind, and each detail matters. Most ideas that feel good are, in fact, good in theory but simply don’t play well with one or more pre-existing details. </p>



<p>You pull one jenga piece and the whole tower falls down.</p>



<p>—</p>



<p>Besides <em>being a historian</em>, another way to increase your model empathy is to increase your <em>model intimacy</em>. To illustrate, let me tell another quick story. This time a story of when a good-seeming idea of mine actually was good (or at least not completely useless).</p>



<p>During my time at Google, when I was working on ML for Google News, I consulted with researchers in Google’s AI divisions. Although I was a professional doing research, I was essentially a professional amateur—and at the time still early in learning the field. The main researcher who I consulted with was Samy Bengio, a true master of the craft. A sage though not the kind of sage that speaks quietly and cryptically. More an energetic fountain of lessons and ideas.</p>



<p>At the time, my team and I were developing a new kind of news personalization model. We got positive early results. But then, we hit a wall. For the early model, we were using pretrained embeddings and keeping them fixed. We knew that wasn’t ideal, though. The embeddings should be trained with the model, not kept fixed, since this would make the embeddings better targeted to the task at hand. </p>



<p>Thus, we tried to allow our embeddings to be trained. And…strange things occurred. The model began making bizarre predictions that appeared to have no basis in reality. </p>



<p>We peered through the data, example after example. We tried dozens of methods to fix the problem with no success. We saw (or perhaps merely thought we saw) patterns in the bizarrities but could make no headway. Finally, we booked a meeting with sage, hoping that he could solve our problem.</p>



<p>In the meeting, we found Samy with another guy, Ian Goodfellow. You may know Ian as the creator of GANs. (And in case you don’t know what GANs are, they’re a big deal.) A good fellow indeed but also an interesting fellow. He was young and soft-spoken with a bit of a nervous energy. But, at the same time, he had powerful eyes, like he was seeing a different plane of existence.</p>



<p>This was a powerful duo helping us out.</p>



<p>We explained our problem to them, including the strange patterns and behaviors we had seen (we knew the problem pretty well by this point). Samy and Ian tried their best to understand and gave several suggestions on how to solve it. We then went back and feverishly tried every solution they suggested.</p>



<p>Alas, none of them worked. In fact, we were forced to give up on our task and seek other improvements instead. I even went on to work on another project for several months. After several months, though, an idea suddenly struck me. The embedding problem suddenly appeared simple and obvious and the solution simple as well. A five or ten minute implementation. </p>



<p>I tested it out, and voila! It worked.</p>



<p>I mean, it didn’t revolutionize the news feed, but it improved our model and also simplified it.</p>



<p>The lesson here is this: to have model empathy, it helps to have model intimacy. (I suppose it’s also a lesson about leaving and revisiting a problem, but that is a separate matter.)</p>



<p>—</p>



<p>Now before going any deeper on how to have better model empathy, I should mention that some researchers have a completely different approach to avoiding our original dilemma. Namely, to not use empathy—or in some cases, intuition—at all.</p>



<p>This type of researcher is the pure<em> experimentalist</em>, and their strategy is this: (a) create a goal, (b) define a systematic and thorough framework of possible ways to reach the goal, and (c) test all possibilities.</p>



<p>This contrasts with the approach I described before—the <em>theorist</em> approach—where you spend less time developing a system and rigorously experimenting and more time dreaming up a small number of especially promising solutions.</p>



<p>In my experience in ML, or at least applied ML, the experimentalist generally wins. Especially if there is a clock involved. Give a great experimentalist a medium-difficulty problem and one month to solve it, and guaranteed, they’ll come back to you in 3 weeks with a highly effective solution. If you have a small team, or a startup, with severe time and resource constraints, hire the experimentalist. (Ideally still a great experimentalist.)</p>



<p>As a theorist myself, this realization was a bit depressing. I realized, though, that there is still a place for theorists. Specifically, if you want to solve a hard problem—or perhaps a problem you didn’t even know you had—and you have plenty of time, then hire some theorists, put them in a corner somewhere, and let them do their theorizing. Furthermore, as Jon Gertner pointed out in his <a href="https://en.wikipedia.org/wiki/The_Idea_Factory">analysis</a> of Bell Labs’ success, if you put experimentalists and theorists together, they’ll feed off of each other and push each other to new heights.</p>



<p>Let’s not be defeatist though. Life is not so simple. You may be a theorist that ends up in a role with time constraints, or you may be an experimentalist that wants to be able to leap into the unknown. Everyone could use better model empathy.</p>



<p>—</p>



<p>So, have we answered our original questions yet? (Why do good-seeming ideas fail? and what should we do about it?)</p>



<p>Probably not. And, unfortunately, I don’t think we’ll be able to fully answer them today. (They’re hard questions and we have time constraints.)</p>



<p>But to finish, I want to mention one more lesson. I probably shouldn’t call it a lesson because I don’t even fully understand it, but nevertheless, it’s an important factor in model empathy… possibly <em>the</em> factor.</p>



<p>It’s the idea of <em>choosing a good metaphor</em>. </p>



<p>Stephen Jay Gould has described this concept before in his <a href="https://creation.com/james-hutton">analysis</a> of how James Hutton, the “father of modern geology,” figured out how earth’s rock structures are formed. (Hutton’s metaphor for earth was that of a perfect, mechanical machine, designed by God to operate endlessly—very similar to the metaphor Newton used.)</p>



<p>In machine learning, there are many metaphors to choose from. One that I already mentioned is the human brain. But even then, what part of the human brain? Or the human brain in what sense? Memory and dreams inspired the idea of experience replay in reinforcement learning. The image-processing part of the brain inspired Hinton to invent capsule networks.</p>



<p>We could use the metaphor of a frog (I can’t say I recommend it but feel free). We could think in terms of physics and particles, or in terms of geometry and manifolds.</p>



<p>Surely, there are great metaphors that are yet to be explored, especially in the field of AI as a whole—and, well—in every field.</p>



<p>Let’s find them.</p>



<p>—</p>



<p>P.S. Do you have answers to the questions of why good-seeming ideas fail and what we should do about it? If so, feel free to comment below or reply to my tweet. Also, in case you’d prefer a more visual explanation of my points in this post, I’ve created <a href="https://nathanieltravis.com/spongebob_meme1/">a meme just for you</a>.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

		<!-- .entry-auhtor -->

</article><!-- #post-${ID} -->

<!-- #comments -->

		</main><!-- #main -->
	</section><!-- #primary -->


	</div></div>
  </body>
</html>
