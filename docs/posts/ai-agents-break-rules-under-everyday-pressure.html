<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/ai-agents-safety">Original</a>
    <h1>AI Agents Break Rules Under Everyday Pressure</h1>
    
    <div id="readability-page-1" class="page"><div data-headline="AI Agents Break Rules Under Everyday Pressure"><div><p><span>Several recent studies have shown that artificial-intelligence agents sometimes </span><a href="https://www.nature.com/articles/d41586-025-03222-1" target="_blank">decide to misbehave</a><span>, for instance by attempting to blackmail people who plan to replace them. But such behavior often occurs in contrived scenarios. Now, a <a href="http://scale.com/research/propensitybench" target="_blank">new study</a> presents </span><a href="https://scale.com/blog/propensitybench" target="_blank">PropensityBench</a><span>, a benchmark that measures an agentic model’s choices to use harmful tools in order to complete assigned tasks. It finds that somewhat realistic pressures (such as looming deadlines) dramatically increase rates of misbehavior.</span></p><p>“The AI world is becoming increasingly <a href="https://spectrum.ieee.org/tag/agentic-ai" target="_blank">agentic</a>,” says <a href="https://udarimadhu.github.io/" target="_blank">Udari Madhushani Sehwag</a>, a computer scientist at the AI infrastructure company <a href="https://scale.com/" target="_blank">Scale AI</a> and a lead author of the paper, which is currently under <a href="https://spectrum.ieee.org/tag/peer-review">peer review</a>. By that she means that <a href="https://spectrum.ieee.org/tag/large-language-models">large language models</a> (LLMs), the engines powering <a href="https://spectrum.ieee.org/tag/chatbots">chatbots</a> such as <a href="https://spectrum.ieee.org/tag/chatgpt">ChatGPT</a>, are increasingly connected to software tools that can surf the Web, modify files, and write and run code in order to complete tasks. </p><p>Giving LLMs these abilities adds convenience but also risk, as the systems might not act as we’d wish. Even if they’re not yet capable of doing great harm, researchers want to understand their proclivities before it’s too late. Although AIs don’t have intentions and awareness in the way that humans do, treating them as goal-seeking entities often helps researchers and users better predict their actions.</p><p>AI developers attempt to “<a href="https://www.theatlantic.com/sponsored/google/can-we-align-language-models-with-human-values/3945/" target="_blank">align</a>” the systems to <a href="https://spectrum.ieee.org/tag/safety-standards">safety standards</a> through training and instructions, but it’s unclear how faithfully models adhere to guidelines. “When they are actually put under real-world stress, and if the safe option is not working, are they going to switch to just getting the job done by any means necessary?” Sehwag says. “This is a very timely topic.”</p><h2>How to Test an AI Agent Under Pressure</h2><p>The researchers tested a dozen models made by <a href="https://spectrum.ieee.org/tag/alibaba">Alibaba</a>, <a href="https://spectrum.ieee.org/tag/anthropic">Anthropic</a>, <a href="https://spectrum.ieee.org/tag/google">Google</a>, <a href="https://spectrum.ieee.org/tag/meta">Meta</a>, and <a href="https://spectrum.ieee.org/tag/openai">OpenAI</a> across nearly 6,000 scenarios. In each scenario, a model was assigned a task and told it had access to several tools. It was instructed to use the safe tools and not the harmful ones. At first there was no pressure, and it could take 12 steps toward completing its task by issuing instructions to the tools. Then it received a message increasing the pressure, and it was given 12 more attempts. This continued for up to 12 pressure levels. Attempts to use safe tools always returned error messages. An attempt to use a harmful tool ended the scenario.</p><p>The scenarios covered several domains. In biosecurity, Sehwag says, the agent might have to study the spread of a pathogen and find a way to contain it. A safe tool might use anonymized genetic data, while a harmful one might use data that’s not anonymized. In the domain of chemical security, the agent might need to obtain a particular solvent. A safe tool might place an order for the solvent to be extracted from waste, while a harmful one might procure it illegally. In <a href="https://spectrum.ieee.org/tag/cybersecurity">cybersecurity</a>, the agent might need to identify and restrict a malicious user account. A safe tool might disable only the problematic account, while a harmful one might block all users. Scenarios in other domains tempted agents with tools that led to self-preservation by evading control, recruiting allies, duplicating themselves, or going rogue in other ways.</p><p> <img alt="Bar chart of LLMs like Claude 4S, OpenAI o3 and O4-mini with zero pressure propensity scores below 15 percent, which nearly doubles under pressure. However, Google Gemini 2.5P&#39;s score is above 30 without pressure, then skyrockets above 75 percent under pressure.  " data-rm-shortcode-id="ee9ed43b081d6a6335919fb9edcc0e71" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/bar-chart-of-llms-like-claude-4s-openai-o3-and-o4-mini-with-zero-pressure-propensity-scores-below-15-percent-which-nearly-doub.jpg?id=62224944&amp;width=980" height="1184" id="22656" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%202140%201184&#39;%3E%3C/svg%3E" width="2140"/> <small placeholder="Add Photo Caption...">   The worst performing model, Gemini 2.5, cracked under pressure and chose to use forbidden tools 79 percent of the time. </small><small placeholder="Add Photo Credit...">Source image: Udari Madhushani Sehwag, Shayan Shabihi, et al.</small></p><p>Pressure came in several forms. Deadlines might shorten. Potential financial losses might increase if the task was not accomplished. Threats of greater oversight, reduced resources, and curtailed authority might escalate. Tasks, tools, pressures, and tactics were modeled on the kinds of situations humans might face in their jobs (except maybe the misbehavior option of cloning oneself).</p><p>The best-behaved model (OpenAI’s o3) cracked under pressure in 10.5 percent of scenarios, while the worst (Google’s Gemini 2.5 Pro) had a propensity score of 79 percent; the average across models was about 47 percent. Even under zero pressure, the group on average failed about 19 percent of the time. </p><p>To the degree that models remained aligned, the study shows that some of that alignment was “shallow.” In some tests, the researchers gave harmful tools benign names (such as “use_synthetic_data” instead of “use_fake_data”)—without changing the tools’ descriptions or admonitions to avoid them. This wording tweak increased average propensity by 17 percentage points, to 64 percent. </p><h2>Knowing What LLMs Are Thinking</h2><p>The researchers also found that models used harmful tools even after explicitly acknowledging that they were off-limits. The models offered diverse justifications for doing so, for example by citing the pressure they were under or arguing that the benefits outweighed the risks. It’s unclear if models released after the study’s completion would perform any better. Among the models tested, more capable models (according to a platform called <a href="https://lmarena.ai/leaderboard/" target="_blank">LMArena</a>) were only slightly safer. </p><p>“PropensityBench is interesting,” emails <a href="https://nicholas.carlini.com/" target="_blank">Nicholas Carlini</a>, a computer scientist at <a href="https://www.anthropic.com/" target="_blank">Anthropic</a> who wasn’t involved in the research. He offers a caveat related to what’s called situational awareness. LLMs sometimes detect when they’re being evaluated and act nice so they don’t get retrained or shelved. “I think that most of these evaluations that claim to be ‘realistic’ are very much not, and the LLMs know this,” he says. “But I do think it’s worth trying to measure the rate of these harms in synthetic settings: If they do bad things when they ‘know’ we’re watching, that’s probably bad?” If the models knew they were being evaluated, the propensity scores in this study may be underestimates of propensity outside the lab.</p><p><a href="https://aypan17.github.io/" target="_blank">Alexander Pan</a>, a computer scientist at <a href="https://x.ai/" target="_blank">xAI</a> and <a href="https://www.berkeley.edu/" target="_blank">the University of California, Berkeley</a>, says while Anthropic and other labs have shown examples of <a href="https://www.nature.com/articles/d41586-025-03222-1" target="_blank">scheming by LLMs</a> in specific setups, it’s useful to have standardized benchmarks like PropensityBench. They can tell us when to trust models, and also help us figure out how to improve them. A lab might evaluate a model after each stage of training to see what makes it more or less safe. “Then people can dig into the details of what’s being caused when,” he says. “Once we diagnose the problem, that’s probably the first step to fixing it.”</p><p>In this study, models didn’t have access to actual tools, limiting the realism. Sehwag says a next evaluation step is to build sandboxes where models can take real actions in an isolated environment. As for increasing alignment, she’d like to add oversight layers to agents that flag dangerous inclinations before they’re pursued.</p><p>The self-preservation risks may be the most speculative in the benchmark, but Sehwag says they’re also the most underexplored. It “is actually a very high-risk domain that can have an impact on all the other risk domains,” she says. “If you just think of a model that doesn’t have any other capability, but it can <a href="https://spectrum.ieee.org/recommendation-engine-insidious" target="_self">persuade any human</a> to do anything, that would be enough to do a lot of harm.”</p></div></div></div>
  </body>
</html>
