<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openpipe.ai/blog/mistral-7b-fine-tune-optimized">Original</a>
    <h1>Mistral 7B Fine-Tune Optimized</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p><em>Hey there! Iâ€™m Kyle, the founder of OpenPipe. OpenPipe is the fully-managed fine-tuning platform for developers. Our users have already saved over $2M in inference costs by switching to our fine-tuned models, and it only takes a few minutes to get started.</em></p><p>Since its release in September, <!-- --><a href="https://mistral.ai/news/announcing-mistral-7b/" rel="noopener">Mistral 7B</a> has been the model <!-- --><a href="https://openpipe.ai/blog/llama-2-vs-mistral-believe-the-hype" rel="noopener">weâ€™ve recommended</a> to most of our customers. Today, weâ€™re excited to announce an even stronger variant: <!-- --><a href="https://huggingface.co/OpenPipe/mistral-ft-optimized-1218" rel="noopener">Mistral 7B Fine-Tune Optimized</a>.<!-- --></p><p>Letâ€™s start with the punchline: averaged across 4 diverse customer tasks, fine-tunes based on our new model are <!-- --><strong>slightly stronger than GPT-4</strong>, as measured by GPT-4 itself.<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,O2HuVsgNWtYu18MJcUbFEFDZY2g.png" data-framer-height="528" data-framer-width="972" height="264" src="https://framerusercontent.com/images/O2HuVsgNWtYu18MJcUbFEFDZY2g.png" width="486"/></p><p>Read on for the details!</p><h2>FAQs</h2><h3>GPT-4 is ~100x larger than Mistral. How is this possible?</h3><p>The intuition here is actually quite straightforward. A general-purpose model like GPT-3.5 or GPT-4 has to be good at <!-- --><strong>everything</strong>. It doesnâ€™t know ahead of time what prompt itâ€™ll need to respond to, and so it has to try to encode all of human knowledge. Additionally, every time it gets a new prompt it it has to figure out the right way to respond on the flyâ€”it canâ€™t think deeply about the problem and figure out a repeatable strategy to solve it. It canâ€™t remember the times it has solved the same problem before.<!-- --></p><p>The process of fine-tuning, on the other hand, lets a model spend <!-- --><strong>hours</strong> of training time learning about a specific task and developing strategies to reliably solve it. Even if itâ€™s a less capable model overall, those hours of GPU time can help a fine-tuned model learn the right tricks to efficiently and successfully solve a specific problem.[1]<!-- --></p><h3>There are lots of Mistral fine-tunes. Why another one?</h3><p>A <!-- --><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener">very healthy ecosystem</a> of Mistral fine-tunes already exists, but theyâ€™re typically optimized for direct use. We wanted something different â€” a model optimized to be the strongest <!-- --><strong>base model</strong> for further fine-tunes to be built on. This involves carefully optimizing for instruction understanding and reasoning ability while avoiding â€œcatastrophic forgetting,â€ the tendency for fine-tuned models to get worse at out-of-domain tasks when you fine-tune them for a specific purpose.<!-- --></p><h2>Ok, letâ€™s get the details!</h2><h3>You canâ€™t hit a target you canâ€™t see (Metrics)</h3><p>We started by creating a â€œtest setâ€ of 3 different real OpenPipe customer tasks (with permission). These spanned our most common categories of information extraction, classification, and summarization. The goal was to find or create a new model that, when fine-tuned on these customer tasks, could outperform Mistral-based models on our evals, and become our new default base model.</p><h3>Choose your hero</h3><p>We started by evaluating existing Mistral variants to see how theyâ€™d perform as a base model. After playing around with a number of models we selected six that seemed promising: <!-- --><a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="noopener">OpenHermes 2.5</a>, <!-- --><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" rel="noopener">Zephyr</a>, <!-- --><a href="https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16" rel="noopener">Cybertron</a>, <!-- --><a href="https://huggingface.co/Intel/neural-chat-7b-v3-3" rel="noopener">Intel Neural Chat</a>, <!-- --><a href="https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp" rel="noopener">Hermes Neural</a>, and <!-- --><a href="https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling" rel="noopener">Metamath Cybertron Starling</a>. We created a fine-tuned version of each of these models on each of the 3 evaluation datasets, using a development build of OpenPipe that supports custom base models. This gave us 18 new models in total.<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,0YBrNdAVhE5FXP3EYzEARM5tjk.png" data-framer-height="1578" data-framer-width="1388" height="789" src="https://framerusercontent.com/images/0YBrNdAVhE5FXP3EYzEARM5tjk.png" width="694"/></p><p><em>This dropdown ended up getting really long by the end of this project. </em>ğŸ˜‚<!-- --></p><h3>Beauty in the eye of GPT-4 (Evals)</h3><p>To test each modelâ€™s performance, we used our <!-- --><a href="https://openpipe.ai/blog/announcing-automatic-evals-for-fine-tuned-models" rel="noopener">recently released</a> automated LLM-as-judge evals scored by GPT-4, which allowed us to quickly compare our fine-tunes to each other and gauge their strength.<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,WhIW85zZSh5QTVSbJBE74Ggtw.png" data-framer-height="1374" data-framer-width="1364" height="687" src="https://framerusercontent.com/images/WhIW85zZSh5QTVSbJBE74Ggtw.png" width="682"/></p><p>The top model wasnâ€™t consistent from task to task, but we did notice something interestingâ€”two of the best-performing models overall were <!-- --><a href="https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-v3-3-Slerp" rel="noopener">Hermes Neural</a> and <!-- --><a href="https://huggingface.co/Q-bert/MetaMath-Cybertron-Starling" rel="noopener">Metamath Cybertron Starling</a>, which were both created not by fine-tuning directly but rather through <!-- --><strong>model merging.</strong></p><h3>Magical thinking and model merging ğŸª„ğŸ¤¯</h3><p>Model merging is, to me, one of the most counterintuitive empirical results in modern deep learning. It turns out that you can actually <!-- --><a href="https://github.com/yule-BUAA/MergeLM/tree/main" rel="noopener">more-or-less naively</a> merge the weights of two different models and produce a new one that captures some or all of the abilities of its parents! Since we had a candidate set of already-strong models, we decided to merge a few of the best ones and see if we could make one even stronger.<!-- --></p><p>We ended up testing <!-- --><strong>4</strong> models created by merging our candidates and fine-tuning each of them on our 3 datasets, for a total of 12 additional fine-tuned models.<!-- --></p><p>At this stage, evaluating every fine-tune against every other one across our large test sets felt pretty wasteful, since some models were clearly stronger than others. Instead, we ran <!-- --><strong>9,000</strong> comparisons between our modelsâ€™ outputs and those of GPT-4, GPT-4-turbo, and GPT-3.5, and ranked them using a <!-- --><a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model" rel="noopener">Bradley-Terry</a> ranking system, which is conceptually similar to an Elo rating. (You can see our ugly rating calculation code <!-- --><a href="https://github.com/OpenPipe/OpenPipe/blob/add-models-take-2/app/scripts/rank-base-models.ts" rel="noopener">here</a>). Finally, we got our model rankings, which showed one merge in particular was especially strong:<!-- --></p><p><img alt="" data-framer-asset="data:framer/asset-reference,x8EJIYMrB5tcld5dBKipGZA9ck.png" data-framer-height="1186" data-framer-width="1404" height="593" src="https://framerusercontent.com/images/x8EJIYMrB5tcld5dBKipGZA9ck.png" width="702"/></p><h3>Check yoâ€™self (Validation)</h3><p>This was a really exciting resultâ€”averaged over our three example tasks, one of our merges slightly edged out GPT-4 as the strongest model! But thereâ€™s a problem. Weâ€™d been testing all our models, including the merges, on the same 3 datasetsâ€”was it possible that weâ€™d overfit to those specific tasks?</p><p>To address this concern we selected a new customer dataset we hadnâ€™t used at all thus far (a structured data extraction task). We trained our new merge model as well as a base Mistral model on the new dataset, to verify whether its strong performance generalized to new tasks. Excitingly, the same results held!</p><p><img alt="" data-framer-asset="data:framer/asset-reference,kBQT073XTjgRj96ed3L0RSaiTrE.png" data-framer-height="568" data-framer-width="1376" height="284" src="https://framerusercontent.com/images/kBQT073XTjgRj96ed3L0RSaiTrE.png" width="688"/></p><h2>Weâ€™re just getting started</h2><p>Weâ€™re excited to announce that as of today weâ€™re freely releasing Mistral Fine-Tune Optimized <!-- --><a href="https://huggingface.co/OpenPipe/mistral-ft-optimized-1218" rel="noopener">on Hugging Face</a> and as our new default base model within <!-- --><a href="https://openpipe.ai/" rel="noopener">OpenPipe</a>. Weâ€™re excited to see what our users do with it, but this is just the beginning. Over time weâ€™ll continue releasing more base models that are stronger, faster and cheaper. Weâ€™re looking forward to continue growing alongside the small-model community!<!-- --></p><p>â€”â€”â€”â€”</p><p>[1]: As an aside, thereâ€™s an even stronger result that weâ€™ve found through working with our customers: <!-- --><strong>a student model trained on data generated by a teacher model can exceed the performance of the teacher model on its task</strong>. Weâ€™ve had several customers train a model on GPT-4 outputs, and found that their new model was actually <!-- --><strong>better</strong> than GPT-4 at its task. This is likely due to a kind of regularizationâ€”the fine-tuned model is more likely to give the â€œaverageâ€ answer that GPT-4 would give if prompted many times. This result is different but related to OpenAIâ€™s recently-published research on <!-- --><a href="https://openai.com/research/weak-to-strong-generalization" rel="noopener">weak-to-strong generalization</a>.<!-- --></p></div></div>
  </body>
</html>
