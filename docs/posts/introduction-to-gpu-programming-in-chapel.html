<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chapel-lang.org/blog/posts/intro-to-gpus/">Original</a>
    <h1>Introduction to GPU Programming in Chapel</h1>
    
    <div id="readability-page-1" class="page"><div>
    
    
    

    

    <p>Chapel is a programming language for productive parallel computing.
In recent years, a particular subdomain of parallel computing has exploded
in popularity: GPU computing. As a result, the Chapel team has been hard at
work adding GPU support, making it easy to create vendor-neutral and performant
GPU programs. This aspect of the Chapel compiler has seen rapid improvements since
its initial release, and continues to receive enhancements and performance fixes over time.
This tutorial will provide an introduction to Chapel’s GPU programming features.</p>
<p>Although frameworks such as CUDA and HIP are commonly used for programming
GPUs, this tutorial does not assume familiarity with them. Examples will
make use of some general-purpose features of Chapel, and this post will
explain them along the way. For a more deliberate introduction to Chapel,
see the <a href="https://chapel-lang.org/blog/series/advent-of-code-2022/">Advent of Code</a> series on this blog,
or check the <a href="https://chapel-lang.org/learning.html">Learning Chapel</a> page
for more resources.</p>
<p>In this post, we’ll jump right in to using Chapel’s GPU programming features. If
you’re interested in a refresher on what problems GPUs are useful for,
check out the <a href="#appendix-what-sorts-of-problems-benefit-from-gpus"><em>Appendix</em> section containing this information</a>.</p>
<h3 id="locales-and-on-statements-the-foundation-of-chapel">Locales and <code>on</code> Statements: The Foundation of Chapel</h3>
<p>When GPUs are in play, it’s possible for code to be executing
in different places: it could be running either on a CPU, as most programmers
are used to, or on a GPU. GPUs and CPUs differ significantly;
code well-suited for one may not be well-suited for the other.
Chapel gives the programmer control over where their code is running through
its notion of <em>locales</em>. Quoting from the
<a href="https://chapel-lang.org/docs/primers/locales.html">Chapel specification</a>:</p>
<blockquote>
<p>In Chapel, the <code>locale</code> type refers to a unit of the machine resources on
which your program is running.</p>
</blockquote>
<p>In other words, a locale is a part of the computer that can run code;
this might represent a GPU or a CPU. Chapel’s <em><code>on</code> statement</em>, when given
a locale, can be used to explicitly state where a particular piece of code
should be executed. For example, the following code computes and prints the
even numbers up to ten, running the computation on the first GPU locale.</p>
<div data-code-type="main" data-code-section="first"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>config</span><span> </span><span>const</span><span> </span><span>n</span><span> </span><span>=</span><span> </span><span>5</span><span>;</span><span> </span><span>// use 5-element arrays in examples by default, for brevity
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>on</span><span> </span><span>here</span><span>.</span><span>gpus</span><span>[</span><span>0</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span>:</span><span> </span><span>[</span><span>1</span><span>..</span><span>n</span><span>]</span><span> </span><span>int</span><span>;</span><span> </span><span>// declare an array with n elements
</span></span></span><span><span><span></span><span>                     </span><span>// (to benefit from GPUs, you&#39;d probably want n &gt;&gt; 5)
</span></span></span><span><span><span></span><span>  </span><span>foreach</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>A</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>i</span><span> </span><span>*</span><span> </span><span>2</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;The whole array A is: &#34;</span><span>,</span><span> </span><span>A</span><span>);</span><span> 
</span></span></span><span><span><span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>writeln</span><span>(</span><span>&#34;A[&#34;</span><span>,</span><span> </span><span>i</span><span>,</span><span> </span><span>&#34;] = &#34;</span><span>,</span><span> </span><span>A</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>



<p>The code starts with an <code>on</code> block targeting a GPU <em>sub-locale</em>.
This block references <code>here</code>, a special variable that refers to the
locale currently running the code. When GPU support is enabled, locales
include a field called <code>gpus</code>, which is an array of sub-locales, each
representing an installed GPU. On a locale with a single GPU, <code>here.gpus</code>
will be a single-element array. On locales with more than one GPU (which is
<span>
<label for="frontier-note">common in supercomputers</label>

<span>
<span>[note:</span>

I mentioned supercomputers here for a reason. Chapel&#39;s GPU support
has been tested on large machines, including Frontier, the only
exascale supercomputer in the TOP500 list at the time of publishing.

<span>]</span>
</span>
</span>
), <code>here.gpus</code> will have as many elements as
there are GPUs. Thus, <code>here.gpus[0]</code> is the machine’s first GPU.</p>
<details>
    <summary><strong>(How do I enable GPU support?)</strong></summary>

    <div>
        <p>To enable GPU support, Chapel must be built with with the <code>CHPL_LOCALE_MODEL</code>
environment variable set to <code>gpu</code>. In a Bash session, the variable can be
set as follows:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>export</span> <span>CHPL_LOCALE_MODEL</span><span>=</span>gpu
</span></span></code></pre></div><p>If you have an NVIDIA GPU, this should be enough to compile and run GPU-enabled
programs. For AMD GPUs, you will also need to specify your GPU’s architecture
using the <code>CHPL_GPU_ARCH</code> environment variable:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>export</span> <span>CHPL_GPU_ARCH</span><span>=</span>your_arch_here
</span></span></code></pre></div><p>Finally, even if you don’t have a GPU, Chapel provides a mode called
‘cpu-as-device’. In this mode, you still get a <code>here.gpus</code> array, and
can write code targeting GPUS; however, your computer’s CPU will be used
to execute all code. This makes it possible to develop GPU-enabled programs
without access to a GPU. Setting the <code>CHPL_GPU</code> environment variable to
<code>cpu</code> enables ‘cpu-as-device’ mode:</p>
<p>Please see the page on <a href="https://chapel-lang.org/docs/usingchapel/building.html">building chapel</a>
as well as the <a href="https://chapel-lang.org/docs/technotes/gpu.html">GPU technical note</a>
for more information on GPU-related environment variables.</p>

    </div>
</details>

<p>The code generates even numbers by iterating over the indices
one through five, multiplying each by two. GPUs are good at solving the same
sub-problem many times in parallel; doubling each number can be its own sub-problem,
making the loop in the example well-suited for GPU execution. The multiplication
loop is written using the <code>foreach</code> keyword, which tells Chapel that it
is safe to execute in parallel. The language takes care of the rest.
Chapel has traditional <code>for</code> loops as well, like the second loop in the example.
We’ll talk about the difference between the two further down.</p>
<p>Whereas the <code>foreach</code> loop in the example runs on the GPU, the <code>writeln</code>
on line 9
doesn’t. Unlike the multiplications, printing a single string to the
console is not a good match for the GPU. Generally, code that’s
suitable for GPU execution can be broken up into many similar and independent
pieces. For a loop, this translates into <em>order-independence</em>. A loop
is order-independent if no iteration affects any of the others.
Thinking back to our example, let’s examine the multiplication loop again:</p>
<div><pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>foreach</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>A</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>i</span><span> </span><span>*</span><span> </span><span>2</span><span>;</span><span>
</span></span></span></code></pre></div><p>We can observe that the result of <code>5*2</code> does not affect the
computation of <code>3*2</code>. Each iteration accesses a different element of <code>A</code>,
so there are no data races. Thus, our example is an instance of an
order-independent loop. Chapel leaves it up to the programmer to indicate
which loops have this property; to assert that a loop is order-independent, it
should be written <span>
<label for="forall-note">using the <code>foreach</code> keyword.</label>

<span>
<span>[note:</span>

Chapel also features <code>forall</code> loops. These loops allow the
the data structure being traversed to decide how iteration is parallelized.
Data structures that ship with the Chapel standard library are smart enough
to make use of order-independence, so an eligible <code>forall</code> loop on
a GPU locale would also be executed on the GPU.</span>
</span>
</p>
<p>It’s not hard to see why order-independent loops lend themselves well to
GPU execution. If it doesn’t matter in what order the loop’s iterations are executed, then
we can think of each iteration as an independent sub-problem to be handed off
to a GPU core. This observation is the foundation of Chapel’s GPU support:
<strong>order-independent loops can be executed in parallel on the GPU</strong>. In fact,
Chapel will automatically convert order-independent loops into GPU code
whenever it can.</p>
<details>
    <summary><strong>(I know CUDA/HIP. Can you tell me more about how this works?)</strong></summary>

    <div>
        <p>In CUDA and HIP, writing a GPU-enabled program generally involves creating a function
marked as <code>device</code> or <code>global</code>, and using this function in a
kernel launch. Under the hood, Chapel
does the same.</p>
<p>When Chapel encounters a GPU-eligible loop, it converts its body into a function,
named something like <code>chpl_gpu_kernel_filename_linenumber</code>. If the loop
body / newly defined kernel function contains calls to other functions or
methods, Chapel also generates <code>device</code> versions of these.</p>
<p>Chapel inserts a kernel launch alongside the original loop. Since
the same code could be executed from <code>here.gpus[0]</code> (GPU) or the default
locale (CPU), Chapel preserves the loop as well. Thus, on a GPU it performs
the kernel launch, and on a CPU, it falls back to the loop.</p>

    </div>
</details>

<p>In comparison, the second loop in the example is <em>order-dependent</em>.</p>
<div><pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>writeln</span><span>(</span><span>&#34;A[&#34;</span><span>,</span><span> </span><span>i</span><span>,</span><span> </span><span>&#34;] = &#34;</span><span>,</span><span> </span><span>A</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span></code></pre></div><p>We specifically intend for the elements of <code>A</code> to be printed in order. Because of this,
the iteration that prints the fifth element has to happen after printing the
fourth; there’s a dependency. In Chapel, loops whose iterations need to be
executed one after another (called <em>serial loops</em>) are written using the <code>for</code>
keyword. Loops written using <code>for</code> are not considered by the compiler for GPU
execution.</p>
<p>We have now scrutinized almost every piece of the example. One significant
piece remains: we also declared an array <code>A</code> to contain the even numbers:</p>
<div><pre tabindex="0"><code data-lang="Chapel"><span><span><span>on</span><span> </span><span>here</span><span>.</span><span>gpus</span><span>[</span><span>0</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span>:</span><span> </span><span>[</span><span>1</span><span>..</span><span>n</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// ... the rest of the even numbers example
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p>An important aspect of the <code>on</code> statement is that variables declared inside an <code>on</code>
block logically live on the locale targeted by the statement. Typically, this
means that it’s faster to access these variables from that same locale,
and slower from other locales. Here’s an example:</p>
<div><pre tabindex="0"><code data-lang="Chapel"><span><span><span>on</span><span> </span><span>firstLocale</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span>:</span><span> </span><span>[</span><span>0</span><span>..</span><span>10</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>A</span><span>[</span><span>0</span><span>]</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span>   </span><span>// Cheap to access &#39;A&#39;:
</span></span></span><span><span><span></span><span>              </span><span>// &#39;A&#39; is accessed from the same locale that it lives on
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>  </span><span>on</span><span> </span><span>anotherLocale</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>A</span><span>[</span><span>0</span><span>]</span><span> </span><span>=</span><span> </span><span>2</span><span>;</span><span>   </span><span>// More expensive to access &#39;A&#39;:
</span></span></span><span><span><span></span><span>                </span><span>// &#39;A&#39; is accessed from another locale
</span></span></span><span><span><span></span><span>  </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></div><p><span>
<label for="gpu-comm-note">Currently,</label>

<span>
<span>[note:</span>

In the future, Chapel aims to make it possible for GPU code to access arrays
declared outside of the GPU locale. However, this would require some
communication between the GPU and CPU, initiated by the GPU. <em>GPU-driven
communication</em> like that is on our roadmap, but not supported at the
time of writing.

<span>]</span>
</span>
</span>

to make an array accessible from code that runs on the GPU (such as
our <code>foreach</code> loop), the array must live on the GPU locale. This is why
we declare <code>A</code> inside the <code>on</code> block, rather than outside of it. Had <code>A</code>
been declared outside the <code>on</code> block, it would’ve been on the CPU locale,
allocated in CPU memory.</p>
<p>Now we’ve gone over the whole introductory example, demonstrating how GPUs
can be targeted in Chapel using <code>on</code> blocks and <code>foreach</code> loops. However, this example
is a very simple program, meant to introduce key concepts in Chapel’s
GPU support. What we’ve seen so far is only the beginning. In the next
section, we’ll take a look at other features of Chapel that mesh seamlessly
with GPU programming.</p>
<h3 id="what-else-can-you-do-on-a-gpu">What Else Can You Do on a GPU?</h3>
<p>A large portion of the Chapel language can run on the GPU. In this section,
I’ll give a bit of a whirlwind tour of what can be done. To learn more about
the individual language features I’ll be showing off, please take a look
at the resources mentioned at the beginning of this article.</p>
<p>As we go through examples, the reader might wonder: how can we be sure that
the code ran on the GPU? Tracking which code runs where is a slightly more
advanced topic. For the sake of simplicity, I will define a custom function,
<code>numKernelLaunches</code>. The word <em>kernel</em> typically refers to bits of
code that run on a GPU. A <em>kernel launch</em> is the process of a CPU starting
the execution of a kernel on the GPU. You don’t have to understand how my helper
function works; it’s sufficient to think of <code>numKernelLaunches</code> as
returning the total number of kernel launches since the last time we checked.
I’ll be using this function to ensure that code ran on the GPU when we expected
it to.</p>
<details>
    <summary><strong>(I do want to see how the function is defined!)</strong></summary>

    <div>
        <div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>use</span><span> </span><span>GpuDiagnostics</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>proc</span><span> </span><span>startCountingKernelLaunches</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>resetGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>startGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>proc</span><span> </span><span>numKernelLaunches</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>stopGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>result</span><span> </span><span>=</span><span> </span><span>+</span><span> </span><span>reduce</span><span> </span><span>getGpuDiagnostics</span><span>().</span><span>kernel_launch</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>startCountingKernelLaunches</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>return</span><span> </span><span>result</span><span>;</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>startCountingKernelLaunches</span><span>();</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>
    </div>
</details>

<p>All examples in this section will be performed on the GPU locale.</p>
<p>Let’s start with something fun. At the beginning of this article, we used
the GPU to multiply some numbers by two using a <code>foreach</code>. We can do
this even more succinctly using a Chapel feature called
<a href="https://chapel-lang.org/docs/users-guide/datapar/promotion.html"><em>promotion</em></a>.
Promotion allows us to pass an array to an operation that requires a scalar
value. When we do so, the operation is automatically performed on each element
of the array. This is automatically done in parallel —
potentially on the GPU.</p>
<p>If our operation is “multiply by two”, we can write code as follows:</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>var</span><span> </span><span>Evens</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>];</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Evens</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the promoted initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>In just one simple line, we were able to write code that runs on the GPU.</p>
<p>Let’s move on to our next example. We can call most Chapel functions from
code running on the GPU.
The following example samples the built-in sine function <code>sin</code> on ten
increments within the interval \([0, 2\pi)\):</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>use</span><span> </span><span>Math</span><span>;</span><span> </span><span>// Include the &#39;Math&#39; module for access to &#39;sin&#39; and &#39;pi&#39;
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>  </span><span>const</span><span> </span><span>numSamples</span><span> </span><span>=</span><span> </span><span>10</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span> </span><span>=</span><span> </span><span>[</span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..#</span><span>numSamples</span><span>]</span><span> </span><span>sin</span><span>(</span><span>2</span><span> </span><span>*</span><span> </span><span>pi</span><span> </span><span>*</span><span> </span><span>i</span><span> </span><span>/</span><span> </span><span>numSamples</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>A</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the loop expression initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>Functions called from the GPU can be user-defined and arbitrarily complex.
In the following example, we use promotion again to compute the first 20 Fibonacci numbers.
This example
<span>
<label for="fib-note">is not at all optimized,</label>

<span>
<span>[note:</span>

The example is unoptimized in both an algorithmic and GPU-specific sense.</span>
</span>

but serves as a good demonstration of using arbitrary functions in a
kernel, including recursive ones.</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>proc</span><span> </span><span>fib</span><span>(</span><span>x</span><span>:</span><span> </span><span>int</span><span>):</span><span> </span><span>int</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>if</span><span> </span><span>x</span><span> </span><span>&lt;=</span><span> </span><span>1</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>return</span><span> </span><span>fib</span><span>(</span><span>x</span><span>-</span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>fib</span><span>(</span><span>x</span><span>-</span><span>2</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>Fibs</span><span> </span><span>=</span><span> </span><span>fib</span><span>(</span><span>0</span><span>..#</span><span>20</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Fibs</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the promoted expression in the initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>Here, <code>fib</code> is a normal Chapel function that is being promoted by calling
it with the integer range <code>0..#20</code>.
Note that we are able to use it in a GPU kernel without needing to do anything special.
This is true in general: in Chapel, once a function has been defined, it can be
called from both the GPU and the CPU.</p>
<p>Loops can also be executed as part of a kernel. For our last example, we’ll
define a two-dimensional array <code>Square</code>, then sum each of its columns on
the GPU. The following code will initialize, populate, and print this new
square array:</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span><span>66
</span><span>67
</span><span>68
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>var</span><span> </span><span>rows</span><span>,</span><span> </span><span>cols</span><span> </span><span>=</span><span> </span><span>1</span><span>..</span><span>5</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>Square</span><span>:</span><span> </span><span>[</span><span>rows</span><span>,</span><span> </span><span>cols</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>foreach</span><span> </span><span>(</span><span>r</span><span>,</span><span> </span><span>c</span><span>)</span><span> </span><span>in</span><span> </span><span>Square</span><span>.</span><span>indices</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>      </span><span>Square</span><span>[</span><span>r</span><span>,</span><span> </span><span>c</span><span>]</span><span> </span><span>=</span><span> </span><span>r</span><span> </span><span>*</span><span> </span><span>10</span><span> </span><span>+</span><span> </span><span>c</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;Original array:&#34;</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Square</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// Two kernel launches: one from initializing Square, one from the loop
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>2</span><span>);</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>With the new <code>Square</code> array in hand, we move on to summing its columns. Inside
the <code>foreach</code> loop, we use another <code>for</code> loop as we normally would.</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>70
</span><span>71
</span><span>72
</span><span>73
</span><span>74
</span><span>75
</span><span>76
</span><span>77
</span><span>78
</span><span>79
</span><span>80
</span><span>81
</span><span>82
</span><span>83
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>  </span><span>var</span><span> </span><span>ColSums</span><span>:</span><span> </span><span>[</span><span>cols</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>foreach</span><span> </span><span>c</span><span> </span><span>in</span><span> </span><span>cols</span><span> </span><span>do</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>sum</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>r</span><span> </span><span>in</span><span> </span><span>rows</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>      </span><span>sum</span><span> </span><span>+=</span><span> </span><span>Square</span><span>[</span><span>r</span><span>,</span><span> </span><span>c</span><span>];</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>ColSums</span><span>[</span><span>c</span><span>]</span><span> </span><span>=</span><span> </span><span>sum</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;Column sums:&#34;</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>ColSums</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// Two kernel launches: one from initializing ColSums, one from the loop
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>2</span><span>);</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>Having gone through a few examples, we can conclude our <code>on</code> block
and return to computing on the CPU.</p>
<div data-code-type="main" data-code-section="middle"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>85
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>}</span><span> </span><span>// end of `on here.gpus[0]`
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div><p>Now we have seen several example computations using Chapel’s GPU support.
Where can we run all of this code? Let’s talk about that next.</p>
<h3 id="where-can-i-run-chapel-code-for-gpus">Where Can I Run Chapel Code for GPUs?</h3>
<p>You might recall from the introduction that Chapel’s GPU support is
vendor-neutral. This means that GPU-enabled Chapel code can be executed
on both NVIDIA and AMD GPUs, without any modifications! This includes all
of the code presented in this article.</p>
<p>In fact, you don’t even need a GPU. Chapel supports a mode called ‘CPU-as-device’,
which allows code targeting GPUs to transparently execute on the CPU. You
can prototype GPU-enabled code on a laptop without dedicated graphics, then
switch to a GPU-enabled machine whenever you like. In fact, this is how this
article was written.</p>
<p>Finally, Chapel’s GPU support, like the rest of the language, is scalable.
Programs prototyped on a laptop can easily be executed on a supercomputer,
with good performance. Although writing code in a scalable way requires
<span>
<label for="additional-steps-note">some additional steps,</label>

<span>
<span>[note:</span>

Not using only the first GPU via <code>here.gpus[0]</code> is one such step.

<span>]</span>
</span>
</span>

Chapel makes it easy to create parallel code that runs everywhere.</p>
<h3 id="summary">Summary</h3>
<p>Chapel’s GPU support goes much deeper, but this might be a good stopping point
for an introductory article — we’ve already covered a lot of ground! Let’s
look back at a few of the things we’ve covered:</p>
<ul>
<li>Chapel’s <em>locales</em> represent parts of the machine that can run code and store variables.</li>
<li>The <code>on</code> statement specifies where code should be executed, including on the GPU.</li>
<li><em>Order-independent</em> loops, written using <code>foreach</code>, are automatically executed on the GPU.</li>
<li>Much of the Chapel language can be used in GPU code.
<ul>
<li>This includes <em>promotion</em>, functions plain and recursive, and loops.</li>
</ul>
</li>
<li>All of this is vendor-neutral; Chapel works with both NVIDIA and AMD GPUs.</li>
</ul>
<p>If you’d like to see more information on Chapel’s GPU support in particular,
the <a href="https://chapel-lang.org/docs/technotes/gpu.html">tech note</a> contains many
details and examples of GPU code, and the
<a href="https://chapel-lang.org/releaseNotes/1.31-1.32/05-gpus.pdf"><strong>GPU Programming</strong> section of the release notes from Chapel 1.32</a>
contains a “crash course on GPU programming”.</p>
<p>Of course, we have not yet seen a practical example of solving a problem on
the GPU. We also have not yet seen how to analyze the performance of GPU-enabled
programs in Chapel, or how to improve said performance. Finally, we didn’t see
how Chapel’s GPU support frictionlessly integrates with the rest of the language
to allow writing code that runs across all GPUs <strong>and</strong> compute nodes. For
a little sneak peek of that last point, take a look at this version of
the “even numbers” example that runs on all GPUs in the system:</p>
<div data-code-type="main" data-code-section="last"><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>87
</span><span>88
</span><span>89
</span><span>90
</span><span>91
</span><span>92
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="Chapel"><span><span><span>coforall</span><span> </span><span>loc</span><span> </span><span>in</span><span> </span><span>Locales</span><span> </span><span>do</span><span> </span><span>on</span><span> </span><span>loc</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>coforall</span><span> </span><span>gpu</span><span> </span><span>in</span><span> </span><span>here</span><span>.</span><span>gpus</span><span> </span><span>do</span><span> </span><span>on</span><span> </span><span>gpu</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>Evens</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>writeln</span><span>(</span><span>&#34;Even numbers computed on &#34;</span><span>,</span><span> </span><span>gpu</span><span>,</span><span> </span><span>&#34;: &#34;</span><span>,</span><span> </span><span>Evens</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>

<p>In just six lines of code, we wrote a program that can make use of the
computational resources of an entire supercomputer.</p>
<p>We will be coming back to all of the above topics in subsequent posts,
starting with writing multi-node, multi-GPU programs. Stay tuned!</p>
<p><strong>The entire Chapel program presented in this post can be viewed here:</strong>




</p><div data-code-type="main">
    <details>
        
        
        <div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span><span>66
</span><span>67
</span><span>68
</span><span>69
</span><span>70
</span><span>71
</span><span>72
</span><span>73
</span><span>74
</span><span>75
</span><span>76
</span><span>77
</span><span>78
</span><span>79
</span><span>80
</span><span>81
</span><span>82
</span><span>83
</span><span>84
</span><span>85
</span><span>86
</span><span>87
</span><span>88
</span><span>89
</span><span>90
</span><span>91
</span><span>92
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="chpl"><span><span><span>config</span><span> </span><span>const</span><span> </span><span>n</span><span> </span><span>=</span><span> </span><span>5</span><span>;</span><span> </span><span>// use 5-element arrays in examples by default, for brevity
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>on</span><span> </span><span>here</span><span>.</span><span>gpus</span><span>[</span><span>0</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span>:</span><span> </span><span>[</span><span>1</span><span>..</span><span>n</span><span>]</span><span> </span><span>int</span><span>;</span><span> </span><span>// declare an array with n elements
</span></span></span><span><span><span></span><span>                     </span><span>// (to benefit from GPUs, you&#39;d probably want n &gt;&gt; 5)
</span></span></span><span><span><span></span><span>  </span><span>foreach</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>A</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>i</span><span> </span><span>*</span><span> </span><span>2</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;The whole array A is: &#34;</span><span>,</span><span> </span><span>A</span><span>);</span><span> 
</span></span></span><span><span><span>  </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>1</span><span>..</span><span>n</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>    </span><span>writeln</span><span>(</span><span>&#34;A[&#34;</span><span>,</span><span> </span><span>i</span><span>,</span><span> </span><span>&#34;] = &#34;</span><span>,</span><span> </span><span>A</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>use</span><span> </span><span>GpuDiagnostics</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>proc</span><span> </span><span>startCountingKernelLaunches</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>resetGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>startGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>proc</span><span> </span><span>numKernelLaunches</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>stopGpuDiagnostics</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>result</span><span> </span><span>=</span><span> </span><span>+</span><span> </span><span>reduce</span><span> </span><span>getGpuDiagnostics</span><span>().</span><span>kernel_launch</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>startCountingKernelLaunches</span><span>();</span><span>
</span></span></span><span><span><span>  </span><span>return</span><span> </span><span>result</span><span>;</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>startCountingKernelLaunches</span><span>();</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>on</span><span> </span><span>here</span><span>.</span><span>gpus</span><span>[</span><span>0</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>Evens</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>];</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Evens</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the promoted initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>use</span><span> </span><span>Math</span><span>;</span><span> </span><span>// Include the &#39;Math&#39; module for access to &#39;sin&#39; and &#39;pi&#39;
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>  </span><span>const</span><span> </span><span>numSamples</span><span> </span><span>=</span><span> </span><span>10</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>A</span><span> </span><span>=</span><span> </span><span>[</span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..#</span><span>numSamples</span><span>]</span><span> </span><span>sin</span><span>(</span><span>2</span><span> </span><span>*</span><span> </span><span>pi</span><span> </span><span>*</span><span> </span><span>i</span><span> </span><span>/</span><span> </span><span>numSamples</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>A</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the loop expression initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>proc</span><span> </span><span>fib</span><span>(</span><span>x</span><span>:</span><span> </span><span>int</span><span>):</span><span> </span><span>int</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>if</span><span> </span><span>x</span><span> </span><span>&lt;=</span><span> </span><span>1</span><span> </span><span>then</span><span> </span><span>return</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>return</span><span> </span><span>fib</span><span>(</span><span>x</span><span>-</span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>fib</span><span>(</span><span>x</span><span>-</span><span>2</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>Fibs</span><span> </span><span>=</span><span> </span><span>fib</span><span>(</span><span>0</span><span>..#</span><span>20</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Fibs</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// One kernel launch from the promoted expression in the initializer
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>1</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>rows</span><span>,</span><span> </span><span>cols</span><span> </span><span>=</span><span> </span><span>1</span><span>..</span><span>5</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>Square</span><span>:</span><span> </span><span>[</span><span>rows</span><span>,</span><span> </span><span>cols</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>foreach</span><span> </span><span>(</span><span>r</span><span>,</span><span> </span><span>c</span><span>)</span><span> </span><span>in</span><span> </span><span>Square</span><span>.</span><span>indices</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>      </span><span>Square</span><span>[</span><span>r</span><span>,</span><span> </span><span>c</span><span>]</span><span> </span><span>=</span><span> </span><span>r</span><span> </span><span>*</span><span> </span><span>10</span><span> </span><span>+</span><span> </span><span>c</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;Original array:&#34;</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>Square</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// Two kernel launches: one from initializing Square, one from the loop
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>2</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>var</span><span> </span><span>ColSums</span><span>:</span><span> </span><span>[</span><span>cols</span><span>]</span><span> </span><span>int</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>foreach</span><span> </span><span>c</span><span> </span><span>in</span><span> </span><span>cols</span><span> </span><span>do</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>sum</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>r</span><span> </span><span>in</span><span> </span><span>rows</span><span> </span><span>do</span><span>
</span></span></span><span><span><span>      </span><span>sum</span><span> </span><span>+=</span><span> </span><span>Square</span><span>[</span><span>r</span><span>,</span><span> </span><span>c</span><span>];</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>ColSums</span><span>[</span><span>c</span><span>]</span><span> </span><span>=</span><span> </span><span>sum</span><span>;</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>&#34;Column sums:&#34;</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>writeln</span><span>(</span><span>ColSums</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>  </span><span>// Two kernel launches: one from initializing ColSums, one from the loop
</span></span></span><span><span><span></span><span>  </span><span>assert</span><span>(</span><span>numKernelLaunches</span><span>()</span><span> </span><span>==</span><span> </span><span>2</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>}</span><span> </span><span>// end of `on here.gpus[0]`
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span></span><span>coforall</span><span> </span><span>loc</span><span> </span><span>in</span><span> </span><span>Locales</span><span> </span><span>do</span><span> </span><span>on</span><span> </span><span>loc</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>  </span><span>coforall</span><span> </span><span>gpu</span><span> </span><span>in</span><span> </span><span>here</span><span>.</span><span>gpus</span><span> </span><span>do</span><span> </span><span>on</span><span> </span><span>gpu</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>var</span><span> </span><span>Evens</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>[</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>,</span><span>5</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>writeln</span><span>(</span><span>&#34;Even numbers computed on &#34;</span><span>,</span><span> </span><span>gpu</span><span>,</span><span> </span><span>&#34;: &#34;</span><span>,</span><span> </span><span>Evens</span><span>);</span><span>
</span></span></span><span><span><span>  </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table>
</div>
</div>
    </details>
</div>

<h3 id="appendix-what-sorts-of-problems-benefit-from-gpus">Appendix: What Sorts of Problems Benefit from GPUs?</h3>
<p>A huge difference between a GPU and a CPU is the number of <em>cores</em>.
A core is the part of a processor that’s responsible for executing instructions
/ machine code. Whereas I am writing this from a computer with around 10
CPU cores, a cursory Google search indicates that the NVIDIA RTX 4070 GPU
(picked arbitrarily by searching “consumer NVIDIA GPU”)
has 5888 cores —
<span>
<label for="massively-parallel-note">over 500 times the number of cores in my CPU!</label>

<span>
<span>[note:</span>

In fact, because of the huge number of cores that concurrently execute code,
GPUs are an example of a <a href="https://en.wikipedia.org/wiki/Massively_parallel">
massively parallel</a> architecture.

<span>]</span>
</span>
</span>

Of course, a CPU core is not the same as a GPU core: GPU cores tend to be
much weaker individually.</p>
<p>For problems that can be decomposed into a large number of
<span>
<label for="similar-note">similar</label>

<span>
<span>[note:</span>

For GPUs, the sub-problems being similar is actually very significant.
A single GPU core is actually solving multiple instances of
a sub-problem <em>at the same time</em>, executing their code in lock-step.
As soon as lock-step execution is no longer possible, and two sub-problems
need different approaches, the GPU has to struggle a lot more. This problem
is called <em>thread divergence</em>.

<span>]</span>
</span>
</span>

pieces,
having a large number of weak cores is ideal. Each
core can receive its own share of the problem, and work on it
<span>
<label for="sync-note">independently;</label>

<span>
<span>[note:</span>

It&#39;s possible, and not too uncommon, to have a small degree of coordination
between GPU cores when solving problems. GPUs are able allocate memory that&#39;s
shared between cores, and to synchronize the cores&#39; execution. However,
this is slightly more advanced than this introductory article.

<span>]</span>
</span>
</span>

because each sub-problem is independent, all of the cores can be working on their
share at exactly the same time. This can result in a very significant speedup
over a single, powerful core: such a core would need to go through each
of the pieces one after another.</p>
<p>One example of a problem amenable to GPU execution is rendering an image to
a screen. In fact, as the name <em>Graphics Processing Unit</em> suggests, this
was the very problem GPUs were developed to solve. When rendering, the color
of each pixel can be computed relatively independently, based on depth and
texture information; each core of the GPU can thus be working on a handful of
pixels at a time, all in parallel, significantly speeding up the process.</p>
<p>An entire problem domain that is well-suited for GPU execution is linear algebra.
In matrix multiplication, for example, each cell in the output matrix can be
computed independently from all the other cells; once again, this means that
each GPU core can be working on a handful of cells in parallel. Many
things, including neural networks, can be formulated in terms of matrix operations;
fast linear algebra leads to faster machine learning models.</p>
<p>The descriptions above should give you a taste of <em>what</em> can be done with GPUs.
The next obvious question is <em>how</em> we can do all this with Chapel. This
brings us to the topic of this blog: getting Chapel to run code on the GPU.</p>

</div></div>
  </body>
</html>
