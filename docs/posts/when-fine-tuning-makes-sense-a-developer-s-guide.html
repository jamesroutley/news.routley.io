<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://getkiln.ai/blog/why_fine_tune_LLM_models_and_how_to_get_started">Original</a>
    <h1>When Fine-Tuning Makes Sense: A Developer&#39;s Guide</h1>
    
    <div id="readability-page-1" class="page"><article><!----><!----><img src="https://getkiln.ai/images/fine_tuning.png" alt="Fine Tuning Header"/> <p>Fine-tuning solves specific, measurable problems: models that produce
  inconsistent JSON schemas, inference costs that scale beyond your budget,
  prompts so complex they hurt performance, and specialized behavior that&#39;s
  impossible to achieve through prompting alone.</p> <p>This guide walks through the concrete benefits of fine-tuning, helps you
  identify which goals matter for your use case, and shows you how to get
  started with a clear path to measurable results.</p> <p>We&#39;ll cover the real use cases where fine-tuning makes sense—and when it
  doesn&#39;t. Let&#39;s start with the problems it actually solves:</p>  <h3 id="improve-quality">Improve Quality</h3> <p>&#34;Quality&#34; means different things for different tasks. You should already have
  evals set up for the quality metrics you care about (if not, check out our <a href="https://docs.getkiln.ai/docs/evaluations" target="_blank">evals guide</a>).</p> <p>Fine-tuning excels in specific quality areas:</p> <h4 id="task-specific-quality-score">Task-Specific Quality Score</h4> <p>Most products have an overall quality metric—often a 1-5 star rating.
  Fine-tuning can improve this metric by teaching the model how to respond
  through examples.</p> <h4 id="improve-style-conformance">Improve Style Conformance</h4> <p>A customer service chatbot for a bank needs a very different style and tone
  than a fantasy roleplaying agent. Fine-tuning enforces specific styles more
  effectively than style-prompting.</p> <h4 id="better-json-formatting">Better JSON Formatting</h4> <p>We&#39;ve seen JSON formatting accuracy jump from under 5% to over 99% with
  fine-tuning, when compared to the same untuned base-model.</p> <p>When you need a model to produce output in a specific JSON format, smaller
  models often struggle out of the box. Even if your model produces valid JSON,
  it commonly produces the wrong schema (incorrect key names, missing required
  fields, etc). Fine-tuning significantly improves LLMs&#39; ability to produce
  valid JSON in the correct schema.</p> <p>The same applies to other formats like function calls, XML, YAML, and
  markdown.</p> <h3 id="better-cost-and-speed">Lower Cost and Faster Speed</h3> <p>Fine-tuning is a great way to make your AI app faster and cheaper.</p> <h4 id="fine-tuning-for-shorter-prompts">Fine-tuning for Shorter Prompts</h4> <p>Prompts grow quickly as you add details needed to perform a task (task
  description, style guides, rules, formatting, chain-of-thought instructions,
  etc). Long prompts create several problems:</p> <ul><li>Longer processing time, impacting speed</li> <li>More tokens/GPU usage, impacting cost</li> <li>Reduced prompt conformance as length increases, impacting quality</li></ul> <p>Fine-tuning addresses this by moving most of these requirements from prompts
  to the model itself. I typically recommend:</p> <ul><li>Formatting instructions: use fine-tuning</li> <li>Tone/Style: use fine-tuning</li> <li>Rules/logic (if A reply X, if A+B reply Y): use fine-tuning</li> <li>Chain of thought guidance: use fine-tuning (with chain of thought examples)</li> <li>Core task prompt: keep this, but make it much shorter after the above
    changes</li></ul> <p>You can also explore prompt caching as another method for improving the speed
  and cost of long prompts. However, unlike fine-tuning, this does not help with
  reduced prompt conformance.</p> <h4 id="smaller-models">Smaller Models</h4> <p>Fine-tuning often achieves similar quality on much smaller models. Smaller
  models are faster and cheaper to run.</p> <p>Example: Qwen 14B is a good fine-tuning candidate that runs 6x faster and
  costs ~3% of GPT 4.1.</p> <h4 id="local-models">Local Models</h4> <p>For products you distribute to users, fine-tuning can create small models that
  run locally on their devices, reducing your inference cost to zero. However,
  expect a speed tradeoff—local models won&#39;t be particularly fast.</p> <h3 id="privacy">Privacy</h3> <p>Many users don&#39;t want to send private data to providers like OpenAI and
  Anthropic. Fine-tuning can create smaller models that run locally while
  maintaining the same quality metrics for a given task. This benefits everyone
  from hobbyists to companies handling sensitive data.</p> <h3 id="tool-calling">Tool Calling</h3> <p>Fine-tuning effectively teaches LLMs how and when to use specific tools. A
  training set showing the right tools at the right time is easier to manage
  than defining the behavior in prompts. Training samples also help models learn
  the tool calling format (parameters, order), reducing errors.</p> <h3 id="better-logic-rule-following">Better Logic / Rule Following</h3> <p>AI agents need to handle a wide range of inputs. While you can put all needed
  instructions into a prompt, this becomes unwieldy quickly, and instruction
  following can decrease as prompts grow. Providing examples of your logic/rules
  through fine-tuning helps models learn rules more effectively than
  prompt-based logic.</p> <h4 id="product-logic">Product Logic</h4> <p>Fine-tuning helps models learn to respond based on content and context:</p> <ul><li>Conditional logic: If A: respond with X. If B: respond with Y. If A+B:
    respond with Z.</li> <li>Long-tail situations: off-topic requests, questions in other languages,
    profanity/aggression, etc.</li> <li>Uncertain responses: Models usually aren&#39;t trained to respond &#34;I don&#39;t
    know,&#34; but it&#39;s often good practice in real products.</li></ul> <h4 id="fixing-bugs">Fixing Bugs</h4> <p>LLMs can have unexpected or undesired behavior — we&#39;ll call these &#34;bugs.&#34;
  Fine-tuning with examples of bugs effectively eliminates them. Add examples of
  common failure modes to your training set (with the correct expected output),
  and the model will learn to avoid these pitfalls. Simple supervised
  fine-tuning is usually sufficient, though more advanced techniques like
  reinforcement learning can also help.</p> <h3 id="alignment-and-safety">Alignment and Safety</h3> <p>Fine-tuning effectively aligns models with human values and safety
  requirements, including teaching models to refuse harmful requests, follow
  content policies, and behave according to specific ethical guidelines.</p> <h3 id="distillation-from-larger-models">Distillation From Larger Models</h3> <p>&#34;Distillation&#34; is the process of getting a larger model to teach a smaller
  model how to perform a task. The process involves:</p> <ul><li>Using a large model to produce hundreds or thousands of task samples across
    a range of inputs/conditions</li> <li>Fine-tuning a smaller model using the samples from the larger model</li> <li>Evaluating the resulting fine-tune and optionally iterating with more
    samples if quality issues remain</li></ul> <p>See our guide on <a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model" target="_blank">distilling models</a> — it&#39;s easier than you think and only takes about 20 minutes.</p> <h3 id="better-thinking-reasoning-chain-of-thought">Better Thinking / Reasoning / Chain of Thought</h3> <p>Reasoning models and chain-of-thought prompts improve response quality, but
  unoptimized models often waste their &#34;thinking tokens&#34; without improving
  response quality.</p> <p>Specialized reasoning models like R1 and o3 excel at this but tend to be large
  and expensive. They&#39;ve learned to reason about numerous tasks from PhD-level
  science problems to creative writing to financial analysis.</p> <p>If your model targets a specific task, you can easily teach it the necessary
  &#34;thinking patterns&#34; through fine-tuning examples:</p> <ul><li>Generate hundreds or thousands of samples that include &#34;thinking&#34; tokens.
    This can come from distilling a large high-quality thinking model like R1
    (see above), or by creating a custom <a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model#choosing-between-reasoning-and-chain-of-thought" target="_blank">chain-of-thought prompt</a> with detailed reasoning instructions.</li> <li>Fine-tune a model from these samples, including thinking tokens</li> <li>Run the model and verify the thinking tokens mirror the thinking patterns
    from samples.</li></ul> <h3 id="a-note-on-knowledge">Knowledge: Not an Ideal Use Case for Fine-Tuning</h3> <p>Fine-tuning helps with many things, but we advise against using it to add
  knowledge to a model. If adding knowledge is your goal, consider other
  techniques:</p> <ul><li>RAG: let the model search for relevant information</li> <li>Context loading: provide the model with a system prompt containing needed
    knowledge</li> <li>Tool calls: allow the model to call tools to fetch knowledge</li></ul> <p>You can use all of these methods in conjunction with a fine-tuned model.</p> <h2 id="choosing-models">Choosing Models to Fine-Tune</h2> <p>Select your goals from the list above, as these will guide which models you
  try fine-tuning. There&#39;s no point training a 32B parameter model if you want
  it to run on a phone, or a 1B parameter model if you&#39;re maximizing quality.</p> <p>Here&#39;s high-level guidance for selecting models based on your goal:</p> <ul><li><strong>Run locally on mobile:</strong> select tiny models like Gemma 3n/1B or
    Qwen 3 1.7B</li> <li><strong>Run locally on desktops:</strong> select small models like Qwen 3 4B/8B
    or Gemma 3 2B/4B</li> <li><strong>Cost reduction or speed:</strong> choose a range of model sizes from
    1B-32B to compare quality/speed/cost tradeoffs</li> <li><strong>Maximal quality:</strong> larger models like Llama 70b, Gemma 3 27b,
    Qwen3, GPT 4.1, Gemini Flash/Pro (yes, you can fine-tune Gemini and OpenAI models
    using APIs from Google/OpenAI)</li></ul> <h3 id="iterate-and-experiment">Iterate and Experiment When Fine-Tuning</h3> <p>Data science is a &#34;science&#34; — you need to hypothesize, test, and measure to
  get results. Try a few different base models. Try different training data. Try
  training fine-tunes with and without thinking data (reasoning mode). Try
  more/fewer training epochs.</p> <p>If you&#39;ve properly set up evals, it&#39;s easy to compare results and find the
  best model for your task.</p> <h2 id="conclusion">Conclusion and How to Start Fine-tuning</h2> <p>Fine-tuning solves real problems that prompting can&#39;t. If you&#39;re dealing with
  inconsistent outputs, bloated inference costs, or models that won&#39;t follow
  your rules, it&#39;s worth the investment.</p> <p>With the right tooling, the process isn&#39;t complicated: pick a goal, generate
  training data (synthetic works well), train a few candidates, and measure what
  matters. Most teams see meaningful improvements within a few iterations.
  You&#39;ll be set up for future iterations, whether fixing bugs, changing product
  goals, or adopting new state-of-the-art models.</p> <h2 id="get-started">Use Kiln: The Easiest Fine-Tuning Tool</h2> <p>Kiln is a free app which makes it easy to fine-tune models. It will guide you
  through every step of the process including creating training data, fine
  tuning models, and evaluating fine-tunes to find the best one for your
  project. It handles the boring parts so you can focus on what actually moves
  the needle for your project.</p>  <ul><li>Guides and Video Walkthroughs: <ul><li><a href="https://docs.getkiln.ai/docs/fine-tuning-guide" target="_blank">How to Fine Tune a LLM</a></li> <li><a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model" target="_blank">How to distill LLMs</a></li></ul></li> <li><a href="https://github.com/kiln-ai/kiln" target="_blank">Star Kiln on GitHub</a></li> <li><a href="https://getkiln.ai/download" target="_blank">Download Kiln for Free</a></li> <li><a href="https://getkiln.ai/discord" target="_blank">Ask questions on Discord</a></li></ul><!----><!----></article></div>
  </body>
</html>
