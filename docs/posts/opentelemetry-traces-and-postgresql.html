<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.timescale.com/blog/observability-powered-by-sql-understand-your-systems-like-never-before-with-opentelemetry-traces-and-postgresql/">Original</a>
    <h1>OpenTelemetry Traces and PostgreSQL</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p>Troubleshooting problems in distributed systems built on microservices and cloud-native technologies is a challenge, to say the least. With so many different components, all their interactions, and frequent changes introduced with autoscaling and multiple deployments, we can’t possibly predict everything that could go wrong.</p><p>So far, we know that the first step to solving this problem is to start collecting detailed telemetry data from all our cloud-native systems in the form of traces, metrics, and logs. Unlike monoliths, troubleshooting distributed systems is a “trace first, metrics and logs second” process.</p><p>By themselves, metrics and logs don’t provide the level of understanding required to identify anomalous behaviors in those systems. Traces, however, capture a connected view of how the entire system works to process requests, allowing software developers to figure out what is <em>actually</em> happening when a problem occurs. Metrics and logs expand the information that traces provide—especially if directly correlated to traces. <br/></p><p>OpenTelemetry, a Cloud Native Computing Foundation (CNCF) standard for instrumentation, is driving a revolution in this space by unifying the collection of traces, metrics, and logs and making them universally available. Since OpenTelemetry focuses on helping solve the challenges of operating distributed systems, it is no surprise that the observability framework first targeted tracing support, which is currently more mature than the support for the other two signals.</p><p>Given that observability is an analytics problem, it is surprising that the current state of the art in observability tools has turned its back on the most common standard for data analysis broadly used across organizations: SQL.<br/></p><p>Good old SQL could bring some key advantages: it’s surprisingly powerful, with the ability to perform complex data analysis and support joins; it’s widely known, which reduces the barrier to adoption since almost every developer has used relational databases at some point in their career; it is well-structured and can support metrics, traces, logs, and other types of data (like business data) to remove silos and support correlation; and finally, visualization tools widely support it.</p><p>However, implementing full SQL support from scratch on top of an observability store is a major undertaking that would likely take years of development. <br/></p><p>But what if we didn’t have to? What if we could leverage a well-known and reliable open-source SQL database to solve this problem?<br/></p><p>That’s exactly what we’ve done :)</p><p>With PostgreSQL and TimescaleDB, we have a robust and scalable database for time-series data. Now, how do I add my OpenTelemetry data to start using SQL for observability?</p><p>Today, we are announcing the general availability of OpenTelemetry tracing support in <a href="https://www.timescale.com/promscale">Promscale</a>, the observability backend for metrics and traces built on the rock-solid foundations of PostgreSQL and TimescaleDB. We’re now closer to becoming the unified data store for observability powered by SQL, bringing the PostgreSQL and observability worlds together. <br/></p><p>Traces are the first step toward truly understanding cloud-native architectures. They capture a connected view of how requests travel through their applications, helping developers understand (in real time) how the different components in their system behave and interact with one another. </p><h2 id="opentelemetry-tracing-observability-powered-by-sql">OpenTelemetry Tracing: Observability Powered by SQL<br/></h2><p>With tracing support, engineers can interrogate their observability data with arbitrary questions in SQL and get results back in real time to identify production problems faster and reduce mean time to repair (MTTR). <br/></p><p>OpenTelemetry tracing support in Promscale includes all these features:</p><ul><li>A native ingest endpoint for OpenTelemetry traces that understands the OpenTelemetry Protocol (OTLP) to ingest OpenTelemetry data easily</li><li>Trace data compression and configurable retention to manage disk usage</li><li>Full SQL superpowers to interrogate your trace data with arbitrary questions</li><li>A fully customizable, out-of-the-box, and modern application performance management (APM) experience in Grafana using SQL queries on OpenTelemetry traces, to get new insights immediately after deployment</li><li>Out-of-the-box integrations to visualize distributed traces stored in Promscale using Jaeger and Grafana, so you don’t have to learn new tools or change existing workflows</li><li>Support for ingesting traces in Jaeger and Zipkin formats via the OpenTelemetry Collector, so you can also benefit from all these new capabilities even if you’re not yet using OpenTelemetry</li><li>Correlation of Prometheus metrics and OpenTelemetry traces stored in Promscale through <a href="https://github.com/OpenObservability/OpenMetrics/blob/main/specification/OpenMetrics.md#exemplars">exemplars</a> as well as SQL queries</li><li>Integration in <a href="https://github.com/timescale/tobs">tobs</a>, the observability stack for Kubernetes, so you can deploy all those new capabilities with a single command in your Kubernetes cluster</li></ul><p>Keep reading this blog post for more details on these capabilities and an introduction to powerful, pre-built Grafana dashboards that you can integrate into your Grafana instance. These dashboards will give you instant information on your distributed systems’ dependencies, helping you quickly identify (and correct) performance bottlenecks. </p><h2 id="sending-and-storing-trace-data-in-promscale">Sending and Storing Trace Data in Promscale<br/></h2><p>A key advantage of OpenTelemetry is that it is vendor agnostic. This allows us to instrument our code with OpenTelemetry libraries and send the telemetry generated to one or multiple observability backends via exporters. OpenTelemetry also defines a line protocol (OTLP). All the OpenTelemetry language SDKs include an exporter to send telemetry data to OTLP-compliant backends. A number of other exporters have been built by the community and vendors to send the data to non-OTLP-compliant backends.<br/></p><p>Promscale has an endpoint that listens for OTLP over Google Remote Procedure Calls (by default on port 9202). So, it can ingest traces using the standard OpenTelemetry tools without needing a proprietary exporter.<br/></p><p>OpenTelemetry also includes the OpenTelemetry Collector, which is a component that allows it to receive telemetry in multiple formats (not just OTLP), process it, and export it via OTLP with the OpenTelemetry exporter or in other formats through proprietary exporters.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/otel-collector-diagram-timescale.png" alt="An architecture diagram illustrating the core components of the OpenTelemetry collector, the inputs it accepts, and possible outputs." loading="lazy" width="1638" height="878" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/otel-collector-diagram-timescale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/otel-collector-diagram-timescale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/otel-collector-diagram-timescale.png 1600w, https://www.timescale.com/blog/content/images/2022/05/otel-collector-diagram-timescale.png 1638w" sizes="(min-width: 720px) 720px"/><figcaption><em>Architecture diagram illustrating the core components of the OpenTelemetry collector, the inputs it accepts, and possible outputs</em></figcaption></figure><ul><li>You can configure the OTLP exporter of the OpenTelemetry SDK you used to instrument your application to send traces directly to Promscale.</li><li>You can configure the OTLP exporter of the OpenTelemetry SDK to send data to the OpenTelemetry Collector and then from the OpenTelemetry Collector to Promscale.</li></ul><p>We advise you to use the OpenTelemetry Collector for better performance because it can send data to Promscale in larger batches, which speeds up ingestion. Also, if you want to send data to another backend, you can change the configuration in one place. <a href="https://docs.timescale.com/promscale/latest/send-data/opentelemetry/">See our documentation for more details.</a><br/></p><p>You can also easily send <a href="https://docs.timescale.com/promscale/latest/send-data/jaeger/">Jaeger</a> and <a href="https://docs.timescale.com/promscale/latest/send-data/zipkin/">Zipkin</a> traces to Promscale through the OpenTelemetry Collector by configuring it to receive traces in those formats and convert them to OpenTelemetry.<br/></p><p>Promscale supports the full OpenTelemetry trace schema. It stores and queries all the data, including resources, spans, events, and links with their corresponding attributes. Promscale stores OpenTelemetry traces in TimescaleDB using an optimized database schema and leverages TimescaleDB’s compression capabilities to reduce disk space requirements by up to 10-20x. The <a href="https://docs.timescale.com/promscale/latest/manage-data/retention/#configure-data-retention-policies-in-promscale">retention period is configurable</a> and is set to 30 days by default.</p><h2 id="querying-traces-with-sql"></h2><p>To make querying trace data with SQL easier, Promscale exposes database views that make querying spans, events, and links (with all their corresponding attributes) as easy as querying single relational tables.<br/></p><p>For example, the following query returns the average response time (i.e., latency) in milliseconds for requests to each service in the last five minutes:</p><pre><code>SELECT
    service_name AS &#34;service&#34;,
    AVG(duration_ms) as &#34;latency (ms)&#34;
FROM ps_trace.span s
WHERE 
   start_time &gt; NOW() - INTERVAL &#39;5m&#39;
   AND 
   (parent_span_id IS NULL
   OR
   span_kind = &#39;SPAN_KIND_SERVER&#39;
   )
GROUP BY 1
ORDER BY 1 ASC;</code></pre><p><code>ps_trace.span</code> is the span view. The condition <code>parent_span_id IS NULL OR span_kind = &#39;SPAN_KIND_SERVER</code> ensures that only root spans (i.e., those that have no parent span) or spans that represent a service receiving a request from another service (i.e., this is what <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/trace/api.md#spankind">kind server</a> indicates) are selected. Finally, we compute the average of the duration of those spans across the last five minutes for each service.<br/></p><p>This is the result of visualizing the query above in Grafana with a Table panel:</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/query-results-promscale.png" alt="Table showing the latency in milliseconds for requests to each service in the last five minutes." loading="lazy" width="1254" height="462" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/query-results-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/query-results-promscale.png 1000w, https://www.timescale.com/blog/content/images/2022/05/query-results-promscale.png 1254w" sizes="(min-width: 720px) 720px"/><figcaption><em>Latency in milliseconds for requests to each service in the last five minutes</em></figcaption></figure><p>Because traces are represented as a hierarchy of spans, the ability to use SQL joins, aggregates, and recursive queries to perform sophisticated analysis on trace data is extremely valuable. We’ll say it again: observability is essentially an analytics problem, and SQL is the most popular tool for analytics.<br/></p><p>Imagine that we notice our customers are experiencing prolonged response times from a specific API endpoint we offer. When we receive a request to that API endpoint, there are many operations in multiple microservices that are called to fulfill that request. Identifying where the bottleneck is in a distributed environment is not trivial. How could we quickly identify where most of the time is effectively being spent?<br/></p><p>Note that there are a couple of important considerations to take into account.<br/></p><p>First, the microservices and operations involved in requests to that API endpoint could also be involved in many other requests. We only want to measure the performance of those service operations <strong>when they are called as part of a request to that API endpoint</strong> and not when they are called because of requests to other endpoints.<br/></p><p>Secondly, we want to look at the effective execution time, that is, the time when a service operation is actually doing some processing and not just waiting for the response from another service operation (within the same service or another service). The graphic below illustrates the problem.</p><p>In that particular trace, what is the operation that is consuming the most time? Well, if we just look at the duration of each span, the initial span (i.e., the root span) will always take more time since it includes all other spans. But that’s useless because it doesn’t help us identify the bottleneck. To compute the effective execution time of an operation, we need to subtract its duration from the duration of all child spans. In our example, the effective execution time of Operation A is t1 + t2 + t6, which is shorter than t4, which is the effective execution time of Operation C. Therefore, Operation C is the main bottleneck in this request.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/root-span-child-span-promscale.png" alt="Figure representing the relationship between the root span and its child spans, illustrating how to look at the duration of the child spans can give us clues on which operation may be causing a bottleneck. " loading="lazy" width="1796" height="1170" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/root-span-child-span-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/root-span-child-span-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/root-span-child-span-promscale.png 1600w, https://www.timescale.com/blog/content/images/2022/05/root-span-child-span-promscale.png 1796w" sizes="(min-width: 720px) 720px"/><figcaption><em>The root span can be broken down into child spans—the duration of each child span will give us clues on which operation may be causing a bottleneck</em></figcaption></figure><p>Note that if we used parallelization or asynchronous operations, we would need to do this differently, but for the sake of simplicity, we’ll assume operations are sequential.<br/></p><p>To address those two problems, we need to take several steps:</p><ol><li>Compute the duration of each downstream service and operation only when they are being called as part of a request to the slow API endpoint</li><li>Subtract to each of those the duration of child spans</li><li>Aggregate the results by service and operation<br/></li></ol><p>To do this in SQL, we use a recursive query that uses the following syntax:</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/recursive-query-syntax-timescale-1.png" alt="Syntax of a recursive query in SQL." loading="lazy" width="1578" height="870" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/recursive-query-syntax-timescale-1.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/recursive-query-syntax-timescale-1.png 1000w, https://www.timescale.com/blog/content/images/2022/05/recursive-query-syntax-timescale-1.png 1578w" sizes="(min-width: 720px) 720px"/><figcaption><em>Syntax of a recursive query in SQL</em></figcaption></figure><p>This is how it works.<br/></p><p>It first runs the initial query, which returns an initial set of results, and then runs the recursive query against that initial set of results to get another set of results. After that, it reruns the recursive query on that set of results and continues doing so until the recursive query returns no results.<br/></p><p>Then, it takes all the different results from all the individual queries and concatenates them. That’s what the UNION does.</p><pre><code>WITH RECURSIVE x AS
(
    -- initial query
    SELECT
        s.trace_id,
        s.span_id,
        s.parent_span_id,
        s.service_name,
        s.span_name,
        s.duration_ms - coalesce(
        (
            SELECT sum(z.duration_ms)
            FROM ps_trace.span z
            WHERE s.trace_id = z.trace_id
            AND s.span_id = z.parent_span_id
            AND z.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;
        ), 0.0) as effective_duration_ms
    FROM ps_trace.span s 
    WHERE s.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;
    AND s.service_name = &#39;generator&#39;
    AND s.span_name = &#39;generator.generate&#39;

    UNION ALL
    -- recursive query
    SELECT
        s.trace_id,
        s.span_id,
        s.parent_span_id,
        s.service_name,
        s.span_name,
        s.duration_ms - coalesce(
        (
            SELECT sum(z.duration_ms)
            FROM ps_trace.span z
            WHERE s.trace_id = z.trace_id
            AND s.span_id = z.parent_span_id
            AND z.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;
        ), 0.0) as effective_duration_ms
    FROM x
    INNER JOIN ps_trace.span s
    ON (x.trace_id = s.trace_id
    AND x.span_id = s.parent_span_id
    AND s.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;)
    
)
-- final query
SELECT
    service_name,
    span_name,
    sum(effective_duration_ms) as total_exec_time
FROM x
GROUP BY 1, 2
ORDER BY 3 DESC</code></pre><p>These are the key things to note in this query:</p><ul><li>The recursive query syntax</li><li>The initial and the recursive queries compute the effective duration of a span by using a subquery to sum the duration of all child spans. The latter is then subtracted from the duration of the span (coalesce is used to return 0 in case the span has no child spans and the subquery returns NULL)</li></ul><pre><code>s.duration_ms - coalesce(
(
	SELECT sum(z.duration_ms)
	FROM ps_trace.span z
	WHERE s.trace_id = z.trace_id
	AND s.span_id = z.parent_span_id
	AND z.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;
), 0.0) as effective_duration_ms</code></pre><ul><li>The recursive query uses a join to traverse the downstream spans by selecting the child spans. Since the recursive query runs again and again on the results, applying the recursive query to the previous results ends up processing all downstream spans across all traces that originate in our API endpoint</li></ul><pre><code>INNER JOIN ps_trace.span s
ON (x.trace_id = s.trace_id
AND x.span_id = s.parent_span_id
AND s.start_time &gt; NOW() - INTERVAL &#39;30 minutes&#39;)</code></pre><ul><li>The final query aggregates all the individual execution times by service and operation</li></ul><p>This is the result of the query in Grafana using the Pie chart panel. It quickly points out the <code>random_digit</code> operation of the <code>digit service</code> as the main bottleneck.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/total-execution-time-promscale.png" alt="Pie chart in Grafana showing the total execution time of each operation." loading="lazy" width="1340" height="1510" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/total-execution-time-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/total-execution-time-promscale.png 1000w, https://www.timescale.com/blog/content/images/2022/05/total-execution-time-promscale.png 1340w" sizes="(min-width: 720px) 720px"/><figcaption><em>Pie chart in Grafana showing the total execution time of each operation. With such a view, we can clearly identify that the <code>random_digit</code> operation of the <code>digit</code> service is the main bottleneck</em></figcaption></figure><h2 id="a-modern-apm-experience-integrated-into-grafana">A Modern APM Experience Integrated Into Grafana</h2><figure><img src="https://www.timescale.com/blog/content/images/2022/05/Promscale-apm-dashboards-gif.gif" alt="An overview of the Application Performance Monitoring dashboards for Promscale in Grafana." loading="lazy" width="2448" height="1080"/><figcaption><em>An overview of the Application Performance Monitoring dashboards for Promscale in Grafana (right-click on &#34;Open Link in New Tab&#34; for a better view)</em></figcaption></figure><p>Since it is directly integrated into your Grafana instance, you don’t need to set up a new tool or learn a new user interface. Additionally, the Grafana dashboards can be updated, which means you can customize them and extend them to meet your specific needs.<br/></p><p>The new APM experience within Grafana consists of six dashboards linked to each other:</p><p><strong>[1] Overview</strong>: provides a general overview of the performance across all services to surface the most common problems that could require your attention.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/overview-apm-promscale.png" alt="Promscale APM dashboards: Overview (sreenshot)." loading="lazy" width="2000" height="881" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/overview-apm-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/overview-apm-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/overview-apm-promscale.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/05/overview-apm-promscale.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p><strong><strong>[2] Service Details</strong>: </strong>provides a detailed view of the performance of a service to quickly identify specific problems in that service related to throughput, latency, and errors.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/service-details-apm-promscale.png" alt="Promscale APM dashboards: Service Details (screenshot). " loading="lazy" width="2000" height="881" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/service-details-apm-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/service-details-apm-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/service-details-apm-promscale.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/05/service-details-apm-promscale.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p><strong><strong>[3] Service Map</strong>: </strong>a real-time automatic map of how your services communicate to identify dependencies and quickly validate their implementation.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/service-map-apm-promscale.png" alt="Promscale APM dashboards: Service Map (screenshot)." loading="lazy" width="2000" height="881" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/service-map-apm-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/service-map-apm-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/service-map-apm-promscale.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/05/service-map-apm-promscale.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p><strong><strong>[4] Downstream </strong>D<strong>ependencies</strong>: </strong>a real-time detailed node graph with all the downstream services and operations across all selected service and operation traces. This helps you troubleshoot in detail how downstream services and operations impact the performance of an upstream service.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/downstream-dependencies-apm-promscale.png" alt="Promscale APM dashboards: Downstream Dependencies (screenshot)." loading="lazy" width="2000" height="881" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/downstream-dependencies-apm-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/downstream-dependencies-apm-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/downstream-dependencies-apm-promscale.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/05/downstream-dependencies-apm-promscale.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p><strong><strong>[5] Upstream </strong>D<strong>ependencies</strong>: </strong>a real-time detailed node graph with all the upstream services and operations across all selected service and operation traces. This helps you quickly identify the potential blast radius of an issue in a specific service and operation and determine which upstream service and operation are causing problems (like a sudden increase in requests impacting performance) on a downstream service.</p><figure><img src="https://www.timescale.com/blog/content/images/2022/05/upstream-dependencies-apm-experience-promscale.png" alt="Promscale APM dashboards: Upstream Dependencies (screenshot)." loading="lazy" width="2000" height="881" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/upstream-dependencies-apm-experience-promscale.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/upstream-dependencies-apm-experience-promscale.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/upstream-dependencies-apm-experience-promscale.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/05/upstream-dependencies-apm-experience-promscale.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><p>
        <svg width="17" height="16" viewBox="0 0 17 16" fill="none" xmlns="http://www.w3.org/2000/svg">
		<g id="Symbols / Arrow / Arrow right / 24x24">
		<path id="Vector 85 (Stroke)" fill-rule="evenodd" clip-rule="evenodd" d="M8.84937 12.6869C9.04491 12.8821 9.36196 12.8821 9.5575 12.6869L13.7302 8.52022C13.9258 8.32496 13.9258 8.00837 13.7302 7.81311L9.5575 3.64645C9.36196 3.45118 9.04491 3.45118 8.84937 3.64645C8.65382 3.84171 8.65382 4.15829 8.84937 4.35355L12.1673 7.66667H2.52709C2.25055 7.66667 2.02637 7.89052 2.02637 8.16667C2.02637 8.44281 2.25055 8.66667 2.52709 8.66667H12.1673L8.84937 11.9798C8.65382 12.175 8.65382 12.4916 8.84937 12.6869Z" fill="#141E35"></path>
		</g>
	</svg>
        Note: Some of these dashboards use the Node graph panel, which was introduced in recent versions of Grafana. It’s a beta version, so it may break. It worked for us in Grafana 8.5.1.  
    </p>
<!--kg-card-end: html--><p><strong>These dashboards are available on <a href="https://github.com/timescale/promscale/tree/master/docs/mixin/dashboards">GitHub</a> (filename starts with apm-). Check <a href="https://docs.timescale.com/promscale/latest/visualize-data/apm-experience/">our documentation</a> for details on how to import the dashboards into Grafana.</strong></p><h2 id="observability-is-an-analytics-problem">Observability Is An Analytics Problem</h2><p>OpenTelemetry, a CNCF standard for instrumentation, makes telemetry data easier to collect and universally available. Traces, in particular, hold a treasure of valuable information about how distributed systems behave. We need powerful tools to analyze the data to get the most value from tracing. Observability is essentially an analytics problem.<br/></p><p>SQL is the lingua franca of analytics. When applied to trace data, it helps you unlock new insights to troubleshoot production issues faster and proactively improve your applications.</p><p>Promscale is an observability backend built on the rock-solid foundation of PostgreSQL and TimescaleDB. With the new support for OpenTelemetry traces and integrations with the visualization tools you use and love, you have full SQL superpowers to solve even the most complex issues in your distributed systems.</p><!--kg-card-begin: html--><!--kg-card-end: html--><figure><img src="https://www.timescale.com/blog/content/images/2022/05/Twitter-graphic-for-Kubecon-ABL_v1.0--1-.png" alt="" loading="lazy" width="2000" height="1125" srcset="https://www.timescale.com/blog/content/images/size/w600/2022/05/Twitter-graphic-for-Kubecon-ABL_v1.0--1-.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/05/Twitter-graphic-for-Kubecon-ABL_v1.0--1-.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/05/Twitter-graphic-for-Kubecon-ABL_v1.0--1-.png 1600w, https://www.timescale.com/blog/content/images/2022/05/Twitter-graphic-for-Kubecon-ABL_v1.0--1-.png 2400w" sizes="(min-width: 720px) 720px"/></figure>
        <div>
          <p>The open-source relational database for time-series and analytics.</p>
          <p>
            Try Timescale for free
          </p>

        </div>
      </div></div>
  </body>
</html>
