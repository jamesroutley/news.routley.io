<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">Original</a>
    <h1>Show HN: LLM Agent Paper List</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">üî• <strong>Must-read papers for LLM-based agents.</strong></p>
<p dir="auto">üèÉ <strong>Coming soon: Add one-sentence intro to each paper.</strong></p>
<h2 tabindex="-1" id="user-content--news" dir="auto"><a href="#-news">üîî News<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>ü•≥ [2023/09/20] This project has been listed on <a href="https://github.com/trending">GitHub Trendings</a>!  It is a great honor!</li>
<li>üí• [2023/09/15] Our survey is released! See <a href="https://arxiv.org/abs/2309.07864" rel="nofollow">The Rise and Potential of Large Language Model Based Agents: A Survey</a> for the paper!</li>
<li>‚ú® [2023/09/14] We create this repository to maintain a paper list on LLM-based agents. More papers are coming soon!</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure1.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure1.jpg" width="80%"/></a></p>
<h2 tabindex="-1" id="user-content--introduction" dir="auto"><a href="#-introduction">üåü Introduction<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing human level, with AI agents considered as a promising vehicle of this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions.</p>
<p dir="auto">Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and have achieved significant progress.</p>
<p dir="auto">In this repository, we provide a systematic and comprehensive survey on LLM-based agents, and list some must-read papers.</p>
<p dir="auto">Specifically, we start by the general conceptual framework for LLM-based agents: comprising three main components: brain, perception, and action, and the framework can be tailored to suit different applications.
Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation.
Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge when they form societies, and the insights they offer for human society.
Finally, we discuss a range of key topics and open problems within the field.</p>
<p dir="auto"><strong>We greatly appreciate any contributions via PRs, issues, emails, or other methods.</strong></p>
<h2 tabindex="-1" id="user-content-table-of-content-toc" dir="auto"><a href="#table-of-content-toc">Table of Content (ToC)<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li><a href="#the-rise-and-potential-of-large-language-model-based-agents-a-survey">The Rise and Potential of Large Language Model Based Agents: A Survey</a>
<ul dir="auto">
<li><a href="#-news">üîî News</a></li>
<li><a href="#-introduction">üåü Introduction</a></li>
<li><a href="#1-the-birth-of-an-agent-construction-of-llm-based-agents">1. The Birth of An Agent: Construction of LLM-based Agents</a>
<ul dir="auto">
<li><a href="#11-brain-primarily-composed-of-an-llm">1.1 Brain: Primarily Composed of An LLM</a>
<ul dir="auto">
<li><a href="#111-natural-language-interaction">1.1.1 Natural Language Interaction</a></li>
<li><a href="#112-knowledge">1.1.2 Knowledge</a></li>
<li><a href="#113-memory">1.1.3 Memory</a></li>
<li><a href="#114-reasoning--planning">1.1.4 Reasoning &amp; Planning</a></li>
<li><a href="#115-transferability-and-generalization">1.1.5 Transferability and Generalization</a></li>
</ul>
</li>
<li><a href="#12-perception-multimodal-inputs-for-llm-based-agents">1.2 Perception: Multimodal Inputs for LLM-based Agents</a>
<ul dir="auto">
<li><a href="#121-visual">1.2.1 Visual</a></li>
<li><a href="#122-audio">1.2.2 Audio</a></li>
</ul>
</li>
<li><a href="#13-action-expand-action-space-of-llm-based-agents">1.3 Action: Expand Action Space of LLM-based Agents</a>
<ul dir="auto">
<li><a href="#131-tool-using">1.3.1 Tool Using</a></li>
<li><a href="#132-embodied-action">1.3.2 Embodied Action</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-agents-in-practice-applications-of-llm-based-agents">2. Agents in Practice: Applications of LLM-based Agents</a>
<ul dir="auto">
<li><a href="#21-general-ability-of-single-agent">2.1 General Ability of Single Agent</a>
<ul dir="auto">
<li><a href="#211-task-oriented-deployment">2.1.1 Task-oriented Deployment</a></li>
<li><a href="#212-innovation-oriented-deployment">2.1.2 Innovation-oriented Deployment</a></li>
<li><a href="#213-lifecycle-oriented-deployment">2.1.3 Lifecycle-oriented Deployment</a></li>
</ul>
</li>
<li><a href="#22-coordinating-potential-of-multiple-agents">2.2 Coordinating Potential of Multiple Agents</a>
<ul dir="auto">
<li><a href="#221-cooperative-interaction-for-complementarity">2.2.1 Cooperative Interaction for Complementarity</a></li>
<li><a href="#222-adversarial-interaction-for-advancement">2.2.2 Adversarial Interaction for Advancement</a></li>
</ul>
</li>
<li><a href="#23-interactive-engagement-between-human-and-agent">2.3 Interactive Engagement between Human and Agent</a>
<ul dir="auto">
<li><a href="#231-instructor-executor-paradigm">2.3.1 Instructor-Executor Paradigm</a></li>
<li><a href="#232-equal-partnership-paradigm">2.3.2 Equal Partnership Paradigm</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3-agent-society-from-individuality-to-sociality">3. Agent Society: From Individuality to Sociality</a>
<ul dir="auto">
<li><a href="#31-behavior-and-personality-of-llm-based-agents">3.1 Behavior and Personality of LLM-based Agents</a>
<ul dir="auto">
<li><a href="#311-social-behavior">3.1.1 Social Behavior</a></li>
<li><a href="#312-personality">3.1.2 Personality</a></li>
</ul>
</li>
<li><a href="#32-environment-for-agent-society">3.2 Environment for Agent Society</a>
<ul dir="auto">
<li><a href="#321-text-based-environment">3.2.1 Text-based Environment</a></li>
<li><a href="#322-virtual-sandbox-environment">3.2.2 Virtual Sandbox Environment</a></li>
<li><a href="#323-physical-environment">3.2.3 Physical Environment</a></li>
</ul>
</li>
<li><a href="#33-society-simulation-with-llm-based-agents">3.3 Society Simulation with LLM-based Agents</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
<li><a href="#project-maintainers--contributors">Project Maintainers &amp; Contributors</a></li>
<li><a href="#contact">Contact</a></li>
<li><a href="#star-history">Star History</a></li>
</ul>
</li>
</ul>
<h2 tabindex="-1" id="user-content-1-the-birth-of-an-agent-construction-of-llm-based-agents" dir="auto"><a href="#1-the-birth-of-an-agent-construction-of-llm-based-agents">1. The Birth of An Agent: Construction of LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure2.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure2.jpg" width="80%"/></a></p>
<h3 tabindex="-1" id="user-content-11-brain-primarily-composed-of-an-llm" dir="auto"><a href="#11-brain-primarily-composed-of-an-llm">1.1 Brain: Primarily Composed of An LLM<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-111-natural-language-interaction" dir="auto"><a href="#111-natural-language-interaction">1.1.1 Natural Language Interaction<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-high-quality-generation" dir="auto"><a href="#high-quality-generation">High-quality generation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.</strong> <em>Yejin Bang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2302.04023" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models.</strong> <em>Yen-Ting Lin et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.13711" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation.</strong> <em>Tao Fang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2304.01746" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-deep-understanding" dir="auto"><a href="#deep-understanding">Deep understanding<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/06] <strong>Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models.</strong> <em>Natalie Shapira et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.14763" rel="nofollow">paper</a>]</li>
<li>[2022/08] <strong>Inferring Rewards from Language in Context.</strong> <em>Jessy Lin et al. ACL.</em> [<a href="https://doi.org/10.18653/v1/2022.acl-long.585" rel="nofollow">paper</a>]</li>
<li>[2021/10] <strong>Theory of Mind Based Assistive Communication in Complex Human Robot Cooperation.</strong> <em>Moritz C. Buehler et al. arXiv.</em> [<a href="https://arxiv.org/abs/2109.01355" rel="nofollow">paper</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-112-knowledge" dir="auto"><a href="#112-knowledge">1.1.2 Knowledge<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-pretrain-model" dir="auto"><a href="#pretrain-model">Pretrain model<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/04] <strong>Learning Distributed Representations of Sentences from Unlabelled Data.</strong> <em>Felix Hill(University of Cambridge) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1602.03483" rel="nofollow">paper</a>]</li>
<li>[2020/02] <strong>How Much Knowledge Can You Pack Into the Parameters of a Language Model?</strong> <em>Adam Roberts(Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2002.08910" rel="nofollow">paper</a>]</li>
<li>[2020/01] <strong>Scaling Laws for Neural Language Models.</strong> <em>Jared Kaplan(Johns Hopkins University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2001.08361" rel="nofollow">paper</a>]</li>
<li>[2017/12] <strong>Commonsense Knowledge in Machine Intelligence.</strong> <em>Niket Tandon(Allen Institute for Artificial Intelligence) et al. SIGMOD.</em> [<a href="https://sigmodrecord.org/publications/sigmodRecord/1712/pdfs/09_reports_Tandon.pdf" rel="nofollow">paper</a>]</li>
<li>[2011/03] <strong>Natural Language Processing (almost) from Scratch.</strong> <em>Ronan Collobert(Princeton) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1103.0398" rel="nofollow">paper</a>]]</li>
</ul>
<h5 tabindex="-1" id="user-content-linguistic-knowledge" dir="auto"><a href="#linguistic-knowledge">Linguistic knowledge<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/02] <strong>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.</strong> <em>Yejin Bang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.04023" rel="nofollow">paper</a>]</li>
<li>[2021/06] <strong>Probing Pre-trained Language Models for Semantic Attributes and their Values.</strong> <em>Meriem Beloucif et al. EMNLP.</em> [<a href="https://aclanthology.org/2021.findings-emnlp.218/" rel="nofollow">paper</a>]</li>
<li>[2020/10] <strong>Probing Pretrained Language Models for Lexical Semantics.</strong> <em>Ivan Vuliƒá et al. arXiv.</em> [<a href="https://arxiv.org/abs/2010.05731" rel="nofollow">paper</a>]</li>
<li>[2019/04] <strong>A Structural Probe for Finding Syntax in Word Representations.</strong> <em>John Hewitt et al. ACL.</em> [<a href="https://aclanthology.org/N19-1419/" rel="nofollow">paper</a>]</li>
<li>[2016/04] <strong>Improved Automatic Keyword Extraction Given More Semantic Knowledge.</strong> <em>H Leung. Systems for Advanced Applications.</em> [<a href="https://link.springer.com/chapter/10.1007/978-3-319-32055-7_10" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-commonsense-knowledge" dir="auto"><a href="#commonsense-knowledge">Commonsense knowledge<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2022/10] <strong>Language Models of Code are Few-Shot Commonsense Learners.</strong> <em>Aman Madaan et al.arXiv.</em> [<a href="https://arxiv.org/abs/2210.07128" rel="nofollow">paper</a>]</li>
<li>[2021/04] <strong>Relational World Knowledge Representation in Contextual Language Models: A Review.</strong> <em>Tara Safavi et al. arXiv.</em> [<a href="https://arxiv.org/abs/2104.05837" rel="nofollow">paper</a>]</li>
<li>[2019/11] <strong>How Can We Know What Language Models Know?</strong> <em>Zhengbao Jiang et al.arXiv.</em> [<a href="https://arxiv.org/abs/1911.12543" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-actionable-knowledge" dir="auto"><a href="#actionable-knowledge">Actionable knowledge<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/07] <strong>Large language models in medicine.</strong> <em>Arun James Thirunavukarasu et al. nature.</em> [<a href="https://www.nature.com/articles/s41591-023-02448-8" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.</strong> <em>Yuhang Lai et al. ICML.</em> [<a href="https://proceedings.mlr.press/v202/lai23b.html" rel="nofollow">paper</a>]</li>
<li>[2022/10] <strong>Language Models of Code are Few-Shot Commonsense Learners.</strong> <em>Aman Madaan et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.07128" rel="nofollow">paper</a>]</li>
<li>[2022/02] <strong>A Systematic Evaluation of Large Language Models of Code.</strong> <em>Frank F. Xu et al.arXiv.</em> [<a href="https://arxiv.org/abs/2202.13169" rel="nofollow">paper</a>]</li>
<li>[2021/10] <strong>Training Verifiers to Solve Math Word Problems.</strong> <em>Karl Cobbe et al. arXiv.</em> [<a href="https://arxiv.org/abs/2110.14168" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-potential-issues-of-knowledge" dir="auto"><a href="#potential-issues-of-knowledge">Potential issues of knowledge<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/05] <strong>Editing Large Language Models: Problems, Methods, and Opportunities.</strong> <em>Yunzhi Yao et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.13172" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models.</strong> <em>Miaoran Li et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14623" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing.</strong> <em>Zhibin Gou et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11738" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>Tool Learning with Foundation Models.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08354" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.</strong> <em>Potsawee Manakul et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.08896" rel="nofollow">paper</a>]</li>
<li>[2022/06] <strong>Memory-Based Model Editing at Scale.</strong> <em>Eric Mitchell et al. arXiv.</em> [<a href="https://arxiv.org/abs/2206.06520" rel="nofollow">paper</a>]</li>
<li>[2022/04] <strong>A Review on Language Models as Knowledge Bases.</strong> <em>Badr AlKhamissi et al.arXiv.</em> [<a href="https://arxiv.org/abs/2204.06031" rel="nofollow">paper</a>]</li>
<li>[2021/04] <strong>Editing Factual Knowledge in Language Models.</strong> <em>Nicola De Cao et al.arXiv.</em> [<a href="https://arxiv.org/abs/2104.08164" rel="nofollow">paper</a>]</li>
<li>[2017/08] <strong>Measuring Catastrophic Forgetting in Neural Networks.</strong> <em>Ronald Kemker et al.arXiv.</em> [<a href="https://arxiv.org/abs/1708.02072" rel="nofollow">paper</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-113-memory" dir="auto"><a href="#113-memory">1.1.3 Memory<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-memory-capability" dir="auto"><a href="#memory-capability">Memory capability<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<h6 tabindex="-1" id="user-content-raising-the-length-limit-of-transformers" dir="auto"><a href="#raising-the-length-limit-of-transformers">Raising the length limit of Transformers<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h6>
<ul dir="auto">
<li>[2023/05] <strong>Randomized Positional Encodings Boost Length Generalization of Transformers.</strong> <em>Anian Ruoss (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16843" rel="nofollow">paper</a>] [<a href="https://github.com/google-deepmind/randomized_positional_encodings">code</a>]</li>
<li>[2023-03] <strong>CoLT5: Faster Long-Range Transformers with Conditional Computation.</strong> <em>Joshua Ainslie (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.09752" rel="nofollow">paper</a>]</li>
<li>[2022/03] <strong>Efficient Classification of Long Documents Using Transformers.</strong> <em>Hyunji Hayley Park (Illinois University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.11258" rel="nofollow">paper</a>] [<a href="https://github.com/amazon-science/efficient-longdoc-classification">code</a>]</li>
<li>[2021/12] <strong>LongT5: Efficient Text-To-Text Transformer for Long Sequences.</strong> <em>Mandy Guo (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.07916" rel="nofollow">paper</a>] [<a href="https://github.com/google-research/longt5">code</a>]</li>
<li>[2019/10] <strong>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.</strong> <em>Michael Lewis(Facebook AI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1910.13461" rel="nofollow">paper</a>] [<a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart">code</a>]</li>
</ul>
<h6 tabindex="-1" id="user-content-summarizing-memory" dir="auto"><a href="#summarizing-memory">Summarizing memory<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h6>
<ul dir="auto">
<li>[2023/09] <strong>Empowering Private Tutoring by Chaining Large Language Models</strong> <em>Yulin Chen (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2309.08112" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>ExpeL: LLM Agents Are Experiential Learners.</strong> <em>Andrew Zhao (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10144" rel="nofollow">paper</a>] [code]</li>
<li>[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201" rel="nofollow">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</li>
<li>[2023/05] <strong>MemoryBank: Enhancing Large Language Models with Long-Term Memory.</strong> <em>Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10250" rel="nofollow">paper</a>] [<a href="https://github.com/zhongwanjun/memorybank-siliconfriend">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442" rel="nofollow">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/04] <strong>Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System.</strong> <em>Xinnian Liang(Beihang University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.13343" rel="nofollow">paper</a>] [<a href="https://github.com/wbbeyourself/scm4llms">code</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn (Northeastern University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11366" rel="nofollow">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
<li>[2023/05] <strong>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2305.13304.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/RecurrentGPT">code</a>]</li>
</ul>
<h6 tabindex="-1" id="user-content-compressing-memories-with-vectors-or-data-structures" dir="auto"><a href="#compressing-memories-with-vectors-or-data-structures">Compressing memories with vectors or data structures<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h6>
<ul dir="auto">
<li>[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924" rel="nofollow">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</li>
<li>[2023/06] <strong>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.</strong> <em>Chenxu Hu(Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03901" rel="nofollow">paper</a>] [<a href="https://github.com/huchenxucs/ChatDB">code</a>]</li>
<li>[2023/05] <strong>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.</strong> <em>Xizhou Zhu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17144" rel="nofollow">paper</a>] [<a href="https://github.com/OpenGVLab/GITM">code</a>]</li>
<li>[2023/05] <strong>RET-LLM: Towards a General Read-Write Memory for Large Language Models.</strong> <em>Ali Modarressi (LMU Munich) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14322" rel="nofollow">paper</a>] [<a href="https://github.com/tloen/alpaca-lora">code</a>]</li>
<li>[2023/05] <strong>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2305.13304.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/RecurrentGPT">code</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-memory-retrieval" dir="auto"><a href="#memory-retrieval">Memory retrieval<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents.</strong> <em>Ziheng Huang(University of California‚ÄîSan Diego) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01542" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.04026" rel="nofollow">paper</a>] [<a href="https://www.agentsims.com/" rel="nofollow">project page</a>] [<a href="https://github.com/py499372727/AgentSims/">code</a>]</li>
<li>[2023/06] <strong>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.</strong> <em>Chenxu Hu(Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03901" rel="nofollow">paper</a>] [<a href="https://github.com/huchenxucs/ChatDB">code</a>]</li>
<li>[2023/05] <strong>MemoryBank: Enhancing Large Language Models with Long-Term Memory.</strong> <em>Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10250" rel="nofollow">paper</a>] [<a href="https://github.com/zhongwanjun/memorybank-siliconfriend">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442" rel="nofollow">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/05] <strong>RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2305.13304.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/RecurrentGPT">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-114-reasoning--planning" dir="auto"><a href="#114-reasoning--planning">1.1.4 Reasoning &amp; Planning<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-reasoning" dir="auto"><a href="#reasoning">Reasoning<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>
<p dir="auto">[2023/05] <strong>Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement.</strong> <em>Zhiheng Xi (Fudan University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14497" rel="nofollow">paper</a>] [<a href="https://github.com/woooodyy/self-polish">code</a>]</p>
</li>
<li>
<p dir="auto">[2023-03] <strong>Large Language Models are Zero-Shot Reasoners.</strong> <em>Takeshi Kojima (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.11916" rel="nofollow">paper</a>][<a href="https://github.com/kojima-takeshi188/zero_shot_cot">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/03] <strong>Self-Refine: Iterative Refinement with Self-Feedback.</strong> <em>Aman Madaan (Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17651" rel="nofollow">paper</a>] [<a href="https://github.com/madaan/self-refine">code</a>]</p>
</li>
<li>
<p dir="auto">[2022/05] <strong>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning.</strong> <em>Antonia Creswell (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.09712" rel="nofollow">paper</a>]</p>
</li>
<li>
<p dir="auto">[2022/03] <strong>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</strong> <em>Xuezhi Wang(Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.11171" rel="nofollow">paper</a>] [<a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart">code</a>]</p>
</li>
<li>
<p dir="auto">[2022/01] <strong>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</strong> <em>Jason Wei (Google Research,) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2201.11903" rel="nofollow">paper</a>]</p>
</li>
</ul>
<h5 tabindex="-1" id="user-content-planning" dir="auto"><a href="#planning">Planning<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<h6 tabindex="-1" id="user-content-plan-formulation" dir="auto"><a href="#plan-formulation">Plan formulation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h6>
<ul dir="auto">
<li>[2023/05] <strong>Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10601" rel="nofollow">paper</a>] [<a href="https://github.com/princeton-nlp/tree-of-thought-llm">code</a>]</li>
<li>[2023/05] <strong>Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.</strong> <em>Yue Wu(Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.02412" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Reasoning with Language Model is Planning with World Model.</strong> <em>Shibo Hao (UC San Diego) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14992" rel="nofollow">paper</a>] [<a href="https://github.com/Ber666/RAP">code</a>]</li>
<li>[2023/05] <strong>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.</strong> <em>Bill Yuchen Lin (Allen Institute for Artificial Intelligence) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17390" rel="nofollow">paper</a>] [<a href="https://github.com/yuchenlin/swiftsage">code</a>]</li>
<li>[2023/04] <strong>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.</strong> <em>Bo Liu (University of Texas at Austin) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.11477" rel="nofollow">paper</a>] [<a href="https://github.com/Cranial-XIX/llm-pddl">code</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen (Microsoft Research Asia) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580" rel="nofollow">paper</a>] [<a href="https://github.com/microsoft/JARVIS">code</a>]</li>
<li>[2023/02] <strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.</strong> <em>ZiHao Wang (Peking University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.01560" rel="nofollow">paper</a>] [<a href="https://github.com/CraftJarvis/MC-Planner">code</a>]</li>
<li>[2022/05] <strong>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.</strong> <em>Denny Zhou (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.10625" rel="nofollow">paper</a>]</li>
<li>[2022/05] <strong>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.</strong> <em>Ehud Karpas (AI21 Labs) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.00445" rel="nofollow">paper</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn (Robotics at Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.01691" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2309.07870.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]</li>
</ul>
<h6 tabindex="-1" id="user-content-plan-reflection" dir="auto"><a href="#plan-reflection">Plan reflection<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h6>
<ul dir="auto">
<li>[2023/08] <strong>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.</strong> <em>Ning Miao (University of Oxford) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00436" rel="nofollow">paper</a>] [<a href="https://github.com/NingMiao/SelfCheck">code</a>]</li>
<li>[2023/05] <strong>ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models.</strong> <em>Zhipeng Chen (Renmin University of China) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14323" rel="nofollow">paper</a>] [<a href="https://github.com/RUCAIBOX/ChatCoT">code</a>]</li>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291" rel="nofollow">paper</a>] [<a href="https://voyager.minedojo.org/" rel="nofollow">code</a>]</li>
<li>[2023/03] <strong>Chat with the Environment: Interactive Multimodal Perception Using Large Language Models.</strong> <em>Xufeng Zhao (University Hamburg) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.08268" rel="nofollow">paper</a>] [<a href="https://matcha-model.github.io/" rel="nofollow">code</a>]</li>
<li>[2022/12] <strong>LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.</strong> <em>Chan Hee Song (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.04088" rel="nofollow">paper</a>] [<a href="https://dki-lab.github.io/LLM-Planner/" rel="nofollow">code</a>]</li>
<li>[2022/10] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao ( Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.03629" rel="nofollow">paper</a>] [<a href="https://react-lm.github.io/" rel="nofollow">code</a>]</li>
<li>[2022/07] <strong>Inner Monologue: Embodied Reasoning through Planning with Language Models.</strong> <em>Wenlong Huang (Robotics at Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.05608" rel="nofollow">paper</a>] [<a href="https://innermonologue.github.io/" rel="nofollow">code</a>]</li>
<li>[2021/10] <strong>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts.</strong> <em>Tongshuang Wu (University of Washington) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2110.01691" rel="nofollow">paper</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-115-transferability-and-generalization" dir="auto"><a href="#115-transferability-and-generalization">1.1.5 Transferability and Generalization<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-unseen-task-generalization" dir="auto"><a href="#unseen-task-generalization">Unseen task generalization<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/05] <strong>Training language models to follow instructions with human feedback.</strong> <em>Long Ouyang et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" rel="nofollow">paper</a>]</li>
<li>[2023/01] <strong>Multitask Prompted Training Enables Zero-Shot Task Generalization.</strong> <em>Victor Sanh et al. ICLR.</em> [<a href="https://openreview.net/forum?id=9Vrb9D0WI4" rel="nofollow">paper</a>]</li>
<li>[2022/10] <strong>Scaling Instruction-Finetuned Language Models.</strong> <em>Hyung Won Chung et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2210.11416" rel="nofollow">paper</a>]</li>
<li>[2022/08] <strong>Finetuned Language Models are Zero-Shot Learners.</strong> <em>Jason Wei et al. ICLR.</em> [<a href="https://openreview.net/forum?id=gEZrGCozdqR" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-in-context-learning" dir="auto"><a href="#in-context-learning">In-context learning<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>Images Speak in Images: A Generalist Painter for In-Context Visual Learning.</strong> <em>Xinlong Wang et al. IEEE.</em> [<a href="https://doi.org/10.1109/CVPR52729.2023.00660" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.</strong> <em>Chengyi Wang et al. arXiv.</em> [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">paper</a>]</li>
<li>[2023/07] <strong>A Survey for In-context Learning.</strong> <em>Qingxiu Dong et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2301.00234" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Language Models are Few-Shot Learners.</strong> <em>Tom B. Brown (OpenAI) et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-continual-learning" dir="auto"><a href="#continual-learning">Continual learning<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/07] <strong>Progressive Prompts: Continual Learning for Language Models.</strong> <em>Razdaibiedina et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12314" rel="nofollow">paper</a>]</li>
<li>[2023/07] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.16291" rel="nofollow">paper</a>]</li>
<li>[2023/01] <strong>A Comprehensive Survey of Continual Learning: Theory, Method and Application.</strong> <em>Liyuan Wang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.00487" rel="nofollow">paper</a>]</li>
<li>[2022/11] <strong>Continual Learning of Natural Language Processing Tasks: A Survey.</strong> <em>Zixuan Ke et al. arXiv.</em> [<a href="https://arxiv.org/abs/2211.12701" rel="nofollow">paper</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-12-perception-multimodal-inputs-for-llm-based-agents" dir="auto"><a href="#12-perception-multimodal-inputs-for-llm-based-agents">1.2 Perception: Multimodal Inputs for LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-121-visual" dir="auto"><a href="#121-visual">1.2.1 Visual<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/05] <strong>Language Is Not All You Need: Aligning Perception with Language Models.</strong> <em>Shaohan Huang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.14045" rel="nofollow">paper</a>]]</li>
<li>[2023/05] <strong>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.</strong> <em>Wenliang Dai et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.06500" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans.</strong> <em>Tao Gong et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.04790" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>PandaGPT: One Model To Instruction-Follow Them All.</strong> <em>Yixuan Su et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16355" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>Visual Instruction Tuning.</strong> <em>Haotian Liu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08485" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</strong> <em>Deyao Zhu. arXiv.</em> [<a href="https://arxiv.org/abs/2304.10592" rel="nofollow">paper</a>]</li>
<li>[2023/01] <strong>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.</strong> <em>Junnan Li et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12597" rel="nofollow">paper</a>]</li>
<li>[2022/04] <strong>Flamingo: a Visual Language Model for Few-Shot Learning.</strong> <em>Jean-Baptiste Alayrac et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.14198" rel="nofollow">paper</a>]</li>
<li>[2021/10] <strong>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer.</strong> <em>Sachin Mehta et al.arXiv.</em> [<a href="https://arxiv.org/abs/2110.02178" rel="nofollow">paper</a>]</li>
<li>[2021/05] <strong>MLP-Mixer: An all-MLP Architecture for Vision.</strong> <em>Ilya Tolstikhin et al.arXiv.</em> [<a href="https://arxiv.org/abs/2105.01601" rel="nofollow">paper</a>]</li>
<li>[2020/10] <strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</strong> <em>Alexey Dosovitskiy et al. arXiv.</em> [<a href="https://arxiv.org/abs/2010.11929" rel="nofollow">paper</a>]</li>
<li>[2017/11] <strong>Neural Discrete Representation Learning.</strong> <em>Aaron van den Oord et al. arXiv.</em> [<a href="https://arxiv.org/abs/1711.00937" rel="nofollow">paper</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-122-audio" dir="auto"><a href="#122-audio">1.2.2 Audio<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/06] <strong>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding.</strong> <em>Hang Zhang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.02858" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages.</strong> <em>Feilong Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.04160" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language.</strong> <em>Zhaoyang Liu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.05662" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.</strong> <em>Rongjie Huang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.12995" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580" rel="nofollow">paper</a>]</li>
<li>[2021/06] <strong>HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.</strong> <em>Wei-Ning Hsu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2106.07447" rel="nofollow">paper</a>]</li>
<li>[2021/04] <strong>AST: Audio Spectrogram Transformer.</strong> <em>Yuan Gong et al. arXiv.</em> [<a href="https://arxiv.org/abs/2104.01778" rel="nofollow">paper</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-13-action-expand-action-space-of-llm-based-agents" dir="auto"><a href="#13-action-expand-action-space-of-llm-based-agents">1.3 Action: Expand Action Space of LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-131-tool-using" dir="auto"><a href="#131-tool-using">1.3.1 Tool Using<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/07] <strong>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.16789" rel="nofollow">paper</a>] [<a href="https://github.com/openbmb/toolbench">code</a>] [<a href="https://paperswithcode.com/dataset/toolbench" rel="nofollow">dataset</a>]</li>
<li>[2023/05] <strong>Large Language Models as Tool Makers.</strong> <em>Tianle Cai et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17126" rel="nofollow">paper</a>] [<a href="https://github.com/ctlllll/llm-toolmaker">code</a>]</li>
<li>[2023/05] <strong>CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation.</strong> <em>Cheng Qian et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14318" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>Tool Learning with Foundation Models.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08354" rel="nofollow">paper</a>] [<a href="https://github.com/openbmb/bmtools">code</a>]</li>
<li>[2023/04] <strong>ChemCrow: Augmenting large-language models with chemistry tools.</strong> <em>Andres M Bran (Laboratory of Artificial Chemical Intelligence, ISIC, EPFL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05376" rel="nofollow">paper</a>] [<a href="https://github.com/ur-whitelab/chemcrow-public">code</a>]</li>
<li>[2023/04] <strong>GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information.</strong> <em>Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu. arXiv.</em> [<a href="https://arxiv.org/abs/2304.09667" rel="nofollow">paper</a>] [<a href="https://github.com/ncbi/GeneGPT">code</a>]</li>
<li>[2023/04] <strong>OpenAGI: When LLM Meets Domain Experts.</strong> <em>Yingqiang Ge et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.04370" rel="nofollow">paper</a>] [<a href="https://github.com/agiresearch/openagi">code</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580" rel="nofollow">paper</a>] [<a href="https://github.com/microsoft/JARVIS">code</a>]</li>
<li>[2023/03] <strong>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models.</strong> <em>Chenfei Wu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.04671" rel="nofollow">paper</a>] [<a href="https://github.com/microsoft/visual-chatgpt">code</a>]</li>
<li>[2023/02] <strong>Augmented Language Models: a Survey.</strong> <em>Gr√©goire Mialon et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.07842" rel="nofollow">paper</a>]</li>
<li>[2023/02] <strong>Toolformer: Language Models Can Teach Themselves to Use Tools.</strong> <em>Timo Schick et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.04761" rel="nofollow">paper</a>]</li>
<li>[2022/05] <strong>TALM: Tool Augmented Language Models.</strong> <em>Aaron Parisi et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.12255" rel="nofollow">paper</a>]</li>
<li>[2022/05] <strong>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.</strong> <em>Ehud Karpas et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.00445" rel="nofollow">paper</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.01691" rel="nofollow">paper</a>]</li>
<li>[2021/12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332" rel="nofollow">paper</a>]</li>
<li>[2021/07] <strong>Evaluating Large Language Models Trained on Code.</strong> <em>Mark Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2107.03374" rel="nofollow">paper</a>] [<a href="https://github.com/openai/human-eval">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-132-embodied-action" dir="auto"><a href="#132-embodied-action">1.3.2 Embodied Action<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/07] <strong>Interactive language: Talking to robots in real time.</strong> <em>Corey Lynch et al. IEEE(RAL)</em> [<a href="https://arxiv.org/pdf/2210.06407.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Voyager: An open-ended embodied agent with large language models.</strong> <em>Guanzhi Wang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2305.16291.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments.</strong> <em>Sudipta Paul et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/28f699175783a2c828ae74d53dd3da20-Paper-Conference.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought.</strong> <em>Yao Mu et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.15021.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch">code</a>]</li>
<li>[2023/05] <strong>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models.</strong> <em>Gengze Zhou et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.16986.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation.</strong> <em>Chuhao Jin et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.18898.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2303.03378.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2303.11366.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
<li>[2023/02] <strong>Collaborating with language models for embodied reasoning.</strong> <em>Ishita Dasgupta et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2302.00763.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/02] <strong>Code as Policies: Language Model Programs for Embodied Control.</strong> <em>Jacky Liang et al. IEEE(ICRA).</em> [<a href="https://arxiv.org/pdf/2209.07753.pdf" rel="nofollow">paper</a>]</li>
<li>[2022/10] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2210.03629.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/ysymyth/ReAct">code</a>]</li>
<li>[2022/10] <strong>Instruction-Following Agents with Multimodal Transformer.</strong> <em>Hao Liu et al. CVPR</em> [<a href="https://arxiv.org/pdf/2210.13431.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/lhao499/instructrl">code</a>]</li>
<li>[2022/07] <strong>Inner Monologue: Embodied Reasoning through Planning with Language Models.</strong> <em>Wenlong Huang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2207.05608.pdf" rel="nofollow">paper</a>]</li>
<li>[2022/07] <strong>LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.</strong> <em>Dhruv Shahet al. CoRL</em> [<a href="https://proceedings.mlr.press/v205/shah23b/shah23b.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/blazejosinski/lm_nav">code</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2204.01691.pdf" rel="nofollow">paper</a>]</li>
<li>[2022/01] <strong>A Survey of Embodied AI: From Simulators to Research Tasks.</strong> <em>Jiafei Duan et al. IEEE(TETCI).</em> [<a href="https://arxiv.org/pdf/2103.04918.pdf" rel="nofollow">paper</a>]</li>
<li>[2022/01] <strong>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.</strong> <em>Wenlong Huang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2201.07207v2.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/huangwl18/language-planner">code</a>]</li>
<li>[2020/04] <strong>Experience Grounds Language.</strong> <em>Yonatan Bisk et al. EMNLP</em> [<a href="https://arxiv.org/pdf/2004.10151.pdf" rel="nofollow">paper</a>]</li>
<li>[2019/03] <strong>Review of Deep Reinforcement Learning for Robot Manipulation.</strong> <em>Hai Nguyen et al. IEEE(IRC).</em> [<a href="https://www.researchgate.net/profile/Hai-Nguyen-128/publication/355980729_Review_of_Deep_Reinforcement_Learning_for_Robot_Manipulation/links/6187ef153068c54fa5bb977e/Review-of-Deep-Reinforcement-Learning-for-Robot-Manipulation.pdf" rel="nofollow">paper</a>]</li>
<li>[2005/01] <strong>The Development of Embodied Cognition: Six Lessons from Babies.</strong> <em>Linda Smith et al. Artificial Life.</em> [<a href="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf" rel="nofollow">paper</a>]</li>
</ul>
<h2 tabindex="-1" id="user-content-2-agents-in-practice-applications-of-llm-based-agents" dir="auto"><a href="#2-agents-in-practice-applications-of-llm-based-agents">2. Agents in Practice: Applications of LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure7.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure7.jpg" width="60%"/></a></p>
<h3 tabindex="-1" id="user-content-21-general-ability-of-single-agent" dir="auto"><a href="#21-general-ability-of-single-agent">2.1 General Ability of Single Agent<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure8.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure8.jpg" width="60%"/></a></p>
<h4 tabindex="-1" id="user-content-211-task-oriented-deployment" dir="auto"><a href="#211-task-oriented-deployment">2.1.1 Task-oriented Deployment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<p dir="auto"><strong>In web scenarios</strong></p>
<ul dir="auto">
<li>[2023/07] <strong>WebArena: A Realistic Web Environment for Building Autonomous Agents.</strong> <em>Shuyan Zhou (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.13854" rel="nofollow">paper</a>] [<a href="https://webarena.dev/" rel="nofollow">code</a>]</li>
<li>[2023/07] <strong>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.</strong> <em>Izzeddin Gur (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.12856" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>SYNAPSE: Leveraging Few-Shot Exemplars for
Human-Level Computer Control.</strong> <em>Longtao Zheng (Nanyang Technological University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.07863" rel="nofollow">paper</a>] [<a href="https://github.com/ltzheng/synapse">code</a>]</li>
<li>[2023/06] <strong>Mind2Web: Towards a Generalist Agent for the Web.</strong> <em>Xiang Deng (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.06070" rel="nofollow">paper</a>] [<a href="https://osu-nlp-group.github.io/Mind2Web/" rel="nofollow">code</a>]</li>
<li>[2023/05] <strong>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</strong> <em>Hiroki Furuta (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11854" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>Language Models can Solve Computer Tasks.</strong> <em>Geunwoo Kim (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17491" rel="nofollow">paper</a>] [<a href="https://github.com/posgnu/rci-agent">code</a>]</li>
<li>[2022/07] <strong>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.01206" rel="nofollow">paper</a>] [<a href="https://webshop-pnlp.github.io/" rel="nofollow">code</a>]</li>
<li>[2021/12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano (OpenAI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2309.07870.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]</li>
</ul>
<p dir="auto"><strong>In life scenarios</strong></p>
<ul dir="auto">
<li>[2023/08] <strong>InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent.</strong> <em>Po-Lin Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01552" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.</strong> <em>Yue Wu (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.02412" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Augmenting Autotelic Agents with Large Language Models.</strong> <em>C√©dric Colas (MIT) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.12487" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>Planning with Large Language Models via Corrective Re-prompting.</strong> <em>Shreyas Sundara Raman (Brown University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2211.09935" rel="nofollow">paper</a>]</li>
<li>[2022/10] <strong>Generating Executable Action Plans with Environmentally-Aware Language Models.</strong> <em>Maitrey Gramopadhye (University of North Carolina at Chapel Hill) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.04964" rel="nofollow">paper</a>] [<a href="https://github.com/hri-ironlab/scene_aware_language_planner">code</a>]</li>
<li>[2022/01] <strong>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.</strong> <em>Wenlong Huang (UC Berkeley) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2201.07207" rel="nofollow">paper</a>] [<a href="https://wenlong.page/language-planner/" rel="nofollow">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-212-innovation-oriented-deployment" dir="auto"><a href="#212-innovation-oriented-deployment">2.1.2 Innovation-oriented Deployment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/08] <strong>The Hitchhiker&#39;s Guide to Program Analysis: A Journey with Large Language Models.</strong> <em>Haonan Li (UC Riverside) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00245" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.</strong> <em>Yeonghun Kang (Korea Advanced Institute of Science
and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01423" rel="nofollow">paper</a>]</li>
<li>[2023/07] <strong>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics.</strong> <em>Melanie Swan (University College London) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.02502" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>Towards Autonomous Testing Agents via Conversational Large Language Models.</strong> <em>Robert Feldt (Chalmers University of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.05152" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>Emergent autonomous scientific research capabilities of large language models.</strong> <em>Daniil A. Boiko (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05332" rel="nofollow">paper</a>]</li>
<li>[2023/04] <strong>ChemCrow: Augmenting large-language models with chemistry tools.</strong> <em>Andres M Bran (Laboratory of Artificial Chemical Intelligence, ISIC, EPFL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05376" rel="nofollow">paper</a>] [<a href="https://github.com/ur-whitelab/chemcrow-public">code</a>]</li>
<li>[2022/03] <strong>ScienceWorld: Is your Agent Smarter than a 5th Grader?</strong> <em>Ruoyao Wang (University of Arizona) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.07540" rel="nofollow">paper</a>] [<a href="https://sciworld.apps.allenai.org/" rel="nofollow">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-213-lifecycle-oriented-deployment" dir="auto"><a href="#213-lifecycle-oriented-deployment">2.1.3 Lifecycle-oriented Deployment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291" rel="nofollow">paper</a>] [<a href="https://voyager.minedojo.org/" rel="nofollow">code</a>]</li>
<li>[2023/05] <strong>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.</strong> <em>Xizhou Zhu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17144" rel="nofollow">paper</a>] [<a href="https://github.com/OpenGVLab/GITM">code</a>]</li>
<li>[2023/03] <strong>Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.</strong> <em>Haoqi Yuan (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.16563" rel="nofollow">paper</a>] [<a href="https://sites.google.com/view/plan4mc" rel="nofollow">code</a>]</li>
<li>[2023/02] <strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.</strong> <em>Zihao Wang (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.01560" rel="nofollow">paper</a>] [<a href="https://github.com/CraftJarvis/MC-Planner">code</a>]</li>
<li>[2023/01] <strong>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling.</strong> <em>Kolby Nottingham (University of California Irvine, Irvine) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12050" rel="nofollow">paper</a>] [<a href="https://deckardagent.github.io/" rel="nofollow">code</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-22-coordinating-potential-of-multiple-agents" dir="auto"><a href="#22-coordinating-potential-of-multiple-agents">2.2 Coordinating Potential of Multiple Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure9.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure9.jpg" width="60%"/></a></p>
<h4 tabindex="-1" id="user-content-221-cooperative-interaction-for-complementarity" dir="auto"><a href="#221-cooperative-interaction-for-complementarity">2.2.1 Cooperative Interaction for Complementarity<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<p dir="auto"><strong>Disordered cooperation</strong></p>
<ul dir="auto">
<li>[2023/07] <strong>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.</strong> <em>Zhenhailong Wang (University of Illinois Urbana-Champaign) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.05300" rel="nofollow">paper</a>] [<a href="https://github.com/MikeWangWZHL/Solo-Performance-Prompting">code</a>]</li>
<li>[2023/07] <strong>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models.</strong> <em>Zhao Mandi, Shreeya Jain, Shuran Song (Columbia University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04738" rel="nofollow">paper</a>] [<a href="https://project-roco.github.io/" rel="nofollow">code</a>]</li>
<li>[2023/04] <strong>ChatLLM Network: More brains, More intelligence.</strong> <em>Rui Hao (Beijing University of Posts and Telecommunications) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.12998" rel="nofollow">paper</a>]</li>
<li>[2023/01] <strong>Blind Judgement: Agent-Based Supreme Court Modelling With GPT.</strong> <em>Sil Hamilton (McGill University). arXiv.</em> [<a href="https://arxiv.org/abs/2301.05327" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2309.07870.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]</li>
</ul>
<p dir="auto"><strong>Ordered cooperation</strong></p>
<ul dir="auto">
<li>[2023/08] <strong>CGMI: Configurable General Multi-Agent Interaction Framework.</strong> <em>Shi Jinxin (East China Normal University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.12503" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>ProAgent: Building Proactive Cooperative AI with Large Language Models.</strong> <em>Ceyao Zhang (The Chinese University of Hong Kong, Shenzhen) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.11339" rel="nofollow">paper</a>] [<a href="https://pku-proagent.github.io/" rel="nofollow">code</a>]</li>
<li>[2023/08] <strong>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents.</strong> <em>Weize Chen (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10848" rel="nofollow">paper</a>] [<a href="https://github.com/OpenBMB/AgentVerse">code</a>]</li>
<li>[2023/08] <strong>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.</strong> <em>Qingyun Wu (Pennsylvania State University
) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.08155" rel="nofollow">paper</a>] [<a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen/" rel="nofollow">code</a>]</li>
<li>[2023/08] <strong>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.</strong> <em>Sirui Hong (DeepWisdom) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00352" rel="nofollow">paper</a>] [<a href="https://github.com/geekan/MetaGPT">code</a>]</li>
<li>[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924" rel="nofollow">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</li>
<li>[2023/06] <strong>Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents.</strong> <em>Yashar Talebira (University of Alberta) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03314" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960" rel="nofollow">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/05] <strong>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.</strong> <em>Bill Yuchen Lin (Allen Institute for Artificial Intelligence) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17390" rel="nofollow">paper</a>] [<a href="https://yuchenlin.xyz/swiftsage/" rel="nofollow">code</a>]</li>
<li>[2023/05] <strong>ChatGPT as your Personal Data Scientist.</strong> <em>Md Mahadi Hassan (Auburn University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.13657" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society.</strong> <em>Guohao Li (King Abdullah University of Science and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17760" rel="nofollow">paper</a>] [<a href="https://github.com/lightaime/camel">code</a>]</li>
<li>[2023/03] <strong>DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents.</strong> <em>Varun Nair (Curai Health) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17071" rel="nofollow">paper</a>] [<a href="https://github.com/curai/curai-research/tree/main/DERA">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-222-adversarial-interaction-for-advancement" dir="auto"><a href="#222-adversarial-interaction-for-advancement">2.2.2 Adversarial Interaction for Advancement<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201" rel="nofollow">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</li>
<li>[2023/05] <strong>Improving Factuality and Reasoning in Language Models through Multiagent Debate.</strong> <em>Yilun Du (MIT CSAIL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14325" rel="nofollow">paper</a>] [<a href="https://composable-models.github.io/llm_debate/" rel="nofollow">code</a>]</li>
<li>[2023/05] <strong>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback.</strong> <em>Yao Fu (University of Edinburgh) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10142" rel="nofollow">paper</a>] [<a href="https://github.com/FranxYao/GPT-Bargaining">code</a>]</li>
<li>[2023/05] <strong>Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate.</strong> <em>Kai Xiong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11595" rel="nofollow">paper</a>] [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main">code</a>]</li>
<li>[2023/05] <strong>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate.</strong> <em>Tian Liang (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.19118" rel="nofollow">paper</a>] [<a href="https://github.com/Skytliang/Multi-Agents-Debate">code</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-23-interactive-engagement-between-human-and-agent" dir="auto"><a href="#23-interactive-engagement-between-human-and-agent">2.3 Interactive Engagement between Human and Agent<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure10.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure10.jpg" width="60%"/></a></p>
<h4 tabindex="-1" id="user-content-231-instructor-executor-paradigm" dir="auto"><a href="#231-instructor-executor-paradigm">2.3.1 Instructor-Executor Paradigm<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-education" dir="auto"><a href="#education">Education<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/07] <strong>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics.</strong> <em>Melanie Swan (UCL) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2307.02502" rel="nofollow">paper</a>]
<ul dir="auto">
<li>Communicate with humans to help them understand and use mathematics.</li>
</ul>
</li>
<li>[2023/03] <strong>Hey Dona! Can you help me with student course registration?</strong> <em>Vishesh Kalvakurthi (MSU) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2303.13548" rel="nofollow">paper</a>]
<ul dir="auto">
<li>This is a developed application called Dona that offers virtual voice assistance in student course registration, where humans provide instructions.</li>
</ul>
</li>
</ul>
<h5 tabindex="-1" id="user-content-health" dir="auto"><a href="#health">Health<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue.</strong> <em>Songhua Yang (ZZU) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2308.03549" rel="nofollow">paper</a>] [<a href="https://github.com/SupritYoung/Zhongjing">code</a>]</li>
<li>[2023/05] <strong>HuatuoGPT, towards Taming Language Model to Be a Doctor.</strong> <em>Hongbo Zhang (CUHK-SZ) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.15075" rel="nofollow">paper</a>] [<a href="https://github.com/FreedomIntelligence/HuatuoGPT">code</a>] [<a href="https://www.huatuogpt.cn/" rel="nofollow">demo</a>]</li>
<li>[2023/05] <strong>Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.</strong> <em>Shang-Ling Hsu (Gatech) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.08982" rel="nofollow">paper</a>]</li>
<li>[2020/10] <strong>A Virtual Conversational Agent for Teens with Autism Spectrum Disorder: Experimental Results and Design Lessons.</strong> <em>Mohammad Rafayet Ali (U of R) et al. IVA &#39;20.</em> [<a href="https://doi.org/10.1145/3383652.3423900" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-other-application" dir="auto"><a href="#other-application">Other Application<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>RecMind: Large Language Model Powered Agent For Recommendation.</strong> <em>Yancheng Wang (ASU, Amazon) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2308.14296" rel="nofollow">paper</a>]</li>
<li>[2023/08] <strong>Multi-Turn Dialogue Agent as Sales&#39; Assistant in Telemarketing.</strong> <em>Wanting Gao (JNU) et al. IEEE.</em> [<a href="https://doi.org/10.1109/IJCNN54540.2023.10192042" rel="nofollow">paper</a>]</li>
<li>[2023/07] <strong>PEER: A Collaborative Language Model.</strong> <em>Timo Schick (Meta AI) et al. arXiv.</em> [<a href="https://openreview.net/pdf?id=KbYevcLjnc" rel="nofollow">paper</a>]</li>
<li>[2023/07] <strong>DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations.</strong> <em>Bo-Ru Lu (UW) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2307.07047" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn.</strong> <em>Difei Gao (NUS) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2306.08640" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Agents: An Open-source Framework for Autonomous Language Agents.</strong> Wangchunshu Zhou (AIWaves) et al. arXiv.* [<a href="https://arxiv.org/pdf/2309.07870.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/aiwaves-cn/agents">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-232-equal-partnership-paradigm" dir="auto"><a href="#232-equal-partnership-paradigm">2.3.2 Equal Partnership Paradigm<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-empathetic-communicator" dir="auto"><a href="#empathetic-communicator">Empathetic Communicator<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>SAPIEN: Affective Virtual Agents Powered by Large Language Models.</strong> <em>Masum Hasan et al. arXiv.</em> [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">paper</a>] [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">code</a>] [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">project page</a>] [<a href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">dataset</a>]</li>
<li>[2023/05] <strong>Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.</strong> <em>Shang-Ling Hsu (Gatech) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.08982" rel="nofollow">paper</a>]</li>
<li>[2022/07] <strong>Artificial empathy in marketing interactions: Bridging the human-AI gap in affective and social customer experience.</strong> <em>Yuping Liu‚ÄëThompkins et al.</em> [<a href="https://link.springer.com/article/10.1007/s11747-022-00892-5" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-human-level-participant" dir="auto"><a href="#human-level-participant">Human-Level Participant<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/08] <strong>Quantifying the Impact of Large Language Models on Collective Opinion Dynamics.</strong> <em>Chao Li et al. CoRR.</em> [<a href="https://doi.org/10.48550/arXiv.2308.03313" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning.</strong> <em>Anton Bakhtin et al. ICLR.</em> [<a href="https://openreview.net/pdf?id=F61FwJTZhb" rel="nofollow">paper</a>]</li>
<li>[2023/06] <strong>Decision-Oriented Dialogue for Human-AI Collaboration.</strong> <em>Jessy Lin et al. CoRR.</em> [<a href="https://doi.org/10.48550/arXiv.2305.20076" rel="nofollow">paper</a>]</li>
<li>[2022/11] <strong>Human-level play in the game of Diplomacy by combining language models with strategic reasoning.</strong> <em>FAIR et al. Science.</em> [<a href="https://www.science.org/doi/10.1126/science.ade9097" rel="nofollow">paper</a>]</li>
</ul>
<h2 tabindex="-1" id="user-content-3-agent-society-from-individuality-to-sociality" dir="auto"><a href="#3-agent-society-from-individuality-to-sociality">3. Agent Society: From Individuality to Sociality<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure12.jpg"><img src="https://swe-to-mle.pages.dev/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure12.jpg" width="60%"/></a></p>
<h3 tabindex="-1" id="user-content-31-behavior-and-personality-of-llm-based-agents" dir="auto"><a href="#31-behavior-and-personality-of-llm-based-agents">3.1 Behavior and Personality of LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-311-social-behavior" dir="auto"><a href="#311-social-behavior">3.1.1 Social Behavior<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-individual-behaviors" dir="auto"><a href="#individual-behaviors">Individual behaviors<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291" rel="nofollow">paper</a>] [<a href="https://voyager.minedojo.org/" rel="nofollow">code</a>]</li>
<li>[2023/04] <strong>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.</strong> <em>Bo Liu (University of Texas) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.11477" rel="nofollow">paper</a>] [<a href="https://github.com/Cranial-XIX/llm-pddl">code</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn (Northeastern University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11366" rel="nofollow">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess (Google) et al. ICML.</em> [<a href="http://proceedings.mlr.press/v202/driess23a/driess23a.pdf" rel="nofollow">paper</a>] [<a href="https://palm-e.github.io/" rel="nofollow">project page</a>]</li>
<li>[2023/03] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao (Princeton University) et al. ICLR.</em> [<a href="https://openreview.net/pdf?id=WE_vluYUL-X" rel="nofollow">paper</a>] [<a href="https://react-lm.github.io/" rel="nofollow">project page</a>]</li>
<li>[2022/01] <strong>Chain-of-thought prompting elicits reasoning in large language models.</strong>  <em>Jason Wei (Google) et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-group-behaviors" dir="auto"><a href="#group-behaviors">Group behaviors<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>
<p dir="auto">[2023/09] <strong>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf.</strong> <em>Yuzhuang Xu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2309.04658" rel="nofollow">paper</a>]</p>
</li>
<li>
<p dir="auto">[2023/08] <strong>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents.</strong> <em>Weize Chen (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10848" rel="nofollow">paper</a>] [<a href="https://github.com/OpenBMB/AgentVerse">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/08] <strong>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.</strong> <em>Qingyun Wu (Pennsylvania State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.08155" rel="nofollow">paper</a>] [<a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen/" rel="nofollow">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201" rel="nofollow">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924" rel="nofollow">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/07] <strong>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models.</strong> <em>Zhao Mandi, Shreeya Jain, Shuran Song (Columbia University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04738" rel="nofollow">paper</a>] [<a href="https://project-roco.github.io/" rel="nofollow">code</a>]</p>
</li>
<li>
<p dir="auto">[2023/08] <strong>ProAgent: Building Proactive Cooperative AI with Large Language Models.</strong> <em>Ceyao Zhang (The Chinese University of Hong Kong, Shenzhen) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.11339" rel="nofollow">paper</a>] [<a href="https://pku-proagent.github.io/" rel="nofollow">code</a>]</p>
</li>
</ul>
<h4 tabindex="-1" id="user-content-312-personality" dir="auto"><a href="#312-personality">3.1.2 Personality<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<h5 tabindex="-1" id="user-content-cognition" dir="auto"><a href="#cognition">Cognition<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/03] <strong>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.</strong> <em>Thilo Hagendorff (University of Stuttgart) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.13988" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>Mind meets machine: Unravelling GPT-4&#39;s cognitive psychology.</strong> <em>Sifatkaur Dhingra (Nowrosjee Wadia College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11436" rel="nofollow">paper</a>]</li>
<li>[2022/07] <strong>Language models show human-like content effects on reasoning.</strong> <em>Ishita Dasgupta (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.07051" rel="nofollow">paper</a>]</li>
<li>[2022/06] <strong>Using cognitive psychology to understand GPT-3.</strong> <em>Marcel Binz et al. arXiv.</em> [<a href="https://arxiv.org/abs/2206.14576" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-emotion" dir="auto"><a href="#emotion">Emotion<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/07] <strong>Emotional Intelligence of Large Language Models.</strong> <em>Xuena Wang (Tsinghua  University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.09042" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>ChatGPT outperforms humans in emotional awareness evaluations.</strong> <em>Zohar Elyoseph et al. Frontiers in Psychology.</em> [<a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1199058/full" rel="nofollow">paper</a>]</li>
<li>[2023/02] <strong>Empathetic AI for Empowering Resilience in Games.</strong> <em>Reza Habibi (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.09070" rel="nofollow">paper</a>]</li>
<li>[2022/12] <strong>Computer says ‚ÄúNo‚Äù: The Case Against Empathetic Conversational AI.</strong> <em>Alba Curry (University of Leeds) et al. ACL.</em> [<a href="https://aclanthology.org/2023.findings-acl.515.pdf" rel="nofollow">paper</a>]</li>
</ul>
<h5 tabindex="-1" id="user-content-character" dir="auto"><a href="#character">Character<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h5>
<ul dir="auto">
<li>[2023/07] <strong>Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models.</strong> <em>Keyu Pan (ByteDance) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.16180" rel="nofollow">paper</a>] [<a href="https://github.com/HarderThenHarder/transformers_tasks">code</a>]</li>
<li>[2023/07] <strong>Personality Traits in Large Language Models.</strong> <em>Mustafa Safdari (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.00184" rel="nofollow">paper</a>] [<a href="https://github.com/HarderThenHarder/transformers_tasks">code</a>]</li>
<li>[2022/12] <strong>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective.</strong> <em>Xingxuan Li (Alibaba) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.10529" rel="nofollow">paper</a>]</li>
<li>[2022/12] <strong>Identifying and Manipulating the Personality Traits of Language Models.</strong> <em>Graham Caron et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.10276" rel="nofollow">paper</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-32-environment-for-agent-society" dir="auto"><a href="#32-environment-for-agent-society">3.2 Environment for Agent Society<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-321-text-based-environment" dir="auto"><a href="#321-text-based-environment">3.2.1 Text-based Environment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/08] <strong>Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models.</strong> <em>Aidan O‚ÄôGara (University of Southern California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01404" rel="nofollow">paper</a>] [<a href="https://github.com/aogara-ds/hoodwinked">code</a>]</li>
<li>[2023/03] <strong>CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society.</strong> <em>Guohao Li (King Abdullah University of Science and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17760" rel="nofollow">paper</a>] [<a href="https://github.com/lightaime/camel">code</a>]</li>
<li>[2020/12] <strong>Playing Text-Based Games with Common Sense.</strong> <em>Sahith Dambekodi (Georgia Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/pdf/2012.02757.pdf" rel="nofollow">paper</a>]</li>
<li>[2019/09] <strong>Interactive Fiction Games: A Colossal Adventure.</strong> <em>Matthew Hausknecht (Microsoft Research) et al. AAAI.</em> [<a href="https://cdn.aaai.org/ojs/6297/6297-13-9522-1-10-20200516.pdf" rel="nofollow">paper</a>] [<a href="https://github.com/microsoft/jericho">code</a>]</li>
<li>[2019/03] <strong>Learning to Speak and Act in a Fantasy Text Adventure Game.</strong> <em>Jack Urbanek (Facebook) et al. ACL.</em> [<a href="https://aclanthology.org/D19-1062.pdf" rel="nofollow">paper</a>] [<a href="https://parl.ai/projects/light/" rel="nofollow">code</a>]</li>
<li>[2018/06] <strong>TextWorld: A Learning Environment for Text-based Games.</strong> <em>Marc-Alexandre C√¥t√© (Microsoft Research) et al. IJCAI.</em> [<a href="https://link.springer.com/chapter/10.1007/978-3-030-24337-1_3" rel="nofollow">paper</a>] [<a href="https://github.com/Microsoft/TextWorld">code</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-322-virtual-sandbox-environment" dir="auto"><a href="#322-virtual-sandbox-environment">3.2.2 Virtual Sandbox Environment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.04026" rel="nofollow">paper</a>] [<a href="https://www.agentsims.com/" rel="nofollow">project page</a>] [<a href="https://github.com/py499372727/AgentSims/">code</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960" rel="nofollow">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291" rel="nofollow">paper</a>] [<a href="https://voyager.minedojo.org/" rel="nofollow">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442" rel="nofollow">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/03] <strong>Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.</strong> <em>Haoqi Yuan (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.16563" rel="nofollow">paper</a>] [<a href="https://sites.google.com/view/plan4mc" rel="nofollow">code</a>]</li>
<li>[2022/06] <strong>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.</strong> <em>Linxi Fan (NVIDIA) et al. NeurIPS.</em> [<a href="https://papers.nips.cc/paper_files/paper/2022/file/74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf" rel="nofollow">paper</a>] [<a href="https://minedojo.org/" rel="nofollow">project page</a>]</li>
</ul>
<h4 tabindex="-1" id="user-content-323-physical-environment" dir="auto"><a href="#323-physical-environment">3.2.3 Physical Environment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<ul dir="auto">
<li>[2023/09] <strong>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking.</strong> <em>Homanga Bharadhwaj (Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2309.01918" rel="nofollow">paper</a>] [<a href="https://robopen.github.io/" rel="nofollow">project page</a>]</li>
<li>[2023/05] <strong>AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments.</strong> <em>Sudipta Paul et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/28f699175783a2c828ae74d53dd3da20-Paper-Conference.pdf" rel="nofollow">paper</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess (Google) et al. ICML.</em> [<a href="http://proceedings.mlr.press/v202/driess23a/driess23a.pdf" rel="nofollow">paper</a>] [<a href="https://palm-e.github.io/" rel="nofollow">project page</a>]</li>
<li>[2022/10] <strong>Interactive Language: Talking to Robots in Real Time.</strong> <em>Corey Lynch (Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.06407" rel="nofollow">paper</a>] [<a href="https://github.com/google-research/language-table">code</a>]</li>
</ul>
<h3 tabindex="-1" id="user-content-33-society-simulation-with-llm-based-agents" dir="auto"><a href="#33-society-simulation-with-llm-based-agents">3.3 Society Simulation with LLM-based Agents<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<ul dir="auto">
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.04026" rel="nofollow">paper</a>] [<a href="https://www.agentsims.com/" rel="nofollow">project page</a>] [<a href="https://github.com/py499372727/AgentSims/">code</a>]</li>
<li>[2023/07] <strong>S$^3$ : Social-network Simulation System with Large Language Model-Empowered Agents.</strong> <em>Chen Gao (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.14984" rel="nofollow">paper</a>]</li>
<li>[2023/07] <strong>Epidemic Modeling with Generative Agents.</strong> <em>Ross Williams (Virginia Tech) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04986" rel="nofollow">paper</a>] [<a href="https://github.com/bear96/GABM-Epidemic">code</a>]</li>
<li>[2023/06] <strong>RecAgent: A Novel Simulation Paradigm for Recommender Systems.</strong> <em>Lei Wang (Renmin University of China) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.02552" rel="nofollow">paper</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960" rel="nofollow">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442" rel="nofollow">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2022/08] <strong>Social Simulacra: Creating Populated Prototypes for Social Computing Systems.</strong> <em>Joon Sung Park (Stanford University) et al. UIST.</em> [<a href="https://dl.acm.org/doi/10.1145/3526113.3545616" rel="nofollow">paper</a>]</li>
</ul>
<h2 tabindex="-1" id="user-content-citation" dir="auto"><a href="#citation">Citation<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">If you find this repository useful, please cite our paper:</p>
<div data-snippet-clipboard-copy-content="@misc{xi2023rise,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}"><pre><code>@misc{xi2023rise,
      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, 
      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
      year={2023},
      eprint={2309.07864},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
</code></pre></div>
<h2 tabindex="-1" id="user-content-project-maintainers--contributors" dir="auto"><a href="#project-maintainers--contributors">Project Maintainers &amp; Contributors<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>Zhiheng Xi ÔºàÂ•öÂøóÊÅí, <a href="https://github.com/WooooDyy">@WooooDyy</a>Ôºâ</li>
<li>Wenxiang Chen ÔºàÈôàÊñáÁøî, <a href="https://github.com/chenwxOggai">@chenwxOggai</a>Ôºâ</li>
<li>Xin Guo ÔºàÈÉ≠Êòï, <a href="https://github.com/XinGuo2002">@XinGuo2002</a>Ôºâ</li>
<li>Wei HeÔºà‰Ωï‰∏∫, <a href="https://github.com/hewei2001">@hewei2001</a>Ôºâ</li>
<li>Yiwen Ding Ôºà‰∏ÅÊÄ°Êñá, <a href="https://github.com/Yiwen-Ding">@Yiwen-Ding</a>Ôºâ</li>
<li>Boyang HongÔºàÊ¥™ÂçöÊù®, <a href="https://github.com/HBY-hub">@HongBoYang</a>Ôºâ</li>
<li>Ming Zhang ÔºàÂº†Êòé, <a href="https://github.com/KongLongGeFDU">@KongLongGeFDU</a>Ôºâ</li>
<li>Junzhe WangÔºàÁéãÊµöÂì≤, <a href="https://github.com/zsxmwjz">@zsxmwjz</a>Ôºâ</li>
<li>Senjie JinÔºàÈáëÊ£ÆÊù∞, <a href="https://github.com/Leonnnnnn929">@Leonnnnnn929</a>Ôºâ</li>
</ul>
<h2 tabindex="-1" id="user-content-contact" dir="auto"><a href="#contact">Contact<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>Zhiheng Xi: <a href="mailto:zhxi22@m.fudan.edu.cn">zhxi22@m.fudan.edu.cn</a></li>
</ul>
<h2 tabindex="-1" id="user-content-star-history" dir="auto"><a href="#star-history">Star History<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto"><a href="https://star-history.com/#WooooDyy/LLM-Agent-Paper-List&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/f063694fbe70a152b6357ba3b7a6398414cd1f452239dcb3f32cff66c1e28abf/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d576f6f6f6f4479792f4c4c4d2d4167656e742d50617065722d4c69737426747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=WooooDyy/LLM-Agent-Paper-List&amp;type=Date"/></a></p>
</article>
          </div></div>
  </body>
</html>
