<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.inferless.com/serverless-gpu-market">Original</a>
    <h1>The State of Serverless GPUs</h1>
    
    <div id="readability-page-1" class="page"><div><section></section><section><div><div><div><div><div><div id="Executive-Summary"><div><h3>Executive Summary</h3><p>The demand for high-performance computing resources is soaring due to rapid advancements in artificial intelligence and generative models, particularly for efficient GPU-based solutions that accelerate complex AI tasks. As serverless computing gains traction for CPU utilization, we predict a similar trend for GPUs.</p><p>This benchmarking report evaluates the current landscape of serverless GPU inference platforms for AI-driven organizations. We assessed major providers against user needs and highlight critical factors for selecting a serverless GPU platform. Our analysis includes a thorough comparison of each provider&#39;s advantages and disadvantages, enabling businesses to make well-informed decisions that align with their unique requirements.</p><p>This report serves as a valuable resource for generative AI companies seeking to leverage the potential of serverless GPU inference platforms, aiding them in navigating the ever-evolving technology landscape with confidence.</p></div></div><div id="What-do-users-look-for-in-an-ideal-serverless-GPU-offering"><div><h3>What do users look for in an ideal serverless GPU offering?</h3><p>So far, we have interviewed hundreds of ML Engineers &amp; Data Scientists, and, apart from trust, system reliability, integrations, some specific features that users are looking for in their ideal serverless GPU offerings are:</p><ol role="list"><li><strong>Cost Efficiency</strong>: Many organizations or users have less than 50% GPU utilization, resulting in expensive hourly or contractual rental fees. Serverless platforms enable dynamic scaling of GPU resources, allowing users to pay only for what they use, significantly reducing average monthly expenses.</li><li><strong>Model Support (Multiple Frameworks)</strong>: Users require support for various model frameworks, such as ONNX or PyTorch, depending on their organization&#39;s needs. An ideal platform should support all major frameworks, avoiding user friction caused by forced conversions or limitations.</li><li><strong>Minimal Cold Start Latency &amp; Inference Time</strong>: Low cold start latency and low inference time are critical aspects for optimal user experiences, except in batch processing or non-production environments. An ideal platform should offer consistently low cold start latency across all calls or loads.</li><li><strong>Effortless Scalable Infrastructure (0→1→n) and (n→0)</strong>: Configuring and scaling GPU infrastructure can be a complex and time-consuming process. An ideal platform should be able to automate scaling, requiring minimal user input beyond setting limits or billing parameters.</li><li><strong>Comprehensive Logging &amp; Visible Metrics</strong>: Users need detailed logs of API calls for analyzing loads, scaling, success vs. failure rates, and general analytics. An ideal platform should offer options for exporting or connecting users&#39; observability stacks.</li></ol><p>By addressing these key considerations, serverless GPU platforms can deliver high-performance, cost-effective solutions that cater to the diverse needs of organizations relying on AI and generative models.</p></div></div><div id="Key-Findings"><div><h3>Key Findings</h3><p>Several startups have entered the serverless GPU market, each attempting to address specific pain points for both training and inferencing. Despite the growing demand for GPU resources, major big tech players have yet to offer serverless GPU solutions. Given the high costs associated with underutilized GPUs, users and companies are eagerly seeking more economical alternatives.</p><p>This study examines five companies pioneering serverless GPU offerings, revealing key findings after thoroughly stress-testing their products:</p></div><div><div><p>https://wwww.replicate.com/</p></div></div><div><ol role="list"><li><strong>True serverless computing products remain scarce in the current market</strong>‍</li><li><strong>No product currently delivers an exceptional user experience<br/></strong>Crafting an outstanding user experience for serverless computing has proven difficult. While startups are striving to create serverless GPU platforms, aspects like cold start times, latency, autoscaling, and reliability still require refinement. Product enhancements have not been the primary focus, and it may take 6-12 months before more advanced features become available.</li><li><strong>Seamless infrastructure scaling is the most significant challenge for market players<br/></strong>Platforms encounter numerous technical issues when attempting to scale up or down to accommodate increased loads. Such issues become more prevalent during peak times, causing users to be reluctant to adopt these solutions for production workloads.</li><li><strong>Emphasis on technical metrics, specifically lowest cold start time and lowest inference time, is crucial<br/></strong>There is a considerable gap between simply offering inferencing or serverless capabilities and establishing a technical edge. Many companies have yet to develop reliable metrics for essential factors such as latency, cold start times, and scalability in large-scale deployments. As a result, extensive experimentation and exploration of various architectures are underway to identify the most effective solutions.</li><li><strong>Cost transparency and accountability are insufficient among current serverless providers<br/></strong>A notable number of serverless providers lack transparent cost calculators, leading to uncertainty and concealed fees. Users may also be billed for inefficiencies or scaling delays and for keeping machines active. To enhance transparency and accountability, a standardized and transparent billing system is indispensable.</li></ol><p>These findings underscore the challenges and opportunities within the serverless GPU market. As companies continue to innovate and refine their offerings, we anticipate more robust, user-friendly, and efficient solutions to emerge, enabling users to harness the full potential of serverless GPU platforms confidently.</p></div></div><div id="TL-DR-Analysis-and-review-of-all-the-current-players"><div><h3>TL;DR A<strong>nalysis and review of all the current players.</strong></h3><p>If you wish to go through the summary sheet, you can check out the analysis below:</p></div></div><div id="Review-and-analyze-all-current-players"><p><h3>Review and analyze all current players.</h3></p></div><div id="overview-1"><div><h4>A brief overview of Banana.dev</h4><p>Banana.dev is a platform that simplifies deployment of machine learning models with robust inference endpoints, scalable infrastructure, and a cost-effective pay-per-second billing model. It offers templates for popular models and one-touch deployment for open-source ones. Users are incentivized to share their models, promoting a collaborative environment. To import a model, users create a file using a template and import it from their GitHub repository, making deployment and management more accessible and user-friendly.</p></div></div><div id="like-1"><div><h4>What did we like about <strong>Banana.dev?</strong></h4><ol role="list"><li>Serverless, pay-per-second billing with an hour of free credit.</li><li>Developer-friendly: GitHub integration, templates, and simplified process.</li><li>Quick setup: takes less than 3-4 hours.</li><li>Community features for model-sharing and collaboration.</li><li>Transparent and engaged: shares roadmap, feature requests, and bug list, and maintains active social media presence.</li></ol></div></div><div id="gaps-1"><div><h4>What gaps did we find<strong> in Banana.dev?</strong></h4><ol role="list"><li><strong>Billing for platform-induced delays/issues:</strong> </li><li><strong>Significant variability in cold start and inference times:<br/></strong>- Banana.dev is best suited for batch processing or for users who can tolerate longer cold start times and potential downtimes. </li><li><strong>Auto-scaling challenges:<br/></strong>- Autoscaling is not optimized, and machine provisioning lacks clarity.</li><li>Limited logging and monitoring capabilities. The platform does not provide integration or export options for metrics and logs, nor does it support integration with observability tools.</li><li>Restricted to GitHub uploads in a specific format. Users with different formats or repositories must undertake additional preparation before uploading their models, which can be cumbersome and time-consuming.</li></ol><p>‍</p></div></div><div id="pricing-1"><div><h3>Pricing</h3><p>GPU usage is billed at USD$ 0.00051992 per second, which is equivalent to $1.87 per hour. This is significantly cheaper compared to the average cloud provider charge of ~3 dollars for an A100 40gb machine.</p></div></div><div id="tb-1"><div><h3><strong>Technical benchmarking</strong></h3><p>We have tested out the platform by using 3 majorly used models as given below:</p></div><p><strong><em>Our </em><em>Comments:</em></strong> The cold start and inference time can vary greatly. Additionally, the availability of GPUs is not always 100%. The platform may experience &#34;degraded performance&#34; at times, as mentioned by the provider.</p></div><div id="overview-2"><div><h4>Brief Overview of Beam.cloud</h4><p>Beam.cloud, formerly known as <a href="http://slai.io/">Slai.io</a>, has evolved from providing an end-to-end solution encompassing both training and inference to focusing exclusively on inference. This pivot has allowed them to better cater to developers by offering a user-friendly, command-line/terminal-based approach. To streamline the integration process, Beam.cloud also offers an optional SDK. Additionally, it can be installed on existing Kubernetes clusters with automatic log export to S3.</p></div></div><div id="like-2"><div><h4>What did we like about<strong> Beam.cloud? </strong></h4><ol role="list"><li>Supports API endpoints, webhooks, and cron jobs for inference.</li><li>Seamless onboarding with a strong focus on the user journey.</li><li>Developer-friendly, terminal-based approach, catering to developers and DevOps professionals.</li><li>SDK integration option for real-time logs and simplified development.</li><li>Compatible with existing Kubernetes clusters and supports automatic log export to S3.</li><li>Flexible billing structure for CPU, GPU, and RAM requirements.</li></ol></div></div><div id="gaps-2"><div><h4>What gaps did we find<strong> in Beam.cloud?</strong></h4><ol role="list"><li>This tool is exclusively command-line/terminal-based. Users without experience using a terminal or CMD prompt may find it difficult to use.</li><li>Loading occasionally encounters issues that require trial and error to resolve. Additionally, memory limitations may be encountered.</li><li><strong>High variability in the cold start and inference times:</strong> </li><li><strong>Auto-scaling for REST API is enabled only on request.</strong> Restrictions should be communicated more clearly to users.</li></ol></div></div><div id="pricing-2"><div><h4><strong>Pricing</strong></h4><p>The pricing structure is interesting; CPU, RAM, and GPU are all charged separately. They offer a convenient pricing calculator for this purpose.</p><ol role="list"><li>GPU usage is billed at USD$ 0.00056944 per second, which equates to $2.04 per hour. On average, cloud providers charge approximately 3 dollars for an A100 40GB machine.</li><li>There&#39;s an added bonus of 10 hours of free compute, which is significant.</li></ol><p>‍</p><p>Additionally, they also charge a subscription fee and have a tier-based structure for using the platform:</p><ol role="list"><li>Developer - $0 (usage only)</li><li>Team - $25 per seat + usage</li><li>Professional - monthly fee + usage</li></ol></div></div><div id="tb-2"><div><h4>Technical benchmarking</h4><p>We have tested out the platform by using 3 majorly used models as given below:</p></div><div><p>*these results are what we got when we tested and there is a possibility of variable results</p><p><strong><em>Our Comments:</em></strong> The cold start and inference times are reasonable. The results above are the best we have achieved on the platform. However, there were multiple cases in which we encountered memory issues or technical errors that prevented us from obtaining satisfactory results.</p></div></div><div id="overview-3"><div><h4>A little brief about the product</h4><p>Replicate is a platform that supports custom and pre-trained machine learning models. It offers standard models and emphasizes pre-trained models for user convenience. Users can share their models and collaborate with others. Replicate has a diverse collection of models, many of which have been executed numerous times. It also offers options for depth and flexibility to customize models. Replicate aims to serve as a comprehensive solution in the machine learning model deployment landscape.</p></div></div><div id="like-3"><div><h4>What did we like about Replicate?</h4><p>1. Emphasis on open usage of popular pre-trained models for free, allowing users to explore before moving to custom models.</p><p>2. Encourages open-sourcing models through a waitlist concept, fostering more consumer use cases.</p><p>3. User-friendly platform with an intuitive interface.</p><p>4. Offers a choice between Nvidia T4 and A100, catering to users with lower budgets.</p><p>5. Boasts a large community with some models receiving over 47 million calls from public users.</p><p>6. Provides an open-source library called COG for deploying models on the platform.</p></div></div><div id="gaps-3"><div><h4>What gaps did we find in Replicate?</h4><ol role="list"><li>Replicate has limited post-deployment offerings, such as monitoring, logging, or data streaming.</li><li>Support is limited to email or Discord, with no instant support available.</li><li>Waitlist requirements for deploying custom models may deter users.</li><li>Output from inference APIs is stored in unique links, which may not be user-friendly, as users must individually delete past outputs.</li><li>Replicate doesn&#39;t have an option to import models from multiple sources like GitHub or SageMaker, relying solely on their COG library for uploads.</li></ol><p>‍</p></div></div><div id="pricing-3"><div><h4>Pricing</h4><p>Below is the pricing as mentioned by them on the site:</p><p>CPU - $0.0002/secNVIDIA T4 - $0.00055/secNVIDIA A100 - $ 0.0023/sec</p><p>Replicate NVIDIA A100 costs $8.28 per hour to use. On average, cloud providers charge approximately $3 for an A100 40GB machine.</p><p><strong>Free tier users are provided with T4 machines.</strong> Although there is a free tier, its limitations are not specified anywhere on the website.</p></div></div><div id="tb-3"><div><h4>Technical benchmarking.</h4><p>We have tested out the platform by using 3 majorly used models as given below:</p></div><div><p>*these results are what we got when we tested and there is a possibility of variable results</p><p>Our Comments: This product is very simple, yet boasts one of the best technical benchmarks. It offers a wide range of open source models, enabling quick deployment, while also providing a high degree of technical flexibility to tweak the models.</p></div></div><div id="overview-4"><div><h4>A little brief about the product</h4><p>Runpod is a platform that lets users choose between machines and serverless endpoints. It uses a Bring Your Own Container (BYOC) approach and has features such as GPU instances, serverless GPUs, and AI endpoints. The platform allows deploying container-based GPU instances from public and private repositories and accessing the SSH terminal through a web portal. Runpod offers fully managed and scalable AI endpoints for diverse workloads and applications. While it aims to address various user needs in machine learning model deployment, the performance of its features in the real world needs further evaluation for effectiveness and reliability.</p></div></div><div id="like-4"><div><h4>What did we like About Runpod?</h4><ol role="list"><li>This platform offers servers for all types of users.</li><li>The loading process is simple and only requires dropping a container link to pull a pod.</li><li>Payment and billing are based on credits and not directly billed to a card.</li><li>Although the number of models is limited, the platform has a community feature where users can fork models.</li><li>Users can access the SSH terminal through the web portal.</li></ol></div></div><div id="gaps-4"><div><h4>What gaps did we find in Runpod?</h4><ol role="list"><li>The current logging and tracking metrics are limited and do not add much value.</li><li>Post-deployment, it can be confusing to understand how the platform works, which may result in users receiving a bill if they are not careful.</li><li>The product&#39;s scope is limited, as it is only based on BYOC (Bring Your Own Container).</li><li><strong>Asynchronous Inferencing</strong> </li><li>If the number of machines is set to 0, the API will not work. If it is set to 1, the user will be billed for no usage.</li><li>There is no bot or instant support mechanism available.</li></ol><p>‍</p></div></div><div id="pricing-4"><div><h4><strong>Pricing</strong></h4><p><strong>How does Pod billing work?</strong></p><p>Each pod has an hourly cost based on its GPU type. You will be charged for the compute every minute that the pod is running. The charge is deducted from your RunPod credits. If you run out of credits, the pods will be automatically stopped and you will receive an email notification. If you don&#39;t refill your credits, the pods will eventually be terminated.</p><p><strong>Overall Pricing for machines</strong></p><p>The A100 (80GB) starts at $2.09/hour.</p><p><strong>Pricing for server-less APIs only</strong></p><p>The pricing ranges from $0.0002/second (16GB VRAM) to $0.001/second for an 80GB VRAM GPU.</p><p>Note: There is no trial period, so you have to pay to use the service.</p></div></div><div id="tb-4"><div><h4><strong>Technical benchmarking.</strong></h4><p>We have tested out the platform by using 3 majorly used models as given below:</p></div><p>*these results are what we got when we tested and there is a possibility of variable results. <strong>As on April 2023</strong></p></div><div id="overview-5"><div><h4>A little brief about the product.</h4><p>Pipeline is a serverless platform that hosts machine learning models via an inference API. It offers both custom and pre-trained models, including standard open-source models for immediate use. With per millisecond billing, the platform is cost-effective. Users can seek assistance and exchange insights on the active community on Discord. Pipeline provides a user-friendly experience backed by a supportive community.</p></div></div><div id="like-5"><div><h4>What did we like about pipeline.ai?</h4><ol role="list"><li>They offer over 15 pre-trained models, including Stable Diffusion, GPT, and Whisper, which can be easily deployed with one touch, and APIs can be used instantly.</li><li>The inference time for an ON model is very low, the lowest among competitors.</li><li>They have structured their model uploading process by focusing on one method, ONNX. The onboarding process is easy to understand, and if ONNX is not being used, there is documentation provided for other methods.</li><li>The community and support are excellent, with a super active Discord channel where queries are answered quickly.</li><li>Pricing is at a millisecond level, an industry-first.</li></ol></div></div><div id="gaps-5"><div><h4>What gaps did we find in pipeline.ai?</h4><ol role="list"><li><strong>No chat/helpbot/ticket-based support.</strong> Support is only Discord-based. While this is a good option for collaborating, it can impact response time.</li><li>Documentation can be a little clearer on what a user can achieve with a pipeline. They have built a Pipeline cloud with the Pipeline library, but need more use-case examples of the library.</li><li>User journey-based upload is only available for ONNX.</li><li>The website is a little broken when it comes to uploading methods. The ONNX quickstart option only appears once. (This issue has been raised with them.)</li><li><strong>The payment integration is a little buggy and lacks features.</strong></li><li>The metrics (logging and tracking) are very limited and don&#39;t add much value to the user. There are no ways to go deeper into the metrics, and there is no option to export data from the platform.</li><li>There is an added platform usage fee with not much ROI, according to us. A platform fee would make sense when there are many more ROI/feature additions.</li></ol></div></div><div id="pricing-5"><div><h4><strong>Pricing</strong></h4><p>They have a platform + per use billing:</p><ol role="list"><li>$<strong>0.00055/ sec is the</strong> compute cost, billed per millisecond</li><li>There is a platform fee of $12.99/month.</li><li>There is also a custom enterprise plan with added support.</li><li>They offer 20$ of free credits to try the platform.</li></ol></div></div><div id="tb-5"><div><h4><strong>Technical benchmarking.</strong></h4><p>We have tested out the platform by using 3 majorly used models as given below:</p></div><div><p>*these results are what we got when we tested and there is a possibility of variable results. <strong>As on february 2022</strong></p><p><strong><em>Our Comments</em></strong>: The benchmarking results are satisfactory. The platform works well and is designed for vertical deployment options.</p></div></div><div id="summary"><div><h3>Summary</h3><p>This comprehensive guide offers an in-depth analysis of the leading serverless platforms available in the market today, namely Banana.dev, Beam.cloud, Replicate, Runpod, and Pipeline.ai. The goal of this document is to equip readers with a thorough understanding of each platform&#39;s distinct strengths and weaknesses, enabling them to make informed decisions when choosing a serverless solution.</p><p>In our evaluations, we meticulously examined each platform based on critical factors such as pricing, features, user experience, and technical feasibility. By doing so, we present a clear and concise comparison, showcasing how these platforms measure up against one another.</p><p>Furthermore, this guide delves into the broader challenges and limitations faced by serverless platforms in general. For instance, we discuss the importance of cost management, as serverless solutions can become prohibitively expensive if not managed appropriately. Another critical consideration is ensuring that the chosen platform aligns with specific customer requirements, as not all features and capabilities may be an ideal fit for every use case.</p><p>In conclusion, this document serves as an invaluable resource for developers, DevOps engineers, and other stakeholders involved in the development and deployment of serverless applications. By providing an extensive examination of various serverless platforms and addressing the challenges and limitations inherent to serverless solutions, we empower readers to make well-informed decisions that align with their unique needs and requirements.</p><p>The culmination of the article is this huge summary matrix:</p><p><strong>Reminder:</strong> We welcome any feedback or updates to refine this comparison and ensure its accuracy. Our aim is to foster awareness of the current market landscape in the serverless GPU space, not to diminish any particular provider. Your input is invaluable – thank you in advance.</p><p>‍</p></div></div></div></div></div></div></div></section></div></div>
  </body>
</html>
