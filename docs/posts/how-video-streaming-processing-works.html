<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://howvideo.works/#processing">Original</a>
    <h1>How Video Streaming Processing Works</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>It might seem obvious, but there&#39;s a lot wrapped up in the word &#34;playback&#34;! The video player&#39;s UI is the first thing that most people associate with playback since it&#39;s what they see and interact with the most whenever they watch video on a site or platform. The icons and colors used in a player’s controls might be the most visible to the end-user, but that&#39;s just the tip of the playback iceberg.</p>
<p>The player controls or exposes other vital aspects of playback beyond just the controls themselves. Its functionality includes features like subtitles and captions, programmatic APIs for controlling playback, hooks for things like client-side analytics, ads, and much more.</p>
<p>Perhaps most importantly, a modern video platform will use what&#39;s called <strong>adaptive bitrate streaming</strong>, which means they provide a few different versions of a video, also known as renditions, for the player to pick from. These different versions vary in display sizes (resolutions) and file sizes (bitrate), and the player selects the best version it thinks it can stream smoothly without needing to pause so it can load more of the video (buffering). Different players make different decisions around how and when to switch to the different versions, so the player can make a big difference in the viewer&#39;s experience!</p>
<p>You might remember watching videos on Netflix or Youtube and noticing that sometimes in the middle of the video the quality will get worse for a few minutes, and then suddenly it will get better. That is what you saw when the quality changes you are experiencing <strong>adaptive bitrate streaming</strong>. If you are doing any kind of video streaming over the internet your solution must support this feature, without it it&#39;s likely that a large number of your viewers will be unable to stream your content.</p>
<p><span>
      <span></span>
  <img alt="Iceberg Image" title="Iceberg Image" src="https://mikkel.ca/static/9ad2976cae306d360833692e5aa7d469/0ec92/iceberg.png" srcset="/static/9ad2976cae306d360833692e5aa7d469/772e8/iceberg.png 200w,
/static/9ad2976cae306d360833692e5aa7d469/e17e5/iceberg.png 400w,
/static/9ad2976cae306d360833692e5aa7d469/0ec92/iceberg.png 402w" sizes="(max-width: 402px) 100vw, 402px" loading="lazy"/>
    </span></p>
<h2 id="hls"><a href="#hls" aria-label="hls permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>HLS</h2>
<p>While the concept (and spec overall) can be intimidating, the basic concept behind HLS is surprisingly simple. Even though the term stands for &#34;HTTP Live Streaming”, this technology has been adopted as the standard way to play video on demand. You take one big video file and break it up into small segments that can be anywhere from 2-12 seconds. So if you have a two-hour-long video, broken up into 10-second segments, you would have 720 segments.</p>
<p>Each segment is a file that ends with .ts. They are usually numbered sequentially so you get a directory that looks like this:</p>
<ul>
<li>
<p>segments/</p>
<ul>
<li>00001.ts</li>
<li>00002.ts</li>
<li>00003.ts</li>
<li>00004.ts</li>
</ul>
</li>
</ul>
<p>The player will download and play each segment as the user is streaming. And the player will keep a buffer of segments in case it loses network connection later.</p>
<p>Now let’s take this simple HLS idea a step further. What we can do here is create the segment files at different renditions. Working off our example above, using a 2-hour long video with 10-second segments we can create:</p>
<ul>
<li>720 segment files at 1080p</li>
<li>720 segment files at 720p</li>
<li>720 segment files at 360p</li>
</ul>
<p>The directory structure might look something like this:</p>
<ul>
<li>
<p>segments/</p>
<ul>
<li>
<p>1080p/</p>
<ul>
<li>00001.ts</li>
<li>00002.ts</li>
<li>00003.ts</li>
<li>00004.ts</li>
</ul>
</li>
<li>
<p>720p/</p>
<ul>
<li>00001.ts</li>
<li>00002.ts</li>
<li>00003.ts</li>
<li>00004.ts</li>
</ul>
</li>
<li>
<p>360p/</p>
<ul>
<li>00001.ts</li>
<li>00002.ts</li>
<li>00003.ts</li>
<li>00004.ts</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Now, this starts to get cool, the player that is playing our HLS files can decide for itself which rendition it wants to consume. To do this, the player will try to estimate the amount of bandwidth available and then it will make its best guess as to which rendition it wants to download and show to you.</p>
<p>The coolest part is that if the amount of bandwidth available to the player changes then the player can adapt quickly, this is called <strong>adaptive bitrate streaming</strong>.</p>
<h2 id="manifest-files-aka-playlist-files"><a href="#manifest-files-aka-playlist-files" aria-label="manifest files aka playlist files permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Manifest Files (a.k.a. Playlist Files)</h2>
<p>Since the individual segment files are the actual content broken up into little pieces it is the job of the <strong>manifest files</strong> (aka <strong>playlist files</strong>) to tell the player where to find the segment files.</p>
<p>There are two different kinds of manifest files. For a single video there is <strong>one master manifest</strong> and multiple <strong>rendition manifests</strong>. The master manifest file is the first point of contact for the player. For an HTML player in the browser, the master manifest is what would get loaded as the <code>src=</code> attribute on the player. The master manifest will tell the player about each rendition. For example, it might say:</p>
<ul>
<li>I have a 1080p rendition that uses 2,300,000 bits per second of bandwidth. It’s using these particular codecs, and the relative path for that manifest file is &#34;manifests/rendition_1.m3u8”.</li>
<li>I have a 720p rendition that uses 1,700,000 bits per second of bandwidth. It’s using these particular codecs, and the relative path for that manifest file is &#34;manifests/rendition_2.m3u8”.</li>
<li>I have a 360p rendition that uses 900,000 bits per second of bandwidth. It’s using these particular codecs, and the relative path for that manifest file is &#34;manifests/rendition_3.m3u8”.</li>
</ul>
<p>When the player loads the master manifest it observes all the renditions that are available and picks the best one. To continue this example, let’s say our player picks the 1080p resolution because the player has enough bandwidth available. So now it’s time to load the <strong>rendition manifest</strong> (&#34;manifests/rendition_1.m3u8&#34;)</p>
<p>The rendition manifest looks a lot different than the master. It is going to have some metadata and a link to every individual segment. Remember the segments from up above are the actual pieces of video content. In our example of a 2-hour long video broken up into 720 segments of 10 seconds each. The <strong>rendition_1.m3u8</strong> manifest is going to have an ordered list of the 720 segment files. As one would expect, for a long video, this file can get quite large:</p>
<ul>
<li>segments/1080p/00001.ts</li>
<li>segments/1080p/00002.ts</li>
<li>segments/1080p/00003.ts</li>
<li>segments/1080p/00004.ts</li>
<li>segments/1080p/00005.ts</li>
<li>….etc for 720 segments</li>
</ul>
<p>In summary, these are the steps the player goes through to play a video:</p>
<ol>
<li>Load the master manifest which has information about each rendition</li>
<li>Find out which renditions are available and pick the best one (based on available bandwidth)</li>
<li>Load the rendition manifest to find out where the segments are</li>
<li>Load the segments and start playback</li>
<li>After playback starts, that is when we get into <strong>adaptive bitrate streaming</strong>.</li>
</ol>
<h2 id="dash"><a href="#dash" aria-label="dash permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DASH</h2>
<p>DASH employs the same strategy as HLS. One video file is broken up into small segments of different resolutions. The format of a DASH playlist file is in XML instead of plaintext like it is with HLS. The specifics of how the DASH manifest tells the player where to find each segment file is a little different. Instead of linking to each segment specifically, the DASH manifest supplies a &#34;SegmentTemplate” value that tells the player how to calculate the specific link for each segment.</p>
<p>Whether using HLS or DASH, the biggest benefit that they both bring to the table is that manifest files and segments are delivered over standard HTTP. Having a streaming format that works over standard HTTP means that all of this content can be served over tried and true HTTP servers and it can be cached of existing CDN infrastructure. Moving all of this video content around is as simple as sending and receiving HTTP requests.</p>
<p><span>
      <span></span>
  <img alt="Adaptive Bitrate" title="Adaptive Bitrate" src="https://mikkel.ca/static/bbdb0c47c08c607fde1fbb92fc921d2c/50e4b/bitrate-streaming.png" srcset="/static/bbdb0c47c08c607fde1fbb92fc921d2c/772e8/bitrate-streaming.png 200w,
/static/bbdb0c47c08c607fde1fbb92fc921d2c/e17e5/bitrate-streaming.png 400w,
/static/bbdb0c47c08c607fde1fbb92fc921d2c/50e4b/bitrate-streaming.png 438w" sizes="(max-width: 438px) 100vw, 438px" loading="lazy"/>
    </span></p>
<h2 id="adaptive-bitrate-streaming"><a href="#adaptive-bitrate-streaming" aria-label="adaptive bitrate streaming permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Adaptive Bitrate Streaming</h2>
<p>For both HLS and DASH, since we are creating all these different renditions of our content, players can adapt to the different renditions in real-time on a segment-by-segment basis.</p>
<p>For example, in the beginning, the player might have a lot of bandwidth, so it starts streaming at the highest resolution available (1080p). Streaming is going smooth for the first 5 minutes. At the end of the first 5 minutes maybe the internet connection starts to suffer and now less bandwidth is available, so the player will degrade to 360p for as long as it needs to. Then, as more bandwidth becomes available again, the player will ratchet back up to higher resolutions.</p>
<p>All of this resolution switching is entirely up to the player. Using the right player can make a huge difference.</p>
<p>Let’s take a real-world example. You open up Netflix on your mobile device and decide to watch The Office for the 11th time in a row. After you scroll around and pick your favorite episode (Season 2 Episode 4) and hit play.</p>
<p>Now the player kicks into gear and you see a red spinner. What is going on behind the scenes while you see the red spinner is that the player is making an HTTP request to Netflix’s servers to determine what resolutions are available for this video. Next, the player will run a bandwidth estimation algorithm to get a sense of how strong your internet connection is. Right now, you are on a good wifi connection so the player will start playing at the highest rendition available for your screen size.</p>
<p>As you are watching this 23-minute episode of The Office the player is working hard in the background to keep up with your streaming. Let’s say you go for a walk and get off the WiFi and now you’re on a cellular network and you don’t have a strong signal. You may notice that at times the video gets a little blurry for a few minutes, then it recovers. Right before the video got blurry the player determined that there was not enough bandwidth available to keep streaming at a high rendition, so it has two options (1) Buffer, meaning pause the video and show a loading spinner and make you wait while it downloads more segments or (2) Degrade to a lower resolution so you can keep watching. A good player will pick number 2: it’s better to give you a lower resolution instead of making you wait.</p>
<p>The player’s goal is always to give you the highest rendition that it can, without making you wait.</p>
<h2 id="mp4--webm"><a href="#mp4--webm" aria-label="mp4  webm permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MP4 &amp; WebM</h2>
<p>MP4 and WebM formats are what we would call pseudo-streaming or &#34;progressive download”. These formats <strong>do not support adaptive bitrate streaming</strong>. If you have ever taken an HTML <code>&lt;video&gt;</code> element and added a &#34;src” attribute that points directly to an mp4 than this is what you are doing. When linking directly to a file most players will progressively download the file. The good thing about progressive downloads is that you don’t have to wait for the player to download the entire file before you start watching. You can click play and start watching while the file is being downloaded in the background. Most players will also allow you to drag the playhead to specific places in the video timeline and the player will use byte-range requests to estimate which part of the file corresponds to the place in the video you are attempting to seek.</p>
<p>What makes MP4 and WebM playback problematic is the lack of adaptive bitrate support. Every user who watches your content must have enough bandwidth available to download the file faster than it can playback the file. When using these formats you constantly have to make a tradeoff between serving a high-resolution file that requires more bandwidth (and thus, locking out users with lower bandwidth) vs. serving a lower resolution file and requires less bandwidth (and thus, unnecessarily lower the quality for the high-bandwidth users). This becomes especially important as we are experiencing a shift as more and more users are streaming video from mobile devices on cellular connections. Cellular connections are notoriously inconsistent and flakey and the way to reliably stream to these devices is through formats that support <strong>adaptive bitrate streaming</strong>.</p>
<h2 id="players"><a href="#players" aria-label="players permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Players</h2>
<p>There&#39;s nearly an infinite number of players on the market to choose from for HLS and DASH. Some are free and open-sourced, some are paid and require a proper license to use. Each player supports different features, for example, captions, DRM, ad injection, and thumbnail previews. When choosing a player you will need to make sure it supports the features you need and allows you to customize the UI elements enough for you to control the look and feel. These are all decisions you will need to make for web-based playback in the browsers, native app playback on iOS and Android or any other operating systems where your content will be streamed.</p></div><div>
<p>For video delivery, the goal always remains the same: deliver the video segment as fast as possible to avoid buffering and ensure an uninterrupted viewing experience.</p>
<p>As we learned in the <a href="https://mikkel.ca/#playback">Playback section</a>, any video player supporting adaptive bitrate (ABR) has the goal to provide the highest quality video without interruption. For this goal to be achieved, the system that stores the video content must deliver it as fast as possible.</p>
<p>Video content can include segmented delivery files such as TS segments (for older HLS delivery), MP4 fragments, or CMAF chunks for DASH and modern HLS. As for the system that’s responsible for delivering that content, there are two primary components: the origin server and the <a href="https://www.stackpath.com/edge-academy/what-is-a-cdn/">content delivery network (CDN)</a>.</p>
<p>As a video developer, the origin is your source of truth. It’s where you upload your original video files and where other system components like the CDN pull files from to help you deliver your video content faster.</p>
<p>You can deliver video content directly from your origin, but it’s not a good idea if your audience is large and dispersed. When this is the case, CDNs help you scale your service to distribute videos to many viewers with higher speed.</p>
<p>Below, we’ll talk about CDNs and other system components that are responsible for the smooth streaming experiences we’ve all come to expect.</p>
<h2 id="content-delivery-networks"><a href="#content-delivery-networks" aria-label="content delivery networks permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Content Delivery Networks</h2>
<p>Before diving into more technical details about CDNs and video streaming, let’s break the CDN concept down into a simple analogy.</p>
<p>A single video server (i.e. origin) responding to numerous requests from streaming devices in multiple regions is like a single cashier responding to numerous requests for purchases from customers in multiple service lines. Just like the cashier experiences stress, so does the video server. And when things come under stress they tend to slow down or shut down completely. The CDN prevents this from happening, especially for video content that is popular among a dispersed audience.</p>
<p><span>
      <span></span>
  <img alt="Cashier Image" title="Cashier Image" src="https://mikkel.ca/static/fec8bfaa4adde15e61859876d6fb2c11/5a190/cashier.png" srcset="/static/fec8bfaa4adde15e61859876d6fb2c11/772e8/cashier.png 200w,
/static/fec8bfaa4adde15e61859876d6fb2c11/e17e5/cashier.png 400w,
/static/fec8bfaa4adde15e61859876d6fb2c11/5a190/cashier.png 800w,
/static/fec8bfaa4adde15e61859876d6fb2c11/c1b63/cashier.png 1200w,
/static/fec8bfaa4adde15e61859876d6fb2c11/29007/cashier.png 1600w,
/static/fec8bfaa4adde15e61859876d6fb2c11/27f8b/cashier.png 1730w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<p>In more technical terms, a CDN is a system of interconnected servers located across the globe that uses geographical proximity as the main criteria for distributing cached content (e.g. segmented video files) to viewers. When a viewer requests content from their device (i.e. clicks a video play button), the request is routed to the closest server in the content delivery network.</p>
<p>If this is the first request for the video segment to that CDN server, the server will forward the request to the origin server where the original file is stored. The origin will respond to the CDN server with the requested file, and, in addition to delivering the file to the viewer, the CDN server will cache (i.e. store) a copy of that file locally. Now, when future viewers request the same file, the origin server is bypassed and the video is served immediately from the local CDN server.</p>
<p><img src="https://mikkel.ca/POP.gif" alt="POP Image"/></p>
<p>Considering that the internet is mostly made up of fiber buried underground and underwater, it makes sense why the location of video servers is important. For example, a viewer in Asia wanting to stream content from an origin server based in the United States will experience poor loading times as this request has to travel across oceans and continents. But with a CDN, the video is almost always delivered from a CDN server in Asia. These locations that store cached video content are known as <a href="https://blog.stackpath.com/point-of-presence/">points of presence (PoPs)</a>.</p>
<p><img src="https://mikkel.ca/populate-to-all.gif" alt="POP animation"/></p>
<p>The business case for improving the viewer experience with CDNs should make sense at this point. By getting video from a local cache, viewers don’t have to wait more than a couple hundred milliseconds for a video they want to watch to start playing. This amount of response time is virtually unnoticeable and creates an experience that incentivizes viewers to stay on the platform where the video is playing.</p>
<p>As for the business case from a cost perspective, delivering video with CDNs significantly reduces bandwidth costs because content doesn’t have to travel as far. Delivering content from local caches requires less stress on networks, and lower costs related to data transmission efficiency are passed on to the video provider. Imagine all the network overhead if requests from all over the world were being served from a single server!</p>
<p>Another business case deals with uptime and reliability. CDNs have traffic capacity that exceeds most normal enterprise network capabilities. Where a self-hosted video may be unavailable due to unexpected traffic peaks, CDNs are more distributed and remain stable during peak traffic instances. For this reason content <em>delivery</em> networks are also referred to as content distribution networks.</p>
<h2 id="how-video-content-travels"><a href="#how-video-content-travels" aria-label="how video content travels permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How Video Content Travels</h2>
<p>When talking about how content travels (and how the Internet works) it’s common to use the terms first mile, middle mile, and last mile.</p>
<p><strong>First mile</strong>: <em>When content travels from the origin to the CDN</em>. For example, video content located on an origin server like Amazon S3 is sent to a CDN like StackPath when it’s requested for the first time by a viewer. This content travels from the origin to the CDN using the Internet backbone as a highway. This backbone is composed of various networks owned by different companies that link up using an agreement called <a href="https://blog.stackpath.com/peering/">peering</a>.</p>
<p><strong>Middle mile</strong>: <em>When content travels from the CDN to the ISP</em>. After the video content is cached in the CDN, it can directly connect to Internet Service Providers (ISPs) that the viewer uses to connect to the Internet with. This direct connection to tier-1 network carriers like Verizon and Comcast allows the CDN to provide enterprise-grade performance and route traffic around network congestion and weather-caused outages that’s common in the public Internet.</p>
<p><strong>Last mile</strong>: <em>When content travels from the ISP to the end user</em>. At this point, the video content is traveling through buried fiber, telephone lines, or cellular towers to make it to the viewer’s device.</p>
<p>As for how these various pieces of the Internet communicate, the most common language is the HTTP protocol. This is defined as a stateless protocol under the GET method, meaning that the content does not change when it travels.</p>
<p>With this in mind, a CDN that had dozens or even hundreds of locations does not need the servers in those locations to communicate with one another to make sure it has the right video content. As long as each server fetches the video content from the origin, every viewer will receive the same video, regardless of their location in the world.</p>
<h2 id="multi-cdn"><a href="#multi-cdn" aria-label="multi cdn permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-CDN</h2>
<p>In a multi-CDN environment, the goal is to distribute load among two or more CDNs. Multi-CDN enables you to direct user requests to the optimal CDN according to your business needs.</p>
<p>In the following figure, there are three CDN providers A, B, and C. CDN provider A is providing an excellent service for users 1 and 2, but CDN provider C offers a better experience for user 3. In a multi-CDN environment, requests by users 1 and 2 can be directed to CDN A and requests by user 3 can be directed to CDN C.</p>
<p><span>
      <span></span>
  <img alt="CDN selection" title="CDN selection" src="https://mikkel.ca/static/cd9279437c418fc46d233252efd3d175/5a190/cdn-selection.png" srcset="/static/cd9279437c418fc46d233252efd3d175/772e8/cdn-selection.png 200w,
/static/cd9279437c418fc46d233252efd3d175/e17e5/cdn-selection.png 400w,
/static/cd9279437c418fc46d233252efd3d175/5a190/cdn-selection.png 800w,
/static/cd9279437c418fc46d233252efd3d175/c1b63/cdn-selection.png 1200w,
/static/cd9279437c418fc46d233252efd3d175/4719e/cdn-selection.png 1330w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<p>Depending on the technique, selecting an optimal CDN could be based on a number of criteria: availability, geographic location, traffic type, capacity, cost, performance, or combinations of the above. Services like NS1 make this type of routing possible.</p>
<p>Multi-CDN is something Mux uses itself. For example, in June 2019, Verizon made an erroneous routing announcement update which channeled a major chunk of Internet traffic through a small ISP in Pennsylvania. This led to significant network congestion and degraded performance for some of the large CDNs in its multi-CDN setup. By using CDN switching, Mux was able to divert most video traffic to StackPath’s CDN and continue to deliver optimal viewing performance during the outage.</p>
<p>Using various content delivery networks, Mux is driving HTTP Live Streaming (HLS) latency down to the lowest levels possible levels, and partnering with the best services at every mile of delivery is crucial in supporting this continued goal.</p></div><div>
<p>Before you can binge watch <em>The Office</em> or <em>Friends</em> over the internet, the media files that were shot and edited into &#34;video&#34; have to go through a major transformation. This post is about the different types of processing that need to occur after a film is shot but before it can be watched online.</p>
<p>However, <strong>preliminary to the dive</strong> into the media processing explanation, it might be necessary to quickly <strong>review the very basic</strong> building blocks of digital video and audio, so that the rest of the information will be easier to understand for those starting to learn about media streaming.</p>
<h2 id="foundational-concepts"><a href="#foundational-concepts" aria-label="foundational concepts permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Foundational concepts</h2>
<h3 id="video"><a href="#video" aria-label="video permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Video</h3>
<p>When the eyeballs see a sequence of changing images, the brain creates the <a href="https://www.youtube.com/watch?v=SGw6VREYkLE">illusion of movement</a>. It&#39;s partially the way that the human visual system works. The fundamental concept of a video exploits this particularity. <strong>A video</strong> can be perceived as <strong>a series of pictures being shown, one after the other, at a given pace.</strong></p>
<p><span>
      <span></span>
  <img alt="Video" title="Video" src="https://mikkel.ca/static/205d636c5ec558fb5a05c241590c70a8/774b6/video.png" srcset="/static/205d636c5ec558fb5a05c241590c70a8/772e8/video.png 200w,
/static/205d636c5ec558fb5a05c241590c70a8/e17e5/video.png 400w,
/static/205d636c5ec558fb5a05c241590c70a8/774b6/video.png 738w" sizes="(max-width: 738px) 100vw, 738px" loading="lazy"/>
    </span></p>
<h3 id="audio"><a href="#audio" aria-label="audio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Audio</h3>
<p>A harder concept to <a href="https://www.youtube.com/watch?v=to_dtcZP1EE">visualize is sound</a>, which is the <strong>vibration that propagates as a wave of pressure</strong>, through the air or any other transmission medium, such as a gas, liquid, or solid. The human brain perceives these waves and produces what we understand as sound.</p>
<p><span>
      <span></span>
  <img alt="Audio" title="Audio" src="https://mikkel.ca/static/afe82cf6404c40051f99d9798f7ed690/17a7a/audio.png" srcset="/static/afe82cf6404c40051f99d9798f7ed690/772e8/audio.png 200w,
/static/afe82cf6404c40051f99d9798f7ed690/e17e5/audio.png 400w,
/static/afe82cf6404c40051f99d9798f7ed690/17a7a/audio.png 753w" sizes="(max-width: 753px) 100vw, 753px" loading="lazy"/>
    </span></p>
<h3 id="digital-video"><a href="#digital-video" aria-label="digital video permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Digital video</h3>
<p>To materialize the idea of a <a href="https://github.com/leandromoreira/digital_video_introduction#intro">digital video file</a>, one might think of grouping <strong>a list of ordered digital pictures</strong> captured at a given frequency, let&#39;s say <strong>30 per second</strong>. Later, when the content will be played, a given device could display all the pictures in sequence, each one in a 33 ms interval (1000 ms / 30 fps), and thus we&#39;ll perceive it as a movie.</p>
<p><span>
      <span></span>
  <img alt="Digital video" title="Digital video" src="https://mikkel.ca/static/ff9e8045d6ee93b2666b55be015bfede/5a190/digital-video.png" srcset="/static/ff9e8045d6ee93b2666b55be015bfede/772e8/digital-video.png 200w,
/static/ff9e8045d6ee93b2666b55be015bfede/e17e5/digital-video.png 400w,
/static/ff9e8045d6ee93b2666b55be015bfede/5a190/digital-video.png 800w,
/static/ff9e8045d6ee93b2666b55be015bfede/c6bbc/digital-video.png 910w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<p>Each <strong>frame is created by an arrangement of</strong> little &#34;dots&#34; known as <strong>pixels</strong>, and each one of them holds color information that can be represented by using the <a href="https://en.wikipedia.org/wiki/Primary_color">primary colors RGB</a>. An easy way to think about it is to visualize a picture as three matrices of numbers, one for each color of RGB. The different colors are obtained by mixing the red, green and blue colors.</p>
<p><span>
      <span></span>
  <img alt="RGB" title="RGB" src="https://mikkel.ca/static/547484a82a48e55146a2b46f2df695c6/a242d/RGB.png" srcset="/static/547484a82a48e55146a2b46f2df695c6/772e8/RGB.png 200w,
/static/547484a82a48e55146a2b46f2df695c6/e17e5/RGB.png 400w,
/static/547484a82a48e55146a2b46f2df695c6/a242d/RGB.png 724w" sizes="(max-width: 724px) 100vw, 724px" loading="lazy"/>
    </span></p>
<h3 id="digital-audio"><a href="#digital-audio" aria-label="digital audio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Digital audio</h3>
<p>For the audio file creation, one needs to somehow find a way <strong>to <a href="https://en.wikipedia.org/wiki/Analog-to-digital_converter">convert the waves</a> into discrete numbers</strong>. In the end, that&#39;s what a computer can store. We could &#34;<strong>sample the waves regularly at uniform intervals</strong>, and each sample is quantized to the nearest value within a range of digital steps.&#34; <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation">Wikipedia</a></p>
<p>Putting into another perspective, let&#39;s say we describe the waves by using 48000 points per second (48Khz), and for each point will use a 2 bytes long number to represent its value. Later we can <a href="https://www.youtube.com/watch?v=RxdFP31QYAg">recreate the physical wave of pressure</a> with <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">minimal loss</a> by reading the data associated with its properties, 48Khz and 16 bits each point.</p>
<p><span>
      <span></span>
  <img alt="Digital Audio" title="Digital Audio" src="https://mikkel.ca/static/6e030488fbe30a802a9ffc006d97c637/c7dcc/digital-audio.png" srcset="/static/6e030488fbe30a802a9ffc006d97c637/772e8/digital-audio.png 200w,
/static/6e030488fbe30a802a9ffc006d97c637/e17e5/digital-audio.png 400w,
/static/6e030488fbe30a802a9ffc006d97c637/c7dcc/digital-audio.png 641w" sizes="(max-width: 641px) 100vw, 641px" loading="lazy"/>
    </span></p>
<blockquote>
<p><a href="https://www.audacityteam.org/">Audacity</a> is an open-source tool aiming to do audio editing but it also has a way to visualize the sound waves for a given audio file. Did you know <a href="https://www.youtube.com/watch?v=VQOdmckqNro">it&#39;s possible to generate audio from a sound wave image graphic</a>?</p>
</blockquote>
<h3 id="codec"><a href="#codec" aria-label="codec permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Codec</h3>
<p>The previous explanation about digital media poses a serious problem. The generated files are huge in terms of storage. Let&#39;s see the numbers in detail. Five seconds of raw 5.1 audio: 6 channels, 48khz, 16 bits uses circa 2.8MB.</p>
<p>  <code>6 channels * 48000 hz * 5s * 2 bytes = 2.8MB</code></p>
<p>The same five seconds of raw video: 1920 x 1080, 24 bits (8 bit R, 8 bits G, and 8 bits for B) per pixel, 60 frames per second will require about 1.7GB of storage.</p>
<p>  <code>1920 * 1080 * 3 bytes * 5s * 60 fps = 1.7GB</code></p>
<p>The <strong>amount of bytes needed</strong> for merely 5 seconds of media is <strong>unacceptable</strong>. Such a big size would require a tremendous bandwidth to watch. To overcome this problem, we <strong>compress the media using codecs</strong>. <strong>A codec is a hardware or software capable</strong> of significantly reducing the required memory for a media.</p>
<p><span>
      <span></span>
  <img alt="Codec" title="Codec" src="https://mikkel.ca/static/49767113acf23feba6422ad25eafedff/5a190/codec.png" srcset="/static/49767113acf23feba6422ad25eafedff/772e8/codec.png 200w,
/static/49767113acf23feba6422ad25eafedff/e17e5/codec.png 400w,
/static/49767113acf23feba6422ad25eafedff/5a190/codec.png 800w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="encoding"><a href="#encoding" aria-label="encoding permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Encoding</h3>
<p>The process of taking an uncompressed media file (usually referred to as “raw”) and converting it to a compressed one is known as <strong>encoding</strong>. By the way, this is usually <strong>the first processing step</strong> that the media is submitted to. Let&#39;s see <strong>how much a codec can compress</strong> the audio and video stream.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Uncompressed size (raw)</th>
<th>Compressed size (encoded)</th>
</tr>
</thead>
<tbody>
<tr>
<td>audio</td>
<td>2.8M</td>
<td>243K (0.243M)</td>
</tr>
<tr>
<td>video</td>
<td>1.7GB</td>
<td>2.0MB (0.002GB)</td>
</tr>
</tbody>
</table>
<p>As we can see, the introduction of <strong>a codec reduces</strong> the file size <strong>about 11 times for the audio and 850 times for the video</strong>, making streaming media a sustainable business. </p>
<blockquote>
<h2 id="there-is-no-free-lunch"><a href="#there-is-no-free-lunch" aria-label="there is no free lunch permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>There is no free lunch</h2>
<p>The almost miraculously encoding process takes <a href="https://github.com/leandromoreira/digital_video_introduction#redundancy-removal">advantage of how our vision works</a>. The codecs usually exploit the temporal and spatial repetition to reduce bits. However it also discards information through a task known as quantization, meaning that there is a loss of quality after the encoding. The codec strength lies in dropping the bits that don&#39;t matter much for our view.</p>
</blockquote>
<p>This compression ratio also introduces an <strong>explicit requirement</strong>. The <strong>device</strong> where the video will play <strong>must know how to decode</strong> the compressed media, which means that the playback system must have a codec (at the least the decoder part) installed into it.</p>
<h3 id="ffmpeg"><a href="#ffmpeg" aria-label="ffmpeg permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FFmpeg</h3>
<p>From now on, we&#39;ll use a program called <a href="https://ffmpeg.org/">FFmpeg</a> to illustrate and prove many of the ideas discussed here in a <strong>practical way</strong>. For instance, how can we be sure that the previous calculations are right? For this task, we can use FFmpeg to compare the compressed vs uncompressed files.</p>
<blockquote>
<p><strong>FFmpeg</strong> is arguably the most <a href="https://www.linkedin.com/pulse/ffmpeg-ubiquitous-leandro-moreira/">ubiquitous open source media program</a> available. It&#39;s a complete solution to read, change, and write back media files.</p>
<p>The command line has a general syntax that looks like <code>ffmpeg {1} {2} -i {3} {4} {5}</code>, where:</p>
<ol>
<li>global options</li>
<li>input file options</li>
<li>input url</li>
<li>output file options</li>
<li>output url</li>
</ol>
<p>Learn more about the <a href="http://slhck.info/ffmpeg-encoding-course/#/21">FFmpeg command line</a>.</p>
</blockquote>
<ol>
<li>Download a sample file for testing, for this task I took the big buck bunny 1080p 60fps video file at <a href="https://peach.blender.org/">https://peach.blender.org/</a></li>
<li>Run the following commands on your terminal.</li>
<li><code>ffmpeg -i bunny_1080p_60fps.mp4 -ss 00:01:24 -t 00:00:05 compressed_h264.mp4</code></li>
</ol>
<p>This command uses the bunny video as the input (<code>-i</code>), it seeks (<code>-ss</code>) to the time 00:01:24, and then takes (<code>-t</code>) 5s generating an output video.</p>
<ol start="4">
<li><code>ffmpeg -i compressed_h264.mp4 -c copy -vn compressed_aac.aac</code></li>
</ol>
<p>This command uses the compressed bunny, it copies (<code>-c copy</code>) the streams but it skips the video streams (<code>-vn</code>), effectively generating an audio only file.</p>
<ol start="5">
<li><code>ffmpeg -i compressed_h264.mp4 -c copy -an compressed_h264.h264</code></li>
</ol>
<p>This command does almost the same as the previous command but it skips the audio (<code>-an</code>) generating a video only file.</p>
<ol start="6">
<li><code>ffmpeg -i compressed_h264.mp4 -f s16le -acodec pcm_s16le raw_6ch_s16b_48khz_audio.raw</code></li>
</ol>
<p>This command takes the same input as before but now it changes its format to an uncompressed audio only using (<code>-f s16le</code>) a signed 16 bits generating a pcm audio (<code>-acodec pcm_s16le</code>).</p>
<ol start="7">
<li><code>ffmpeg -i compressed_h264.mp4 -c:v rawvideo -pix_fmt yuv444p raw_1920p_60fps_444p_video.yuv</code></li>
</ol>
<p>Finally we&#39;re creating a raw (<code>-c:v rawvideo</code>) video using a pixel format (<code>yuv444p</code>) that offers zero compression.</p>
<p><span>
      <span></span>
  <img alt="Figure A" title="Figure A" src="https://mikkel.ca/static/3faadde2948b1aa6adbbd047f463b607/5a190/a.png" srcset="/static/3faadde2948b1aa6adbbd047f463b607/772e8/a.png 200w,
/static/3faadde2948b1aa6adbbd047f463b607/e17e5/a.png 400w,
/static/3faadde2948b1aa6adbbd047f463b607/5a190/a.png 800w,
/static/3faadde2948b1aa6adbbd047f463b607/c1b63/a.png 1200w,
/static/3faadde2948b1aa6adbbd047f463b607/081d5/a.png 1264w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="containers"><a href="#containers" aria-label="containers permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Containers</h3>
<p>Once we have the compressed individual streams of audio and video, we wrap them into a file known as the container. A <a href="https://bitmovin.com/container-formats-fun-1/">container</a> <strong>provides a way to interleave different media data types</strong>, such as video, audio, <a href="https://mux.com/blog/subtitles-captions-webvtt-hls-and-those-magic-flags/">subtitles</a>, etc. </p>
<p>Besides that, it also supplies a <a href="https://ir.cwi.nl/pub/23650/23650B.pdf">timing</a> model, <strong>synchronization</strong>, <strong>metadata</strong>, and other data that will help a device to play the media properly, like audio and video in sync. It&#39;s good to note that all these extra features bring some expected <a href="https://mux.com/blog/quantifying-packaging-overhead-2/">overhead</a>.</p>
<p><span>
      <span></span>
  <img alt="Containers" title="Containers" src="https://mikkel.ca/static/9feff98d8210a33d2d81441ebdf777b7/1d69c/container.png" srcset="/static/9feff98d8210a33d2d81441ebdf777b7/772e8/container.png 200w,
/static/9feff98d8210a33d2d81441ebdf777b7/e17e5/container.png 400w,
/static/9feff98d8210a33d2d81441ebdf777b7/1d69c/container.png 750w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy"/>
    </span></p>
<p>When you read the given definition of a container you might have a <strong>false feeling</strong> that a <strong>container can carry any kind of media data types</strong> (codecs) combination, right?! But that&#39;s not the case at all.</p>
<p>For reasons such as: patents, availability, outdated devices, underpowered devices, device specifics, and transport medium specifics, a plethora of containers coexist and they offer <a href="https://en.wikipedia.org/wiki/Comparison_of_video_container_formats#Video_coding_formats_support">a constrained support</a> for codecs and general features.</p>
<p>There are many containers available, some of them are exclusive for audio, i.e. WAV, but the majority can hold both audio and video streams. You may recognize some of them like MPEG-4 PART 14, Matroska, MPEG transport stream mostly by their file extensions MP4, MKV, TS in order.</p>
<blockquote>
<h2 id="the-confusing-world-of-containers"><a href="#the-confusing-world-of-containers" aria-label="the confusing world of containers permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The confusing world of containers</h2>
<p>Below is a graph showing some containers and their capabilities. The arrows aren&#39;t always an indication of direct heritage. They indicate only that a given container was inspired/followed/improved over the other part.</p>
</blockquote>
<p><span>
      <span></span>
  <img alt="Container Graph" title="Container Graph" src="https://mikkel.ca/static/513d52bae855428873288f14f367e501/1d69c/container-graph.png" srcset="/static/513d52bae855428873288f14f367e501/772e8/container-graph.png 200w,
/static/513d52bae855428873288f14f367e501/e17e5/container-graph.png 400w,
/static/513d52bae855428873288f14f367e501/1d69c/container-graph.png 750w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy"/>
    </span></p>

<p>Now that we know many of the primary foundations of digital media. We can proceed with the discussion about the transformations that video and audio can go through.</p>
<p>Most of the time, we change media components to improve their compatibility. For instance, some devices won&#39;t play a given codec due to patent issues; others can&#39;t handle high-resolution videos due to their limited processing power. In the end, some factors force us to modify digital media to increase its reach.</p>
<p>We also process media to do lots of improvements, for instance, we can validate a media doing volume loudness checking, augment its metadata to find what actress is working on a given tv show, and so on.</p>
<p>Here we&#39;re going to discuss some of the main processing tasks, describe them, understand the logic behind them, and put them to practice.</p>
<h3 id="common-language"><a href="#common-language" aria-label="common language permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Common language</h3>
<p>When you read or <a href="https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA">watch content about digital media</a>, you&#39;re going to see people using <strong>lots of acronyms</strong> and giving <strong>different nouns for the same idea</strong>. You might see ABR meaning adaptive bitrate, but it sometimes will be used for average bitrate.</p>
<p>We&#39;re going to adopt the language and workflow used in <a href="https://github.com/leandromoreira/ffmpeg-libav-tutorial#ffmpeg-libav-architecture">FFmpeg&#39;s internal architecture</a> to unify the terms and avoid further confusion. FFmpeg organizes its components into required phases to provoke any change to a media file.</p>
<p><span>
      <span></span>
  <img alt="Common language" title="Common language" src="https://mikkel.ca/static/0020951775d05a7197087803306d55c7/1d69c/common-language.png" srcset="/static/0020951775d05a7197087803306d55c7/772e8/common-language.png 200w,
/static/0020951775d05a7197087803306d55c7/e17e5/common-language.png 400w,
/static/0020951775d05a7197087803306d55c7/1d69c/common-language.png 750w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy"/>
    </span></p>
<p>Whether you&#39;re reading or writing a media you:</p>
<ol>
<li>start by reading/writing it through/to a <strong>protocol</strong> (e.g. file, udp, rtmp, http, etc), </li>
<li>and then you can read/write (<strong>demux/mux</strong>) the <strong>container</strong> (e.g. mp4, webm, mkv, ts, etc),</li>
<li>from here you can read/write (<strong>decode/encode</strong>) the <strong>codecs</strong>, the container usually contains audio and video, you may have to do this step multiple times (e.g. h264, av1, aac, etc),</li>
<li>finally exposing their <strong>raw format</strong> (e.g. rgb, yuv 420, pcm, etc ) for the filtering phase, where a lot of cool processing can be done.</li>
</ol>
<p>If you only need to modify the container, <strong>you can skip</strong> the other <strong>steps</strong>. Assuming that you want to work with the raw frame, you must go through all the phases. Another compelling aspect of this architecture is that it treats audio and video likewise.</p>
<h3 id="transcoding"><a href="#transcoding" aria-label="transcoding permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transcoding</h3>
<h4 id="what"><a href="#what" aria-label="what permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What?</h4>
<p>Transcoding is <strong>the process of converting a media from codec A to B</strong>, but we&#39;ll also treat the terms encoding (from raw to compressed) and re-encoding (encoding to the same codec changing its properties) as if they were the same process.</p>
<p>Some of the current most important codecs are <a href="https://en.wikipedia.org/wiki/Advanced_Video_Coding">h264 (avc)</a>, <a href="https://www.webmproject.org/vp9/">vp9</a>, h265 (hevc), <a href="http://aomedia.org/av1-features/get-started/">av1</a>, aac, <a href="https://opus-codec.org/">opus</a>, Vorbis, and ac3.</p>
<p><span>
      <span></span>
  <img alt="Transcoding" title="Transcoding" src="https://mikkel.ca/static/e96c58dd6e825243f5c20e378628132c/1d69c/transcoding.png" srcset="/static/e96c58dd6e825243f5c20e378628132c/772e8/transcoding.png 200w,
/static/e96c58dd6e825243f5c20e378628132c/e17e5/transcoding.png 400w,
/static/e96c58dd6e825243f5c20e378628132c/1d69c/transcoding.png 750w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy"/>
    </span></p>
<p>An example of a media being transcoded from H264 to AV1.</p>
<h4 id="why"><a href="#why" aria-label="why permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why?</h4>
<p>The codec conversion is mostly necessary due to:</p>
<ul>
<li><strong>compatibility</strong>: outdated hardware or software, some devices might require specific codecs</li>
<li><strong>economics</strong>: avoid patent costs, saving on storage and bandwidth</li>
<li><strong>enhancements</strong>: better compression ratio, higher resolutions, better framerate, new features (such as HDR, 3D sound)</li>
</ul>
<h4 id="how"><a href="#how" aria-label="how permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How?</h4>
<p>This FFmpeg command takes an input (-i) and converts its video codec (-c:v) to av1 (using the libaom-av1 library) and don&#39;t worry if you don&#39;t understand all the options here. The idea is to transcode a video from h264 to av1.</p>
<p><code>ffmpeg -i compressed_h264.mp4 -c:v libaom-av1 -crf 30 -b:v 2000k -cpu-used 8 -row-mt 1 compressed_av1.mp4</code></p>
<p>Transcoding is an expensive operation in terms of video duration versus time to transcode. On a 2018 Mac (i5, 8GB), it took about 2 minutes to transcode 5 seconds of video.</p>
<p><span>
      <span></span>
  <img alt="Figure B" title="Figure B" src="https://mikkel.ca/static/c7a3179de4c01853b0988a628ef95cdd/5a190/b.png" srcset="/static/c7a3179de4c01853b0988a628ef95cdd/772e8/b.png 200w,
/static/c7a3179de4c01853b0988a628ef95cdd/e17e5/b.png 400w,
/static/c7a3179de4c01853b0988a628ef95cdd/5a190/b.png 800w,
/static/c7a3179de4c01853b0988a628ef95cdd/97bfd/b.png 1078w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="transrating"><a href="#transrating" aria-label="transrating permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transrating</h3>
<h4 id="what-1"><a href="#what-1" aria-label="what 1 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What?</h4>
<p>Transrating is <strong>the process of changing the bitrate</strong> (amount of bytes required per unit of time) of a media, aiming to <strong>shrink its file size</strong>. Mostly three aspects guide this transformation: quality, speed, and space. </p>
<p>Quality is the measure of how good the final media looks to our visual judgment, speed is about time to do the process utilizing the CPU/GPU, and space is the required volume of memory by the media.</p>
<p><span>
      <span></span>
  <img alt="Transrating" title="Transrating" src="https://mikkel.ca/static/67df247fdc5594fa133e9acb4b9748d9/ef9e5/transrating.png" srcset="/static/67df247fdc5594fa133e9acb4b9748d9/772e8/transrating.png 200w,
/static/67df247fdc5594fa133e9acb4b9748d9/e17e5/transrating.png 400w,
/static/67df247fdc5594fa133e9acb4b9748d9/ef9e5/transrating.png 607w" sizes="(max-width: 607px) 100vw, 607px" loading="lazy"/>
    </span></p>
<p>We usually need to favor two of these features at a given time. It means that if we want quality media and fewer bytes, we&#39;ll need to give it more CPU time. Frequently what we do is partially compromise some of these features based on context.</p>
<h4 id="why-1"><a href="#why-1" aria-label="why 1 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why?</h4>
<p>The change of bitrate is mostly necessary due to:</p>
<ul>
<li><strong>compatibility</strong>: works on devices where the network quality is poor or low bandwidth</li>
<li><strong>economics</strong>: saving on storage and bandwidth</li>
</ul>
<h4 id="how-1"><a href="#how-1" aria-label="how 1 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How?</h4>
<p>To change the bitrate we first need to <a href="https://slhck.info/video/2017/03/01/rate-control.html">select a proper rate control</a>. The rate control manages how the encoder spends the bits during the compression phase.</p>
<p>The following FFmpeg command takes an input (-i), keeps the audio untouched (-c:a copy), but it changes the video bitrate, suggesting a peak value (-maxrate) and maintaining the quality (-crf) up to a certain level. For h264, the crf ranges from 0 to 51, where 23 is a good middle value.</p>
<p><code>ffmpeg -i compressed_h264.mp4 -c:a copy -c:v libx264 -crf 23 -maxrate 1M -bufsize 2M compressed_h264_1M.mp4</code></p>
<p>As you can observe, we changed the bit rate from 3.4Mb/s to 1.1Mb/s.</p>
<p><span>
      <span></span>
  <img alt="Figure C" title="Figure C" src="https://mikkel.ca/static/4fe1b0d81d0f04f1ab37f992d14df40d/2e237/c.png" srcset="/static/4fe1b0d81d0f04f1ab37f992d14df40d/772e8/c.png 200w,
/static/4fe1b0d81d0f04f1ab37f992d14df40d/e17e5/c.png 400w,
/static/4fe1b0d81d0f04f1ab37f992d14df40d/2e237/c.png 790w" sizes="(max-width: 790px) 100vw, 790px" loading="lazy"/>
    </span></p>
<h3 id="transsizing"><a href="#transsizing" aria-label="transsizing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transsizing</h3>
<h4 id="what-2"><a href="#what-2" aria-label="what 2 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What?</h4>
<p>Transsizing <strong>is the act of changing the resolution</strong> of a media, scaling up or down. For instance, one can create a lower resolution version from a higher resolution video.</p>
<p><span>
      <span></span>
  <img alt="Transsizing" title="Transsizing" src="https://mikkel.ca/static/7cc0981df29417b55fe7725a545ea8e9/5a190/transizing.png" srcset="/static/7cc0981df29417b55fe7725a545ea8e9/772e8/transizing.png 200w,
/static/7cc0981df29417b55fe7725a545ea8e9/e17e5/transizing.png 400w,
/static/7cc0981df29417b55fe7725a545ea8e9/5a190/transizing.png 800w,
/static/7cc0981df29417b55fe7725a545ea8e9/82b28/transizing.png 931w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h4 id="why-2"><a href="#why-2" aria-label="why 2 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why?</h4>
<p>The change of resolution is mostly necessary due to:</p>
<ul>
<li><strong>compatibility</strong>: works on devices where the network quality is poor or low bandwidth, a small resolution requires less data, older devices might not be able to handle high-resolution version of a given media</li>
<li><strong>economics</strong>: saving on storage and bandwidth</li>
</ul>
<h4 id="how-2"><a href="#how-2" aria-label="how 2 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How?</h4>
<p>To change the <a href="https://en.wikipedia.org/wiki/Display_resolution">resolution</a>, we need to re-encode the media. The activity of modifying the resolution is also known as upscaling when you&#39;re converting a smaller video to a bigger one, and downscaling on the contrary.</p>
<p>The following FFmpeg command takes an input (-i), keeps the audio untouched (-c:a copy), changes the video bitrate, and also changes the resolution keeping the aspect of the video (-vf scale=384:-1).</p>
<p><code>ffmpeg -i compressed_h264.mp4 -c:a copy -c:v libx264 -crf 23 -maxrate 500k -bufsize 1M -vf scale=384:-1 compressed_h264_384x216_500k.mp4</code></p>
<p>We <a href="https://ffmpeg.org/ffmpeg-scaler.html#scaler_005foptions">scaled down our media</a> to 216p, and its size also went down significantly from 2.3MB to 0.4MB.</p>
<p><span>
      <span></span>
  <img alt="Figure D" title="Figure D" src="https://mikkel.ca/static/7fefc9e416f46bc81fa998ae24cb040b/5a190/d.png" srcset="/static/7fefc9e416f46bc81fa998ae24cb040b/772e8/d.png 200w,
/static/7fefc9e416f46bc81fa998ae24cb040b/e17e5/d.png 400w,
/static/7fefc9e416f46bc81fa998ae24cb040b/5a190/d.png 800w,
/static/7fefc9e416f46bc81fa998ae24cb040b/c1b63/d.png 1200w,
/static/7fefc9e416f46bc81fa998ae24cb040b/01a87/d.png 1288w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="transmuxing"><a href="#transmuxing" aria-label="transmuxing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transmuxing</h3>
<h4 id="what-3"><a href="#what-3" aria-label="what 3 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What?</h4>
<p>Transmuxing <strong>is the process of altering a media format container</strong>. For instance, one can change an mp4 file to MPEG-ts keeping the audio and video streams intact.</p>
<p><span>
      <span></span>
  <img alt="Transmuxing" title="Transmuxing" src="https://mikkel.ca/static/62a2c3bfb7f3c29c8eee352cd5751cb4/1d69c/transmuxing.png" srcset="/static/62a2c3bfb7f3c29c8eee352cd5751cb4/772e8/transmuxing.png 200w,
/static/62a2c3bfb7f3c29c8eee352cd5751cb4/e17e5/transmuxing.png 400w,
/static/62a2c3bfb7f3c29c8eee352cd5751cb4/1d69c/transmuxing.png 750w" sizes="(max-width: 750px) 100vw, 750px" loading="lazy"/>
    </span></p>
<h4 id="why-3"><a href="#why-3" aria-label="why 3 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why?</h4>
<p>The container conversion is mostly necessary due to:</p>
<ul>
<li><strong>compatibility</strong>: outdated hardware or software, some devices might require a specific container</li>
<li><strong>economics</strong>: avoid patent costs, saving on storage and bandwidth</li>
</ul>
<h4 id="how-3"><a href="#how-3" aria-label="how 3 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How?</h4>
<p>Transmuxing is a very cheap operation, compared to the main processing tasks. The following FFmpeg command takes an input (-i), keeps the audio and video untouched (-c copy), and it only changes the container hinting its desired container by the output extension.</p>
<p><code>ffmpeg -i compressed_h264.mp4 -c copy compressed_h264.ts</code></p>
<p>We can notice that there was <a href="https://mux.com/blog/quantifying-packaging-overhead-2/">a small increase in size coming from an mp4 file to a MPEG-ts</a> one.</p>
<p><span>
      <span></span>
  <img alt="Figure E" title="Figure E" src="https://mikkel.ca/static/a1c2293beecba29ebcc6b0c56a5f5048/5a190/e.png" srcset="/static/a1c2293beecba29ebcc6b0c56a5f5048/772e8/e.png 200w,
/static/a1c2293beecba29ebcc6b0c56a5f5048/e17e5/e.png 400w,
/static/a1c2293beecba29ebcc6b0c56a5f5048/5a190/e.png 800w,
/static/a1c2293beecba29ebcc6b0c56a5f5048/6937a/e.png 1094w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="packaging"><a href="#packaging" aria-label="packaging permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Packaging</h3>
<h4 id="what-4"><a href="#what-4" aria-label="what 4 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What?</h4>
<p>A modern <strong>player must</strong> know how to deal with <strong>different hardware and context situations</strong> while playing the media content. A part of these devices are underpowered, therefore can&#39;t handle complex video or audio related tasks. Sometimes the <strong>network bandwidth will vary</strong> thus making the user wait for the content to be downloaded.</p>
<p>To overcome these issues, the streaming industry created what we call <a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">adaptive bitrate streaming</a>. The idea behind this is to <strong>segment a whole media file into</strong> two distinct dimensions: time and resolution. You take an hour-long video and splice it into <strong>several X seconds segments</strong>, and you also <strong>create multiple resolutions</strong> (e.g. 256x144, 640x360, 1280x720, etc), also known as renditions.</p>
<p>When the <strong>device is not able</strong> to handle a media, due to its current context, <strong>it can dynamically adapt</strong>. For instance, while the network is facing a hard time to download a high resolution content, the player can pick a lower resolution of a video.</p>
<p>But now we have another problem, <strong>how will the player acknowledge this protocol</strong>? How many renditions do we own? or how many segments does the media have? To answer these questions <strong>a text file format</strong> is introduced, known as a <strong>manifest</strong>, which contains all the information for the player to make decisions.</p>
<p>The whole process of <strong>creating multiple renditions, segmenting media by time, and creating a manifest</strong> describing this scheme is known as <strong>packaging</strong>. There are two major packaging formats known, <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS</a> which was created by Apple and <a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP">MPEG-dash</a> created by MPEG.</p>
<p><span>
      <span></span>
  <img alt="Packaging" title="Packaging" src="https://mikkel.ca/static/322b01a32c6c68030257f115aef6ae97/5a190/packaging.png" srcset="/static/322b01a32c6c68030257f115aef6ae97/772e8/packaging.png 200w,
/static/322b01a32c6c68030257f115aef6ae97/e17e5/packaging.png 400w,
/static/322b01a32c6c68030257f115aef6ae97/5a190/packaging.png 800w,
/static/322b01a32c6c68030257f115aef6ae97/4ff83/packaging.png 843w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h4 id="why-4"><a href="#why-4" aria-label="why 4 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why?</h4>
<p>The packaging format is mostly necessary due to:</p>
<ul>
<li><strong>compatibility</strong>: outdated hardware or software, some devices might require specific packaging format</li>
<li><strong>economics</strong>: avoid patent costs, use known scalable/cacheable HTTP servers solution to distribute the content</li>
<li><strong>enhancements</strong>: adaptive smooth playback even when the network is fluctuating or the device is doing lots of processing</li>
</ul>
<h4 id="how-4"><a href="#how-4" aria-label="how 4 permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How?</h4>
<p>One way to understand how packaging works is by doing it by hand, we&#39;re going to take an mp4 file and generate multiple renditions, an m3u8 manifest, and lots of MPEG-ts segments. </p>
<blockquote>
<p>Architectural trivia: FFmpeg seemed to <a href="https://ffmpeg.org/ffmpeg-protocols.html#hls">start treating HLS as a protocol</a> but later they changed to <a href="https://ffmpeg.org/ffmpeg-formats.html#hls-1">handle it as a container</a> in its workflow.</p>
</blockquote>
<p>The following large FFmpeg command takes an input, re-encode it into <strong>two videos</strong>, one for <strong>1080p</strong> and other for the <strong>720p</strong> rendition. Both videos will include the same audio track. We then instruct FFmpeg to <strong>segment the video into files of 1s</strong> (-hls-time 1) and create a <strong>manifest</strong> named master.m3u8.</p>
<pre><code>ffmpeg -hide_banner \
  -i compressed_h264.mp4 \
  -c:v libx264 -preset ultrafast -tune zerolatency -profile:v high \
  -b:v:0 2500k -bufsize 5m -x264opts keyint=60:min-keyint=60:scenecut=-1 \
  -b:v:1 1500k -s:v:0 1280:720 \
  -bufsize 3m -x264opts keyint=60:min-keyint=60:scenecut=-1 \
  -map 0:v:0 -map 0:v:0 \
  -c:a libfdk_aac -profile:a aac_low -b:a 64k \
  -map 0:a:0 \
  -f hls -hls_time 1 -var_stream_map &#34;v:0,a:0 v:1,a:0&#34; \
  -master_pl_name master.m3u8 \
  -hls_segment_filename &#39;vs%v/file_%03d.ts&#39; vs%v/out.m3u8
</code></pre>
<p>You can see the manifests and segments created by the FFmpeg packaging process.</p>
<p><span>
      <span></span>
  <img alt="Figure F" title="Figure F" src="https://mikkel.ca/static/c5124fd95e45ff72036181efc0a66ba7/5a190/f.png" srcset="/static/c5124fd95e45ff72036181efc0a66ba7/772e8/f.png 200w,
/static/c5124fd95e45ff72036181efc0a66ba7/e17e5/f.png 400w,
/static/c5124fd95e45ff72036181efc0a66ba7/5a190/f.png 800w,
/static/c5124fd95e45ff72036181efc0a66ba7/c1b63/f.png 1200w,
/static/c5124fd95e45ff72036181efc0a66ba7/8d68c/f.png 1234w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h3 id="going-beyond-processing"><a href="#going-beyond-processing" aria-label="going beyond processing permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Going beyond processing</h3>
<p>Up until now, we discussed essentially the necessary tasks over the media to make it available for the majority of users, however <strong>other types of processing</strong> will offer analytical, monitoring, or even expand the common user experience.</p>
<h4 id="monitoring"><a href="#monitoring" aria-label="monitoring permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Monitoring</h4>
<p>You can do some kind of processing over the raw frames to detect black or muted scenes, denormalized audio, etc. The idea is to <strong>constantly check if the media output is conforming</strong> to some kind of expected behavior that we look for.</p>
<p>You can use a filter on FFmpeg called <a href="https://ffmpeg.org/ffmpeg-filters.html#blackdetect">blackdetect</a> to find out if your media has a black video of <code>d</code> duration. Noticed that some chapter transition inserts intentionally black video.</p>
<p><code>ffmpeg -i media.mp4 -vf blackdetect=d=1 -f null</code></p>
<p><span>
      <span></span>
  <img alt="Figure G" title="Figure G" src="https://mikkel.ca/static/b3924c088493634f0026f4e5e8fa4869/5a190/g.png" srcset="/static/b3924c088493634f0026f4e5e8fa4869/772e8/g.png 200w,
/static/b3924c088493634f0026f4e5e8fa4869/e17e5/g.png 400w,
/static/b3924c088493634f0026f4e5e8fa4869/5a190/g.png 800w,
/static/b3924c088493634f0026f4e5e8fa4869/c1b63/g.png 1200w,
/static/b3924c088493634f0026f4e5e8fa4869/01a87/g.png 1288w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<p>You could also use the <a href="https://ffmpeg.org/ffmpeg-filters.html#silencedetect">silencedetect</a> filter to discover where there&#39;s a silence of a d duration.</p>
<p><code>ffmpeg -i media.mp4 -af silencedetect=d=2 -f null -</code></p>
<p><span>
      <span></span>
  <img alt="Figure H" title="Figure H" src="https://mikkel.ca/static/31aa89f4faad812b4ad7d666c388a2a0/5a190/h.png" srcset="/static/31aa89f4faad812b4ad7d666c388a2a0/772e8/h.png 200w,
/static/31aa89f4faad812b4ad7d666c388a2a0/e17e5/h.png 400w,
/static/31aa89f4faad812b4ad7d666c388a2a0/5a190/h.png 800w,
/static/31aa89f4faad812b4ad7d666c388a2a0/1132d/h.png 1158w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<p>This kind of processing can happen before or after the media was processed, and it tries to point out possible issues that can be automatically checked by a computer.</p>
<h4 id="analytical"><a href="#analytical" aria-label="analytical permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Analytical</h4>
<p>When we&#39;re <strong>defining the multiple renditions and bitrates</strong> for media packaging, we need to specify the values we desire. We could <strong>prescribe a set of fixed resolutions and bitrates</strong> for all content, but <strong>a soccer game differs a lot from an anime</strong> in terms of bytes required. A soccer match has more movements and details while an anime has fewer characteristics and consistently fewer colors.</p>
<p>One solution to this dilemma is to create the <strong>encoding parameters</strong> testing values and putting real <a href="https://en.wikipedia.org/wiki/Mean_opinion_score">people to evaluate the visual quality</a> per bitrate values. Once we have these grades, we can pick the most adequate (the best quality storage ratio) bitrate for a given title.</p>
<p>But such a task requires lots of time and people. Alternatively, there are some tools like <a href="https://netflixtechblog.com/toward-a-practical-perceptual-video-quality-metric-653f208b9652">VMAF</a> <strong>where we can check</strong>, given two videos, what&#39;s the <strong>quality difference between them</strong>. Now, we can do the same method to find the best effective bitrate and quality parameters, but using an <strong>automated tool</strong>.</p>
<p>This FFmpeg command line compares two videos using VMAF and shows a score that ranges from 0 to 100, where 100 means identical videos.</p>
<pre><code>ffmpeg -i compressed_h264.mp4 -i compressed_h264_1M.mp4 \
  -lavfi libvmaf=log_fmt=json -f null -
</code></pre>
<p>A possible usage would be to understand how much quality we lose with a transrating operation. Comparing our initial 3.4Mb/s bitrate with a 1Mb/s version of it, we got an 81 <a href="https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12">VMAF score</a>. We could set a cutoff and try different encoding parameters to achieve the best ratio quality vs storage.</p>
<p><span>
      <span></span>
  <img alt="Figure I" title="Figure I" src="https://mikkel.ca/static/31146e7caf0300674207f1df992e2fd9/5a190/i.png" srcset="/static/31146e7caf0300674207f1df992e2fd9/772e8/i.png 200w,
/static/31146e7caf0300674207f1df992e2fd9/e17e5/i.png 400w,
/static/31146e7caf0300674207f1df992e2fd9/5a190/i.png 800w,
/static/31146e7caf0300674207f1df992e2fd9/ea64c/i.png 1116w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy"/>
    </span></p>
<h4 id="enhancements"><a href="#enhancements" aria-label="enhancements permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enhancements</h4>
<p>We can also augment the user experience by extracting data from the media that can be obtained after some kind of processing, things like:</p>
<ul>
<li>automatically converting speech to closed caption</li>
<li>creating thumbnails facilitating the scene navigation on the player</li>
<li><a href="https://github.com/akai-katto/dandere2x">scaling up videos to super resolution</a> using AI</li>
<li>enriching metadata by identifying characters, objects, time, places, etc.</li>
<li>finding the intro/credits pattern so we can add cue point data to the media, enabling the users to skip the intro/credits </li>
<li>selecting good places in the timeline to insert ads that aren&#39;t so obtuse, like a long scene change, long black transition, etc.</li>
</ul>
<p>The opportunities are endless and with the ML/IA landscape being more explored, people are getting more and more creative about what we can do with media.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="conclusion permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>In a real-world media processing deployment, a lot of video processing happens in a pipeline. The media is ingested, validated, transcoded, packaged, <a href="#delivery">hosted</a> and finally <a href="#playback">played</a>. Most of these processing steps are mandatory to make the streaming available for the final users.</p>
<p>Beyond the operations we do to improve the compatibility, there are far more complex tasks we perform to improve the user experience or even to increase profits. Processing digital media demands time and hardware, and we need to balance what&#39;s worth the time and money.</p>
<p>We’re only just scratching the surface of digital media processing, but I hope this post helps you to start on this exciting journey.</p></div></div>
  </body>
</html>
