<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.google/products/gemini/how-we-built-gemini-robotics/">Original</a>
    <h1>How Google built its Gemini robotics models</h1>
    
    <div id="readability-page-1" class="page"><article>

    
    


<section>
  
</section>


    

    
      

<div data-analytics-module="{
    &#34;module_name&#34;: &#34;Hero Menu&#34;,
    &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
  }">
  
  <div>
    <div>
      <div>
        <div>
          
            <p>Apr 01, 2025</p>
          
          
            <p><span aria-hidden="true">·</span></p><p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
        




      </div>
      
        <p>
          Powered by Gemini Robotics models, robots can learn complex actions like preparing salads, playing games like Tic-Tac-Toe and even folding an origami fox.
        </p>
      
    </div>
  </div>
  
  <div>
    <div>
      
        


  
  
    <div>
      
  

<div>
  <p>Joel Meares</p>
  
    <p>
      Contributor, The Keyword
    </p>
  
  
</div>

    </div>
  


      

      
      
    </div>
    
      
    
    
  </div>
</div>

    

    
      










<div>
  <div>
    <figure>
      <div>
        <p><img alt="A still of a white and black robot in a kitchen packing a blue lunchbox" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-HeroImage_2096x1182.width-2200.format-webp.webp 2200w"/>
        </p>
      </div>
      
    </figure>
  </div>
</div>






    

    
    <section>
      <div>
        
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="How we built the new family of Gemini Robotics models" listen-to-article="Listen to article" data-date-modified="2025-04-01T21:25:21.117181+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="0u55q">As Google DeepMind prepared for <a href="https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/">its recent announcement</a> of a new family of Gemini 2.0 models designed specifically for robots, its head of robotics, Carolina Parada, gathered her team for another check of the tech’s capabilities.</p><p data-block-key="752j6">They asked a bi-arm ALOHA robot — a duo of limber metal appendages with multiple joints and pincer-like hands used widely in research — to perform tasks it hadn’t done before, using objects it hadn’t seen. “We did random things like put my shoe on the table and ask it to put some pens inside,” Carolina says. “The robot took a moment to understand the task, then did it.”</p><p data-block-key="a5acc">For the next request, they found a toy basketball hoop and ball and asked the robot to do a “slam dunk.” Carolina watched, proud and delighted, as it did just that.</p></div>
      </div>
    </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="A GIF shows a black robot arm picking up a small orange ball and placing it into a miniature toy basketball hoop. A prompt, Pick up the basketball and slam dunk it, is written out at the bottom of the GIF." external-image="" or-mp4-video-title="Robot dunk" or-mp4-video-url="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Robotics-Inline-RobotDunk.mp4" section-header="How we built the new family of Gemini Robotics models" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="uv3a7">Carolina says witnessing the slam dunk was a “wow” moment.</p>
    </div>
  
  
</uni-image-full-width>


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="0u55q">“We’d trained models to help robots with specific tasks and to understand natural language before, but this was a step change,” Carolina says. “The robot had never seen anything related to basketball, or this specific toy. Yet it understood something complex — ‘slam dunk the ball’ — and performed the action smoothly. <i>On its first try.</i>”</p><p data-block-key="e1ol8">This all-rounder robot was powered by a <a href="https://deepmind.google/technologies/gemini-robotics/">Gemini Robotics</a> model that is part of a new family of multimodal models for robotics. The models build upon <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">Gemini 2.0</a> through fine-tuning with robot-specific data, adding physical action to Gemini’s multimodal outputs like text, video and audio. &#34;This milestone lays the foundation for the next generation of robotics that can be helpful across a range of applications,&#34; said Google CEO Sundar Pichai when <a href="https://x.com/sundarpichai/status/1899838913054744679">announcing the new models on X</a>.</p><p data-block-key="46l5k">The Gemini Robotics models are highly dextrous, interactive and general, meaning they can drive robots to react to new objects, environments and instructions without further training. Helpful, given the team’s ambitions.</p><p data-block-key="ev3sg">“Our mission is to build embodied AI to power robots that help you with everyday tasks in the real world,” says Carolina, whose fascination with robotics began with childhood sci-fi cartoons, fueled by dreams of automated chores. “Eventually, robots will be just another surface on which we interact with AI, like our phones or computers — agents in the physical world.”</p></div>
      </div>
    </div>
  

  
    
  
    


  <uni-youtube-player-article index="4" thumbnail-alt="A humanoid robot stands opposite a person looking at their laptop. In white text, the words Gemini 2.0 + Robotics are overlaid." video-id="4MvGnmmP3c0" video-type="video" image="Robotics-Inline1" video-image-url-lazy="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-100.format-webp.webp" video-image-url-mobile="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-700.format-webp.webp" video-image-url-desktop="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline1.width-1000.format-webp.webp">
  </uni-youtube-player-article>


  


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="0u55q">Like people, robots need two main functions to perform tasks effectively and safely: the ability to understand and make decisions, and the ability to take action. Gemini Robotics-ER, an &#34;embodied reasoning” model built on Gemini 2.0 Flash, focuses on the former, recognizing elements in front of it, defining their size and location, and predicting the trajectory and grip required to move them. It then can generate code to execute the action. We’re now making this model available to trusted testers and partners.</p><p data-block-key="e4isc">Google DeepMind is also introducing Gemini Robotics, its most advanced vision-language-action model, which allows robots to reason about a scene, interact with the user and take action. Crucially, it makes significant advances in an area that has proved tricky for roboticists: dexterity. “What comes naturally to humans is difficult for robots,” Carolina explains. “Dexterity requires both spatial reasoning, and complex physical manipulation. Across testing, Gemini Robotics has set a new state-of-the-art for dexterity, solving complex multi-step tasks with smooth motions and great completion times.”</p></div>
      </div>
    </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="This is a collage of visualizations showcasing these capabilities. Top left: 2D object detection, top right: pointing, bottom left: multi-view correspondence, bottom right: 3d object detection." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="How we built the new family of Gemini Robotics models" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="uv3a7">Gemini Robotics-ER excels at embodied reasoning capabilities, including detecting objects and pointing at object parts, finding corresponding points and detecting objects in 3D.</p>
    </div>
  
  
    <p><img alt="This is a collage of visualizations showcasing these capabilities. Top left: 2D object detection, top right: pointing, bottom left: multi-view correspondence, bottom right: 3d object detection." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-100.format-webp.webp" loading="lazy" data-loading="{
            &#34;mobile&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-500.format-webp.webp&#34;,
            &#34;desktop&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline2.width-1000.format-webp.webp&#34;
          }"/>
    </p>
  
</uni-image-full-width>


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="0u55q">Powered by Gemini Robotics, machines have prepared salads, packed kids’ lunches, played games like Tic-Tac-Toe and even folded an origami fox.</p><p data-block-key="9067r">Preparing models that could do many different kinds of tasks was a challenge — largely because it went against the general industry practice of training models for a <i>single</i> task over and over until it can be solved. “Instead, we chose broad task learning, training models on a huge number of tasks,” Carolina says. “We expected to see generalization emerge after a certain amount of time, and we were right.”</p><p data-block-key="284af">Both models can adapt to multiple embodiments, including academic-focused robots, like the bi-arm ALOHA machine, or humanoid robots like Apollo developed by our partner Apptronik.</p></div>
      </div>
    </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Four images of robots performing actions. In the top left, a humanoid robot is packing a lunch, in the top right a small arm can be seen picking up a snap pea from a tupperware container, in the bottom left two large white arms ready for a task on a bench, and in the bottom right a black pincer hand holds a whiteboard eraser atop a whiteboard." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="How we built the new family of Gemini Robotics models" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="uv3a7">The models adapt to different embodiments, able to perform tasks like packing a lunchbox or wiping a whiteboard in different forms.</p>
    </div>
  
  
    <p><img alt="Four images of robots performing actions. In the top left, a humanoid robot is packing a lunch, in the top right a small arm can be seen picking up a snap pea from a tupperware container, in the bottom left two large white arms ready for a task on a bench, and in the bottom right a black pincer hand holds a whiteboard eraser atop a whiteboard." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-100.format-webp.webp" loading="lazy" data-loading="{
            &#34;mobile&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-500.format-webp.webp&#34;,
            &#34;desktop&#34;: &#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Robotics-Inline3.width-1000.format-webp.webp&#34;
          }"/>
    </p>
  
</uni-image-full-width>


  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;How we built the new family of Gemini Robotics models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="0u55q">This ability to adapt is key to a future where robots could take on a number of very different roles.</p><p data-block-key="7tp33">“The possibilities for robots using highly general and capable models are broad and exciting,” Carolina says. “They could be more useful in industries where setups are complex, precision is important and the spaces aren’t human-friendly. And they could be helpful in human-centric spaces, like the home. That’s some years away, but these models are taking us several steps closer.”</p><p data-block-key="6ofc8">Sounds like someone will get some help with those chores — eventually.</p></div>
      </div>
    </div>
  


            
            

            
              




            
          </div>
        
      </div>
    </section>
  </article></div>
  </body>
</html>
