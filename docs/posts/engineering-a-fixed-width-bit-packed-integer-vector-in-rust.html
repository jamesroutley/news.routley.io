<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lukefleed.xyz/posts/compressed-fixedvec/">Original</a>
    <h1>Engineering a fixed-width bit-packed integer vector in Rust</h1>
    
    <div id="readability-page-1" class="page"><article id="article" role="article">
      <p>If you’ve ever worked with massive datasets, you know that memory usage can quickly become a bottleneck. While developing succinct data structures, I found myself needing to store large arrays of integers—values with no monotonicity or other exploitable patterns, that I knew came from a universe much smaller than their type’s theoretical capacity.</p>
<p>In this post, we will explore the engineering challenges involved in implementing an efficient vector-like data structure in Rust that stores integers in a compressed, bit-packed format. We will focus on achieving O(1) random access performance while minimizing memory usage. We will try to mimic the ergonomics of Rust’s standard <code>Vec&lt;T&gt;</code> as closely as possible, including support for mutable access and zero-copy slicing.</p>
<ul>
<li>All the code can be found on github: <a href="https://github.com/lukefleed/compressed-intvec">compressed-intvec</a></li>
<li>This is also published as a crate on crates.io: <a href="https://crates.io/crates/compressed-intvec">compressed-intvec</a></li>
</ul>
<hr/>

<p>In Rust, the contract of a <code>Vec&lt;T&gt;</code> (where <code>T</code> is a primitive integer type like <code>u64</code> or <code>i32</code>) is simple: O(1) random access in exchange for a memory layout that is tied to the static size of <code>T</code>. This is a good trade-off, until it isn’t. When the dynamic range of the stored values is significantly smaller than the type’s capacity, this memory layout leads to substantial waste.</p>
<p>Consider storing the value <code>5</code> within a <code>Vec&lt;u64&gt;</code>. Its 8-byte in-memory representation is:</p>
<p><code>00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101</code></p>
<p>Only 3 bits are necessary to represent the value, leaving 61 bits as zero-padding. The same principle applies, albeit less dramatically, when storing <code>5</code> in a <code>u32</code> or <code>u16</code>. At scale, this overhead becomes prohibitive. A vector of one billion <code>u64</code> elements consumes <code>10^9 * std::mem::size_of::&lt;u64&gt;()</code>, or approximately 8 GB of memory, even if every element could fit within a single byte.</p>
<p>The canonical solution is bit packing, which aligns data end-to-end in a contiguous bitvector. However, this optimization has historically come at the cost of random access performance. The O(1) access guarantee of <code>Vec&lt;T&gt;</code> is predicated on simple pointer arithmetic: <code>address = base_address + index * std::mem::size_of::&lt;T&gt;()</code>. Tightly packing the bits invalidates this direct address calculation, seemingly forcing a trade-off between memory footprint and access latency.</p>
<p>This raises the central question that with this post we aim to answer: is it possible to design a data structure that decouples its memory layout from the static size of <code>T</code>, adapting instead to the data’s true dynamic range, without sacrificing the O(1) random access that makes <code>Vec&lt;T&gt;</code> so effective?</p>

<p>If the dynamic range of our data is known, we can define a fixed <code>bit_width</code> for every integer. For instance, if the maximum value in a dataset is <code>1000</code>, we know every number can be represented in 10 bits, since <code>2^10 = 1024</code>. Instead of allocating 64 bits per element, we can store them back-to-back in a contiguous bitvector, forming the core of what from now on we’ll refer as <code>FixedVec</code>. We can immagine such data strucutre as a logical array of <code>N</code> integers, each <code>bit_width</code> bits wide, stored in a backing buffer of <code>u64</code> words.</p>
<pre is:raw=""><code><span><span>struct</span><span> </span><span>FixedVec</span><span> {</span></span>
<span><span>    </span><span>limbs</span><span>: </span><span>Vec</span><span>&lt;</span><span>u64</span><span>&gt;,</span><span> // Backing storage</span></span>
<span><span>    </span><span>bit_width</span><span>: </span><span>usize</span><span>,</span><span> // Number of bits per element</span></span>
<span><span>    </span><span>len</span><span>: </span><span>usize</span><span>,</span><span> // Number of elements</span></span>
<span><span>    </span><span>mask</span><span>: </span><span>u64</span><span>,</span><span> // Precomputed mask for extraction</span></span>
<span><span>}</span></span></code></pre>
<p>Where the role of the <code>mask</code> field is to isolate the relevant bits during extraction. For a <code>bit_width</code> of 10, the mask would be <code>0b1111111111</code>.</p>
<p>This immediately solves the space problem, but how can we find the <code>i</code>-th element in O(1) time if it doesn’t align to a byte boundary? The answer lies in simple arithmetic. The starting bit position of any element is a direct function of its index. Given a backing store of <code>u64</code> words, we can locate any value by calculating its absolute bit position and then mapping that to a specific word and an offset within that word.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>bit_pos</span><span> </span><span>=</span><span> </span><span>index</span><span> * </span><span>bit_width</span><span>;</span><span> // Absolute bit position of the value</span></span>
<span><span>let</span><span> </span><span>word_index</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> / </span><span>64</span><span>;</span><span> // Which u64 word to read</span></span>
<span><span>let</span><span> </span><span>bit_offset</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> % </span><span>64</span><span>;</span><span> // Where the value starts in that word</span></span></code></pre>
<p>With these values, the implementation of <code>get_unchecked</code> becomes straightforward. It’s a two-step process: fetch the correct word from our backing <code>Vec&lt;u64&gt;</code>, then use bitwise operations to isolate the specific bits we need.</p>
<pre is:raw=""><code><span><span>// A simplified look at the core get_unchecked logic</span></span>
<span><span>unsafe</span><span> </span><span>fn</span><span> </span><span>get_unchecked</span><span>(&amp;</span><span>self</span><span>, </span><span>index</span><span>: </span><span>usize</span><span>) -&gt; </span><span>u64</span><span> {</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_width</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>bit_width</span><span>();</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_pos</span><span> </span><span>=</span><span> </span><span>index</span><span> * </span><span>bit_width</span><span>;</span></span>
<span><span>    </span><span>let</span><span> </span><span>word_index</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> / </span><span>64</span><span>;</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_offset</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> % </span><span>64</span><span>;</span></span>
<span></span>
<span><span>    // 1. Fetch the word from the backing store</span></span>
<span><span>    </span><span>let</span><span> </span><span>word</span><span> </span><span>=</span><span> *</span><span>self</span><span>.limbs.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>);</span></span>
<span></span>
<span><span>    // 2. Shift and mask to extract the value</span></span>
<span><span>    (</span><span>word</span><span> </span><span>&gt;&gt;</span><span> </span><span>bit_offset</span><span>) &amp; </span><span>self</span><span>.mask</span></span>
<span><span>}</span></span></code></pre>
<p>Let’s trace an access with <code>bit_width = 10</code> for the element at <code>index = 7</code>. The starting bit position is <code>7 * 10 = 70</code>. This maps to <code>word_index = 1</code> and <code>bit_offset = 6</code>. Our 10-bit integer begins at the 6th bit of the <em>second</em> <code>u64</code> word in our storage.</p>
<p>The right-shift <code>&gt;&gt;</code> operation moves the bits of the entire <code>u64</code> word to the right by <code>bit_offset</code> positions. This aligns the start of our desired 10-bit value with the least significant bit (LSB) of the word. The final step is to isolate our value. A pre-calculated <code>mask</code> (e.g., <code>0b1111111111</code> for 10 bits) is applied with a bitwise AND <code>&amp;</code>. This zeroes out any high-order bits from the word, leaving just our target integer.</p>
<h2 id="crossing-word-boundaries">Crossing Word Boundaries</h2>
<p>The single-word access logic is fast, but it only works as long as <code>bit_offset + bit_width &lt;= 64</code>. This assumption breaks down as soon as an integer’s bit representation needs to cross the boundary from one <code>u64</code> word into the next. This is guaranteed to happen for any <code>bit_width</code> that is not a power of two. For example, with a 10-bit width, the element at <code>index = 6</code> starts at bit position 60. Its 10 bits will occupy bits 60-63 of the first word and bits 0-5 of the second. The simple right-shift-and-mask trick fails here.</p>
<p><img src="https://lukefleed.xyz/assets/fixedvec.svg" alt="crossing word boundaries"/></p>
<p>To correctly decode the value, we must read <em>two</em> consecutive <code>u64</code> words and combine their bits. This splits our <code>get_unchecked</code> implementation into two paths. The first is the fast path we’ve already seen. The second is a new path for spanning values.</p>
<p>To get the lower bits of the value, we read the first word and shift right, just as before. This leaves the upper bits of the word as garbage.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>low_part</span><span> </span><span>=</span><span> *</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>) </span><span>&gt;&gt;</span><span> </span><span>bit_offset</span><span>;</span></span></code></pre>
<p>To get the upper bits of the value, we read the <em>next</em> word. The bits we need are at the beginning of this word, so we shift them left to align them correctly.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>high_part</span><span> </span><span>=</span><span> *</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span> + </span><span>1</span><span>) </span><span>&lt;&lt;</span><span> (</span><span>64</span><span> - </span><span>bit_offset</span><span>);</span></span></code></pre>
<p>Finally, we combine the two parts with a bitwise OR and apply the mask to discard any remaining high-order bits from the <code>high_part</code>.</p>
<pre is:raw=""><code><span><span>(</span><span>low_part</span><span> </span><span>|</span><span> </span><span>high_part</span><span>) &amp; </span><span>self</span><span>.mask</span></span></code></pre>
<p>The line <code>limbs.get_unchecked(word_index + 1)</code> introduces a safety concern: if we are reading the last element of the vector, <code>word_index + 1</code> could point past the end of our buffer, leading to undefined behavior. To prevent this, our builder must always allocate one extra padding word at the end of the storage.</p>
<p>Integrating these two paths gives us our final <code>get_unchecked</code> implementation:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>unsafe</span><span> </span><span>fn</span><span> </span><span>get_unchecked</span><span>(&amp;</span><span>self</span><span>, </span><span>index</span><span>: </span><span>usize</span><span>) -&gt; </span><span>u64</span><span> {</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_width</span><span> </span><span>=</span><span> </span><span>self</span><span>.bit_width;</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_pos</span><span> </span><span>=</span><span> </span><span>index</span><span> * </span><span>bit_width</span><span>;</span></span>
<span><span>    </span><span>let</span><span> </span><span>word_index</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> / </span><span>64</span><span>;</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_offset</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> % </span><span>64</span><span>;</span></span>
<span></span>
<span><span>    </span><span>let</span><span> </span><span>limbs</span><span> </span><span>=</span><span> </span><span>self</span><span>.bits.</span><span>as_ref</span><span>();</span></span>
<span></span>
<span><span>    </span><span>if</span><span> </span><span>bit_offset</span><span> + </span><span>bit_width</span><span> </span><span>&lt;=</span><span> </span><span>64</span><span> {</span></span>
<span><span>        // Fast path: value is fully within one word</span></span>
<span><span>        (*</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>) </span><span>&gt;&gt;</span><span> </span><span>bit_offset</span><span>) &amp; </span><span>self</span><span>.mask</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>        // Slow path: value spans two words.</span></span>
<span><span>        </span><span>let</span><span> </span><span>low_part</span><span> </span><span>=</span><span> *</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>) </span><span>&gt;&gt;</span><span> </span><span>bit_offset</span><span>;</span></span>
<span><span>        </span><span>let</span><span> </span><span>high_part</span><span> </span><span>=</span><span> *</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span> + </span><span>1</span><span>) </span><span>&lt;&lt;</span><span> (</span><span>64</span><span> - </span><span>bit_offset</span><span>);</span></span>
<span><span>        (</span><span>low_part</span><span> </span><span>|</span><span> </span><span>high_part</span><span>) &amp; </span><span>self</span><span>.mask</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<h2 id="faster-reads-unaligned-access">Faster Reads: Unaligned Access</h2>
<p>Our <code>get_unchecked</code> implementation is correct, but the slow path for spanning values requires two separate, aligned memory reads. The instruction sequence for this method involves at least two load instructions, multiple shifts, and a bitwise OR. These instructions have data dependencies: the shifts cannot execute until the loads complete, and the OR cannot execute until both shifts are done. This dependency chain can limit the CPU’s instruction-level parallelism and create pipeline stalls if the memory accesses miss the L1 cache.</p>
<p>Let’s have a look at the machine code generated by this method. We can create a minimal binary with an <code>#[inline(never)]</code> function that calls <code>get_unchecked</code> on a known spanning index, then use <a href="https://crates.io/crates/cargo-asm"><code>cargo asm</code></a> to inspect the disassembly.</p>
<pre is:raw=""><code><span><span>; asm_test::aligned::get_aligned (spanning path)</span></span>
<span><span>.</span><span>LBB14_2:</span></span>
<span><span>        </span><span>; let low = *limbs.get_unchecked(word_index) &gt;&gt; bit_offset;</span></span>
<span><span>        </span><span>mov</span><span> </span><span>rsi</span><span>, </span><span>qword</span><span> ptr [</span><span>r8</span><span> + </span><span>8</span><span>*</span><span>rdx</span><span>]      </span><span>; &lt;&lt;&lt; LOAD #1 (low_part)</span></span>
<span></span>
<span><span>        </span><span>; let high = *limbs.get_unchecked(word_index + 1) &lt;&lt; (64 - bit_offset);</span></span>
<span><span>        </span><span>mov</span><span> </span><span>rdx</span><span>, </span><span>qword</span><span> ptr [</span><span>r8</span><span> + </span><span>8</span><span>*</span><span>rdx</span><span> + </span><span>8</span><span>]  </span><span>; &lt;&lt;&lt; LOAD #2 (high_part)</span></span>
<span></span>
<span><span>        </span><span>shr</span><span> </span><span>rsi</span><span>, </span><span>cl</span><span>                          </span><span>; shift right of low_part</span></span>
<span><span>        </span><span>shl</span><span> </span><span>rdx</span><span>, </span><span>cl</span><span>                          </span><span>; shift left of high_part</span></span>
<span><span>        </span><span>or</span><span> </span><span>rdx</span><span>, </span><span>rsi</span><span>                          </span><span>; combine results</span></span></code></pre>
<p>The instruction <code>mov rsi, qword ptr [r8 + 8*rdx]</code> is our first memory access. It loads a 64-bit value (<code>qword</code>) into the <code>rsi</code> register. The address is calculated using base-plus-index addressing: <code>r8</code> holds the base address of our <code>limbs</code> buffer, and <code>rdx</code> holds the <code>word_index</code>. This corresponds directly to <code>limbs[word_index]</code>.</p>
<p>Immediately following is <code>mov rdx, qword ptr [r8 + 8*rdx + 8]</code>. This is our second, distinct memory access. It loads the <em>next</em> 64-bit word from memory by adding an 8-byte offset to the previous address. This corresponds to <code>limbs[word_index + 1]</code>.</p>
<p>Only after both of these <code>mov</code> instructions complete can the CPU proceed. The <code>shr rsi, cl</code> instruction (shift right <code>rsi</code> by the count in <code>cl</code>) cannot execute until the first <code>mov</code> has placed a value in <code>rsi</code>. Similarly, <code>shl rdx, cl</code> depends on the second <code>mov</code>. Finally, <code>or rdx, rsi</code> depends on both shifts.</p>
<p>This sequence of operations—loading two adjacent 64-bit words, shifting each, and combining them is a software implementation of what is, conceptually, a <a href="https://en.wikipedia.org/wiki/Barrel_shifter">128-bit barrel shifter</a>. We are selecting a 64-bit window from a virtual 128-bit integer formed by concatenating the two words from memory.</p>
<p><strong>Can we do better then this?</strong> Potentially, yes. We can replace this multi-instruction sequence with something more direct by delegating the complexity to the hardware and performing a single unaligned memory read. Modern x86-64 CPUs handle this directly: when an unaligned load instruction is issued, the CPU’s memory controller fetches the necessary cache lines and the load/store unit reassembles the bytes into the target register. This entire process is a single, optimized micro-operation.</p>
<p>We can try to implement a more aggressive access method. The strategy is to calculate the exact <em>byte</em> address where our data begins and perform a single, unaligned read of a full <code>W</code> word from that position.</p>
<p>The implementation first translates the absolute bit position into a byte address and a residual bit offset within that byte.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>bit_pos</span><span> </span><span>=</span><span> </span><span>index</span><span> * </span><span>self</span><span>.bit_width;</span></span>
<span><span>let</span><span> </span><span>byte_pos</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> / </span><span>8</span><span>;</span></span>
<span><span>let</span><span> </span><span>bit_rem</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> % </span><span>8</span><span>;</span></span></code></pre>
<p>With the byte-level address, we get a raw <code>*const u8</code> pointer to our storage and perform the unaligned read. The <a href="https://doc.rust-lang.org/std/ptr/fn.read_unaligned.html">read_unaligned</a> intrinsic in Rust compiles down to a single machine instruction that the hardware can execute efficiently.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>limbs_ptr</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>as_limbs</span><span>().</span><span>as_ptr</span><span>() </span><span>as</span><span> *</span><span>const</span><span> </span><span>u8</span><span>;</span></span>
<span><span>// This read may cross hardware word boundaries, but the CPU handles it.</span></span>
<span><span>let</span><span> </span><span>word</span><span>: </span><span>W</span><span> </span><span>=</span><span> (</span><span>limbs_ptr</span><span>.</span><span>add</span><span>(</span><span>byte_pos</span><span>) </span><span>as</span><span> *</span><span>const</span><span> </span><span>W</span><span>).</span><span>read_unaligned</span><span>();</span></span></code></pre>
<p>On a Little-Endian system, the loaded <code>word</code> now contains our target integer, but it’s shifted by <code>bit_rem</code> positions. A simple right shift aligns our value to the LSB, and applying the mask isolates it.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>extracted_word</span><span> </span><span>=</span><span> </span><span>word</span><span> </span><span>&gt;&gt;</span><span> </span><span>bit_rem</span><span>;</span></span>
<span><span>let</span><span> </span><span>final_value</span><span> </span><span>=</span><span> </span><span>extracted_word</span><span> &amp; </span><span>self</span><span>.mask;</span></span></code></pre>
<p>This operation is safe only because, as said before, we are supposing that our builder guarantees a padding word at the end of the storage buffer. Combining all these steps, we get our final implementation:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>unsafe</span><span> </span><span>fn</span><span> </span><span>get_unaligned_unchecked</span><span>(&amp;</span><span>self</span><span>, </span><span>index</span><span>: </span><span>usize</span><span>) -&gt; </span><span>u64</span><span> {</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_pos</span><span> </span><span>=</span><span> </span><span>index</span><span> * </span><span>self</span><span>.bit_width;</span></span>
<span><span>    </span><span>let</span><span> </span><span>byte_pos</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> / </span><span>8</span><span>;</span></span>
<span><span>    </span><span>let</span><span> </span><span>bit_rem</span><span> </span><span>=</span><span> </span><span>bit_pos</span><span> % </span><span>8</span><span>;</span></span>
<span></span>
<span><span>    </span><span>let</span><span> </span><span>limbs_ptr</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>as_limbs</span><span>().</span><span>as_ptr</span><span>() </span><span>as</span><span> *</span><span>const</span><span> </span><span>u8</span><span>;</span></span>
<span><span>    // SAFETY: The builder guarantees an extra padding word at the end.</span></span>
<span><span>    </span><span>let</span><span> </span><span>word</span><span> </span><span>=</span><span> (</span><span>limbs_ptr</span><span>.</span><span>add</span><span>(</span><span>byte_pos</span><span>) </span><span>as</span><span> *</span><span>const</span><span> </span><span>W</span><span>).</span><span>read_unaligned</span><span>();</span></span>
<span><span>    </span><span>let</span><span> </span><span>extracted_word</span><span> </span><span>=</span><span> </span><span>word</span><span> </span><span>&gt;&gt;</span><span> </span><span>bit_rem</span><span>;</span></span>
<span><span>    </span><span>extracted_word</span><span> &amp; </span><span>self</span><span>.mask</span></span>
<span><span>}</span></span></code></pre>
<p>Let’s have a look at the generated machine code for this new method when accessing an index that spans words:</p>
<pre is:raw=""><code><span><span>; asm_test::unaligned::get_unaligned</span></span>
<span><span>.</span><span>LBB15_1:</span></span>
<span><span>        </span><span>; let bit_pos = index * self.bit_width;</span></span>
<span><span>        </span><span>imul</span><span> </span><span>rcx</span><span>, </span><span>rax</span></span>
<span><span>        </span><span>; let byte_pos = bit_pos / 8;</span></span>
<span><span>        </span><span>mov</span><span> </span><span>rax</span><span>, </span><span>rcx</span></span>
<span><span>        </span><span>shr</span><span> </span><span>rax</span><span>, </span><span>3</span></span>
<span><span>        </span><span>; self.bits.as_ref()</span></span>
<span><span>        </span><span>mov</span><span> </span><span>rdx</span><span>, </span><span>qword</span><span> ptr [</span><span>rdi</span><span> + </span><span>8</span><span>]</span></span>
<span><span>        </span><span>; unsafe { crate::intrinsics::copy_nonoverlapping(src, dst, count) }</span></span>
<span><span>        </span><span>mov</span><span> </span><span>rax</span><span>, </span><span>qword</span><span> ptr [</span><span>rdx</span><span> + </span><span>rax</span><span>]       </span><span>; &lt;&lt;&lt; SINGLE UNALIGNED LOAD</span></span>
<span><span>        </span><span>; self &gt;&gt; other</span></span>
<span><span>        </span><span>and</span><span> </span><span>cl</span><span>, </span><span>7</span></span>
<span><span>        </span><span>shr</span><span> </span><span>rax</span><span>, </span><span>cl</span></span>
<span><span>        </span><span>; fn bitand(self, rhs: $t) -&gt; $t { self &amp; rhs }</span></span>
<span><span>        </span><span>and</span><span> </span><span>rax</span><span>, </span><span>qword</span><span> ptr [</span><span>rdi</span><span> + </span><span>32</span><span>]</span></span></code></pre>
<p>The initial <code>imul</code> and <code>shr rax, 3</code> (a fast division by 8) correspond to the calculation of <code>byte_pos</code>. The instruction <code>mov rdx, qword ptr [rdi + 8]</code> loads the base address of our <code>limbs</code> buffer into the <code>rdx</code> register.</p>
<p>The instruction <code>mov rax, qword ptr [rdx + rax]</code> is our single unaligned load. The address <code>[rdx + rax]</code> is the sum of the buffer’s base address and our calculated <code>byte_pos</code>. This <code>mov</code> instruction reads 8 bytes (a <code>qword</code>) directly from this potentially unaligned memory location into the <code>rax</code> register. We can see that as we hoped, the <a href="https://doc.rust-lang.org/std/ptr/fn.read_unaligned.html">read_unaligned</a> intrinsic has been compiled down to a single hardware instruction.</p>
<p>The next instructions handle the extraction. The <code>and cl, 7</code> and <code>shr rax, cl</code> sequence corresponds to our <code>&gt;&gt; bit_rem</code>. <code>cl</code> holds the lower bits of the original <code>bit_pos</code> (our <code>bit_rem</code>), and the shift aligns our desired value to the LSB of the <code>rax</code> register. Finally, <code>and rax, qword ptr [rdi + 32]</code> applies the pre-calculated mask, which is stored at an offset from the <code>self</code> pointer in <code>rdi</code>.</p>
<h2 id="random-access-performance">Random Access Performance</h2>
<p>We can now benchmark the latency of 1 million random access operations on a vector containing 10 million elements. For each <code>bit_width</code>, we generate data with a uniform random distribution in the range <code>[0, 2^bit_width)</code>. The code for the benchmark is available here: <a href="https://github.com/lukefleed/compressed-intvec/blob/master/benches/fixed/bench_random_access.rs"><code>bench-intvec</code></a></p>
<p>Our baseline is the smallest standard <code>Vec&lt;T&gt;</code> capable of holding the data (<code>Vec&lt;u8&gt;</code> for <code>bit_width &lt;= 8</code>, etc.). We also include results from <a href="https://docs.rs/sux/latest/sux/bits/bit_field_vec/index.html"><code>sux</code></a>, <a href="https://docs.rs/succinct/latest/succinct/trait.IntVec.html"><code>succinct</code></a>, and <a href="https://docs.rs/simple-sds-sbwt/latest/simple_sds_sbwt/int_vector/struct.IntVector.html"><code>simple-sds-sbwt</code></a> for context. I am not aware of any other Rust crates that implement fixed-width bit-packed integer vectors, so if you know of any, please let me know!</p>

<p>We can see that for <code>bit_width</code> values below 32, the <code>get_unaligned_unchecked</code> of our <code>FixedVec</code> is almost always faster than the corresponding <code>Vec&lt;T&gt;</code> baseline. This is a result of improved cache locality. A 64-byte L1 cache line can hold 64 elements from a <code>Vec&lt;u8&gt;</code>. With a <code>bit_width</code> of 4, the same cache line holds <code>(64 * 8) / 4 = 128</code> elements from our <code>FixedVec</code>. This increased density improves the cache hit rate for random access patterns, and the latency reduction from avoiding DRAM access outweighs the instruction cost of the bitwise extraction. For values of <code>bit_width</code> above 32, the performance of <code>FixedVec</code> are <em>very slightly</em> worse than the <code>Vec&lt;T&gt;</code> baseline, as the cache locality advantage diminishes. However, the memory savings remain.</p>
<p>The performance delta between <code>get_unaligned_unchecked</code> and <code>get_unchecked</code> confirms the unaligned access strategy discussed before: a single <code>read_unaligned</code> instruction is more efficient than the two dependent aligned reads required by the logic for spanning words.</p>
<p>We can see that the implementation of <a href="https://docs.rs/sux/latest/sux/bits/bit_field_vec/index.html"><code>sux</code></a> is almost on par with ours. The other two crates, <a href="https://docs.rs/succinct/latest/succinct/trait.IntVec.html"><code>succinct</code></a> and <a href="https://docs.rs/simple-sds-sbwt/latest/simple_sds_sbwt/int_vector/struct.IntVector.html"><code>simple-sds-sbwt</code></a>, are significantly slower (note that the Y-axis is logarithmic). Tho, it’s worth noting that neither of these last two crates provides unchecked or unaligned access methods, so their implementations are inherently more conservative.</p>
<h2 id="iterating-over-values">Iterating Over Values</h2>
<p>The most common operation on any <code>Vec</code>-like structure is, after all, a simple <code>for</code> loop. The simplest way to implement <code>iter()</code> would be to just wrap <code>get()</code> in a loop:</p>
<pre is:raw=""><code><span><span>// A naive, inefficient iterator</span></span>
<span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>vec</span><span>.</span><span>len</span><span>() {</span></span>
<span><span>    </span><span>let</span><span> </span><span>value</span><span> </span><span>=</span><span> </span><span>vec</span><span>.</span><span>get</span><span>(</span><span>i</span><span>);</span></span>
<span><span>    // ... do something with value</span></span>
<span><span>}</span></span></code></pre>
<p>This works, but it’s terribly inefficient. Every single call to <code>get(i)</code> independently recalculates the <code>word_index</code> and <code>bit_offset</code> from scratch. We’re throwing away valuable state, our current position in the bitstream, on every iteration, forcing the CPU to perform redundant multiplications and divisions.</p>
<p>We can think then about a <em>stateful</em> iterator. It should operate directly on the bitvector, maintaining its own position. Instead of thinking in terms of logical indices, it should think in terms of a “bit window”, a local <code>u64</code> register that holds the current chunk of bits being processed.</p>
<p>The idea is simple: the iterator loads one <code>u64</code> word from the backing store into its window. It then satisfies <code>next()</code> calls by decoding values directly from this in-register window. Only when the window runs out of bits does it need to go back to memory for the next <code>u64</code> word. This amortizes the cost of memory access over many <code>next()</code> calls.</p>
<p>For forward iteration, the state is minimal:</p>
<pre is:raw=""><code><span><span>struct</span><span> </span><span>FixedVecIter</span><span>&lt;&#39;</span><span>a</span><span>, ...&gt; {</span></span>
<span><span>    // ...</span></span>
<span><span>    </span><span>front_window</span><span>: </span><span>u64</span><span>,</span></span>
<span><span>    </span><span>front_bits_in_window</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>front_word_index</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span></code></pre>
<p>The <code>next()</code> method first checks if the current <code>front_window</code> has enough bits to satisfy the request. If <code>self.front_bits_in_window &gt;= bit_width</code>, it’s the fast path: a simple shift and mask on a register, which is incredibly fast.</p>
<pre is:raw=""><code><span><span>// Inside next(), fast path:</span></span>
<span><span>if</span><span> </span><span>self</span><span>.front_bits_in_window </span><span>&gt;=</span><span> </span><span>bit_width</span><span> {</span></span>
<span><span>    </span><span>let</span><span> </span><span>value</span><span> </span><span>=</span><span> </span><span>self</span><span>.front_window &amp; </span><span>self</span><span>.mask;</span></span>
<span><span>    </span><span>self</span><span>.front_window </span><span>&gt;&gt;=</span><span> </span><span>bit_width</span><span>;</span></span>
<span><span>    </span><span>self</span><span>.front_bits_in_window </span><span>-=</span><span> </span><span>bit_width</span><span>;</span></span>
<span><span>    </span><span>return</span><span> </span><span>Some</span><span>(</span><span>value</span><span>);</span></span>
<span><span>}</span></span></code></pre>
<p>If the window is running low on bits, we hit the slower path. The next value spans the boundary between our current window and the next <code>u64</code> word in memory. We must read the next word, combine its bits with the remaining bits in our current window, and then extract the value. This is the same logic as the spanning-word <code>get()</code>, but it’s performed incrementally.</p>
<h3 id="double-ended-iteration">Double-Ended Iteration</h3>
<p>But I want my iterator to be bidirectional! Well, then we need to ensure it implements <a href="https://doc.rust-lang.org/std/iter/trait.DoubleEndedIterator.html"><code>DoubleEndedIterator</code></a> and supports <a href="https://doc.rust-lang.org/std/iter/trait.DoubleEndedIterator.html#tymethod.next_back"><code>next_back()</code></a>. This throws a wrench in our simple stateful model. A single window and cursor can only move in one direction.</p>
<p>The solution is to maintain two independent sets of state: one for the front and one for the back. The <code>FixedVecIter</code> needs to track two windows, two bit counters, and two word indices.</p>
<pre is:raw=""><code><span><span>struct</span><span> </span><span>FixedVecIter</span><span>&lt;&#39;</span><span>a</span><span>, ...&gt; {</span></span>
<span><span>    // ...</span></span>
<span><span>    </span><span>front_index</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>back_index</span><span>: </span><span>usize</span><span>,</span></span>
<span></span>
<span><span>    // State for forward iteration</span></span>
<span><span>    </span><span>front_window</span><span>: </span><span>u64</span><span>,</span></span>
<span><span>    </span><span>front_bits_in_window</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>front_word_index</span><span>: </span><span>usize</span><span>,</span></span>
<span></span>
<span><span>    // State for backward iteration</span></span>
<span><span>    </span><span>back_window</span><span>: </span><span>u64</span><span>,</span></span>
<span><span>    </span><span>back_bits_in_window</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>back_word_index</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span></code></pre>
<p>Initializing the front is easy: we load <code>limbs[0]</code> into <code>front_window</code>. The back is more complex. We must calculate the exact word index and the number of valid bits in the <em>last</em> word that contains data. This requires a bit of arithmetic to handle cases where the data doesn’t perfectly fill the final word.</p>
<p>The <code>next()</code> method consumes from the <code>front_window</code>, advancing the front state. The <code>next_back()</code> method consumes from the <code>back_window</code>, advancing the back state. The iterator is exhausted when <code>front_index</code> meets <code>back_index</code>.</p>
<blockquote>
<p>The full implementation can be found in the iter module of the <a href="https://github.com/lukefleed/compressed-intvec/blob/master/src/fixed/iter.rs">library</a></p>
</blockquote>

<p>We have solved the read problem, but we may also want to modify values in place. A method like <code>set(index, value)</code> seems simple, but it opens up the same can of worms as <code>get</code>, just in reverse. We can’t just write the new value; we have to do so without clobbering the adjacent, unrelated data packed into the same <code>u64</code> word.</p>
<p>Just like with reading, the logic splits into two paths. The “fast path” handles values that are fully contained within a single <code>u64</code>. Here, we can’t just overwrite the word. We first need to clear out the bits for the element we’re replacing and then merge in the new value.</p>
<h2 id="in-word-write">In-Word Write</h2>
<p>Our goal here is to update a <code>bit_width</code>-sized slice of a <code>u64</code> word while leaving the other bits untouched. This operation must be a read-modify-write sequence to avoid corrupting adjacent elements. The most efficient way to implement this is to load the entire word into a register, perform all bitwise modifications locally, and then write the final result back to memory in a single store operation.</p>
<p>First, we load the word from our backing <code>limbs</code> slice.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>mut</span><span> </span><span>word</span><span> </span><span>=</span><span> *</span><span>limbs</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>);</span></span></code></pre>
<p>Next, we need to create a “hole” in our local copy where the new value will go. We do this by creating a mask that has ones <em>only</em> in the bit positions we want to modify, and then inverting it to create a clearing mask.</p>
<pre is:raw=""><code><span><span>// For a value at bit_offset, the mask must also be shifted.</span></span>
<span><span>let</span><span> </span><span>clear_mask</span><span> </span><span>=</span><span> </span><span>!</span><span>(</span><span>self</span><span>.mask </span><span>&lt;&lt;</span><span> </span><span>bit_offset</span><span>);</span></span>
<span><span>// Applying this mask zeroes out the target bits in our register copy.</span></span>
<span><span>word</span><span> </span><span>&amp;=</span><span> </span><span>clear_mask</span><span>;</span></span></code></pre>
<p>With the target bits zeroed, we can merge our new value. The value is first shifted left by <code>bit_offset</code> to align it correctly within the 64-bit word. Then, a bitwise OR merges it into the “hole” we just created.</p>
<pre is:raw=""><code><span><span>// Shift the new value into position and merge it.</span></span>
<span><span>word</span><span> </span><span>|=</span><span> </span><span>value_w</span><span> </span><span>&lt;&lt;</span><span> </span><span>bit_offset</span><span>;</span></span></code></pre>
<p>Finally, with the modifications complete, we write the updated word from the register back to memory in a single operation.</p>
<pre is:raw=""><code><span><span>*</span><span>limbs</span><span>.</span><span>get_unchecked_mut</span><span>(</span><span>word_index</span><span>) </span><span>=</span><span> </span><span>word</span><span>;</span></span></code></pre>
<p>This entire sequence of one read, two bitwise operations in-register, one write is the canonical and most efficient way to perform a sub-word update.</p>
<h2 id="spanning-write">Spanning Write</h2>
<p>Now for the hard part: writing a value that crosses a word boundary. This operation must modify two separate <code>u64</code> words in our backing store. It’s the inverse of the spanning read. We need to split our <code>value_w</code> into a low part and a high part and write each to the correct word, minimizing memory accesses.</p>
<p>To operate on two distinct memory locations, <code>limbs[word_index]</code> and <code>limbs[word_index + 1]</code>, we first need mutable access to both. In a safe, hot path like this, we can use <code>split_at_mut_unchecked</code> to bypass Rust’s borrow checker bounds checks, as we have already guaranteed through our logic that both indices are valid.</p>
<pre is:raw=""><code><span><span>// SAFETY: We know word_index and word_index + 1 are valid.</span></span>
<span><span>let</span><span> (</span><span>left</span><span>, </span><span>right</span><span>) </span><span>=</span><span> </span><span>limbs</span><span>.</span><span>split_at_mut_unchecked</span><span>(</span><span>word_index</span><span> + </span><span>1</span><span>);</span></span></code></pre>
<p>Our strategy is to read both words into registers, perform all bitwise logic locally, and then write both modified words back to memory. This minimizes the time we hold mutable references and can improve performance.</p>
<p>First, we handle the <code>low_word</code>. We need to replace its high bits (from <code>bit_offset</code> onwards) with the low bits of our new value. The most direct way is to create a mask for the bits we want to <em>keep</em>. The expression <code>(1 &lt;&lt; bit_offset) - 1</code> is a bit-twiddling trick to generate a mask with <code>bit_offset</code> ones at the least significant end.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>mut</span><span> </span><span>low_word_val</span><span> </span><span>=</span><span> *</span><span>left</span><span>.</span><span>get_unchecked</span><span>(</span><span>word_index</span><span>);</span></span>
<span></span>
<span><span>// Create a mask to preserve the low `bit_offset` bits of the word.</span></span>
<span><span>let</span><span> </span><span>low_mask</span><span> </span><span>=</span><span> (</span><span>1</span><span>u64</span><span> </span><span>&lt;&lt;</span><span> </span><span>bit_offset</span><span>).</span><span>wrapping_sub</span><span>(</span><span>1</span><span>);</span></span>
<span><span>low_word_val</span><span> </span><span>&amp;=</span><span> </span><span>low_mask</span><span>;</span></span></code></pre>
<p>With the target bits zeroed out, we merge in the low part of our new value. A left shift aligns it correctly, and the high bits of <code>value_w</code> are naturally shifted out of the register.</p>
<pre is:raw=""><code><span><span>// Merge in the low part of our new value.</span></span>
<span><span>low_word_val</span><span> </span><span>|=</span><span> </span><span>value_w</span><span> </span><span>&lt;&lt;</span><span> </span><span>bit_offset</span><span>;</span></span>
<span><span>*</span><span>left</span><span>.</span><span>get_unchecked_mut</span><span>(</span><span>word_index</span><span>) </span><span>=</span><span> </span><span>low_word_val</span><span>;</span></span></code></pre>
<p>Next, we handle the <code>high_word</code> in a symmetrical fashion. We need to write the remaining high bits of <code>value_w</code> into the low-order bits of this second word. First, we calculate how many bits of our value actually belong in the first word:</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>remaining_bits_in_first_word</span><span> </span><span>=</span><span> </span><span>64</span><span> - </span><span>bit_offset</span><span>;</span></span></code></pre>
<p>Now, we read the second word and create a mask to clear the low-order bits where our data will be written. With the operation <code>self.mask &gt;&gt; remaining_bits_in_first_word</code> we can determine how many bits of our value spill into the second word, creating a mask for them. Inverting this gives us a mask to <em>preserve</em> the existing high-order bits of the <code>high_word</code>.</p>
<pre is:raw=""><code><span><span>let</span><span> </span><span>mut</span><span> </span><span>high_word_val</span><span> </span><span>=</span><span> *</span><span>right</span><span>.</span><span>get_unchecked</span><span>(</span><span>0</span><span>);</span></span>
<span></span>
<span><span>// Clear the low bits of the second word that will be overwritten.</span></span>
<span><span>high_word_val</span><span> </span><span>&amp;=</span><span> </span><span>!</span><span>(</span><span>self</span><span>.mask </span><span>&gt;&gt;</span><span> </span><span>remaining_bits_in_first_word</span><span>);</span></span></code></pre>
<p>Finally, we isolate the high part of <code>value_w</code> by right-shifting it by the number of bits we already wrote, and merge it into the cleared space.</p>
<pre is:raw=""><code><span><span>// Merge in the high part of our new value.</span></span>
<span><span>high_word_val</span><span> </span><span>|=</span><span> </span><span>value_w</span><span> </span><span>&gt;&gt;</span><span> </span><span>remaining_bits_in_first_word</span><span>;</span></span>
<span><span>*</span><span>right</span><span>.</span><span>get_unchecked_mut</span><span>(</span><span>0</span><span>) </span><span>=</span><span> </span><span>high_word_val</span><span>;</span></span></code></pre>
<h2 id="random-write-performance">Random Write Performance</h2>
<p>As with reads, we can benchmark the latency of 1 million random write operations on a vector containing 10 million elements. The code for the benchmark is available here: [<code>bench-intvec-writes</code>].</p>

<p>Here, the <code>Vec&lt;T&gt;</code> baseline is the clear winner across almost all bit-widths. This isn’t surprising. A <code>set</code> operation in a <code>Vec&lt;T&gt;</code> compiles down to a single <code>MOV</code> instruction with a simple addressing mode (<code>[base + index * element_size]</code>). It’s about as fast as the hardware allows.</p>
<p>As for the reads, the performance of our <code>FixedVec</code> is almost identical to that of <a href="https://docs.rs/sux/latest/sux/bits/bit_field_vec/index.html"><code>sux</code></a>. The other two crates, <a href="https://docs.rs/succinct/latest/succinct/trait.IntVec.html"><code>succinct</code></a> and <a href="https://docs.rs/simple-sds-sbwt/latest/simple_sds_sbwt/int_vector/struct.IntVector.html"><code>simple-sds-sbwt</code></a>, are again slower. It’s worth noting that also for the writes, neither of these last two crates provides unchecked methods.</p>
<blockquote>
<p>For the 64-bit width case, I honestly have no idea what is going on with <a href="https://docs.rs/sux/latest/sux/bits/bit_field_vec/index.html"><code>sux</code></a> being so much faster than everything else, even then <code>Vec&lt;u64&gt;</code>! Mybe some weird compiler optimization? If you have any insight, please let me know.</p>
</blockquote>

<p>With the access patterns defined, we need to think about the overall architecture of this data structure. A solution hardcoded to <code>u64</code> would lack the flexibility to adapt to different use cases. We need a structure that is generic over the its principal components: the logical type, the physical storage type, the bit-level ordering, and ownership. We can define a struct that is generic over these four parameters:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>struct</span><span> </span><span>FixedVec</span><span>&lt;</span><span>T</span><span>: </span><span>Storable</span><span>&lt;</span><span>W</span><span>&gt;, </span><span>W</span><span>: </span><span>Word</span><span>, </span><span>E</span><span>: </span><span>Endianness</span><span>, </span><span>B</span><span>: </span><span>AsRef</span><span>&lt;[</span><span>W</span><span>]&gt; </span><span>=</span><span> </span><span>Vec</span><span>&lt;</span><span>W</span><span>&gt;&gt; {</span></span>
<span><span>    </span><span>bits</span><span>: </span><span>B</span><span>,</span></span>
<span><span>    </span><span>bit_width</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>mask</span><span>: </span><span>W</span><span>,</span></span>
<span><span>    </span><span>len</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>}</span></span></code></pre>
<p>Where:</p>
<p><code>T</code> is the <strong>logical element type</strong>, the type as seen by the user of the API (e.g., <code>i16</code>, <code>u32</code>). By abstracting <code>T</code>, we divide the user-facing type from the internal storage representation.</p>
<p><code>W</code> is the <strong>physical storage word</strong>, which must implement our <code>Word</code> trait. It defines the primitive unsigned integer (<code>u32</code>, <code>u64</code>, <code>usize</code>) of the backing buffer and sets the granularity for all bitwise operations.</p>
<p><code>E</code> requires the <code>dsi-bitstream::Endianness</code> trait, allowing us to specify either Little-Endian (<code>LE</code>) or Big-Endian (<code>BE</code>) byte order.</p>
<p><code>B</code>, which must implement <code>AsRef&lt;[W]&gt;</code>, represents the <strong>backing storage</strong>. This abstraction over ownership allows <code>FixedVec</code> to be either an owned container where <code>B = Vec&lt;W&gt;</code>, or a zero-copy, borrowed view where <code>B = &amp;[W]</code>. This makes it possible to, for example, construct a <code>FixedVec</code> directly over a memory-mapped slice without any heap allocation.</p>
<p>In this way, the compiler monomorphizes the struct and its methods for each concrete instantiation (e.g., <code>FixedVec&lt;i16, u64, LE, Vec&lt;u64&gt;&gt;</code>), resulting in specialized code with no runtime overhead from the generic abstractions.</p>
<h2 id="word-trait-the-physical-storage-layer"><code>Word</code> Trait: The Physical Storage Layer</h2>
<p>The first step is to abstract the physical storage layer. The <code>W</code> parameter must be a primitive unsigned integer type that supports bitwise operations. We can define a <code>Word</code> trait that captures these requirements:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>trait</span><span> </span><span>Word</span><span>:</span></span>
<span><span>    </span><span>UnsignedInt</span><span> + </span><span>Bounded</span><span> + </span><span>ToPrimitive</span><span> + </span><span>dsi_bitstream</span><span>::</span><span>traits</span><span>::</span><span>Word</span></span>
<span><span>    + </span><span>NumCast</span><span> + </span><span>Copy</span><span> + </span><span>Send</span><span> + </span><span>Sync</span><span> + </span><span>Debug</span><span> + </span><span>IntoAtomic</span><span> + &#39;</span><span>static</span></span>
<span><span>{</span></span>
<span><span>    </span><span>const</span><span> </span><span>BITS</span><span>: </span><span>usize</span><span> </span><span>=</span><span> </span><span>std</span><span>::</span><span>mem</span><span>::</span><span>size_of</span><span>::&lt;</span><span>Self</span><span>&gt;() * </span><span>8</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>The numeric traits (<code>UnsignedInt</code>, <code>Bounded</code>, <code>NumCast</code>, <code>ToPrimitive</code>) are necessary for the arithmetic of offset and mask calculations. The <code>dsi_bitstream::traits::Word</code> bound allows us to integrate with its <code>BitReader</code> and <code>BitWriter</code> implementations, offloading the bitstream logic. <code>Send</code> and <code>Sync</code> are non-negotiable requirements for any data structure that might be used in a concurrent context. The <code>IntoAtomic</code> bound is particularly forward-looking: it establishes a compile-time link between a storage word like <code>u64</code> and its atomic counterpart, <code>AtomicU64</code>. We will use it later to build a thread safe, atomic version of <code>FixedVec</code>. Finally, the <code>const BITS</code> associated constant lets us write architecture-agnostic code that correctly adapts to <code>u32</code>, <code>u64</code>, or <code>usize</code> words without <code>cfg</code> flags.</p>
<h2 id="storable-trait-the-logical-type-layer"><code>Storable</code> Trait: The Logical Type Layer</h2>
<p>With the physical storage layer defined, we need a formal contract to connect it to the user’s logical type <code>T</code>. We can do this by creating the <code>Storable</code> trait, which defines a bidirectional, lossless conversion.</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>trait</span><span> </span><span>Storable</span><span>&lt;</span><span>W</span><span>: </span><span>Word</span><span>&gt;: </span><span>Sized</span><span> + </span><span>Copy</span><span> {</span></span>
<span><span>    </span><span>fn</span><span> </span><span>into_word</span><span>(</span><span>self</span><span>) -&gt; </span><span>W</span><span>;</span></span>
<span><span>    </span><span>fn</span><span> </span><span>from_word</span><span>(</span><span>word</span><span>: </span><span>W</span><span>) -&gt; </span><span>Self</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>For unsigned types, the implementation is a direct cast. For signed types, however, we must map the <code>iN</code> domain to the <code>uN</code> domain required for bit-packing. A simple two’s complement bitcast is unsuitable, as <code>i64(-1)</code> would become <code>u64::MAX</code>, a value requiring the maximum number of bits. We need a better mapping that preserves small absolute values.</p>
<p>We can use <strong>ZigZag encoding</strong>, which maps integers with small absolute values to small unsigned integers. This is implemented via the <code>ToNat</code> and <code>ToInt</code> traits from <code>dsi-bitstream</code>. The core encoding logic in <code>to_nat</code> is:</p>
<pre is:raw=""><code><span><span>// From dsi_bitstream::traits::ToNat</span></span>
<span><span>fn</span><span> </span><span>to_nat</span><span>(</span><span>self</span><span>) -&gt; </span><span>Self</span><span>::</span><span>UnsignedInt</span><span> {</span></span>
<span><span>    (</span><span>self</span><span> </span><span>&lt;&lt;</span><span> </span><span>1</span><span>).</span><span>to_unsigned</span><span>() </span><span>^</span><span> (</span><span>self</span><span> </span><span>&gt;&gt;</span><span> (</span><span>Self</span><span>::</span><span>BITS</span><span> - </span><span>1</span><span>)).</span><span>to_unsigned</span><span>()</span></span>
<span><span>}</span></span></code></pre>
<p>This operation works as follows: <code>(self &lt;&lt; 1)</code> creates a space at the LSB. The term <code>(self &gt;&gt; (Self::BITS - 1))</code> is an arithmetic right shift, which generates a sign mask—all zeros for non-negative numbers, all ones for negative numbers. The final XOR uses this mask to interleave positive and negative integers: 0 becomes 0, -1 becomes 1, 1 becomes 2, -2 becomes 3, and so on.</p>
<p>The decoding reverses this transformation:</p>
<pre is:raw=""><code><span><span>// From dsi_bitstream::traits::ToInt</span></span>
<span><span>fn</span><span> </span><span>to_int</span><span>(</span><span>self</span><span>) -&gt; </span><span>Self</span><span>::</span><span>SignedInt</span><span> {</span></span>
<span><span>    (</span><span>self</span><span> </span><span>&gt;&gt;</span><span> </span><span>1</span><span>).</span><span>to_signed</span><span>() </span><span>^</span><span> (-(</span><span>self</span><span> &amp; </span><span>1</span><span>).</span><span>to_signed</span><span>())</span></span>
<span><span>}</span></span></code></pre>
<p>Here, <code>(self &gt;&gt; 1)</code> shifts the value back. The term <code>-(self &amp; 1)</code> creates a mask from the LSB (the original sign bit). In two’s complement, this becomes <code>0</code> for even numbers (originally positive) and <code>-1</code> (all ones) for odd numbers (originally negative). The final XOR with this mask correctly restores the original two’s complement representation.</p>
<p>We might ask ourselves: why not just a direct bitcast for signed types, perhaps via a <code>uN -&gt; iN</code> chain. Well, while a direct <a href="https://doc.rust-lang.org/std/mem/fn.transmute.html">transmute</a> between same-sized integer types is a no-op, the approach fails when the logical <code>bit_width</code> is smaller than the physical type size. <code>FixedVec</code>’s core logic extracts a <code>bit_width</code>-sized unsigned integer from its storage. For example, when reading a 4-bit representation of <code>-1</code> (binary <code>1111</code>), the <code>from_word</code> function receives the value <code>15u64</code>. At this point, the context that those four bits represented a negative number is lost. A cast chain like <code>15u64 as u8 as i8</code> would simply yield <code>15i8</code>, not <code>-1i8</code>.</p>
<p>To correctly reconstruct the signed value, one would need to manually perform sign extension based on the known <code>bit_width</code>. This involves checking the most significant bit of the extracted value and, if set, filling the higher-order bits of the word with ones. This logic requires a conditional branch, which can introduce pipeline stalls and degrade performance in tight loops. ZigZag decoding, by contrast, is a purely arithmetic, branch-free transformation. Its reconstruction logic is a simple sequence of bitwise shifts and XORs, making it a faster and more consistent choice for the hot path of data access.</p>
<p>With this logic within the trait system, the main <code>FixedVec</code> implementation remains clean and agnostic to the signedness of the data it stores.</p>
<h2 id="builder-pattern">Builder Pattern</h2>
<p>Once the structure logic is in place, we have to design an ergonomic way to construct it. A simple <code>new()</code> function isn’t sufficient because the vector’s memory layout depends on parameters that must be determined <em>before</em> allocation, most critically the <code>bit_width</code>. This is a classic scenario for a builder pattern.</p>
<p>The central problem is that the optimal <code>bit_width</code> often depends on the data itself. We need a mechanism to specify the <em>strategy</em> for determining this width. We can create the <code>BitWidth</code> enum:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>enum</span><span> </span><span>BitWidth</span><span> {</span></span>
<span><span>    </span><span>Minimal</span><span>,</span></span>
<span><span>    </span><span>PowerOfTwo</span><span>,</span></span>
<span><span>    </span><span>Explicit</span><span>(</span><span>usize</span><span>),</span></span>
<span><span>}</span></span></code></pre>
<p>With this enum, the user can choose between three strategies:</p>
<ul>
<li><code>Minimal</code>: The builder scans the input data to find the maximum value, then calculates the minimum number of bits required to represent it. This is the most space-efficient option but requires a full pass over the data.</li>
<li><code>PowerOfTwo</code>: Similar to <code>Minimal</code>, but rounds the bit width up to the next power of two. This can simplify certain bitwise operations and align better with hardware word sizes, at the cost of some additional space.</li>
<li><code>Explicit(n)</code>: The user provides a fixed bit width. This avoids the data scan but requires the user to ensure that all values fit within the specified width.</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Yes, I could have also made three different build functions: <code>new_with_minimal_bit_width()</code>, <code>new_with_power_of_two_bit_width()</code>, and <code>new_with_explicit_bit_width(n)</code>. However, this would lead to a combinatorial explosion if we later wanted to add more configuration options. The builder pattern scales better.</p>
</blockquote>
<p>With this, the <code>FixedVecBuilder</code> could be designed as a state machine. It holds the chosen <code>BitWidth</code> strategy. The final <code>build()</code> method takes the input slice and executes the appropriate logic.</p>
<pre is:raw=""><code><span><span>// A look at the builder&#39;s logic flow</span></span>
<span><span>pub</span><span> </span><span>fn</span><span> </span><span>build</span><span>(</span><span>self</span><span>, </span><span>input</span><span>: &amp;[</span><span>T</span><span>]) -&gt; </span><span>Result</span><span>&lt;</span><span>FixedVec</span><span>&lt;...&gt;, </span><span>Error</span><span>&gt; {</span></span>
<span></span>
<span><span>    </span><span>let</span><span> </span><span>final_bit_width</span><span> </span><span>=</span><span> </span><span>match</span><span> </span><span>self</span><span>.bit_width_strategy {</span></span>
<span><span>        </span><span>BitWidth</span><span>::</span><span>Explicit</span><span>(</span><span>n</span><span>) =&gt; </span><span>n</span><span>,</span></span>
<span><span>        </span><span>_</span><span> =&gt; {</span></span>
<span><span>            // For Minimal or PowerOfTwo, we first find the max value.</span></span>
<span><span>            </span><span>let</span><span> </span><span>max_val</span><span> </span><span>=</span><span> </span><span>input</span><span>.</span><span>iter</span><span>().</span><span>map</span><span>(</span><span>|</span><span>v</span><span>|</span><span> </span><span>v</span><span>.</span><span>into_word</span><span>()).</span><span>max</span><span>().</span><span>unwrap_or</span><span>(</span><span>0</span><span>);</span></span>
<span><span>            </span><span>let</span><span> </span><span>min_bits</span><span> </span><span>=</span><span> (</span><span>64</span><span> - </span><span>max_val</span><span>.</span><span>leading_zeros</span><span>()).</span><span>max</span><span>(</span><span>1</span><span>) </span><span>as</span><span> </span><span>usize</span><span>;</span></span>
<span></span>
<span><span>            </span><span>match</span><span> </span><span>self</span><span>.bit_width_strategy {</span></span>
<span><span>                </span><span>BitWidth</span><span>::</span><span>Minimal</span><span> =&gt; </span><span>min_bits</span><span>,</span></span>
<span><span>                </span><span>BitWidth</span><span>::</span><span>PowerOfTwo</span><span> =&gt; </span><span>min_bits</span><span>.</span><span>next_power_of_two</span><span>(),</span></span>
<span><span>                </span><span>_</span><span> =&gt; </span><span>unreachable!</span><span>(),</span></span>
<span><span>            }</span></span>
<span><span>        }</span></span>
<span><span>    };</span></span>
<span></span>
<span><span>    // ... (rest of the logic: allocate buffer, write data) ...</span></span>
<span><span>}</span></span></code></pre>
<p>This design cleanly separates the configuration phase from the execution phase. The user can declaratively state their requirements, and the builder handles the implementation details, whether that involves a full data scan or a direct construction. For example:</p>
<pre is:raw=""><code><span><span>use</span><span> </span><span>compressed_intvec</span><span>::</span><span>prelude</span><span>::*;</span></span>
<span></span>
<span><span>let</span><span> </span><span>data</span><span>: &amp;[</span><span>u32</span><span>] </span><span>=</span><span> &amp;[</span><span>100</span><span>, </span><span>200</span><span>, </span><span>500</span><span>];</span><span> // Max value 500 requires 9 bits</span></span>
<span></span>
<span><span>// The builder will scan the data, find max=500, calculate min_bits=9,</span></span>
<span><span>// and then round up to the next power of two.</span></span>
<span><span>let</span><span> </span><span>vec_pow2</span><span>: </span><span>UFixedVec</span><span>&lt;</span><span>u32</span><span>&gt; </span><span>=</span><span> </span><span>FixedVec</span><span>::</span><span>builder</span><span>()</span></span>
<span><span>    .</span><span>bit_width</span><span>(</span><span>BitWidth</span><span>::</span><span>PowerOfTwo</span><span>)</span></span>
<span><span>    .</span><span>build</span><span>(</span><span>data</span><span>)</span></span>
<span><span>    .</span><span>unwrap</span><span>();</span></span>
<span></span>
<span><span>assert_eq!</span><span>(</span><span>vec_pow2</span><span>.</span><span>bit_width</span><span>(), </span><span>16</span><span>);</span></span></code></pre>
<blockquote>
<p><code>UFixedVec&lt;T&gt;</code> is a type alias for <code>FixedVec&lt;T, u64, LE, Vec&lt;u64&gt;&gt;</code>, the most common instantiation.</p>
</blockquote>

<p>The design of <code>FixedVec</code> allows for more than just efficient reads. We can extend it to support mutation and even thread-safe (almost atomic) concurrent access.</p>
<h2 id="mutability-proxy-objects">Mutability: Proxy Objects</h2>
<p>A core feature of <code>std::vec::Vec</code> is mutable, index-based access via <code>&amp;mut T</code>. This is fundamentally impossible for <code>FixedVec</code>. An element, such as a 10-bit integer, is not a discrete, byte-aligned entity in memory. It is a virtual value extracted from a bitstream, potentially spanning the boundary of two different <code>u64</code> words. It has no stable memory address, so a direct mutable reference cannot be formed.</p>
<p>To provide an ergonomic mutable API, we must emulate the behavior of a mutable reference. We achieve this through a proxy object pattern, implemented in a struct named <code>MutProxy</code>.</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>struct</span><span> </span><span>MutProxy</span><span>&lt;&#39;</span><span>a</span><span>, </span><span>T</span><span>, </span><span>W</span><span>, </span><span>E</span><span>, </span><span>B</span><span>&gt;</span></span>
<span><span>where</span></span>
<span><span>    </span><span>T</span><span>: </span><span>Storable</span><span>&lt;</span><span>W</span><span>&gt;,</span></span>
<span><span>    </span><span>W</span><span>: </span><span>Word</span><span>,</span></span>
<span><span>    </span><span>E</span><span>: </span><span>Endianness</span><span>,</span></span>
<span><span>    </span><span>B</span><span>: </span><span>AsRef</span><span>&lt;[</span><span>W</span><span>]&gt; + </span><span>AsMut</span><span>&lt;[</span><span>W</span><span>]&gt;,</span></span>
<span><span>{</span></span>
<span><span>    </span><span>vec</span><span>: &amp;&#39;</span><span>a</span><span> </span><span>mut</span><span> </span><span>FixedVec</span><span>&lt;</span><span>T</span><span>, </span><span>W</span><span>, </span><span>E</span><span>, </span><span>B</span><span>&gt;,</span></span>
<span><span>    </span><span>index</span><span>: </span><span>usize</span><span>,</span></span>
<span><span>    </span><span>value</span><span>: </span><span>T</span><span>,</span><span> // A temporary, decoded copy of the element&#39;s value.</span></span>
<span><span>}</span></span></code></pre>
<p>When we call a method like <code>at_mut(index)</code>, it does not return a reference. Instead, it constructs and returns a <code>MutProxy</code> instance. The proxy’s lifecycle manages the entire modification process:</p>
<ol>
<li><strong>Construction:</strong> The proxy is created. Its first action is to call the parent <code>FixedVec</code>’s internal <code>get</code> logic to read and decode the value at the specified <code>index</code>. This decoded value is stored as a temporary copy inside the proxy object itself.</li>
<li><strong>Modification:</strong> The <code>MutProxy</code> implements <code>Deref</code> and <code>DerefMut</code>, allowing the user to interact with the temporary copy as if it were the real value. Any modifications (<code>*proxy = new_value</code>, <code>*proxy += 1</code>) are applied to this local copy, not to the underlying bitstream.</li>
<li><strong>Destruction:</strong> When the <code>MutProxy</code> goes out of scope, its <code>Drop</code> implementation is executed. This is the critical step where the potentially modified value from the temporary copy is taken, re-encoded, and written back into the correct bit position in the parent <code>FixedVec</code>’s storage.</li>
</ol>
<p>This is a classic copy-on-read, write-on-drop mechanism. It provides a safe and ergonomic abstraction for mutating non-addressable data, preserving the feel of direct manipulation while correctly handling the bit-level operations under the hood. The overhead is a single read at the start of the proxy’s life and a single write at the end.</p>
<pre is:raw=""><code><span><span>use</span><span> </span><span>compressed_intvec</span><span>::</span><span>fixed</span><span>::{</span><span>FixedVec</span><span>, </span><span>UFixedVec</span><span>, </span><span>BitWidth</span><span>};</span></span>
<span></span>
<span><span>let</span><span> </span><span>data</span><span>: &amp;[</span><span>u32</span><span>] </span><span>=</span><span> &amp;[</span><span>10</span><span>, </span><span>20</span><span>, </span><span>30</span><span>];</span></span>
<span><span>let</span><span> </span><span>mut</span><span> </span><span>vec</span><span>: </span><span>UFixedVec</span><span>&lt;</span><span>u32</span><span>&gt; </span><span>=</span><span> </span><span>FixedVec</span><span>::</span><span>builder</span><span>()</span></span>
<span><span>    .</span><span>bit_width</span><span>(</span><span>BitWidth</span><span>::</span><span>Explicit</span><span>(</span><span>7</span><span>))</span></span>
<span><span>    .</span><span>build</span><span>(</span><span>data</span><span>)</span></span>
<span><span>    .</span><span>unwrap</span><span>();</span></span>
<span></span>
<span><span>// vec.at_mut(1) returns an Option&lt;MutProxy&lt;...&gt;&gt;</span></span>
<span><span>if</span><span> </span><span>let</span><span> </span><span>Some</span><span>(</span><span>mut</span><span> </span><span>proxy</span><span>) </span><span>=</span><span> </span><span>vec</span><span>.</span><span>at_mut</span><span>(</span><span>1</span><span>) {</span></span>
<span><span>    // The DerefMut trait allows us to modify the proxy&#39;s internal copy.</span></span>
<span><span>    *</span><span>proxy</span><span> </span><span>=</span><span> </span><span>99</span><span>;</span></span>
<span><span>}</span><span> // The proxy is dropped here. Its Drop impl writes 99 back to the vec.</span></span>
<span></span>
<span><span>assert_eq!</span><span>(</span><span>vec</span><span>.</span><span>get</span><span>(</span><span>1</span><span>), </span><span>Some</span><span>(</span><span>99</span><span>));</span></span></code></pre>
<p>The overhead of this approach is a single read on the proxy’s construction and a single write on its destruction, which is an acceptable trade-off for an ergonomic and safe mutable API.</p>
<h3 id="zero-copy-views">Zero-Copy Views</h3>
<p>A <code>Vec</code>-like API needs to support slicing. Creating a <code>FixedVec</code> that borrows its data (<code>B = &amp;[W]</code>) is the first step for this, but we also need a dedicated slice type to represent a sub-region of another <code>FixedVec</code> without copying data. For this we can create <code>FixedVecSlice</code>.</p>
<p>We can implement this as a classic “fat pointer” struct. It holds a reference to the parent <code>FixedVec</code> and a <code>Range&lt;usize&gt;</code> that defines the logical boundaries of the view.</p>
<pre is:raw=""><code><span><span>// A zero-copy view into a contiguous portion of a FixedVec.</span></span>
<span><span>#[derive(</span><span>Debug</span><span>)]</span></span>
<span><span>pub</span><span> </span><span>struct</span><span> </span><span>FixedVecSlice</span><span>&lt;</span><span>V</span><span>&gt; {</span></span>
<span><span>    </span><span>pub</span><span>(</span><span>super</span><span>) </span><span>parent</span><span>: </span><span>V</span><span>,</span></span>
<span><span>    </span><span>pub</span><span>(</span><span>super</span><span>) </span><span>range</span><span>: </span><span>Range</span><span>&lt;</span><span>usize</span><span>&gt;,</span></span>
<span><span>}</span></span></code></pre>
<p>The generic parameter <code>V</code> is a reference to the parent vector. This allows the same <code>FixedVecSlice</code> struct to represent both immutable (<code>V = &amp;FixedVec&lt;...&gt;</code>) and mutable (<code>V = &amp;mut FixedVec&lt;...&gt;</code>) views.</p>
<p>We can implement all the operations on the slice by translating the slice-relative index into an absolute index in the parent vector. For example, we can easily implement <code>get_unchecked</code> with this delegation:</p>
<pre is:raw=""><code><span><span>// Index translation within the slice&#39;s get_unchecked</span></span>
<span><span>pub</span><span> </span><span>unsafe</span><span> </span><span>fn</span><span> </span><span>get_unchecked</span><span>(&amp;</span><span>self</span><span>, </span><span>index</span><span>: </span><span>usize</span><span>) -&gt; </span><span>T</span><span> {</span></span>
<span><span>    </span><span>debug_assert!</span><span>(</span><span>index</span><span> </span><span>&lt;</span><span> </span><span>self</span><span>.</span><span>len</span><span>());</span></span>
<span><span>    // The index is relative to the slice, so we add the slice&#39;s start</span></span>
<span><span>    // offset to get the correct index in the parent vector.</span></span>
<span><span>    </span><span>self</span><span>.parent.</span><span>get_unchecked</span><span>(</span><span>self</span><span>.range.start + </span><span>index</span><span>)</span></span>
<span><span>}</span></span></code></pre>
<p>In this way there is no code duplication; the slice re-uses the access logic of the parent <code>FixedVec</code>.</p>
<h2 id="mutable-slices">Mutable Slices</h2>
<p>For mutable slices (<code>V = &amp;mut FixedVec&lt;...&gt;</code> ), we can provide mutable access to the slice’s elements. We can easily implement the <code>at_mut</code> method on <code>FixedVecSlice</code> using the same principle of index translation:</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>fn</span><span> </span><span>at_mut</span><span>(&amp;</span><span>mut</span><span> </span><span>self</span><span>, </span><span>index</span><span>: </span><span>usize</span><span>) -&gt; </span><span>Option</span><span>&lt;</span><span>MutProxy</span><span>&lt;&#39;</span><span>_</span><span>, </span><span>T</span><span>, </span><span>W</span><span>, </span><span>E</span><span>, </span><span>B</span><span>&gt;&gt; {</span></span>
<span><span>    </span><span>if</span><span> </span><span>index</span><span> </span><span>&gt;=</span><span> </span><span>self</span><span>.</span><span>len</span><span>() {</span></span>
<span><span>        </span><span>return</span><span> </span><span>None</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>    // The index is translated to the parent vector&#39;s coordinate space.</span></span>
<span><span>    </span><span>Some</span><span>(</span><span>MutProxy</span><span>::</span><span>new</span><span>(&amp;</span><span>mut</span><span> </span><span>self</span><span>.parent, </span><span>self</span><span>.range.start + </span><span>index</span><span>))</span></span>
<span><span>}</span></span></code></pre>
<p>A mutable slice borrows the parent <code>FixedVec</code> mutably. This means that while the slice exists, the parent vector cannot be accessed directly due to Rust’s borrowing rules. Let’s consider the following situation: we may need to split a mutable slice into two non-overlapping mutable slices. This is common for example in algorithms that operate on sub-regions of an array. However, implementing such a method requires to use some unsafe code. The method, let’s say <code>split_at_mut</code>, must produce two <code>&amp;mut</code> references from a single one. In order to be safe, we must prove to the compiler that the logical ranges they represent (<code>0..mid</code> and <code>mid..len</code>) are disjoint.</p>
<pre is:raw=""><code><span><span>pub</span><span> </span><span>fn</span><span> </span><span>split_at_mut</span><span>(&amp;</span><span>mut</span><span> </span><span>self</span><span>, </span><span>mid</span><span>: </span><span>usize</span><span>) -&gt; (</span><span>FixedVecSlice</span><span>&lt;&amp;</span><span>mut</span><span> </span><span>Self</span><span>&gt;, </span><span>FixedVecSlice</span><span>&lt;&amp;</span><span>mut</span><span> </span><span>Self</span><span>&gt;) {</span></span>
<span><span>    </span><span>assert!</span><span>(</span><span>mid</span><span> </span><span>&lt;=</span><span> </span><span>self</span><span>.len, </span><span>&#34;mid &gt; len in split_at_mut&#34;</span><span>);</span></span>
<span><span>    // SAFETY: The two slices are guaranteed not to overlap.</span></span>
<span><span>    </span><span>unsafe</span><span> {</span></span>
<span><span>        </span><span>let</span><span> </span><span>ptr</span><span> </span><span>=</span><span> </span><span>self</span><span> </span><span>as</span><span> *</span><span>mut</span><span> </span><span>Self</span><span>;</span></span>
<span><span>        </span><span>let</span><span> </span><span>left</span><span> </span><span>=</span><span> </span><span>FixedVecSlice</span><span>::</span><span>new</span><span>(&amp;</span><span>mut</span><span> *</span><span>ptr</span><span>, </span><span>0</span><span>..</span><span>mid</span><span>);</span></span>
<span><span>        </span><span>let</span><span> </span><span>right</span><span> </span><span>=</span><span> </span><span>FixedVecSlice</span><span>::</span><span>new</span><span>(&amp;</span><span>mut</span><span> *</span><span>ptr</span><span>, </span><span>mid</span><span>..</span><span>self</span><span>.</span><span>len</span><span>());</span></span>
<span><span>        (</span><span>left</span><span>, </span><span>right</span><span>)</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>This combination of a generic slice struct and careful pointer manipulation allows us to build a rich, safe, and zero-copy API for both immutable and mutable views, mirroring Rust’s native slice</p>

<p>Our <code>FixedVec</code> works pretty well at the current state, but its strengths are tied to two key assumptions: a single thread of execution and uniformly bounded data. Real-world systems often challenge both. This opens up two distinct paths for us to extend our design: one to handle concurrency, and another to adapt to more complex data distributions.</p>
<h2 id="concurrent-access">Concurrent Access</h2>
<p>The design of <code>FixedVec</code> is fundamentally single-threaded. Its mutable access relies on direct writes to the underlying bit buffer, a model that offers no guarantees in a concurrent setting. The core conflict lies between our data layout and the hardware’s atomic primitives. CPU instructions like compare-and-swap operate on aligned machine words, typically <code>u64</code>. Our elements, however, are packed at arbitrary bit offsets and can span the boundary between two words.</p>
<p>This means a single logical update—modifying one element—might require writing to two separate <code>u64</code> words. Performing this as two distinct atomic writes would create a race condition, leaving the data in a corrupt state if another thread reads between them. A single atomic write to one of the underlying <code>u64</code> words would be equally disastrous, as it could simultaneously alter parts of two different elements. The problem then is how to build atomic semantics on top of a non-atomic memory layout. In the next post, we will construct a solution that provides thread-safe, atomic operations for our bit-packed vector.</p>
<h2 id="variable-length-encoding">Variable Length Encoding</h2>
<p>Let’s consider the following scenario: we have a large collection of integers, all of which are small, say in the range <code>[0, 255]</code>, but with a few outliers that are much larger, perhaps up to <code>u64::MAX</code>. If we were to use our <code>FixedVec</code> with a <code>bit_width</code> of 64 to accommodate the outliers, we would waste a significant amount of memory on the small integers. Conversely, if we chose a smaller <code>bit_width</code>, we would be unable to represent the outliers at all.</p>
<p>The fixed-width model rests on that critical assumption of uniformly bounded data. Its performance comes from this predictability, but this rigid structure is also its main weakness.</p>
<p>For skewed data distributions, we need a different model. Instead of a fixed number of bits per element, we can use variable-length instantaneous codes, where smaller numbers are represented by shorter bit sequences. This gives us excellent compression, but it breaks our O(1) random access guarantee. We can no longer compute the location of the i-th element with a simple multiplication. The solution is to trade a small amount of space for a speedup in access time. We can build a secondary index that stores the bit-offset of every k-th element. This sampling allows us to seek to a nearby checkpoint and decode sequentially from there, restoring amortized O(1) access. In a future article, we’ll explore this second vector type, its own set of performance trade-offs, and how we can choose the best encoding for our data.</p>
<hr/>
<p>We will explore this two paths in future articles. In the meantime, if you want to try them out, they are both already implemented in the library. You can find the atomic version in the <a href="https://docs.rs/compressed-intvec/latest/compressed_intvec/fixed/atomic/index.html">atomic</a> module and the variable-length version in the <a href="https://docs.rs/compressed-intvec/latest/compressed_intvec/variable/struct.IntVec.html">variable</a> module.</p>
    </article></div>
  </body>
</html>
