<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://metaphysic.ai/nvidias-implicit-warping-is-a-potentially-powerful-deepfake-technique/">Original</a>
    <h1>Nvidia’s Implicit Warping is a potentially powerful deepfake technique</h1>
    
    <div id="readability-page-1" class="page"><div data-id="230321ec" data-element_type="widget" data-widget_type="theme-post-content.default"><div><div data-elementor-type="wp-post" data-elementor-id="3118"><section data-id="84cf2eb" data-element_type="section"><div><div data-id="a37a300" data-element_type="column"><div><div data-id="bef57e7" data-element_type="widget" data-widget_type="text-editor.default"><p>Over the past 10-20 years, and particularly in recent years, the computer vision research community has produced an abundance of frameworks capable of taking a single image and using it to perform ‘deepfake puppetry’ – the use of the facial and body movements of one person to simulate a secondary, fictitious identity.</p></div></div></div></div></section><section data-id="de2b501" data-element_type="section"><div><div data-id="a4d06ba" data-element_type="column"><div><div data-id="e2b5195" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="474" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/FOMM.gif" alt="FOMM"/><figcaption> A collaboration between researchers across Europe and Snap Inc., 2019&#39;s First Order Motion Model (FOMM) captured the imagination of some viewers at the time, even though it was clear that some of the movements it tried to simulate were literally a &#39;stretch&#39; (see the middle row Mona Lisa above, which does not have enough base information to pull off the angle it is attempting) . Source: https://aliaksandrsiarohin.github.io/first-order-model-website/</figcaption></figure></div></div></div></div></div></section><section data-id="4423871" data-element_type="section"><div><div data-id="e3729b5" data-element_type="column"><div><div data-id="ba2c553" data-element_type="widget" data-widget_type="text-editor.default"><div><p>This plethora of academic interest hails back to <a href="https://people.csail.mit.edu/drdaniel/research/vlasic-2005-ftm.pdf"><span>at least 2005</span></a>, and includes projects such as <a href="http://www.graphics.stanford.edu/~niessner/thies2015realtime.html"><span>Real-time Expression Transfer for Facial Reenactment</span></a>, <a href="http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf"><span>Face2Face</span></a>, <a href="https://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf"><span>Synthesizing Obama</span></a>, <a href="https://github.com/aayushbansal/Recycle-GAN"><span>Recycle-GAN</span></a>, <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Wayne_Wu_Learning_to_Reenact_ECCV_2018_paper.pdf"><span>ReenactGAN</span></a>, <a href="https://arxiv.org/pdf/2012.03065.pdf"><span>Dynamic Neural Radiance Fields</span></a>, and many others, diversely leveraging the limited available technologies, such as Generative Adversarial Networks (<a href="https://metaphysic.ai/the-future-of-generative-adversarial-networks-in-deepfakes/"><span>GANs</span></a>), Neural Radiance Fields (<a href="https://metaphysic.ai/nerf-successor-deepfakes/"><span>NeRF</span></a>) and <a href="https://metaphysic.ai/future-autoencoder-deepfakes/"><span>autoencoders</span></a>.</p><p>Not all of these initiatives attempt to derive video from a single frame; some of them perform computationally expensive calculations of each frame in a video, which is effectively exactly what deepfakes (in the sense of AI-powered viral celebrity impersonations) do. But since they are operating with <i>less information</i>, that kind of approach requires <i>per-clip training</i> – which is a step down from the open source approach of <a href="https://github.com/iperov/DeepFaceLab"><span>DeepFaceLab</span></a> or <a href="https://faceswap.dev/"><span>FaceSwap</span></a>, where one can train and use models capable of imposing an identity into <i>any</i> number of clips, not just one.<span> </span></p><p>The others attempt to derive multiple poses and expressions from a <i>single face</i> or full-body representation; but this kind of approach usually only works with the most expressionless and immobile of subjects – and usually only in a relatively static ‘talking head’ situation, since there are no ‘sudden changes’ in facial expression or pose that the network will have to account for.</p></div></div></div></div></div></section><section data-id="bc32791" data-element_type="section"><div><div data-id="37889c7" data-element_type="column"><div><div data-id="23dbcd9" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="406" height="197" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/statue-2.gif" alt=""/><figcaption>FOMM gives life to a mere statue, driven by footage of a real person – the same kind of &#39;deepfake puppetry&#39; that can also now be done with tweening packages such as EbSynth, combined with fictitious of altered images from Stable Diffusion and other image synthesis models (see below). Source: https://www.youtube.com/watch?v=3Mi1Ofdc5t4</figcaption></figure></div></div></div></div></div></section><section data-id="aedee79" data-element_type="section"><div><div data-id="36db546" data-element_type="column"><div><div data-id="44cebfc" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Though some of these techniques and approaches gained public traction in the less desensitized time before the wider proliferation of deepfake technologies and – lately – <a href="https://metaphysic.ai/stable-diffusion-is-video-coming-soon/"><span>latent diffusion</span></a> image synthesis methods, they all seemed to arrive at the same end via slightly differing means, with their applicability limited and their versatility in question.<span> </span></p><p>To be honest, we’re a little immured to this kind of thing now, and more dazzling innovations have diverted our attention.</p></div></div></div></div></div></section><section data-id="3b2ffd8" data-element_type="section"></section><section data-id="2d6806a" data-element_type="section"><div><div data-id="058838c" data-element_type="column"><div><div data-id="1fee783" data-element_type="widget" data-widget_type="text-editor.default"><p>NVIDIA’s computer vision research division has been developing a similar kind of system over the past few years, and lately the company has presented it in such a dull context (i.e., the by-now formulaic exact recreation of source videos via machine learning, which typifies this strand of research), that many may not have noticed how significant it could be.</p></div></div></div></div></section><section data-id="1f1c854" data-element_type="section"><div><div data-id="4b792c1" data-element_type="column"><div><div data-id="2f79587" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="800" height="377" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/talking_head_1-2.gif" alt=""/><figcaption>Rather than trying to get all the necessary pose information from a single frame, Implicit Warping can &#39;tween&#39; between multiple frames, and even as few as two, in accordance with some of the oldest principles of traditional animation – a facility that is either absent or works very poorly in all rival or prior frameworks. Source: https://deepimagination.cc/implicit_warping/</figcaption></figure></div></div></div></div></div></section><section data-id="f95a2ac" data-element_type="section"><div><div data-id="5a23728" data-element_type="column"><div><div data-id="bf89d44" data-element_type="widget" data-widget_type="text-editor.default"><div><p>NVIDIA’s recently-published <a href="https://arxiv.org/pdf/2210.01794.pdf"><span>paper</span></a> <i>Implicit Warping for Animation with Image Sets</i>, has done little to attract further attention to the project; likewise the extensive accompanying videos at the <a href="https://deepimagination.cc/implicit_warping/"><span>main project page</span></a> and the <a href="https://deepimagination.cc/implicit_warping/video_viewer.html"><span>additional results</span></a> page – because, ironically, the more you succeed at recreating a source video by methods of this nature, the less evident the significance of the achievement is, with the results appearing redundant and repetitive of previous efforts.</p><p>In fact, Implicit Warping has extraordinary potential to create hyper-realistic deepfake motion, to an extent that none of its predecessors have been equipped to do.</p></div></div></div></div></div></section><section data-id="757af0c" data-element_type="section"></section><section data-id="e8dd6b3" data-element_type="section"><div><div data-id="5930e1b" data-element_type="column"><div><div data-id="f9a5890" data-element_type="widget" data-widget_type="text-editor.default"><p>The difference with the new method, titled <i>Implicit Warping</i>, is that, harking back to the earliest days of animation, it can <a href="https://media.disneyanimation.com/uploads/production/publication_asset/120/asset/Whi10.pdf"><span>‘tween’</span></a> two (or any arbitrary number of) keyframes, instead of attempting to torment a single image into a range of dynamic poses and expressions that no single image can possible yield.</p></div></div></div></div></section><section data-id="b0c7b0c" data-element_type="section"><div><div data-id="87903f7" data-element_type="column"><div><div data-id="2b29ac0" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="1000" height="443" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/tinkerbell.jpg" alt="" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/tinkerbell.jpg 1000w, https://metaphysic.ai/wp-content/uploads/2022/10/tinkerbell-300x133.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/10/tinkerbell-768x340.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"/><figcaption>From a 2010 Disney paper, the earliest principles of inbetweening – where more junior animators would receive &#39;master frames&#39; or keyframes from senior animators, and would be tasked with producing the interstitial frames. Source: https://media.disneyanimation.com/uploads/production/publication_asset/120/asset/Whi10.pdf</figcaption></figure></div></div></div></div></div></section><section data-id="56196a0" data-element_type="section"><div><div data-id="b231d5c" data-element_type="column"><div><div data-id="32d1308" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Tested against prior versions, the researchers of the new paper found that the quality of results from the older approaches actually <i>deteriorates</i> with extra ‘keyframes’, whereas the new method, in line with the logic of animation itself, improves in a quite linear manner as the number of keyframes rises.</p><p>But, impressively, Implicit Warping can recreate video with as little as two frames, depending on the motion in question.</p></div></div></div></div></div></section><section data-id="77b183e" data-element_type="section"><div><div data-id="4852712" data-element_type="column"><div><div data-id="7f3302c" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="798" height="374" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/implicit-1.gif" alt=""/><figcaption>The video recreation in the far right column uses only the two frames depicted in the first column, while Implicit Warping derives the entire motion from a combination of the source clip and the face information in the frames.</figcaption></figure></div></div></div></div></div></section><section data-id="a41da09" data-element_type="section"><div><div data-id="2d40f09" data-element_type="column"><div><div data-id="ce6c383" data-element_type="widget" data-widget_type="text-editor.default"><div><p>If something abrupt should occur in the middle of the clip, such as an event or expression that is not represented either in the starting or end frame, Implicit Warping can <i>add a frame at that point</i>, and the added information will feed into the clip-wide attention mechanisms for the entire clip.</p><p>This kind of keyframed approach is currently being pursued both by amateurs and professional developers interested in expanding the video potential of the Stable Diffusion text-to-image synthesis system, and many (including myself – scroll down <a href="https://metaphysic.ai/the-road-to-realistic-full-body-deepfakes/"><span>at this link</span></a>) have experimented with using the non-AI tweening software <a href="https://ebsynth.com/"><span>EbSynth</span></a> to create deepfake puppetry for complex and changing motion, by adding multiple Stable Diffusion renders to a video sequence powered by a real person.</p></div></div></div></div></div></section><section data-id="87bd539" data-element_type="section"><div><div data-id="ff22ea1" data-element_type="column"><div><div data-id="fc68372" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="500" height="380" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/EbSynth-1.gif" alt=""/><figcaption>Examples of Stable Diffusion output, animated via EbSynth, from users at Reddit*.</figcaption></figure></div></div></div></div></div></section><section data-id="34e1297" data-element_type="section"><div><div data-id="b87eaf0" data-element_type="column"><div><div data-id="9c221d5" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The power and potential of Implicit Warping notably outstrips not only prior works, but also EbSynth itself, which was not designed for this task, and, arguably, is difficult to adapt to it.</p><p>It seems likely that the researchers have chosen not to demonstrate actual ‘transformations’ of this kind due to a growing timidity in the image synthesis research sector regarding techniques that could as easily be used for deepfaking as for their chosen purpose. Acknowledging this, the by-now standard deepfake disclaimer in the new paper steers the customary path between enthusiasm and caution:<span> </span></p><p><i>‘Our method has the potential for negative impact if used to create deepfakes. Via the use of cross-identity transfer and speech synthesis, a malicious actor can create faked videos of a person, resulting in identity theft or dissemination of fake news. However, in controlled settings, the same technology can also be used for entertainment purposes.’</i></p><p>The paper also notes the potential of the system to power neural video reconstruction, in frameworks such as Google’s <a href="https://www.cnet.com/tech/computing/google-starts-testing-holographic-video-chats-at-real-offices/"><span>Project Starline</span></a>, where the work of recreating the correspondent appears to occur primarily on the client-side, using sparse motion information from the person at the other end. This schema is of <a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression"><span>growing interest</span></a> to the research community, and is intended also to enable low bandwidth teleconferencing, by sending either pure motion data, or sparsely-intervalled keyframes that will be interpreted and interpolated into full, HD video on arrival.</p></div></div></div></div></div></section><section data-id="70d718f" data-element_type="section"></section><section data-id="66f398d" data-element_type="section"><div><div data-id="72fb754" data-element_type="column"><div><div data-id="238e027" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Implicit Warping departs from prior approaches such as FOMM, <a href="https://arxiv.org/pdf/1812.08861.pdf"><span>Monkey-Net</span></a>, and NVIDIA’s own <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf"><span>face-vid2vid</span></a>, which <a href="https://www.youtube.com/watch?v=nLYg9Waw72U"><span>use</span></a> explicit warping to map out a temporal sequence into which information extracted from the source face and the controlling motion must be adapted, and to which it must conform. The final mapping of keypoints is fairly rigid under this regime.</p><p>By contrast, Implicit Warping uses a cross-modal attention layer that produces a workflow with less pre-defined bootstrapping, and which can adapt to input from multiple frames. Neither does the workflow require warping on a per-keypoint basis, which allows the system to select the most apposite features from a range of images.</p></div></div></div></div></div></section><section data-id="82ff10a" data-element_type="section"><div><div data-id="a54870d" data-element_type="column"><div><div data-id="af21073" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="1200" height="642" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/workflow-for-implicit-warping.jpg" alt="The workflow for Implicit Warping." data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/workflow-for-implicit-warping.jpg 1200w, https://metaphysic.ai/wp-content/uploads/2022/10/workflow-for-implicit-warping-300x161.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/10/workflow-for-implicit-warping-1024x548.jpg 1024w, https://metaphysic.ai/wp-content/uploads/2022/10/workflow-for-implicit-warping-768x411.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px"/><figcaption>The workflow for Implicit Warping.</figcaption></figure></div></div></div></div></div></section><section data-id="8f496d2" data-element_type="section"><div><div data-id="cf875be" data-element_type="column"><div><div data-id="332722f" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Nonetheless, the new system repurposes the keypoint prediction components in the prior FOMM framework, ultimately encoding the derived spatial driving keypoint representations with a simple <a href="https://www.educative.io/answers/what-is-u-net"><span>U-net</span></a>. A separate U-net is used to encode the source image in tandem with the derived spatial representation, and both networks can operate at a range of resolutions at 64px (for 256px square output), to 384x384px.</p><p>Because this ‘mechanization’ can’t automatically account for all the possible variations of pose and movement in any given video, additional necessary, keyframes can be added <i>ad hoc</i>. Without this ability to intervene, keys with inadequate point-similarity to the target motion would automatically be uprated, lowering the quality of output.</p><p>The researchers explain:</p><p><i>‘While it is the key most similar to the query in the given set of keys, it may not be similar enough to produce a good output. For example, suppose the source image has a face with lips closed, while the driving image has one with lips open and teeth exposed.<span> </span></i></p><p><i>‘In this case, there will be no key (and value) in the source image appropriate for the mouth region of the driving image. We overcome this issue by allowing our method to learn additional image-independent key-value pairs, which can be used in the case of missing information in the source image. These additional keys and values are concatenated to the keys and values obtained from the source image.’</i></p><p>Though the current implementation is quite fast, at around 10FPS on 512x512px images, the researchers believe that the pipeline can be optimized in future versions by a factored <a href="https://arxiv.org/pdf/2104.13918.pdf"><span>I-D attention layer</span></a>, or a spatial-reduction attention (SRA) layer (i.e., a <a href="https://arxiv.org/pdf/2102.12122"><span>pyramid vision transformer</span></a>).<span> </span></p></div></div></div></div></div></section><section data-id="f713050" data-element_type="section"><div><div data-id="528b96b" data-element_type="column"><div><div data-id="f94cecc" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="713" height="358" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/non-local-attention.jpg" alt="" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/non-local-attention.jpg 713w, https://metaphysic.ai/wp-content/uploads/2022/10/non-local-attention-300x151.jpg 300w" sizes="(max-width: 713px) 100vw, 713px"/><figcaption>Here Implicit Warping has derived a frontal image (left) from the genuine source image, with the mapped points indicated in various colors. Because Implicit Warping uses global rather than local attention, it can anticipate factors that previous efforts cannot, such as objects that are about to become dis-occluded.</figcaption></figure></div></div></div></div></div></section><section data-id="5c80cbb" data-element_type="section"></section><section data-id="b9e4a78" data-element_type="section"><div><div data-id="c26ffef" data-element_type="column"><div><div data-id="09da793" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The researchers tested the system on the <a href="https://github.com/AliaksandrSiarohin/video-preprocessing"><span>VoxCeleb2</span></a> dataset, the more challenging <a href="https://github.com/AliaksandrSiarohin/video-preprocessing"><span>TED Talk</span></a> dataset, and, for ablation studies, the <a href="https://github.com/deepimagination/TalkingHead-1KH"><span>TalkingHead-1KH</span></a> set, comparing baselines between 256x256px and the full 512x512px resolution. Metrics used were Frechet Inception Distance (<a href="https://papers.nips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf"><span>FID</span></a>), <a href="https://arxiv.org/pdf/1801.03924"><span>LPIPS</span></a> over <a href="https://www.mathworks.com/help/deeplearning/ref/alexnet.html"><span>AlexNet</span></a>, and peak signal-to-noise ratio (<a href="https://www.mathworks.com/help/vision/ref/psnr.html"><span>PSNR</span></a>).<span> </span></p><p>Competing frameworks used for the tests were FOMM and face-vid2vid, in addition to <a href="https://arxiv.org/pdf/2104.11280.pdf"><span>AA-PCA</span></a>. Since prior methods had little or no capacity to use multiple keyframes, which is the primary innovation of Implicit Warping, the researchers devised like-for-like testing methodologies.</p><p>First, Implicit Warping was tested on the ‘home ground’ of the former methods – as a way to derive motion from a single keyframe.</p></div></div></div></div></div></section><section data-id="ecfdca0" data-element_type="section"><div><div data-id="51eccd9" data-element_type="column"><div><div data-id="9beea7c" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="900" height="255" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/results-single-frame.jpg" alt="" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/results-single-frame.jpg 900w, https://metaphysic.ai/wp-content/uploads/2022/10/results-single-frame-300x85.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/10/results-single-frame-768x218.jpg 768w" sizes="(max-width: 900px) 100vw, 900px"/><figcaption>Results for motion transfer from a single frame, against networks specifically designed for this task. For up arrows, larger is better; down arrows, smaller is better.</figcaption></figure></div></div></div></div></div></section><section data-id="c0a4b23" data-element_type="section"><div><div data-id="a9419f2" data-element_type="column"><div><div data-id="a59f914" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Here Implicit Warping outperforms most of the competing methods across most of the metrics, but is not allowed to get to fifth gear, and loses some position to architectures optimized to the task.</p><p>Next, the researchers tested for multiple-keyframe reconstruction, using sequences of at most 180 frames, and selected interstitial frames. Here Implicit Warping achieves a convincing overall victory:</p></div></div></div></div></div></section><section data-id="4ee382e" data-element_type="section"><div><div data-id="e596697" data-element_type="column"><div><div data-id="be827a5" data-element_type="widget" data-widget_type="image.default"><p><img width="900" height="413" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/results-multiple-frame.jpg" alt="results-multiple-frame" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/results-multiple-frame.jpg 900w, https://metaphysic.ai/wp-content/uploads/2022/10/results-multiple-frame-300x138.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/10/results-multiple-frame-768x352.jpg 768w" sizes="(max-width: 900px) 100vw, 900px"/></p></div></div></div></div></section><section data-id="ac8f436" data-element_type="section"><div><div data-id="2560947" data-element_type="column"><div><div data-id="b0c1120" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The researchers note:</p><p><i>‘As the number of source images increases, our method obtains better reconstructions as indicated by the improving scores on all metrics. However, reconstructions by prior work get worse as the number of source images increases, contrary to expectation.’<span> </span></i></p></div></div></div></div></div></section><section data-id="bef912a" data-element_type="section"><div><div data-id="3e3a0d2" data-element_type="column"><div><div data-id="cd87e93" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="1200" height="306" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/source-and-driving-image.jpg" alt="The challenge here is to recreate the &#39;driving&#39; image (second from left) using only the information from the source image (far left). The third and fourth pictures show how previous frameworks compromised either on detail or positioning (or both), while Implicit Warping, far right, has successfully recreated the frame." data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/source-and-driving-image.jpg 1200w, https://metaphysic.ai/wp-content/uploads/2022/10/source-and-driving-image-300x77.jpg 300w, https://metaphysic.ai/wp-content/uploads/2022/10/source-and-driving-image-1024x261.jpg 1024w, https://metaphysic.ai/wp-content/uploads/2022/10/source-and-driving-image-768x196.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px"/><figcaption>The challenge here is to recreate the &#39;driving&#39; image (second from left) using only the information from the source image (far left). The third and fourth pictures show how previous frameworks compromised either on detail or positioning (or both), while Implicit Warping, far right, has successfully recreated the frame.</figcaption></figure></div></div></div></div></div></section><section data-id="8fe4337" data-element_type="section"><div><div data-id="fd75e23" data-element_type="column"><div><div data-id="c66f7d4" data-element_type="widget" data-widget_type="text-editor.default"><div><p>The system is not infallible: in the case of very extreme angles of a head, and where no keyframe offers a more confrontational pose, Implicit Warping has difficulty interpreting a view; however, as we have <a href="https://metaphysic.ai/to-uncover-a-deepfake-video-call-ask-the-caller-to-turn-sideways/"><span>noted elsewhere</span></a>, without the relevant data, this is practically an impossible task for any framework.</p><p>The results were presented also in a qualitative survey to Amazon Mechanical Turk (AMT) workers, who rated Implicit Warping’s results above the rival methods.<span> </span></p></div></div></div></div></div></section><section data-id="4d52075" data-element_type="section"><div><div data-id="14a13b7" data-element_type="column"><div><div data-id="00626d0" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="630" height="301" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/user-study.jpg" alt="" data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/user-study.jpg 630w, https://metaphysic.ai/wp-content/uploads/2022/10/user-study-300x143.jpg 300w" sizes="(max-width: 630px) 100vw, 630px"/><figcaption>Results from the qualitative user study.</figcaption></figure></div></div></div></div></div></section><section data-id="9d48627" data-element_type="section"><div><div data-id="e8867e6" data-element_type="column"><div><div data-id="ccdc866" data-element_type="widget" data-widget_type="text-editor.default"><p>Each worker was shown a pair of videos from a total of 360 and 128 clips, from TalkingHead-1KH and Ted Talk, respectively.<span> </span></p></div></div></div></div></section><section data-id="e832c03" data-element_type="section"><div><div data-id="8eb4cb1" data-element_type="column"><div><div data-id="a52c2b0" data-element_type="widget" data-widget_type="image.default"><div><figure> <img width="650" height="451" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://metaphysic.ai/wp-content/uploads/2022/10/AMT-interface.jpg" alt="The user interface presented to the AMT workers for the qualitative study." data-srcset="https://metaphysic.ai/wp-content/uploads/2022/10/AMT-interface.jpg 650w, https://metaphysic.ai/wp-content/uploads/2022/10/AMT-interface-300x208.jpg 300w" sizes="(max-width: 650px) 100vw, 650px"/><figcaption>The user interface presented to the AMT workers for the qualitative study.</figcaption></figure></div></div></div></div></div></section><section data-id="393c3ea" data-element_type="section"><div><div data-id="c5c2b3d" data-element_type="column"><div><div data-id="87b65ce" data-element_type="widget" data-widget_type="heading.default"><p><h2>What Could We Do With Implicit Warping?</h2></p></div></div></div></div></section><section data-id="f57b667" data-element_type="section"><div><div data-id="20be795" data-element_type="column"><div><div data-id="0617c72" data-element_type="widget" data-widget_type="text-editor.default"><div><p>Given access to this kind of framework, users would be able to produce far more coherent and longer video simulations and full-body deepfake footage, all of which could feature a far greater range of motion than any of the frameworks that the system has been trialed against.</p><p>The value of this kind of approach is in the extent to which a keyframe may be <i>difficult to produce</i> – not something which the paper addresses, since the authors choose only to recreate existing footage.<span> </span></p><p>Reproducing some kind of extraordinary figure in an adequate number of poses to populate the keyframe necessary for complex motion may involve, for instance, the elaborate construction of CGI models; else the training (brief though it is) of temporally consistent DreamBooth models for Stable Diffusion, capable of depicting a character in different poses without any other physical changes in their appearance (which is otherwise a challenge in latent diffusion models, which may give you something ‘a little different’ every time).</p><p>Thus, a system such as Implicit Warping could enable simulated or deepfaked clips of a length and variability that no other interpretive framework has yet offered. As extraordinary actions occur in the driving source clip, additional keyframes could cover that extra data as necessary, without either needing continuous and contiguous rendering, or to hope that a single frame of data might be enough to populate the clip.<span> </span></p><p>It’s a potentially powerful animation tool, apparently masquerading as yet another constrained ‘talking head’ generator, and is perhaps being downplayed by NVIDIA for ‘optical’ rather than practical reasons.</p><p>* <i>Sources:<small> https://old.reddit.com/r/StableDiffusion/comments/x8gdtu/stable_difussion_img2img_ebsynth_is_a_very/
https://old.reddit.com/r/StableDiffusion/comments/xn2p5s/playing_with_ebsynth_stable_diffusion/
https://old.reddit.com/r/StableDiffusion/comments/xka1do/i_used_sd_and_ebsynth_for_some_horror_makeup_vfx/</small></i></p></div></div></div></div></div></section></div></div></div></div>
  </body>
</html>
