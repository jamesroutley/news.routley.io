<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://awk.dev/eda.html">Original</a>
    <h1>Exploratory Data Analysis Using Awk</h1>
    
    <div id="readability-page-1" class="page">

<h2>Exploratory Data Analysis for Humanities Data</h2>

<h5>by Brian Kernighan</h5>

<p> <SPAN size="-1"> Updated
Sun Sep 10 13:58:41 EDT 2023
 </SPAN>

</p><h4>Introduction</h4>

In the spring of 2023, I co-taught a course called
 <a href="https://humstudies.princeton.edu/courses/literature-as-data/">
 <i>Literature as Data</i></a> with my friend and colleague
 <a href="https://english.princeton.edu/people/meredith-martin">Meredith Martin</a>,
professor of English at Princeton, and director of the
 <a href="https://cdh.princeton.edu">Center for Digital Humanities</a>.

<p> The course was very much an experiment, an attempt to combine
literary study with computing.  The 17 surviving students were largely sophomores.
About two thirds of them were hard-core humanities majors; the rest were
potential or actual CS majors or other STEM concentrators.  The class
met once a week for three hours, 12 weeks in all.

</p><p> One of the goals of the course was to try to teach enough computing 
to a mostly non-technical and definitely not computer-experienced
population that they could use computers to do an interesting and new
(to them) exploration of some dataset that they found intriguing.

</p><p> After much discussion, Meredith and I decided that for the
programming aspects, we would devote half of each class meeting to a
&#34;studio&#34; where I would lead the students through hands-on computing
exercises on some small dataset, followed by having the students do
similar exercises on larger datasets outside the class.

</p><p> We planned to spend one week on Unix file system and command-line
basics, a week on grep and similar tools (including a real text editor),
a week on Awk, and a couple of weeks on Python.  Not surprisingly,
everything took longer than expected, and a fair number of students
struggled mightily in spite of help from their peers, Meredith and
myself, and two exceptional colleagues from the CDH, Sierra Eckert and Ryan Heuser.

</p><p> More about the course can be found elsewhere (at least when we get
around to writing it).  The course had a web site at
 <a href="http://www.hum307.com">hum307.com</a>, though after a
while we stopped using that in favor of Google Docs -- information
was spread out over too many sites, and in addition we had to limit
access since some of our datasets were not public.

</p><h4> Awk vs Pandas </h4>

Arguably, most humanities computing uses Python and
the Pandas library to analyze structured data that often began life
as a CSV file.  If you&#39;re looking for a single language solution,
this is an excellent way to go.  But in my experience, admittedly
biased, Unix command-line tools and Awk can be especially
useful in the initial stages of exploring a new dataset.  They are
better at counting things, looking for anomalies and outliers,
and coping with data that isn&#39;t quite in the right format.
And, again a biased view, looping over a set of input lines seems
more natural than the dataframe selectors that Pandas favors.

<h4> Awk for EDA </h4>

<p> The purpose of this essay is to talk about how we used Awk in a
different EDA setting from the examples in the book.  In keeping with
the &#34;literature&#34; theme of the course, in the first few weeks we asked
students to look at poetry, specifically the <i>Sonnets from the
Portuguese</i> by Elizabeth Barrett Browning, and Shakespeare&#39;s sonnets.
We used grep and wc to count things like the number of times specific
words appeared, and also to observe something unexpected: even though
sonnets always have 14 lines, Shakespeare&#39;s Sonnet 126 has only 12
lines, and Sonnet 99 has 15 lines.  This was a good lesson about
literature but also about how real data does not always behave as
expected.

</p><p> This essay parallels the EDA discussion in Chapter 3 of the new Awk
book, but using different data, a metadata file about 18-th century
sonnets originally curated by Professor
 <a href="https://english.stanford.edu/people/mark-algee-hewitt"> Mark Algee-Hewitt</a>
of the Stanford Literary Lab.  We are very grateful to Mark for
kindly allowing us to use this metadata for the class and for this
page.

</p><p>
 <a href="https://www.hum307.com/studio3.html">What we did in class</a>
and the outside class exercises
can be found at <tt>hum307.com</tt>.


</p><p> The metadata file is <a href="https://awk.dev/18.csv">18.csv</a>.  The name is
not strictly accurate: we converted the original CSV into a file
with fields separated by slashes so that it would work with any version
of Awk.  We also removed some attributes from Mark&#39;s original dataset,
leaving us with 7 columns.

</p><pre>$ wc 18.csv
     508    3499   36876 18.csv
$ sed 1q 18.csv
author_dob/author_dod/author_fname/author_lname/num_lines/title_main/title_sub
$
</pre>

<p> Using the head-tail code of Section 2.2 of the book, we can look at the start
and end of the contents:

</p><pre>$ headtail 18.csv
author_dob/author_dod/author_fname/author_lname/num_lines/title_main/title_sub
1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET I./Sent to Miss  , with a Braid of Hair.
1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET II./Sent to Mrs.  , with a Song.
...
1740/1809/Hugh/Downman/14/SONNET II./[Though here almost eternal Winter reigns]
1740/1809/Hugh/Downman/14/SONNET III./[When Recollection stirs up in the mind]
1740/1809/Hugh/Downman/14/SONNET IV./[Now is the feudal vassalage destroy&#39;d]
$
</pre>

Notice that the first sonnets only have 8 lines, echoing the observation above.


<h4> Validation </h4>

The first step is data validation: does the data satisfy basic
criteria like the right number of fields in each record.
In a separate command, we can look at the distribution of
number of lines in each sonnet:

<pre>$ awk -F/ &#39;{print NF}&#39; 18.csv | sort -u
7
$ awk -F/ &#39;{print $5}&#39; 18.csv | sort | uniq -c | sort -nr
 483 14
  11 8
   3 15
   2 12
   1 num_lines
   1 68
   1 66
   1 55
   1 54
   1 40
   1 28
   1 24
   1 123
$
</pre>

<p> This is a bit surprising: although 95% of the sonnets have the
standard 14 lines, 8 lines is not uncommon and some are remarkably long.
I&#39;m no scholar, so I don&#39;t know what makes these &#34;sonnets&#34; as opposed to
some other poetic form.

</p><p> Maybe printing something about the longer ones might provide more
insight?  We&#39;ll exclude the header line too.

</p><pre>$ awk -F/ &#39;NF &gt; 1 &amp;&amp; $5 &gt; 14&#39; 18.csv
1729/1777/William/Dodd/15/SONNET./OCCASIONED BY HEARING A YOUNG LADY SING SPENSER&#39;S AMORETTI, &amp;c. SET TO MUSIC BY DR. GREENE.
1749/1806/Charlotte Turner/Smith/40/ODE TO DESPAIR./FROM THE NOVEL OF EMMELINE.
1749/1806/Charlotte Turner/Smith/68/ELEGY./[â€˜Dark gathering clouds involve the threatening skies]
1749/1806/Charlotte Turner/Smith/24/SONG./FROM THE FRENCH OF CARDINAL BERNIS.
1749/1806/Charlotte Turner/Smith/123/THE ORIGIN OF FLATTERY./
1749/1806/Charlotte Turner/Smith/54/THE PEASANT OF THE ALPS./FROM THE NOVEL OF CELESTINA.
1749/1806/Charlotte Turner/Smith/55/THIRTY-EIGHT./ADDRESSED TO MRS. HY.
1749/1806/Charlotte Turner/Smith/28/VERSES/INTENDED TO HAVE BEEN PREFIXED TO THE NOVEL OF EMMELINE, BUT THEN SUPPRESSED.
_/_/G./Bent/66/To the SAME,/On receiving his Poems to Thespia with a Sonnet prefixed.
1735/1779/John/Langhorne/15/SONNET CLXXIX./
1735/1788/William Julius/Mickle/15/SONNET TO VASCO DE GAMA: FROM TASSO./
</pre>

<p> This output suggests a variety of other questions and potential issues.
Charlotte Turner Smith wrote the majority of the long sonnets,
including the extreme outlier with 123 lines.
How many did she write overall?

</p><pre>$ grep Charlotte.Turner 18.csv | wc
     101     941    8955
$
</pre>

<p> She certainly was prolific, representing nearly 20% of the data.  
This raises some other questions for scholars, in particular, how
were these sonnets selected and how representative are they?  

</p><p> Here are some commands
to investigate the authors further.  For example, if we do a straightforward
display of unique author names:

</p><pre>$ awk -F/ &#39;{print $3, $4}&#39; 18.csv | uniq | sed 10q  # display the first 10 lines
author_fname author_lname
E. H. (Elihu Hubbard) Smith
M. F. Cogswell
E. H. (Elihu Hubbard) Smith
M. F. Cogswell
John Bampfylde
Thomas Edwards
William Cowper
William Dodd
Thomas Edwards
</pre>

we see that records for at least three authors are not contiguous, as we
might naively have expected.

This raises a new question: what is the order of the records?

<p> How many unique authors are there, and how much did they write?

</p><pre>$ awk -F/ &#39;{print $4 &#34;, &#34; $3}&#39; 18.csv | sort | uniq -c | sort -n
   1 Anon., _
   1 Anstey, Christopher
   1 Bent, G.
   1 Bishop, Samuel
   1 Bradford, A. M.
     ...
  23 Russell, Thomas
  38 Downman, Hugh
  50 Edwards, Thomas
 101 Smith, Charlotte Turner
 103 Seward, Anna
</pre>

<p> There are 42 distinct authors; the display above shows a few of the 14
singletons, and the five most prolific.  Interestingly, the top two are
women.  Guessing from names, there are only a handful of other female
authors.  One might wonder why over 40% of the sonnets are by two women.

How the data was created is a very important consideration for any
dataset, and particularly for data in the humanities.  As Meredith puts
it, <b>there is no such thing as raw data</b>; all datasets are the result of a
selection and curation process.  Historically, this has often been to
the detriment of some classes of authors.

</p><p> As a peripheral question, how many lines did Anna Seward and
Charlotte Turner Smith write?

</p><pre>awk -F/ &#39;/Anna/ { anna += $5 }; /Charlotte/ { char += $5 }; END {print anna, char}&#39; 18.csv
1456 1706
</pre>

Seward wrote fewer lines because all her sonnets were 14 lines long.

<h4>When did they live and die?</h4>

The first two fields are the birth and death dates for each author,
so we can explore questions about the ages of the authors.

The age is just <tt>$2-$1</tt>:

<pre>$ awk -F/ &#39;{age[$3 &#34; &#34; $4] = $2 - $1}
    END {for (i in age) print age[i], i}&#39; 18.csv | sort -n
-1807 M. F. Cogswell
0 A. M. Bradford
0 G. Bent
0 J. Cole
0 R. Hole
0 _ Anon.
0 author_fname author_lname
23 Henry Headley
...
79 John, Mrs. Hunter
81 Christopher Anstey
81 Thomas James Mathias
83 Anne MacVicar Grant
84 William Crowe
88 William Lisle Bowles
</pre>

<p> This tells us pretty clearly that the dates need to be studied further,
most easily by looking for the ages that are zero or negative:

</p><pre>$ awk -F/ &#39;$2-$1 &lt;= 0&#39; 18.csv
1807fl.//M. F./Cogswell/8/SONNET,/Written after hearing a SONG sung by several SISTERS.
1807fl.//M. F./Cogswell/8/THE SMILE./SONNET TO CAROLINE.
_/_/_/Anon./14/SONNET./
_/_/A. M./Bradford/14/To the SAME./
_/_/J./Cole/14/To the SAME./
_/_/G./Bent/66/To the SAME,/On receiving his Poems to Thespia with a Sonnet prefixed.
_/_/R./Hole/14/To the SAME,/On his Poems addressed to Thespia.
</pre>

<p> What do we do about entries like R. Hole and the often prolific
Anon, whose date fields are marked with <tt>_</tt>?

</p><p> What&#39;s with M. F. Cogswell?  He flourished around 1807, with two
8-line sonnets.  Actually, this entry shows one way in which Awk differs
from Python, often usefully.  When converting an arbitrary string to a
number, Awk uses the longest prefix that looks like a number, so
<tt>1807fl.</tt> has numeric value 1807; Awk processes that and
carries on.  Python, by contrast, will
throw an exception that, unless explicitly caught, will terminate execution.

</p><p> Neither of these deals with the real issue, which is that the data
is incomplete so an age computation isn&#39;t possible.

One simple solution, appropriate when unknown dates are
a small fraction of the data, is to ignore those records.
Regular expressions are the easiest way to select the good bits
or reject the bad ones:

</p><pre>$ awk -F/ &#39;$1 ~ /^[0-9]+$/ &amp;&amp; $2 ~ /^[0-9]+$/ {print $2-$1, $3, $4}&#39; 18.csv | uniq | sort -n | uniq
23 Henry Headley
25 Richard Gall
26 Thomas Russell
27 E. H. (Elihu Hubbard) Smith
37 Robert Burns
...
79 John, Mrs. Hunter
81 Christopher Anstey
83 Anne MacVicar Grant
84 William Crowe
88 William Lisle Bowles
</pre>

Rather than repeating the test over and over again,
we could collect all the lines that have valid dates in a new
temporary file, then do some further computations on that.

<pre>$ awk -F/ &#39;$1 ~ /^[0-9]+$/ &amp;&amp; $2 ~ /^[0-9]+$/&#39; 18.csv &gt;temp
$ wc temp
     486    3350   35265 temp
</pre>

<p> After that, we can compute the average age, and determine
the youngest and oldest.

</p><pre>$ awk -F/ &#39;{ ages = ages + $2 - $1 } # add up all the ages
       END { print &#34;average age =&#34;, ages / NR }&#39; temp
average age = 60.2942
$ awk -F/ &#39;$2-$1 &gt; max { max = $2 - $1; fname = $3; lname = $4 }
       END { print &#34;oldest:&#34;, fname, lname, &#34; age&#34;, max }&#39; temp
oldest: William Lisle Bowles  age 88
</pre>

<p> As noted above, the author names are not contiguous, and the dates
are not in any obvious order either, which makes one wonder what order
the data has been sorted into.

How would you detect such anomalies mechanically in a larger dataset, in
particular if it were not related to dates)?

</p><h4> Associative Arrays </h4>

None of the examples so far have used associative arrays, which are Awk&#39;s
only data structure.  We didn&#39;t spend much if any time on associative
arrays in the class, since it felt like a slightly too-advanced topic
for our population.

<p> An associative array, equivalent to a hash in Java or a dictionary
in Python, is an array whose subscripts are not integers but arbitrary strings.

Several of the examples above that used <tt>sort|uniq -c</tt> can be
written equivalently with associative arrays to bring together
identical items.  For example, to see the distribution of sonnet lengths,
we can write:

</p><pre># awk -F/ &#39;{print $5}&#39; 18.csv | sort | uniq -c | sort -nr

$ awk -F/ &#39;{ len[$5]++ }; END { for (i in len) print len[i], i }&#39; 18.csv | sort -nr
</pre>

This produces the same result as the previous version.  We could even
embed the sort command in the Awk program:

<pre>$ awk -F/ &#39;{ len[$5]++ }; END { for (i in len) print len[i], i | &#34;sort -nr&#34; }&#39; 18.csv
</pre>

This computation is essentially the same as the Pandas <tt>unique_id</tt>
dataframe function.



<h4> Wrap-up </h4>

Awk is good for the kind of preliminary analysis we&#39;ve
shown here, particularly when coupled with basic Unix
tools like <tt>sort</tt>, <tt>uniq</tt>, and <tt>wc</tt>.

At some point it&#39;s time to switch to Python and some
of its libraries, especially for plotting, but that
can be deferred until you really understand what&#39;s in the
data and where it might be flaky.



</div>
  </body>
</html>
