<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ahrm.github.io/jekyll/update/2023/01/02/three-eyed-forehead.html">Original</a>
    <h1>Three-eyed forehead in Stable Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    

<p>Today I saw an <a href="https://jalammar.github.io/ai-image-generation-tools/">interesting post</a> on hackernews where the author tried to remake an old game by recreating the pixelated art using some AI image generation models for example:</p>

<hr/>

<p>
  <img src="https://ahrm.github.io/images/2023-01-02-three-eyes/comparison.png"/>
</p>
<hr/>



<p>It worked reasonably well for most of the images, but there is one image which could not be easily created using the models, not even with stable diffusion inpainting:</p>
<hr/>

<p>
  <img width="400px" src="https://ahrm.github.io/images/2023-01-02-three-eyes/nemesis.png"/>
</p>
<hr/>



<p>Apparently it was impossible to recreate the three eyes on the forehead. I have a few theories why this is the case:</p>

<ul>
  <li>Probably there are a lot of “normal” looking humanoid portraits in the dataset, so the model is probably heavily biased towards producing “normal” humanoids.</li>
  <li>These models usually have trouble with numbers, so even when there are eyes in the forehead, it is rarely exactly three eyes</li>
</ul>

<p>I was wondering if using the advanced inpainting of my <a href="https://github.com/ahrm/UnstableFusion#how-to-use-advanced-inpainting">Stable Diffusion desktop frontend</a> frontend, we could achieve the illusive three-eyed forehead.</p>

<p>Before we begin, let me give you a quick overview of how stable diffusion inpainting (and my advanced inpainting implementation) work. I assume you are already familiar with the basics of how diffusion models work, if you are not, there are <a href="https://stable-diffusion-art.com/how-stable-diffusion-work/">excellent resources</a> on the web.</p>

<p>In order to inpaint, first the masked portion of the image is filled in using a “dumb” inpainting algorithm (for example color each pixel with the closest non-masked pixel’s color). Then we use the encoder to encode this image to the latent representation of the diffusion model. Then we add some noise to the masked part of the image and run the normal diffusion process.</p>

<p>Using advanced inpainting, we modify the first part of this process, so instead of using an algorithm to inpaint the missing parts, we could manually specify an initial image in the masked area. This heavily guides the diffusion process to generate something resembling the initial image. Here is a demo of this method:</p>

<hr/>

<video muted="" controls="" width="100%">
    <source src="/images/2023-01-02-three-eyes/inpainting.mp4" type="video/mp4"/>
</video>
<hr/>



<p>Here is how I approached this problem:
First I downloaded a random eye image from the web and used advanced inpainting to create a version with just one eye:</p>

<hr/>

<p>
  <img src="https://ahrm.github.io/images/2023-01-02-three-eyes/one-eye.png"/>
</p>
<p><small><i>Prompt: Demonic red eye on the forehead</i></small></p>
<p><small><i>Negative Prompt: Eyelashes</i></small></p>
<p><small><i>Generated using advanced inpainting by pasting an eye image from the web on the forehead</i></small></p>
<hr/>



<p>I didn’t bother making this look good, because we will have to inpaint over it anyway to generate the three eyes. I just needed something reasonable. Now we paste this eye on the forehead to create an initial image for the advanced inpainting:</p>

<hr/>

<p>
  <img src="https://ahrm.github.io/images/2023-01-02-three-eyes/initial.png"/>
</p>
<hr/>



<p>Now we mask the three eyes, but use the original image as the initial image. This will guide the diffusion process to put three eyes in the masked location. We can even repeatedly apply this process, using the same mask each time but using the newer images (undoing changes if the new images were not as good), we can gradually guide it to generate something that we want (we can even change the prompt and parameters each time). Here is a sequence of generated images:</p>

<hr/>

<p>
  
  
  
  
  
  <img id="seq" src="https://ahrm.github.io/images/2023-01-02-three-eyes/initial.png"/>
</p>
<hr/>



<p>You may notice the border around the masked area, but we could fix that we normal inpainting:</p>

<hr/>

<p>
  
  
  <img id="mask" src="https://ahrm.github.io/images/2023-01-02-three-eyes/masked.png"/>
</p>
<hr/>

<p>And here is the final result:</p>
<hr/>

<p>
  <img id="mask" src="https://ahrm.github.io/images/2023-01-02-three-eyes/after.png"/>
</p>
<hr/>

<p>Of course it is not a masterpiece, but it was a very fun experiment. And it has the potential to be way more fun, because I was running it on an old 1070, each inpainting took about 20 seconds which was quite annoying. But I could envision a future where generation is basically real-time, imagine navigating through possible generations using mouse wheel and tweaking the parameters and seeing the effects in real-time. With the <a href="https://twitter.com/emostaque/status/1598131202044866560">supposed improvements</a> in stable diffusion, this future might not be far away.</p>



  </div>
</article>

      </div>
    </div></div>
  </body>
</html>
