<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cesarsotovalero.net/blog/i-am-switching-to-python-and-actually-liking-it.html">Original</a>
    <h1>I&#39;m Switching to Python and Actually Liking It</h1>
    
    <div id="readability-page-1" class="page"><article role="main"><p>I started to code more in <a href="https://www.python.org/">Python</a> around 6 months ago.
Why?
Because of AI, obviously.
It’s clear (to me) that big <del>money</del> opportunities are all over AI these days.
And guess what’s the <em>de facto</em> programming language for AI?
Yep, that sneaky one.</p><p>I had used Python before, but only for small scripts.
For example, <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/scripts/fetch_all_youtube_videos.py">this script</a> scrapes metadata from all videos on <a href="https://www.youtube.com/channel/UCR4rI98w6-MqYoCS6jR9LGg">my YouTube channel</a>.
The metadata is dumped as <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/_data/youtube-videos.json">a JSON file</a> that I use to nicely display statistics of the videos <a href="https://www.cesarsotovalero.net/youtube">on this static page</a>.
As you can <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/.github/workflows/update-youtube-videos.yml">see here</a>, this little script runs in solo mode every Monday via GitHub Actions.
Doing this kind of thing in Python is just way more convenient than, say, using Batch.
Not only because the syntax is more human-friendly, but also because the Python interpreter is natively integrated in all Unix distros.
Isn’t that cool?</p><p>So yeah, Python is powerful, and it couples very well with the now ubiquitous <a href="https://code.visualstudio.com/">VSCode</a> editor.
But I didn’t treat it seriously until recently,<sup id="fnref:3"><a href="#fn:3" rel="footnote" role="doc-noteref">1</a></sup> it was just after I wanted to build AI applications (RAG, Agents, GenAI tools, etc.) for the “real world” that I realized that whether you like it or not, Python is the language of choice for that matters.</p><p>So I decided to give it a serious try, and to my great surprise, I’ve found that Python, and everything around it, has really improved a lot over the last decades.</p><p>Here’re just three examples:</p><ol><li>Python has created a very complete ecosystem of libraries and tools for processing and analyzing data.<sup id="fnref:4"><a href="#fn:4" rel="footnote" role="doc-noteref">2</a></sup></li><li>Python has gotten faster with optimized static compilers like <a href="https://cython.org/">Cython</a>.</li><li>Python has done a good job of hiding its legacy ugliness (such as <code>__init__</code>, <code>__new__</code>, and similar aberrations), swettening its syntax to accomodate developers <del>with good taste</del>.</li></ol><p>Thanks to this and many other things, I’m now feeling a particular joy for the language.</p><p>However, during this time, I’ve found that there’s still a big gap between using Python for “production-ready”<sup id="fnref:7"><a href="#fn:7" rel="footnote" role="doc-noteref">3</a></sup> apps vs. the usual Jupyter notebook or script-based workflow.</p><p>So in this post, I share the tools, libraries, configs, and other integrations that bring me joy, and that I now use for building my Python applications.</p><p>⚠️ This post is highly biased toward the tools I personally use today, and if you think I’m missing some gem, please let me/us know (preferably in the comment section below).</p><p>I prefer to use a <a href="https://en.wikipedia.org/wiki/Monorepo">monorepo</a> structure (backend and frontend) for my Python projects.<sup id="fnref:1"><a href="#fn:1" rel="footnote" role="doc-noteref">4</a></sup></p><p>Why?</p><ol><li>Because of my bad memory: I don’t like code parts scattered across multiple repositories (it’s definitely not search-friendly).</li><li>Because having multiple repost is mostly unnecessary: I’m just one guy, and I believe that if a project grows to the point that it needs to be split into multiple repositories, then it’s a sign of over-engineering.</li><li>Because I’m lazy: I like to keep things as simple as possible, compile, test, containerize, and deploy from a single location.<sup id="fnref:5"><a href="#fn:5" rel="footnote" role="doc-noteref">5</a></sup></li></ol><p>I would like to have a tool that generates the project structure for me, but I haven’t found one that fits me yet.
In the past, I used <a href="https://cookiecutter-data-science.drivendata.org/">CCDS</a>, a project initialization tool mostly for Data Science projects.
It’s very good, but it’s not targeting full-stack developers as its core users.</p><p>Here’s the typical structure of a project with a frontend-backend architecture (I’ll go through each subpart later in this post):</p><figure><pre><code data-lang="bash">project/
│
├── .github/ <span># GitHub Actions workflows for CI/CD pipelines</span>
│   ├── workflows/ <span># Directory containing YAML files for automated workflows</span>
│   └── dependabot.yml <span># Configuration for Dependabot to manage dependencies</span>
│
├── .vscode/ <span># VSCode configuration for the project</span>
│   ├── launch.json <span># Debugging configurations for VSCode</span>
│   └── settings.json <span># Project-specific settings for VSCode</span>
│
├── docs/ <span># Website and docs (a static SPA with MkDocs)</span>
│
├── project-api/ <span># Backend API for handling business logic and heavy processing</span>
│   ├── data/ <span># Directory for storing datasets or other static files</span>
│   ├── notebooks/ <span># Jupyter notebooks for quick (and dirty) experimentation and prototyping</span>
│   ├── tools/ <span># Utility scripts and tools for development or deployment</span>
│   ├── src/ <span># Source code for the backend application</span>
│   │   ├── app/ <span># Main application code</span>
│   │   └── tests/ <span># Unit tests for the backend</span>
│   │
│   ├── .dockerignore <span># Specifies files to exclude from Docker builds</span>
│   ├── .python-version <span># Python version specification for pyenv</span>
│   ├── Dockerfile <span># Docker configuration for containerizing the backend</span>
│   ├── Makefile <span># Automation tasks for building, testing, and deploying</span>
│   ├── pyproject.toml <span># Python project configuration file</span>
│   ├── README.md <span># Documentation for the backend API</span>
│   └── uv.lock <span># Lock file for dependencies managed by UV</span>
│
├── project-ui/ <span># Frontend UI for the project (Next.js, React, etc.)</span>
│
├── .gitignore <span># Global Git ignore file for the repository</span>
├── .pre-commit-config.yaml <span># Configuration for pre-commit hooks</span>
├── CONTRIBUTING.md <span># Guidelines for contributing to the project</span>
├── docker-compose.yml <span># Docker Compose configuration for multi-container setups</span>
├── LICENSE <span># License information for the project (I always choose MIT)</span>
├── Makefile <span># Automation tasks for building, testing, and deploying</span>
└── README.md <span># Main documentation for the project (main features, installation, and usage)</span></code></pre></figure><p>My <code>project</code> is the root directory and the name of my GitHub repo.
I like short names for projects, ideally less than 10 characters long. No <code>snake_case</code>; separation with hyphens is OK to me.
Note that the project should be self-contained, meaning it includes documentation, build/deployment infrastructure, and any other necessary files to run it standalone.</p><p>It’s important not to do any heavy data processing steps in the <code>project-ui</code>, as I opted to separate frontend logic from backend responsibilities.
Instead, I choose to make HTTP requests to the <code>project-api</code> server that contains the Python code.
This way, we keep the browser application light while delegating the heavy lifting and business logic to the server.</p><p>There’s an <code>__init__.py</code> file in <code>project-api/src/app</code> to indicate that <code>app</code> is a Python module (it can be imported from other modules).</p><h2 id="uv">uv</h2><p>I use <a href="https://github.com/astral-sh/uv">uv</a> as my Python package manager and build tool. It’s all I need to install and manage dependencies.</p><p>Here are the core commands to set it up:</p><figure><pre><code data-lang="bash"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td><pre><span># Install uv globally if not already installed</span>
curl <span>-sSfL</span> &lt;https://astral.sh/install.sh&gt; | sh

<span># Initialize a new project (adds .gitignore, .python-version, pyproject.toml, etc.)</span>
uv init project-api

<span># Add some dependencies into the project and update pyproject.toml</span>
uv add <span>--dev</span> pytest ruff pre-commit mkdocs gitleaks fastapi pydantic

<span># Update the lock file with the latest versions of the dependencies (creates a .venv if not already created)</span>
uv <span>sync</span>

<span># Activate the .venv</span>
uv venv activate
</pre></td></tr></tbody></table></code></pre></figure><p>Note that the most important file for <code>uv</code> is <code>pyproject.toml</code>.<sup id="fnref:2"><a href="#fn:2" rel="footnote" role="doc-noteref">6</a></sup>
This file <a href="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/">contains</a> metadata and the list of dependencies required to build and run the project.</p><h2 id="ruff">ruff</h2><p>I really like <a href="https://github.com/astral-sh/ruff">ruff</a>.
It’s a super-fast Python linter and code formatter, designed to help lazy developers like me keep our codebases clean and maintainable.
Ruff combines <code>isort</code>, <code>flake8</code>, <code>autoflake</code>, and similar tools into a single command-line interface:</p><figure><pre><code data-lang="bash"><table><tbody><tr><td><pre>1
2
3
4
</pre></td><td><pre><span># Lint all files in `/path/to/code` (and any subdirectories).</span>
ruff check path/to/code/
<span># Format all files in `/path/to/code` (and any subdirectories).</span>
ruff format path/to/code/
</pre></td></tr></tbody></table></code></pre></figure><p>Ruff supports the <a href="https://pep8.org/">PEP 8</a> style guide out of the box.</p><h2 id="ty">ty</h2><p><a href="https://github.com/astral-sh/ty">ty</a> is a type checker for Python.
It is a great combo for <a href="https://docs.python.org/3/library/typing.html">typing</a>, the popular Python module for adding static typing.
I think typing really helps me catch type errors early in the development process. I actually don’t care about having to write more code, in fact, I prefer it if it improves code quality and reduces the likelihood of runtime errors.</p><p><strong>NOTE:</strong> At the time of writing, <code>ty</code> is still in early development by Astral (the same company behind <code>uv</code> and <code>ruff</code>), but I’ve been using it and haven’t found any noticeable flaws so far.</p><h2 id="pytest">pytest</h2><p><a href="https://docs.pytest.org/en/stable/">pytest</a> is <em>THE</em> testing library for Python.
Writing simple and scalable test cases with it is just super easy.
It supports fixtures, parameterized tests, and has a rich ecosystem of plugins.
Just create a file named <code>test_&lt;unit_or_module&gt;.py</code> in <code>project-api/src/app/tests/</code>, and run:</p><figure><pre><code data-lang="bash">uv run pytest</code></pre></figure><p>That’s it!</p><h2 id="pydantic">Pydantic</h2><p><a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> is a data validation and settings management library for Python.
It helps manage all kinds of configuration settings, such as API keys, database connection details, or model parameters (hardcoding these values is a very bad practice, btw).</p><p>In particular, <a href="https://docs.pydantic.dev/latest/concepts/pydantic_settings/">Pydantic Settings</a> allows you to define application configurations using Pydantic models.
It can automatically load settings from environment variables or special <code>.env</code> files, validate their types, and make them easily accessible in your code.</p><p>Here’s an illustrative example:</p><figure><pre><code data-lang="python"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseSettings</span>

<span>class</span> <span>Settings</span><span>(</span><span>BaseSettings</span><span>):</span>
    <span>api_key</span><span>:</span> <span>str</span>
    <span>db_url</span><span>:</span> <span>str</span>

    <span>class</span> <span>Config</span><span>:</span>
        <span>env_file</span> <span>=</span> <span>&#34;</span><span>.env</span><span>&#34;</span>

<span>settings</span> <span>=</span> <span>Settings</span><span>()</span>
</pre></td></tr></tbody></table></code></pre></figure><p>Now, when you run this code, Pydantic will automatically load the values of <code>api_key</code> and <code>db_url</code> from the <code>.env</code> file or environment variables.
These values will be accessible and validated according to the types defined in the <code>Settings</code> model.
Just great!</p><h2 id="mkdocs">MkDocs</h2><p>I use <a href="https://www.mkdocs.org/">MkDocs</a> for documentation and static generation of the website for the project.<sup id="fnref:6"><a href="#fn:6" rel="footnote" role="doc-noteref">7</a></sup>
I’m not a designer, so I prefer to just copy an aesthetically pleasing design from another similar open-source project and make some simple modifications to the CSS (like changing fonts and colors).</p><h2 id="fastapi">FastAPI</h2><p>I use <a href="https://fastapi.tiangolo.com/">FastAPI</a> for building APIs.
It has been a game changer for me, it allows for easy creation of RESTful APIs with automatic validation, serialization, and documentation.
FastAPI is built on top of Starlette and Pydantic, which means it provides excellent performance and type safety.
It’s fast, easy to use, and integrates seamlessly with Pydantic for data validation.</p><h2 id="dataclasses">Dataclasses</h2><p><a href="https://docs.python.org/3/library/dataclasses.html">Dataclasses</a> is not a library but a Python feature that provides a way to define classes that are primarily used to store data.
They offer a simple syntax for creating classes that automatically generate special methods like <code>__init__()</code>, <code>__repr__()</code>, and <code>__eq__()</code>.</p><p>This greatly reduces boilerplate when creating data containers.</p><p>Here’s an example:</p><figure><pre><code data-lang="python"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
</pre></td><td><pre><span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span>

<span>@dataclass</span>
<span>class</span> <span>Point</span><span>:</span>
    <span>x</span><span>:</span> <span>int</span>
    <span>y</span><span>:</span> <span>int</span>

<span>p</span> <span>=</span> <span>Point</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<span>print</span><span>(</span><span>p</span><span>)</span>  <span># Output: Point(x=1, y=2)</span>
</pre></td></tr></tbody></table></code></pre></figure><p>So goodbye boilerplate and cryptic code!</p><h2 id="github-actions">GitHub Actions</h2><p>I’m a fanboy of <a href="https://github.com/features/actions">GitHub Actions</a>, especially for CI across different OSs.
I recommend using it for both API and UI pipelines.</p><p>A typical workflow for <code>project-api</code> looks like this:</p><figure><pre><code data-lang="yaml"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td><pre><span>name</span><span>:</span> <span>CI-API</span>

<span>on</span><span>:</span>
  <span>push</span><span>:</span>
    <span>branches</span><span>:</span>
      <span>-</span> <span>main</span>
  <span>pull_request</span><span>:</span>
    <span>branches</span><span>:</span>
      <span>-</span> <span>main</span>

<span>jobs</span><span>:</span>
  <span>build-and-test</span><span>:</span>
    <span>runs-on</span><span>:</span> <span>ubuntu-latest</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>name</span><span>:</span> <span>Checkout code</span>
        <span>uses</span><span>:</span> <span>actions/checkout@v3</span>
      <span>-</span> <span>name</span><span>:</span> <span>Build Docker image</span>
        <span>run</span><span>:</span> <span>docker build -t project-api:ci ./project-api</span>
      <span>-</span> <span>name</span><span>:</span> <span>Run tests</span>
        <span>run</span><span>:</span> <span>docker run --rm project-api:ci pytest</span>
</pre></td></tr></tbody></table></code></pre></figure><p>Note that this workflow uses Docker to run the tests in an isolated environment.<sup id="fnref:8"><a href="#fn:8" rel="footnote" role="doc-noteref">8</a></sup>
You can change the OS by setting the <code>runs-on</code> parameter to <code>windows-latest</code> or <code>macos-latest</code>.</p><h2 id="dependabot">Dependabot</h2><p>Handling dependencies is a pain, but <a href="https://dependabot.com/">Dependabot</a> makes it easier.
It automatically checks for outdated dependencies and creates pull requests to update them.</p><p>Here’s a sample configuration for Dependabot in the <code>.github/dependabot.yml</code> file:</p><figure><pre><code data-lang="yaml"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
</pre></td><td><pre><span>version</span><span>:</span> <span>2</span>
<span>updates</span><span>:</span>

<span>-</span> <span>package-ecosystem</span><span>:</span> <span>&#34;</span><span>uv&#34;</span>
    <span>directory</span><span>:</span> <span>&#34;</span><span>/&#34;</span>
    <span>schedule</span><span>:</span>
      <span>interval</span><span>:</span> <span>&#34;</span><span>weekly&#34;</span>
</pre></td></tr></tbody></table></code></pre></figure><h2 id="gitleaks">Gitleaks</h2><p>If there’s something that could hurt our reputation, it’s committing sensitive information, like API keys or passwords, directly to a repository.
Fortunately, <a href="https://github.com/gitleaks/gitleaks">Gitleaks</a> helps prevent this from happening.
There’s just no reason not to use it.</p><h2 id="pre-commit-hooks">Pre-commit Hooks</h2><p>I use <a href="https://pre-commit.com/">pre-commit</a> to run checks and format code before committing.
It helps ensure that the code is always in a good state and follows the project’s coding standards.
For example, I use it to run <a href="https://github.com/astral-sh/ruff-pre-commit">ruff-pre-commit</a> and <code>gitleaks</code> before committing my code.</p><p>Here’s a sample <code>.pre-commit-config.yaml</code> file that I use:</p><figure><pre><code data-lang="yaml"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td><pre><span>repos</span><span>:</span>
<span>repos</span><span>:</span>
  <span>-</span> <span>repo</span><span>:</span> <span>https://github.com/astral-sh/ruff-pre-commit</span>
    <span>rev</span><span>:</span> <span>v0.12.3</span>  <span># Ruff version.</span>
    <span>hooks</span><span>:</span>
      <span>-</span> <span>id</span><span>:</span> <span>ruff-check</span> <span># Run the linter.</span>
        <span>args</span><span>:</span> <span>[</span> <span>--fix</span> <span>]</span>
      <span>-</span> <span>id</span><span>:</span> <span>ruff-format</span>  <span># Run the formatter.</span>
  <span>-</span> <span>repo</span><span>:</span> <span>https://github.com/gitleaks/gitleaks</span>
    <span>rev</span><span>:</span> <span>v8.27.2</span>
    <span>hooks</span><span>:</span>
      <span>-</span> <span>id</span><span>:</span> <span>gitleaks</span>
</pre></td></tr></tbody></table></code></pre></figure><h2 id="make">Make</h2><p><a href="https://www.gnu.org/software/make/">Make</a> is a Swiss Army knife, a classic utility for automating tasks.
I use it to create simple shortcuts for common development commands.
Instead of remembering and typing out long CLI incantations to run tests, build Docker images, or start services, I define these tasks in a <code>Makefile</code>.
Then I just run commands like <code>make test</code> or <code>make infrastructure-up</code>.</p><p>As you might have noticed, there is a <code>Makefile</code> in both the <code>project-api</code> and the global <code>project</code> directories:</p><ol><li><code>project/project-api/Makefile</code>: For linting, testing, and running the API.</li><li><code>project/Makefile</code>: For building and running the infrastructure (via <code>docker-compose</code>).</li></ol><p>Here’s an extremely simple example of the <code>project-api</code> Makefile:</p><figure><pre><code data-lang="plaintext"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td><td><pre>DIR := . # project/project-api/Makefile

test:
 uv run pytest

format-fix:
 uv run ruff format $(DIR)
 uv run ruff check --select I --fix

lint-fix:
 uv run ruff check --fix
</pre></td></tr></tbody></table></code></pre></figure><p>Now, if I want to run the tests, I just run <code>make test</code>, and it executes <code>uv run pytest</code> in the current directory.</p><p>For the global project, I use the following <code>Makefile</code>:</p><figure><pre><code data-lang="plaintext"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
</pre></td><td><pre>infrastructure-build:
 docker compose build

infrastructure-up:
 docker compose up --build -d

infrastructure-stop:
 docker compose stop
</pre></td></tr></tbody></table></code></pre></figure><p><code>make</code> is a powerful tool that can help you automate almost anything in your development workflow.
Although the examples above are very simple, just imagine how you can add more complex tasks as needed.</p><h2 id="docker">Docker</h2><p><a href="https://www.docker.com/">Docker</a> is a tool that allows you to package your application and its dependencies into a container,including everything needed to run: dependencies, system tools, code, and runtime OS.
When working locally, I use <a href="https://docs.docker.com/compose/">Docker Compose</a> to connect all Docker images into the same network.
Like Docker for dependencies, Docker Compose allows encapsulating the whole application stack and separating it from the rest of your local environment.</p><p>To fully grasp this concept, let’s take a look at a simple <code>docker-compose.yml</code> file:</p><figure><pre><code data-lang="yaml"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td><pre><span>version</span><span>:</span> <span>&#39;</span><span>3.8&#39;</span>
<span>services</span><span>:</span>
  <span>project-api</span><span>:</span>
    <span>build</span><span>:</span>
      <span>context</span><span>:</span> <span>./project-api</span>
      <span>dockerfile</span><span>:</span> <span>Dockerfile</span>
    <span>ports</span><span>:</span>
      <span>-</span> <span>&#34;</span><span>8000:8000&#34;</span>
    <span>volumes</span><span>:</span>
      <span>-</span> <span>./project-api:/app</span>
    <span>environment</span><span>:</span>
      <span>-</span> <span>ENV_VAR=value</span>
    <span>networks</span><span>:</span>
      <span>-</span> <span>project-network</span>

  <span>project-ui</span><span>:</span>
    <span>build</span><span>:</span>
      <span>context</span><span>:</span> <span>./project-ui</span>
      <span>dockerfile</span><span>:</span> <span>Dockerfile</span>
    <span>ports</span><span>:</span>
      <span>-</span> <span>&#34;</span><span>3000:3000&#34;</span>
    <span>networks</span><span>:</span>
      <span>-</span> <span>project-network</span>

<span>networks</span><span>:</span>
  <span>project-network</span><span>:</span>
    <span>driver</span><span>:</span> <span>bridge</span>
</pre></td></tr></tbody></table></code></pre></figure><p>In this file, we define two services: <code>project-api</code> and <code>project-ui</code>.
Each service has its own build context (<code>Dockerfile</code>), ports, volumes, and environment variables.</p><p>Here’s a sample <code>Dockerfile</code> for the <code>project-api</code> service:</p><figure><pre><code data-lang="dockerfile"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td><pre><span>FROM</span><span> python:3.11-slim</span>

<span># Install system dependencies</span>

<span>COPY</span><span> --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/</span>

<span>WORKDIR</span><span> /app</span>

<span>COPY</span><span> uv.lock pyproject.toml README.md ./</span>
<span>RUN </span>uv <span>sync</span> <span>--frozen</span> <span>--no-cache</span>

<span># Bring in the actual application code</span>

<span>COPY</span><span> src/app app/</span>
<span>COPY</span><span> tools tools/</span>

<span># Define a command to run the application</span>

<span>CMD</span><span> [&#34;/app/.venv/bin/fastapi&#34;, &#34;run&#34;, &#34;project/infrastructure/api.py&#34;, &#34;--port&#34;, &#34;8000&#34;, &#34;--host&#34;, &#34;0.0.0.0&#34;]</span>
</pre></td></tr></tbody></table></code></pre></figure><p>As you can see, the Dockerfile starts from a Python base image, installs dependencies, copies the project files, and defines the command to run the FastAPI application.</p><p>This way, you can run the entire application stack with a single command:</p><figure><pre><code data-lang="bash">docker compose up <span>--build</span> <span>-d</span></code></pre></figure></article></div>
  </body>
</html>
