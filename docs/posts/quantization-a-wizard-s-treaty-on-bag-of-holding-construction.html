<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/quantization-a-wizards-treaty-on-bag-of-holding-construction/">Original</a>
    <h1>Quantization a Wizard&#39;s Treaty on Bag of Holding Construction</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p><em>In the hallowed halls of the Arcanum, apprentice quantizers gather around ancient scrolls, their eager hands weaving complex patterns of magic, each fold compressing vast models into the nebulous depths of the Bag of Holding. As they manipulate these arcane energies, the fabric of reality thins, threatening to fray at the edges of their understanding. Those who delve too recklessly into its powers may find their meticulously crafted models reduced to incomprehensible noise, lost in the echoing void of the bag’s mysterious expanse.</em></p>
<figure><a href="https://blog.val.town/blog/codegen/bag-of-holding.png" title="bag-of-holding" data-thumbnail="bag-of-holding.png" data-sub-html="&lt;h2&gt;Bag of Holding&lt;/h2&gt;&lt;p&gt;bag-of-holding&lt;/p&gt;">
        <img src="https://blog.val.town/svg/loading.min.svg" data-src="bag-of-holding.png" data-srcset="bag-of-holding.png, bag-of-holding.png 1.5x, bag-of-holding.png 2x" data-sizes="auto" alt="bag-of-holding.png"/>
    </a><figcaption>Bag of Holding</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Make the fifth circle model runnable by a low level machine by squeezing its essence into a smaller form factor.</p>
<h2 id="why-quantization">Why Quantization?</h2>
<p>Quantization reduces the size of a model by decreasing the number of bits used to represent its weights. This reduction has two main benefits:</p>
<ul>
<li>It alleviates the memory bandwidth bottleneck by reducing the amount of data transferred between memory and compute units.</li>
<li>It decreases the amount of computation needed. Fewer bits require fewer transistors, and simpler types like integers require fewer transistors than more complex types like floats.</li>
</ul>
<h2 id="quantize-and-unquantize">Quantize and Unquantize</h2>
<p>For all upcoming examples, we will use fp32 as the original precision, and int8 as the quantized precision. This gives us a 4x reduction in size, and at the time of writing int8 is the smallest type supported by torch and triton.</p>
<p>And we’ll focus on:</p>
<ul>
<li><strong>Linear Quantization</strong>: quantized values are evenly spaced.</li>
<li><strong>Per-tensor Quantization</strong>: each tensor is quantized independently, and each element for a given tensor is quantized with the same scale.</li>
</ul>
<h3 id="symmetric-quantization">Symmetric Quantization</h3>
<p>Let’s start with the simplest case, symmetric quantization. In order to convert the original range of values to the quantized one, we find the biggest absolute value and define the original range as <code>[-max_abs, max_abs]</code> then map it to the quantized range <code>[-127, 127]</code> (using restricted range here for simplicity so we ignore -128).</p>
<p>$scale = \frac{max\_abs}{q\_max}$</p>
<figure><img src="https://blog.val.town/blog/codegen/symmetric-quantization.svg" alt="Symmetric Quantization"/>
</figure>

<p>Note: The piece of the range highlighted in orange is wasted, because we don’t have any values in the <code>[-max_abs, min[</code> range for this example.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>quantize</span><span>(</span><span>weights</span><span>,</span> <span>bits</span><span>=</span><span>8</span><span>):</span>
</span></span><span><span>    <span>assert</span> <span>bits</span> <span>&lt;=</span> <span>8</span> <span># keep my life simple</span>
</span></span><span><span>    <span>maxi</span> <span>=</span> <span>weights</span><span>.</span><span>abs</span><span>()</span><span>.</span><span>max</span><span>()</span>
</span></span><span><span>    <span>q_maxi</span> <span>=</span> <span>2</span> <span>**</span> <span>(</span><span>bits</span> <span>-</span> <span>1</span><span>)</span> <span>-</span> <span>1</span>
</span></span><span><span>    <span>scale</span> <span>=</span> <span>maxi</span> <span>/</span> <span>q_maxi</span>
</span></span><span><span>    <span>quantized_weights</span> <span>=</span> <span>t</span><span>.</span><span>clamp</span><span>(</span><span>t</span><span>.</span><span>round</span><span>(</span><span>weights</span> <span>/</span> <span>scale</span><span>),</span> <span>-</span><span>q_maxi</span><span>,</span> <span>q_maxi</span><span>)</span><span>.</span><span>to</span><span>(</span><span>t</span><span>.</span><span>int8</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>quantized_weights</span><span>,</span> <span>scale</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>unquantize</span><span>(</span><span>quantized_weights</span><span>,</span> <span>scale</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>quantized_weights</span> <span>*</span> <span>scale</span>
</span></span></code></pre></div><h3 id="asymmetric-quantization">Asymmetric Quantization</h3>
<p>To prevent wasting a piece of the range we can use asymmetric quantization. In this case we define the original range as <code>[min, max]</code> and map it to the quantized range <code>[0, 255]</code>. In exchange for representing the full range, we have to introduce a zero point to remember the offset during unquantization.</p>
<p>$scale = \frac{max - min}{q\_max}$</p>
<p>$zero\_point = \left\lfloor\frac{-min}{scale}\right\rceil$</p>
<figure><img src="https://blog.val.town/blog/codegen/asymmetric-quantization.svg" alt="Asymmetric Quantization"/>
</figure>

<p>Note: The scale is a floating point number, but the zero point is rounded to the nearest integer.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>quantize</span><span>(</span><span>weights</span><span>,</span> <span>bits</span><span>=</span><span>8</span><span>):</span>
</span></span><span><span>    <span>&#39;&#39;&#39; using the min-max strategy, this is vulnerable to outliers &#39;&#39;&#39;</span>
</span></span><span><span>    <span>assert</span> <span>bits</span> <span>&lt;=</span> <span>8</span> <span># keep my life simple</span>
</span></span><span><span>    <span>maxi</span> <span>=</span> <span>weights</span><span>.</span><span>max</span><span>()</span>
</span></span><span><span>    <span>mini</span> <span>=</span> <span>weights</span><span>.</span><span>min</span><span>()</span>
</span></span><span><span>    <span>qmaxi</span> <span>=</span> <span>2</span> <span>**</span> <span>bits</span> <span>-</span> <span>1</span>
</span></span><span><span>    <span>scale</span> <span>=</span> <span>(</span><span>maxi</span> <span>-</span> <span>mini</span><span>)</span> <span>/</span> <span>qmaxi</span>
</span></span><span><span>    <span>zero</span> <span>=</span> <span>int</span><span>(</span><span>t</span><span>.</span><span>round</span><span>(</span><span>-</span><span>mini</span> <span>/</span> <span>scale</span><span>))</span>
</span></span><span><span>    <span>quantized_weights</span> <span>=</span> <span>t</span><span>.</span><span>clamp</span><span>(</span><span>t</span><span>.</span><span>round</span><span>(</span><span>weights</span> <span>/</span> <span>scale</span><span>)</span> <span>+</span> <span>zero</span><span>,</span> <span>0</span><span>,</span> <span>qmaxi</span><span>)</span><span>.</span><span>to</span><span>(</span><span>t</span><span>.</span><span>uint8</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>quantized_weights</span><span>,</span> <span>scale</span><span>,</span> <span>zero</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>unquantize</span><span>(</span><span>quantized_weights</span><span>,</span> <span>scale</span><span>,</span> <span>zero</span><span>):</span>
</span></span><span><span>    <span>quantized_weights</span> <span>=</span> <span>quantized_weights</span><span>.</span><span>to</span><span>(</span><span>t</span><span>.</span><span>int32</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>(</span><span>quantized_weights</span> <span>-</span> <span>zero</span><span>)</span> <span>*</span> <span>scale</span>
</span></span></code></pre></div><h2 id="matmul">Matmul</h2>
<p>We need a few more ingredients. We need a way to do a matmul between two int8 tensors, and we need a way to unquantize the result.</p>
<h3 id="triton-kernel">Triton Kernel</h3>
<p>For the matmul, we’ll use a Triton kernel. Triton is a DSL for writing GPU kernels. It’s similar to CUDA but simpler. In practice, I just took the tiled matmul from the Triton tutorial and changed the types to int8 with int32 accumulators. The goal is to perform int8 by int8 products but avoid overflow.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>@triton.jit</span>
</span></span><span><span><span>def</span> <span>matmul_kernel</span><span>(</span><span>...</span><span>):</span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>    <span>accumulator</span> <span>=</span> <span>tl</span><span>.</span><span>zeros</span><span>((</span><span>BLOCK_SIZE_M</span><span>,</span> <span>BLOCK_SIZE_N</span><span>),</span> <span>dtype</span><span>=</span><span>tl</span><span>.</span><span>int32</span><span>)</span>
</span></span><span><span>    <span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> <span>tl</span><span>.</span><span>cdiv</span><span>(</span><span>K</span><span>,</span> <span>BLOCK_SIZE_K</span><span>)):</span>
</span></span><span><span>        <span>a</span> <span>=</span> <span>tl</span><span>.</span><span>load</span><span>(</span><span>a_ptrs</span><span>,</span> <span>mask</span><span>=</span><span>offs_k</span><span>[</span><span>None</span><span>,</span> <span>:]</span> <span>&lt;</span> <span>K</span> <span>-</span> <span>k</span> <span>*</span> <span>BLOCK_SIZE_K</span><span>,</span> <span>other</span><span>=</span><span>0.0</span><span>)</span>
</span></span><span><span>        <span>b</span> <span>=</span> <span>tl</span><span>.</span><span>load</span><span>(</span><span>b_ptrs</span><span>,</span> <span>mask</span><span>=</span><span>offs_k</span><span>[:,</span> <span>None</span><span>]</span> <span>&lt;</span> <span>K</span> <span>-</span> <span>k</span> <span>*</span> <span>BLOCK_SIZE_K</span><span>,</span> <span>other</span><span>=</span><span>0.0</span><span>)</span>
</span></span><span><span>        <span>accumulator</span> <span>=</span> <span>tl</span><span>.</span><span>dot</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>,</span> <span>accumulator</span><span>)</span>
</span></span><span><span>        <span>a_ptrs</span> <span>+=</span> <span>BLOCK_SIZE_K</span> <span>*</span> <span>stride_ak</span>
</span></span><span><span>        <span>b_ptrs</span> <span>+=</span> <span>BLOCK_SIZE_K</span> <span>*</span> <span>stride_bk</span>
</span></span><span><span>    <span>c</span> <span>=</span> <span>accumulator</span><span>.</span><span>to</span><span>(</span><span>tl</span><span>.</span><span>int32</span><span>)</span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>matmul_i8i32</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
</span></span><span><span>    <span>&#39;&#39;&#39; matmul for int8 with int32 accumulators &#39;&#39;&#39;</span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>    <span>c</span> <span>=</span> <span>t</span><span>.</span><span>empty</span><span>((</span><span>M</span><span>,</span> <span>N</span><span>),</span> <span>device</span><span>=</span><span>a</span><span>.</span><span>device</span><span>,</span> <span>dtype</span><span>=</span><span>t</span><span>.</span><span>int32</span><span>)</span>
</span></span><span><span>    <span>grid</span> <span>=</span> <span>lambda</span> <span>META</span><span>:</span> <span>(</span><span>triton</span><span>.</span><span>cdiv</span><span>(</span><span>M</span><span>,</span> <span>META</span><span>[</span><span>&#39;BLOCK_SIZE_M&#39;</span><span>])</span> <span>*</span> <span>triton</span><span>.</span><span>cdiv</span><span>(</span><span>N</span><span>,</span> <span>META</span><span>[</span><span>&#39;BLOCK_SIZE_N&#39;</span><span>]),</span> <span>)</span>
</span></span><span><span>    <span>matmul_kernel</span><span>[</span><span>grid</span><span>](</span><span>...</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>c</span>
</span></span></code></pre></div><h3 id="symmetric-quantized-matmul">Symmetric Quantized Matmul</h3>
<p>The symmetric quantized matmul is pretty straightforward. We do a matmul between two int8 tensors using our handmade kernel, and then unquantize it by multiplying by the product of scales.</p>
<figure><img src="https://blog.val.town/blog/codegen/symmetric-matmul.svg" alt="Symmetric Matmul"/>
</figure>

<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>symmetric_quantized_matmul</span><span>(</span><span>xq</span><span>,</span> <span>wq</span><span>,</span> <span>scale_x</span><span>,</span> <span>scale_w</span><span>):</span>
</span></span><span><span>    <span>yq</span> <span>=</span> <span>matmul_i8i32</span><span>(</span><span>xq</span><span>,</span> <span>wq</span><span>)</span>
</span></span><span><span>    <span>scale_y</span> <span>=</span> <span>scale_x</span> <span>*</span> <span>scale_w</span>
</span></span><span><span>    <span>return</span> <span>unquantize</span><span>(</span><span>yq</span><span>,</span> <span>scale_y</span><span>)</span>
</span></span></code></pre></div><h3 id="asymmetric-quantized-matmul">Asymmetric Quantized Matmul</h3>
<p>This one is a bit more involved because of the zero points.</p>
<figure><img src="https://blog.val.town/blog/codegen/asymmetric-matmul.svg" alt="Asymmetric Matmul"/>
</figure>

<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>asymmetric_quantized_matmul</span><span>(</span><span>xq</span><span>,</span> <span>wq</span><span>,</span> <span>scale_x</span><span>,</span> <span>scale_w</span><span>,</span> <span>zero_x</span><span>,</span> <span>zero_w</span><span>):</span>
</span></span><span><span>    <span>unscaled_y</span> <span>=</span> <span>(</span>
</span></span><span><span>        <span>matmul_ui8i32</span><span>(</span><span>xq</span><span>,</span> <span>wq</span><span>)</span>
</span></span><span><span>        <span>-</span> <span>xq</span><span>.</span><span>sum</span><span>(</span><span>1</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span> <span>*</span> <span>zero_w</span>
</span></span><span><span>        <span>-</span> <span>zero_x</span> <span>*</span> <span>wq</span><span>.</span><span>sum</span><span>(</span><span>0</span><span>,</span> <span>keepdim</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>        <span>+</span> <span>xq</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span> <span>*</span> <span>zero_x</span> <span>*</span> <span>zero_w</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>scale_x</span> <span>*</span> <span>scale_w</span> <span>*</span> <span>unscaled_y</span>
</span></span></code></pre></div><p>Note: in practice some of the terms can be precomputed.</p>
<h2 id="quantization-of-the-network">Quantization of the Network</h2>
<p>First we make a QuantizedLinear module.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>QuantizedLinear</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>linear</span><span>):</span>
</span></span><span><span>        <span>super</span><span>()</span><span>.</span><span>__init__</span><span>()</span>
</span></span><span><span>        <span>w</span><span>,</span> <span>scale</span> <span>=</span> <span>quantize</span><span>(</span><span>linear</span><span>.</span><span>weight</span><span>.</span><span>T</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>w</span> <span>=</span> <span>w</span>
</span></span><span><span>        <span>self</span><span>.</span><span>register_buffer</span><span>(</span><span>&#39;w_matrix&#39;</span><span>,</span> <span>self</span><span>.</span><span>w</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>scale_w</span> <span>=</span> <span>scale</span>
</span></span><span><span>        <span>self</span><span>.</span><span>bias</span> <span>=</span> <span>linear</span><span>.</span><span>bias</span> <span># keep bias unquantized</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>        <span>xq</span><span>,</span> <span>scale_x</span> <span>=</span> <span>quantize</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>        <span>yq</span> <span>=</span> <span>matmul_i8i32</span><span>(</span><span>xq</span><span>,</span> <span>self</span><span>.</span><span>w</span><span>)</span>
</span></span><span><span>        <span>scale_y</span> <span>=</span> <span>self</span><span>.</span><span>scale_w</span> <span>*</span> <span>scale_x</span>
</span></span><span><span>        <span>y</span> <span>=</span> <span>unquantize</span><span>(</span><span>yq</span><span>,</span> <span>scale_y</span><span>)</span>
</span></span><span><span>        <span>y</span> <span>=</span> <span>y</span> <span>+</span> <span>self</span><span>.</span><span>bias</span>
</span></span><span><span>        <span>return</span> <span>y</span>
</span></span></code></pre></div><p>We have all the pieces. Let’s write the code to quantize a network. We recursively search for <code>nn.Linear</code> modules and replace them with our quantized version.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>quantize_module</span><span>(</span><span>module</span><span>):</span>
</span></span><span><span>    <span>for</span> <span>name</span><span>,</span> <span>node</span> <span>in</span> <span>module</span><span>.</span><span>named_children</span><span>():</span>
</span></span><span><span>        <span>if</span> <span>isinstance</span><span>(</span><span>node</span><span>,</span> <span>nn</span><span>.</span><span>Linear</span><span>):</span>
</span></span><span><span>            <span>setattr</span><span>(</span><span>module</span><span>,</span> <span>name</span><span>,</span> <span>QuantizedLinear</span><span>(</span><span>node</span><span>))</span>
</span></span><span><span>        <span>else</span><span>:</span>
</span></span><span><span>            <span>quantize_module</span><span>(</span><span>node</span><span>)</span>
</span></span></code></pre></div><p>Quantize a model and test it for accuracy.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>weights</span> <span>=</span> <span>t</span><span>.</span><span>load</span><span>(</span><span>&#39;weights/mnist.pt&#39;</span><span>)</span>
</span></span><span><span><span>mnist_base</span> <span>=</span> <span>Mnist</span><span>()</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span><span>mnist_base</span><span>.</span><span>load_state_dict</span><span>(</span><span>weights</span><span>)</span>
</span></span><span><span><span>mnist</span> <span>=</span> <span>Mnist</span><span>()</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span><span>mnist</span><span>.</span><span>load_state_dict</span><span>(</span><span>weights</span><span>)</span>
</span></span><span><span><span>quantize_module</span><span>(</span><span>mnist</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;base fp32: </span><span>{</span><span>eval</span><span>(</span><span>mnist_base</span><span>)[</span><span>1</span><span>]</span><span>}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;quantized int8: </span><span>{</span><span>eval</span><span>(</span><span>mnist</span><span>)[</span><span>1</span><span>]</span><span>}</span><span>&#39;</span><span>)</span>
</span></span></code></pre></div><p>And for size.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>model_size</span><span>(</span><span>model</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>sum</span><span>(</span><span>p</span><span>.</span><span>numel</span><span>()</span> <span>*</span> <span>p</span><span>.</span><span>element_size</span><span>()</span> <span>for</span> <span>p</span> <span>in</span> <span>model</span><span>.</span><span>parameters</span><span>())</span> <span>+</span> \
</span></span><span><span>        <span>sum</span><span>(</span><span>b</span><span>.</span><span>numel</span><span>()</span> <span>*</span> <span>b</span><span>.</span><span>element_size</span><span>()</span> <span>for</span> <span>b</span> <span>in</span> <span>model</span><span>.</span><span>buffers</span><span>())</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;base fp32 size: </span><span>{</span><span>model_size</span><span>(</span><span>mnist_base</span><span>)</span><span>}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;quantized int8 size: </span><span>{</span><span>model_size</span><span>(</span><span>mnist</span><span>)</span><span>}</span><span>&#39;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>f</span><span>&#39;quantize / base ratio: </span><span>{</span><span>model_size</span><span>(</span><span>mnist</span><span>)</span> <span>/</span> <span>model_size</span><span>(</span><span>mnist_base</span><span>)</span><span>:</span><span>.2f</span><span>}</span><span>&#39;</span><span>)</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Size (bytes)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base fp32</td>
<td>0.9468</td>
<td>560424</td>
</tr>
<tr>
<td>Quantized int8</td>
<td>0.9464</td>
<td>140712</td>
</tr>
</tbody>
</table>
<p>This gives us a 4x reduction in size for a 0.04% accuracy drop.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/bag-of-holding" target="_blank" rel="noopener noreffer ">https://github.com/peluche/bag-of-holding</a></p>
<h2 id="sources">Sources</h2>
<ul>
<li>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference: <a href="https://arxiv.org/abs/1712.05877" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/1712.05877</a></li>
<li>A White Paper on Neural Network Quantization: <a href="https://arxiv.org/abs/2106.08295" target="_blank" rel="noopener noreffer ">https://arxiv.org/abs/2106.08295</a></li>
<li>Triton matmul: <a href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html" target="_blank" rel="noopener noreffer ">https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html</a></li>
</ul>
</div></div>
  </body>
</html>
