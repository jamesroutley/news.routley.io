<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jayconrod.com/posts/128/goroutines-the-concurrency-model-we-wanted-all-along">Original</a>
    <h1>Goroutines: The concurrency model we wanted all along</h1>
    
    <div id="readability-page-1" class="page"><div>








<p>I often hear criticisms of Go that seem superficial: the variable names are too short, the functions are not overloadable, the exporting-symbols-by-capitalizing thing is weird, and until recently, generics are non-existent. Go is an idiosyncratic language, and of course I have my own opinions and my own set of gripes too, but today I want to highlight one of the features I like most about the language: <em>goroutines</em>.</p>
<p>Goroutines are Go&#39;s main concurrency primitives. They look very much like threads, but they are cheap to create and manage. Go&#39;s runtime schedules goroutines onto real threads efficiently to avoid wasting resources, so you can easily create <em>lots</em> of goroutines (like one goroutine per request), and you can write simple, imperative, blocking code. Consequently, Go networking code tends to be straightforward and easier to reason about than the equivalent code in other languages.</p>
<p>For me, goroutines are the single feature that distinguishes Go from other languages. They are why I prefer Go for writing code that requires any amount of parallelism.</p>
<p>Before we talk more about goroutines, let&#39;s cover some history so it makes sense why you&#39;d want them.</p>
<h2>Forked and threaded servers</h2>
<p>High performance servers need to handle requests from many clients at the same time. There are many ways to design a server to handle that.</p>
<p>The simplest na√Øve design is to have a main process that calls <a href="https://manpages.debian.org/bookworm/manpages-dev/accept.2.en.html"><code>accept</code></a> in a loop, then calls <a href="https://manpages.debian.org/bookworm/manpages-dev/fork.2.en.html"><code>fork</code></a> to create a child process that handles the request.</p>
<p>I learned about this pattern from the excellent <a href="https://beej.us/guide/bgnet/html/">Beej&#39;s Guide to Network Programming</a> in the early 2000s when I was a student. <code>fork</code> is a nice pattern to use when learning network programming since you can focus on networking and not server architecture. It&#39;s hard to write an efficient server following this pattern though, and I don&#39;t think anyone uses it in practice anymore.</p>
<p>There are a lot of problems with <code>fork</code>. First is cost: a <code>fork</code> call on Linux appears to be fast, but it marks all your memory as copy-on-write. Each write to a copy-on-write page causes a minor page fault, a small delay that&#39;s hard to measure. Context switching between processes is also expensive. Another problem is scale: it is difficult to coordinate use of shared resources like CPU, memory, database connections, and whatever else among a large number of child processes. If you get a surge of traffic and you create too many processes, they&#39;ll contend with each other for the CPU. If you limit the number of processes you create, then a large number of slow clients can block your service for everyone while your CPU sits idle. Careful use of timeouts helps (and is necessary regardless of server architecture).</p>
<p>These problems are somewhat mitigated by using threads instead of processes. A thread is cheaper to create than a process since it shares memory and most other resources. It&#39;s also relatively easy to communicate between threads in a shared address space, using semaphores and other constructs to manage shared resources.</p>
<p>Threads do still have a substantial cost though, and you&#39;ll run into scaling problems if you create a new thread for every connection. As with processes, you need to limit the number of running threads to avoid heavy CPU contention, and you need to time out slow requests. Creating a new thread still takes time, though you can mitigate that by recycling threads across requests with a thread pool.</p>
<p>Whether you&#39;re using processes or threads, you still have a question that&#39;s difficult to answer: how many should you create? If you allow an unlimited number of threads, clients can use up all your memory and CPU with a small surge in traffic. If you limit your server to a maximum number of threads, then a bunch of slow clients can clog up your server. Timeouts help, but it&#39;s still difficult use your hardware resources efficiently.</p>
<h2>Event-driven servers</h2>
<p>Since we can&#39;t easily predict how many threads we&#39;ll need, what happens when we try to decouple requests from threads? What if we have just one thread dedicated to application logic (or perhaps a small, fixed number of threads), and we handle all the network traffic in the background using asynchronous system calls? This is an <em>event-driven server architecture</em>.</p>
<p>Event-driven servers were designed around the <a href="https://manpages.debian.org/unstable/manpages-dev/select.2.en.html"><code>select</code></a> system call. (Later mechanisms like <a href="https://manpages.debian.org/bookworm/manpages-dev/poll.2.en.html"><code>poll</code></a> have replaced <code>select</code>, but <code>select</code> is widely known, and they all serve the same conceptual purpose here.) <code>select</code> accepts a list of file descriptors (generally sockets) and returns which ones are ready to read or write. If none of the file descriptors are ready, <code>select</code> blocks until at least one is.</p>
<pre><code><span>#include &lt;sys/select.h&gt;</span>

<span>int</span> <span>select</span>(<span>int</span> <span>nfds</span>, <span>fd_set</span> *<span>restrict</span> <span>readfds</span>,
           <span>fd_set</span> *<span>restrict</span> <span>writefds</span>, <span>fd_set</span> *<span>restrict</span> <span>exceptfds</span>,
           <span>struct</span> <span>timeval</span> *<span>restrict</span> <span>timeout</span>);
</code></pre>
<pre><code><span>#include &lt;poll.h&gt;</span>

<span>int</span> <span>poll</span>(<span>struct</span> <span>pollfd</span> *<span>fds</span>, <span>nfds_t</span> <span>nfds</span>, <span>int</span> <span>timeout</span>);
</code></pre>
<p>(I want to elaborate a bit on what it means for a socket to be &#34;ready&#34; because I had some trouble understanding this initially. Each socket has a kernel buffer for receiving and a buffer for sending. When your computer receives a packet, the kernel stores its data in the receive buffer for the appropriate socket until your program gets it with <code>recv</code>. Likewise, when your program calls <code>send</code>, the kernel stores the data in the socket&#39;s send buffer until it can be transmitted. A socket is ready to receive if there&#39;s data in the receive buffer. It&#39;s ready to send if there&#39;s available space in the send buffer. If the socket is not ready, <code>recv</code> and <code>send</code> block until it is, by default.)</p>
<p>To implement an event-driven server, you track a socket and some state for each request that&#39;s blocked on the network. The server has a single main event loop where it calls <code>select</code> with all those blocked sockets. When <code>select</code> returns, the server knows which requests can make progress, so for each request, it calls into the application logic with the stored state. When the application needs to use the network again, it adds the socket back to the &#34;blocked&#34; pool together with new state. The <em>state</em> here can be anything the application needs to resume what it was doing: a closure to be called back, or a promise to be completed.</p>
<p>This can all technically be implemented with a single thread. I can&#39;t speak to the details of any particular implementation, but languages that lack threading like JavaScript follow this model pretty closely. <a href="https://nodejs.org/en/about">Node.js describes itself</a> as &#34;an event-driven JavaScript runtime ... designed to build scalable network applications.&#34; This is exactly what they mean.</p>
<p>Event-driven servers can generally make better use of CPU and memory than purely fork- or thread-based servers. You can spawn an application thread per core to handle requests in parallel. The threads don&#39;t contend with each other for CPU since the number of threads equals the number of cores. The threads are never idle when there are requests that can make progress. Efficient. So efficient that it&#39;s hard to justify writing a server any other way these days.</p>
<p>It sounds like a good idea on paper anyway, but it&#39;s a nightmare to write application code that works like this. The specific way in which it&#39;s a nightmare depends on the language and framework you&#39;re using. In JavaScript, asynchronous functions typically return a <code>Promise</code>, to which you attach callbacks. In Java gRPC, you&#39;re dealing with <code>StreamObserver</code>. If you&#39;re not careful, you end up with lots of deeply nested &#34;arrow code&#34; functions. If you <em>are</em> careful, you split up your functions and classes, obfuscating your control flow. Either way, you&#39;re in callback hell.</p>
<p>To show what I mean, below is an example from the <a href="https://grpc.io/docs/languages/java/basics/#bidirectional-streaming-rpc">Java gRPC Basics Tutorial</a>. Does the control flow here make sense?</p>
<pre><code><span>public</span> <span>void</span> <span>routeChat</span>() <span>throws</span> <span>Exception</span> {
  <span>info</span>(<span>&#34;*** RoutChat&#34;</span>);
  <span>final</span> <span>CountDownLatch</span> <span>finishLatch</span> = <span>new</span> <span>CountDownLatch</span>(<span>1</span>);
  <span>StreamObserver</span>&lt;<span>RouteNote</span>&gt; <span>requestObserver</span> =
      <span>asyncStub</span>.<span>routeChat</span>(<span>new</span> <span>StreamObserver</span>&lt;<span>RouteNote</span>&gt;() {
        @<span>Override</span>
        <span>public</span> <span>void</span> <span>onNext</span>(<span>RouteNote</span> <span>note</span>) {
          <span>info</span>(<span>&#34;Got message \&#34;</span>{<span>0</span>}\<span>&#34; at {1}, {2}&#34;</span>, <span>note</span>.<span>getMessage</span>(), <span>note</span>.<span>getLocation</span>()
              .<span>getLatitude</span>(), <span>note</span>.<span>getLocation</span>().<span>getLongitude</span>());
        }

        @<span>Override</span>
        <span>public</span> <span>void</span> <span>onError</span>(<span>Throwable</span> <span>t</span>) {
          <span>Status</span> <span>status</span> = <span>Status</span>.<span>fromThrowable</span>(<span>t</span>);
          <span>logger</span>.<span>log</span>(<span>Level</span>.<span>WARNING</span>, <span>&#34;RouteChat Failed: {0}&#34;</span>, <span>status</span>);
          <span>finishLatch</span>.<span>countDown</span>();
        }

        @<span>Override</span>
        <span>public</span> <span>void</span> <span>onCompleted</span>() {
          <span>info</span>(<span>&#34;Finished RouteChat&#34;</span>);
          <span>finishLatch</span>.<span>countDown</span>();
        }
      });

  <span>try</span> {
    <span>RouteNote</span>[] <span>requests</span> =
        {<span>newNote</span>(<span>&#34;First message&#34;</span>, <span>0</span>, <span>0</span>), <span>newNote</span>(<span>&#34;Second message&#34;</span>, <span>0</span>, <span>1</span>),
            <span>newNote</span>(<span>&#34;Third message&#34;</span>, <span>1</span>, <span>0</span>), <span>newNote</span>(<span>&#34;Fourth message&#34;</span>, <span>1</span>, <span>1</span>)};

    <span>for</span> (<span>RouteNote</span> <span>request</span> : <span>requests</span>) {
      <span>info</span>(<span>&#34;Sending message \&#34;</span>{<span>0</span>}\<span>&#34; at {1}, {2}&#34;</span>, <span>request</span>.<span>getMessage</span>(), <span>request</span>.<span>getLocation</span>()
          .<span>getLatitude</span>(), <span>request</span>.<span>getLocation</span>().<span>getLongitude</span>());
      <span>requestObserver</span>.<span>onNext</span>(<span>request</span>);
    }
  } <span>catch</span> (<span>RuntimeException</span> <span>e</span>) {
    
    <span>requestObserver</span>.<span>onError</span>(<span>e</span>);
    <span>throw</span> <span>e</span>;
  }
  
  <span>requestObserver</span>.<span>onCompleted</span>();

  
  <span>finishLatch</span>.<span>await</span>(<span>1</span>, <span>TimeUnit</span>.<span>MINUTES</span>);
}
</code></pre>
<p>This is from the <em>beginner</em> tutorial, and it&#39;s not a complete example either. The sending code is synchronous while the receiving code is asynchronous.</p>
<p>In Java, you may be dealing different with asynchronous types for your HTTP server, gRPC, SQL database, cloud SDK and whatever else, and you need adapters between all of them. It gets to be a mess very quickly. Locks are dangerous, too. You need to be careful about holding locks across network calls. It&#39;s also easy to make a mistake with locking and callbacks. For example, if a <code>synchronized</code> method calls a function that returns a <code>ListenableFuture</code> then attaches an inline callback, the callback also needs a <code>synchronized</code> block, even though it&#39;s nested inside the parent method.</p>
<h2>goroutines</h2>
<p>What was this post about again? Oh yes, goroutines.</p>
<p>A <em>goroutine</em> is Go&#39;s version of a thread. Like threads in other languages, each goroutine has its own stack. Goroutines may execute in parallel, concurrently with other goroutines. Unlike threads, goroutines are very cheap to create: they aren&#39;t bound to an OS thread, and their stacks start out very small (2 KiB) but can grow as needed. When you create a goroutine, you&#39;re essentially allocating a closure and adding it to a queue in the runtime.</p>
<p>Internally, Go&#39;s runtime has a set of OS threads that execute goroutines (normally one thread per core). When a thread is available and a goroutine is ready to run, the runtime schedules the goroutine onto the thread, executing application logic. If a goroutine blocks on something like a mutex or channel, the runtime adds it to a set of blocked goroutines then schedules the next ready goroutine onto the same OS thread. <em>This also applies to the network</em>: when a goroutine sends or receives data on a socket that&#39;s not ready, it yields its OS thread to the scheduler.</p>
<p>Sound familiar? Go&#39;s scheduler acts a lot like the main loop in an event-driven server. Except instead of relying solely on <code>select</code> and focusing on file descriptors, the scheduler handles everything in the language that might block. You no longer need to avoid blocking calls because the scheduler makes efficient use of the CPU either way. You&#39;re free to spawn lots of goroutines (one per request!) because they&#39;re cheap to create, and the threads don&#39;t contend for the CPU (minimal context switching). You don&#39;t need to worry about thread pools and executor services because the runtime effectively has one big thread pool.</p>
<p>In short, you can write simple blocking application code in a clean imperative style as if you were writing a thread-based server, but you keep all the efficiency advantages of an event-driven server. Best of both worlds. This kind of code composes well across frameworks. You don&#39;t need adapters between your <code>StreamObservers</code> and <code>ListenableFutures</code>.</p>
<p>Let&#39;s take a look at the same example from the <a href="https://grpc.io/docs/languages/go/basics/#bidirectional-streaming-rpc-1">Go gRPC Basics Tutorial</a>. I find the control flow here easier to comprehend than the Java example because both the sending and receiving code are synchronous. In both goroutines, we&#39;re able to call <code>stream.Recv</code> and <code>stream.Send</code> in a <code>for</code> loop. No need for callbacks, subclasses, or executors.</p>
<pre><code><span>stream</span>, <span>err</span> := <span>client</span>.<span>RouteChat</span>(<span>context</span>.<span>Background</span>())
<span>waitc</span> := <span>make</span>(<span>chan</span> <span>struct</span>{})
<span>go</span> <span>func</span>() {
  <span>for</span> {
    <span>in</span>, <span>err</span> := <span>stream</span>.<span>Recv</span>()
    <span>if</span> <span>err</span> == <span>io</span>.<span>EOF</span> {
      
      <span>close</span>(<span>waitc</span>)
      <span>return</span>
    }
    <span>if</span> <span>err</span> != <span>nil</span> {
      <span>log</span>.<span>Fatalf</span>(<span>&#34;Failed to receive a note : %v&#34;</span>, <span>err</span>)
    }
    <span>log</span>.<span>Printf</span>(<span>&#34;Got message %s at point(%d, %d)&#34;</span>, <span>in</span>.<span>Message</span>, <span>in</span>.<span>Location</span>.<span>Latitude</span>, <span>in</span>.<span>Location</span>.<span>Longitude</span>)
  }
}()
<span>for</span> <span>_</span>, <span>note</span> := <span>range</span> <span>notes</span> {
  <span>if</span> <span>err</span> := <span>stream</span>.<span>Send</span>(<span>note</span>); <span>err</span> != <span>nil</span> {
    <span>log</span>.<span>Fatalf</span>(<span>&#34;Failed to send a note: %v&#34;</span>, <span>err</span>)
  }
}
<span>stream</span>.<span>CloseSend</span>()
&lt;-<span>waitc</span>
</code></pre>
<h2><code>async</code> / <code>await</code>, virtual threads</h2>
<p>Go is a fairly new language, and it was designed at a time when event-driven servers were already popular and well-understood. Java, C++, Python, and other languages are older and don&#39;t have that same luxury. So as a language designer, what features can you add to make writing asynchronous application code less painful?</p>
<p><a href="https://en.wikipedia.org/wiki/Async/await"><code>async</code> / <code>await</code></a> has become the main solution in most languages. Wikipedia tells me that it was first added to F# in 2007 (around the same time Go was being developed). It&#39;s now in C#, C++, JavaScript, Python, Rust, Swift, and a bunch of other languages.</p>
<p><code>async</code> / <code>await</code> let you write code that works with asynchronous functions in a style that resembles imperative blocking code. An asynchronous function is marked with the <code>async</code> keyword and returns a promise (or whatever the language equivalent is). When you call an asynchronous function, you can &#34;block&#34; and get the value in the promise with the <code>await</code> keyword. If a function uses <code>await</code>, it must also be marked <code>async</code>, and the compiler rewrites it to return a promise. There are different ways to implement <code>async</code> and <code>await</code>, but the feature generally implies a language has support for cooperatively scheduled coroutines if not full support for threads. When a coroutine <code>awaits</code> a promise that&#39;s not completed yet, it yields to the language&#39;s runtime, which may schedule another ready coroutine on the same thread.</p>
<p>I&#39;m pleased that so many languages have adopted <code>async</code> / <code>await</code>. Since I&#39;ve worked primarily in Go, Java, and C++, I haven&#39;t personally had much opportunity to use it, but whenever I need to do something in JavaScript, I&#39;m really glad it&#39;s there. <code>await</code> control flow is much easier to understand than asynchronous callback code. It is a bit annoying though to annotate functions with <code>async</code>. Bob Nystrom&#39;s <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">What Color is Your Function</a> is a humorous criticism of that.</p>
<p>Java is conspicuously absent from the list of languages above. Until now, you&#39;ve had to either spawn an unreasonable number of threads or deal with Java&#39;s particular circle of callback hell. Happily, <a href="https://openjdk.org/jeps/444">JEP 444</a> adds <a href="https://blog.rockthejvm.com/ultimate-guide-to-java-virtual-threads/">virtual threads</a>, which sound a lot like goroutines. Virtual threads are cheap to create. The JVM schedules them onto <em>platform threads</em> (real threads the kernel knows about). There are a fixed number of platform threads, generally one per core. When a virtual thread performs a blocking operation, it releases its platform thread, and the JVM may schedule another virtual thread onto it. Unlike goroutines, virtual thread scheduling is cooperative: a virtual thread doesn&#39;t yield to the scheduler until it performs a blocking operation. This means that a tight loop can hold a thread indefinitely. I don&#39;t know whether this is an implementation limitation or if there&#39;s a deeper issue. Go used to have this problem until fully preemptive scheduling was implemented in 1.14.</p>
<p>Virtual threads are available for preview in the current release and are expected to become stable in JDK 21 (due September 2023). I&#39;m looking forward to deleting a lot of <code>ListenableFutures</code>, but it&#39;s unclear to me how virtual threads will interact with existing frameworks designed around a more purely event-driven model. Whenever a new language or runtime feature is introduced, there&#39;s a long cultural migration period, and I think the Java ecosystem is pretty conservative in that regard. Still, I&#39;m encouraged by the enthusiastic adoption of <code>async</code> / <code>await</code> in other languages. I hope Java will be similar with virtual threads.</p>


</div></div>
  </body>
</html>
