<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/monkeypatch/monkeypatch.py">Original</a>
    <h1>Show HN: MonkeyPatch – Cheap, fast and predictable LLM functions in Python</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">The easiest way to build scalable, LLM-powered functions and applications that get cheaper and faster the more you use them.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contents" aria-hidden="true" tabindex="-1" href="#contents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contents</h2>

<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#installation-and-getting-started">Installation and Getting Started</a></li>
<li><a href="#how-it-works">How It Works</a></li>
<li><a href="#typed-outputs">Typed Outputs</a></li>
<li><a href="#test-driven-alignment">Test-Driven Alignment</a></li>
<li><a href="#scaling-and-finetuning">Scaling and Finetuning</a></li>
<li><a href="#frequently-asked-questions">Frequently Asked Questions</a></li>
<li><a href="#simple-todo-list-app">Simple ToDo List App</a></li>
</ul>

<a name="user-content-introduction"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-introduction" aria-hidden="true" tabindex="-1" href="#introduction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Introduction</h2>
<p dir="auto">MonkeyPatch is a way to easily call an LLM in place of the function body in Python, with the same parameters and output that you would expect from a function implemented by hand.</p>
<p dir="auto">These LLM-powered functions are well-typed, reliable, stateless, and production-ready to be dropped into your app seamlessly. Rather than endless prompt-wrangling and nasty surprises, these LLM-powered functions and applications behave like traditional functions with proper error handling.</p>
<p dir="auto">Lastly, the more you use MonkeyPatch functions, the cheaper and faster they gets (up to 9-10x!) through automatic model distillation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@monkey.patch
def some_function(input: TypedInput) -&gt; TypedOutput:
    &#34;&#34;&#34;(Optional) Include the description of how your function will be used.&#34;&#34;&#34;

@monkey.align
def test_some_function(example_typed_input: TypedInput, 
                       example_typed_output: TypedOutput):
	
    assert some_function(example_typed_input) == example_typed_output
	"><pre><span>@<span>monkey</span>.<span>patch</span></span>
<span>def</span> <span>some_function</span>(<span>input</span>: <span>TypedInput</span>) <span>-&gt;</span> <span>TypedOutput</span>:
    <span>&#34;&#34;&#34;(Optional) Include the description of how your function will be used.&#34;&#34;&#34;</span>

<span>@<span>monkey</span>.<span>align</span></span>
<span>def</span> <span>test_some_function</span>(<span>example_typed_input</span>: <span>TypedInput</span>, 
                       <span>example_typed_output</span>: <span>TypedOutput</span>):
	
    <span>assert</span> <span>some_function</span>(<span>example_typed_input</span>) <span>==</span> <span>example_typed_output</span>
	</pre></div>
<a name="user-content-features"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-features" aria-hidden="true" tabindex="-1" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<ul dir="auto">
<li><strong>Easy and seamless integration</strong> - Add LLM augmented functions to any workflow within seconds. Decorate a function stub with <code>@monkey.patch</code> and optionally add type hints and docstrings to guide the execution. That’s it.</li>
<li><strong>Type aware</strong> - Ensure that the outputs of the LLM adhere to the type constraints of the function (Python Base types, Pydantic classes, Literals, Generics etc) to guard against bugs or unexpected side-effects of using LLMs.</li>
<li><strong>Aligned outputs</strong> - LLMs are unreliable, which makes them difficult to use in place of classically programmed functions. Using simple assert statements in a function decorated with <code>@monkey.align</code>, you can align the behaviour of your patched function to what you expect.</li>
<li><strong>Lower cost and latency</strong> - Achieve up to 90% lower cost and 80% lower latency with increased usage. The package will take care of model training, MLOps and DataOps efforts to improve LLM capabilities through distillation.</li>
<li><strong>Batteries included</strong> - No remote dependencies other than OpenAI.</li>
</ul>
<a name="user-content-installation-and-getting-started"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-installation-and-getting-started" aria-hidden="true" tabindex="-1" href="#installation-and-getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation and Getting Started</h2>
<a name="user-content-installation"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" tabindex="-1" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h3>
<div data-snippet-clipboard-copy-content="pip install monkey-patch.py"><pre><code>pip install monkey-patch.py
</code></pre></div>
<p dir="auto">or with Poetry</p>
<div data-snippet-clipboard-copy-content="poetry add monkey-patch.py"><pre><code>poetry add monkey-patch.py
</code></pre></div>
<p dir="auto">Set your OpenAI key using:</p>
<div data-snippet-clipboard-copy-content="export OPENAI_API_KEY=sk-..."><pre><code>export OPENAI_API_KEY=sk-...
</code></pre></div>
<a name="user-content-getting-started"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h3>
<p dir="auto">To get started:</p>
<ol dir="auto">
<li>Create a python function stub decorated with <code>@monkey.patch</code> including type hints and a docstring.</li>
<li>(Optional) Create another function decorated with <code>@monkey.align</code> containing normal <code>assert</code> statements declaring the expected behaviour of your patched function with different inputs.</li>
</ol>
<p dir="auto">The patched function can now be called as normal in the rest of your code.</p>
<p dir="auto">To add functional alignment, the functions annotated with <code>align</code> must also be called if:</p>
<ul dir="auto">
<li>It is the first time calling the patched function (including any updates to the function signature, i.e docstring, input arguments, input type hints, naming or the output type hint)</li>
<li>You have made changes to your assert statements.</li>
</ul>
<p dir="auto">Here is what it could look like for a simple classification function:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@monkey.patch
def classify_sentiment(msg: str) -&gt; Optional[Literal[&#39;Good&#39;, &#39;Bad&#39;]]:
    &#34;&#34;&#34;Classifies a message from the user into Good, Bad or None.&#34;&#34;&#34;

@monkey.align
def align_classify_sentiment():
    assert classify_sentiment(&#34;I love you&#34;) == &#39;Good&#39;
    assert classify_sentiment(&#34;I hate you&#34;) == &#39;Bad&#39;
    assert not classify_sentiment(&#34;People from Phoenix are called Phoenicians&#34;)

if __name__ == &#34;__main__&#34;:
    align_classify_sentiment()
    print(classify_sentiment(&#34;I like you&#34;)) # Good
    print(classify_sentiment(&#34;Apples might be red&#34;)) # None"><pre><span>@<span>monkey</span>.<span>patch</span></span>
<span>def</span> <span>classify_sentiment</span>(<span>msg</span>: <span>str</span>) <span>-&gt;</span> <span>Optional</span>[<span>Literal</span>[<span>&#39;Good&#39;</span>, <span>&#39;Bad&#39;</span>]]:
    <span>&#34;&#34;&#34;Classifies a message from the user into Good, Bad or None.&#34;&#34;&#34;</span>

<span>@<span>monkey</span>.<span>align</span></span>
<span>def</span> <span>align_classify_sentiment</span>():
    <span>assert</span> <span>classify_sentiment</span>(<span>&#34;I love you&#34;</span>) <span>==</span> <span>&#39;Good&#39;</span>
    <span>assert</span> <span>classify_sentiment</span>(<span>&#34;I hate you&#34;</span>) <span>==</span> <span>&#39;Bad&#39;</span>
    <span>assert</span> <span>not</span> <span>classify_sentiment</span>(<span>&#34;People from Phoenix are called Phoenicians&#34;</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>align_classify_sentiment</span>()
    <span>print</span>(<span>classify_sentiment</span>(<span>&#34;I like you&#34;</span>)) <span># Good</span>
    <span>print</span>(<span>classify_sentiment</span>(<span>&#34;Apples might be red&#34;</span>)) <span># None</span></pre></div>
<a name="user-content-how-it-works"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" tabindex="-1" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How It Works</h2>
<p dir="auto">When you call a monkey-patched function during development, an LLM in a n-shot configuration is invoked to generate the typed response.</p>
<p dir="auto">The number of examples used is dependent on the number of align statements supplied in functions annotated with the align decorator.</p>
<p dir="auto">The response will be post-processed and the supplied output type will be programmatically instantiated ensuring that the correct type is returned.</p>
<p dir="auto">This response can be passed through to the rest of your app / stored in the DB / displayed to the user.</p>
<p dir="auto">Make sure to execute all align functions at least once before running your patched functions to ensure that the expected behaviour is registered. These are cached onto the disk for future reference.</p>
<p dir="auto">The inputs and outputs of the function will be stored during execution as future training data.
As your data volume increases, smaller and smaller models will be distilled using the outputs of larger models.</p>
<p dir="auto">The smaller models will capture the desired behaviour and performance at a lower computational cost, lower latency and without any MLOps effort.</p>
<a name="user-content-typed-outputs"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-typed-outputs" aria-hidden="true" tabindex="-1" href="#typed-outputs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Typed Outputs</h2>
<p dir="auto">LLM API outputs are typically in natural language. In many instances, it’s preferable to have constraints on the format of the output to integrate them better into workflows.</p>
<p dir="auto">A core concept of monkey-patch is the support for typed parameters and outputs. Supporting typed outputs of patched functions allows you to declare <em>rules about what kind of data the patched function is allowed to pass back</em> for use in the rest of your program. This will guard against the verbose or inconsistent outputs of the LLMs that are trained to be as “helpful as possible”.</p>
<p dir="auto">You can use Literals or create custom types in Pydantic to express very complex rules about what the patched function can return. These act as guard-rails for the model preventing a patched function breaking the code or downstream workflows, and means you can avoid having to write custom validation logic in your application.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@dataclass
class ActionItem:
    goal: str = Field(description=&#34;What task must be completed&#34;)
    deadline: datetime = Field(description=&#34;The date the goal needs to be achieved&#34;)
    
@monkey.patch
def action_items(input: str) -&gt; List[ActionItem]:
    &#34;&#34;&#34;Generate a list of Action Items&#34;&#34;&#34;

@monkey.align
def align_action_items():
    goal = &#34;Can you please get the presentation to me by Tuesday?&#34;
    next_tuesday = (datetime.now() + timedelta((1 - datetime.now().weekday() + 7) % 7)).replace(hour=0, minute=0, second=0, microsecond=0)

    assert action_items(goal) == ActionItem(goal=&#34;Prepare the presentation&#34;, deadline=next_tuesday)"><pre><span>@<span>dataclass</span></span>
<span>class</span> <span>ActionItem</span>:
    <span>goal</span>: <span>str</span> <span>=</span> <span>Field</span>(<span>description</span><span>=</span><span>&#34;What task must be completed&#34;</span>)
    <span>deadline</span>: <span>datetime</span> <span>=</span> <span>Field</span>(<span>description</span><span>=</span><span>&#34;The date the goal needs to be achieved&#34;</span>)
    
<span>@<span>monkey</span>.<span>patch</span></span>
<span>def</span> <span>action_items</span>(<span>input</span>: <span>str</span>) <span>-&gt;</span> <span>List</span>[<span>ActionItem</span>]:
    <span>&#34;&#34;&#34;Generate a list of Action Items&#34;&#34;&#34;</span>

<span>@<span>monkey</span>.<span>align</span></span>
<span>def</span> <span>align_action_items</span>():
    <span>goal</span> <span>=</span> <span>&#34;Can you please get the presentation to me by Tuesday?&#34;</span>
    <span>next_tuesday</span> <span>=</span> (<span>datetime</span>.<span>now</span>() <span>+</span> <span>timedelta</span>((<span>1</span> <span>-</span> <span>datetime</span>.<span>now</span>().<span>weekday</span>() <span>+</span> <span>7</span>) <span>%</span> <span>7</span>)).<span>replace</span>(<span>hour</span><span>=</span><span>0</span>, <span>minute</span><span>=</span><span>0</span>, <span>second</span><span>=</span><span>0</span>, <span>microsecond</span><span>=</span><span>0</span>)

    <span>assert</span> <span>action_items</span>(<span>goal</span>) <span>==</span> <span>ActionItem</span>(<span>goal</span><span>=</span><span>&#34;Prepare the presentation&#34;</span>, <span>deadline</span><span>=</span><span>next_tuesday</span>)</pre></div>
<p dir="auto">By constraining the types of data that can pass through your patched function, you are declaring the potential outputs that the model can return and specifying the world where the program exists in.</p>
<p dir="auto">You can add integer constraints to the outputs for Pydantic field values, and generics if you wish.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@monkey.patch
def score_sentiment(input: str) -&gt; Optional[Annotated[int, Field(gt=0, lt=10)]]:
    &#34;&#34;&#34;Scores the input between 0-10&#34;&#34;&#34;

@monkey.align
def align_score_sentiment():
    &#34;&#34;&#34;Register several examples to align your function&#34;&#34;&#34;
    assert score_sentiment(&#34;I love you&#34;) == 10
    assert score_sentiment(&#34;I hate you&#34;) == 0
    assert score_sentiment(&#34;You&#39;re okay I guess&#34;) == 5

# This is a normal test that can be invoked with pytest or unittest
def test_score_sentiment():
    &#34;&#34;&#34;We can test the function as normal using Pytest or Unittest&#34;&#34;&#34;
    score = score_sentiment(&#34;I like you&#34;) 
    assert score &gt;= 7

if __name__ == &#34;__main__&#34;:
    align_score_sentiment()
    print(score_sentiment(&#34;I like you&#34;)) # 7
    print(score_sentiment(&#34;Apples might be red&#34;)) # None"><pre><span>@<span>monkey</span>.<span>patch</span></span>
<span>def</span> <span>score_sentiment</span>(<span>input</span>: <span>str</span>) <span>-&gt;</span> <span>Optional</span>[<span>Annotated</span>[<span>int</span>, <span>Field</span>(<span>gt</span><span>=</span><span>0</span>, <span>lt</span><span>=</span><span>10</span>)]]:
    <span>&#34;&#34;&#34;Scores the input between 0-10&#34;&#34;&#34;</span>

<span>@<span>monkey</span>.<span>align</span></span>
<span>def</span> <span>align_score_sentiment</span>():
    <span>&#34;&#34;&#34;Register several examples to align your function&#34;&#34;&#34;</span>
    <span>assert</span> <span>score_sentiment</span>(<span>&#34;I love you&#34;</span>) <span>==</span> <span>10</span>
    <span>assert</span> <span>score_sentiment</span>(<span>&#34;I hate you&#34;</span>) <span>==</span> <span>0</span>
    <span>assert</span> <span>score_sentiment</span>(<span>&#34;You&#39;re okay I guess&#34;</span>) <span>==</span> <span>5</span>

<span># This is a normal test that can be invoked with pytest or unittest</span>
<span>def</span> <span>test_score_sentiment</span>():
    <span>&#34;&#34;&#34;We can test the function as normal using Pytest or Unittest&#34;&#34;&#34;</span>
    <span>score</span> <span>=</span> <span>score_sentiment</span>(<span>&#34;I like you&#34;</span>) 
    <span>assert</span> <span>score</span> <span>&gt;=</span> <span>7</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:
    <span>align_score_sentiment</span>()
    <span>print</span>(<span>score_sentiment</span>(<span>&#34;I like you&#34;</span>)) <span># 7</span>
    <span>print</span>(<span>score_sentiment</span>(<span>&#34;Apples might be red&#34;</span>)) <span># None</span></pre></div>
<p dir="auto">To see more examples using MonkeyPatch for different use cases (including how to integrate with FastAPI), have a look at <a href="https://github.com/monkey-patch-sdk/monkey-patch.py/tree/master/examples">examples</a>.</p>
<a name="user-content-test-driven-alignment"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-test-driven-alignment" aria-hidden="true" tabindex="-1" href="#test-driven-alignment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Test-Driven Alignment</h2>
<p dir="auto">In classic <a href="https://en.wikipedia.org/wiki/Test-driven_development" rel="nofollow">test-driven development (TDD)</a>, the standard practice is to write a failing test before writing the code that makes it pass.</p>
<p dir="auto">Test-Driven Alignment (TDA) adapts this concept to align the behavior of a patched function with an expectation defined by a test.</p>
<p dir="auto">To align the behaviour of your patched function to your needs, decorate a function with <code>@align</code> and assert the outputs of the function with the ‘assert’ statement as is done with standard tests.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@monkey.align 
def align_classify_sentiment(): 
    assert classify_sentiment(&#34;I love this!&#34;) == &#39;Good&#39; 
    assert classify_sentiment(&#34;I hate this.&#34;) == &#39;Bad&#39;
   
@monkey.align
def align_score_sentiment():
    assert score_sentiment(&#34;I like you&#34;) == 7"><pre><span>@<span>monkey</span>.<span>align</span></span> 
<span>def</span> <span>align_classify_sentiment</span>(): 
    <span>assert</span> <span>classify_sentiment</span>(<span>&#34;I love this!&#34;</span>) <span>==</span> <span>&#39;Good&#39;</span> 
    <span>assert</span> <span>classify_sentiment</span>(<span>&#34;I hate this.&#34;</span>) <span>==</span> <span>&#39;Bad&#39;</span>
   
<span>@<span>monkey</span>.<span>align</span></span>
<span>def</span> <span>align_score_sentiment</span>():
    <span>assert</span> <span>score_sentiment</span>(<span>&#34;I like you&#34;</span>) <span>==</span> <span>7</span></pre></div>
<p dir="auto">By writing a test that encapsulates the expected behaviour of the monkeypatched function, you declare the contract that the function must fulfill. This enables you to:</p>
<ol dir="auto">
<li><strong>Verify Expectations:</strong> Confirm that the function adheres to the desired output.</li>
<li><strong>Capture Behavioural Nuances:</strong> Make sure that the LLM respects the edge cases and nuances stipulated by your test.</li>
<li><strong>Develop Iteratively:</strong> Refine and update the behavior of the monkeypatched function by declaring the desired behaviour as tests.</li>
</ol>
<p dir="auto">Unlike traditional TDD, where the objective is to write code that passes the test, TDA flips the script: <strong>tests do not fail</strong>. Their existence and the form they take are sufficient for LLMs to align themselves with the expected behavior.</p>
<p dir="auto">TDA offers a lean yet robust methodology for grafting machine learning onto existing or new Python codebases. It combines the preventive virtues of TDD while addressing the specific challenges posed by the dynamism of LLMs.</p>
<hr/>
<p dir="auto">(Aligning function chains is work in progress)</p>
<div dir="auto" data-snippet-clipboard-copy-content="def test_score_sentiment():
    &#34;&#34;&#34;We can test the function as normal using Pytest or Unittest&#34;&#34;&#34;
    assert multiply_by_two(score_sentiment(&#34;I like you&#34;)) == 14
    assert 2*score_sentiment(&#34;I like you&#34;) == 14"><pre><span>def</span> <span>test_score_sentiment</span>():
    <span>&#34;&#34;&#34;We can test the function as normal using Pytest or Unittest&#34;&#34;&#34;</span>
    <span>assert</span> <span>multiply_by_two</span>(<span>score_sentiment</span>(<span>&#34;I like you&#34;</span>)) <span>==</span> <span>14</span>
    <span>assert</span> <span>2</span><span>*</span><span>score_sentiment</span>(<span>&#34;I like you&#34;</span>) <span>==</span> <span>14</span></pre></div>
<a name="user-content-scaling-and-finetuning"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-scaling-and-finetuning" aria-hidden="true" tabindex="-1" href="#scaling-and-finetuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Scaling and Finetuning</h2>
<p dir="auto">An advantage of using MonkeyPatch in your workflow is the cost and latency benefits that will be provided as the number of datapoints increases.</p>
<p dir="auto">Successful executions of your patched function suitable for finetuning will be persisted to a training dataset, which will be used to distil smaller models for each patched function. Model distillation and pseudo-labelling is a verified way how to cut down on model sizes and gain improvements in latency and memory footprints while incurring insignificant and minor cost to performance (<a href="https://arxiv.org/pdf/2305.02301.pdf" rel="nofollow">https://arxiv.org/pdf/2305.02301.pdf</a>, <a href="https://arxiv.org/pdf/2306.13649.pdf" rel="nofollow">https://arxiv.org/pdf/2306.13649.pdf</a>, <a href="https://arxiv.org/pdf/2311.00430.pdf" rel="nofollow">https://arxiv.org/pdf/2311.00430.pdf</a>, etc).</p>
<p dir="auto">Training smaller function-specific models and deploying them is handled by the MonkeyPatch library, so the user will get the benefits without any additional MLOps or DataOps effort. Currently only OpenAI GPT style models are supported (Teacher - GPT4, Student GPT-3.5)</p>
<p dir="auto">We tested out model distillation using MonkeyPatch using OpenAI models on Squad2, Spider and IMDB Movie Reviews datasets. We finetuned the gpt-3.5-turbo model (student) using few-shot responses of gpt-4 (teacher) and our preliminary tests show that using less than 600 datapoints in the training data we were able to get gpt 3.5 turbo to perform essentialy equivalent (less than 1.5% of performance difference on held-out dev sets) to gpt4 while achieving up to 12 times lower cost and over 6 times lower latency (cost and latency reduction are very dependent on task specific characteristics like input-output token sizes and align statement token sizes). These tests show the potential in model-distillation in this form for intelligently cutting costs and lowering latency without sacrificing performance.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/113173969/282715762-2ac4c2fd-7ba6-4598-891d-6aa2c85827c9.png"><img src="https://user-images.githubusercontent.com/113173969/282715762-2ac4c2fd-7ba6-4598-891d-6aa2c85827c9.png" alt="Example distillation results"/></a></p>
<a name="user-content-frequently-asked-questions"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-frequently-asked-questions" aria-hidden="true" tabindex="-1" href="#frequently-asked-questions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Frequently Asked Questions</h2>
<a name="user-content-intro"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-intro" aria-hidden="true" tabindex="-1" href="#intro"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Intro</h3>
<a name="user-content-what-is-monkey-patch-in-plain-words"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-what-is-monkeypatch-in-plain-words" aria-hidden="true" tabindex="-1" href="#what-is-monkeypatch-in-plain-words"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>What is MonkeyPatch in plain words?</h4>
<p dir="auto">MonkeyPatch is a simple and seamless way to create LLM augmented functions in python, which ensure the outputs of the LLMs follow a specific structure. Moreover, the more you call a patched function, the cheaper and faster the execution gets.</p>
<a name="user-content-how-does-this-compare-to-other-frameworks-like-langchain"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-does-this-compare-to-other-frameworks-like-langchain" aria-hidden="true" tabindex="-1" href="#how-does-this-compare-to-other-frameworks-like-langchain"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How does this compare to other frameworks like LangChain?</h4>
<ul dir="auto">
<li><strong>Langchain</strong>: MonkeyPatch has a narrower scope than Langchain. Our mission is to ensure predictable and consistent LLM execution, with automatic reductions in cost and latency through finetuning.</li>
<li><strong>Magentic</strong> / <strong>Marvin</strong>: MonkeyPatch offers two main benefits compared to Magentic/Marvin, namely; lower cost and latency through automatic distillation, and more predictable behaviour through test-driven alignment. Currently, there are two cases where you should use Magentic, namely: where you need support for tools (functions) - a feature that is on our roadmap, and where you need support for asynchronous functions.</li>
</ul>
<a name="user-content-what-are-some-sample-use-cases"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-what-are-some-sample-use-cases" aria-hidden="true" tabindex="-1" href="#what-are-some-sample-use-cases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>What are some sample use-cases?</h4>
<p dir="auto">We&#39;ve created a few examples to show how to use MonkeyPatch for different problems. You can find them <a href="https://github.com/monkey-patch-sdk/monkey-patch.py/tree/master/examples">here</a>.
A few ideas are as follows:</p>
<ul dir="auto">
<li>Adding an importance classifier to customer requests</li>
<li>Creating a offensive-language classification feature</li>
<li>Creating a food-review app</li>
<li>Generating data that conforms to your DB schema that can immediately</li>
</ul>
<a name="user-content-why-would-i-need-typed-responses"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-why-would-i-need-typed-responses" aria-hidden="true" tabindex="-1" href="#why-would-i-need-typed-responses"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why would I need typed responses?</h4>
<p dir="auto">When invoking LLMs, the outputs are free-form. This means that they are less predictable when used in software products. Using types ensures that the outputs adhere to specific constraints or rules which the rest of your program can work with.</p>
<a name="user-content-do-you-offer-this-for-other-languages-eg-typescript"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-do-you-offer-this-for-other-languages-eg-typescript" aria-hidden="true" tabindex="-1" href="#do-you-offer-this-for-other-languages-eg-typescript"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Do you offer this for other languages (eg Typescript)?</h4>
<p dir="auto">Not right now but reach out on <a href="https://discord.gg/kEGS5sQU" rel="nofollow">our Discord server</a> or make a Github issue if there’s another language you would like to see supported.</p>
<a name="user-content-getting-started-1"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-getting-started-1" aria-hidden="true" tabindex="-1" href="#getting-started-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h3>
<a name="user-content-how-do-i-get-started"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-do-i-get-started" aria-hidden="true" tabindex="-1" href="#how-do-i-get-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How do I get started?</h4>
<p dir="auto">Follow the instructions in the <a href="https://github.com/monkeypatch/monkeypatch.py/blob/master">Installation and getting started</a> and <a href="https://github.com/monkeypatch/monkeypatch.py/blob/master">How it works</a> sections</p>
<a name="user-content-how-do-i-align-my-functions"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-do-i-align-my-functions" aria-hidden="true" tabindex="-1" href="#how-do-i-align-my-functions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How do I align my functions?</h4>
<p dir="auto">See <a href="https://github.com/monkeypatch/monkeypatch.py/blob/master">How it works</a> and <a href="https://github.com/monkeypatch/monkeypatch.py/blob/master">Test-Driven Alignment</a> sections or the examples shown <a href="https://github.com/monkey-patch-sdk/monkey-patch.py/tree/master/examples">here</a>.</p>
<a name="user-content-do-i-need-my-own-openai-key"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-do-i-need-my-own-openai-key" aria-hidden="true" tabindex="-1" href="#do-i-need-my-own-openai-key"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Do I need my own OpenAI key?</h4>
<p dir="auto">Yes</p>
<a name="user-content-does-it-only-work-with-openai"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-does-it-only-work-with-openai" aria-hidden="true" tabindex="-1" href="#does-it-only-work-with-openai"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Does it only work with OpenAI?</h4>
<p dir="auto">Currently yes but there are plans to support Anthropic and popular open-source models. If you have a specific request, either join <a href="https://discord.gg/kEGS5sQU" rel="nofollow">our Discord server</a>, or create a Github issue.</p>
<a name="user-content-how-it-works-1"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-how-it-works-1" aria-hidden="true" tabindex="-1" href="#how-it-works-1"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How It Works</h3>
<a name="user-content-how-does-the-llm-get-cheaper-and-faster-over-time-and-by-how-much"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-does-the-llm-get-cheaper-and-faster-over-time-and-by-how-much" aria-hidden="true" tabindex="-1" href="#how-does-the-llm-get-cheaper-and-faster-over-time-and-by-how-much"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How does the LLM get cheaper and faster over time? And by how much?</h4>
<p dir="auto">In short, we use distillation of LLM models.</p>
<p dir="auto">Expanded, using the outputs of the larger (teacher) model, a smaller (student) model will be trained to emulate the teacher model behaviour while being faster and cheaper to run due to smaller size. In some cases it is possible to achieve up to 90% lower cost and 80% lower latency with a small number of executions of your patched functions.</p>
<a name="user-content-how-many-calls-does-it-require-to-get-the-improvement"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-many-calls-does-it-require-to-get-the-improvement" aria-hidden="true" tabindex="-1" href="#how-many-calls-does-it-require-to-get-the-improvement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How many calls does it require to get the improvement?</h4>
<p dir="auto">The default minimum is 200 calls, although this can be changed by adding flags to the patch decorator.</p>
<a name="user-content-can-i-link-functions-together"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-can-i-link-functions-together" aria-hidden="true" tabindex="-1" href="#can-i-link-functions-together"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Can I link functions together?</h4>
<p dir="auto">Yes! It is possible to use the output of one patched function as the input to another patched function. Simply carry this out as you would do with normal python functions.</p>
<a name="user-content-does-fine-tuning-reduce-the-performance-of-the-llm"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-does-fine-tuning-reduce-the-performance-of-the-llm" aria-hidden="true" tabindex="-1" href="#does-fine-tuning-reduce-the-performance-of-the-llm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Does fine-tuning reduce the performance of the LLM?</h4>
<p dir="auto">Not necessarily. Currently the only way to improve the LLM performance is to have better align statements. As the student model is trained on both align statements and input-output calls, it is possible for the fine tuned student model to exceed the performance of the N-shot teacher model during inference.</p>
<a name="user-content-accuracy-reliability"></a>
<h3 tabindex="-1" dir="auto"><a id="user-content-accuracy--reliability" aria-hidden="true" tabindex="-1" href="#accuracy--reliability"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Accuracy &amp; Reliability</h3>
<a name="user-content-how-do-you-guarantee-consistency-in-the-output-of-patched-functions"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-do-you-guarantee-consistency-in-the-output-of-patched-functions" aria-hidden="true" tabindex="-1" href="#how-do-you-guarantee-consistency-in-the-output-of-patched-functions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How do you guarantee consistency in the output of patched functions?</h4>
<p dir="auto">Each output of the LLM will be programmatically instantiated into the output class ensuring the output will be of the correct type, just like your Python functions. If the output is incorrect and instantiating the correct output object fails, an automatic feedback repair loop kicks in to correct the mistake.</p>
<a name="user-content-how-reliable-are-the-typed-outputs"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-reliable-are-the-typed-outputs" aria-hidden="true" tabindex="-1" href="#how-reliable-are-the-typed-outputs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How reliable are the typed outputs?</h4>
<p dir="auto">For simpler-medium complexity classes GPT4 with align statements has been shown to be very reliable in outputting the correct type. Additionally we have implemented a repair loop with error feedback to “fix” incorrect outputs and add the correct output to the training dataset.</p>
<a name="user-content-how-do-you-deal-with-hallucinations"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-do-you-deal-with-hallucinations" aria-hidden="true" tabindex="-1" href="#how-do-you-deal-with-hallucinations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How do you deal with hallucinations?</h4>
<p dir="auto">Hallucinations can’t be 100% removed from LLMs at the moment, if ever. However, by creating test functions decorated with <code>@monkey.align</code>, you can use normal <code>assert</code> statements to align the model to behave in the way that you expect. Additionally, you can create types with Pydantic, which act as guardrails to prevent any nasty surprises and provide correct error handling.</p>
<a name="user-content-how-do-you-deal-with-bias"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-how-do-you-deal-with-bias" aria-hidden="true" tabindex="-1" href="#how-do-you-deal-with-bias"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How do you deal with bias?</h4>
<p dir="auto">By adding more align statements that cover a wider range of inputs, you can ensure that the model is less biased.</p>
<a name="user-content-will-distillation-impact-performance"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-will-distillation-impact-performance" aria-hidden="true" tabindex="-1" href="#will-distillation-impact-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Will distillation impact performance?</h4>
<p dir="auto">It depends. For tasks that are challenging for even the best models (e.g GPT4), distillation will reduce performance.
However, distillation can be manually turned off in these cases. Additionally, if the distilled model frequently fails to generate correct outputs, the distilled model will be automatically turned off.</p>
<a name="user-content-what-is-this-not-suitable-for"></a>
<h4 tabindex="-1" dir="auto"><a id="user-content-what-is-this-not-suitable-for" aria-hidden="true" tabindex="-1" href="#what-is-this-not-suitable-for"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>What is this not suitable for?</h4>
<ul dir="auto">
<li>Time-series data</li>
<li>Tasks that requires a lot of context to completed correctly</li>
<li>For tasks that directly output complex natural language, you will get less value from MonkeyPatch and may want to consider the OpenAI API directly.</li>
</ul>
<hr/>
<a name="user-content-simple-todo-list-app"></a>
<h2 tabindex="-1" dir="auto"><a id="user-content-simple-todo-list-app" aria-hidden="true" tabindex="-1" href="#simple-todo-list-app"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><a href="https://github.com/monkey-patch-sdk/monkey-patch.py/tree/master/examples/todolist">Simple ToDo List App</a></h2>
</article>
          </div></div>
  </body>
</html>
