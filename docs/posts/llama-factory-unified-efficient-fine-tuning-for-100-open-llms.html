<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/hiyouga/LLaMA-Factory">Original</a>
    <h1>Llama-Factory: Unified, Efficient Fine-Tuning for 100 Open LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/logo.png"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/logo.png" alt="# LLaMA Factory"/></a></p>
<p dir="auto"><a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"><img src="https://camo.githubusercontent.com/a866b1b1a70946801d92c53cbed8f7a3ab3faff7db341e697eb4a51d952381a0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6869796f7567612f4c4c614d412d466163746f72793f7374796c653d736f6369616c" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social"/></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"><img src="https://camo.githubusercontent.com/ff3d65656a8ef3f86e5ac227da90da84104735d095711bdf886d3ac97f933ccd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6173742d636f6d6d69742f6869796f7567612f4c4c614d412d466163746f7279" alt="GitHub last commit" data-canonical-src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory"/></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"><img src="https://camo.githubusercontent.com/2c9600a703038a041088635b552a0e70e5ff0438cd5fcad1aa0b2a38470d8ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6869796f7567612f4c4c614d412d466163746f72793f636f6c6f723d6f72616e6765" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange"/></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"><img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg" alt="GitHub workflow"/></a>
<a href="https://pypi.org/project/llamafactory/" rel="nofollow"><img src="https://camo.githubusercontent.com/816aba4a24ac2e71b3f5a5cbd3c0ecfd9403ce1ece26ee0c631cf4613b1c4697/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6c616d61666163746f7279" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/llamafactory"/></a>
<a href="https://scholar.google.com/scholar?cites=12620864006390196564" rel="nofollow"><img src="https://camo.githubusercontent.com/a5cc965e2b198801cb90ccf04bc9a224c5607561b28911fd8515d2b0560699f3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6369746174696f6e2d3834302d677265656e" alt="Citation" data-canonical-src="https://img.shields.io/badge/citation-840-green"/></a>
<a href="https://hub.docker.com/r/hiyouga/llamafactory/tags" rel="nofollow"><img src="https://camo.githubusercontent.com/69abb5bb195ec014b38ca025e9b603971b727f20af6ac5a3927e30e90fb7e141/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6869796f7567612f6c6c616d61666163746f7279" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/hiyouga/llamafactory"/></a></p>
<p dir="auto"><a href="https://twitter.com/llamafactory_ai" rel="nofollow"><img src="https://camo.githubusercontent.com/b50fda1cfeea7651440816260e5f5c190e06d54a141f4bd34a94bfdcc866ffdd/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c6c616d61666163746f72795f6169" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/llamafactory_ai"/></a>
<a href="https://discord.gg/rKfvV9r9FK" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/discord.svg" alt="Discord"/></a></p>
<p dir="auto"><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/colab.svg" alt="Open in Colab"/></a>
<a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/dsw.svg" alt="Open in DSW"/></a>
<a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/lab4ai.svg" alt="Open in Lab4ai"/></a>
<a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory" rel="nofollow"><img src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/thirdparty/online.svg" alt="Open in Online"/></a>
<a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board" rel="nofollow"><img src="https://camo.githubusercontent.com/f302d00c36a87c4bac8741c397b6a55f1017c961cec10b0990879eda5e15a3f4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4f70656e253230696e2532305370616365732d626c7565" alt="Open in Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"/></a>
<a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board" rel="nofollow"><img src="https://camo.githubusercontent.com/a93364cb869924b559dbdeedb28d66ba5fe6b70b4467aca043dd03a26a953fbe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c53636f70652d4f70656e253230696e25323053747564696f732d626c7565" alt="Open in Studios" data-canonical-src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue"/></a>
<a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47" rel="nofollow"><img src="https://camo.githubusercontent.com/5e329b98c5848f2215e3e24ec4f581a22c28b129faaccbebb24c4ee8ed15ec92/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6f766974612d4465706c6f7925323054656d706c6174652d626c7565" alt="Open in Novita" data-canonical-src="https://img.shields.io/badge/Novita-Deploy%20Template-blue"/></a></p>

<div dir="auto">

<markdown-accessiblity-table><table>
<thead>
<tr>
<th><div dir="auto"><p><a href="https://warp.dev/llama-factory" rel="nofollow"><img alt="Warp sponsorship" width="400" src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/sponsors/warp.jpg"/></a></p></div></th>
<th><a href="https://serpapi.com" rel="nofollow"><img alt="SerpAPI sponsorship" width="250" src="https://github.com/hiyouga/LLaMA-Factory/raw/main/assets/sponsors/serpapi.svg"/> </a></th>
</tr>
</thead>
</table></markdown-accessiblity-table>
<hr/>
<div dir="auto"><h3 tabindex="-1" dir="auto">Easily fine-tune 100+ large language models with zero-code <a href="#quickstart">CLI</a> and <a href="#fine-tuning-with-llama-board-gui-powered-by-gradio">Web UI</a></h3><a id="user-content-easily-fine-tune-100-large-language-models-with-zero-code-cli-and-web-ui" aria-label="Permalink: Easily fine-tune 100+ large language models with zero-code CLI and Web UI" href="#easily-fine-tune-100-large-language-models-with-zero-code-cli-and-web-ui"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/11737cc9e31d781879299fd57dd917a557a4d2268085b1ef0fc80471df4b599b/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f34353335"><img src="https://camo.githubusercontent.com/11737cc9e31d781879299fd57dd917a557a4d2268085b1ef0fc80471df4b599b/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f34353335" alt="GitHub Trend" data-canonical-src="https://trendshift.io/api/badge/repositories/4535"/></a></p>
</div>
<p dir="auto">ðŸ‘‹ Join our <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/main.jpg">WeChat</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/npu.jpg">NPU</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/lab4ai.jpg">Lab4AI</a>, <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/assets/wechat/online.jpg">LLaMA Factory Online</a> user group.</p>
<p dir="auto">[ English | <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md">ä¸­æ–‡</a> ]</p>
<p dir="auto"><strong>Fine-tuning a large language model can be easy as...</strong></p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span>train_en.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/16256802/423362179-3991a3a8-4276-4d30-9cab-4cb0c4b9b99e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgyNjY1NjEsIm5iZiI6MTc1ODI2NjI2MSwicGF0aCI6Ii8xNjI1NjgwMi80MjMzNjIxNzktMzk5MWEzYTgtNDI3Ni00ZDMwLTljYWItNGNiMGM0YjliOTllLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE5VDA3MTc0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2OWZjZGYwNDJhOTM2ZTQ3Mzg0ZjhlMzY4NDRmMzhlOTljYThkNDk5ZDkzNjZmMTRhMThmN2Y4NmYyNDIxZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BNL62WVzJ7FIQuv3hGxpd3u-jOJ_gxTaGFtnt-k76JY" data-canonical-src="https://private-user-images.githubusercontent.com/16256802/423362179-3991a3a8-4276-4d30-9cab-4cb0c4b9b99e.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTgyNjY1NjEsIm5iZiI6MTc1ODI2NjI2MSwicGF0aCI6Ii8xNjI1NjgwMi80MjMzNjIxNzktMzk5MWEzYTgtNDI3Ni00ZDMwLTljYWItNGNiMGM0YjliOTllLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTE5VDA3MTc0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2OWZjZGYwNDJhOTM2ZTQ3Mzg0ZjhlMzY4NDRmMzhlOTljYThkNDk5ZDkzNjZmMTRhMThmN2Y4NmYyNDIxZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.BNL62WVzJ7FIQuv3hGxpd3u-jOJ_gxTaGFtnt-k76JY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Choose your path:</p>
<ul dir="auto">
<li><strong>Documentation (WIP)</strong>: <a href="https://llamafactory.readthedocs.io/en/latest/" rel="nofollow">https://llamafactory.readthedocs.io/en/latest/</a></li>
<li><strong>Documentation (AMD GPU)</strong>: <a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html" rel="nofollow">https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html</a></li>
<li><strong>Colab (free)</strong>: <a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow">https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing</a></li>
<li><strong>Local machine</strong>: Please refer to <a href="#getting-started">usage</a></li>
<li><strong>PAI-DSW (free trial)</strong>: <a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow">https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory</a></li>
<li><strong>Alaya NeW (cloud GPU deal)</strong>: <a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory" rel="nofollow">https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory</a></li>
<li><strong>Official Course</strong>: <a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory" rel="nofollow">https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory</a></li>
<li><strong>LLaMA Factory Online</strong>: <a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory" rel="nofollow">https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory</a></li>
</ul>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.</p>
</div>

<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#blogs">Blogs</a></li>
<li><a href="#changelog">Changelog</a></li>
<li><a href="#supported-models">Supported Models</a></li>
<li><a href="#supported-training-approaches">Supported Training Approaches</a></li>
<li><a href="#provided-datasets">Provided Datasets</a></li>
<li><a href="#requirement">Requirement</a></li>
<li><a href="#getting-started">Getting Started</a>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#data-preparation">Data Preparation</a></li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#fine-tuning-with-llama-board-gui-powered-by-gradio">Fine-Tuning with LLaMA Board GUI</a></li>
<li><a href="#llama-factory-online">LLaMA Factory Online</a></li>
<li><a href="#build-docker">Build Docker</a></li>
<li><a href="#deploy-with-openai-style-api-and-vllm">Deploy with OpenAI-style API and vLLM</a></li>
<li><a href="#download-from-modelscope-hub">Download from ModelScope Hub</a></li>
<li><a href="#download-from-modelers-hub">Download from Modelers Hub</a></li>
<li><a href="#use-wb-logger">Use W&amp;B Logger</a></li>
<li><a href="#use-swanlab-logger">Use SwanLab Logger</a></li>
</ul>
</li>
<li><a href="#projects-using-llama-factory">Projects using LLaMA Factory</a></li>
<li><a href="#license">License</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#acknowledgement">Acknowledgement</a></li>
</ul>

<ul dir="auto">
<li><strong>Various models</strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.</li>
<li><strong>Integrated methods</strong>: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.</li>
<li><strong>Scalable resources</strong>: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.</li>
<li><strong>Advanced algorithms</strong>: <a href="https://github.com/jiaweizzhao/GaLore">GaLore</a>, <a href="https://github.com/Ledzy/BAdam">BAdam</a>, <a href="https://github.com/zhuhanqing/APOLLO">APOLLO</a>, <a href="https://github.com/zyushun/Adam-mini">Adam-mini</a>, <a href="https://github.com/KellerJordan/Muon">Muon</a>, <a href="https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft">OFT</a>, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.</li>
<li><strong>Practical tricks</strong>: <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a>, <a href="https://github.com/unslothai/unsloth">Unsloth</a>, <a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a>, RoPE scaling, NEFTune and rsLoRA.</li>
<li><strong>Wide tasks</strong>: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.</li>
<li><strong>Experiment monitors</strong>: LlamaBoard, TensorBoard, Wandb, MLflow, <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a>, etc.</li>
<li><strong>Faster inference</strong>: OpenAI-style API, Gradio UI and CLI with <a href="https://github.com/vllm-project/vllm">vLLM worker</a> or <a href="https://github.com/sgl-project/sglang">SGLang worker</a>.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Day-N Support for Fine-Tuning Cutting-Edge Models</h3><a id="user-content-day-n-support-for-fine-tuning-cutting-edge-models" aria-label="Permalink: Day-N Support for Fine-Tuning Cutting-Edge Models" href="#day-n-support-for-fine-tuning-cutting-edge-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Support Date</th>
<th>Model Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Day 0</td>
<td>Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6</td>
</tr>
<tr>
<td>Day 1</td>
<td>Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li>ðŸ’¡ <a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g" rel="nofollow">Easy Dataset Ã— LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge</a> (English)</li>
<li><a href="https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;type=project&amp;utm_source=LLaMA-Factory" rel="nofollow">Fine-tune a mental health LLM using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/" rel="nofollow">A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/" rel="nofollow">How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod</a> (English)</li>
</ul>
<details><summary>All Blogs</summary>
<ul dir="auto">
<li><a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory" rel="nofollow">Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b" rel="nofollow">LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/" rel="nofollow">A One-Stop Code-Free Model Fine-Tuning &amp; Deployment Platform based on SageMaker and LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl" rel="nofollow">LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory" rel="nofollow">LLaMA Factory: Fine-tuning Llama3 for Role-Playing</a> (Chinese)</li>
</ul>
</details>

<p dir="auto">[25/08/22] We supported <strong><a href="https://arxiv.org/abs/2306.07280" rel="nofollow">OFT</a></strong> and <strong><a href="https://arxiv.org/abs/2506.19847" rel="nofollow">OFTv2</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/08/20] We supported fine-tuning the <strong><a href="https://huggingface.co/internlm/Intern-S1-mini" rel="nofollow">Intern-S1-mini</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8976" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/8976/hovercard">PR #8976</a> to get started.</p>
<p dir="auto">[25/08/06] We supported fine-tuning the <strong><a href="https://github.com/openai/gpt-oss">GPT-OSS</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/8826/hovercard">PR #8826</a> to get started.</p>
<details><summary>Full Changelog</summary>
<p dir="auto">[25/07/02] We supported fine-tuning the <strong><a href="https://github.com/THUDM/GLM-4.1V-Thinking">GLM-4.1V-9B-Thinking</a></strong> model.</p>
<p dir="auto">[25/04/28] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen3/" rel="nofollow">Qwen3</a></strong> model family.</p>
<p dir="auto">[25/04/21] We supported the <strong><a href="https://github.com/KellerJordan/Muon">Muon</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage. Thank <a href="https://github.com/tianshijing">@tianshijing</a>&#39;s PR.</p>
<p dir="auto">[25/04/16] We supported fine-tuning the <strong><a href="https://huggingface.co/OpenGVLab/InternVL3-8B" rel="nofollow">InternVL3</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7258/hovercard">PR #7258</a> to get started.</p>
<p dir="auto">[25/04/14] We supported fine-tuning the <strong><a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414" rel="nofollow">GLM-Z1</a></strong> and <strong><a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct" rel="nofollow">Kimi-VL</a></strong> models.</p>
<p dir="auto">[25/04/06] We supported fine-tuning the <strong><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" rel="nofollow">Llama 4</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7611/hovercard">PR #7611</a> to get started.</p>
<p dir="auto">[25/03/31] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5-omni/" rel="nofollow">Qwen2.5 Omni</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537" data-hovercard-type="pull_request" data-hovercard-url="/hiyouga/LLaMA-Factory/pull/7537/hovercard">PR #7537</a> to get started.</p>
<p dir="auto">[25/03/15] We supported <strong><a href="https://github.com/sgl-project/sglang">SGLang</a></strong> as inference backend. Try <code>infer_backend: sglang</code> to accelerate inference.</p>
<p dir="auto">[25/03/12] We supported fine-tuning the <strong><a href="https://huggingface.co/blog/gemma3" rel="nofollow">Gemma 3</a></strong> model.</p>
<p dir="auto">[25/02/24] Announcing <strong><a href="https://github.com/hiyouga/EasyR1">EasyR1</a></strong>, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.</p>
<p dir="auto">[25/02/11] We supported saving the <strong><a href="https://github.com/ollama/ollama">Ollama</a></strong> modelfile when exporting the model checkpoints. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/02/05] We supported fine-tuning the <strong><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/Qwen/Qwen2-Audio-7B-Instruct">Qwen2-Audio</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6" rel="nofollow">MiniCPM-o-2.6</a></strong> on audio understanding tasks.</p>
<p dir="auto">[25/01/31] We supported fine-tuning the <strong><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" rel="nofollow">DeepSeek-R1</a></strong> and <strong><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" rel="nofollow">Qwen2.5-VL</a></strong> models.</p>
<p dir="auto">[25/01/15] We supported <strong><a href="https://arxiv.org/abs/2412.05270" rel="nofollow">APOLLO</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6" rel="nofollow">MiniCPM-o-2.6</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-V-2_6" rel="nofollow">MiniCPM-V-2.6</a></strong> models. Thank <a href="https://github.com/BUAADreamer">@BUAADreamer</a>&#39;s PR.</p>
<p dir="auto">[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/collections/internlm/" rel="nofollow">InternLM 3</a></strong> models. Thank <a href="https://github.com/hhaAndroid">@hhaAndroid</a>&#39;s PR.</p>
<p dir="auto">[25/01/10] We supported fine-tuning the <strong><a href="https://huggingface.co/microsoft/phi-4" rel="nofollow">Phi-4</a></strong> model.</p>
<p dir="auto">[24/12/21] We supported using <strong><a href="https://github.com/SwanHubX/SwanLab">SwanLab</a></strong> for experiment tracking and visualization. See <a href="#use-swanlab-logger">this section</a> for details.</p>
<p dir="auto">[24/11/27] We supported fine-tuning the <strong><a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B" rel="nofollow">Skywork-o1</a></strong> model and the <strong><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT" rel="nofollow">OpenO1</a></strong> dataset.</p>
<p dir="auto">[24/10/09] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelers.cn/models" rel="nofollow">Modelers Hub</a></strong>. See <a href="#download-from-modelers-hub">this tutorial</a> for usage.</p>
<p dir="auto">[24/09/19] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5/" rel="nofollow">Qwen2.5</a></strong> models.</p>
<p dir="auto">[24/08/30] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2-vl/" rel="nofollow">Qwen2-VL</a></strong> models. Thank <a href="https://github.com/simonJJJ">@simonJJJ</a>&#39;s PR.</p>
<p dir="auto">[24/08/27] We supported <strong><a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a></strong>. Try <code>enable_liger_kernel: true</code> for efficient training.</p>
<p dir="auto">[24/08/09] We supported <strong><a href="https://github.com/zyushun/Adam-mini">Adam-mini</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage. Thank <a href="https://github.com/relic-yuexi">@relic-yuexi</a>&#39;s PR.</p>
<p dir="auto">[24/07/04] We supported <a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing">contamination-free packed training</a>. Use <code>neat_packing: true</code> to activate it. Thank <a href="https://github.com/chuan298">@chuan298</a>&#39;s PR.</p>
<p dir="auto">[24/06/16] We supported <strong><a href="https://arxiv.org/abs/2404.02948" rel="nofollow">PiSSA</a></strong> algorithm. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/06/07] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2/" rel="nofollow">Qwen2</a></strong> and <strong><a href="https://github.com/THUDM/GLM-4">GLM-4</a></strong> models.</p>
<p dir="auto">[24/05/26] We supported <strong><a href="https://arxiv.org/abs/2405.14734" rel="nofollow">SimPO</a></strong> algorithm for preference learning. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/05/20] We supported fine-tuning the <strong>PaliGemma</strong> series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with <code>paligemma</code> template for chat completion.</p>
<p dir="auto">[24/05/18] We supported <strong><a href="https://arxiv.org/abs/2402.01306" rel="nofollow">KTO</a></strong> algorithm for preference learning. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/05/14] We supported training and inference on the Ascend NPU devices. Check <a href="#installation">installation</a> section for details.</p>
<p dir="auto">[24/04/26] We supported fine-tuning the <strong>LLaVA-1.5</strong> multimodal LLMs. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/22] We provided a <strong><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing" rel="nofollow">Colab notebook</a></strong> for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check <a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat" rel="nofollow">Llama3-8B-Chinese-Chat</a> and <a href="https://huggingface.co/zhichen/Llama3-Chinese" rel="nofollow">Llama3-Chinese</a> for details.</p>
<p dir="auto">[24/04/21] We supported <strong><a href="https://arxiv.org/abs/2404.02258" rel="nofollow">Mixture-of-Depths</a></strong> according to <a href="https://github.com/astramind-ai/Mixture-of-depths">AstraMindAI&#39;s implementation</a>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/16] We supported <strong><a href="https://arxiv.org/abs/2404.02827" rel="nofollow">BAdam</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/04/16] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves <strong>117%</strong> speed and <strong>50%</strong> memory compared with FlashAttention-2, more benchmarks can be found in <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a>.</p>
<p dir="auto">[24/03/31] We supported <strong><a href="https://arxiv.org/abs/2403.07691" rel="nofollow">ORPO</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/21] Our paper &#34;<a href="https://arxiv.org/abs/2403.13372" rel="nofollow">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>&#34; is available at arXiv!</p>
<p dir="auto">[24/03/20] We supported <strong>FSDP+QLoRA</strong> that fine-tunes a 70B model on 2x24GB GPUs. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/13] We supported <strong><a href="https://arxiv.org/abs/2402.12354" rel="nofollow">LoRA+</a></strong>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/07] We supported <strong><a href="https://arxiv.org/abs/2403.03507" rel="nofollow">GaLore</a></strong> optimizer. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/03/07] We integrated <strong><a href="https://github.com/vllm-project/vllm">vLLM</a></strong> for faster and concurrent inference. Try <code>infer_backend: vllm</code> to enjoy <strong>270%</strong> inference speed.</p>
<p dir="auto">[24/02/28] We supported weight-decomposed LoRA (<strong><a href="https://arxiv.org/abs/2402.09353" rel="nofollow">DoRA</a></strong>). Try <code>use_dora: true</code> to activate DoRA training.</p>
<p dir="auto">[24/02/15] We supported <strong>block expansion</strong> proposed by <a href="https://github.com/TencentARC/LLaMA-Pro">LLaMA Pro</a>. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this <a href="https://qwenlm.github.io/blog/qwen1.5/" rel="nofollow">blog post</a> for details.</p>
<p dir="auto">[24/01/18] We supported <strong>agent tuning</strong> for most models, equipping model with tool using abilities by fine-tuning with <code>dataset: glaive_toolcall_en</code>.</p>
<p dir="auto">[23/12/23] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try <code>use_unsloth: true</code> argument to activate unsloth patch. It achieves <strong>170%</strong> speed in our benchmark, check <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a> for details.</p>
<p dir="auto">[23/12/12] We supported fine-tuning the latest MoE model <strong><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" rel="nofollow">Mixtral 8x7B</a></strong> in our framework. See hardware requirement <a href="#hardware-requirement">here</a>.</p>
<p dir="auto">[23/12/01] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelscope.cn/models" rel="nofollow">ModelScope Hub</a></strong>. See <a href="#download-from-modelscope-hub">this tutorial</a> for usage.</p>
<p dir="auto">[23/10/21] We supported <strong><a href="https://arxiv.org/abs/2310.05914" rel="nofollow">NEFTune</a></strong> trick for fine-tuning. Try <code>neftune_noise_alpha: 5</code> argument to activate NEFTune.</p>
<p dir="auto">[23/09/27] We supported <strong><math-renderer data-run-id="6ea096da10f44ae87bfa452bd2f635ee">$S^2$</math-renderer>-Attn</strong> proposed by <a href="https://github.com/dvlab-research/LongLoRA">LongLoRA</a> for the LLaMA models. Try <code>shift_attn: true</code> argument to enable shift short attention.</p>
<p dir="auto">[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[23/09/10] We supported <strong><a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a></strong>. Try <code>flash_attn: fa2</code> argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.</p>
<p dir="auto">[23/08/12] We supported <strong>RoPE scaling</strong> to extend the context length of the LLaMA models. Try <code>rope_scaling: linear</code> argument in training and <code>rope_scaling: dynamic</code> argument at inference to extrapolate the position embeddings.</p>
<p dir="auto">[23/08/11] We supported <strong><a href="https://arxiv.org/abs/2305.18290" rel="nofollow">DPO training</a></strong> for instruction-tuned models. See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
<p dir="auto">[23/07/31] We supported <strong>dataset streaming</strong>. Try <code>streaming: true</code> and <code>max_steps: 10000</code> arguments to load your dataset in streaming mode.</p>
<p dir="auto">[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (<a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat" rel="nofollow">LLaMA-2</a> / <a href="https://huggingface.co/hiyouga/Baichuan-13B-sft" rel="nofollow">Baichuan</a>) for details.</p>
<p dir="auto">[23/07/18] We developed an <strong>all-in-one Web UI</strong> for training, evaluation and inference. Try <code>train_web.py</code> to fine-tune models in your Web browser. Thank <a href="https://github.com/KanadeSiina">@KanadeSiina</a> and <a href="https://github.com/codemayq">@codemayq</a> for their efforts in the development.</p>
<p dir="auto">[23/07/09] We released <strong><a href="https://github.com/hiyouga/FastEdit">FastEdit</a></strong> âš¡ðŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow <a href="https://github.com/hiyouga/FastEdit">FastEdit</a> if you are interested.</p>
<p dir="auto">[23/06/29] We provided a <strong>reproducible example</strong> of training a chat model using instruction-following datasets, see <a href="https://huggingface.co/hiyouga/Baichuan-7B-sft" rel="nofollow">Baichuan-7B-sft</a> for details.</p>
<p dir="auto">[23/06/22] We aligned the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/api_demo.py">demo API</a> with the <a href="https://platform.openai.com/docs/api-reference/chat" rel="nofollow">OpenAI&#39;s</a> format where you can insert the fine-tuned model in <strong>arbitrary ChatGPT-based applications</strong>.</p>
<p dir="auto">[23/06/03] We supported quantized training and inference (aka <strong><a href="https://github.com/artidoro/qlora">QLoRA</a></strong>). See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples</a> for usage.</p>
</details>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p dir="auto">If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.</p>
</div>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Model size</th>
<th>Template</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/baichuan-inc" rel="nofollow">Baichuan 2</a></td>
<td>7B/13B</td>
<td>baichuan2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigscience" rel="nofollow">BLOOM/BLOOMZ</a></td>
<td>560M/1.1B/1.7B/3B/7.1B/176B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/THUDM" rel="nofollow">ChatGLM3</a></td>
<td>6B</td>
<td>chatglm3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/CohereForAI" rel="nofollow">Command R</a></td>
<td>35B/104B</td>
<td>cohere</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek (Code/MoE)</a></td>
<td>7B/16B/67B/236B</td>
<td>deepseek</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek 2.5/3</a></td>
<td>236B/671B</td>
<td>deepseek3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai" rel="nofollow">DeepSeek R1 (Distill)</a></td>
<td>1.5B/7B/8B/14B/32B/70B/671B</td>
<td>deepseekr1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon</a></td>
<td>7B/11B/40B/180B</td>
<td>falcon</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae" rel="nofollow">Falcon-H1</a></td>
<td>0.5B/1.5B/3B/7B/34B</td>
<td>falcon_h1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">Gemma/Gemma 2/CodeGemma</a></td>
<td>2B/7B/9B/27B</td>
<td>gemma/gemma2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">Gemma 3/Gemma 3n</a></td>
<td>270M/1B/4B/6B/8B/12B/27B</td>
<td>gemma3/gemma3n</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4/GLM-4-0414/GLM-Z1</a></td>
<td>9B/32B</td>
<td>glm4/glmz1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4.1V</a></td>
<td>9B</td>
<td>glm4v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org" rel="nofollow">GLM-4.5/GLM-4.5V</a></td>
<td>106B/355B</td>
<td>glm4_moe/glm4v_moe</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai-community" rel="nofollow">GPT-2</a></td>
<td>0.1B/0.4B/0.8B/1.5B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai" rel="nofollow">GPT-OSS</a></td>
<td>20B/120B</td>
<td>gpt</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite" rel="nofollow">Granite 3.0-3.3</a></td>
<td>1B/2B/3B/8B</td>
<td>granite3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite" rel="nofollow">Granite 4</a></td>
<td>7B</td>
<td>granite4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tencent/" rel="nofollow">Hunyuan</a></td>
<td>7B</td>
<td>hunyuan</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IndexTeam" rel="nofollow">Index</a></td>
<td>1.9B</td>
<td>index</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm" rel="nofollow">InternLM 2-3</a></td>
<td>7B/8B/20B</td>
<td>intern2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/OpenGVLab" rel="nofollow">InternVL 2.5-3.5</a></td>
<td>1B/2B/4B/8B/14B/30B/38B/78B/241B</td>
<td>intern_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm/" rel="nofollow">InternLM/Intern-S1-mini</a></td>
<td>8B</td>
<td>intern_s1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/moonshotai" rel="nofollow">Kimi-VL</a></td>
<td>16B</td>
<td>kimi_vl</td>
</tr>
<tr>
<td><a href="https://github.com/facebookresearch/llama">Llama</a></td>
<td>7B/13B/33B/65B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 2</a></td>
<td>7B/13B/70B</td>
<td>llama2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 3-3.3</a></td>
<td>1B/3B/8B/70B</td>
<td>llama3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 4</a></td>
<td>109B/402B</td>
<td>llama4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama" rel="nofollow">Llama 3.2 Vision</a></td>
<td>11B/90B</td>
<td>mllama</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-1.5</a></td>
<td>7B/13B</td>
<td>llava</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-NeXT</a></td>
<td>7B/8B/13B/34B/72B/110B</td>
<td>llava_next</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf" rel="nofollow">LLaVA-NeXT-Video</a></td>
<td>7B/34B</td>
<td>llava_next_video</td>
</tr>
<tr>
<td><a href="https://huggingface.co/XiaomiMiMo" rel="nofollow">MiMo</a></td>
<td>7B</td>
<td>mimo</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb" rel="nofollow">MiniCPM 1-4.1</a></td>
<td>0.5B/1B/2B/4B/8B</td>
<td>cpm/cpm3/cpm4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb" rel="nofollow">MiniCPM-o-2.6/MiniCPM-V-2.6</a></td>
<td>8B</td>
<td>minicpm_o/minicpm_v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Ministral/Mistral-Nemo</a></td>
<td>8B/12B</td>
<td>ministral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral/Mixtral</a></td>
<td>7B/8x7B/8x22B</td>
<td>mistral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Mistral Small</a></td>
<td>24B</td>
<td>mistral_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/allenai" rel="nofollow">OLMo</a></td>
<td>1B/7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google" rel="nofollow">PaliGemma/PaliGemma2</a></td>
<td>3B/10B/28B</td>
<td>paligemma</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-1.5/Phi-2</a></td>
<td>1.3B/2.7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-3/Phi-3.5</a></td>
<td>4B/14B</td>
<td>phi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-3-small</a></td>
<td>7B</td>
<td>phi_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft" rel="nofollow">Phi-4</a></td>
<td>14B</td>
<td>phi4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai" rel="nofollow">Pixtral</a></td>
<td>12B</td>
<td>pixtral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen (1-2.5) (Code/Math/MoE/QwQ)</a></td>
<td>0.5B/1.5B/3B/7B/14B/32B/72B/110B</td>
<td>qwen</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen3 (MoE/Instruct/Thinking/Next)</a></td>
<td>0.6B/1.7B/4B/8B/14B/32B/80B/235B</td>
<td>qwen3/qwen3_nothink</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2-Audio</a></td>
<td>7B</td>
<td>qwen2_audio</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2.5-Omni</a></td>
<td>3B/7B</td>
<td>qwen2_omni</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen" rel="nofollow">Qwen2-VL/Qwen2.5-VL/QVQ</a></td>
<td>2B/3B/7B/32B/72B</td>
<td>qwen2_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ByteDance-Seed" rel="nofollow">Seed (OSS/Coder)</a></td>
<td>8B/36B</td>
<td>seed_oss/seed_coder</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Skywork" rel="nofollow">Skywork o1</a></td>
<td>8B</td>
<td>skywork_o1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigcode" rel="nofollow">StarCoder 2</a></td>
<td>3B/7B/15B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Tele-AI" rel="nofollow">TeleChat2</a></td>
<td>3B/7B/35B/115B</td>
<td>telechat2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/xverse" rel="nofollow">XVERSE</a></td>
<td>7B/13B/65B</td>
<td>xverse</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi/Yi-1.5 (Code)</a></td>
<td>1.5B/6B/9B/34B</td>
<td>yi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai" rel="nofollow">Yi-VL</a></td>
<td>6B/34B</td>
<td>yi_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IEITYuan" rel="nofollow">Yuan 2</a></td>
<td>2B/51B/102B</td>
<td>yuan</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">For the &#34;base&#34; models, the <code>template</code> argument can be chosen from <code>default</code>, <code>alpaca</code>, <code>vicuna</code> etc. But make sure to use the <strong>corresponding template</strong> for the &#34;instruct/chat&#34; models.</p>
<p dir="auto">Remember to use the <strong>SAME</strong> template in training and inference.</p>
<p dir="auto">*: You should install the <code>transformers</code> from main branch and use <code>DISABLE_VERSION_CHECK=1</code> to skip version check.</p>
<p dir="auto">**: You need to install a specific version of <code>transformers</code> to use the corresponding model.</p>
</div>
<p dir="auto">Please refer to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/extras/constants.py">constants.py</a> for a full list of models we supported.</p>
<p dir="auto">You also can add a custom chat template to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/data/template.py">template.py</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Supported Training Approaches</h2><a id="user-content-supported-training-approaches" aria-label="Permalink: Supported Training Approaches" href="#supported-training-approaches"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Approach</th>
<th>Full-tuning</th>
<th>Freeze-tuning</th>
<th>LoRA</th>
<th>QLoRA</th>
<th>OFT</th>
<th>QOFT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pre-Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>Supervised Fine-Tuning</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>Reward Modeling</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>PPO Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>DPO Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>KTO Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>ORPO Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>SimPO Training</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p dir="auto">The implementation details of PPO can be found in <a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html" rel="nofollow">this blog</a>.</p>
</div>

<details><summary>Pre-training datasets</summary>
<ul dir="auto">
<li><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/wiki_demo.txt">Wiki Demo (en)</a></li>
<li><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" rel="nofollow">RefinedWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2" rel="nofollow">RedPajama V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220" rel="nofollow">Wikipedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered" rel="nofollow">Wikipedia (zh)</a></li>
<li><a href="https://huggingface.co/datasets/EleutherAI/pile" rel="nofollow">Pile (en)</a></li>
<li><a href="https://huggingface.co/datasets/Skywork/SkyPile-150B" rel="nofollow">SkyPile (zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb" rel="nofollow">FineWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="nofollow">FineWeb-Edu (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-HQ" rel="nofollow">CCI3-HQ (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-Data" rel="nofollow">CCI3-Data (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1" rel="nofollow">CCI4.0-M2-Base-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1" rel="nofollow">CCI4.0-M2-CoT-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1" rel="nofollow">CCI4.0-M2-Extra-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/the-stack" rel="nofollow">The Stack (en)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/starcoderdata" rel="nofollow">StarCoder (en)</a></li>
</ul>
</details>
<details><summary>Supervised fine-tuning datasets</summary>
<ul dir="auto">
<li><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/identity.json">Identity (en&amp;zh)</a></li>
<li><a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca (en)</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3">Stanford Alpaca (zh)</a></li>
<li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Alpaca GPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2" rel="nofollow">Glaive Function Calling V2 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/GAIR/lima" rel="nofollow">LIMA (en)</a></li>
<li><a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset" rel="nofollow">Guanaco Dataset (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN" rel="nofollow">BELLE 2M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN" rel="nofollow">BELLE 1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN" rel="nofollow">BELLE 0.5M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M" rel="nofollow">BELLE Dialogue 0.4M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M" rel="nofollow">BELLE School Math 0.25M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M" rel="nofollow">BELLE Multiturn Chat 0.8M (zh)</a></li>
<li><a href="https://github.com/thunlp/UltraChat">UltraChat (en)</a></li>
<li><a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" rel="nofollow">OpenPlatypus (en)</a></li>
<li><a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k" rel="nofollow">CodeAlpaca 20k (en)</a></li>
<li><a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT" rel="nofollow">Alpaca CoT (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/OpenOrca" rel="nofollow">OpenOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/SlimOrca" rel="nofollow">SlimOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct" rel="nofollow">MathInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M" rel="nofollow">Firefly 1.1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/wiki_qa" rel="nofollow">Wiki QA (en)</a></li>
<li><a href="https://huggingface.co/datasets/suolyer/webqa" rel="nofollow">Web QA (zh)</a></li>
<li><a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn" rel="nofollow">WebNovel (zh)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar" rel="nofollow">Nectar (en)</a></li>
<li><a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data" rel="nofollow">deepctrl (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HasturOfficial/adgen" rel="nofollow">Advertise Generating (zh)</a></li>
<li><a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k" rel="nofollow">ShareGPT Hyperfiltered (en)</a></li>
<li><a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4" rel="nofollow">ShareGPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" rel="nofollow">UltraChat 200k (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct" rel="nofollow">Infinity Instruct (zh)</a></li>
<li><a href="https://huggingface.co/datasets/THUDM/AgentInstruct" rel="nofollow">AgentInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" rel="nofollow">LMSYS Chat 1M (en)</a></li>
<li><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k" rel="nofollow">Evol Instruct V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia" rel="nofollow">Cosmopedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/stem_zh_instruction" rel="nofollow">STEM (zh)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo" rel="nofollow">Ruozhiba (zh)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2" rel="nofollow">Neo-sft (zh)</a></li>
<li><a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered" rel="nofollow">Magpie-Pro-300K-Filtered (en)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1" rel="nofollow">Magpie-ultra-v0.1 (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub" rel="nofollow">WebInstructSub (en)</a></li>
<li><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT" rel="nofollow">OpenO1-SFT (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k" rel="nofollow">Open-Thoughts (en)</a></li>
<li><a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k" rel="nofollow">Open-R1-Math (en)</a></li>
<li><a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT" rel="nofollow">Chinese-DeepSeek-R1-Distill (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k" rel="nofollow">LLaVA mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions" rel="nofollow">Pokemon-gpt4o-captions (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de" rel="nofollow">Open Assistant (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de" rel="nofollow">Dolly 15k (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de" rel="nofollow">Alpaca GPT4 (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de" rel="nofollow">OpenSchnabeltier (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de" rel="nofollow">Evol Instruct (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de" rel="nofollow">Dolphin (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de" rel="nofollow">Booksum (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de" rel="nofollow">Airoboros (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de" rel="nofollow">Ultrachat (de)</a></li>
</ul>
</details>
<details><summary>Preference datasets</summary>
<ul dir="auto">
<li><a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k" rel="nofollow">DPO mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized" rel="nofollow">UltraFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/COIG-P" rel="nofollow">COIG-P (zh)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset" rel="nofollow">RLHF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Zhihui/VLFeedback" rel="nofollow">VLFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset" rel="nofollow">RLAIF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs" rel="nofollow">Orca DPO Pairs (en)</a></li>
<li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf" rel="nofollow">HH-RLHF (en)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar" rel="nofollow">Nectar (en)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de" rel="nofollow">Orca DPO (de)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/kto-mix-15k" rel="nofollow">KTO mixed (en)</a></li>
</ul>
</details>
<p dir="auto">Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install --upgrade huggingface_hub
huggingface-cli login"><pre>pip install --upgrade huggingface_hub
huggingface-cli login</pre></div>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Mandatory</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>3.9</td>
<td>3.10</td>
</tr>
<tr>
<td>torch</td>
<td>2.0.0</td>
<td>2.6.0</td>
</tr>
<tr>
<td>torchvision</td>
<td>0.15.0</td>
<td>0.21.0</td>
</tr>
<tr>
<td>transformers</td>
<td>4.49.0</td>
<td>4.50.0</td>
</tr>
<tr>
<td>datasets</td>
<td>2.16.0</td>
<td>3.2.0</td>
</tr>
<tr>
<td>accelerate</td>
<td>0.34.0</td>
<td>1.2.1</td>
</tr>
<tr>
<td>peft</td>
<td>0.14.0</td>
<td>0.15.1</td>
</tr>
<tr>
<td>trl</td>
<td>0.8.6</td>
<td>0.9.6</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Optional</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>CUDA</td>
<td>11.6</td>
<td>12.2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.10.0</td>
<td>0.16.4</td>
</tr>
<tr>
<td>bitsandbytes</td>
<td>0.39.0</td>
<td>0.43.1</td>
</tr>
<tr>
<td>vllm</td>
<td>0.4.3</td>
<td>0.8.2</td>
</tr>
<tr>
<td>flash-attn</td>
<td>2.5.6</td>
<td>2.7.2</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">* <em>estimated</em></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Method</th>
<th>Bits</th>
<th>7B</th>
<th>14B</th>
<th>30B</th>
<th>70B</th>
<th><code>x</code>B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full (<code>bf16</code> or <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code>GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code>GB</td>
</tr>
<tr>
<td>Freeze/LoRA/GaLore/APOLLO/BAdam/OFT</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code>GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>


<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p dir="auto">Installation is mandatory.</p>
</div>

<div dir="auto" data-snippet-clipboard-copy-content="git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &#34;.[torch,metrics]&#34; --no-build-isolation"><pre>git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
<span>cd</span> LLaMA-Factory
pip install -e <span><span>&#34;</span>.[torch,metrics]<span>&#34;</span></span> --no-build-isolation</pre></div>
<p dir="auto">Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Install from Docker Image</h4><a id="user-content-install-from-docker-image" aria-label="Permalink: Install from Docker Image" href="#install-from-docker-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest"><pre>docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest</pre></div>
<p dir="auto">This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.</p>
<p dir="auto">Find the pre-built images: <a href="https://hub.docker.com/r/hiyouga/llamafactory/tags" rel="nofollow">https://hub.docker.com/r/hiyouga/llamafactory/tags</a></p>
<p dir="auto">Please refer to <a href="#build-docker">build docker</a> to build the image yourself.</p>
<details><summary>Setting up a virtual environment with <b>uv</b></summary>
<p dir="auto">Create an isolated Python environment with <a href="https://github.com/astral-sh/uv">uv</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv sync --extra torch --extra metrics --prerelease=allow"><pre>uv sync --extra torch --extra metrics --prerelease=allow</pre></div>
<p dir="auto">Run LLaMA-Factory in the isolated environment:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml"><pre>uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml</pre></div>
</details>
<details><summary>For Windows users</summary>

<p dir="auto">You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the <a href="https://pytorch.org/get-started/locally/" rel="nofollow">official website</a> and the following command to install PyTorch with CUDA support:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c &#34;import torch; print(torch.cuda.is_available())&#34;"><pre>pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c <span><span>&#34;</span>import torch; print(torch.cuda.is_available())<span>&#34;</span></span></pre></div>
<p dir="auto">If you see <code>True</code> then you have successfully installed PyTorch with CUDA support.</p>
<p dir="auto">Try <code>dataloader_num_workers: 0</code> if you encounter <code>Can&#39;t pickle local object</code> error.</p>

<p dir="auto">If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of <code>bitsandbytes</code> library, which supports CUDA 11.1 to 12.2, please select the appropriate <a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels">release version</a> based on your CUDA version.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl"><pre>pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl</pre></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">Install Flash Attention-2</h4><a id="user-content-install-flash-attention-2" aria-label="Permalink: Install Flash Attention-2" href="#install-flash-attention-2"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To enable FlashAttention-2 on the Windows platform, please use the script from <a href="https://huggingface.co/lldacing/flash-attention-windows-wheel" rel="nofollow">flash-attention-windows-wheel</a> to compile and install it by yourself.</p>
</details>
<details><summary>For Ascend NPU users</summary>
<p dir="auto">To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: <code>pip install -e &#34;.[torch-npu,metrics]&#34;</code>. Additionally, you need to install the <strong><a href="https://www.hiascend.com/developer/download/community/result?module=cann" rel="nofollow">Ascend CANN Toolkit and Kernels</a></strong>. Please follow the <a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html" rel="nofollow">installation tutorial</a> or use the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&#34;$(uname -i)&#34;.run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh"><pre><span><span>#</span> replace the url according to your CANN version and devices</span>
<span><span>#</span> install CANN Toolkit</span>
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-<span><span>&#34;</span><span><span>$(</span>uname -i<span>)</span></span><span>&#34;</span></span>.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-<span><span>&#34;</span><span><span>$(</span>uname -i<span>)</span></span><span>&#34;</span></span>.run --install

<span><span>#</span> install CANN Kernels</span>
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-<span><span>&#34;</span><span><span>$(</span>uname -i<span>)</span></span><span>&#34;</span></span>.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-<span><span>&#34;</span><span><span>$(</span>uname -i<span>)</span></span><span>&#34;</span></span>.run --install

<span><span>#</span> set env variables</span>
<span>source</span> /usr/local/Ascend/ascend-toolkit/set_env.sh</pre></div>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody>
<tr>
<td>CANN</td>
<td>8.0.RC1</td>
<td>8.0.0.alpha002</td>
</tr>
<tr>
<td>torch</td>
<td>2.1.0</td>
<td>2.4.0</td>
</tr>
<tr>
<td>torch-npu</td>
<td>2.1.0</td>
<td>2.4.0.post2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.13.2</td>
<td>0.13.2</td>
</tr>
<tr>
<td>vllm-ascend</td>
<td>-</td>
<td>0.7.3</td>
</tr>
</tbody>
</table>
<p dir="auto">Remember to use <code>ASCEND_RT_VISIBLE_DEVICES</code> instead of <code>CUDA_VISIBLE_DEVICES</code> to specify the device to use.</p>
<p dir="auto">If you cannot infer model on NPU devices, try setting <code>do_sample: false</code> in the configurations.</p>
<p dir="auto">Download the pre-built Docker images: <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html" rel="nofollow">32GB</a> | <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html" rel="nofollow">64GB</a></p>
<h4 dir="auto">Install BitsAndBytes</h4>
<p dir="auto">To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:</p>
<ol dir="auto">
<li>Manually compile bitsandbytes: Refer to <a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;platform=Ascend+NPU" rel="nofollow">the installation documentation</a> for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install ."><pre><span><span>#</span> Install bitsandbytes from source</span>
<span><span>#</span> Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch</span>
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
<span>cd</span> bitsandbytes/

<span><span>#</span> Install dependencies</span>
pip install -r requirements-dev.txt

<span><span>#</span> Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference</span>
apt-get install -y build-essential cmake

<span><span>#</span> Compile &amp; install  </span>
cmake -DCOMPUTE_BACKEND=npu -S <span>.</span>
make
pip install <span>.</span></pre></div>
<ol start="2" dir="auto">
<li>Install transformers from the main branch.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install ."><pre>git clone -b main https://github.com/huggingface/transformers.git
<span>cd</span> transformers
pip install <span>.</span></pre></div>
<ol start="3" dir="auto">
<li>Set <code>double_quantization: false</code> in the configuration. You can refer to the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml">example</a>.</li>
</ol>
</details>

<p dir="auto">Please refer to <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README.md">data/README.md</a> for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">Please update <code>data/dataset_info.json</code> to use your custom dataset.</p>
</div>
<p dir="auto">You can also use <strong><a href="https://github.com/ConardLi/easy-dataset">Easy Dataset</a></strong>, <strong><a href="https://github.com/OpenDCAI/DataFlow">DataFlow</a></strong> and <strong><a href="https://github.com/open-sciencelab/GraphGen">GraphGen</a></strong> to create synthetic data for fine-tuning.</p>

<p dir="auto">Use the following 3 commands to run LoRA <strong>fine-tuning</strong>, <strong>inference</strong> and <strong>merging</strong> of the Llama3-8B-Instruct model, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml"><pre>llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli <span>export</span> examples/merge_lora/llama3_lora_sft.yaml</pre></div>
<p dir="auto">See <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README.md">examples/README.md</a> for advanced usage (including distributed training).</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p dir="auto">Use <code>llamafactory-cli help</code> to show help information.</p>
<p dir="auto">Read <a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614" data-hovercard-type="issue" data-hovercard-url="/hiyouga/LLaMA-Factory/issues/4614/hovercard">FAQs</a> first if you encounter any problems.</p>
</div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Fine-Tuning with LLaMA Board GUI (powered by <a href="https://github.com/gradio-app/gradio">Gradio</a>)</h3><a id="user-content-fine-tuning-with-llama-board-gui-powered-by-gradio" aria-label="Permalink: Fine-Tuning with LLaMA Board GUI (powered by Gradio)" href="#fine-tuning-with-llama-board-gui-powered-by-gradio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>


<p dir="auto">Read our <a href="https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory" rel="nofollow">documentation</a>.</p>

<p dir="auto">For CUDA users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-cuda/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<p dir="auto">For Ascend NPU users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-npu/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<p dir="auto">For AMD ROCm users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash"><pre><span>cd</span> docker/docker-rocm/
docker compose up -d
docker compose <span>exec</span> llamafactory bash</pre></div>
<details><summary>Build without Docker Compose</summary>
<p dir="auto">For CUDA users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
<p dir="auto">For Ascend NPU users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
<p dir="auto">For AMD ROCm users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash"><pre>docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest <span>.</span>

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker <span>exec</span> -it llamafactory bash</pre></div>
</details>
<details><summary>Use Docker volumes</summary>
<p dir="auto">You can uncomment <code>VOLUME [ &#34;/root/.cache/huggingface&#34;, &#34;/app/shared_data&#34;, &#34;/app/output&#34; ]</code> in the Dockerfile to use data volumes.</p>
<p dir="auto">When building the Docker image, use <code>-v ./hf_cache:/root/.cache/huggingface</code> argument to mount the local directory to the container. The following data volumes are available.</p>
<ul dir="auto">
<li><code>hf_cache</code>: Utilize Hugging Face cache on the host machine.</li>
<li><code>shared_data</code>: The directionary to store datasets on the host machine.</li>
<li><code>output</code>: Set export dir to this location so that the merged result can be accessed directly on the host machine.</li>
</ul>
</details>
<div dir="auto"><h3 tabindex="-1" dir="auto">Deploy with OpenAI-style API and vLLM</h3><a id="user-content-deploy-with-openai-style-api-and-vllm" aria-label="Permalink: Deploy with OpenAI-style API and vLLM" href="#deploy-with-openai-style-api-and-vllm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true"><pre>API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true</pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto">Download from ModelScope Hub</h3><a id="user-content-download-from-modelscope-hub" aria-label="Permalink: Download from ModelScope Hub" href="#download-from-modelscope-hub"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows"><pre><span>export</span> USE_MODELSCOPE_HUB=1 <span><span>#</span> `set USE_MODELSCOPE_HUB=1` for Windows</span></pre></div>
<p dir="auto">Train the model by specifying a model ID of the ModelScope Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelscope.cn/models" rel="nofollow">ModelScope Hub</a>, e.g., <code>LLM-Research/Meta-Llama-3-8B-Instruct</code>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Download from Modelers Hub</h3><a id="user-content-download-from-modelers-hub" aria-label="Permalink: Download from Modelers Hub" href="#download-from-modelers-hub"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can also use Modelers Hub to download models and datasets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows"><pre><span>export</span> USE_OPENMIND_HUB=1 <span><span>#</span> `set USE_OPENMIND_HUB=1` for Windows</span></pre></div>
<p dir="auto">Train the model by specifying a model ID of the Modelers Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelers.cn/models" rel="nofollow">Modelers Hub</a>, e.g., <code>TeleAI/TeleChat-7B-pt</code>.</p>

<p dir="auto">To use <a href="https://wandb.ai" rel="nofollow">Weights &amp; Biases</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="report_to: wandb
run_name: test_run # optional"><pre><span>report_to</span>: <span>wandb</span>
<span>run_name</span>: <span>test_run </span><span><span>#</span> optional</span></pre></div>
<p dir="auto">Set <code>WANDB_API_KEY</code> to <a href="https://wandb.ai/authorize" rel="nofollow">your key</a> when launching training tasks to log in with your W&amp;B account.</p>

<p dir="auto">To use <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use_swanlab: true
swanlab_run_name: test_run # optional"><pre><span>use_swanlab</span>: <span>true</span>
<span>swanlab_run_name</span>: <span>test_run </span><span><span>#</span> optional</span></pre></div>
<p dir="auto">When launching training tasks, you can log in to SwanLab in three ways:</p>
<ol dir="auto">
<li>Add <code>swanlab_api_key=&lt;your_api_key&gt;</code> to the yaml file, and set it to your <a href="https://swanlab.cn/settings" rel="nofollow">API key</a>.</li>
<li>Set the environment variable <code>SWANLAB_API_KEY</code> to your <a href="https://swanlab.cn/settings" rel="nofollow">API key</a>.</li>
<li>Use the <code>swanlab login</code> command to complete the login.</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">Projects using LLaMA Factory</h2><a id="user-content-projects-using-llama-factory" aria-label="Permalink: Projects using LLaMA Factory" href="#projects-using-llama-factory"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you have a project that should be incorporated, please contact via email or create a pull request.</p>
<details><summary>Click to show</summary>
<ol dir="auto">
<li>Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. <a href="https://arxiv.org/abs/2308.02223" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. <a href="https://arxiv.org/abs/2308.10092" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. <a href="https://arxiv.org/abs/2308.10526" rel="nofollow">[arxiv]</a></li>
<li>Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. <a href="https://arxiv.org/abs/2311.07816" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. <a href="https://arxiv.org/abs/2312.15710" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. <a href="https://arxiv.org/abs/2401.04319" rel="nofollow">[arxiv]</a></li>
<li>Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. <a href="https://arxiv.org/abs/2401.07286" rel="nofollow">[arxiv]</a></li>
<li>Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. <a href="https://arxiv.org/abs/2402.05904" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. <a href="https://arxiv.org/abs/2402.07625" rel="nofollow">[arxiv]</a></li>
<li>Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11176" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. <a href="https://arxiv.org/abs/2402.11187" rel="nofollow">[arxiv]</a></li>
<li>Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. <a href="https://arxiv.org/abs/2402.11746" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. <a href="https://arxiv.org/abs/2402.11801" rel="nofollow">[arxiv]</a></li>
<li>Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. <a href="https://arxiv.org/abs/2402.11809" rel="nofollow">[arxiv]</a></li>
<li>Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11819" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. <a href="https://arxiv.org/abs/2402.12204" rel="nofollow">[arxiv]</a></li>
<li>Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.14714" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. <a href="https://arxiv.org/abs/2402.15043" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. <a href="https://arxiv.org/abs/2403.02333" rel="nofollow">[arxiv]</a></li>
<li>Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. <a href="https://arxiv.org/abs/2403.03419" rel="nofollow">[arxiv]</a></li>
<li>Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. <a href="https://arxiv.org/abs/2403.08228" rel="nofollow">[arxiv]</a></li>
<li>Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2403.09073" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. <a href="https://arxiv.org/abs/2403.14541" rel="nofollow">[arxiv]</a></li>
<li>Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. <a href="https://arxiv.org/abs/2403.15246" rel="nofollow">[arxiv]</a></li>
<li>Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. <a href="https://arxiv.org/abs/2403.16008" rel="nofollow">[arxiv]</a></li>
<li>Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. <a href="https://arxiv.org/abs/2403.16443" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. <a href="https://arxiv.org/abs/2404.00604" rel="nofollow">[arxiv]</a></li>
<li>Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.02827" rel="nofollow">[arxiv]</a></li>
<li>Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. <a href="https://arxiv.org/abs/2404.04167" rel="nofollow">[arxiv]</a></li>
<li>Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. <a href="https://arxiv.org/abs/2404.04316" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.07084" rel="nofollow">[arxiv]</a></li>
<li>Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.09836" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.11581" rel="nofollow">[arxiv]</a></li>
<li>Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. <a href="https://arxiv.org/abs/2404.14215" rel="nofollow">[arxiv]</a></li>
<li>Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. <a href="https://arxiv.org/abs/2404.16621" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. <a href="https://arxiv.org/abs/2404.17140" rel="nofollow">[arxiv]</a></li>
<li>Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. <a href="https://arxiv.org/abs/2404.18585" rel="nofollow">[arxiv]</a></li>
<li>Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. <a href="https://arxiv.org/abs/2405.04760" rel="nofollow">[arxiv]</a></li>
<li>Dammu et al. &#34;They are uncultured&#34;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. <a href="https://arxiv.org/abs/2405.05378" rel="nofollow">[arxiv]</a></li>
<li>Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. <a href="https://arxiv.org/abs/2405.09055" rel="nofollow">[arxiv]</a></li>
<li>Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. <a href="https://arxiv.org/abs/2405.12739" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2405.13816" rel="nofollow">[arxiv]</a></li>
<li>Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. <a href="https://arxiv.org/abs/2405.20215" rel="nofollow">[arxiv]</a></li>
<li>Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. <a href="https://aclanthology.org/2024.lt4hala-1.30" rel="nofollow">[paper]</a></li>
<li>Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. <a href="https://arxiv.org/abs/2406.00380" rel="nofollow">[arxiv]</a></li>
<li>Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. <a href="https://arxiv.org/abs/2406.02106" rel="nofollow">[arxiv]</a></li>
<li>Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. <a href="https://arxiv.org/abs/2406.03136" rel="nofollow">[arxiv]</a></li>
<li>Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. <a href="https://arxiv.org/abs/2406.04496" rel="nofollow">[arxiv]</a></li>
<li>Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. <a href="https://arxiv.org/abs/2406.05688" rel="nofollow">[arxiv]</a></li>
<li>Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. <a href="https://arxiv.org/abs/2406.05955" rel="nofollow">[arxiv]</a></li>
<li>Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. <a href="https://arxiv.org/abs/2406.06973" rel="nofollow">[arxiv]</a></li>
<li>Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. <a href="https://arxiv.org/abs/2406.07115" rel="nofollow">[arxiv]</a></li>
<li>Zhu et al. Are Large Language Models Good Statisticians?. 2024. <a href="https://arxiv.org/abs/2406.07815" rel="nofollow">[arxiv]</a></li>
<li>Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. <a href="https://arxiv.org/abs/2406.10099" rel="nofollow">[arxiv]</a></li>
<li>Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. <a href="https://arxiv.org/abs/2406.10173" rel="nofollow">[arxiv]</a></li>
<li>He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. <a href="https://arxiv.org/abs/2406.12074" rel="nofollow">[arxiv]</a></li>
<li>Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. <a href="https://arxiv.org/abs/2406.14408" rel="nofollow">[arxiv]</a></li>
<li>Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. <a href="https://arxiv.org/abs/2406.14546" rel="nofollow">[arxiv]</a></li>
<li>Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. <a href="https://arxiv.org/abs/2406.15695" rel="nofollow">[arxiv]</a></li>
<li>Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. <a href="https://arxiv.org/abs/2406.17233" rel="nofollow">[arxiv]</a></li>
<li>Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. <a href="https://arxiv.org/abs/2406.18069" rel="nofollow">[arxiv]</a></li>
<li>Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. <a href="https://aclanthology.org/2024.americasnlp-1.25" rel="nofollow">[paper]</a></li>
<li>Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. <a href="https://arxiv.org/abs/2406.19949" rel="nofollow">[arxiv]</a></li>
<li>Yang et al. Financial Knowledge Large Language Model. 2024. <a href="https://arxiv.org/abs/2407.00365" rel="nofollow">[arxiv]</a></li>
<li>Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. <a href="https://arxiv.org/abs/2407.01470" rel="nofollow">[arxiv]</a></li>
<li>Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. <a href="https://arxiv.org/abs/2407.06129" rel="nofollow">[arxiv]</a></li>
<li>Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. <a href="https://arxiv.org/abs/2407.08044" rel="nofollow">[arxiv]</a></li>
<li>Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. <a href="https://arxiv.org/abs/2407.09756" rel="nofollow">[arxiv]</a></li>
<li>Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. <a href="https://scholarcommons.scu.edu/cseng_senior/272/" rel="nofollow">[paper]</a></li>
<li>Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. <a href="https://arxiv.org/abs/2407.13561" rel="nofollow">[arxiv]</a></li>
<li>Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. <a href="https://arxiv.org/abs/2407.16637" rel="nofollow">[arxiv]</a></li>
<li>Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. <a href="https://arxiv.org/abs/2407.17535" rel="nofollow">[arxiv]</a></li>
<li>Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. <a href="https://arxiv.org/abs/2407.19705" rel="nofollow">[arxiv]</a></li>
<li>Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. <a href="https://arxiv.org/abs/2408.00137" rel="nofollow">[arxiv]</a></li>
<li>Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. <a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf" rel="nofollow">[paper]</a></li>
<li>Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11" rel="nofollow">[paper]</a></li>
<li>Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23" rel="nofollow">[paper]</a></li>
<li>Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. <a href="https://arxiv.org/abs/2408.04693" rel="nofollow">[arxiv]</a></li>
<li>Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. <a href="https://arxiv.org/abs/2408.04168" rel="nofollow">[arxiv]</a></li>
<li>Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. <a href="https://aclanthology.org/2024.finnlp-2.1/" rel="nofollow">[paper]</a></li>
<li>Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. <a href="https://arxiv.org/abs/2408.08072" rel="nofollow">[arxiv]</a></li>
<li>Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. <a href="https://dl.acm.org/doi/10.1145/3627673.3679611" rel="nofollow">[paper]</a></li>
<li>Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. <a href="https://aclanthology.org/2024.findings-acl.830.pdf" rel="nofollow">[paper]</a></li>
<li><strong><a href="https://github.com/Yu-Yang-Li/StarWhisper">StarWhisper</a></strong>: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.</li>
<li><strong><a href="https://github.com/FudanDISC/DISC-LawLLM">DISC-LawLLM</a></strong>: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.</li>
<li><strong><a href="https://github.com/X-D-Lab/Sunsimiao">Sunsimiao</a></strong>: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.</li>
<li><strong><a href="https://github.com/WangRongsheng/CareGPT">CareGPT</a></strong>: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.</li>
<li><strong><a href="https://github.com/PKU-YuanGroup/Machine-Mindset/">MachineMindset</a></strong>: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.</li>
<li><strong><a href="https://huggingface.co/Nekochu/Luminia-13B-v3" rel="nofollow">Luminia-13B-v3</a></strong>: A large language model specialized in generate metadata for stable diffusion. <a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt" rel="nofollow">[demo]</a></li>
<li><strong><a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med">Chinese-LLaVA-Med</a></strong>: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.</li>
<li><strong><a href="https://github.com/THUDM/AutoRE">AutoRE</a></strong>: A document-level relation extraction system based on large language models.</li>
<li><strong><a href="https://github.com/NVIDIA/RTX-AI-Toolkit">NVIDIA RTX AI Toolkit</a></strong>: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.</li>
<li><strong><a href="https://github.com/LazyAGI/LazyLLM">LazyLLM</a></strong>: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.</li>
<li><strong><a href="https://github.com/NLPJCL/RAG-Retrieval">RAG-Retrieval</a></strong>: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. <a href="https://zhuanlan.zhihu.com/p/987727357" rel="nofollow">[blog]</a></li>
<li><strong><a href="https://github.com/Qihoo360/360-LLaMA-Factory">360-LLaMA-Factory</a></strong>: A modified library that supports long sequence SFT &amp; DPO using ring attention.</li>
<li><strong><a href="https://novasky-ai.github.io/posts/sky-t1/" rel="nofollow">Sky-T1</a></strong>: An o1-like model fine-tuned by NovaSky AI with very small cost.</li>
<li><strong><a href="https://github.com/xming521/WeClone">WeClone</a></strong>: One-stop solution for creating your digital avatar from chat logs.</li>
<li><strong><a href="https://github.com/SmartFlowAI/EmoLLM">EmoLLM</a></strong>: A project about large language models (LLMs) and mental health.</li>
</ol>
</details>

<p dir="auto">This repository is licensed under the <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Apache-2.0 License</a>.</p>
<p dir="auto">Please follow the model licenses to use the corresponding model weights: <a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf" rel="nofollow">Baichuan 2</a> / <a href="https://huggingface.co/spaces/bigscience/license" rel="nofollow">BLOOM</a> / <a href="https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE">ChatGLM3</a> / <a href="https://cohere.com/c4ai-cc-by-nc-license" rel="nofollow">Command R</a> / <a href="https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL">DeepSeek</a> / <a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt" rel="nofollow">Falcon</a> / <a href="https://ai.google.dev/gemma/terms" rel="nofollow">Gemma</a> / <a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE" rel="nofollow">GLM-4</a> / <a href="https://github.com/openai/gpt-2/blob/master/LICENSE">GPT-2</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Granite</a> / <a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE" rel="nofollow">Index</a> / <a href="https://github.com/InternLM/InternLM#license">InternLM</a> / <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">Llama</a> / <a href="https://ai.meta.com/llama/license/" rel="nofollow">Llama 2</a> / <a href="https://llama.meta.com/llama3/license/" rel="nofollow">Llama 3</a> / <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE">Llama 4</a> / <a href="https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md">MiniCPM</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Mistral/Mixtral/Pixtral</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">OLMo</a> / <a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx" rel="nofollow">Phi-1.5/Phi-2</a> / <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE" rel="nofollow">Phi-3/Phi-4</a> / <a href="https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT">Qwen</a> / <a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf" rel="nofollow">Skywork</a> / <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" rel="nofollow">StarCoder 2</a> / <a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf" rel="nofollow">TeleChat2</a> / <a href="https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf">XVERSE</a> / <a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE" rel="nofollow">Yi</a> / <a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/LICENSE">Yi-1.5</a> / <a href="https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan">Yuan 2</a></p>

<p dir="auto">If this work is helpful, please kindly cite as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}"><pre><span>@inproceedings</span>{<span>zheng2024llamafactory</span>,
  <span>title</span>=<span><span>{</span>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)<span>}</span></span>,
  <span>address</span>=<span><span>{</span>Bangkok, Thailand<span>}</span></span>,
  <span>publisher</span>=<span><span>{</span>Association for Computational Linguistics<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
  <span>url</span>=<span><span>{</span>http://arxiv.org/abs/2403.13372<span>}</span></span>
}</pre></div>

<p dir="auto">This repo benefits from <a href="https://github.com/huggingface/peft">PEFT</a>, <a href="https://github.com/huggingface/trl">TRL</a>, <a href="https://github.com/artidoro/qlora">QLoRA</a> and <a href="https://github.com/lm-sys/FastChat">FastChat</a>. Thanks for their wonderful works.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dbff38fb23c7398d1ac1b63e4685ebda2bf94c75cb36001bb964f15b4619546c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6869796f7567612f4c4c614d412d466163746f727926747970653d44617465"><img src="https://camo.githubusercontent.com/dbff38fb23c7398d1ac1b63e4685ebda2bf94c75cb36001bb964f15b4619546c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6869796f7567612f4c4c614d412d466163746f727926747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;type=Date"/></a></p>
</article></div></div>
  </body>
</html>
