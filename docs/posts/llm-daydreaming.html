<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gwern.net/ai-daydreaming">Original</a>
    <h1>LLM Daydreaming</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata">
          
        <p>Proposal &amp; discussion of how default mode networks for LLMs are an example of missing capabilities for search and novelty in contemporary AI systems.</p>
        
      </div><div id="markdownBody"><div>
<blockquote>
<p>Despite impressive capabilities, large language models have yet to produce a genuine breakthrough. The puzzle is why.</p>
<p>A reason may be that they lack some fundamental aspects of human thought: they are frozen, unable to learn from experience, and they have no “default mode” for background processing, a source of spontaneous human insight.</p>
<p>To solve this, I propose a <strong>day-dreaming loop (DDL)</strong>: a background process that continuously samples pairs of concepts from memory. A generator model explores non-obvious links between them, and a critic model filters the results for genuinely valuable ideas. These discoveries are fed back into the system’s memory, creating a compounding feedback loop where new ideas themselves become seeds for future combinations.</p>
<p>The cost of this process—a “daydreaming tax”—would be substantial, given the low hit rate for truly novel connections. This expense, however, may be the necessary price for innovation. It would also create a moat against model distillation, as valuable insights emerge from the combinations no one would know to ask for.</p>
<p>The strategic implication is counterintuitive: to make AI cheaper and faster for end users, we might first need to build systems that spend most of their compute on this “wasteful” background search. This suggests a future where expensive, daydreaming AIs are used primarily to generate proprietary training data for the next generation of efficient models, offering a path around the looming data wall.</p>
</blockquote>
</div>
<div>
<blockquote>
<p>…I feel I am nibbling on the edges of this world when I am capable of getting what <a href="https://en.wikipedia.org/wiki/Pablo_Picasso" id="_4GvilLi6" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Pablo_Picasso#bodyContent" title="Pablo Picasso">Picasso</a> means when he says to me—perfectly straight-facedly—later of the enormous new mechanical brains or calculating machines:</p>
<p>“But they are useless. They can only give you answers.”</p>
<p><a href="https://en.wikipedia.org/wiki/William_Fifield" id="_K5rEEW0d" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/William_Fifield#bodyContent" title="William Fifield">William Fifield</a>, <a href="https://www.theparisreview.org/miscellaneous/4487/pablo-picasso-a-composite-interview-william-fifield" id="_9zNgvCzZ" data-link-icon="PR" data-link-icon-type="text">“Pablo Picasso—A Composite Interview”</a> (<span>1964<sub><span title="1964 was 61 years ago.">61ya</span></sub></span>)</p>
</blockquote>
</div>
<p><a href="https://x.com/dwarkesh_sp/status/1727004083113128327" id="dwarkesh_sp-2024" data-link-icon="twitter" data-link-icon-type="svg" data-link-icon-color="#1da1f2" data-href-mobile="https://nitter.net/dwarkesh_sp/status/1727004083113128327" title="dwarkesh_sp 2024">Dwarkesh Patel</a> asks why no <a href="https://en.wikipedia.org/wiki/Large_language_model" id="_r_7Wv5_b" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Large_language_model#bodyContent" title="Large language model">LLM</a> has (apparently) ever made a major breakthrough or unexpected insight, no matter how vast their knowledge or how high their benchmark scores. While those are, by definition, extremely rare, contemporary chatbot-style LLMs have now been used seriously by tens of millions of people since <a href="https://gwern.net/doc/www/openai.com/71d89c66e64656d27dded64f2a102337e410bcd1.html" id="_iOQalFYH" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/openai.com/71d89c66e64656d27dded64f2a102337e410bcd1.html" data-url-original="https://openai.com/blog/chatgpt/" data-filesize-bytes="1631677" data-filesize-percentage="59">ChatGPT</a> (November 2022), and it does seem like there ought to be at least <em>some</em> examples at this point. This is a genuine puzzle: when prompted with the right hints, these models can synthesize information in ways that feel tantalizingly close to true insight; the raw components of intelligence seem to be present; but… they don’t. What is missing?</p>
<p>It’s hard to say because there are so many differences between LLMs and human researchers.</p>
<section id="missing-faculties">

<section id="continual-learning">
<h2><a href="#continual-learning" title="Link to section: § &#39;Continual Learning&#39;">Continual Learning</a></h2>
<p>Frozen NNs are amnesiacs. One salient difference is that LLMs are ‘frozen’, and are not allowed to change; they don’t have to be, and could be trained on the fly (eg by the longstanding technique of <a href="https://gwern.net/doc/ai/nn/dynamic-evaluation/index" id="_89UNMIwO" data-filesize-bytes="51417" data-filesize-percentage="41" title="‘dynamic evaluation (NN)’ directory">dynamic evaluation</a>), but they aren’t.</p>
<p>So perhaps that’s a reason they struggle to move beyond their initial guesses or obvious answers, and come up with truly novel insights—in a very real sense, LLMs are <em>unable to learn</em>. They are truly amnesiac. And there are no cases anywhere in human history, as far as I am aware, of any human with anterograde amnesia producing major novelties.</p>
<p>That may be an adequate answer all on its own: they are trapped in their prior knowledge, and cannot move far beyond their known knowledge; but by definition, all that is either known or almost known, and cannot be impressively novel.</p>
</section>
<section id="continual-thinking">
<h2><a href="#continual-thinking" title="Link to section: § &#39;Continual Thinking&#39;">Continual Thinking</a></h2>
<p>But another notable difference is that human researchers <em>never stop thinking</em>. We are doing our continual learning on not just observations, but on our own thoughts—even when asleep, a human is still computing and processing. (This helps account for the shocking metabolic demands of <a href="https://www.quantamagazine.org/how-much-energy-does-it-take-to-think-20250604/" id="feehly-2025" data-link-icon="quanta" data-link-icon-type="svg" data-link-icon-color="#ff8600" title="‘How Much Energy Does It Take To Think? Studies of neural metabolism reveal our brain’s effort to keep us alive and the evolutionary constraints that sculpted our most complex organ’, Feehly 2025">even a brain which is ‘doing nothing’</a>—it’s actually still doing a lot! As difficult as it may <em>feel</em> to think hard, from a biological perspective, it’s trivial.)</p>
<p>Research on science &amp; creativity emphasizes the benefit of time &amp; sleep in creating effects like the <a href="https://en.wikipedia.org/wiki/Incubation_(psychology)" id="_kCnWGTHK" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Incubation_(psychology)#bodyContent" title="Incubation (psychology)">incubation effect</a>, and some researchers have famously had sudden insights from dreams. And we have all had the experience of a thought erupting into consciousness, <span id="kohls">whether it’s just an inane pun (“you can buy <a href="https://en.wikipedia.org/wiki/Kohl_(cosmetics)" id="_2mUeRgzy" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Kohl_(cosmetics)#bodyContent" title="&lt;em&gt;Kohl&lt;/em&gt; (cosmetics)">kohl</a> at <a href="https://en.wikipedia.org/wiki/Kohl" id="_kI_BuYZ1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Kohl#bodyContent" title="Kohl">Kohl’s</a>, LOL”)</span>, a clever retort <a href="https://en.wikipedia.org/wiki/L%27esprit_de_l%27escalier" id="_VXjo7JG1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/L%27esprit_de_l%27escalier#bodyContent" title="&lt;em&gt;L’esprit de l’escalier&lt;/em&gt;">hours too late</a>, a <a href="https://en.wikipedia.org/wiki/Tip_of_the_tongue" id="_8XAv2Or0" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Tip_of_the_tongue#bodyContent" title="Tip of the tongue">frustrating word</a> finally coming to mind, suddenly recalling anxious worries (“did I <em>really</em> turn off the stove?”) like <a href="https://en.wikipedia.org/wiki/Intrusive_thought" id="_jIYXpx3w" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Intrusive_thought#bodyContent" title="Intrusive thought">intrusive thoughts</a>, or, once in a lifetime, a brilliant idea. (Try meditating for the first time and writing down all the thoughts that pop up until they finally stop coming, and one may be amazed &amp; frustrated!)</p>
<p>Often these eruptions have nothing at all to do with anything we have been thinking about, or have thought about in decades (“wait—back at that college party, when that girl looked at my hand—<a href="https://gwern.net/doc/psychology/man-hands/index" id="_nrq65EkM" data-filesize-bytes="20185" data-filesize-percentage="25">she was hitting on me</a>, wasn’t she?”) Indeed, this essay is itself the product of such an eruption—“what is the LLM equivalent of a default mode network? Well, it could look something like <a href="#jones-2021-combinatorial"><span><span>2021</span></span></a>, couldn’t it?”—and had nothing to do with what I had been writing about (the esthetics of video games).</p>
</section>
</section>
<section id="hypothesis-day-dreaming-loop">

<p>So… <em>where &amp; when &amp; how</em> does this thinking happen?</p>
<p>It is clearly not happening in the conscious mind. It is also involuntary: you have no idea some arcane random topic is bubbling up in your mind until it does, and then it is too late.</p>
<p>And it is a universal phenomenon: they can happen spontaneously on seemingly any topic you have learned about. It seems difficult to exhaust—after a lifetime, I still have the same rate, and few people report ever having <em>no</em> such thoughts (except perhaps after highly unusual experiences like psychedelics or meditative enlightenment).</p>
<p>It is also probably expensive, given the cost of the brain and the implication that nontrivial thought goes into each connection. It is hard to tell, but my guess is that almost all animals do not have ‘eureka!’ moments. We can further guess that it is probably parallelizable, because the connections are between such ‘distant’ pairs of concepts that it is hard to imagine that the brain has a very large prior on them being related and is only doing a handful of serial computations in between each ‘hit’; they are probably extremely unlikely to be related, hence, many of them are being done, hence, they are being done in parallel to fit into a human lifetime.</p>
<p>It is presumably only <em>partially</em> related to the <a href="https://en.wikipedia.org/wiki/Hippocampal_replay" id="_goT_pk4B" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Hippocampal_replay#bodyContent" title="Hippocampal replay">experience replay done by the hippocampus</a> during sleep, because that is for long-term memory while we have these thoughts about things in <a href="https://en.wikipedia.org/wiki/Working_memory" id="_1r4qYs-3" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Working_memory#bodyContent" title="Working memory">Working memory</a> or <a href="https://en.wikipedia.org/wiki/Short-term_memory" id="_JYYuNy2J" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Short-term_memory#bodyContent" title="Short-term memory">short-term memory</a> (eg about things during the day, before any sleep); there may well be connections, but they are not the same thing. And it is likely related to the <a href="https://en.wikipedia.org/wiki/Default_mode_network" id="_dS7d4FEa" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Default_mode_network#bodyContent" title="Default mode network">default mode network</a>, which activates when we are not thinking anything explicitly, because that is strongly associated with <a href="https://en.wikipedia.org/wiki/Daydreaming" id="_RAO4JNsz" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Daydreaming#bodyContent" title="Daydreaming">daydreaming</a> or ‘woolgathering’ or ‘zoning out’, which is when such thoughts are especially likely to erupt. (The default mode network is especially surprising because there is no reason to expect the human brain to have such a thing, rather than go quiescent, and indeed, it took a long time for neuroscientists to accept its existence. And there is little evidence for a default mode network outside <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9088817/" id="garin-et-al-2022" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="‘An evolutionary gap in primate default mode network organization’, Garin et al 2022">primates</a> and possibly some mammals like <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3309754/" id="lu-et-al-2012" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="‘Rat brains also have a default mode network’, Lu et al 2012">rats</a>.)</p>
<p>It further appears to be ‘crowded out’ and probably <em>not</em> happening when doing ‘focused’ learning or thinking: in my personal observation, when I have been intensively doing something (whether reading research, writing, coding, or anything else novel &amp; intellectually demanding), the thoughts stop happening… but if I take a break, they may suddenly surge, as if there was a dam holding them back or my brain is making up for lost time.</p>
<p>So where is it?</p>
<section id="day-dreaming-loop">
<h2><a href="#day-dreaming-loop" title="Link to section: § &#39;Day-Dreaming Loop&#39;">Day-Dreaming Loop</a></h2>
<p>I don’t know.</p>
<p>But to illustrate what I think answers here <em>look like</em>, here is an example of an answer, which satisfies our criteria, and is loosely inspired by <a href="https://en.wikipedia.org/wiki/Wake-sleep_algorithms" id="_3sTX22nO" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Wake-sleep_algorithms#bodyContent" title="Wake-sleep algorithms">wake-sleep algorithms</a> &amp; default mode network, and is not obviously wrong.</p>
<p>Let’s call this <strong>Day-dreaming loop (DDL)</strong>: The brain is doing combinatorial search over its store of facts &amp; skills. This is useful for sample efficiency by replaying old memories to extract new knowledge from them, or to do implicit planning (eg to patch up flaws in temporally-extended tasks, like a whole human lifetime). DDL does this in a simple way: it retrieves 2 random facts, ‘thinks about them’, and if the result is ‘interesting’, it is promoted to consciousness and possibly added to the store / trained on. (It is not obvious how important it is to do higher-order combinations of <em>k</em> &gt; 2, because as long as useful combinations keep getting added, the higher-order combinations become implicitly encoded: as long as 1 of the possible 3 pairs gets stored as a new combination, then the other can be retrieved and combined afterwards. Higher-order combinations where all members are uninteresting in any lower-order combos may be too sparse to be worth caring about.) DDL happens in the background when the brain is otherwise unoccupied, for one’s entire lifetime. So an example like <a href="#kohls">the Kohl’s example</a> would have happened like ‘retrieve 2 loosely semantic-net-related concepts; think about just those two; is the result interesting? yes, because there’s a pun about an unexpected connection between the two. Promoted!’</p>
<p>We can elaborate on DDL in various ways, like training on both interesting and uninteresting results, labeled as such, and then try to sample ‘interesting’-prefixed; or try to come up with a more efficient way of doing sampling (sampling-without-replacement? <a href="https://en.wikipedia.org/wiki/Reservoir_sampling" id="_hrlzUOeO" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Reservoir_sampling#bodyContent" title="Reservoir sampling">reservoir sampling</a>? <a href="https://en.wikipedia.org/wiki/Importance_sampling" id="_KdpKg5YT" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Importance_sampling#bodyContent" title="Importance sampling">importance sampling</a> approaches? <a href="https://gwern.net/note/statistic#program-for-non-spaced-repetition-review-of-past-written-materials-for-serendipity-rediscovery-archive-revisiter" id="gwern-note-statistic--program-for-non-spaced-repetition-review-of-past-written-materials-for-serendipity-rediscovery-archive-revisiter" data-filesize-bytes="456894" data-filesize-percentage="88" title="‘Statistical Notes § Program for Non-Spaced-Repetition Review of past Written Materials for Serendipity &amp; Rediscovery: Archive Revisiter’, Gwern 2014">anti-spaced repetition</a>?), or fiddling with the verification step (do they need to be passed to oracles for review before being saved, because this search process is dangerously self-adversarial?).</p>
<p>But that’s unnecessary, as DDL already satisfies all the criteria, and so worth discussing:</p>
<p>It is plausible from a RL perspective that such a bootstrap can work, because we are exploiting the <a href="https://gwern.net/doc/www/arxiv.org/4fb990ca46a83db6f82c62e81d85915687ecbee6.pdf" id="swamy-et-al-2025" data-link-icon="𝛘" data-link-icon-type="text" data-link-icon-color="#b31b1b" data-href-mobile="https://arxiv.org/html/2503.01067?fallback=original" data-url-archive="/doc/www/arxiv.org/4fb990ca46a83db6f82c62e81d85915687ecbee6.pdf" data-url-original="https://arxiv.org/abs/2503.01067" data-filesize-bytes="326761" data-filesize-percentage="23" title="‘All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning’, Swamy et al 2025">generator-verifier gap</a>, where it is easier to <em>discriminate</em> than to <em>generate</em> (eg laughing at a pun is easier than making it). It is entirely unconscious. Since it is lightweight, it can happen in parallel, in independent modalities/tasks (eg verbal replay can happen separate from episodic memory replay). And by the nature of <a href="https://en.wikipedia.org/wiki/Genetic_recombination" id="_7n1UEsMR" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genetic_recombination#bodyContent" title="Genetic recombination">recombination</a>, it is difficult to ‘exhaust’ this process because every ‘hit’ which gets added to the store will add many new combinations—surprisingly, in <a href="https://gwern.net/doc/www/www.newthingsunderthesun.com/630276e56e5691cf2398a29ea7fde6a88a5bc9b0.html" id="_wX9HaksZ" data-url-archive="/doc/www/www.newthingsunderthesun.com/630276e56e5691cf2398a29ea7fde6a88a5bc9b0.html" data-url-original="https://www.newthingsunderthesun.com/pub/8f77puuw/release/13" data-filesize-bytes="2823610" data-filesize-percentage="70" title="Combinatorial innovation and technological progress in the very long run">a statistical toy model of economic innovation</a>, economist <a href="https://gwern.net/doc/www/www.nber.org/fe331ec57adcf0e10e14cc01edc047aa79dfd7c8.pdf" id="jones-2021-combinatorial" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.nber.org/fe331ec57adcf0e10e14cc01edc047aa79dfd7c8.pdf" data-url-original="https://www.nber.org/system/files/working_papers/w28340/w28340.pdf" data-filesize-bytes="271758" data-filesize-percentage="20" title="Recipes and Economic Growth: A Combinatorial March Down an Exponential Tail">Charles I. <span><span>Jones</span><span>2021</span></span></a> shows that even though we pick the low-hanging fruit first, we can still see a constant stream of innovation (or even an explosion of innovation). It is, however, highly expensive, because almost all combinations are useless. And it is difficult to optimize this too much because by the nature of online learning and the passage of time, the brain will change, and even if a pair has been checked before and was uninteresting, that might change at any time, and so it can be useful to <em>re</em>check.</p>
<section id="llm-analogy">
<h3><a href="#llm-analogy" title="Link to section: § &#39;LLM Analogy&#39;">LLM Analogy</a></h3>
<p>Clearly, a LLM does nothing at all like this normally, nor does any LLM system do this. They are called with a specific prompt to do a task, and they do it. They do not simply sample random facts and speculatively roll out some inner-monologues about the facts to see if they can think of anything ‘interesting’.</p>
<p>But it wouldn’t be hard to do my proposed algorithm. For example, retrieval of random sets of datapoints from a vector database, then roll out a “brainstorm” prompt, then a judgment. Hypothetical prompts:</p>
<pre><code>[SYSTEM]
You are a creative synthesizer. Your task is to find deep, non-obvious,
and potentially groundbreaking connections between the two following concepts.
Do not state the obvious. Generate a hypothesis, a novel analogy,
a potential research question, or a creative synthesis.
Be speculative but ground your reasoning.

Concept 1: {Chunk A}
Concept 2: {Chunk B}

Think step-by-step to explore potential connections:

#. Are these concepts analogous in some abstract way?
#. Could one concept be a metaphor for the other?
#. Do they represent a similar problem or solution in different domains?
#. Could they be combined to create a new idea or solve a problem?
#. What revealing contradiction or tension exists between them?

Synthesize your most interesting finding below.
[ASSISTANT]

...

[SYSTEM]
You are a discerning critic. Evaluate the following hypothesis
on a scale of 1--10 for each of the following criteria:

- **Novelty:** Is this idea surprising and non-obvious? (1=obvious, 10=paradigm-shifting)
- **Coherence:** Is the reasoning logical and well-formed? (1=nonsense, 10=rigorous)
- **Usefulness:** Could this idea lead to a testable hypothesis, a new product,
  or a solution to a problem? (1=useless, 10=highly applicable)

Hypothesis: {Synthesizer Output}

Provide your scores and a brief justification.
[ASSISTANT]</code></pre>
</section>
</section>
</section>
<section id="obstacles-and-open-questions">

<p>…Just expensive. We could ballpark it as &lt;20:1 based on the human example, as an upper bound, which would have severe implications for LLM-based research—a good LLM solution might be 2 OOMs more expensive than the LLM itself per task.</p>
<p><span>Cheap, good, fast: pick 2.</span> So LLMs may gain a lot of their economic efficiency over humans by making a severe tradeoff, in avoiding generating novelty or being long-duration agents. And if this is the case, few users will want to pay 20× more for their LLM uses, just because once in a while there may be a novel insight.</p>
<p>This will be especially true if there is no way to narrow down the retrieved facts to ‘just’ the user-relevant ones to save compute; it may be that the most far-flung and low-prior connections <em>are</em> the important ones, and so there is no easy way to improve, no matter how annoyed the user is at receiving random puns or interesting facts about the <a href="https://history.howstuffworks.com/world-history/cia-vampires-communist-rebels-philippines.htm#page-wrap4" id="roos-2023" title="‘How the CIA Used ‘Vampires’ to Fight Communism in the Philippines § Blood-sucking CIA Agents’, Roos 2023">CIA faking vampire attacks</a>.</p>
</section>
<section id="implications">

<p>Only power-users, researchers, or autonomous agents will want to pay the ‘daydreaming tax’ (either in the form of higher upfront capital cost of training, or in paying for online daydreaming to specialize to the current problem for the asymptotic scaling improvements, see AI researcher <a href="https://gwern.net/doc/www/arxiv.org/dc5e847727ef4250dc9db1b8d854c3d3528ea2cb.pdf" id="jones-2021-2" data-link-icon="𝛘" data-link-icon-type="text" data-link-icon-color="#b31b1b" data-href-mobile="https://arxiv.org/html/2104.03113?fallback=original" data-url-archive="/doc/www/arxiv.org/dc5e847727ef4250dc9db1b8d854c3d3528ea2cb.pdf" data-url-original="https://arxiv.org/abs/2104.03113" data-filesize-bytes="353523" data-filesize-percentage="24" title="‘Scaling Scaling Laws with Board Games’, Jones 2021">Andy <span><span>Jones</span><span>2021</span></span></a>).</p>
<p><span>Data moat.</span> So this might become a major form of RL scaling, with billions of dollars of compute going into ‘daydreaming AIs’, to avoid the “data wall” and <a href="https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety?commentId=MPNF8uSsi9mvZLxqz" id="_uHr0JEEN" data-link-icon="LW" data-link-icon-type="text" data-link-icon-color="#7faf83" data-url-iframe="https://www.greaterwrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety/comment/MPNF8uSsi9mvZLxqz?format=preview&amp;theme=classic">create proprietary training data</a> for the next generation of small cheap LLMs. (And it is those which are served directly to most paying users, with the most expensive tiers reserved for the most valuable purposes, like R&amp;D.) These daydreams serve as an interesting moat against naive data distillation from API transcripts and cheap cloning of frontier models—that kind of distillation works only for things that you know to ask about, but the point here is that you don’t know what to ask about. (And if you did, it wouldn’t be important to use any API, either.)</p>
<p>Given RL <a href="https://en.wikipedia.org/wiki/Neural_scaling_law" id="_pcG7Y5Oo" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Neural_scaling_law#bodyContent" title="Neural scaling law">scaling laws</a> and rising capital investments, it may be that LLMs will need to become slow &amp; expensive so they can be fast &amp; cheap.</p>
</section>

      
      
      
      
      
      
      </div></div>
  </body>
</html>
