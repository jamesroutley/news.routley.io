<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.meta.com/blog/meta-fair-updates-agents-robustness-safety-architecture/?_fb_noscript=1">Original</a>
    <h1>Sharing new research, models, and datasets from Meta FAIR</h1>
    
    <div id="readability-page-1" class="page"><div><p>Takeaways</p><div><ul><li>Today, Meta FAIR is releasing several new research artifacts that highlight our recent innovations in developing agents, robustness and safety, and architectures that facilitate machine learning.</li><li>The work we’re sharing advances our goal of achieving advanced machine intelligence and includes Meta Motivo, a foundation model for controlling the behavior of virtual embodied agents, and Meta Video Seal, an open source model for video watermarking.</li><li>We aim to democratize access to state-of-the-art technologies that transform our interaction with the physical world, which is why we&#39;re committed to fostering a collaborative and open ecosystem that accelerates progress and discovery.</li></ul></div><div><p>As we continue to work towards our goal of achieving advanced machine intelligence, we want to share our progress with the research community so they can build upon our work. Today, we’re excited to release some of the latest research, code, models, and datasets from Meta Fundamental AI Research (FAIR). The artifacts we’re sharing today focus on building more capable agents, robustness and safety, and architecture innovations that enable models to learn new information more effectively and scale beyond current limits.</p><p>In this release, we’re sharing a demo and code for Meta Video Seal, an open source model work video watermarking that builds on the popular <a href="https://github.com/facebookresearch/audioseal" target="_blank" data-lnfb-mode="ie"><u>Meta Audio Seal</u></a> work we shared last year. We’re also sharing a variety of other artifacts, including a foundation model for controlling the behavior of virtual embodied agents, a method for scaling memory layers that will enable more factual information, and code to help models become more socially intelligent. There’s plenty more to explore in this post with nine total projectsand artifacts ready for people to download and start using today.</p><p>This work supports our long and proven track record of sharing open reproducible science with the community. By publicly sharing our early research work, we hope to inspire iterations and ultimately help advance AI in a responsible way. As always, we look forward to seeing what the community will build using these new releases and continuing the dialogue about how we can all advance AI together responsibly and build for the greater good.</p></div><p>Meta Motivo</p><div><p>Unsupervised reinforcement learning involves pre-training models to solve a wide range of downstream tasks in complex environments. Most methods require highly curated interaction datasets and often rely on unsupervised losses that lead to policies that may not align well with target tasks. Today, we’re sharing Meta Motivo, a first-of-its-kind behavioral foundation model that controls the movements of a virtual embodied humanoid agent to perform complex tasks.</p><p>Meta Motivo is trained with a novel algorithm that leverages an unlabeled dataset of motions to ground unsupervised reinforcement learning towards learning human-like behaviors while retaining zero-shot inference capabilities. The key technical novelty of our algorithm is to learn a representation that can be used to embed states, motions, and rewards into the same latent space. As a result, Meta Motivo is able to solve a wide range of whole-body control tasks, including motion tracking, goal pose reaching, and reward optimization, without any additional training or planning.</p><br/></div><div data-testid="responsive-video" id="u_0_5_hw"><video src="https://video-iad3-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQOXF-Jw-1A8GpIrM7nhQUr0qguZFteEwejY1V0JFpcZYINsKW03TG3NORxBOk8omSYPdMb_MdCyvwOJAS0au9C4.mp4?efg=eyJ4cHZfYXNzZXRfaWQiOjIzMjMxNTc1NDgwNjMzODQsInZlbmNvZGVfdGFnIjoieHB2X3Byb2dyZXNzaXZlLkZBQ0VCT09LLi5DMy4xOTIwLmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwIn0&amp;_nc_ht=video-iad3-1.xx.fbcdn.net&amp;_nc_cat=100&amp;strext=1&amp;vs=80d525cd85e14bfa&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HSERCQWh3RnUyX0JtQ0FFQUtQRHlSandlaVYwYnY0R0FBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dQX3cteHQ2QTM3RzAxSURBRVJMZENYZU9GY05idjRHQUFBRhUCAsgBACgAGAAbAogHdXNlX29pbAExEnByb2dyZXNzaXZlX3JlY2lwZQExFQAAJrDqxsbIuaAIFQIoAkMzLBdAUbKfvnbItBgaZGFzaF9oMjY0LWJhc2ljLWdlbjJfMTA4MHARAHUCAA&amp;ccb=9-4&amp;oh=00_AYB5-aMPTzx5UYkAAcf4HWuF0XcGP2ezYDgeAB9NWqSZ-Q&amp;oe=675FDB8F&amp;_nc_sid=1d576d" autoplay="1" controls="1" controlslist="nodownload" height="1080" loop="1" muted="1" poster="https://scontent-iad3-1.xx.fbcdn.net/v/t15.5256-10/469954727_562619859841797_7007887738015205070_n.jpg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=282d23&amp;_nc_ohc=iZlIvw95UPkQ7kNvgELQt5Y&amp;_nc_zt=23&amp;_nc_ht=scontent-iad3-1.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYD0f14kZS5KMZ6VlDqXh7VbdzgQ8DyWP_f8BTWzHG7D_A&amp;oe=6763A848" width="1920" id="u_0_7_Vu"></video></div></div><div><div><p>Meta Motivo achieves competitive performance compared to task-specific methods and outperforms state-of-the-art unsupervised reinforcement learning and model-based baselines, while exhibiting more human-like behaviors. The model also displays a surprising level of robustness to changes in the environment, such as gravity, wind, or direct perturbations, despite not being trained for them.</p><p>In the future, we believe this research could pave the way for fully embodied agents in the Metaverse, leading to more lifelike NPCs, democratization of character animation, and new types of immersive experiences.</p><p><a href="https://ai.meta.com/research/publications/zero-shot-whole-body-humanoid-control-via-behavioral-foundation-models/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://metamotivo.metademolab.com/" target="_blank" data-lnfb-mode="ie"><u>Try the demo</u></a></p><p><a href="https://github.com/facebookresearch/metamotivo" target="_blank" data-lnfb-mode="ie"><u>Download the code and model</u></a></p></div></div><div><p>Meta Video Seal</p><p>While AI tools can help bring the world closer together, it’s important that we implement safeguards to mitigate the risks of imitation, manipulation, and other forms of misuse that can undermine their benefits. Post-hoc watermarking is a crucial step towards better traceability for content and AI models.</p><div data-testid="responsive-video" id="u_0_8_ET"><video src="https://video-iad3-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQN3JToN3vyl8_ZEV1Y1oiXXpFLzPjq-P00pK5gmNxdi3yWVuRtSDmtRAI8TNyxBs6I3q1wrsgwaSB-IWsTrQuPB.mp4?efg=eyJ4cHZfYXNzZXRfaWQiOjkyOTkwNjY0NTk1MDUwNiwidmVuY29kZV90YWciOiJ4cHZfcHJvZ3Jlc3NpdmUuRkFDRUJPT0suLkMzLjE5MjAuZGFzaF9oMjY0LWJhc2ljLWdlbjJfMTA4MHAifQ&amp;_nc_ht=video-iad3-1.xx.fbcdn.net&amp;_nc_cat=104&amp;strext=1&amp;vs=46ea31c3399f820b&amp;_nc_vs=HBksFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTUVxLWh2ZWlKcXdtaE1EQVB5WUxZSV84bnhtYnY0R0FBQUYVAALIAQAVAhg6cGFzc3Rocm91Z2hfZXZlcnN0b3JlL0dKQmMtaHU2VDBoeklKRUVBRFdvTU9kYXZ1VnVidjRHQUFBRhUCAsgBACgAGAAbAogHdXNlX29pbAExEnByb2dyZXNzaXZlX3JlY2lwZQExFQAAJtSA87TY76YDFQIoAkMzLBdAQSU_fO2RaBgaZGFzaF9oMjY0LWJhc2ljLWdlbjJfMTA4MHARAHUCAA&amp;ccb=9-4&amp;oh=00_AYCgXJlqxtXIei-07ANZTjxq8Gjz9m-l8jm7svSWJeIeEQ&amp;oe=675FBED3&amp;_nc_sid=1d576d" autoplay="1" controls="1" controlslist="nodownload" height="1080" loop="1" muted="1" poster="https://scontent-iad3-2.xx.fbcdn.net/v/t15.5256-10/470000398_1234155357811747_8659696186212707013_n.jpg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=282d23&amp;_nc_ohc=mIxxtxnznGkQ7kNvgHK_fWW&amp;_nc_zt=23&amp;_nc_ht=scontent-iad3-2.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYAlwI3rZ7Xmu1ovwFJX6WBqVqk8o90pfvzc36fn3O4UAg&amp;oe=6763C124" width="1920" id="u_0_a_7R"></video></div><div data-testid="responsive-video" id="u_0_b_RW"><video src="https://video-iad3-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQOzJ7MnoJlEMkIU2qiWClLjKryQS9sHpqy4BHZnVgIjC9jgBrQ_VOHKOFtFUBURZPtlNlK_GZVog4x5t80GR9RS.mp4?efg=eyJ4cHZfYXNzZXRfaWQiOjEwODk4NDI3MTk1MDA5OTAsInZlbmNvZGVfdGFnIjoieHB2X3Byb2dyZXNzaXZlLkZBQ0VCT09LLi5DMy4xOTIwLmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwIn0&amp;_nc_ht=video-iad3-1.xx.fbcdn.net&amp;_nc_cat=103&amp;strext=1&amp;vs=79a91b6e7ff08d8f&amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTkJYLWhzYkkzM3hCS2tFQUgwQU9EUjVZM2RZYnY0R0FBQUYVAALIAQAoABgAGwKIB3VzZV9vaWwBMRJwcm9ncmVzc2l2ZV9yZWNpcGUBMRUAACb8qsGCmc3vAxUCKAJDMywXQC8Q5WBBiTcYGmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwEQB1AgA&amp;ccb=9-4&amp;oh=00_AYA4a0StA8qKbXGtfK93y39Q31-Fi3V7Fy3zmIfWJzbdag&amp;oe=675FB2F2&amp;_nc_sid=1d576d" autoplay="1" controlslist="nodownload" height="1080" loop="1" muted="1" poster="https://scontent-iad3-2.xx.fbcdn.net/v/t15.5256-10/469914590_2083372392125552_7809128774711154767_n.jpg?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=282d23&amp;_nc_ohc=pvcGw-_cs_sQ7kNvgF--Iv7&amp;_nc_zt=23&amp;_nc_ht=scontent-iad3-2.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYCFBaKqlya4zz2rNqyKpGwLKAYVOA13EbCp55KQ13L8Bg&amp;oe=6763B786" width="1920" id="u_0_d_PG"></video></div><div><p>Along with Video Seal, we’re also releasing Meta Omni Seal Bench, a leaderboard dedicated to neural watermarking covering several modalities, enabling the research community to easily test and add their own work in the field. We’re also re-releasing our Meta Watermark Anything model under a permissive license and will organize <a href="https://sites.google.com/view/genai-watermark" target="_blank" data-lnfb-mode="ie"><u>a workshop on watermarking at ICLR in 2025.</u></a></p><p>This research is a testimony to our commitment to responsible AI. We hope that other researchers and developers will join our efforts by integrating watermarking capabilities when building generative AI models. Watermark Anything, Video Seal, and Audio Seal—our previous work on post-hoc audio watermarking—are now all available for download and ready to be integrated.</p><p><a href="https://ai.meta.com/research/publications/video-seal-open-and-efficient-video-watermarking/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://aidemos.meta.com/videoseal" target="_blank" data-lnfb-mode="ie"><u>Try the demo</u></a></p><p><a href="https://github.com/facebookresearch/videoseal" target="_blank" data-lnfb-mode="ie"><u>Download the Video Seal code and model </u></a></p><p><a href="https://github.com/facebookresearch/watermark-anything" target="_blank" data-lnfb-mode="ie"><u>Download the Watermark Anything code and model</u></a></p><p><a href="https://github.com/facebookresearch/omnisealbench" target="_blank" data-lnfb-mode="ie"><u>View the Omni Seal Bench leaderboard</u></a></p></div><p>Flow Matching guide and codebase, a Meta FAIR release</p><div><p>Flow Matching is a state-of-the-art generative paradigm for many modalities including generation of images, videos, audio, music, 3D structures like proteins, and more. Our method has already replaced classical diffusion in many generative applications at Meta, including Meta <a href="https://ai.meta.com/blog/movie-gen-media-foundation-models-generative-ai-video/" target="_blank" data-lnfb-mode="ie"><u>Movie Gen</u></a>, Meta <a href="https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts/" target="_blank" data-lnfb-mode="ie"><u>Audiobox</u></a>, and Meta <a href="https://voicebox.metademolab.com/" target="_blank" data-lnfb-mode="ie"><u>Melody Flow</u></a>, and across the industry in works such as Stable-Diffusion-3, Flux, Fold-Flow, and Physical Intelligence Pi_0. Flow Matching provides a simple yet flexible generative AI framework, improving performance and efficiency while allowing easy generalization to complex data. Today, we’re sharing a paper and code, including core implementations of both continuous and discrete Flow Matching, alongside state-of-the-art training scripts to enable the research community to easily use and iterate on the Flow Matching method. By publicly sharing this work, we hope to inspire wider adoption of Flow Matching and enable people to use it in their own generative projects.</p><p><a href="https://ai.meta.com/research/publications/flow-matching-guide-and-code/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/flow_matching" target="_blank" data-lnfb-mode="ie"><u>Download the code</u></a></p></div><p>Meta Explore Theory-of-Mind</p><p>A key aspect of our social intelligence enables us to reason about the thoughts and beliefs of other agents, both human and artificial. Existing Theory-of-Mind (ToM) datasets have limitations, focusing solely on evaluation and depicting only a narrow range of interactions. To address this and move closer to achieving advanced machine intelligence, we introduce Meta Explore Theory-of-Mind, a program-guided adversarial data generation for theory of mind reasoning. Our novel framework enables the generation of diverse, challenging, and scalable ToM reasoning data for both training and evaluation, which will help accelerate progress in this critical area of research.</p><p><img src="https://scontent-iad3-2.xx.fbcdn.net/v/t39.8562-6/470090194_885853727093453_4651375064551711472_n.jpg?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=f537c7&amp;_nc_ohc=-zbNMSDbk6MQ7kNvgFZ-Q5J&amp;_nc_zt=14&amp;_nc_ht=scontent-iad3-2.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYD1-OrkqPMyoFo7KsWhUaUvjeuWMMWZLaxN8IlKyWY6Fg&amp;oe=6763A734" alt="" id="u_0_e_+O"/></p><div><p>Explore Theory-of-Mind generates robust and reliable stories that push the limits of large language models (LLMs), making it ideal for evaluating frontier models or fine-tuning data, resulting in significant improvements on classic theory of mind benchmarks. Our first-of-its-kind approach led to a 27-point accuracy improvement on the commonly used ToMi benchmark when fine-tuning a Llama-3.1 7B model, which means unprecedented accuracy in evaluating the theory of mind training data. Explore Theory-of-Mind can be used to generate datasets for improving LLMs, enhance goal-oriented scenarios, and collect interaction datasets, while also serving as a benchmark for evaluating LLM performance.</p><p><a href="https://ai.meta.com/research/publications/explore-theory-of-mind-program-guided-adversarial-data-generation-for-theory-of-mind-reasoning/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/ExploreToM" target="_blank" data-lnfb-mode="ie"><u>Download the code</u></a></p><p><a href="https://huggingface.co/datasets/facebook/ExploreToM" target="_blank" data-lnfb-mode="ie"><u>Download the dataset</u></a></p></div><p>Meta Large Concept Models</p><div><p>As we work toward advanced machine intelligence, models will need to be able to reason across languages and modalities and to excel at long-form generational capabilities that require explicit hierarchical thinking, such as writing an essay. Current mainstream language modeling approaches typically operate at the token level and don’t explicitly reason in a hierarchical manner.</p><p>Today, we’re introducing a fundamentally different training paradigm for language modeling: the Large Concept Model (LCM). The core idea of the LCM is to decouple reasoning from language representation, and it’s inspired by how humans can plan high-level thoughts to communicate. For example, when giving a presentation multiple times, a presenter always has the same series of ideas they want to convey (materialized by their slides projected on screen), but their exact choice of words might vary from one run to the other.</p></div><p><img src="https://scontent-iad3-1.xx.fbcdn.net/v/t39.8562-6/469933570_3803908129873587_2022361715417707411_n.png?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=f537c7&amp;_nc_ohc=8_KFyIUNTkkQ7kNvgHeTsnf&amp;_nc_zt=14&amp;_nc_ht=scontent-iad3-1.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYCn7Fl2R3B373J2cf-OJGSnlsd-38dNqmCHx0fJIoiUjQ&amp;oe=6763B8F7" alt="" id="u_0_f_Gr"/></p><div><p>Guided by that principle, the LCM is a significant departure from a typical LLM. Rather than predicting the next token, the LCM is trained to predict the next concept or high-level idea, represented by a full sentence in a multimodal and multilingual embedding space. Our work explores how predictions can be made for text in such a continuous space. Overall, the LCM outperforms or matches recent LLMs in the pure generative task of summarization, offers strong zero-shot generalization to unseen languages, and is more computationally efficient as input context grows. We hope the research community uses this work to improve language models that can operate on any modality or language, in an explicit hierarchical manner.</p><p><a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/large_concept_model" target="_blank" data-lnfb-mode="ie"><u>Download the code </u></a></p></div><p>Meta Dynamic Byte Latent Transformer</p><p>Language models assume text has been tokenized in a heuristic preprocessing step, breaking words into smaller units that are easier to process. This limits end-to-end learning, is difficult to optimize in practice, and can hurt performance on rare text sequences. To address this, we’re introducing Dynamic Byte Latent Transformer, a hierarchical byte-level (tokenizer-free) model with dynamic patching schemes that are able to operate over bytes—without any tokenization heuristics—while also improving efficiency for long sequences during training and inference.</p><div data-testid="responsive-video" id="u_0_g_8Z"><video src="https://video-iad3-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQM0dYYqLwWzxrKWK7BYWIo4Cp1hQ4vRoCIzfjc39Rb_GYN5F6I4PslYUdWaiMG1NaVi0wlTgNVq-YwPF3I82q9S.mp4?efg=eyJ4cHZfYXNzZXRfaWQiOjEyNjMzNTE0MjgyNzE1MDksInZlbmNvZGVfdGFnIjoieHB2X3Byb2dyZXNzaXZlLkZBQ0VCT09LLi5DMy4xNjQyLmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwIn0&amp;_nc_ht=video-iad3-1.xx.fbcdn.net&amp;_nc_cat=102&amp;strext=1&amp;vs=b352304e533a15ee&amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HQ2VMQXh5RFI3MFdIOVFEQUNwanRQYllfaklJYnY0R0FBQUYVAALIAQAoABgAGwKIB3VzZV9vaWwBMRJwcm9ncmVzc2l2ZV9yZWNpcGUBMRUAACaqtt7R3cC-BBUCKAJDMywXQCgQ5WBBiTcYGmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwEQB1AgA&amp;ccb=9-4&amp;oh=00_AYCFn9XgrLcJHrEjHkZX7LjjFI9rRHeMJuZqKCFwnRojUA&amp;oe=675FB08D&amp;_nc_sid=1d576d" autoplay="1" muted="1" loop="1" poster="https://scontent-iad3-2.xx.fbcdn.net/v/t15.5256-10/469959358_909346194712466_8084758572037598414_n.jpg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=282d23&amp;_nc_ohc=9OG9luvfQwMQ7kNvgEMwJNi&amp;_nc_zt=23&amp;_nc_ht=scontent-iad3-2.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYApk-rP365mjjPs2u77wlJafdn_JmMZTZDG81o0CDVVcw&amp;oe=6763A0CA" width="1920" height="1262" controlslist="nodownload" data-ms="{&#34;group_id&#34;:8667050176757396,&#34;video_id&#34;:1412289043490883,&#34;video_title&#34;:&#34;Aryanne Burrell&#34;}" id="u_0_i_TT"></video></div><div><p>Dynamic Byte Latent Transformer outperforms tokenizer-based models across the board in terms of robustness, with a seven point advantage on average, and excels at processing longtail and rare sequences of unseen symbols. By sharing this work, we hope to accelerate advancements that will enable us to better reason over a variety of domains that are important to advanced machine intelligence, including low resource languages, coding, and factuality.</p><p><a href="https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/blt" target="_blank" data-lnfb-mode="ie"><u>Download the code </u></a></p></div><p>Meta Memory Layers</p><div><p>Parametric memory, the repository of factual information stored in the weights on a neural network during pretraining, enables LLMs to understand complex concepts and linguistic nuances. As current scaling methods approach their limit of efficient scaling, new architectures that enable models to learn information more effectively must be explored. Today, we’re sharing a research paper and code for Meta Memory Layers at Scale, a method for scaling memory layers that enables an increase in factuality against commonly used benchmarks as we work toward achieving advanced machine intelligence.</p><p>Memory Layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Sparsely activated memory layers complement the compute-heavy nature of dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as MoE models when matched for both compute and parameters.</p><p>Contrary to the prevailing perception in the field that sparse memory architectures cannot be scaled competitively, we demonstrate efficient scaling of sparse memory layers up to 128 billion parameters and 8B base models, with significant improvements at comparable compute across the board for commonly used factuality benchmarks.</p><p><a href="https://ai.meta.com/research/publications/memory-layers-at-scale/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/memory" target="_blank" data-lnfb-mode="ie"><u>Download the code </u></a></p></div><p>Meta Image Diversity Modeling</p><div><p>This year, FAIR has focused on research to better understand and develop new methods for the safe development of image generation models. Today, we’re announcing updates on this research and releasing a comprehensive evaluation toolbox for text-to-image generative models. The image generation model we’ve developed through the course of this research builds on our prior research on generative models’ <a href="https://arxiv.org/abs/2411.03177" target="_blank" data-lnfb-mode="ie"><u>architectures</u></a> and <a href="https://arxiv.org/abs/2411.04873" target="_blank" data-lnfb-mode="ie"><u>losses</u></a> and prioritizes generating images that are representative of the physical world while maintaining competitive image quality with state-of-the-art models.</p><p>To further the research into new methods and techniques for responsible development, we’re collaborating with external experts, whom we’re inviting to use our model to carry out research in areas that can help us to improve the safety and responsibility across image diversity modeling. This initiative highlights our commitment to collaborating with the wider AI research community to collectively advance AI responsibility.</p><p>Additionally, we will be open sourcing a comprehensive evaluation toolbox for text-to-image generative models to improve the ease and reproducibility of image generation benchmarking while promoting interpretable takeaways that inform future responsible text-to-image research.</p><p>Through our continued work, we hope to better understand and offer new methods for responsible development of image generative models that can be adopted by the broader research community.</p><p><a href="https://ai.meta.com/research/publications/evalgim-a-library-for-evaluating-generative-image-models/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://github.com/facebookresearch/EvalGIM/tree/main" target="_blank" data-lnfb-mode="ie"><u>Download the code</u></a></p></div><p>Meta CLIP 1.2</p><p>We’re excited to release Meta CLIP 1.2, a milestone in our ongoing efforts to develop a high-performance vision-language encoder. We have been working on advanced algorithms to effectively curate and align vast amounts of image-text data, unlocking the learning of human knowledge about the world. This enables our models to learn efficiently and accurately, capturing the nuances of fine-grained mapping between image and language semantics.</p><div data-testid="responsive-video" id="u_0_j_qg"><video src="https://video-iad3-1.xx.fbcdn.net/o1/v/t2/f2/m69/AQO82CG-nPXRfpbmbzgbrm7sWDllkgJaodjVtgIMKn7oMBZkbUK6NtKyd2WolFdK7OcFtV9VMLRIiPG5q3ablnm6.mp4?efg=eyJ4cHZfYXNzZXRfaWQiOjExMDg0MDMzODc2MTA5NDAsInZlbmNvZGVfdGFnIjoieHB2X3Byb2dyZXNzaXZlLkZBQ0VCT09LLi5DMy4xMjM0LmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwIn0&amp;_nc_ht=video-iad3-1.xx.fbcdn.net&amp;_nc_cat=106&amp;strext=1&amp;vs=bb80315064d294d3&amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HRHAtQUJ3RW50VEQtc2dpQUxlUkZEN0dsU1ZaYnY0R0FBQUYVAALIAQAoABgAGwKIB3VzZV9vaWwBMRJwcm9ncmVzc2l2ZV9yZWNpcGUBMRUAACb43P3iyIX4AxUCKAJDMywXQDcIcrAgxJwYGmRhc2hfaDI2NC1iYXNpYy1nZW4yXzEwODBwEQB1AgA&amp;ccb=9-4&amp;oh=00_AYBdpq0iL_CJzKxpu71CPQn_j0hs_kaSIgQRLpd2p7Wf-w&amp;oe=675FBD45&amp;_nc_sid=1d576d" autoplay="1" muted="1" loop="1" poster="https://scontent-iad3-1.xx.fbcdn.net/v/t15.5256-10/469911901_890636659531456_3231754191365808574_n.jpg?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=282d23&amp;_nc_ohc=q70Re8bfbxkQ7kNvgFH8oP5&amp;_nc_zt=23&amp;_nc_ht=scontent-iad3-1.xx&amp;_nc_gid=AX3pOy6e7Nnh0_e7A9dNtxC&amp;oh=00_AYBnIzsFQ8MphQxKKqOJbfXgq9F_8wu1Oq5YVDhqq0c6_A&amp;oe=6763A847" width="1920" height="1678" controlslist="nodownload" data-ms="{&#34;group_id&#34;:1484430138895829,&#34;video_id&#34;:890981719849394,&#34;video_title&#34;:&#34;Aryanne Burrell&#34;}" id="u_0_l_sL"></video></div><div><p>Large-scale, high-quality, and diverse datasets are essential for building foundation models that can learn about the world. Meta CLIP is our effort towards building such datasets and foundation models. To ensure a high-quality and safe vision-language encoder <i>foundation model</i>, we’ve developed algorithms to effectively curate and align data with human knowledge from vast data pools, enabling our models to learn efficiently and cover all possibilities. We also conducted rigorous data research while applying robust integrity and privacy-protective measures.</p><p>By releasing our data algorithms, training recipes, and foundation models trained on our curated dataset, we’re providing researchers and developers with the tools they need to advance the field of vision-language understanding. These foundation models can be used as vision encoding for MLLM, multi-modal embedding for retrieval, and zero-shot classification, while serving as a starting point for research on data quality. Additionally, our algorithms and training methods can also be used to create high-quality, large-scale, CLIP-like datasets from scratch, which can help with new research or production use cases.</p><p><a href="https://ai.meta.com/research/publications/meta-clip-12/" target="_blank" data-lnfb-mode="ie"><u>Read the paper</u></a></p><p><a href="https://huggingface.co/datasets/activebus/Altogether-FT" target="_blank" data-lnfb-mode="ie"><u>Download the dataset</u></a></p><p><a href="https://github.com/facebookresearch/MetaCLIP" target="_blank" data-lnfb-mode="ie"><u>Download the code </u></a></p><p><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/h14_v1.2_altogether.pt" target="_blank" data-lnfb-mode="ie"><u>Download the model</u></a></p></div></div></div>
  </body>
</html>
