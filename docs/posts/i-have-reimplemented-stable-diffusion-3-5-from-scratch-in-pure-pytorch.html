<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/yousef-rafat/miniDiffusion">Original</a>
    <h1>I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://rrampage.github.io/yousef-rafat/miniDiffusion/blob/main/assets/display.png"><img src="https://rrampage.github.io/yousef-rafat/miniDiffusion/raw/main/assets/display.png" alt="SD3 Diagram"/></a></p>
<p dir="auto">miniDiffusion is a reimplementation of the Stable Diffusion 3.5 model in pure PyTorch with minimal dependencies. It&#39;s designed for educational, experimenting, and hacking purposes.
It&#39;s made with the mindset of having the least amount of code necessary to recreate Stable Diffusion 3.5 from scratch, with only ~2800 spanning from VAE to DiT to the Train and Dataset scripts.</p>
<p dir="auto"><strong>-Files:</strong> The main Stable Diffusion model code is located in dit.py, dit_components.py, and attention.py. The dit.py file contains the main model, dit_components.py contains the embedding, normalization, patch embedding, and help functions for the DiT code, and attention.py contains the Joint Attention implementation.
The noise.py is where the Euler Scheduler is located for solving the ODE of Rectified Flow.</p>
<p dir="auto">The text encoders are in t5_encoder.py and clip.py, and their tokenizers are both in tokenizer.py. The metrics.py implements the Fréchet inception distance (FID).</p>
<p dir="auto">The common.py is a place for helper functions for training, the common_ds.py is an implementation of an iterable dataset that converts image data to trainable data for the DiT model.</p>
<p dir="auto"><strong>-Folders:</strong> The model folder saves the model&#39;s checkpoint and logs after training. The encoders folder saves other modules&#39; checkpoints (e.g., VAE, CLIP).</p>
<blockquote>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>Warning</strong>:
This repository still has experimental features and requires more testing.</p>
</blockquote>

<div dir="auto"><h3 tabindex="-1" dir="auto">Core Image Generation Modules</h3><a id="user-content-core-image-generation-modules" aria-label="Permalink: Core Image Generation Modules" href="#core-image-generation-modules"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Implementations of VAE, CLIP, and T5 Text Encoders</li>
<li>Implementation of Byte-Pair &amp; Unigram tokenizers</li>
</ul>

<ul dir="auto">
<li>Multi-Modal Diffusion Transformer Model</li>
<li>Flow-Matching Euler Scheduler</li>
<li>Logit-Normal Sampling</li>
<li>Joint Attention</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Train and Inference Scripts For SD3</h3><a id="user-content-train-and-inference-scripts-for-sd3" aria-label="Permalink: Train and Inference Scripts For SD3" href="#train-and-inference-scripts-for-sd3"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">Get the repo</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone &#34;https://github.com/yousef-rafat/miniDiffusion&#34;"><pre>git clone <span><span>&#34;</span>https://github.com/yousef-rafat/miniDiffusion<span>&#34;</span></span></pre></div>
<p dir="auto">Install Dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Install Checkpoints for Models</p>
<ul dir="auto">
<li><em>Add a Hugging Face Token in get_checkpoints.py before running the script.</em></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 encoders/get_checkpoints.py"><pre>python3 encoders/get_checkpoints.py</pre></div>

<p dir="auto">This project is under the MIT License and is made for educational and experimental purposes.</p>
</article></div></div>
  </body>
</html>
