<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/mlecauchois/micrograd-cuda">Original</a>
    <h1>Micrograd-CUDA: adapting Karpathy&#39;s tiny autodiff engine for GPU acceleration</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mlecauchois/micrograd-cuda/blob/main/front.jpg"><img src="https://github.com/mlecauchois/micrograd-cuda/raw/main/front.jpg" alt=""/></a></p>
<p dir="auto">Teaching myself basic CUDA by building GPU-accelerated tensor-based autodiff from the ground up, inspired by <a href="https://github.com/karpathy/micrograd/tree/master">Andrej&#39;s micrograd</a>.</p>
<p dir="auto">No dependencies other than Python&#39;s standard library and CUDA.</p>

<p dir="auto">To compile the CUDA kernels:</p>
<div dir="auto" data-snippet-clipboard-copy-content="nvcc -shared -o liboperations.so micrograd_cuda/operations.cu -Xcompiler -fPIC"><pre>nvcc -shared -o liboperations.so micrograd_cuda/operations.cu -Xcompiler -fPIC</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="import random
import time

from micrograd_cuda.mlp import MLP
from micrograd_cuda.tensor import Tensor
from micrograd_cuda.operations import Operations

# Model
model = MLP(100, [100, 100, 1])
epochs = 20
device = &#34;cpu&#34;

# Data
xs_batch = Tensor([[random.choice([-1, 1]) for _ in range(100)] for _ in range(10)]).T
ys_batch = Tensor([[random.choice([-1, 1])] for _ in range(10)]).T

# Move to device
model.to(device)
xs_batch.to(device)
ys_batch.to(device)

start = time.time()

for k in range(epochs):

    # Forward pass
    ypred = model(xs_batch)
    diff = ypred - ys_batch
    loss = (diff**2).sum()

    # Backward pass
    for p in model.parameters():
        p.zero_grad()
        
    loss.backward()
    
    # Update
    for p in model.parameters():
        p.data = (-0.1 * p.grad + p).data_copy()

print(f&#34;Elapsed: {time.time() - start:.2f} sec&#34;)
    
loss.to(&#34;cpu&#34;)
print(loss.data)"><pre><span>import</span> <span>random</span>
<span>import</span> <span>time</span>

<span>from</span> <span>micrograd_cuda</span>.<span>mlp</span> <span>import</span> <span>MLP</span>
<span>from</span> <span>micrograd_cuda</span>.<span>tensor</span> <span>import</span> <span>Tensor</span>
<span>from</span> <span>micrograd_cuda</span>.<span>operations</span> <span>import</span> <span>Operations</span>

<span># Model</span>
<span>model</span> <span>=</span> <span>MLP</span>(<span>100</span>, [<span>100</span>, <span>100</span>, <span>1</span>])
<span>epochs</span> <span>=</span> <span>20</span>
<span>device</span> <span>=</span> <span>&#34;cpu&#34;</span>

<span># Data</span>
<span>xs_batch</span> <span>=</span> <span>Tensor</span>([[<span>random</span>.<span>choice</span>([<span>-</span><span>1</span>, <span>1</span>]) <span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>100</span>)] <span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>10</span>)]).<span>T</span>
<span>ys_batch</span> <span>=</span> <span>Tensor</span>([[<span>random</span>.<span>choice</span>([<span>-</span><span>1</span>, <span>1</span>])] <span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>10</span>)]).<span>T</span>

<span># Move to device</span>
<span>model</span>.<span>to</span>(<span>device</span>)
<span>xs_batch</span>.<span>to</span>(<span>device</span>)
<span>ys_batch</span>.<span>to</span>(<span>device</span>)

<span>start</span> <span>=</span> <span>time</span>.<span>time</span>()

<span>for</span> <span>k</span> <span>in</span> <span>range</span>(<span>epochs</span>):

    <span># Forward pass</span>
    <span>ypred</span> <span>=</span> <span>model</span>(<span>xs_batch</span>)
    <span>diff</span> <span>=</span> <span>ypred</span> <span>-</span> <span>ys_batch</span>
    <span>loss</span> <span>=</span> (<span>diff</span><span>**</span><span>2</span>).<span>sum</span>()

    <span># Backward pass</span>
    <span>for</span> <span>p</span> <span>in</span> <span>model</span>.<span>parameters</span>():
        <span>p</span>.<span>zero_grad</span>()
        
    <span>loss</span>.<span>backward</span>()
    
    <span># Update</span>
    <span>for</span> <span>p</span> <span>in</span> <span>model</span>.<span>parameters</span>():
        <span>p</span>.<span>data</span> <span>=</span> (<span>-</span><span>0.1</span> <span>*</span> <span>p</span>.<span>grad</span> <span>+</span> <span>p</span>).<span>data_copy</span>()

<span>print</span>(<span>f&#34;Elapsed: <span><span>{</span><span>time</span>.<span>time</span>() <span>-</span> <span>start</span>:.2f<span>}</span></span> sec&#34;</span>)
    
<span>loss</span>.<span>to</span>(<span>&#34;cpu&#34;</span>)
<span>print</span>(<span>loss</span>.<span>data</span>)</pre></div>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mlecauchois/micrograd-cuda/blob/main/speedup.jpg"><img src="https://github.com/mlecauchois/micrograd-cuda/raw/main/speedup.jpg" alt=""/></a></p>

<p dir="auto">The codebase is still WIP with some rough spots, especially around CUDA Tensor data manipulation and copying.</p>
<ul>
<li> Micrograd extension with basic 2D tensors and na√Øve matrix multiplication for MLP</li>
<li> Batching</li>
<li> CUDA kernel for matrix multiplication</li>
<li> Less verbose code</li>
<li> Error handling</li>
<li> CUDA optimizations</li>
<li> &gt;2D tensors</li>
<li> Rust</li>
</ul>



<p dir="auto">MIT</p>
</article></div></div>
  </body>
</html>
