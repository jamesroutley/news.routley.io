<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://justine.lol/mutex/">Original</a>
    <h1>The Fastest Mutexes</h1>
    
    <div id="readability-page-1" class="page">

<p>
Oct 2<sup>nd</sup>, 2024 @ <a href="https://abstract.properties/index.html">justine&#39;s web page</a>
</p>

<div>
<p><img src="https://abstract.properties/honeybadger.png" alt="[cosmpolitan honeybadger logo]" width="334" height="200"/></p></div>

<p>
<a href="https://abstract.properties/cosmo3/">Cosmopolitan Libc</a> is well-known for
its <a href="https://justine.lol/ape.html">polyglot fat binary</a> hack
that lets your executables run on six OSes for AMD64 / ARM64. What may
surprise you is that it could also be the best C library for your
production workloads too. To demonstrate this point, let&#39;s compare
Cosmo&#39;s mutex library with other platforms.

</p><p>
We&#39;ll do this by writing a simple test that spawns <strong>30
threads</strong> which increment the same integer <strong>100,000
times</strong>. This tests how well a mutex implementation performs in
the heavily contended use case. In essence, that means the following
(see the segment at the bottom of the page for the full source code).

</p><pre><span>int</span> g_chores;
<span>pthread_mutex_t</span> g_locker = PTHREAD_MUTEX_INITIALIZER;

<span>void</span> *<span>worker</span>(<span>void</span> *arg) {
  <span>for</span> (<span>int</span> i = 0; i &lt; ITERATIONS; ++i) {
    pthread_mutex_lock(&amp;g_locker);
    ++g_chores;
    pthread_mutex_unlock(&amp;g_locker);
  }
  <span>return</span> 0;
}
</pre>

<p>
Now let&#39;s start with the exciting part, which are my benchmark results.

</p><h3>Benchmarks</h3>

<p>
Times will be measured in microseconds. Wall time is how long the test
program takes to run. That includes the overhead of spawning and joining
threads. User time is how much CPU time was spent in userspace, and
system time is how much CPU time was spent in the kernel. System and
user time can exceed the actual wall time because multiple threads are
running in parallel.

</p><p>
The first results I&#39;ll show are for Windows because Mark Waterman did an
excellent <a href="https://github.com/markwaterman/MutexShootout">mutex
shootout</a> three months ago, where he said, &#34;in the highly contended
scenario, Windows wins the day with its SRWLOCK&#34;. Contention is where
mutex implementations show their inequality. Mark was so impressed by
Microsoft&#39;s SRWLOCK that he went on to recommend Linux and FreeBSD users
consider targeting Windows if mutex contention is an issue.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      Windows Mutex Implementations</p></th></tr><tr>
  <th>wall</th><th>user</th><th>system</th><th>implementation

</th></tr><tr>
  <td>148,940
  </td><td>328,125
  </td><td>62,500
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr><tr>
  <td>410,416
  </td><td>5,515,625
  </td><td>1,640,625
  </td><td>Microsoft SRWLOCK

</td></tr><tr>
  <td>949,187
  </td><td>7,937,500
  </td><td>5,078,125
  </td><td>Microsoft CRITICAL_SECTION

</td></tr><tr>
  <td>991,750
  </td><td>12,156,250
  </td><td>4,031,250
  </td><td>MSVC 2022 std::mutex

</td></tr><tr>
  <td>1,165,435
  </td><td>24,515,000
  </td><td>15,000
  </td><td>spin lock

</td></tr><tr>
  <td>9,780,803
  </td><td>1,937,000
  </td><td>6,156,000
  </td><td>Cygwin pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
As we can see, Cosmopolitan Libc mutexes go 2.75x faster than
Microsoft&#39;s SRWLOCK (which was previously believed to be the best of the
best) while consuming 18x fewer CPU resources. Cosmopolitan mutexes were
also 65x faster than Cygwin, which like Cosmopolitan provides a POSIX
implementation on Windows. Cygwin&#39;s mutexes are so bad that they would
have been better off for this use case just using a spin lock.

</p><p>
Now onto Linux, the lord of all operating systems.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      Linux Mutex Implementations</p></th></tr><tr>
  <th>wall</th><th>user</th><th>system</th><th>implementation

</th></tr><tr>
  <td>36,905
  </td><td>44,511
  </td><td>23,492
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr><tr>
  <td>101,353
  </td><td>150,706
  </td><td>2,724,851
  </td><td>glibc pthread_mutex_t

</td></tr><tr>
  <td>202,423
  </td><td>4,694,749
  </td><td>2,000
  </td><td>spin lock

</td></tr><tr>
  <td>411,013
  </td><td>2,167,898
  </td><td>9,926,850
  </td><td>Musl libc pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
  Here we see Cosmopolitan mutexes are:

</p><ul>
<li>3x faster than glibc
</li><li>11x faster than musl libc
</li><li>42x less CPU time than glibc
</li><li>178x less CPU time than musl libc
</li></ul>

<p>
Here&#39;s how things actually work in practice. Imagine you have a workload
where all your threads need to do a serialized operation. With Cosmo, if
you&#39;re looking at htop, then it&#39;s going to appear like only one core is
active, whereas glibc and musl libc will fill up your entire CPU meter.
That&#39;s bad news if you&#39;re running a lot of jobs on the same server. If
just one of your servers has a mutex bloodbath, then all your resources
are gone, unless you&#39;re using cosmo. It&#39;s still a new C library and it&#39;s
a little rough around the edges. But it&#39;s getting so good, so fast, that
I&#39;m starting to view <em>not</em> using it in production as an
abandonment of professional responsibility. The C library is so deeply
embedded in the software supply chain, and so depended upon, that you
really don&#39;t want it to be a planet killer. If essential unquestioned
tools are this wasteful then it&#39;s no wonder Amazon Cloud makes such a
fortune.

</p><p>
Last but not least, we have MacOS.

</p><div>
<table>
<tbody><tr>
  <th colspan="4"><p>
      MacOS Mutex Implementations</p></th></tr><tr>
  <th>wall</th><th>user</th><th>system</th><th>implementation

</th></tr><tr>
  <td>52,263
  </td><td>43,202
  </td><td>911,009
  </td><td>Apple Libc

</td></tr><tr>
  <td>54,700
  </td><td>63,055
  </td><td>1,003,674
  </td><td>Cosmopolitan pthread_mutex_t

</td></tr></tbody></table>
</div>

<p>
On MacOS with an M2 ARM64 microprocessor, Apple&#39;s Libc slightly
outperforms Cosmopolitan&#39;s mutexes. For reasons I do not yet fully
understand, Cosmopolitan&#39;s normal mutex implementation doesn&#39;t work well
on this platform. It&#39;s possibly because the M2 and XNU are in league. So
on MacOS ARM, Cosmopolitan uses a simpler algorithm based on Ulrich
Drepper&#39;s
&#34;<a href="https://dept-info.labri.fr/~denis/Enseignement/2008-IR/Articles/01-futex.pdf">Futexes
Are Tricky</a>&#34; paper that basically just farms out all the heavy
lifting to XNU&#39;s ulock system calls. That&#39;s why performance is nearly
identical to what Apple does.

</p><p>
So in summary, these benchmark results indicate that Cosmopolitan
mutexes, in the best case, will be overwhelmingly better than
alternatives at the contended + tiny critical section use case, and in
the worst case, Cosmopolitan will be roughly as good.

</p><h3>How I Did It</h3>

<p>
The reason why Cosmopolitan Mutexes are so good is because I used a
library called <a href="https://github.com/google/nsync">nsync</a>. It
only has 371 stars on GitHub, but it was written by a distinguished
engineer at Google named Mike Burrows. If you don&#39;t know who he is, he&#39;s
the guy who coded Google&#39;s fiercest competitor, which was Altavista. If
you&#39;re not old enough to remember Altavista, it was the first search
engine that was good, and it ran on a single computer.

</p><p>
I&#39;ve had a lot of fun integrating nsync into Cosmopolitan. I&#39;ve even
had the opportunity to make upstream contributions. For example, I found
and fixed a bug in his mutex unlock function that had gone undiscovered
for years. I also managed to make contended nsync mutexes go 30% faster
than nsync upstream on AARCH64, by porting it to use C11 atomics. I
wrote new system integration for things like futexes that enable it do
portability at runtime. Lastly I made it work seamlessly with POSIX
thread cancelations.

</p><p>
So how does nsync do it? What are the tricks that it uses? Here&#39;s some
of my takes and analysis:

</p><ul>

<li>
nsync uses an optimistic CAS (compare and swap) immediately, so that
locking happens quickly when there&#39;s no contention.

</li><li>
When a lock can&#39;t be acquired, nsync adds the calling thread to a
doubly linked list of waiters. Each waiter gets its own semaphore on a
separate independent cacheline. This serves an important purpose. Once a
thread enters the wait state, it&#39;s no longer touching the main lock. To
understand why that&#39;s important, read Ulrich Drepper&#39;s paper
&#34;<a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">What
Every Programmer Should Know About Memory</a>&#34;. He goes into great depth
on the coherency protocols used by modern microprocessors, where cores
basically talk to each other under the hood about which cachelines
they&#39;re using. When multiple cores touch the same ones, that creates a
lot of communication overhead within the processor.

</li><li>
nsync enlists the help of the operating system by using futexes. This is
a great abstraction invented by Linux some years ago, that quickly found
its way into other OSes. On MacOS, futexes are called ulock. On Windows,
futexes are called <code>WaitOnAddress()</code>. The only OS Cosmo
supports that doesn&#39;t have futexes is NetBSD, which implements POSIX
semaphores in kernelspace, and each semaphore sadly requires creating a
new file descriptor. But the important thing about futexes and
semaphores is they allow the OS to put a thread to sleep. That&#39;s what
lets nsync avoid consuming CPU time when there&#39;s nothing to do.

</li><li>
nsync avoids starvation with this concept of a &#34;long wait&#34;. If a waiter
gets woken 30 times and fails to acquire the lock internally every time,
then nsync adds a bit to the lock that prevents threads that haven&#39;t
waited yet from acquiring. This means that initial CAS at the beginning
will fail for everyone else until the queue has had some time to clear.

</li><li>
nsync makes the use case we benchmarked go fast (contended lock with a
small critical section) using this concept of a &#34;designated waker&#34;. This
bit on the main lock is set when a thread is awake and trying to acquire
the lock. In nsync, the unlock function is what&#39;s responsible for waking
the next thread in line waiting for the lock. Having this bit allows the
unlocking thread to know it needn&#39;t bother waking a second locker since
one is already awake.

</li></ul>

<p>
To learn more of nsync&#39;s secrets, you can read the source code here:
<a href="https://github.com/jart/cosmopolitan/tree/master/third_party/nsync/mu.c"><code>cosmopolitan/third_party/nsync/mu.c</code></a>.
See also
<a href="https://github.com/jart/cosmopolitan/tree/master/libc/intrin/pthread_mutex_lock.c"><code>cosmopolitan/libc/intrin/pthread_mutex_lock.c</code></a>.

</p><h3>Online Proof</h3>

<p>
If you want to see a live demo of a piece of software built with Cosmo
mutexes, then do your worst DDOS to the
<a href="http://ipv4.games/">http://ipv4.games/</a> web server. Now this
is truly a game for hackers, competing to dominate the Internet. You&#39;re
already playing this game because your IP was just claimed for jart. The
service runs on a GCE VM with 2 cores and so far it&#39;s managed to survive
being DDOS&#39;d by botnets as large as 49,131,669 IPs. A lot of that is
thanks to nsync which allowed me to move SQL queries to background
threads which send messages to each other. There are still improvements
to be made, but overall it&#39;s held up well. You can even monitor
its <a href="http://ipv4.games/statusz">/statusz</a> health metrics.

</p><h3>Source Code</h3>

<pre><span>#define ITERATIONS 100000
#define THREADS    30</span>

<span>int</span> g_chores;
<span>pthread_mutex_t</span> g_locker = PTHREAD_MUTEX_INITIALIZER;

<span>void</span> *<span>worker</span>(<span>void</span> *arg) {
  <span>for</span> (<span>int</span> i = 0; i &lt; ITERATIONS; ++i) {
    pthread_mutex_lock(&amp;g_locker);
    ++g_chores;
    pthread_mutex_unlock(&amp;g_locker);
  }
  <span>return</span> 0;
}

<span>struct</span> <span>timeval</span> <span>tub</span>(<span>struct</span> <span>timeval</span> a, <span>struct</span> <span>timeval</span> b) {
  a.tv_sec -= b.tv_sec;
  <span>if</span> (a.tv_usec &lt; b.tv_usec) {
    a.tv_usec += 1000000;
    a.tv_sec--;
  }
  a.tv_usec -= b.tv_usec;
  <span>return</span> a;
}

<span>long</span> <span>tomicros</span>(<span>struct</span> <span>timeval</span> x) {
  <span>return</span> x.tv_sec * 1000000ul + x.tv_usec;
}

<span>int</span> <span>main</span>() {
  <span>struct</span> <span>timeval</span> start;
  gettimeofday(&amp;start, 0);

  <span>pthread_t</span> th[THREADS];
  for (int i = 0; i &lt; THREADS; ++i)
    pthread_create(&amp;th[i], 0, worker, 0);
  for (int i = 0; i &lt; THREADS; ++i)
    pthread_join(th[i], 0);
  assert(g_chores == THREADS * ITERATIONS);

  <span>struct</span> <span>rusage</span> ru;
  <span>struct</span> <span>timeval</span> end;
  gettimeofday(&amp;end, 0);
  getrusage(RUSAGE_SELF, &amp;ru);
  printf(<span>&#34;%16ld us real\n&#34;</span>
         <span>&#34;%16ld us user\n&#34;</span>
         <span>&#34;%16ld us sys\n&#34;</span>,
         tomicros(tub(end, start)),
         tomicros(ru.ru_utime),
         tomicros(ru.ru_stime));
}

</pre>

<p>
The reason we care about the contended case, because that&#39;s where mutex
implementations show their inequality. Uncontended mutexes usually
perform the same across implementations, and even then you might be
better off with a spin lock, which only takes a few lines:

</p><pre><span>void</span> <span>spin_lock</span>(<span>atomic_int</span> *lock) {
  if (atomic_exchange_explicit(lock, 1, memory_order_acquire)) {
    <span>for</span> (;;) {
      <span>for</span> (;;)
        <span>if</span> (!atomic_load_explicit(lock, memory_order_relaxed))
          <span>break</span>;
      <span>if</span> (!atomic_exchange_explicit(lock, 1, memory_order_acquire))
        <span>break</span>;
    }
  }
}

<span>void</span> <span>spin_unlock</span>(<span>atomic_int</span> *lock) {
  atomic_store_explicit(lock, 0, memory_order_release);
}
</pre>

<p>
Please note that spin locks should really only be used when you have no
other choice. They&#39;re useful in kernels, where extreme low level
constraints disallow anything fancy. Spin locks are also a useful
implementation detail in nsync locks. But overall they&#39;re bad. I imagine
many developers believe they&#39;re good. If so, it&#39;s probably because
they&#39;ve only benchmarked wall time. With locks, it&#39;s important to take
CPU time into consideration too. That&#39;s why we use
<code>getrusage()</code>.

</p><h2 class="page" id="funding"><a href="#funding">Funding</a></h2>

<p>
  <a href="https://justine.lol/lemuria.png">
    <picture>
      <source srcset="//worker.jart.workers.dev/sectorlisp2/lemuria.webp" type="image/webp"/>
      <img src="https://worker.jart.workers.dev/sectorlisp2/lemuria.png" width="850" height="360" alt="[United States of Lemuria - two dollar bill - all debts public and primate]"/>
    </picture>
  </a>

</p><p>
Funding for The Fastest Mutex was crowdsourced from Justine
Tunney&#39;s <a href="https://github.com/sponsors/jart">GitHub sponsors</a>
and <a href="https://www.patreon.com/jart">Patreon subscribers</a>, the
backing of
<a href="https://future.mozilla.org/mieco/">Mozilla&#39;s MIECO program</a>,
and the generous contributions of our
<a href="https://discord.gg/FwAVVu7eJ4">developer community</a> on
Discord. Your support is what makes projects like Cosmopolitan possible.
Thank you!

</p>
<img src="https://ipv4.games/claim?name=jart"/>
</div>
  </body>
</html>
