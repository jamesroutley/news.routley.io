<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/tracel-ai/cubecl">Original</a>
    <h1>CubeCL: GPU Kernels in Rust for CUDA, ROCm, and WGPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/tracel-ai/cubecl/blob/main/assets/logo.drawio.svg"><img src="https://github.com/tracel-ai/cubecl/raw/main/assets/logo.drawio.svg" width="400px"/></a></p></div>

<p dir="auto">With CubeCL, you can program your GPU using Rust, taking advantage of zero-cost abstractions to develop maintainable, flexible, and efficient compute kernels.
CubeCL currently fully supports functions, generics, and structs, with partial support for traits, methods and type inference.
As the project evolves, we anticipate even broader support for Rust language primitives, all while maintaining optimal performance.</p>

<p dir="auto">Simply annotate functions with the <code>cube</code> attribute to indicate that they should run on the GPU.</p>
<div dir="auto" data-snippet-clipboard-copy-content="use cubecl::prelude::*;

#[cube(launch_unchecked)]
/// A [Line] represents a contiguous series of elements where SIMD operations may be available.
/// The runtime will automatically use SIMD instructions when possible for improved performance.
fn gelu_array&lt;F: Float&gt;(input: &amp;Array&lt;Line&lt;F&gt;&gt;, output: &amp;mut Array&lt;Line&lt;F&gt;&gt;) {
    if ABSOLUTE_POS &lt; input.len() {
        output[ABSOLUTE_POS] = gelu_scalar(input[ABSOLUTE_POS]);
    }
}

#[cube]
fn gelu_scalar&lt;F: Float&gt;(x: Line&lt;F&gt;) -&gt; Line&lt;F&gt; {
    // Execute the sqrt function at comptime.
    let sqrt2 = F::new(comptime!(2.0f32.sqrt()));
    let tmp = x / Line::new(sqrt2);

    x * (Line::erf(tmp) + 1.0) / 2.0
}"><pre><span>use</span> cubecl<span>::</span>prelude<span>::</span><span>*</span><span>;</span>

<span>#<span>[</span>cube<span>(</span>launch_unchecked<span>)</span><span>]</span></span>
<span>/// A [Line] represents a contiguous series of elements where SIMD operations may be available.</span>
<span></span><span>/// The runtime will automatically use SIMD instructions when possible for improved performance.</span>
<span></span><span>fn</span> <span>gelu_array</span><span>&lt;</span><span>F</span><span>:</span> <span>Float</span><span>&gt;</span><span>(</span><span>input</span><span>:</span> <span>&amp;</span><span>Array</span><span>&lt;</span><span>Line</span><span>&lt;</span><span>F</span><span>&gt;</span><span>&gt;</span><span>,</span> <span>output</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Array</span><span>&lt;</span><span>Line</span><span>&lt;</span><span>F</span><span>&gt;</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>if</span> <span>ABSOLUTE_POS</span> &lt; input<span>.</span><span>len</span><span>(</span><span>)</span> <span>{</span>
        output<span>[</span><span>ABSOLUTE_POS</span><span>]</span> = <span>gelu_scalar</span><span>(</span>input<span>[</span><span>ABSOLUTE_POS</span><span>]</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>#<span>[</span>cube<span>]</span></span>
<span>fn</span> <span>gelu_scalar</span><span>&lt;</span><span>F</span><span>:</span> <span>Float</span><span>&gt;</span><span>(</span><span>x</span><span>:</span> <span>Line</span><span>&lt;</span><span>F</span><span>&gt;</span><span>)</span> -&gt; <span>Line</span><span>&lt;</span><span>F</span><span>&gt;</span> <span>{</span>
    <span>// Execute the sqrt function at comptime.</span>
    <span>let</span> sqrt2 = <span>F</span><span>::</span><span>new</span><span>(</span><span>comptime</span><span>!</span><span>(</span><span>2.0f32</span><span>.</span>sqrt<span>(</span><span>)</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> tmp = x / <span>Line</span><span>::</span><span>new</span><span>(</span>sqrt2<span>)</span><span>;</span>

    x <span>*</span> <span>(</span><span>Line</span><span>::</span><span>erf</span><span>(</span>tmp<span>)</span> + <span>1.0</span><span>)</span> / <span>2.0</span>
<span>}</span></pre></div>
<p dir="auto">You can then launch the kernel using the autogenerated <code>gelu_array::launch_unchecked</code> function.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub fn launch&lt;R: Runtime&gt;(device: &amp;R::Device) {
    let client = R::client(device);
    let input = &amp;[-1., 0., 1., 5.];
    let vectorization = 4;
    let output_handle = client.empty(input.len() * core::mem::size_of::&lt;f32&gt;());
    let input_handle = client.create(f32::as_bytes(input));

    unsafe {
        gelu_array::launch_unchecked::&lt;f32, R&gt;(
            &amp;client,
            CubeCount::Static(1, 1, 1),
            CubeDim::new(input.len() as u32 / vectorization, 1, 1),
            ArrayArg::from_raw_parts::&lt;f32&gt;(&amp;input_handle, input.len(), vectorization as u8),
            ArrayArg::from_raw_parts::&lt;f32&gt;(&amp;output_handle, input.len(), vectorization as u8),
        )
    };

    let bytes = client.read_one(output_handle.binding());
    let output = f32::from_bytes(&amp;bytes);

    // Should be [-0.1587,  0.0000,  0.8413,  5.0000]
    println!(&#34;Executed gelu with runtime {:?} =&gt; {output:?}&#34;, R::name());
}"><pre><span>pub</span> <span>fn</span> <span>launch</span><span>&lt;</span><span>R</span><span>:</span> <span>Runtime</span><span>&gt;</span><span>(</span><span>device</span><span>:</span> <span>&amp;</span><span>R</span><span>::</span><span>Device</span><span>)</span> <span>{</span>
    <span>let</span> client = <span>R</span><span>::</span><span>client</span><span>(</span>device<span>)</span><span>;</span>
    <span>let</span> input = <span>&amp;</span><span>[</span>-<span>1.</span><span>,</span> <span>0.</span><span>,</span> <span>1.</span><span>,</span> <span>5.</span><span>]</span><span>;</span>
    <span>let</span> vectorization = <span>4</span><span>;</span>
    <span>let</span> output_handle = client<span>.</span><span>empty</span><span>(</span>input<span>.</span><span>len</span><span>(</span><span>)</span> <span>*</span> core<span>::</span>mem<span>::</span><span>size_of</span><span>::</span><span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> input_handle = client<span>.</span><span>create</span><span>(</span>f32<span>::</span><span>as_bytes</span><span>(</span>input<span>)</span><span>)</span><span>;</span>

    <span>unsafe</span> <span>{</span>
        gelu_array<span>::</span><span>launch_unchecked</span><span>::</span><span>&lt;</span><span>f32</span><span>,</span> <span>R</span><span>&gt;</span><span>(</span>
            <span>&amp;</span>client<span>,</span>
            <span>CubeCount</span><span>::</span><span>Static</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span><span>,</span>
            <span>CubeDim</span><span>::</span><span>new</span><span>(</span>input<span>.</span><span>len</span><span>(</span><span>)</span> <span>as</span> <span>u32</span> / vectorization<span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span><span>,</span>
            <span>ArrayArg</span><span>::</span><span>from_raw_parts</span><span>::</span><span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span><span>&amp;</span>input_handle<span>,</span> input<span>.</span><span>len</span><span>(</span><span>)</span><span>,</span> vectorization <span>as</span> <span>u8</span><span>)</span><span>,</span>
            <span>ArrayArg</span><span>::</span><span>from_raw_parts</span><span>::</span><span>&lt;</span><span>f32</span><span>&gt;</span><span>(</span><span>&amp;</span>output_handle<span>,</span> input<span>.</span><span>len</span><span>(</span><span>)</span><span>,</span> vectorization <span>as</span> <span>u8</span><span>)</span><span>,</span>
        <span>)</span>
    <span>}</span><span>;</span>

    <span>let</span> bytes = client<span>.</span><span>read_one</span><span>(</span>output_handle<span>.</span><span>binding</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> output = f32<span>::</span><span>from_bytes</span><span>(</span><span>&amp;</span>bytes<span>)</span><span>;</span>

    <span>// Should be [-0.1587,  0.0000,  0.8413,  5.0000]</span>
    <span>println</span><span>!</span><span>(</span><span>&#34;Executed gelu with runtime {:?} =&gt; {output:?}&#34;</span><span>,</span> <span>R</span><span>::</span>name<span>(</span><span>)</span><span>)</span><span>;</span>
<span>}</span></pre></div>
<p dir="auto">To see it in action, run the working GELU example with the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo run --example gelu --features cuda # cuda runtime
cargo run --example gelu --features wgpu # wgpu runtime"><pre>cargo run --example gelu --features cuda <span><span>#</span> cuda runtime</span>
cargo run --example gelu --features wgpu <span><span>#</span> wgpu runtime</span></pre></div>

<p dir="auto">We support the following GPU runtimes:</p>
<ul dir="auto">
<li><a href="https://github.com/gfx-rs/wgpu">WGPU</a> for cross-platform GPU support (Vulkan, Metal, DirectX, WebGPU)</li>
<li><a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow">CUDA</a> for NVIDIA GPU support</li>
<li><a href="https://www.amd.com/en/products/software/rocm.html" rel="nofollow">ROCm/HIP</a> for AMD GPU support (WIP)</li>
</ul>
<p dir="auto">We also plan to develop an optimized JIT CPU runtime with SIMD instructions, leveraging <a href="https://cranelift.dev" rel="nofollow">Cranelift</a>.</p>

<p dir="auto">The goal of CubeCL is to ease the pain of writing highly optimized compute kernels that are portable across hardware.
There is currently no adequate solution when you want optimal performance while still being multi-platform.
You either have to write custom kernels for different hardware, often with different languages such as CUDA, Metal, or ROCm.
To fix this, we created a Just-in-Time compiler with three core features: <strong>automatic vectorization</strong>, <strong>comptime</strong>, and <strong>autotune</strong>!</p>
<p dir="auto">These features are extremely useful for anyone writing high-performance kernels, even when portability is not a concern.
They improve code composability, reusability, testability, and maintainability, all while staying optimal.
CubeCL also ships with a memory management strategy optimized for throughput with heavy buffer reuse to avoid allocations.</p>
<p dir="auto">Our goal extends beyond providing an optimized compute language; we aim to develop an ecosystem of high-performance and scientific computing in Rust.
To achieve this, we&#39;re developing linear algebra components that you can integrate into your own kernels.
We currently have an highly optimized matrix multiplication module, leveraging Tensor Cores on NVIDIA hardware where available, while gracefully falling back to basic instructions on other platforms.
While there&#39;s room for improvement, particularly in using custom instructions from newer NVIDIA GPUs, our implementation already delivers impressive performance.</p>
<p dir="auto">This is just the beginning.
We plan to include more utilities such as convolutions, random number generation, fast Fourier transforms, and other essential algorithms.
We are a small team also building <a href="https://burn.dev" rel="nofollow">Burn</a>, so don&#39;t hesitate to contribute and port algorithms; it can help more than you would imagine!</p>

<p dir="auto">CubeCL leverages Rust&#39;s proc macro system in a unique two-step process:</p>
<ol dir="auto">
<li>Parsing: The proc macro parses the GPU kernel code using the syn crate.</li>
<li>Expansion: Instead of immediately generating an Intermediate Representation (IR), the macro generates a new Rust function.</li>
</ol>
<p dir="auto">The generated function, semantically similar to the original, is responsible for creating the IR when called.
This approach differs from traditional compilers, which typically generate IR directly after parsing.
Our method enables several key features:</p>
<ul dir="auto">
<li><strong>Comptime</strong>: By not transforming the original code, it becomes remarkably easy to integrate compile-time optimizations.</li>
<li><strong>Automatic Vectorization</strong>: By simply vectorizing the inputs of a CubeCL function, we can determine the vectorization factor of each intermediate variable during the expansion.</li>
<li><strong>Rust Integration</strong>: The generated code remains valid Rust code, allowing it to be bundled without any dependency on the specific runtime.</li>
</ul>

<p dir="auto">CubeCL is designed around - you guessed it - Cubes! More specifically, it&#39;s based on cuboids, because not all axes are the same size.
Since all compute APIs need to map to the hardware, which are tiles that can be accessed using a 3D representation, our topology can easily be mapped to concepts from other APIs.</p>
<div dir="auto">

<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/tracel-ai/cubecl/blob/main/assets/cubecl.drawio.svg"><img src="https://github.com/tracel-ai/cubecl/raw/main/assets/cubecl.drawio.svg" width="100%"/></a>
<br/>
</p></div>
</article></div></div>
  </body>
</html>
