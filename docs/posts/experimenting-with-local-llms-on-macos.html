<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.6nok.org/experimenting-with-local-llms-on-macos/">Original</a>
    <h1>Experimenting with Local LLMs on macOS</h1>
    
    <div id="readability-page-1" class="page"><article><header><p>September 08, 2025<!-- --> · <!-- -->9 minutes<!-- --> to read</p></header><section>
<p>So, this blog post will be about LLMs, and everyone has <span><span>o</span><span>p</span><span>i</span><span>n</span><span>i</span><span>o</span><span>n</span><span>s</span> about that. To be upfront about it, I’m a skeptic (bordering on hater), yet I like experimenting with stuff so I download and run them locally on my Mac.<sup id="fnref-0"><a href="#fn-0">0</a></sup> And I’ll teach you how to do it too, if you’d like!</span></p>
<p><span>
      <a href="https://blog.6nok.org/static/207dbd07fb221f623d3055156906c562/0c02e/orihime_leek.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="classic orihime with leek meme, but with multicolored &#34;you&#39;re absolutely correct&#34; overlaid on top of it" title="" src="https://blog.6nok.org/static/207dbd07fb221f623d3055156906c562/1c72d/orihime_leek.jpg" srcset="/static/207dbd07fb221f623d3055156906c562/a80bd/orihime_leek.jpg 148w,
/static/207dbd07fb221f623d3055156906c562/1c91a/orihime_leek.jpg 295w,
/static/207dbd07fb221f623d3055156906c562/1c72d/orihime_leek.jpg 590w,
/static/207dbd07fb221f623d3055156906c562/a8a14/orihime_leek.jpg 885w,
/static/207dbd07fb221f623d3055156906c562/fbd2c/orihime_leek.jpg 1180w,
/static/207dbd07fb221f623d3055156906c562/0c02e/orihime_leek.jpg 1206w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy" decoding="async"/>
  </a>
    </span></p>
<p>Some call them fancy autocomplete, some argue that they are sentient and should have rights. The truth is somewhere in between. Yes, they perform next word prediction, but it’s so complex that there’s nontrivial emergent behavior. No, they don’t have creativity or a mind. I believe one day we can create sentient machines, but not in this current iteration, maybe not before we go extinct.</p>
<p>Now that we’re out of the science fiction territory, let’s talk about their strengths. <a href="https://seldo.com/posts/what-ive-learned-about-writing-ai-apps-so-far" target="_blank" rel="nofollow noopener noreferrer">Laurie has a great post about it</a>, which I highly recommend, but in summary they are generally good at summarizing text, regurgitating home maintenance advice from reddit, or telling you that you have cancer.</p>
<p>I also use them for brain-dumping. I find it hard to keep a journal, because I find it boring, but when you’re pretending to be writing to someone, it’s easier. If you have friends, that’s much better, but some topics are too personal and a friend may not be available at 4 AM.</p>
<p>I mostly ignore its responses, because it’s for me to unload, not to listen to a machine spew slop. I suggest you do the same, because we’re anthropomorphization machines and I’d rather not experience <a href="https://www.bbc.com/news/articles/c24zdel5j18o" target="_blank" rel="nofollow noopener noreferrer">AI psychosis</a>. It’s better if you don’t give it a chance to convince you it’s real. I could use a system prompt so it doesn’t follow up with dumb questions (or “YoU’Re AbSoLuTeLy CoRrEcT”s), but I never bothered as I already don’t read it.</p>
<p>Lastly, I’m interested in them because it’s tech and I like tech. <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/" target="_blank" rel="nofollow noopener noreferrer">I don’t believe they make you that much productive</a>, and I never let them write for me. When I ask it something, I always fact-check, they are known to hallucinate (or bullshit, if you prefer) and I’ve experienced this too many times to trust it. Most importantly, just avoid asking questions that can’t be easily verified, there’s enough fake news around anyway.</p>
<p>You may be asking, “Well I can do everything you said with ChatGPT free tier, why bother running them locally?” There are a few reasons, and I’ve already mentioned them briefly:</p>
<ol>
<li>
<p>I like experimenting with things. It’s marvelous that you can download this 12 GB file and your computer talks to you marginally better than <a href="https://en.wikipedia.org/wiki/ELIZA" target="_blank" rel="nofollow noopener noreferrer">ELIZA</a>! Joking aside, we accepted the concept of LLMs too quickly, when the truth is that we never expected computers to figure out human speech before robots were walking among us. So it feels a bit magical when my computer has better grammar than I do, and I can summon one whenever I want without a supercomputer.</p>
</li>
<li>
<p>People have secrets and some secrets shouldn’t leave your computer. Companies are not well equipped to keep your sensitive data, and current trends show that they can <a href="https://www.theverge.com/news/681280/openai-storing-deleted-chats-nyt-lawsuit" target="_blank" rel="nofollow noopener noreferrer">retain your data</a> or <a href="https://www.theverge.com/anthropic/767507/anthropic-user-data-consumers-ai-models-training-privacy" target="_blank" rel="nofollow noopener noreferrer">use it for training</a>. I feel like a local LLM is a better choice for these use cases.</p>
</li>
<li>
<p>I don’t feel comfortable giving money to the AI companies. Every single one of them has done some kind of unethical thing, and the way the AI race is going, a hypothetical ethical one will be left behind. They intentionally hype AI, destroy the environment, and plagiarize people’s hard work. I’m thankful for the open-weight models they provide and will keep using them, luckily they can’t take that away from me.</p>
</li>
</ol>
<p>If you still want to run an LLM on your macOS<sup id="fnref-1"><a href="#fn-1">1</a></sup>, let’s get started. There are two options that I recommend, one is open-source and the other is easier to use, as all things are. I only cover macOS because that’s what I have; if that’s not your platform, you can still follow this guide with platform-specific tweaks, or find another guide.</p>
<h2 id="llamacpp"><a href="#llamacpp" aria-label="llamacpp permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="nofollow noopener noreferrer">Llama.cpp</a></h2>
<p>It’s created by Georgi Gerganov and works really well with a ton of configuration options. It supports a lot of platforms, can download models, and has a basic web UI for convenience. You can <a href="https://blog.6nok.org/how-i-use-nix-on-macos">install it with Nix</a> using the following command: <code>nix profile install nixpkgs#llama-cpp</code>. For other installation methods, check <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md" target="_blank" rel="nofollow noopener noreferrer">their instructions</a>.</p>
<p>To download and run a small and good enough model for experimentation, I recommend Gemma 3 4B QAT, which you can do with the following command:</p>
<div data-language="bash"><pre><code>$ llama-server <span>-hf</span> ggml-org/gemma-3-4b-it-qat-GGUF</code></pre></div>
<p>If you open your web browser and navigate to <code>http://127.0.0.1:8080</code>, you’ll see a very bare bones web UI that’s eerily similar to ChatGPT, but that should be enough. You can experiment with it and exit the server once you’re done.</p>
<h2 id="lm-studio"><a href="#lm-studio" aria-label="lm studio permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="https://lmstudio.ai" target="_blank" rel="nofollow noopener noreferrer">LM Studio</a></h2>
<p>This is the closed-source but easier to use one. It has a superb UI that lets you browse models, manage downloads, organize chats, and even tells you if a model can run on your machine. It has guardrails so you don’t crash your system by loading a model that’s too large. I like using it a lot, but enshittification is always a risk with closed-source for-profit software.</p>
<p>You can download a DMG from its website and install it as usual. LM Studio has two runtimes on macOS, <code>llama.cpp</code> which we covered earlier, and <a href="https://github.com/ml-explore/mlx" target="_blank" rel="nofollow noopener noreferrer">MLX</a>, which is an ML engine developed by Apple and runs a bit faster, but offers less configuration in the UI. (I didn’t try running MLX directly, maybe it’s configurable but not exposed.)</p>
<p>I will not provide a walkthrough because this is not a product tour and the UI is pretty user-friendly anyway. I can give you some tips and tricks though.</p>
<ul>
<li>You can switch the model mid-conversation, it won’t have any problem</li>
<li>You can branch off the current conversation, which is good for experimentation</li>
<li>You can regenerate the assistant message, like ChatGPT</li>
<li>You can edit your own messages, also like ChatGPT, but you can also edit assistant messages, which allows you to put words into its mouth</li>
<li>You can create presets for system prompts and reuse them for different personas</li>
<li>There are a lot of model settings you can configure; for example, you can customize the context overflow behavior when the context grows larger than the context window (the default is to truncate the middle, so first and last messages are kept, which is a good default)</li>
</ul>
<h2 id="how-to-choose-a-good-llm-for-your-use-case"><a href="#how-to-choose-a-good-llm-for-your-use-case" aria-label="how to choose a good llm for your use case permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to choose a good LLM for your use case</h2>
<p>Right now there are a lot of open-weight models around, almost all AI companies have released one. There are a few things you need to pay attention to when choosing a model. These things take a lot of disk space so be mindful of filling your disk!</p>
<h3 id="model-size"><a href="#model-size" aria-label="model size permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model size</h3>
<p>You may have plenty of free space on your drive, but for the LLM to work you need to load it into memory. So your RAM is the bottleneck. Since the operating system also needs memory to work, if you have 16 GB RAM, like me, then models should be less than 12 GB. Loading larger models may cause you to run out of memory, your system will be unresponsive, and you’ll have to perform a hard reboot. Larger models will also run slower.<sup id="fnref-2"><a href="#fn-2">2</a></sup></p>
<h3 id="runtime"><a href="#runtime" aria-label="runtime permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Runtime</h3>
<p>If you’re using <code>llama.cpp</code> directly, or as the runtime in LM Studio, you need GGUF models. If you’re using the MLX runtime in LM Studio, you need MLX models. As mentioned before, MLX models run slightly faster, but GGUF models provide more configuration, and since they can run on many platforms they are ubiquitous and better tested.</p>
<h3 id="quantization"><a href="#quantization" aria-label="quantization permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quantization</h3>
<p>Most LLMs are trained at <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" target="_blank" rel="nofollow noopener noreferrer">16-bit precision</a>, but you can downcast (or quantize) the weights to lower precision and they still perform well up to some point. The sweet point is around 4 bits, which is noted as <code>Q4</code>. LM Studio defaults to this.</p>
<p>Quantization is a rabbit hole; different kernels are used for quantization, which is included in the notation, such as <code>Q4_K_M</code>, which is a bit too much for me and I decided against learning it. Just download the default and be done with it.</p>
<h3 id="vision-models"><a href="#vision-models" aria-label="vision models permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Vision models</h3>
<p>Some models can tokenize image inputs and can “see” what’s inside them. This feels pretty magical; they can read text, recognize objects, and determine the mood or <a href="https://gist.github.com/frontsideair/0b0e681ef8c4c2eaaed1deeeb804f358" target="_blank" rel="nofollow noopener noreferrer">art style</a>. You can use them as basic OCRs but in my experience dedicated OCRs perform better at serious tasks, LLMs tend to make shit up when they can’t read.</p>
<h3 id="reasoning"><a href="#reasoning" aria-label="reasoning permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reasoning</h3>
<p>Some models can “think” before generating an answer, which is sometimes called “inference time scaling”<sup id="fnref-3"><a href="#fn-3">3</a></sup>. The general wisdom is that smaller reasoning models can compete with larger non-reasoning models, which is reflected in benchmarks. The downside is that it takes longer to get an answer. So you need to decide if you want a larger but smarter model, or a smaller one that reasons. Keep in mind that reasoning sometimes takes minutes, and fills up the context pretty quickly.</p>
<h3 id="tool-use"><a href="#tool-use" aria-label="tool use permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tool use</h3>
<p>Some models are taught to emit special tokens that can call tools specified in the system prompt with the correct arguments. LM Studio has a UI for adding MCP servers and managing the capabilities provided by them.</p>
<p>By default, LM Studio asks you to confirm each tool call request, which is great for security. Tool calls are commonly used for <a href="https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/" target="_blank" rel="nofollow noopener noreferrer">data exfiltration attacks</a>, which is as bad as it sounds.</p>
<p>LM Studio by default comes with a JavaScript MCP, powered by <a href="https://deno.com" target="_blank" rel="nofollow noopener noreferrer">Deno</a>, which provides the LLM the ability to execute code in a sandbox. This is really powerful, you can make it perform hard calculations, analyze data, even generate random numbers. I created a <a href="https://gist.github.com/frontsideair/68d3eb675471ca4bc0388b4a3a32ac3d" target="_blank" rel="nofollow noopener noreferrer">number guessing game</a>, which works better than expected.</p>
<p>You can also plug in a web search MCP to give it up-to-date knowledge retrieval capabilities. Since small models have limited world knowledge, this makes them work remarkably well for applicable use cases. I have used <a href="https://exa.ai" target="_blank" rel="nofollow noopener noreferrer">Exa</a>’s free tier for this, which worked well.</p>
<p>Lastly, if you want a model to have long-term memory, there are a ton of MCPs that can do that. But keeping with the theme of keeping things local, I found <a href="https://github.com/MarkusPfundstein/mcp-obsidian" target="_blank" rel="nofollow noopener noreferrer">MCP server for Obsidian</a> to be a good candidate.</p>
<p>One thing to keep in mind regarding MCPs is that since they have to teach the model about themselves, they pollute the context pretty quickly, so be sure to only enable those you need.</p>
<h3 id="aside-agents"><a href="#aside-agents" aria-label="aside agents permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Aside: Agents</h3>
<p><a href="https://simonwillison.net/2025/May/22/tools-in-a-loop/" target="_blank" rel="nofollow noopener noreferrer">“Agents are models using tools in a loop.”</a> So a model that has both reasoning and tool use capabilities hits a sweet spot, and is commonly called an “agent”. These can call tools repeatedly while reasoning, and provide a complete answer. The truth is far from perfect, in my experience, but it’s still a cool concept.</p>
<h3 id="finding-a-good-model"><a href="#finding-a-good-model" aria-label="finding a good model permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Finding a good model</h3>
<p>LM Studio has a built-in UI for finding models, which shows runtime, quantization, model capabilities, and size in a user-friendly way. For <code>llama.cpp</code>, you can check <a href="https://huggingface.co/models?library=gguf&amp;sort=trending" target="_blank" rel="nofollow noopener noreferrer">Hugging Face for GGUF models</a>.</p>
<p>You may not find a model that hits all the capability boxes, so it’s better to download a variety of models and experiment with them. Here are some of my favorites, in no particular order:</p>
<ul>
<li><a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf" target="_blank" rel="nofollow noopener noreferrer">Gemma 3 12B QAT</a>: for visual intelligence and it’s generally a good non-reasoning model that’s fast and produces good text</li>
<li><a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507" target="_blank" rel="nofollow noopener noreferrer">Qwen3 4B 2507 Thinking</a>: This is the updated version of Qwen3 4B, which also has a non-reasoning variant; it’s really small, fast, and good quality for its size</li>
<li><a href="https://huggingface.co/openai/gpt-oss-20b" target="_blank" rel="nofollow noopener noreferrer">GPT-OSS 20B</a>: The largest and most capable model that can run on my machine, has three levels of reasoning; it’s rather slow but very capable, smartest of all</li>
<li><a href="https://huggingface.co/microsoft/phi-4" target="_blank" rel="nofollow noopener noreferrer">Phi-4 (14B)</a>: It was my favorite before GPT-OSS, now has reasoning and reasoning plus variants, but I haven’t used it lately</li>
</ul>
<h2 id="final-words"><a href="#final-words" aria-label="final words permalink"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Final words</h2>
<p>Small models may not replace frontier models in terms of speed or quality, but I still see utility in them. Running them locally is also a good test bed for understanding how they work and learning to work around their weaknesses.</p>
<p>Let me part with a final tip: LM Studio shows you how much of the context window is being used. So you may find it useful to ask for it to summarize the conversation so far, when the context window gets close to being filled. This way you can help it remember important information that it would otherwise forget.<sup id="fnref-4"><a href="#fn-4">4</a></sup></p>
<p>Have fun with your brand new genie in your computer!</p>
<p><em>Thanks to <a href="https://monkeykode.com/us#jull-weber" target="_blank" rel="nofollow noopener noreferrer">Jull</a> for reviewing an early draft, and my girlfriend for the leekspin hero image.</em></p>
</section><a target="_blank" rel="noopener noreferrer" href="https://bsky.app/intent/compose?text=You should totally read &#34;Experimenting with local LLMs on macOS&#34; https%3A%2F%2Fblog.6nok.org%2Fexperimenting-with-local-llms-on-macos%2F">Share on Bluesky</a> · <a target="_blank" rel="noopener noreferrer" href="https://github.com/frontsideair/blog/edit/main/content/blog/experimenting-with-local-llms-on-macos/index.md">Edit on GitHub</a><hr/></article></div>
  </body>
</html>
