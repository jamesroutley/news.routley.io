<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/positional-encoding-for-self-attention/">Original</a>
    <h1>Positional Encoding for Self Attention</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p><em>In the dimly lit chambers of his ancient library, the wizard Eldron carefully weaves his spell over a complex array of arcane symbols. With each precise gesture, he transmutes these symbols, imbuing them with a hidden layer of meaning: the magic of positional encoding. This enchantment allows the symbols to hold not just the essence of words, but also their place in the grand tapestry of language. Eldron’s eyes gleam with satisfaction as the embeddings shimmer, now ready to reveal their secrets in perfect harmony and sequence.</em></p>
<figure><a href="https://sagittarius.greg.technology/eldron.png" title="eldron" data-thumbnail="eldron.png" data-sub-html="&lt;h2&gt;Eldron&#39;s transmutation&lt;/h2&gt;&lt;p&gt;eldron&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="eldron.png" data-srcset="eldron.png, eldron.png 1.5x, eldron.png 2x" data-sizes="auto" alt="eldron.png"/>
    </a><figcaption>Eldron&#39;s transmutation</figcaption>
    </figure>
<h2 id="the-quest">The Quest</h2>
<p>Eldron left many manuscripts of his work on positional encoding. Figure out the intricacy of his spells and compare their efficacy.</p>
<h2 id="toy-problem-copy-task">Toy problem: Copy-Task</h2>
<p>To illustrate the different positional encoding schemes we will work on a toy problem.</p>
<p>The task is to replicate the input sequence before the special <code>&lt;copy&gt;</code> token, up to the end of the padding.</p>
<p>e.g.:</p>
<pre tabindex="0"><code data-lang="ascii">1 7 2 &lt;copy&gt; _ _ _ _ _ _ →  1 7 2 &lt;copy&gt; 1 7 2 _ _ _
9 &lt;copy&gt; _ _ _ _ _ _ _ _ →  9 &lt;copy&gt; 9 _ _ _ _ _ _ _
2 2 4 3 &lt;copy&gt; _ _ _ _ _ →  2 2 4 3 &lt;copy&gt; 2 2 4 3 _
1 2 3 4 5 6 7 &lt;copy&gt; _ _ →  1 2 3 4 5 6 7 &lt;copy&gt; 1 2
</code></pre><h3 id="why-this-task">why this task?</h3>
<p>I chose this task because it requires attention and some awarness of position to be solvable.</p>
<ul>
<li>The offset of the <code>&lt;copy&gt;</code> is variable between examples so the model cannot hardwire inputs to outputs, so self-attention is important.</li>
<li>The problem is not well suited for a bag of words approach because order is very important, so positional encoding is important.</li>
</ul>
<h3 id="why-encoder-instead-of-decoder">why Encoder instead of Decoder?</h3>
<p>For this task we chose to use an encoder-only transformer. Taking a sequence of tokens, a single copy token, and several padding tokens as input and producing the entire resulting sequence in a single pass.</p>
<p>The reason we are not using an autoregressive decoder-only model for this is because the first half of the problem is unlearnable with a causal mask.</p>
<p>let’s imagine what the equivalent training set would be for:</p>
<pre tabindex="0"><code data-lang="ascii">2 7 4 3 &lt;copy&gt; _ _ _ _ _ →  2 7 4 3 &lt;copy&gt; 2 7 4 3 _
</code></pre><p>Using a causal mask with a decoder-only autoregressive transformer would be equivalent to ask the model to predict all these pairs of input / labels:</p>
<pre tabindex="0"><code data-lang="ascii">2                        →  7
2 7                      →  7 4
2 7 4                    →  7 4 3
2 7 4 3                  →  7 4 3 &lt;copy&gt;
2 7 4 3 &lt;copy&gt;           →  7 4 3 &lt;copy&gt; 2
2 7 4 3 &lt;copy&gt; _         →  7 4 3 &lt;copy&gt; 2 7
2 7 4 3 &lt;copy&gt; _ _       →  7 4 3 &lt;copy&gt; 2 7
2 7 4 3 &lt;copy&gt; _ _ _     →  7 4 3 &lt;copy&gt; 2 7 4 
2 7 4 3 &lt;copy&gt; _ _ _ _   →  7 4 3 &lt;copy&gt; 2 7 4 3
2 7 4 3 &lt;copy&gt; _ _ _ _ _ →  7 4 3 &lt;copy&gt; 2 7 4 3 _
</code></pre><p>And the problem here is that the first 4 pairs are not possible to guess. There is equal probability (10%) to chose the correct next token. Once the <code>&lt;copy&gt;</code> token appear in the input the output becomes totally inferable. But before that it’s a coin toss. So decoder-only are not a good fit.</p>
<h2 id="sinusoidal">Sinusoidal</h2>
<p>The first positional encoding proposed in the Attention Is All You Need paper is Sinusoidal. It consists of taking a bunch of offsets on a set of <code>sin()</code> and <code>cos()</code> waves with increasing frequencies as a representation of the position of a given word.</p>
<p>Intuitively it bothers me. I have the sentiment that it creates a failure mode where the network just “duplicate” tokens. Where token <code>1</code> at offset 0 is a different token from <code>1</code> at offset 1 or 2 …, conceptually duplicating the vocabulary for each possible offset in the context window.</p>
<p><em>But Eldron’s manuscripts are littered with references to it, so let’s cast the spell to get a better feel for it.</em></p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>get_sinusoidal_positional_encoding</span><span>(</span><span>context_size</span><span>=</span><span>CONTEXT_SIZE</span><span>,</span> <span>embed_size</span><span>=</span><span>EMBED_SIZE</span><span>):</span>
</span></span><span><span>  <span>position</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>context_size</span><span>)</span><span>.</span><span>unsqueeze</span><span>(</span><span>1</span><span>)</span>
</span></span><span><span>  <span>div_term</span> <span>=</span> <span>torch</span><span>.</span><span>exp</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>embed_size</span><span>,</span> <span>2</span><span>)</span> <span>*</span> <span>-</span><span>(</span><span>math</span><span>.</span><span>log</span><span>(</span><span>10000.0</span><span>)</span> <span>/</span> <span>embed_size</span><span>))</span>
</span></span><span><span>  <span>positional_encoding</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>context_size</span><span>,</span> <span>embed_size</span><span>)</span>
</span></span><span><span>  <span>positional_encoding</span><span>[:,</span> <span>0</span><span>::</span><span>2</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>sin</span><span>(</span><span>position</span> <span>*</span> <span>div_term</span><span>)</span>
</span></span><span><span>  <span>positional_encoding</span><span>[:,</span> <span>1</span><span>::</span><span>2</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>cos</span><span>(</span><span>position</span> <span>*</span> <span>div_term</span><span>)</span>
</span></span><span><span>  <span>return</span> <span>positional_encoding</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>Net</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>  <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>    <span>self</span><span>.</span><span>positional_embedding</span> <span>=</span> <span>get_sinusoidal_positional_encoding</span><span>()</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>
</span></span><span><span>  <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>self</span><span>.</span><span>token_embedding</span><span>(</span><span>x</span><span>)</span>  <span># (batch_size, context_size, embedding_size)</span>
</span></span><span><span>    <span># sinusoidal positional encoding</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>get_positional_embedding</span><span>()</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span></code></pre></div><h2 id="learned">Learned</h2>
<p>Learned positional encoding are similar in concept. But instead of hardcoding the values we let the network learn whatever it prefers.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Net</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>  <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>    <span>self</span><span>.</span><span>positional_embedding</span> <span>=</span> <span>nn</span><span>.</span><span>Embedding</span><span>(</span><span>context_size</span><span>,</span> <span>embed_size</span><span>)</span>      
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>
</span></span><span><span>  <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>self</span><span>.</span><span>token_embedding</span><span>(</span><span>x</span><span>)</span>  <span># (batch_size, context_size, embedding_size)</span>
</span></span><span><span>    <span># learned positional encoding</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>self</span><span>.</span><span>positional_embedding</span><span>(</span><span>torch</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>self</span><span>.</span><span>context_size</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>))</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span></code></pre></div><h3 id="visualize-the-learned-positional-encodings">Visualize the Learned positional encodings</h3>
<p>In our case what the network prefers is:</p>
<figure><a href="https://sagittarius.greg.technology/learned-20d-pca.gif" title="learned-20d-pca" data-thumbnail="learned-20d-pca.gif" data-sub-html="&lt;h2&gt;PCA of the Learned Positional Encodings during training&lt;/h2&gt;&lt;p&gt;learned-20d-pca&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="learned-20d-pca.gif" data-srcset="learned-20d-pca.gif, learned-20d-pca.gif 1.5x, learned-20d-pca.gif 2x" data-sizes="auto" alt="learned-20d-pca.gif"/>
    </a><figcaption>PCA of the Learned Positional Encodings during training</figcaption>
    </figure>
<h3 id="compare-the-2-schemes">Compare the 2 schemes</h3>
<p>Looking at the similarities (dot product and cosine similarity) for different positional offsets
</p><figure><a href="https://sagittarius.greg.technology/attention_random.png" title="attention_random" data-thumbnail="attention_random.png" data-sub-html="&lt;h2&gt;Uninitialized Positional Encoding weights&lt;/h2&gt;&lt;p&gt;attention_random&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="attention_random.png" data-srcset="attention_random.png, attention_random.png 1.5x, attention_random.png 2x" data-sizes="auto" alt="attention_random.png"/>
    </a><figcaption>Uninitialized Positional Encoding weights</figcaption>
    </figure>
<figure><a href="https://sagittarius.greg.technology/attention_learned.png" title="attention_learned" data-thumbnail="attention_learned.png" data-sub-html="&lt;h2&gt;Trained Positional Encoding weights&lt;/h2&gt;&lt;p&gt;attention_learned&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="attention_learned.png" data-srcset="attention_learned.png, attention_learned.png 1.5x, attention_learned.png 2x" data-sizes="auto" alt="attention_learned.png"/>
    </a><figcaption>Trained Positional Encoding weights</figcaption>
    </figure>
<figure><a href="https://sagittarius.greg.technology/attention_sinusoidal.png" title="attention_sinusoidal" data-thumbnail="attention_sinusoidal.png" data-sub-html="&lt;h2&gt;Sinusoidal Positional Encoding Weights&lt;/h2&gt;&lt;p&gt;attention_sinusoidal&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="attention_sinusoidal.png" data-srcset="attention_sinusoidal.png, attention_sinusoidal.png 1.5x, attention_sinusoidal.png 2x" data-sizes="auto" alt="attention_sinusoidal.png"/>
    </a><figcaption>Sinusoidal Positional Encoding Weights</figcaption>
    </figure>
<h2 id="rope">RoPE</h2>
<p>RoPE is a scheme that plugs inside the Attention block and is based on rotation instead of translation.</p>
<p>Intuitively I like the idea to not “pollute” the residual stream of embeddings. Only applying the transformation to the <code>Query</code> and <code>Key</code> inside the attention block.</p>
<h3 id="using-a-giant-rotation-matrix-slow">Using a giant rotation matrix (Slow)</h3>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># slow (but intuitive?) using matrix multiplication for the rotations</span>
</span></span><span><span><span># e.g. of rotation matrix for embed_size=4</span>
</span></span><span><span><span># [[cos(t),   -sin(t),          0,          0],</span>
</span></span><span><span><span>#  [sin(t),    cos(t),          0,          0],</span>
</span></span><span><span><span>#  [     0,         0,    cos(t2),   -sin(t2)],</span>
</span></span><span><span><span>#  [     0,         0,    sin(t2),    cos(t2)]]</span>
</span></span><span><span><span>def</span> <span>get_rotation_matrix</span><span>(</span><span>m</span><span>,</span> <span>embed_size</span><span>=</span><span>EMBED_SIZE</span><span>):</span>
</span></span><span><span>  <span>thetas</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>10000</span> <span>**</span> <span>(</span><span>-</span><span>(</span><span>2</span> <span>*</span> <span>(</span><span>i</span> <span>//</span> <span>2</span><span>))</span> <span>/</span> <span>embed_size</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embed_size</span><span>)])</span>
</span></span><span><span>  <span>thetas</span> <span>*=</span> <span>m</span>
</span></span><span><span>  <span>rotation</span> <span>=</span> <span>torch</span><span>.</span><span>eye</span><span>(</span><span>embed_size</span><span>)</span>
</span></span><span><span>  <span>rotation</span> <span>*=</span> <span>thetas</span><span>.</span><span>cos</span><span>()</span>
</span></span><span><span>  <span>col_vals</span> <span>=</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>embed_size</span><span>,</span> <span>2</span><span>)</span>
</span></span><span><span>  <span>row_vals</span> <span>=</span> <span>col_vals</span> <span>+</span> <span>1</span>
</span></span><span><span>  <span>rotation</span><span>[</span><span>col_vals</span><span>,</span> <span>row_vals</span><span>]</span> <span>=</span> <span>-</span><span>thetas</span><span>.</span><span>sin</span><span>()[::</span><span>2</span><span>]</span>
</span></span><span><span>  <span>rotation</span><span>[</span><span>row_vals</span><span>,</span> <span>col_vals</span><span>]</span> <span>=</span> <span>thetas</span><span>.</span><span>sin</span><span>()[::</span><span>2</span><span>]</span>
</span></span><span><span>  <span>return</span> <span>rotation</span><span>.</span><span>T</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_rotation_matrices</span><span>(</span><span>context_size</span><span>=</span><span>CONTEXT_SIZE</span><span>,</span> <span>embed_size</span><span>=</span><span>EMBED_SIZE</span><span>):</span>
</span></span><span><span>  <span>rotations</span> <span>=</span> <span>[</span><span>get_rotation_matrix</span><span>(</span><span>m</span><span>,</span> <span>embed_size</span><span>)</span> <span>for</span> <span>m</span> <span>in</span> <span>range</span><span>(</span><span>context_size</span><span>)]</span>
</span></span><span><span>  <span>return</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>rotations</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span></code></pre></div><h3 id="using-the-fast-way">Using the fast way</h3>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># e.g. for embed_size=4</span>
</span></span><span><span><span># | x1 |         | cos(t)  |         | -x2 |         | sin(t)  |</span>
</span></span><span><span><span># | x2 |    *    | cos(t)  |    +    |  x1 |    *    | sin(t)  |</span>
</span></span><span><span><span># | x3 |         | cos(t2) |         | -x4 |         | sin(t2) |</span>
</span></span><span><span><span># | x3 |         | cos(t2) |         |  x3 |         | sin(t2) |</span>
</span></span><span><span><span>def</span> <span>get_efficient_rotation_matrix</span><span>(</span><span>context_size</span><span>,</span> <span>embed_size</span><span>=</span><span>EMBED_SIZE</span><span>):</span>
</span></span><span><span>  <span>thetas</span> <span>=</span> <span>torch</span><span>.</span><span>tensor</span><span>([</span><span>10000</span> <span>**</span> <span>(</span><span>-</span><span>(</span><span>2</span> <span>*</span> <span>(</span><span>i</span> <span>//</span> <span>2</span><span>))</span> <span>/</span> <span>embed_size</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>embed_size</span><span>)])</span>
</span></span><span><span>  <span>cos_</span> <span>=</span> <span>[(</span><span>m</span> <span>*</span> <span>thetas</span><span>)</span><span>.</span><span>cos</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>range</span><span>(</span><span>context_size</span><span>)]</span>
</span></span><span><span>  <span>sin_</span> <span>=</span> <span>[(</span><span>m</span> <span>*</span> <span>thetas</span><span>)</span><span>.</span><span>sin</span><span>()</span> <span>for</span> <span>m</span> <span>in</span> <span>range</span><span>(</span><span>context_size</span><span>)]</span>
</span></span><span><span>  <span>return</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>cos_</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>),</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>sin_</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>compute_efficient_rotation_matrices</span><span>(</span><span>x</span><span>):</span>
</span></span><span><span>  <span>B</span><span>,</span> <span>C</span><span>,</span> <span>E</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>
</span></span><span><span>  <span>cos_</span><span>,</span> <span>sin_</span> <span>=</span> <span>get_efficient_rotation_matrix</span><span>(</span><span>C</span><span>,</span> <span>E</span><span>)</span>
</span></span><span><span>  <span>x1</span> <span>=</span> <span>x</span><span>[:,</span> <span>:,</span> <span>::</span><span>2</span><span>]</span>
</span></span><span><span>  <span>x2</span> <span>=</span> <span>-</span><span>x</span><span>[:,</span> <span>:,</span> <span>1</span><span>::</span><span>2</span><span>]</span>
</span></span><span><span>  <span>y</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>((</span><span>x2</span><span>,</span> <span>x1</span><span>),</span> <span>dim</span><span>=-</span><span>1</span><span>)</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>,</span> <span>E</span><span>)</span>
</span></span><span><span>  <span>rotated</span> <span>=</span> <span>x</span> <span>*</span> <span>cos_</span> <span>+</span> <span>y</span> <span>*</span> <span>sin_</span>
</span></span><span><span>  <span>return</span> <span>rotated</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>apply_rope</span><span>(</span><span>x</span><span>,</span> <span>fast</span><span>=</span><span>True</span><span>):</span>
</span></span><span><span>  <span>if</span> <span>fast</span><span>:</span> <span>return</span> <span>compute_efficient_rotation_matrices</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>  <span>B</span><span>,</span> <span>C</span><span>,</span> <span>E</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>
</span></span><span><span>  <span>return</span> <span>(</span><span>x</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>,</span> <span>1</span><span>,</span> <span>E</span><span>)</span> <span>@</span> <span>get_rotation_matrices</span><span>(</span><span>C</span><span>,</span> <span>E</span><span>))</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span> <span>C</span><span>,</span> <span>E</span><span>)</span>
</span></span></code></pre></div><h3 id="applying-it-in-the-attention-code">Applying it in the Attention code</h3>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MultiheadAttention</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>  <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>
</span></span><span><span>  <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>    <span>B</span><span>,</span> <span>C</span><span>,</span> <span>E</span> <span>=</span> <span>x</span><span>.</span><span>shape</span>
</span></span><span><span>    <span># pre-layernorm</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>self</span><span>.</span><span>ln</span><span>(</span><span>x</span><span>)</span>
</span></span><span><span>    <span>q</span><span>,</span> <span>k</span><span>,</span> <span>v</span> <span>=</span> <span>self</span><span>.</span><span>qkv</span><span>(</span><span>x</span><span>)</span><span>.</span><span>chunk</span><span>(</span><span>3</span><span>,</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
</span></span><span><span>    <span># --- RoPE ---</span>
</span></span><span><span>    <span>q</span> <span>=</span> <span>apply_rope</span><span>(</span><span>q</span><span>)</span>
</span></span><span><span>    <span>k</span> <span>=</span> <span>apply_rope</span><span>(</span><span>k</span><span>)</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span></code></pre></div><h2 id="alibi">ALiBi</h2>
<p>ALiBi is also applied in the Attention block, but this time it’s applied on top of the <code>Attention Scores</code> instead of the <code>Query</code> and <code>Key</code>. It is meant to represent the relative distance between tokens instead of the absolute position of a token in the sequence.</p>
<p>Intuitively this is very seductive. Relative position means that it should not “duplicate” tokens, and in theory it should scale to arbitrary offsets (for arbitrary context size). On the flip side it seems to me to be a very “weak” way to signal position. It influences how relevant a token is to the current one based on distance. This seems pretty fuzzy, and not a robust way to encode distance.</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># e.g.: 4-dimensional mask for decoder-only</span>
</span></span><span><span><span># [[ 0., -0., -0., -0.],</span>
</span></span><span><span><span>#  [-1.,  0., -0., -0.],</span>
</span></span><span><span><span>#  [-2., -1.,  0., -0.],</span>
</span></span><span><span><span>#  [-3., -2., -1.,  0.]]</span>
</span></span><span><span><span>#</span>
</span></span><span><span><span># e.g.: 4dimensional mask for encoder-only</span>
</span></span><span><span><span># [[ 0., -1., -2., -3.],</span>
</span></span><span><span><span>#  [-1.,  0., -1., -2.],</span>
</span></span><span><span><span>#  [-2., -1.,  0., -1.],</span>
</span></span><span><span><span>#  [-3., -2., -1.,  0.]]</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>alibi_distances</span><span>(</span><span>n_context</span><span>,</span> <span>is_encoder</span><span>=</span><span>True</span><span>):</span>
</span></span><span><span>    <span>diag_dists</span> <span>=</span> <span>torch</span><span>.</span><span>cumsum</span><span>(</span><span>torch</span><span>.</span><span>triu</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>n_context</span><span>,</span> <span>n_context</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)),</span> <span>dim</span><span>=</span><span>1</span><span>)</span> <span>*</span> <span>-</span><span>1</span>
</span></span><span><span>    <span>diag_dists</span><span>[</span><span>diag_dists</span> <span>!=</span> <span>0</span><span>]</span> <span>+=</span> <span>1</span>
</span></span><span><span>
</span></span><span><span>    <span>dists</span> <span>=</span> <span>diag_dists</span><span>.</span><span>transpose</span><span>(</span><span>-</span><span>2</span><span>,</span> <span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span>if</span> <span>is_encoder</span><span>:</span>
</span></span><span><span>        <span>dists</span> <span>=</span> <span>dists</span> <span>+</span> <span>diag_dists</span>
</span></span><span><span>    <span>return</span> <span>dists</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>alibi_scalers</span><span>(</span><span>n_heads</span><span>):</span>
</span></span><span><span>    <span># vector with M values in a geometric sequence (starting at 2^-(8/n)) where n == number of heads.</span>
</span></span><span><span>    <span>m_vector</span> <span>=</span> <span>2</span> <span>**</span> <span>-</span><span>((</span><span>8</span> <span>/</span> <span>n_heads</span><span>)</span> <span>*</span> <span>torch</span><span>.</span><span>arange</span><span>(</span><span>1</span><span>,</span> <span>n_heads</span> <span>+</span> <span>1</span><span>)</span><span>.</span><span>to</span><span>(</span><span>device</span><span>))</span>
</span></span><span><span>    <span>return</span> <span>m_vector</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span> <span># reshape to broadcast correctly</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>apply_alibi</span><span>(</span><span>correlation</span><span>,</span> <span>is_encoder</span><span>=</span><span>True</span><span>):</span>
</span></span><span><span>    <span>n_context</span> <span>=</span> <span>correlation</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>1</span><span>]</span>
</span></span><span><span>    <span>n_heads</span> <span>=</span> <span>correlation</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>3</span><span>]</span>
</span></span><span><span>    <span>scaled_dists</span> <span>=</span> <span>alibi_scalers</span><span>(</span><span>n_heads</span><span>)</span> <span>*</span> <span>alibi_distances</span><span>(</span><span>n_context</span><span>,</span> <span>is_encoder</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>correlation</span> <span>+</span> <span>scaled_dists</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>MultiheadAttention</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
</span></span><span><span>  <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>):</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>
</span></span><span><span>  <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span><span><span>    <span>correlation</span> <span>=</span> <span>q</span> <span>@</span> <span>k</span><span>.</span><span>transpose</span><span>(</span><span>-</span><span>2</span><span>,</span> <span>-</span><span>1</span><span>)</span>
</span></span><span><span>    <span>correlation</span> <span>=</span> <span>correlation</span> <span>/</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>k</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>1</span><span>])</span>
</span></span><span><span>    <span># --- ALiBi ---</span>
</span></span><span><span>    <span>correlation</span> <span>=</span> <span>apply_alibi</span><span>(</span><span>correlation</span><span>)</span>
</span></span><span><span>    <span># --- Optionally apply causal mask ---</span>
</span></span><span><span>    <span>if</span> <span>self</span><span>.</span><span>positional</span> <span>==</span> <span>Positional</span><span>.</span><span>ALIBI_AND_CAUSAL</span><span>:</span>
</span></span><span><span>      <span>mask</span> <span>=</span> <span>torch</span><span>.</span><span>tril</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>C</span><span>,</span> <span>C</span><span>))</span><span>.</span><span>to</span><span>(</span><span>device</span><span>)</span>
</span></span><span><span>      <span>correlation</span> <span>=</span> <span>correlation</span><span>.</span><span>masked_fill</span><span>(</span><span>mask</span> <span>==</span> <span>0</span><span>,</span> <span>float</span><span>(</span><span>&#39;-inf&#39;</span><span>))</span>
</span></span><span><span>    <span># &lt;...&gt;</span>
</span></span></code></pre></div><h2 id="compare-schemes-accuracies">Compare Schemes Accuracies</h2>
<p>On the toy task for a sample of 5 runs each, we have:</p>
<figure><a href="https://sagittarius.greg.technology/results.png" title="results" data-thumbnail="results.png" data-sub-html="&lt;h2&gt;Positional Encoding Schemes Accuracies on the toy copy-task&lt;/h2&gt;&lt;p&gt;results&lt;/p&gt;">
        <img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="results.png" data-srcset="results.png, results.png 1.5x, results.png 2x" data-sizes="auto" alt="results.png"/>
    </a><figcaption>Positional Encoding Schemes Accuracies on the toy copy-task</figcaption>
    </figure>
<p>I couldn’t get ALiBi to produce good results on my task. This could be due to:</p>
<ul>
<li>Some bug in my code?</li>
<li>The model being too small.</li>
<li>The task could be a very bad fit for ALiBi.</li>
</ul>
<p>I was advised to use a Causal mask in combination with ALiBi which helped a lot but didn’t place it on a par with the other three.</p>
<p>I also experimented with using only powers of 2 number of multi-headed attention but it didn’t close the gap with other schemes either.</p>
<h2 id="attention-activation">Attention Activation</h2>
<p>Looking at the attention activations for an example is pretty cool :)</p>
<pre tabindex="0"><code>7 1 8 2 &lt;copy&gt; _ _ _ _ _ → 
</code></pre><p><img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="attention_activation_0.png" data-srcset="attention_activation_0.png, attention_activation_0.png 1.5x, attention_activation_0.png 2x" data-sizes="auto" alt="attention_activation_0.png" title="attention_activation_0"/>
<img src="https://sagittarius.greg.technology/svg/loading.min.svg" data-src="attention_activation_1.png" data-srcset="attention_activation_1.png, attention_activation_1.png 1.5x, attention_activation_1.png 2x" data-sizes="auto" alt="attention_activation_1.png" title="attention_activation_1"/></p>
<h2 id="bonus-note-about-transformers">Bonus Note about Transformers</h2>
<p>I spent a lot of time getting no results until I got the godsent advice that: <strong>One-layer models are such a special case – they can’t form induction heads</strong>.</p>
<p>And you can read more about it in this <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank" rel="noopener noreffer ">Anthropic paper</a>.</p>
<h2 id="the-code">The code</h2>
<p>You can get the code at <a href="https://github.com/peluche/self-attention" target="_blank" rel="noopener noreffer ">https://github.com/peluche/self-attention</a></p>
<h2 id="sources">Sources</h2>
<ul>
<li>Sinusoidal / Learned <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreffer ">Attention Is All You Need</a></li>
<li>RoPE <a href="https://arxiv.org/pdf/2104.09864.pdf" target="_blank" rel="noopener noreffer ">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></li>
<li>ALiBi <a href="https://arxiv.org/pdf/2108.12409.pdf" target="_blank" rel="noopener noreffer ">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a></li>
<li>Ofir Press’ <a href="https://github.com/ofirpress/attention_with_linear_biases" target="_blank" rel="noopener noreffer ">github</a>, <a href="https://github.com/ofirpress/attention_with_linear_biases/issues/5" target="_blank" rel="noopener noreffer ">especially this issue</a>, <a href="https://ofir.io/" target="_blank" rel="noopener noreffer ">blog</a>, and <a href="https://www.youtube.com/@ofirpress/videos" target="_blank" rel="noopener noreffer ">YouTube</a></li>
<li>One-Layer Models being weird <a href="https://transformer-circuits.pub/2021/framework/index.html" target="_blank" rel="noopener noreffer ">A Mathematical Framework for Transformer Circuits</a></li>
<li>Nelhage’s <a href="https://blog.nelhage.com/" target="_blank" rel="noopener noreffer ">blog</a></li>
</ul>
</div></div>
  </body>
</html>
