<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.crosslabs.org//blog/diffusion-with-offset-noise">Original</a>
    <h1>Diffusion with Offset Noise: Finetuning SD to generate very dark or light images</h1>
    
    <div id="readability-page-1" class="page"><p>Fine-tuning against a modified noise, enables Stable Diffusion to generate very dark or light images easily.</p><div><p><a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf" target="_blank">Denoising Diffusion Probabilistic Models</a> are a relatively new form of generative neural network model - models which produce samples from a high-dimensional probability distribution learned from data. Other approaches to the same class of problem include Generative Adversarial Networks, Normalizing Flows, and various forms of autoregressive models that sample dimensions one at a time or in blocks. One of the major applications of this kind of modelling is in image synthesis, and diffusion models have recently been very competitive with regards to image quality, particularly with regards to producing globally coherent composition across the image. </p><p><a href="https://stability.ai/blog/stable-diffusion-public-release" target="_blank">Stable Diffusion</a> is a pre-trained, publicly available model that can use this technique to produce some stunning results. However, it has an interesting limitation that seems to have mostly gone unnoticed. If you try to ask it to generate images that should be particularly dark or light, it almost always generates images whose average value is relatively close to 0.5 (with an entirely black image being 0, and an entirely white image being 1). For example:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d4b445644ead435a37a928_1080x1080-4x4-grid-basemodel1.jpg" loading="lazy" alt=""/></p><figcaption>Top left: A dark alleyway in a rainstorm (0.301); Top right: Monochrome line-art logo on a white background (0.709); Bottom left: A snow-covered ski slope on a sunny day (0.641); Bottom right: A town square lit only by torchlight (0.452)</figcaption></figure><p>But why is it doing this? Am I just imagining the effect and these results are ‘correct’? Is it just a matter of the training data, something about the architecture, <strong>or something about diffusion models in general?</strong> (It was the last).</p><p>First, though, to make sure I wasn’t just imagining things, I tried fine-tuning Stable Diffusion against a single solid black image. Generally fine-tuning Stable Diffusion (SD) works pretty well - there’s a technique called Dreambooth to teach SD new, specific concepts like a particular person’s face or a particular cat, and a few dozen images and a few thousand gradient updates are enough for the model to learn what that particular subject looks like. Extend that to ten thousand steps and it can start to memorize specific images.</p><p>But when I fine-tuned against this single, solid black image, even after 3000 steps I was still getting results like this for “A solid black image”:</p><figure class="w-richtext-align-normal w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d35f2d19e3bc9f34b8f59f_YZq5t9PWHMg529mQVsnkxPa1Xrpqqtfzmndj0t9v9RI40wM104EGBbLsMpx3LtxzdngAW7SW7No_VlX_E4tDPJSOqCsL0qbq1HwvhiSW3JtplIuyf5R1md835T0o2oP9eDWbhemSUshyTsXyF9sySRilGRYpJuZyYWFo4ojcmeFRio3UwuHxeFUaGla1Qw.png" alt=""/></p><figcaption>Using the prompt: &#34;A solid black image&#34;</figcaption></figure><p>Well, not without changing one thing about it.</p><p>To understand what’s going on, it helps to example what exactly a diffusion model is learning to reverse. The usual way diffusion models are formulated is as the inverse of a particular forward stochastic process - repeated addition of small amounts of ‘independently and identically distributed’ (iid) Gaussian noise. That is to say, each pixel in the latent space receives its own random sample at each step. The diffusion model learns to take, say, an image after some number of these steps have been performed, and to figure out the direction to go in to follow that trajectory back to the original image. Given this model that can ‘step backwards towards a real image’, you start with pure noise and reverse the noising process to get a novel image.</p><p>The issue turns out to be that you don’t ever <strong>completely</strong> erase the original image during the forward process, so in turn the reverse model starting from pure noise doesn’t exactly get back to the complete true distribution of images. Instead, those things which noise destroys <strong>last</strong> are in turn most <strong>weakly </strong>altered by the reverse process - those things are instead inherited from the latent noise sample that is used to start to process. </p><p>It might not be obvious at first glance, but if you look at the forward process and how it disrupts an image, the longer wavelength features take longer for the noise to destroy:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-video"><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/3C6yEYXDijM" title="Noise process"></iframe></p></figure><p>‍</p><figure class="w-richtext-align-floatright w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d3601ff8a046255cf86b16_noise_graph3.png" loading="lazy" alt=""/></p></figure><p>That’s why for example using the same latent noise seed but different prompts tends to give images that are related to each-other at the level of overall composition, but not at the level of individual textures or small-scale patterns. The diffusion process doesn’t know how to change those long-wavelength features. And the longest wavelength feature is the average value of the image as a whole, which is <strong>also</strong> the feature that is least likely to vary between independent samples of the latent noise. </p><p>This problem gets worse the higher the dimensionality of the target object is, because the standard deviation of a set of independent noise samples scales like 1/N. So if you’re generating a 4d vector this might not be much of a problem - you just need twice as many samples to get the lowest-frequency component as for the highest frequency component. But in Stable Diffusion at 512x512 resolution, you’re generating a 3 x 64^2 = 12288 dimensional object. So the longest wavelengths change about a factor of 100 slower than the shortest ones, meaning you’d need to be considering hundreds or thousands of steps of the process to capture that, when the default is around 50 (or for some sophisticated samplers, as low as 20).</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d3c49ba6b96f54fae8def9_1020-graphs.jpg" loading="lazy" alt=""/></p></figure><p>It does seem like increasing the number of sampling steps does help SD make more extreme images, but we can do a bit better and make a drop-in solution.</p><p>The trick has to do with the structure of the noise that we teach a diffusion model to reverse. Because we’re using iid samples, we have this 1/N. But what if we use noise that looks like an iid sample per pixel added to a single iid sample that is the same over the entire image instead? </p><p>In code terms, the current training loop uses noise that looks like: </p><h6>noise = torch.randn_like(latents)</h6><h6>noise = torch.randn_like(latents) + 0.1 * torch.randn(latents.shape[0], latents.shape[1], 1, 1)</h6><p>Fine-tuning with noise like this for a thousand steps or so on just 40 hand-labeled images is enough to significantly change the behavior of Stable Diffusion, without making it get any worse at the things it could previously generate. Here are the results of the four prompts higher up in the article for comparison:</p><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d4b4792108fe7f6b4ebe5c_1080x1080-4x4-grid-offset1.jpg" loading="lazy" alt=""/></p><figcaption>Top right: A dark alleyway in a rainstorm (0.032); Top left: Monochrome line-art logo on a white background (0.974); Bottom left: Snow-covered ski slope on a sunny day (0.858); Bottom right: A town square lit only by torchlight. (0.031)</figcaption></figure><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d3ce86df6ef1b3e1e58fe8_1080-additional-starry.jpg" loading="lazy" alt=""/></p><figcaption>A starry sky before and after offset noise.</figcaption></figure><figure class="w-richtext-align-fullwidth w-richtext-figure-type-image"><p><img src="https://uploads-ssl.webflow.com/5e338abdcf951ffb0e1fadb1/63d3ce985fed08ce7da4651b_1080-additional-dark.jpg" loading="lazy" alt=""/></p><figcaption>Superheroes fighting plant monsters in a dark alley before and after offset noise.</figcaption></figure><p>‍</p><p>There are a number of papers talking about changing the noise schedule of denoising diffusion models, as well as using <a href="https://arxiv.org/pdf/2106.07582.pdf" target="_blank">different distributions</a> than Gaussian for the noise, or even <a href="https://arxiv.org/pdf/2208.09392.pdf" target="_blank">removing noise altogether</a> and instead using other destructive operations like blurring or masking. However, most of the focus seems to be on accelerating the process of inference - being able to use fewer steps, basically. There doesn’t seem to be as much attention on how design decisions about the noise (or image-destroying operation) could constrain the types of images that can easily be synthesized. However, it’s quite relevant for the aesthetic and artistic uses of these models. </p><p>For an individual artist who is digging a bit into customizing these models and doing their own fine-tuning, adjusting to use this offset noise for one project or another wouldn’t be so difficult. You could just use <a href="https://drive.google.com/file/d/1Ah0RDr2RmPwaDmZOZz5LA2Wx_u7-NPgb/view?usp=share_link">our checkpoint</a> if you like (please read the note at the end before accessing this file) , for that matter. But with fine-tuning on a small number of images like this, the results aren’t ever going to be quite as general or quite as good as large projects could achieve.</p><p>So I’d like to conclude this with a request to those involved in training these large models: please incorporate a little bit of offset noise like this into the training process the next time you do a big run. It should significantly increase the expressive range of the models, allowing much better results for things like logos, cut-out figures, naturally bright and dark scenes, scenes with strongly colored lighting, etc. It’s a very easy trick!</p><p>‍</p><p>NOTE: We wanted to acknowledge that we were recently made aware of a trojan virus in the original file we uploaded. In an abundance of caution, we made the file private until we fully investigated the issue. After testing on multiple devices and with multiple anti-virus software programs, we were not able to replicate the finding of the trojan.</p><p>However, in light of the event, we have taken additional steps to enhance the security of our file and community. We are now posting a new file which uses SafeTensors for added security, as well as providing a checksum for our file a5d6ee70bf9edf1527a1659900eb1248 (md5sum), as we have also discovered that third-party sites are hosting the original checkpoint with Offset Noise and we would like to maintain the integrity of the file.The resources used for this checkpoint were Stable Diffusion&#39;s 1.5 model from runwayml on huggingface and royalty free images from Pexels. Finally, we provide this file as is, please use it under your own risk and considerations regarding your device&#39;s safety.</p></div></div>
  </body>
</html>
