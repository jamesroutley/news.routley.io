<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.daily.co/blog/video-live-streaming/">Original</a>
    <h1>Video Live Streaming: Notes on RTMP, HLS, and WebRTC</h1>
    
    <div id="readability-page-1" class="page"><article>

                

                        <figure>
                            <img srcset="/blog/content/images/size/w300/2022/05/Directory-Image--32--1.png 300w,
                                    /blog/content/images/size/w600/2022/05/Directory-Image--32--1.png 600w,
                                    /blog/content/images/size/w1000/2022/05/Directory-Image--32--1.png 1000w,
                                    /blog/content/images/size/w2000/2022/05/Directory-Image--32--1.png 2000w" sizes="(max-width: 800px) 400px,
                                (max-width: 1170px) 1170px,
                                    2000px" src="https://www.daily.co/blog/content/images/size/w2000/2022/05/Directory-Image--32--1.png" alt="Video live streaming: Notes on RTMP, HLS, and WebRTC"/>
                        </figure>

                <section>
                    <div>
                        <!--kg-card-begin: markdown--><p>These days, when people talk about &#34;live streaming,&#34; they might be talking about three quite different underlying technologies.</p>
<ul>
<li>RTMP is widely used to send video into a live session, but is rarely used for viewing video streams.</li>
<li>HLS is how platforms like Twitch deliver live streams to large audiences.</li>
<li>WebRTC was designed to support interactive use cases like video calls, and is the underlying technology in applications like Google Meet, Microsoft Teams video, and Discord video.</li>
</ul>
<p>Why are there three standards?</p>
<p>Well, partly these standards evolved over time, and are still evolving. Our computers and phones and networks continue to get faster, people keep thinking up new things to do with faster computers and networks, and better and better technical building blocks are created to support these new things we&#39;re all doing.</p>
<p>But also, delivering video at scale is challenging, and engineering trade-offs abound. HLS and WebRTC are nice examples of two (good) approaches to solving a problem (video streaming) that optimize for very different aspects of that problem.</p>
<h2 id="from-rtmp-to-hls-two-decades-of-progress-in-live-streaming">From RTMP to HLS: Two decades of progress in live streaming</h2>
<p>RTMP is 20 years old, and was originally developed by Macromedia for the Flash server and player. That&#39;s pretty ancient technology in Internet video terms. But because RTMP has been around so long, it&#39;s widely supported as a way to send live video from one computer system to another.</p>
<p>If you are a Twitch streamer, for example, you&#39;re almost certainly using RTMP to send video from your gaming machine/music studio/vibe lab to Twitch&#39;s cloud infrastructure.</p>
<p>On the other hand, if you&#39;re watching a stream on Twitch, you&#39;re not watching that original RTMP stream. Twitch uses a much newer standard, HLS, to deliver video to the millions and millions of people who watch live streams every day.</p>
<p>There are quite a few nice things about HLS, but two advantages in particular over RTMP.</p>
<p>First, HLS supports multiple bitrates and allows viewing clients to switch between multiple bitrates dynamically. This is generally important for video delivery at scale, and it&#39;s especially important for live streams. If I&#39;m watching a video on my phone on a not-very-great cellular data connection, I have a lot less bandwidth available than if I were watching the same video at home where my Internet connection is (usually) very fast.</p>
<p>What this means is that I actually need to watch a different encoding of the video if I&#39;m on my cellular data connection than if I&#39;m using my home internet. On my phone, I just want to see the video and I know the quality&#39;s not going to be great. At home, I want the best quality video available.</p>
<p>A further wrinkle is that the bandwidth I have available might change while I’m watching the video. My phone could move closer to a cell tower and have faster service. Or at home, my laptop might decide to download a big system update which could cut in half the bandwidth available for video.</p>
<h2 id="just-the-chunks-ma%E2%80%99am">Just the chunks, ma’am</h2>
<p>To support dynamic playback bitrates, HLS video is packaged as a series of short time chunks. Each chunk is encoded at several different bitrates. A typical HLS &#34;bitrate ladder&#34; today will include five encodings ranging from about 6 megabits per second on the high end to about 250 kilobits per second at the bottom. (Higher bitrates deliver higher video quality.)</p>
<p>Video players that support HLS are able to continuously monitor approximately how much bandwidth is available and to switch to downloading smaller-bitrate or larger-bitrate chunks.</p>
<p>The metadata about chunks is stored in manifest files, which are usually named <em>something</em>.m3u8. If you open Chrome devtools and start playing a video on Twitch, you can look in the network panel for fetches of .m3u8 file segments, and see a bunch of interesting information about the HLS encoding!</p>
<h2 id="standing-on-the-shoulders-of-giants">Standing on the shoulders of giants</h2>
<p>The second advantage that HLS has over RTMP is that the HLS chunks can be delivered efficiently by HTTP CDNs like Cloudflare, Fastly, Akamai, and Cloudfront.</p>
<p>&#34;HLS&#34; is an acronym that expands to &#34;HTTP Live Streaming.&#34; HLS chunks and manifests mostly just look like files. Players fetch manifests and chunks with normal HTTP requests. So HLS benefits from the amazing capabilities of modern CDNs.</p>
<p>RTMP, on the other hand, is its own TCP-level thing. It is a streaming-oriented protocol, rather than a request-oriented protocol like HLS. To send video and audio over an RTMP connection, you open up a TCP socket and you start pushing RTMP-formatted data over that socket.</p>
<p>An enormous amount of infrastructure engineering work has gone into making HTTP fast, scalable, and cost effective everywhere in the world. The CDN-friendly nature of HLS was a core design goal and is a big deal.</p>
<p>But there&#39;s a catch. Latency. Which will bring us to WebRTC.</p>
<h2 id="but-first%E2%80%A6-an-interlude-on-latency">But first… an interlude on latency</h2>
<p>Let&#39;s do some hand-waving and define the latency we&#39;re interested in for live streaming as &#34;the end-to-end delay between a video sender and a video viewer.&#34;</p>
<p>Here&#39;s a very schematic view of the stages that video bits go through to get from a video sender to a viewer, relayed through a media server:</p>
<ol>
<li>capture (for example, a webcam)</li>
<li>encoding (into a compressed video format like AVC)</li>
<li>transmission to the media server (move those bytes across the network somehow)</li>
<li>processing the video on the media server</li>
<li>transmission to the viewer (move those bytes across the network again)</li>
<li>playback (decode, decompress, and finally show the video on a screen)</li>
</ol>
<p>For each of these steps, there are complicated trade-offs between video quality, reliability, cost, and latency.</p>
<p>With today&#39;s fast computers and nifty video codecs it&#39;s possible to compress video at fairly high quality with just a few tens of milliseconds of latency delay. Similarly, networking in general is pretty fast. We&#39;ve gotten good at routing packets on the Internet. Steps 1-6 typically add up to somewhere between 50ms and 300ms.</p>
<p>Except... for possible knock-on effects from how we choose to encode, package, and transfer the video data. (Steps two through four, above.) The chunked, request-oriented approach of HLS forces a lot of extra latency at several stages.</p>
<p>Six seconds is a typical HLS chunk length. To oversimplify a little bit, that means there&#39;s six seconds of latency added to the pipeline while we encode, package, and upload each chunk. Then our CDN has to fetch and cache the chunks. And finally, on the viewer side, most players will download two full chunks and cache them locally before starting to play the video while the third chunk is downloading.</p>
<p>Our latency is now somewhere between 12 and 20 seconds.</p>
<p>It&#39;s possible to push HLS latency down by using shorter chunk sizes, streaming the chunk uploads to our CDN origin server, extending our CDN&#39;s low-level mechanics so that data can start propagating while chunks are in flight, and optimizing buffering on the playback side.</p>
<p>All of these changes make playback less resilient and cut against the core engineering trade-off at the heart of HLS: optimizing video for delivery via HTTP infrastructure.</p>
<p>Below chunk sizes of about one second, we lose most of the benefits of delivering video via CDNs. A request-oriented approach like HLS can therefore get down to about two seconds of latency in the best case, and four or five seconds of latency in the typical case.</p>
<p>If our target latency is lower than that, we&#39;ll need to take a different approach.</p>
<h2 id="webrtc-%E2%80%93-an-industry-standard-for-low-latency-audio-and-video">WebRTC – an industry standard for low-latency audio and video</h2>
<p>WebRTC is a stream-oriented standard (like RTMP) that supports adaptive bitrates and is natively supported in today&#39;s web browsers (like HLS). Low latency was the primary design goal of WebRTC.</p>
<p>With WebRTC, video encoding happens on the sending side and video is sent as a continuous stream. WebRTC media servers don&#39;t usually transcode the video. When possible, WebRTC connections are UDP rather than TCP, which lowers networking overhead and latency. And, finally, on the receiving side, a typical playback buffer is 30ms, rather than HLS&#39;s typical multi-second buffers.</p>
<p>All of this together means that WebRTC end-to-end latency will usually be between 50ms and 200ms. That&#39;s low enough to work well for two people having a conversation and is in line with typical latencies for telephone calls.</p>
<p>WebRTC&#39;s low latency comes with two big trade-offs.</p>
<p>First, the approach to maintaining video quality has to be different from the approach that HLS relies on. And second, it&#39;s harder to support large audiences with WebRTC.</p>
<h2 id="bandwidth-and-packet-loss">Bandwidth and packet loss</h2>
<p>From a network engineer&#39;s perspective, video quality hinges on the answers to just two simple questions:</p>
<ol>
<li>How many video packets can you push through a network connection? (This is typically called &#34;bandwidth.&#34;) And,</li>
<li>How reliably do those packets arrive within a specific latency window? (This is a combination of packet loss and jitter.)</li>
</ol>
<p>These two metrics are both important, and because a network stack is a complicated <a href="https://en.wikipedia.org/wiki/OSI_model">layer cake of protocols</a>, they interact with each other in interesting and sometimes surprising ways. In all cases, though, the end user experience of a “worse network” is the same because there are only two degrees of freedom available to the video stack: the video either pauses while the player buffers (or re-buffers), or the user sees a lower-quality (lower-bitrate) version of the video.</p>
<p>Very approximately, as packet loss or jitter go up, the effective bandwidth of a connection goes down. However, it&#39;s fair to say that at moderate levels of packet loss, if you don&#39;t care too much about latency, you can just buffer a lot of packets and not think too much about packet loss and jitter.</p>
<p>Buffering a lot of packets — many seconds worth of packets — is the approach that HLS takes to maintaining video quality in the face of real-world network behavior.</p>
<h2 id="microwave-ovens-and-latency-budgets">Microwave ovens and latency budgets</h2>
<p>Let&#39;s say you&#39;re using your home wifi connection to watch a movie on Netflix. You decide to microwave some popcorn. Your wifi connection is using one of the 2.4ghz bands, so packet loss on the network spikes terribly as soon as the microwave oven turns on.</p>
<p>HLS can deal with this pretty well. There&#39;s already several seconds of video cached, so that video keeps playing just fine. The player will notice that the in-progress chunk downloads are taking longer. If the packet loss spike was shorter than a few seconds, TCP retries will probably just compensate for the packet loss and the player won&#39;t need to do anything at all. If the packet loss continues, the player has plenty of time to decide whether to switch gears and start downloading lower-bitrate chunks.</p>
<p>The very tight latency budget for WebRTC makes for a completely different situation.</p>
<p>WebRTC playback buffers usually don&#39;t hold more than 50ms or so of video packets. If there&#39;s a big packet loss spike, the player can&#39;t just keep rendering cached video data. And the round-trip time to re-request a lost packet will often be slow enough that it&#39;s not worth asking the server to send us packets we missed. Finally, the player has to make any decision about hopping down to a lower bitrate very quickly.</p>
<p>All of this together means that the typical WebRTC failure mode for dropped packets is just to skip that packet and continue playing the video as well as possible.</p>
<p>In a real-time video call, video quality issues look like small framerate glitches, worsening to either visual corruption or long freezes if a video keyframe is missed. To compensate for packet loss, the media server and the player <strong>can</strong> cooperate to choose a lower bitrate stream. But that probably won&#39;t happen quickly enough to avoid visual artifacts. To compensate for all of this, WebRTC implementations usually default to somewhat lower bitrates than HLS implementations do.</p>
<p>HLS is more resilient to network issues, at the cost of much higher latency than WebRTC.</p>
<p>WebRTC can deliver sub-200ms latencies in the average case (and sub-400ms latencies almost always) but at somewhat lower average video quality.</p>
<p>If interactive latencies – latencies below 400ms – are the goal, WebRTC&#39;s trade-offs are clearly the right ones.</p>

<p>The other big difference between HLS and WebRTC is how difficult it is to support large audiences for a live stream and large numbers of users across a service or application.</p>
<p>HLS offloads to HTTP CDNs most of the complexity of scaling up distribution. This is great, because modern CDNs are very good.</p>
<p>WebRTC is newer than HTTP, and vastly fewer engineering hours have gone into building infrastructure at scale for WebRTC than for HTTP.</p>
<p>However, more and more work is going into WebRTC infrastructure, because low-latency video is more and more widely used. A number of companies now offer scalable WebRTC infrastructure as a service. And an increasing number of open source projects provide excellent building blocks for deploying production infrastructure and for experimenting with new approaches.</p>
<h2 id="improving-webrtc-infrastructure">Improving WebRTC infrastructure</h2>
<p>The big challenges involved in building out a &#34;WebRTC CDN&#34; are:</p>
<ul>
<li>
<p>Media servers need to copy incoming UDP video and audio packets and route the copies to each viewer of a live stream. This has to be done fast. This copy-and-routing job is relatively inexpensive from a compute perspective, and it&#39;s relatively simple conceptually. But down at the implementation level, there are a number of tricky components, such as estimating the bandwidth available to each viewer and switching to different bitrates as needed.</p>
<p>At some point, the number of packets that need to be routed exceeds the limits of a single machine. So it&#39;s necessary to implement cascading or mesh networking between servers.</p>
<p>Relatedly, the building blocks that are now standard for horizontally scaling HTTP and other request-oriented workloads don’t really work for WebRTC servers. So to scale an application or service (lots of live sessions in parallel) requires writing custom scale-out/scale-in logic, implementing appropriate service discovery, creating new monitoring and observability tooling, etc, etc.</p>
</li>
<li>
<p>Packet loss and jitter are generally much lower when connecting to nearby servers than to servers that are far away. So if the audience for a live stream is geographically distributed, it&#39;s important to have a distributed infrastructure of media servers and (again) necessary to implement mesh or cascading server-to-server media transit.</p>
</li>
<li>
<p>Encoding a stream in real time for playback with very small buffers is challenging. The current state of the art is to encode to three bitrates (rather than to five or six, as is typical with HLS).</p>
</li>
</ul>
<p>Today, WebRTC is mature enough that for live streams with 15,000 viewers, it&#39;s often preferable to use WebRTC over HLS. Low latency allows features like bringing viewers onto a &#34;stage&#34; to participate in the live stream, interactive audience features like polls and emoji reactions, and real-time bidding in live auctions.</p>
<p>As WebRTC infrastructure continues to improve, larger and larger low-latency live streams are likely to become more and more widely used.</p>


<!--kg-card-end: markdown-->
                    </div>
                </section>


            </article></div>
  </body>
</html>
