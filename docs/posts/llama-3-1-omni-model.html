<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ictnlp/LLaMA-Omni">Original</a>
    <h1>Llama 3.1 Omni Model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<blockquote>
<p dir="auto"><strong>Authors: <a href="https://fangqingkai.github.io/" rel="nofollow">Qingkai Fang</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=XwHtPyAAAAAJ" rel="nofollow">Shoutao Guo</a>, <a href="https://zhouyan19.github.io/zhouyan/" rel="nofollow">Yan Zhou</a>, <a href="https://scholar.google.com.hk/citations?user=dUgq6tEAAAAJ" rel="nofollow">Zhengrui Ma</a>, <a href="https://zhangshaolei1998.github.io/" rel="nofollow">Shaolei Zhang</a>, <a href="https://people.ucas.edu.cn/~yangfeng?language=en" rel="nofollow">Yang Feng*</a></strong></p>
</blockquote>
<p dir="auto"><a href="https://arxiv.org/abs/2409.06666" rel="nofollow"><img src="https://camo.githubusercontent.com/cf848c9cd376e15932b0cba092d5c6053463835849a56eac0c6a011d3ab107d5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323430392e30363636362d6233316231622e7376673f6c6f676f3d6172586976" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2409.06666-b31b1b.svg?logo=arXiv"/></a>
<a href="https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni" rel="nofollow"><img src="https://camo.githubusercontent.com/43ed5a97ea9a77d1afe95566b086cb8aed400254b78842e11ed83614f503df95/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e675f466163652d4d6f64656c2d626c75652e737667" alt="model" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging_Face-Model-blue.svg"/></a>
<a href="https://github.com/ictnlp/LLaMA-Omni"><img src="https://camo.githubusercontent.com/77762e523530a02dd6ad9f1b9e4e9dd91c0bf748c684bd5cced64a3302047441/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4769746875622d436f64652d6b657967656e2e7376673f6c6f676f3d676974687562" alt="code" data-canonical-src="https://img.shields.io/badge/Github-Code-keygen.svg?logo=github"/></a></p>
<p dir="auto">LLaMA-Omni is a speech-language model built upon Llama-3.1-8B-Instruct. It supports low-latency and high-quality speech interactions, simultaneously generating both text and speech responses based on speech instructions.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/ictnlp/LLaMA-Omni/blob/main/images/model.png"><img src="https://github.com/ictnlp/LLaMA-Omni/raw/main/images/model.png" width="75%"/></a></p>

<ul dir="auto">
<li>
<p dir="auto">üí™ <strong>Built on Llama-3.1-8B-Instruct, ensuring high-quality responses.</strong></p>
</li>
<li>
<p dir="auto">üöÄ <strong>Low-latency speech interaction with a latency as low as 226ms.</strong></p>
</li>
<li>
<p dir="auto">üéß <strong>Simultaneous generation of both text and speech responses.</strong></p>
</li>
<li>
<p dir="auto">‚ôªÔ∏è <strong>Trained in less than 3 days using just 4 GPUs.</strong></p>
</li>
</ul>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19513464/366072569-2b097af8-47d7-494f-b3b3-6be17ca0247a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjY3MjMyNTQsIm5iZiI6MTcyNjcyMjk1NCwicGF0aCI6Ii8xOTUxMzQ2NC8zNjYwNzI1NjktMmIwOTdhZjgtNDdkNy00OTRmLWIzYjMtNmJlMTdjYTAyNDdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTE5VDA1MTU1NFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczMjBmZDIxZTg4MDc4MzMwMDRlNGVmMTU3Yzk4MDk3YzE1ZDk2MTQxZjliZDZmM2U2OWU4ZGI4NWQyOGMxNzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.LwAH1m_VmxeIl318qe13D2e04ukm-VrlchYfoXc3U4Q" data-canonical-src="https://private-user-images.githubusercontent.com/19513464/366072569-2b097af8-47d7-494f-b3b3-6be17ca0247a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjY3MjMyNTQsIm5iZiI6MTcyNjcyMjk1NCwicGF0aCI6Ii8xOTUxMzQ2NC8zNjYwNzI1NjktMmIwOTdhZjgtNDdkNy00OTRmLWIzYjMtNmJlMTdjYTAyNDdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MTklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTE5VDA1MTU1NFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTczMjBmZDIxZTg4MDc4MzMwMDRlNGVmMTU3Yzk4MDk3YzE1ZDk2MTQxZjliZDZmM2U2OWU4ZGI4NWQyOGMxNzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.LwAH1m_VmxeIl318qe13D2e04ukm-VrlchYfoXc3U4Q" controls="controls" muted="muted">

  </video>
</details>


<ol dir="auto">
<li>Clone this repository.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/ictnlp/LLaMA-Omni
cd LLaMA-Omni"><pre>git clone https://github.com/ictnlp/LLaMA-Omni
<span>cd</span> LLaMA-Omni</pre></div>
<ol start="2" dir="auto">
<li>Install packages.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n llama-omni python=3.10
conda activate llama-omni
pip install pip==24.0
pip install -e ."><pre>conda create -n llama-omni python=3.10
conda activate llama-omni
pip install pip==24.0
pip install -e <span>.</span></pre></div>
<ol start="3" dir="auto">
<li>Install <code>fairseq</code>.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/pytorch/fairseq
cd fairseq
pip install -e . --no-build-isolation"><pre>git clone https://github.com/pytorch/fairseq
<span>cd</span> fairseq
pip install -e <span>.</span> --no-build-isolation</pre></div>
<ol start="4" dir="auto">
<li>Install <code>flash-attention</code>.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pip install flash-attn --no-build-isolation"><pre>pip install flash-attn --no-build-isolation</pre></div>

<ol dir="auto">
<li>
<p dir="auto">Download the <code>Llama-3.1-8B-Omni</code> model from ü§ó<a href="https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni" rel="nofollow">Huggingface</a>.</p>
</li>
<li>
<p dir="auto">Download the <code>Whisper-large-v3</code> model.</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import whisper
model = whisper.load_model(&#34;large-v3&#34;, download_root=&#34;models/speech_encoder/&#34;)"><pre>import whisper
model = whisper.load_model(<span><span>&#34;</span>large-v3<span>&#34;</span></span>, download_root=<span><span>&#34;</span>models/speech_encoder/<span>&#34;</span></span>)</pre></div>
<ol start="3" dir="auto">
<li>Download the unit-based HiFi-GAN vocoder.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/g_00500000 -P vocoder/
wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/config.json -P vocoder/"><pre>wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/g_00500000 -P vocoder/
wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/config.json -P vocoder/</pre></div>

<ol dir="auto">
<li>Launch a controller.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python -m omni_speech.serve.controller --host 0.0.0.0 --port 10000"><pre>python -m omni_speech.serve.controller --host 0.0.0.0 --port 10000</pre></div>
<ol start="2" dir="auto">
<li>Launch a gradio web server.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python -m omni_speech.serve.gradio_web_server --controller http://localhost:10000 --port 8000 --model-list-mode reload --vocoder vocoder/g_00500000 --vocoder-cfg vocoder/config.json"><pre>python -m omni_speech.serve.gradio_web_server --controller http://localhost:10000 --port 8000 --model-list-mode reload --vocoder vocoder/g_00500000 --vocoder-cfg vocoder/config.json</pre></div>
<ol start="3" dir="auto">
<li>Launch a model worker.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python -m omni_speech.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path Llama-3.1-8B-Omni --model-name Llama-3.1-8B-Omni --s2s"><pre>python -m omni_speech.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path Llama-3.1-8B-Omni --model-name Llama-3.1-8B-Omni --s2s</pre></div>
<ol start="4" dir="auto">
<li>Visit <a href="http://localhost:8000/" rel="nofollow">http://localhost:8000/</a> and interact with LLaMA-3.1-8B-Omni!</li>
</ol>
<p dir="auto"><strong>Note: Due to the instability of streaming audio playback in Gradio, we have only implemented streaming audio synthesis without enabling autoplay. If you have a good solution, feel free to submit a PR. Thanks!</strong></p>

<p dir="auto">To run inference locally, please organize the speech instruction files according to the format in the <code>omni_speech/infer/examples</code> directory, then refer to the following script.</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash omni_speech/infer/run.sh omni_speech/infer/examples"><pre>bash omni_speech/infer/run.sh omni_speech/infer/examples</pre></div>

<p dir="auto">Our code is released under the Apache-2.0 License. Our model, as it is built on Llama 3.1, is required to comply with the <a href="https://llama.meta.com/llama3_1/license/" rel="nofollow">Llama 3.1 License</a>.</p>

<ul dir="auto">
<li><a href="https://github.com/haotian-liu/LLaVA">LLaVA</a>: The codebase we built upon.</li>
<li><a href="https://github.com/X-LANCE/SLAM-LLM">SLAM-LLM</a>: We borrow some code about speech encoder and speech adaptor.</li>
</ul>

<p dir="auto">If you have any questions, please feel free to submit an issue or contact <code>fangqingkai21b@ict.ac.cn</code>.</p>
<p dir="auto">If our work is useful for you, please cite as:</p>
<div data-snippet-clipboard-copy-content="@article{fang-etal-2024-llama-omni,
  title={LLaMA-Omni: Seamless Speech Interaction with Large Language Models},
  author={Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2409.06666},
  year={2024}
}"><pre><code>@article{fang-etal-2024-llama-omni,
  title={LLaMA-Omni: Seamless Speech Interaction with Large Language Models},
  author={Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang},
  journal={arXiv preprint arXiv:2409.06666},
  year={2024}
}
</code></pre></div>

<p dir="auto"><a href="https://star-history.com/#ictnlp/llama-omni&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/73a57c27c1f48fd90f61261b56699215fdd4b2267edd91ec8494c550fd34bc04/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6963746e6c702f6c6c616d612d6f6d6e6926747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=ictnlp/llama-omni&amp;type=Date"/></a></p>
</article></div></div>
  </body>
</html>
