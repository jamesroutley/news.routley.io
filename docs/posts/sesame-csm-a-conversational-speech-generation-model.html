<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/SesameAILabs/csm">Original</a>
    <h1>Sesame CSM: A Conversational Speech Generation Model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>2025/03/13</strong> - We are releasing the 1B CSM variant. The checkpoint is <a href="https://huggingface.co/sesame/csm_1b" rel="nofollow">hosted on Hugging Face</a>.</p>
<hr/>
<p dir="auto">CSM (Conversational Speech Model) is a speech generation model from <a href="https://www.sesame.com" rel="nofollow">Sesame</a> that generates RVQ audio codes from text and audio inputs. The model architecture employs a <a href="https://www.llama.com/" rel="nofollow">Llama</a> backbone and a smaller audio decoder that produces <a href="https://huggingface.co/kyutai/mimi" rel="nofollow">Mimi</a> audio codes.</p>
<p dir="auto">A fine-tuned variant of CSM powers the <a href="https://www.sesame.com/voicedemo" rel="nofollow">interactive voice demo</a> shown in our <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" rel="nofollow">blog post</a>.</p>
<p dir="auto">A hosted <a href="https://huggingface.co/spaces/sesame/csm-1b" rel="nofollow">Hugging Face space</a> is also available for testing audio generation.</p>

<ul dir="auto">
<li>A CUDA-compatible GPU</li>
<li>The code has been tested on CUDA 12.4 and 12.6, but it may also work on other versions</li>
<li>Similarly, Python 3.10 is recommended, but newer versions may be fine</li>
<li>For some audio operations, <code>ffmpeg</code> may be required</li>
<li>Access to the following Hugging Face models:
<ul dir="auto">
<li><a href="https://huggingface.co/meta-llama/Llama-3.2-1B" rel="nofollow">Llama-3.2-1B</a></li>
<li><a href="https://huggingface.co/sesame/csm-1b" rel="nofollow">CSM-1B</a></li>
</ul>
</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:SesameAILabs/csm.git
cd csm
python3.10 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# You will need access to CSM-1B and Llama-3.2-1B
huggingface-cli login"><pre>git clone git@github.com:SesameAILabs/csm.git
<span>cd</span> csm
python3.10 -m venv .venv
<span>source</span> .venv/bin/activate
pip install -r requirements.txt

<span><span>#</span> You will need access to CSM-1B and Llama-3.2-1B</span>
huggingface-cli login</pre></div>

<p dir="auto">The <code>triton</code> package cannot be installed in Windows. Instead use <code>pip install triton-windows</code>.</p>

<p dir="auto">Generate a sentence</p>
<div dir="auto" data-snippet-clipboard-copy-content="from generator import load_csm_1b
import torchaudio
import torch

if torch.backends.mps.is_available():
    device = &#34;mps&#34;
elif torch.cuda.is_available():
    device = &#34;cuda&#34;
else:
    device = &#34;cpu&#34;

generator = load_csm_1b(device=device)

audio = generator.generate(
    text=&#34;Hello from Sesame.&#34;,
    speaker=0,
    context=[],
    max_audio_length_ms=10_000,
)

torchaudio.save(&#34;audio.wav&#34;, audio.unsqueeze(0).cpu(), generator.sample_rate)"><pre><span>from</span> <span>generator</span> <span>import</span> <span>load_csm_1b</span>
<span>import</span> <span>torchaudio</span>
<span>import</span> <span>torch</span>

<span>if</span> <span>torch</span>.<span>backends</span>.<span>mps</span>.<span>is_available</span>():
    <span>device</span> <span>=</span> <span>&#34;mps&#34;</span>
<span>elif</span> <span>torch</span>.<span>cuda</span>.<span>is_available</span>():
    <span>device</span> <span>=</span> <span>&#34;cuda&#34;</span>
<span>else</span>:
    <span>device</span> <span>=</span> <span>&#34;cpu&#34;</span>

<span>generator</span> <span>=</span> <span>load_csm_1b</span>(<span>device</span><span>=</span><span>device</span>)

<span>audio</span> <span>=</span> <span>generator</span>.<span>generate</span>(
    <span>text</span><span>=</span><span>&#34;Hello from Sesame.&#34;</span>,
    <span>speaker</span><span>=</span><span>0</span>,
    <span>context</span><span>=</span>[],
    <span>max_audio_length_ms</span><span>=</span><span>10_000</span>,
)

<span>torchaudio</span>.<span>save</span>(<span>&#34;audio.wav&#34;</span>, <span>audio</span>.<span>unsqueeze</span>(<span>0</span>).<span>cpu</span>(), <span>generator</span>.<span>sample_rate</span>)</pre></div>
<p dir="auto">CSM sounds best when provided with context. You can prompt or provide context to the model using a <code>Segment</code> for each speaker&#39;s utterance.</p>
<div dir="auto" data-snippet-clipboard-copy-content="speakers = [0, 1, 0, 0]
transcripts = [
    &#34;Hey how are you doing.&#34;,
    &#34;Pretty good, pretty good.&#34;,
    &#34;I&#39;m great.&#34;,
    &#34;So happy to be speaking to you.&#34;,
]
audio_paths = [
    &#34;utterance_0.wav&#34;,
    &#34;utterance_1.wav&#34;,
    &#34;utterance_2.wav&#34;,
    &#34;utterance_3.wav&#34;,
]

def load_audio(audio_path):
    audio_tensor, sample_rate = torchaudio.load(audio_path)
    audio_tensor = torchaudio.functional.resample(
        audio_tensor.squeeze(0), orig_freq=sample_rate, new_freq=generator.sample_rate
    )
    return audio_tensor

segments = [
    Segment(text=transcript, speaker=speaker, audio=load_audio(audio_path))
    for transcript, speaker, audio_path in zip(transcripts, speakers, audio_paths)
]
audio = generator.generate(
    text=&#34;Me too, this is some cool stuff huh?&#34;,
    speaker=1,
    context=segments,
    max_audio_length_ms=10_000,
)

torchaudio.save(&#34;audio.wav&#34;, audio.unsqueeze(0).cpu(), generator.sample_rate)"><pre><span>speakers</span> <span>=</span> [<span>0</span>, <span>1</span>, <span>0</span>, <span>0</span>]
<span>transcripts</span> <span>=</span> [
    <span>&#34;Hey how are you doing.&#34;</span>,
    <span>&#34;Pretty good, pretty good.&#34;</span>,
    <span>&#34;I&#39;m great.&#34;</span>,
    <span>&#34;So happy to be speaking to you.&#34;</span>,
]
<span>audio_paths</span> <span>=</span> [
    <span>&#34;utterance_0.wav&#34;</span>,
    <span>&#34;utterance_1.wav&#34;</span>,
    <span>&#34;utterance_2.wav&#34;</span>,
    <span>&#34;utterance_3.wav&#34;</span>,
]

<span>def</span> <span>load_audio</span>(<span>audio_path</span>):
    <span>audio_tensor</span>, <span>sample_rate</span> <span>=</span> <span>torchaudio</span>.<span>load</span>(<span>audio_path</span>)
    <span>audio_tensor</span> <span>=</span> <span>torchaudio</span>.<span>functional</span>.<span>resample</span>(
        <span>audio_tensor</span>.<span>squeeze</span>(<span>0</span>), <span>orig_freq</span><span>=</span><span>sample_rate</span>, <span>new_freq</span><span>=</span><span>generator</span>.<span>sample_rate</span>
    )
    <span>return</span> <span>audio_tensor</span>

<span>segments</span> <span>=</span> [
    <span>Segment</span>(<span>text</span><span>=</span><span>transcript</span>, <span>speaker</span><span>=</span><span>speaker</span>, <span>audio</span><span>=</span><span>load_audio</span>(<span>audio_path</span>))
    <span>for</span> <span>transcript</span>, <span>speaker</span>, <span>audio_path</span> <span>in</span> <span>zip</span>(<span>transcripts</span>, <span>speakers</span>, <span>audio_paths</span>)
]
<span>audio</span> <span>=</span> <span>generator</span>.<span>generate</span>(
    <span>text</span><span>=</span><span>&#34;Me too, this is some cool stuff huh?&#34;</span>,
    <span>speaker</span><span>=</span><span>1</span>,
    <span>context</span><span>=</span><span>segments</span>,
    <span>max_audio_length_ms</span><span>=</span><span>10_000</span>,
)

<span>torchaudio</span>.<span>save</span>(<span>&#34;audio.wav&#34;</span>, <span>audio</span>.<span>unsqueeze</span>(<span>0</span>).<span>cpu</span>(), <span>generator</span>.<span>sample_rate</span>)</pre></div>

<p dir="auto"><strong>Does this model come with any voices?</strong></p>
<p dir="auto">The model open-sourced here is a base generation model. It is capable of producing a variety of voices, but it has not been fine-tuned on any specific voice.</p>
<p dir="auto"><strong>Can I converse with the model?</strong></p>
<p dir="auto">CSM is trained to be an audio generation model and not a general-purpose multimodal LLM. It cannot generate text. We suggest using a separate LLM for text generation.</p>
<p dir="auto"><strong>Does it support other languages?</strong></p>
<p dir="auto">The model has some capacity for non-English languages due to data contamination in the training data, but it likely won&#39;t do well.</p>

<p dir="auto">This project provides a high-quality speech generation model for research and educational purposes. While we encourage responsible and ethical use, we <strong>explicitly prohibit</strong> the following:</p>
<ul dir="auto">
<li><strong>Impersonation or Fraud</strong>: Do not use this model to generate speech that mimics real individuals without their explicit consent.</li>
<li><strong>Misinformation or Deception</strong>: Do not use this model to create deceptive or misleading content, such as fake news or fraudulent calls.</li>
<li><strong>Illegal or Harmful Activities</strong>: Do not use this model for any illegal, harmful, or malicious purposes.</li>
</ul>
<p dir="auto">By using this model, you agree to comply with all applicable laws and ethical guidelines. We are <strong>not responsible</strong> for any misuse, and we strongly condemn unethical applications of this technology.</p>
<hr/>

<p dir="auto">Johan Schalkwyk, Ankit Kumar, Dan Lyth, Sefik Emre Eskimez, Zack Hodari, Cinjon Resnick, Ramon Sanabria, Raven Jiang, and the Sesame team.</p>
</article></div></div>
  </body>
</html>
