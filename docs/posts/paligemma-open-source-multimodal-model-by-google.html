<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.roboflow.com/paligemma-multimodal-vision/">Original</a>
    <h1>PaliGemma: Open-Source Multimodal Model by Google</h1>
    
    <div id="readability-page-1" class="page"><div>
            <div>
              <div>
                <div><p><a href="https://ai.google.dev/gemma/docs/paligemma?ref=blog.roboflow.com"><u>PaliGemma</u></a> is a vision language model (VLM) developed and released by Google that has <a href="https://blog.roboflow.com/multimodal-models/"><u>multimodal</u></a> capabilities.¬†</p><p>Unlike other VLMs, such as <a href="https://blog.roboflow.com/gpt-4o-vision-use-cases/"><u>OpenAI‚Äôs GPT-4o</u></a>, <a href="https://blog.roboflow.com/gemini-what-we-know/"><u>Google Gemini</u></a>, and <a href="https://blog.roboflow.com/claude-3-opus-multimodal/"><u>Anthropic‚Äôs Claude 3</u></a> which have struggled with object detection and segmentation, PaliGemma has a wide range of abilities, paired with the ability to fine-tune for better performance on specific tasks. </p><p>Google‚Äôs decision to open source a highly capable multimodal model with the ability to fine-tune on custom data is a major breakthrough for open-source AI. PaliGemma gives you the opportunity to create custom multimodal models which you can self-host in the cloud and potentially on larger edge devices like NVIDIA Jetsons.</p><h2 id="what-is-paligemma">What is PaliGemma?</h2><p>PaliGemma, <a href="https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/?ref=blog.roboflow.com"><u>released alongside other products</u></a> at the <a href="https://io.google/2024/?ref=blog.roboflow.com"><u>2024 Google I/O event</u></a>, is a combined multimodal model based on two other models from Google research: SigLIP, a vision model, and Gemma, a large language model, which means the model is a composition of a Transformer decoder and a Vision Transformer image encoder. It takes both image and text as input and generates text as output, supporting multiple languages. </p><p>Important aspects of PaliGemma:</p><ul><li>Relatively small 3 billion combined parameter model</li><li><a href="https://ai.google.dev/gemma/terms?ref=blog.roboflow.com"><u>Permissible commercial use terms</u></a></li><li><a href="https://ai.google.dev/gemma/docs/paligemma/fine-tuning-paligemma?ref=blog.roboflow.com"><u>Ability to fine-tune</u></a> for image and short video caption, visual question answering, text reading, object detection, and object segmentation</li></ul><p>While PaliGemma is useful without fine-tuning, Google says it is ‚Äúnot designed to be used directly, but to be transferred (by fine-tuning) to specific tasks using a similar prompt structure‚Äù which means whatever baseline we can observe with the model weights is only the tip of the iceberg for how useful the model may be in a given context. <a href="https://ai.google.dev/gemma/docs/paligemma/model-card?ref=blog.roboflow.com#pre-train_datasets"><u>PaliGemma is pre-trained</u></a> on WebLI, CC3M-35L, VQ¬≤A-CC3M-35L/VQG-CC3M-35L, OpenImages, and WIT.</p><h3 id="links-to-paligemma-resources">Links to PaliGemma Resources</h3><p>Google supplied ample resources to start prototyping with PaliGemma and we‚Äôve curated the highest quality information for those of you who want to jump into using PaliGemma immediately. We suggest getting started with the following resources:</p><ul><li><a href="https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md?ref=blog.roboflow.com"><u>PaliGemma Github README</u></a></li><li><a href="https://ai.google.dev/gemma/docs/paligemma?ref=blog.roboflow.com"><u>PaliGemma documentation</u></a></li><li><a href="https://ai.google.dev/gemma/docs/paligemma/fine-tuning-paligemma?ref=blog.roboflow.com"><u>PaliGemma fine-tuning documentation</u></a></li><li><a href="https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb?ref=blog.roboflow.com"><u>Fine-tune PaliGemma in Google Colab</u></a></li><li><a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/363?project=roboflow-marketing&amp;ref=blog.roboflow.com"><u>Access PaliGemma in Google Vertex</u></a></li></ul><p>In this post we will explore what PaliGemma can do, compare PaliGemma benchmarks to other LMMs, understand PaliGemma‚Äôs limitations, and see how it performs in real world use cases. We‚Äôve put together learnings that can save you time while testing PaliGemma.</p><p>Let‚Äôs get started!</p><h2 id="what-can-paligemma-do">What can PaliGemma do?</h2><p>PaliGemma is a single-turn vision language model and it works best when fine-tuning to a specific use case. This means you can input an image and text string, such as a prompt to caption the image, or a question and PaliGemma will output text in response to the input, such as a caption of the image, an answer to a question, or a list of object bounding box coordinates.</p><p>Tasks PaliGemma is suited to perform relate to the <a href="https://ai.google.dev/gemma/docs/paligemma/model-card?ref=blog.roboflow.com#benchmark-results"><u>benchmarking results Google</u></a> released across the following tasks:</p><ul><li>Fine-tuning on single tasks</li><li>Image question answering and captioning</li><li>Video question answering and captioning</li><li>Segmentation</li></ul><p>This means PaliGemma is useful for straightforward and specific questions related to visual data.</p><p>We‚Äôve created a table to show PaliGemma results relative to other models based on reported results on common benchmarks.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-6.png" alt="" loading="lazy" width="896" height="146" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-6.png 600w, https://blog.roboflow.com/content/images/2024/05/image-6.png 896w" sizes="(min-width: 720px) 720px"/></figure><p>While benchmarks are helpful data points, they do not tell the entire story. PaliGemma is <strong>built to be fine-tuned</strong> and the other models are closed-source. For the purposes of showing which options are available, we compare against other, often much larger, models that are unable to be fine-tuned.</p><p>It is worth experimenting to see if fine-tuning with custom data will lead to better performance for your specific use case than out-of-the-box performance with other models.</p><p>Later in this post, we will compare PaliGemma to other <a href="https://blog.roboflow.com/gpt-4-vision-alternatives/"><u>open-source VLMs and LMMs</u></a> using a standard set of tests. Continue reading to see how it performs.</p><h2 id="how-to-fine-tune-paligemma">How to Fine-tune PaliGemma</h2><p>One of the exciting aspects of PaliGemma is its ability to finetune on custom use-case data. <a href="https://ai.google.dev/gemma/docs/paligemma/fine-tuning-paligemma?ref=blog.roboflow.com"><u>A notebook</u></a> published by Google‚Äôs PaliGemma team showcases how to fine-tune on a small dataset.</p><p>It‚Äôs important to note that in this example, only the attention layers are fine-tuned and therefore the performance improvements may be limited.</p><h2 id="how-to-deploy-and-use-paligemma">How to Deploy and Use PaliGemma</h2><p>You can deploy PaliGemma using an open-source <a href="https://inference.roboflow.com/?ref=blog.roboflow.com"><u>Inference package</u></a>. First, we will need to install Inference, as well as some other packages needed to run PaliGemma.</p><div><p>üìì</p><div><p>See the PaliGemma inference notebook, which contains the code below </p><a href="https://colab.research.google.com/drive/1_q09OjR2Ldl1FZnvfqwckvxrW_FIYclC?usp=sharing&amp;ref=blog.roboflow.com"><u>here</u></a><p>.</p></div></div><pre><code>!git clone https://github.com/roboflow/inference.git
%cd inference
!pip install -e .</code></pre><pre><code>!pip install git+https://github.com/huggingface/transformers.git accelerate -q</code></pre><p>Next, we will set up PaliGemma by importing the module from Inference and putting in our <a href="https://docs.roboflow.com/api-reference/authentication?ref=blog.roboflow.com"><u>Roboflow API key</u></a>.</p><pre><code>import inference
from inference.models.paligemma.paligemma import PaliGemma

pg = PaliGemma(api_key=&#34;YOUR ROBOFLOW API KEY&#34;)</code></pre><p>Last, we can input a test image as a Pillow image, pair it with a prompt, and wait for the result.</p><pre><code>from PIL import Image

image = Image.open(&#34;/content/dog.webp&#34;) # Change to your image
prompt = &#34;How many dogs are in this image?&#34;

result = pg.predict(image,prompt)</code></pre><p>When prompted with this image, we get the accurate answer of¬† `1`.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/dog.webp" alt="" loading="lazy" width="960" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/dog.webp 600w, https://blog.roboflow.com/content/images/2024/05/dog.webp 960w" sizes="(min-width: 720px) 720px"/></figure><h2 id="paligemma-evaluation-for-computer-vision">PaliGemma Evaluation¬†for Computer Vision</h2><p>Next, we will evaluate how PaliGemma does on various computer vision tasks that we‚Äôve tested using <a href="https://blog.roboflow.com/gpt-4o-vision-use-cases/"><u>GPT-4o</u></a>, <a href="https://blog.roboflow.com/claude-3-opus-multimodal/"><u>Claude 3</u></a>, <a href="https://blog.roboflow.com/first-impressions-with-google-gemini/"><u>Gemini</u></a>, and other models.</p><p>Here, we will test several different use cases including optical character recognition (OCR), document OCR, document understanding, <a href="https://blog.roboflow.com/what-is-vqa/"><u>visual question answering</u></a> (VQA), and <a href="https://blog.roboflow.com/object-detection/"><u>object detection</u></a>.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-18.png" alt="" loading="lazy" width="909" height="168" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-18.png 600w, https://blog.roboflow.com/content/images/2024/05/image-18.png 909w" sizes="(min-width: 720px) 720px"/></figure><h3 id="paligemma-for-optical-character-recognition-ocr">PaliGemma for Optical Character Recognition (OCR)</h3><p><a href="https://blog.roboflow.com/what-is-optical-character-recognition-ocr/"><u>Optical character recognition</u></a> is a computer vision task to return the visible text from an image in machine-readable text format. While its a simple task in concept, it can be a difficult task to accomplish in production applications.</p><p>Below we try OCR with both the prompts that we‚Äôve seen work with other LMMs, asking it to ‚ÄúRead the serial number. Return the number with no additional text.‚Äù With this prompt, it failed, claiming that it did not have the training or capability to answer that question.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-7.png" alt="" loading="lazy" width="683" height="347" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-7.png 600w, https://blog.roboflow.com/content/images/2024/05/image-7.png 683w"/></figure><p>However, we know from the model documentation that it should be capable of OCR. We tried with the example prompt provided in the documentation, `ocr`, where we got a successful, correct result. </p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-8.png" alt="" loading="lazy" width="686" height="341" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-8.png 600w, https://blog.roboflow.com/content/images/2024/05/image-8.png 686w"/></figure><p>Trying with a different image with the first prompt also yielded correct results, bringing up a potential limitation of prompt sensitivity.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-9.png" alt="" loading="lazy" width="685" height="424" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-9.png 600w, https://blog.roboflow.com/content/images/2024/05/image-9.png 685w"/></figure><p>Next, testing on an <a href="https://blog.roboflow.com/best-ocr-models-text-recognition/"><u>OCR benchmark</u></a> that we have used previously to test other OCR models like Tesseract, Gemini, Claude, GPT-4o and others, we saw very impressive results.¬†</p><p>In average accuracy, we saw 85.84%, beating all other OCR models except for Anthropic‚Äôs Claude 3 Opus. </p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-10.png" alt="Average accuracy of all tested models, all except PaliGemma are cached results." loading="lazy" width="504" height="421"/><figcaption><span>Average accuracy of all tested models, where all except PaliGemma are cached results. The graph labels on the right side are overlapped. They are Gemini 1.5, GPT-4o and PaliGemma in that order.</span></figcaption></figure><p>PaliGemma also achieved relatively fast speeds. Combined with the cheaper local nature of the model, PaliGemma seems to be the top OCR model in terms of speed efficiency and cost efficiency.</p><div><p>üóíÔ∏è</p><p>Speed efficiency and cost efficiency, metrics introduced in the OCR benchmarking post, refer to a metric of accuracy given (divided by) time elapsed and cost incurred.</p></div><p>In median speed efficiency, it beats the previous leader, GPT-4o, set a day earlier when it was released, by a healthy margin. In terms of cost efficiency, PaliGemma outperformed the previous leader, EasyOCR, by almost three times, running more accurately and cheaper.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-11.png" alt="Median speed and cost efficiency of all tested models" loading="lazy" width="696" height="296" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-11.png 600w, https://blog.roboflow.com/content/images/2024/05/image-11.png 696w"/><figcaption><span>Median speed and cost efficiency of all tested models, where all except PaliGemma are cached results. The graph labels on the right side are overlapped. They are Gemini 1.5, GPT-4o and PaliGemma in that order.</span></figcaption></figure><p>We consider these results to make PaliGemma a top OCR model given the local and more lightweight nature of PaliGemma compared to the models it beat, including the recently released GPT-4o, Gemini, and other OCR packages.</p><h3 id="document-understanding">Document Understanding</h3><p>Document understanding refers to the ability to extract relevant key information from an image, usually with other irrelevant text.</p><p>On an image with a receipt, we ask it to extract the tax paid according to the receipt. Here, PaliGemma gives a close but incorrect result consistently across several attempts.¬†</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-12.png" alt="" loading="lazy" width="691" height="486" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-12.png 600w, https://blog.roboflow.com/content/images/2024/05/image-12.png 691w"/></figure><p>However, on an image with a pizza menu, when asked to provide the cost of a specific pizza, it returned a correct value. </p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-13.png" alt="" loading="lazy" width="690" height="510" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-13.png 600w, https://blog.roboflow.com/content/images/2024/05/image-13.png 690w"/></figure><p>This performs similar or equivalent to the experience we had with GPT-4 with Vision, where it failed tax extraction but answered the pizza menu question correctly. Gemini, Claude 3 and the new GPT-4o did answer both questions correctly, as well as <a href="https://roboflow.com/compare/qwenvl-vs-gpt-4-vision?ref=blog.roboflow.com"><u>Qwen-VL-Plus</u></a>, an open-source VLM.</p><h3 id="visual-question-answering-vqa">Visual Question Answering (VQA)</h3><p>Visual Question Answering involves posing a model with an image and a question requiring some form of recognition, identification, or reasoning.</p><p>When posed with a question on how much money was present in a picture with 4 coins, it answered with 4 coins. A technically correct answer, but the question asked for the amount of money in the image.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-14.png" alt="" loading="lazy" width="686" height="485" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-14.png 600w, https://blog.roboflow.com/content/images/2024/05/image-14.png 686w"/></figure><p>When tasked with identifying a scene featuring Kevin Mcallister from the movie Home Alone, it responded with ‚Äúchristmas‚Äù. We consider this to be an incorrect answer.¬†</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-15.png" alt="" loading="lazy" width="690" height="339" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-15.png 600w, https://blog.roboflow.com/content/images/2024/05/image-15.png 690w"/></figure><h3 id="object-detection">Object Detection</h3><p>As we mentioned earlier, VLMs have traditionally struggled with object detection, much less instance segmentation. However, PaliGemma is reported to have object detection and instance segmentation abilities.</p><p>First, we test with the same prompt we have given other models in the past. Here, it returns an incorrect, likely hallucinated result.¬†</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-16.png" alt="" loading="lazy" width="691" height="479" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-16.png 600w, https://blog.roboflow.com/content/images/2024/05/image-16.png 691w"/></figure><p>However, when prompting with the keyword `detect`, followed by the object `dog` (so `detect dog`) as detailed in the model documentation, it correctly and accurately identifies the dog in the image. Using the keyword `segment dog` also resulted in a correct segmentation.</p><figure><img src="https://blog.roboflow.com/content/images/2024/05/image-17.png" alt="" loading="lazy" width="632" height="419" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/image-17.png 600w, https://blog.roboflow.com/content/images/2024/05/image-17.png 632w"/></figure><p>Although it is very impressive that a VLM is able to provide object detection and recognition capabilities, it is worth noting that only basic examples, such as those possible with a traditional object detection model, succeeded. When prompted to find cars, (there is a car present visible through the door of the building in the back) it returned no results.</p><h2 id="use-cases-for-paligemma">Use Cases for PaliGemma</h2><p>Whether using PaliGemma zero-shot or fine-tuned on custom data, there are specific use cases tailored to PaliGemma‚Äôs strengths that will open the door to new AI use cases. Let‚Äôs take a look at a two of them.</p><h3 id="custom-applications">Custom Applications</h3><p>Models like Claude 3, Gemini 1.5 Pro, and GPT-4o are used out-of-the-box and applied to problems they are suited to solve. PaliGemmi brings multimodal abilities to use cases that are still unsolved by closed-source models because you can fine-tune PaliGemma with proprietary data related to your problem. This is useful in industries like manufacturing, CPG, healthcare, and security. If you have a unique problem that closed-models have not seen, and will never see due to their proprietary nature, then PaliGemma is a great entry point into building custom AI solutions.¬†</p><h3 id="ocr">OCR</h3><p>As shown earlier in this article, PaliGemma is a strong OCR model without any additional fine-tuning. When building OCR applications to scale to billions of predictions, latency, cost, and accuracy can be difficult to balance. Before PaliGemma, closed-source models were the best-in-class option for performance but their cost and lack of model ownership made them difficult to justify in production. This model can provide immediate performance and be improved over time by fine-tuning on your specific data.</p><h2 id="limitations-of-paligemma">Limitations of PaliGemma</h2><p>PaliGemma, and all VLMs, are best suited for tasks with clear instructions and are not the best tool for open-ended, complex, nuanced, or reason based problems. This is where VLMs are distinct from LMMs and you will find the best results if you use the models where they are most likely to perform well.</p><p>In terms of context, PaliGemma has information based on the pre-training datasets and any data supplied during fine-tuning. PaliGemma will not know information outside of this and, barring any weights updates with new data from Google or the open source community, you should not rely on PaliGemma as a knowledge base.</p><p>To get the most out of PaliGemma, and have a reason to use the model over other open source models, you‚Äôll want to train the model on custom data. Its zero-shot performance is not state-of-the-art across most benchmarks. Setting up a custom training pipeline will be necessary to warrant using PaliGemma for most use cases.</p><p>Finally, during various tests, we saw drastic differences in results with slight changes to prompts. This is similar behavior to other LMMs, like <a href="https://blog.roboflow.com/yolo-world-prompting-tips/"><u>YOLO-World</u></a>, and takes time to understand how to best prompt the model. Changes in a prompt, like removing an ‚Äòs‚Äô to make a word singular rather than plural, can be the difference between a perfect detection and an unusable output.</p><figure><div><div><p><img src="https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.44.07-PM.png" width="1610" height="1054" loading="lazy" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/Screenshot-2024-05-14-at-8.44.07-PM.png 600w, https://blog.roboflow.com/content/images/size/w1000/2024/05/Screenshot-2024-05-14-at-8.44.07-PM.png 1000w, https://blog.roboflow.com/content/images/size/w1600/2024/05/Screenshot-2024-05-14-at-8.44.07-PM.png 1600w, https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.44.07-PM.png 1610w" sizes="(min-width: 720px) 720px"/></p><p><img src="https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.43.35-PM.png" width="1688" height="946" loading="lazy" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/Screenshot-2024-05-14-at-8.43.35-PM.png 600w, https://blog.roboflow.com/content/images/size/w1000/2024/05/Screenshot-2024-05-14-at-8.43.35-PM.png 1000w, https://blog.roboflow.com/content/images/size/w1600/2024/05/Screenshot-2024-05-14-at-8.43.35-PM.png 1600w, https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.43.35-PM.png 1688w" sizes="(min-width: 720px) 720px"/></p><p><img src="https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.45.12-PM.png" width="1804" height="1138" loading="lazy" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2024/05/Screenshot-2024-05-14-at-8.45.12-PM.png 600w, https://blog.roboflow.com/content/images/size/w1000/2024/05/Screenshot-2024-05-14-at-8.45.12-PM.png 1000w, https://blog.roboflow.com/content/images/size/w1600/2024/05/Screenshot-2024-05-14-at-8.45.12-PM.png 1600w, https://blog.roboflow.com/content/images/2024/05/Screenshot-2024-05-14-at-8.45.12-PM.png 1804w" sizes="(min-width: 720px) 720px"/></p></div></div><figcaption><p><span>Notice the different results based on plural vs singular nouns</span></p></figcaption></figure><h2 id="conclusion">Conclusion</h2><p>Google‚Äôs release of PaliGemma is incredibly useful for the advancement of multimodal AI. The lightweight open source model built for fine-tuning means anyone can custom train their own large vision-language model and deploy it for any commercial purpose on their own hardware or cloud. </p><p>Previous LMMs have been extremely expensive to fine-tune and often require large amounts of compute to run, making them prohibitive for broad adoption. PaliGemma breaks the mold and offers people building custom AI applications a breakthrough model to create sophisticated applications.</p></div>
                
              </div>
            </div>
          </div></div>
  </body>
</html>
