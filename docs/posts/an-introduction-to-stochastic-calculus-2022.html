<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bjlkeng.io/posts/an-introduction-to-stochastic-calculus/">Original</a>
    <h1>An Introduction to Stochastic Calculus (2022)</h1>
    
    <div id="readability-page-1" class="page"><p>Through a couple of different avenues I wandered, yet again, down a rabbit hole
leading to the topic of this post.  The first avenue was through my main focus
on a particular machine learning topic that utilized some concepts from
physics, which naturally led me to stochastic calculus.  The second avenue was
through some projects at work in the quantitative finance space, which is one
of the main applications of stochastic calculus.  Naively, I thought I could
write a brief post on it that would satisfy my curiosity -- that didn&#39;t work
out at all! The result is this extra long post.</p><p>This post is about stochastic calculus, an extension of regular calculus to
stochastic processes.  It&#39;s not immediately obvious
but the rigour needed to properly understand some of the key ideas requires
going back to the measure theoretic definition of probability theory, so
that&#39;s where I start in the background. From there I quickly move on to
stochastic processes, the Wiener process, a particular flavour of stochastic
calculus called It√¥ calculus, and finally end with a couple of applications.
As usual, I try to include a mix of intuition, rigour where it helps intuition,
and some simple examples.  It&#39;s a deep and wide topic so I hope you enjoy my
digest of it.</p><div id="motivation">
<h2><a href="#id6"><span>1</span> Motivation</a></h2>
<p>Many physical phenomena (and financial ones) can be modelled as a
<a href="https://en.wikipedia.org/wiki/Stochastic_process">stochastic process</a>
that is described using a
<a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">stochastic differential equation</a>.
Both of these things were probably not included in most introductory courses on
either probability or calculus.  Starting with stochastic processes, the
easiest way to think about it is a collection of random variables indexed by
time.  So instead of a single deterministic value at each time <span>\(t\)</span>, we have
a random variable instead (usually with some relationship or common property
with the other ones).
So while on the surface it seems relatively simple, one of the big complexities
we run into is when we let <span>\(t\)</span> be continuous, which we will see in detail
later.</p>
<p>Stochastic differential equations defined on continuous time are a very
natural way to model many different phenomena.  A common stochastic
differential equation called the
<a href="https://en.wikipedia.org/wiki/Langevin_equation">Langevin equation</a>
is used to model many types of stochastic phenomena:</p>
<p>
\begin{equation*}
\frac{dX(t)}{dt} = \alpha(X, t) + \beta(X, t)\eta(t) \tag{1.1}
\end{equation*}
</p>
<p>where <span>\(X(t)\)</span> is a stochastic process, <span>\(\alpha, \beta\)</span> can be a
function of both <span>\(X\)</span> and time <span>\(t\)</span>, and a noise term
<span>\(\eta(t)\)</span>.  The noise term is what makes this differential equation
special by introducing a special type of randomness.  And while this is just a
single example, it does have many characteristics that show up in other
applications of stochastic calculus.</p>
<p>Intuitively, the noise term <span>\(\eta(t)\)</span> represents &#34;random fluctuations&#34;
such as a particle&#39;s random collisions with other molecules in a fluid, or the
random fluctuations of a stock price.  To be precise about these &#34;random
fluctuations&#34;, we first must specify some of their characteristics such as
their
<a href="https://en.wikipedia.org/wiki/Autocorrelation">time correlation</a> function:</p>
<p>
\begin{equation*}
C(\tau) = E[\eta(0)\eta(\tau)] = \lim_{T\to\infty} \frac{1}{T} \int_0^T \eta(t)\eta(t+\tau) dt \tag{1.2}
\end{equation*}
</p>
<p>which should be a decreasing function of <span>\(\tau\)</span> since they are random
fluctuations and shouldn&#39;t have lasting effects.  But this can get messy
relatively quickly so we usually look for more clean abstractions to describe
these systems.</p>
<p>The assumption that is commonly used is that the random fluctuations are not
correlated at all.  This can be justified if the time scale of interest is much
bigger than the random fluctuations.  From this assumption, we have:</p>
<p>
\begin{equation*}
E[\eta(0)\eta(\tau)] = c\delta(\tau) \tag{1.3}
\end{equation*}
</p>
<p>where <span>\(c\)</span> is a constant and <span>\(\delta(\tau)\)</span> is the
<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta</a> function.
This implies that the random fluctuations are entirely uncorrelated even for
infinitesimal timescales.  The other corresponding assumption is that at each
timestep <span>\(t\)</span> the random variable <span>\(\eta(t)\)</span> is a zero mean Gaussian.</p>
<p>In some ways, <span>\(\eta(t)\)</span> simplifies things; in others, it makes them much
more complex.  The first thing to note is that <span>\(\eta(t)\)</span> is a theoretical
construct -- there is no random process that can have its properties.
We can see that from Equation 1.3 where we use the theoretical
<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta</a> function.
This also implies that the variance of <span>\(\eta(t)\)</span> is infinite (<span>\(C(\tau=0)\)</span>).
This construction also has a flat power spectral density of all frequencies,
implying an infinite bandwidth signal (see <a href="https://en.wikipedia.org/wiki/White_noise">Wikipedia</a>),
which again is not physically realizable.</p>
<p>Another consequence of this definition is that <span>\(\eta(t)\)</span> is discontinuous
everywhere.  The value at <span>\(\eta(t)\)</span> can be totally different at a small
time increment later (<span>\(\eta(t + dt)\)</span>).  This makes simple operations like
integration much more difficult.  Going back to our stochastic differential
equation from Equation 1.1, we can multiply through by <span>\(dt\)</span> and integrate
both sides to try to get:</p>
<p>
\begin{equation*}
X(T) = X(0) + \int_0^T \alpha(X, t)dt + \int_0^T \beta(X, t)\eta(t)dt \tag{1.4}
\end{equation*}
</p>
<p>The first integral on the right hand side is a standard one that generally we
know how to solve using the tools of calculus.  The second integral involving
<span>\(\eta(t)\)</span> is where we run into an issue.  It is precisely this problem
that has spawned a new branch of mathematics called <em>stochastic calculus</em>,
which is the topic of this post.</p>
</div><div id="stochastic-processes">
<h2><a href="#id7"><span>2</span> Stochastic Processes</a></h2>
<div id="probability-spaces-random-variables">
<h3><a href="#id8"><span>2.1</span> Probability Spaces &amp; Random Variables</a></h3>
<p><em>(Note: Skip this part if you&#39;re already familiar with the measure theoretic definition of probability.)</em></p>
<p>We&#39;re going to dive into the measure theoretic definition of probability, <em>attempting</em> to
give some intuition while still maintaining some level of rigour.  First, let&#39;s
examine the definition of a <strong>probability space</strong> <span>\((\Omega, {\mathcal
{F}}, P)\)</span>.  This is the same basic idea you learn in a first probability
course except with fancier math.</p>
<p><span>\(\Omega\)</span> is the <strong>sample space</strong>, which defines the set of all possible
outcomes of an experiment.  In finite sample spaces, any subset of
the sample space is called an <strong>event</strong>.  Another way to think about events is
any grouping of objects you would want to measure the probability on (e.g.,
individual elements of <span>\(\Omega\)</span>, unions of elements, or even the empty
set).</p>
<p>However, this type of reasoning breaks down when we have certain types of
infinite sample spaces (e.g., real line).  For this, we need to define an event more precisely
with an <strong>event space</strong> <span>\(\mathcal{F} \subseteq 2^{\Omega}\)</span> (<span>\(2^{\Omega}\)</span> denotes the
<a href="https://en.wikipedia.org/wiki/Power_set">power set</a>) using a construction
called a <span>\(\sigma\)</span>-algebra (&#34;sigma algebra&#34;):</p>
<blockquote>
<p>Let <span>\(\Omega\)</span> be a non-empty set, and let <span>\(\mathcal{F}\)</span> be a collection
of subsets of <span>\(\Omega\)</span>.  We say that <span>\(\mathcal{F}\)</span> is a <span>\(\sigma\)</span>-<a href="https://en.wikipedia.org/wiki/%CE%A3-algebra">algebra</a>:
if:</p>
<ol>
<li><p>The empty set belongs to <span>\(\mathcal{F}\)</span>.</p></li>
<li><p>Whenever a set <span>\(A\)</span> belongs to <span>\(\mathcal{F}\)</span>, its compliment <span>\(A^c\)</span> also belongs to <span>\(\mathcal{F}\)</span>
(closed under complement).</p></li>
<li><p>Whenever a sequence of sets <span>\(A_1, A_2, \ldots\)</span> belongs to <span>\(\mathcal{F}\)</span>,
their union <span>\(\cup_{n=1}^{\infty} A_n\)</span> also belongs to <span>\(\mathcal{F}\)</span>
(closed under countable unions -- implies closed under countable intersection).</p></li>
</ol>
<p>The elements of a <span>\(\sigma\)</span>-algebra are called <a href="https://en.wikipedia.org/wiki/Measure_(mathematics)">measurable sets</a>, and the pair <span>\((\Omega, \mathcal{F})\)</span> define a <a href="https://en.wikipedia.org/wiki/Measurable_space">measurable space</a>.</p>
</blockquote>
<p>Thus, we wish our event space <span>\(\mathcal{F}\)</span> to be a <span>\(\sigma\)</span>-algebra and
when combined with <span>\(\Omega\)</span>, define a measurable space.  This sounds
complicated but it basically guarantees that the subsets of <span>\(\Omega\)</span> that
we use for events have all the nice properties we would expect from
probabilities.</p>
<p>Intuitively, measurable spaces help makes the notion of &#34;size&#34; or &#34;volume&#34;
precise by defining the &#34;chunks&#34; of &#34;volume&#34;.  Using a physical analogy,
you want to make sure that no matter how you combine non-overlapping &#34;chunks&#34;
(i.e., unions of disjoint sets), you end up with a consistent measure of
&#34;volume&#34;.  Again, this is only really needed with infinite (non-countable)
sets.  For finite event spaces, we can usually just use the power set
<span>\(2^{\Omega}\)</span> as the event space, which has all these properties above.</p>
<p>And this brings us to the last part of probability spaces:</p>
<blockquote>
<p>A <strong>probability measure</strong> <span>\(P\)</span> on an event space <span>\(\mathcal{F}\)</span> is a function that:</p>
<ol>
<li><p>Maps events to the unit interval <span>\([0, 1]\)</span>,</p></li>
<li><p>Returns <span>\(0\)</span> for the empty set and <span>\(1\)</span> for the entire space,</p></li>
<li>
<p>Satisfies countable additivity for all countable collections of events
<span>\(\{E_i\}\)</span> of pairwise disjoint sets:</p>
<p>
\begin{equation*}
P(\cup_{i\in I} E_i) = \Sigma_{i\in I} P(E_i) \tag{2.1}
\end{equation*}
</p>
</li>
</ol>
</blockquote>
<p>These properties should look familiar as they are the three basic ones
axioms everyone learns when first studying probability.  The only difference is
that we&#39;re formalizing them, particularly the last one where we may not have
seen it with respect to infinite collections of events.</p>
<p>Going back to the &#34;volume&#34; analogy above, the probability measure maps the
&#34;chunks&#34; of our &#34;volume&#34; to <span>\([0,1]\)</span> (or non-negative real numbers for
general measures) but in a consistent way.  Due to the way we&#39;ve defined
event spaces as <span>\(\sigma\)</span>-algebra&#39;s along with the third condition from
Equation 2.1, we get a consistent measurement of &#34;volume&#34; regardless of how we
combine the &#34;chunks&#34;.  Again, for finite sample spaces, it&#39;s not too hard to
imagine this function but for continuous sample spaces, it gets more
complicated.  All this is essentially to define a rigorous construction that
matches our intuition of basic probability with samples spaces, events, and
probabilities.</p>
<p>Finally, for a given probability space <span>\((\Omega, {\mathcal {F}}, P)\)</span>:</p>
<blockquote>
<p>A <strong>random variable</strong> <span>\(X\)</span> <a href="#id4" id="id1">1</a> is a <a href="https://en.wikipedia.org/wiki/Measurable_function">measurable function</a>
<span>\(X:\Omega \rightarrow E \subseteq \mathbb{R}\)</span> where:</p>
<ol>
<li><p><span>\(X\)</span> must part of a measurable space, <span>\((E, \mathcal{S})\)</span> (recall:
<span>\(\mathcal{S}\)</span> defines a <span>\(\sigma\)</span>-algebra on the set <span>\(E\)</span>).
For finite or countably infinite values of <span>\(X\)</span>, we generally use
the powerset of <span>\(E\)</span>.  Otherwise, we will typically use the <a href="https://en.wikipedia.org/wiki/Borel_set">Borel set</a> for uncountably infinite
sets (e.g., the real numbers).</p></li>
<li>
<p>For all <span>\(s \in \mathcal{S}\)</span>, the pre-image of <span>\(s\)</span> under <span>\(X\)</span>
is in <span>\(\mathcal{F}\)</span>.  More precisely:</p>
<p>
\begin{equation*}
\{X \in \mathcal{s}\} \in \mathcal{F} := \{\omega \in \Omega | X(\omega) \in s\} \in \mathcal{F} \tag{2.2}
\end{equation*}
</p>
</li>
</ol>
</blockquote>
<p>We use random variables to map outcomes from our event space to the real line
(e.g., a RV for a coin flip where heads maps to 1 and tails maps to 0).
However, the mapping must also have the same consistency as we defined above.
So this definition basically ensures that every value that <span>\(X\)</span> can take on
(which must be measurable) has a mapping to one of the measurable events
in our original event space <span>\(\mathcal{F}\)</span>.  We use the notation
<span>\(\sigma(X)\)</span> to denote the collection of all subsets of Equation 2.2,
which form the <span>\(\sigma\)</span>-algebra implied by the random variable <span>\(X\)</span>.</p>
<p>If we didn&#39;t have this condition then either: (a) we couldn&#39;t properly measure
<span>\(X\)</span>&#39;s &#34;volume&#34; because our &#34;chunks&#34; would be inconsistent (constraint 1),
or (b) we wouldn&#39;t be able to map it back to &#34;chunks&#34; in our original
probability space and apply <span>\(P\)</span> to evaluate the random variable&#39;s
probability.  If this all seems a little abstract, it is, but that&#39;s what we need
when we&#39;re dealing with uncountable infinities.  Again, for the finite cases,
all of these properties are trivially met.</p>
<p>Using the probability measure <span>\(P\)</span>, one can calculate the probability of
<span>\(X \in \mathcal{S}\)</span> using Equation 2.2:</p>
<p>
\begin{align*}
P(X \in s) &amp;= P(\{\omega \in \Omega | X(\omega) \in s \}) \\
           &amp;= P(f \subseteq \mathcal{F}) \tag{2.3}
\end{align*}
</p>
<p>where <span>\(s \subseteq \mathcal{S}\)</span> and <span>\(f\)</span> is the corresponding event
in <span>\(\mathcal{F}\)</span>.  We can take <span>\(s = \{x\}\)</span> to evaluate the random
variable at a particular value.  Equation 2.3 basically says that we map
backwards from a set of real numbers (<span>\(s\)</span>) to a set of values in the
sample space (i.e., an event given by Equation 2.2) using the inverse of
function <span>\(X\)</span>.  From the event in our event space <span>\(f \subseteq
\mathcal{F}\)</span>, which is guaranteed to exist because of property (2), we know how
to compute the probability using <span>\(P\)</span>.</p>
<p>So a random variable then allows us to map to real numbers from our original
sample space (<span>\(\Omega\)</span>).  Often times our sample space has no concept
of numbers (e.g., heads or tails) but random variables allow us to assign real
numbers to those events to calculate things like expected values and variances.</p>
<p>For many applications of probability, understanding the above is overkill.
Most practitioners of probability can get away with the &#34;first stage&#34; (see box
below) of learning probability.  However specifically for stochastic calculus,
the above helps us learn it beyond a superficial level (arguably) because we
quickly get into situations where we need to understand the mathematical
rigour needed for uncountable infinities.</p>
<div>
<p>Example 1: Sample Spaces, Events, Probability Measures, and Random Variables</p>
<p>(From <a href="https://en.wikipedia.org/wiki/Event_(probability_theory)#A_simple_example">Wikipedia</a>)</p>
<p>Assume we have a standard 52 card playing deck without any jokers,
and our experiment is that we draw a card randomly from this set.
The sample space <span>\(\Omega\)</span> is a set consisting of the 52 cards.
An event <span>\(A \subseteq \mathcal{F}\)</span> is any subset of <span>\(\Omega\)</span>,
i.e., the powerset <span>\(\mathcal{F} = 2^{\Omega}\)</span>.  So that would include
the empty set, any single element, or even the entire sample space.  Some
examples of events:</p>
<ul>
<li><p>&#34;Cards that are red and black at the same time&#34; (0 elements)</p></li>
<li><p>&#34;The 5 of Hearts&#34; (1 element)</p></li>
<li><p>&#34;A King&#34; (4 elements)</p></li>
<li><p>&#34;A Face card&#34; (12 elements)</p></li>
<li><p>&#34;A card&#34; (52 elements)</p></li>
</ul>
<p>In the case where each card is equally likely to be drawn, we
can define a probability measure for event <span>\(A\)</span> as:</p>
<p>
\begin{equation*}
P(A) = \frac{|A|}{|\Omega|} = \frac{|A|}{52} \tag{2.4}
\end{equation*}
</p>
<p>We can additionally define a random variable as:</p>
<p>
\begin{equation*}
X(\omega \in \Omega) =
\begin{cases}
    1 &amp;\text{if } \omega \text{ is red}\\
    0 &amp;\text{otherwise}
\end{cases}
\tag{2.5}
\end{equation*}
</p>
<p>Which is a mapping from our sample space <span>\(\Omega\)</span> to a (finite) subset
of the real numbers <span>\(\{0, 1\}\)</span>.  We can calculate probabilities using
Equation 2.3, for example <span>\(X = 1\)</span>:</p>
<p>
\begin{align*}
P(X \in \{1\}) &amp;= P(\{\omega \in \Omega | X(\omega) \in \{1\} \}) \\
&amp;= P(\{\omega | \omega \text{ is a red card}\}) \\
&amp;= \frac{|\{\text{all red cards}\}|}{52} \\
&amp;= \frac{1}{2}  \\
\tag{2.6}
\end{align*}
</p>
<p>The implied <span>\(\sigma\)</span>-algebra of this random variable can be defined as:
<span>\(\sigma(X) = \{ \emptyset, \text{&#34;all red cards&#34;}, \text{&#34;all black cards&#34;}, \Omega \} \subset \mathcal{F}\)</span>.</p>
</div>
<div>
<p>The Two Stages of Learning Probability Theory</p>
<p><em>(Inspired by the notes from Chapter 1 in [1])</em></p>
<p>Probability theory is generally learned in two stages.  The first stage
describes discrete random variables that have a probability mass function,
and continuous random variables that have a density.  We learn to compute
basic quantities from these variables such as expectations, variances,
and conditionals.  We learn about standard distributions and their properties
and how to manipulate them such as
<a href="https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function">transforming continuous random variables</a>.
This gets us through most of the standard applications of probability
from basic statistical tests to likelihood functions.</p>
<p>The second stage of probability theory dives deep into the rigorous
measure theoretic definition.  In this definition, one views a
random variable as a function from a sample space <span>\(\Omega\)</span>
to a subset of the real numbers <span>\(\mathbb{R}\)</span>.  Certain subsets
of <span>\(\Omega\)</span> are called events, and the collection of all possible
events form a <span>\(\sigma\)</span>-algebra <span>\(\mathcal {F}\)</span>.  Each
set <span>\(A\)</span> in <span>\(\mathcal {F}\)</span> has probability <span>\(P(A)\)</span>,
defined by the probability measure <span>\(P\)</span>.
This definition handles both discrete and continuous variables in a elegant
way.  It also (as you would expect) introduces a lot of details underlying
the results that we learn in the first stage.  For example, a random
variable is not the same thing as a distribution (random variables can have
multiple probability distributions depending on the associated probability
measure).  Another quirk that we often don&#39;t think about is that not all
distributions have a density function (although most of the distributions
we study will have a density).  Like many things in applied mathematics,
understanding of the rigorous definition is often not needed because
most of the uses do not hit the corner cases where it matters (until it
doesn&#39;t).  It&#39;s also a whole lot of work to dig into so most folks
like me are happy to understand it only &#34;to a satisfactory degree&#34;.</p>
</div>
</div>
<div id="id2">
<h3><a href="#id9"><span>2.2</span> Stochastic Processes</a></h3>
<p>Here&#39;s the formal definition of a
<a href="https://en.wikipedia.org/wiki/Stochastic_process#Stochastic_process">stochastic process</a> from [2]:</p>
<blockquote>
<p>Suppose that <span>\((\Omega,\mathcal{F},P)\)</span> is a probability space, and that <span>\(T \subset \mathbb{R}\)</span>
is of infinite cardinality. Suppose further that for each <span>\(t \in T\)</span>,
there is a random variable <span>\(X_t: \Omega \rightarrow \mathbb{R}\)</span>
defined on <span>\((\Omega,\mathcal{F},P)\)</span>. The function <span>\(X: T \times \Omega \rightarrow \mathbb{R}\)</span>
defined by <span>\(X(t, \omega) = X_t(\omega)\)</span> is called a stochastic process with
indexing set <span>\(T\)</span>, and is written <span>\(X = \{X_t, t \in T\}\)</span>.</p>
</blockquote>
<p>That&#39;s a mouthful!  Let&#39;s break this down and interpret the definition more intuitively.
We&#39;ve already seen probability spaces and random variables in the previous
subsection.  The first layer of a stochastic process is that we have a bunch of
random variables that are indexed by some set <span>\(T\)</span>.  Usually <span>\(T\)</span> is
some total ordered sequence such as a subset of the real line (e.g., <span>\((0,
\infty)\)</span>) or natural numbers (e.g., <span>\(0, 1, 2, 3 \ldots\)</span>), which intuitively
correspond to continuous and discrete time.</p>
<p>Next, we turn to the probability space on which each random variable is defined on
<span>\((\Omega,\mathcal{F},P)\)</span>.  The key thing to note is that the elements of
the sample space <span>\(\omega \in \Omega\)</span> are infinite sets that correspond to
experiments performed at each index in <span>\(T\)</span>. (Note: by definition it&#39;s infinite
because otherwise it would just be a random vector.)  For example, flipping a
coin at every (discrete) time from <span>\(0\)</span> to <span>\(\infty\)</span>, would define a
specific infinite sequence of heads and tails <span>\(\omega = \{H, T, H, H, H, T, \ldots\}\)</span>.
So each random variable <span>\(X_t\)</span> can depend on the entire sequence of the
outcome of this infinite &#34;experiment&#34;.  That is, <span>\(X_t\)</span> is a mapping
from outcomes of our infinite experiment to (a subset of) the real numbers:
<span>\(X_t: \Omega \rightarrow E \subseteq \mathbb{R}\)</span>.
It&#39;s important to note that in this general definition we have no explicit
concept of time, so we can depend on the &#34;future&#34;.  To include our usual
concept of time, we need an additional concept (see adapted processes below).</p>
<p>Finally, instead of viewing the stochastic process as a collection of random variables
indexed by time, we could look at it as a function of both time and the sample space
i.e., <span>\(X(t, \omega) = X_t(\omega)\)</span>.  For a given outcome of an experiment
<span>\(\omega_0\)</span>, the deterministic function generated as <span>\(X(t, \omega=\omega_0)\)</span> is
called the <strong>sample function</strong>.  However, mostly we like to think of it
as having a random variable at each time step indicated by this notation:
<span>\(X = \{X_t, t \in T\}\)</span>.  We sometimes use the notation <span>\(X(t)\)</span> to refer
to the random variable at time <span>\(t\)</span> or the stochastic process itself.</p>
<p>Stochastic processes can be classified by the nature of the values the random variables
take and/or the nature of the index set:</p>
<ul>
<li><p><strong>Discrete and Continuous Value Processes</strong>: <span>\(X(t)\)</span> is discrete if at all &#34;times&#34; <span>\(X(t)\)</span> takes on values in a
<a href="https://en.wikipedia.org/wiki/Countable_set">countable set</a> (i.e., can be mapped to a subset of the natural numbers);
otherwise <span>\(X(t)\)</span> is continuous.</p></li>
<li><p><strong>Discrete and Continuous Time Processes</strong>: <span>\(X(t)\)</span> is discrete time process if the index set is
countable (i.e., can be mapped to a subset of the natural numbers), otherwise it is a continuous time process.</p></li>
</ul>
<p>Generally continuous time processes are harder to analyze and will be the focus
of later sections.  The next two discrete time examples give some intuition about
how to match the formal definition to concrete stochastic processes.</p>
<div>
<p>Example 2: Bernoulli Processes</p>
<p>One of the simplest stochastic processes is a
<a href="https://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli Process</a>, which
is a discrete value, discrete time process.  The main idea is that a
Bernoulli process is a sequence of independent and identically distributed
Bernoulli trials (think coin flips) at each time step.</p>
<p>More formally, our sample space <span>\(\Omega = \{ (a_n)_1^{\infty} : a_n
\in \{H, T\} \}\)</span> is the set of all infinite sequences of &#34;heads&#34; and &#34;tails&#34;.
It turns out the event space and the probability measure are surprisingly
complex to define so I&#39;ve put those details in Appendix A.</p>
<p>We can define the random variable given an outcome of infinite tosses
<span>\(\omega\)</span>:</p>
<p>
\begin{equation*}
X_t(\omega) =  \begin{cases}
    1 &amp;\text{if } \omega_t = H\\
    -1 &amp;\text{otherwise}
\end{cases} \tag{2.7}
\end{equation*}
</p>
<p>for <span>\(\omega = \omega_1 \omega_2 \omega_3 \ldots\)</span>, where each <span>\(\omega_i\)</span>
is the outcome of the <span>\(i^{th}\)</span> toss.
For all values of <span>\(t\)</span>, the probability <span>\(P(X_t = 1) = p\)</span>, for
some constant <span>\(p \in [0, 1]\)</span>.</p>
</div>
<div>
<p>Example 3: One Dimensional Symmetric Random Walk</p>
<p>A simple one dimensional symmetric <a href="https://en.wikipedia.org/wiki/Random_walk">random walk</a>
is a discrete value, discrete time stochastic process.  An easy way to
think of it is: starting at 0, at each time step, flip a fair coin and move
up (+1) if heads, otherwise move down (-1).</p>
<div>
<p><img alt="Scaled Symmetric Random Walk" src="https://bjlkeng.io/images/stochastic_calculus_random_walk.png"/></p><p><strong>Figure 1: 1D Symmetric Random Walk</strong> (<a href="https://towardsdatascience.com/random-walks-with-python-8420981bc4bc">source</a>)</p>
</div>
<p>This can be defined in terms of the Bernoulli process <span>\(X_t\)</span> from
Example 2 with <span>\(p=0.5\)</span> (with the same probability space):</p>
<p>
\begin{equation*}
S_t(\omega) =  \sum_{i=1}^t X_t \tag{2.8}
\end{equation*}
</p>
<p>Notice that the random variable at each time step depends on <em>all</em> the &#34;coin
flips&#34; <span>\(X_t\)</span> that came before it, which is in contrast to just the
current &#34;coin flip&#34; for the Bernoulli process.</p>
<p>Another couple of results that we&#39;ll use later.  The first is that the increments
between any two given non-overlapping pairs of integers
<span>\(0 = k_0 &lt; k_1 &lt; k_2 &lt; \ldots &lt; k_m\)</span> are independent.  That is,
<span>\((S_{k_1} - S_{k_0}), (S_{k_2} - S_{k_1}), (S_{k_3} - S_{k_2}), \ldots, (S_{k_m} - S_{k_{m-1}})\)</span>
are independent.  We can see this because for any combination of pairs of
these differences, we see that the independent <span>\(X_t\)</span> variables don&#39;t
overlap, so the sum of them must also be independent.</p>
<p>Moreover, the expected value and variance of the differences is given by:</p>
<p>
\begin{align*}
E[S_{k_{i+1}} - S_{k_i}] &amp;= E[\sum_{j=k_i + 1}^{k_{i+1}} X_j] \\
                         &amp;= \sum_{j=k_i + 1}^{k_{i+1}} E[X_j] \\
                         &amp;= 0 \\
Var[S_{k_{i+1}} - S_{k_i}] &amp;= Var[\sum_{j=k_i + 1}^{k_{i+1}} X_j] \\
                           &amp;= \sum_{j=k_i + 1}^{k_{i+1}} Var[X_j]  &amp;&amp; X_j \text{ independent}\\
                           &amp;= \sum_{j=k_i + 1}^{k_{i+1}} 1 &amp;&amp; Var[X_j] = E[X_j^2] = 1 \\
                           &amp;= k_{i+1} - k_i \\
\tag{2.9}
\end{align*}
</p>
<p>Which means that the variance of the symmetric random walk accumulates
at a rate of one per unit time.  So if you take <span>\(l\)</span> steps from the
current position, you can expect a variance of <span>\(l\)</span>.  We&#39;ll see this
pattern when we discuss the extension to continuous time.</p>
</div>
</div>
<div id="adapted-processes">
<h3><a href="#id10"><span>2.3</span> Adapted Processes</a></h3>
<p>Notice that in the previous section, our definition of stochastic process
included a random variable <span>\(X_t: \Omega \rightarrow E \subseteq \mathbb{R}\)</span>
where each <span>\(\omega \in \Omega\)</span> is an infinite sequence representing a
given outcome for the infinitely long experiment.  This implicitly means
that at &#34;time&#34; <span>\(t\)</span>, we could depend on the &#34;future&#34; because we are
allowed to depend on any tosses, including those greater than <span>\(t\)</span>.  In
many applications, we do want to interpret <span>\(t\)</span> as time so we wish to
restrict our definition of stochastic processes.</p>
<p>An <a href="https://en.wikipedia.org/wiki/Adapted_process">adapted stochastic process</a>
is one that cannot &#34;see into the future&#34;.  Informally, it means that for
any <span>\(X_t\)</span>, you can determine it&#39;s value by <em>only</em> seeing the outcome
of the experiment up to time <span>\(t\)</span> (i.e., <span>\(\omega_1\omega_2\ldots\omega_t\)</span> only).</p>
<p>To define this more formally, we need to introduce a few technical definitions.
We&#39;ve already seen the definition of the
<span>\(\sigma\)</span>-algebra <span>\(\sigma(X)\)</span> implied by the random variable
<span>\(X\)</span> in a previous subsections.  Suppose we have a subset of our event
space <span>\(\mathcal{G}\)</span>, we say that <span>\(X\)</span> is
<span>\(\mathcal{G}\)</span>-measurable if every set in <span>\(\sigma(X) \subseteq \mathcal{G}\)</span>.
That is, we can use <span>\(\mathcal{G}\)</span> to &#34;measure&#34; anything we do with <span>\(X\)</span>.</p>
<p>Using this idea, we define the concept of a filtration
on our event space <span>\(\mathcal{F}\)</span> and our index set <span>\(T\)</span>:</p>
<blockquote>
<p>A <strong>filtration</strong> <span>\(\mathbb{F}\)</span> is a ordered collection
of subsets <span>\(\mathbb{F} := (\mathcal{F_t})_{t\in T}\)</span> where
<span>\(\mathcal{F_t}\)</span> is a sub-<span>\(\sigma\)</span>-algebra of <span>\(\mathcal{F}\)</span>
and <span>\(\mathcal{F_{t_1}} \subseteq \mathcal{F_{t_2}}\)</span> for all
<span>\(t_1 \leq t_2\)</span>.</p>
</blockquote>
<p>To break this down, we&#39;re basically saying that our event space <span>\(\mathcal{F}\)</span>
can be broken down into logical &#34;sub event spaces&#34; <span>\(\mathcal{F_t}\)</span> such
that each one is a superset of the next one.  This is precisely what we want
where as we progress through time, we gain more &#34;information&#34; but never lose
any.  We can also use this idea of defining a sub-<span>\(\sigma\)</span>-algebra to
formally define conditional probabilities, although we won&#39;t cover that in this
post (see [1] for a more detailed treatment).</p>
<p>Using the construct of a filtration, we can define:</p>
<blockquote>
<p>A stochastic process <span>\(X_t : T \times \Omega\)</span> is <strong>adapted to the
filtration</strong> <span>\((\mathcal{F_t})_{t\in T}\)</span> if the random variable
<span>\(X_t\)</span> is <span>\(F_t\)</span>-measurable for all <span>\(t\)</span>.</p>
</blockquote>
<p>This basically says that <span>\(X_t\)</span> can only depend on &#34;information&#34; before or
at time <span>\(t\)</span>.  The &#34;information&#34; available is encapsulated by the
<span>\(\mathcal{F_t}\)</span> subsets of the event space.  These subsets of events are
the only ones we can compute probabilities on for that particular random
variable, thus effectively restricting the &#34;information&#34; we can use.
As with much of this topic, we require a lot of rigour in order to make sure we
don&#39;t have weird corner cases.  The next example gives more intuition on
the interplay between filtrations and random variables.</p>
<div>
<p>Example 4: An Adapted Bernoulli Processes</p>
<p>First, we need to define the filtration that we wish to adapt to our
Bernoulli Process.  Borrowing from Appendix A, repeating the two equations:</p>
<p>
\begin{align*}
A_H &amp;= \text{the set of all sequences beginning with } H = \{\omega: \omega_1 = H\} \\
A_T &amp;= \text{the set of all sequences beginning with } T = \{\omega: \omega_1 = T\} \\
\tag{2.10}
\end{align*}
</p>
<p>This basically defines two events (i.e., sets of infinite coin toss
sequences) that we use to define our probability measure.  We define our
first sub-<span>\(\sigma\)</span>-algebra using these two sets:</p>
<p>
\begin{equation*}
\mathcal{F}_1 = \{\emptyset, \Omega, A_H, A_T\} \tag{2.11}
\end{equation*}
</p>
<p>Let&#39;s notice that <span>\(\mathcal{F}_1 \subset \mathcal{F}\)</span> (by definition
since this is how we defined it). Also let&#39;s take a look at the events generated
by the random variable for heads and tails:</p>
<p>
\begin{align*}
\{X_1 \in \{1\}\} &amp;= \{\omega \in \Omega | X_1(\omega) \in \{1\}\} \\
 &amp;= \{\omega: \omega_1 = H\} \\
 &amp;= A_H \\
\{X_1 \in \{-1\}\} &amp;= \{\omega \in \Omega | X_1(\omega) \in \{-1\}\} \\
 &amp;= \{\omega: \omega_1 = T\} \\
 &amp;= A_T \\
 \tag{2.12}
\end{align*}
</p>
<p>Thus, <span>\(\sigma(X_1) = \mathcal{F}_1\)</span> (the <span>\(\sigma\)</span>-algebra implied by
the random variable <span>\(X_1\)</span>), meaning that <span>\(X_1\)</span> is indeed
<span>\(\mathcal{F}_1\)</span>-measurable as required.</p>
<p>Let&#39;s take a closer look at what this means.  For <span>\(X_1\)</span>, Equation 2.11 defines
the only types of events we can measure probability on, in plain English:
empty set, every possible outcome, outcomes starting with the first coin
flip as heads, and outcomes starting with the first coin flip as tails.
This corresponds to probabilities of <span>\(0, 1, p\)</span> and <span>\(1-p\)</span>
respectively, precisely the outcomes we would expect <span>\(X_1\)</span> to be able
to calculate.</p>
<p>On closer examination though, this is not exactly the same as a naive understanding
of the situation would imply.  <span>\(A_H\)</span> contains <em>every</em> infinitely long
sequence starting with heads -- not just the result of the first flip.
Recall, each &#34;time&#34;-indexed random variable in a stochastic process is a
function of an element of our sample space, which is an infinitely long sequence.
So we cannot naively pull out just the result of the first toss.  Instead, we
group all sequences that match our criteria (heads on the first toss) together
and use that as a grouping to perform our probability &#34;measurement&#34; on.  Again,
it may seem overly complicated but this rigour is needed to ensure we don&#39;t
run into weird problems with infinities.</p>
<p>Continuing on for later &#34;times&#34;, we can define <span>\(\mathcal{F}_2,
\mathcal{F}_3, \ldots\)</span> and so on in a similar manner. We&#39;ll find that each
<span>\(X_t\)</span> is indeed <span>\(\mathcal{F}_t\)</span> measurable (see Appendix A for
more details), and also find that each one is a superset of its
predecessor.  As a result, we can say that the Bernoulli process
<span>\(X(t)\)</span> is adapted to the filtration <span>\((\mathcal{F_t})_{t\in
\mathbb{N}}\)</span> as defined in Appendix A.</p>
</div>
</div>
<div id="weiner-process">
<h3><a href="#id11"><span>2.4</span> Weiner Process</a></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Wiener_process">Weiner process</a> (also known as
Brownian motion) is one of the most widely studied continuous time
stochastic processes.  It occurs frequently in many different domains such as
applied math, quantitative finance, and physics.  As alluded to previously, it
has many &#34;corner case&#34; properties that do not allow simple manipulation, and
it is one of the reasons why stochastic calculus was discovered.
Interestingly, there are several equivalent definitions but we&#39;ll start with
the one defined in [1] using scaled symmetric random walks.</p>
<div id="scaled-symmetric-random-walk">
<h4>
<span>2.4.1</span> Scaled Symmetric Random Walk</h4>
<p>A scaled symmetric random walk process is an extension of the simple random
walk we showed in Example 3 except that we &#34;speed up time and scale down the
step size&#34; and extend it to continuous time.  More precisely, for a fixed
positive integer <span>\(n\)</span>, we define the scaled random walk as:</p>
<p>
\begin{equation*}
W^{(n)}(t) = \frac{1}{\sqrt{n}}S_{nt} \tag{2.13}
\end{equation*}
</p>
<p>where <span>\(S_{nt}\)</span> is a simple symmetric random walk process, provided that
<span>\(nt\)</span> is an integer.  If <span>\(nt\)</span> is not an integer, we&#39;ll simply define
<span>\(W^{(n)}(t)\)</span> as the linear interpolation between it&#39;s nearest integer
values.</p>
<p>A simple way to think about Equation 2.13 is that it&#39;s just a regular random walk
with a scaling factor.  For example, <span>\(W^{(100)}(t)\)</span> has it&#39;s first step
(integer step) at <span>\(t=\frac{1}{100}\)</span> instead of <span>\(t=1\)</span>.  To adjust
for this compression of time we scale the process by <span>\(\frac{1}{\sqrt{n}}\)</span>
to make the math work out (more on this later).  The linear interpolation is
not that relevant except that we want to start working in continuous time.
Figure 2 shows a visualization of this compressed random walk.</p>
<div>
<p><img alt="Scaled Symmetric Random Walk" src="https://bjlkeng.io/images/stochastic_calculus_scaled_random_walk.png"/></p><p><strong>Figure 2: Scaled Symmetric Random Walk</strong> (<a href="https://slideplayer.com/slide/4387046/">source</a>)</p>
</div>
<p>Since this is just a simple symmetric random walk (assuming we&#39;re analyzing
it with its integer steps), the same properties hold as we discussed in Example
3.  Namely, that non-overlapping increments are independent.  Additionally, for
<span>\(0 \leq s \leq t\)</span>, we have:</p>
<p>
\begin{align*}
E[W^{(n)}(t) - W^{(n)}(s)] &amp;= 0 \\
Var[W^{(n)}(t) - W^{(n)}(s)] &amp;= t - s \\
\tag{2.14}
\end{align*}
</p>
<p>where we use the square root scaling to end up with variance accumulating still
at one unit per time.</p>
<p>Another important property is called the
<a href="https://en.wikipedia.org/wiki/Quadratic_variation">quadratic variation</a>,
which is calculated <em>along a specific path</em> (i.e., there&#39;s no randomness
involved).  For a scaled symmetric random walk where we know the exact path it
took up to time <span>\(t\)</span>, we get:</p>
<p>
\begin{align*}
[W^{(n)}, W^{(n)}]_t &amp;= \sum_{j=1}^{nt} (W^{(n)}(\frac{j}{n}) - W^{(n)}(\frac{j-1}{n}))^2 \\
&amp;= \sum_{j=1}^{nt} [\frac{1}{\sqrt{n}} X_j]^2  \\
&amp;= \sum_{j=1}^{nt} \frac{1}{n} = t \\
\tag{2.15}
\end{align*}
</p>
<p>This results in the same quantity as the variance computation we have (for
<span>\(s=0\)</span>) in Equation 2.14 but is conceptually different.  The variance
is an average over all paths while the quadratic variation is taking a
realized path, squaring all the values, and then summing them up.
In the specific case of a Wiener process, they result in the same thing (not
always the case for general stochastic processes).</p>
<p>Finally, as you might expect, we wish to understand what happens
to the scaled symmetric random walk when <span>\(n \to \infty\)</span>.
For a given <span>\(t\geq 0\)</span>, let&#39;s recall a few things:</p>
<ul>
<li><p><span>\(E[W^{(n)}(t)] = 0\)</span> (from Equation 2.14 with <span>\(s = 0\)</span>).</p></li>
<li><p><span>\(Var[W^{(n)}(t)] = t\)</span> (from Equation 2.14 with <span>\(s = 0\)</span>).</p></li>
<li><p><span>\(W^{(n)}(t) = \frac{1}{\sqrt{n}} \sum_{i=1}^t X_t\)</span> for Bernoulli process <span>\(X(t)\)</span>.</p></li>
<li><p>The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT">central limit theorem</a>
states that <span>\(\frac{1}{\sqrt{n}}\sum_{i=1}^n Y_i\)</span> converges
to <span>\(\mathcal{N}(\mu_Y, \sigma_Y^2)\)</span> as <span>\(n \to \infty\)</span> for IID
random variables <span>\(Y_i\)</span> (given some mild conditions).</p></li>
</ul>
<p>We can see that our symmetric scaled random walk fits precisely the conditions
as the central limit theorem, which means that as <span>\(n \to \infty\)</span>,
<span>\(W^{(n)}(t)\)</span> converges to a normal distribution with mean <span>\(0\)</span> and
variance <span>\(t\)</span>.  This limit is in fact the method in which we&#39;ll define
the Wiener process in the next subsection.</p>
</div>
<div id="wiener-process-definition">
<h4>
<span>2.4.2</span> Wiener Process Definition</h4>
<p>We finally arrive at the definition of the Wiener process, which will be the limit
of the scaled symmetric random walk as <span>\(n \to \infty\)</span>.  We&#39;ll define it
in terms of the properties of this limiting distribution, many of which are inherited
from the scaled symmetric random walk:</p>
<blockquote>
<p>Given probability space <span>\((\Omega, \mathcal{F}, P)\)</span>,
suppose there is a continuous function of <span>\(t \geq 0\)</span> that also
depends on <span>\(\omega \in \Omega\)</span> denoted as <span>\(W(t) := W(t, \omega)\)</span>.
<span>\(W(t)\)</span> is a <strong>Wiener process</strong> if the following are satisfied:</p>
<ol>
<li><p><span>\(W(0) = 0\)</span>;</p></li>
<li><p>All increments <span>\(W(t_1) - W(t_0), \ldots, W(t_m) - W(t_{m-1})\)</span>
for <span>\(0 = t_0 &lt; t_1 &lt; \ldots &lt; t_{m-1} &lt; t_{m}\)</span> are independent; and</p></li>
<li><p>Each increment is distributed normally with <span>\(E[W(t_{i+1} - t_i)] = 0\)</span> and
<span>\(Var[W(t_{i+1} - t_i)] = t_{i+1} - t_i\)</span>.</p></li>
</ol>
</blockquote>
<p>We can see that the Weiner process inherits many of the same properties as our scaled
symmetric random walk.  Namely, independent increments with each one being
distributed normally.  With the Weiner process the increments are exactly normal
instead of approximately normal (for large <span>\(n\)</span>) with the scaled symmetric
random walk.</p>
<p>One way to think of the Weiner process is that each <span>\(\omega\)</span> is a path generated
by a random experiment, for example, the random motion of a particle suspended
in a fluid.  At each infinitesimal point in time, it is perturbed randomly
(distributed normally) into a different direction.  In fact, this is the origin
of the phenomenon by botanist <a href="https://en.wikipedia.org/wiki/Robert_Brown_(botanist,_born_1773)">Robert Brown</a>
(although the math describing it came after by several others including Einstein).</p>
<p>Another way to think about the random motion is using our analogy of coin tosses.
<span>\(\omega\)</span> is still the outcome of an infinite sequence of coin tosses but
instead of happening at each integer value of <span>\(t\)</span>, they are happening
&#34;infinitely fast&#34;.  This is essentially the result of taking our limit to infinity.</p>
<p>We can ask any question that we would usually ask about random variables to the
Wiener process at a particular <span>\(t\)</span>.  The next example shows a few of
them.</p>
<div>
<p>Example 5: Weiner Process</p>
<p>Suppose we wish to determine the probability that the Weiner process
at <span>\(t=0.25\)</span> is between <span>\(0\)</span> and <span>\(0.25\)</span>.  Using
our rigourous jargon, we would say that we want to determine
the probability of the set <span>\(A \in \mathcal{F}\)</span> containing
<span>\(\omega \in \Omega\)</span> satisfying <span>\(0 \leq W(0.25) \leq 0.2\)</span>.</p>
<p>We know that each increment is normally distributed with expectation of
<span>\(0\)</span> and variance of <span>\(t_{i+1}-t_{i}\)</span>, so for the <span>\([0, 0.25]\)</span>
increment, we have:</p>
<p>
\begin{equation*}
W(0.25) - W(0) = W(0.25) - 0 = W(0.25) \sim N(0, 0.25) \tag{2.16}
\end{equation*}
</p>
<p>Thus, we are just asking the probability that a normal distribution takes
on these values, which we can easily compute using the normal distribution density:</p>
<p>
\begin{align*}
P(0 \leq W(0.25) \leq 0.2) &amp;= \frac{1}{\sqrt{2\pi(0.25)}} \int_0^{0.2} e^{-\frac{1}{2}(\frac{x}{\sqrt{0.25}})^2}  \\
                           &amp;= \frac{2}{\sqrt{2\pi}} \int_0^{0.2} e^{-2x^2}  \\
                           &amp;\approx 0.155 \\
                           \tag{2.17}
\end{align*}
</p>
</div>
<p>We also have the concept of filtrations for the Wiener process.  It uses the same definition
as we discussed previously except it also adds the condition that future increments
are independent of any <span>\(\mathcal{F_t}\)</span>.  As we will see below, we will be
using more complex adapted stochastic processes as integrands against a Wiener
process integrator.  This is why it&#39;s important to add this additional
condition of independence for future increments.  It&#39;s so the adapted
stochastic process (with respect to the Wiener process filtration) can be
properly integrated and cannot &#34;see into the future&#34;.</p>
</div>
<div id="quadratic-variation-of-wiener-process">
<h4>
<span>2.4.3</span> Quadratic Variation of Wiener Process</h4>
<p>We looked at the quadratic variation above for the scaled symmetric random walk
and concluded that it accumulates quadratic variation one unit per time (i.e.,
quadratic variation is <span>\(T\)</span> for <span>\([0, T]\)</span>) regardless of the value of
<span>\(n\)</span>.  We&#39;ll see that this is also true for the Wiener process but before we
do, let&#39;s first appreciate why this is strange.</p>
<blockquote>
<p>Let <span>\(f(t)\)</span> be a function defined on <span>\([0, T]\)</span>.  The
<strong>quadratic variation</strong> of <span>\(f\)</span> up to <span>\(T\)</span> is</p>
<p>
\begin{equation*}
[f, f](T) = \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}[f(t_{j+1}) - f(t_j)]^2 \tag{2.18}
\end{equation*}
</p>
<p>for <span>\(\Pi = \{t_0, t_1, \ldots, t_n\}\)</span>, <span>\(0\leq t_1 \leq t_2 &lt; \ldots &lt; t_n = T\)</span>
and <span>\(||\Pi|| = \max_{j=0,\ldots,n} (t_{j+1}-t_j)\)</span>.</p>
</blockquote>
<p>This is basically the same idea that we discussed before: for infinitesimally
small intervals, take the difference of the function for each interval,
square them, and then sum them all up.  Here we can have unevenly spaced
partitions with the only condition being that the largest partition has to go to
zero.  This is called the mesh or norm of the partitions, which is similar to
the formal definition of
<a href="https://en.wikipedia.org/wiki/Riemann_integral">Riemannian integrals</a>
(even though many of us, like myself, didn&#39;t learn it this way).  In any
case, the idea is very similar to just having evenly spaced intervals that go to zero.</p>
<p>Now that we have Equation 2.18, let&#39;s see how it behaves on a function
<span>\(f(t)\)</span> that has a continuous derivative:
(recall the <a href="https://en.wikipedia.org/wiki/Mean_value_theorem">mean value theorem</a>
states that <span>\(f&#39;(c) = \frac{f(a) - f(b)}{b-a}\)</span> for <span>\(c \in (a,b)\)</span>
if <span>\(f(x)\)</span> is a continuous function with derivatives on the respective interval):</p>
<blockquote>
<p>
\begin{align*}
[f, f](T) &amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}[f(t_{j+1}) - f(t_j)]^2   &amp;&amp; \text{definition} \\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}|f&#39;(t_j^*)|^2 (t_{j+1} - t_j)^2 &amp;&amp; \text{mean value theorem} \\
&amp;\leq \lim_{||\Pi|| \to 0} ||\Pi|| \sum_{j=0}^{n-1}|f&#39;(t_j^*)|^2 (t_{j+1} - t_j)  \\
&amp;= \big[\lim_{||\Pi|| \to 0} ||\Pi||\big] \big[\lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}|f&#39;(t_j^*)|^2 (t_{j+1} - t_j)\big] &amp;&amp; \text{limit product rule}  \\
&amp;= \big[\lim_{||\Pi|| \to 0} ||\Pi||\big] \int_0^T |f&#39;(t)|^2 dt = 0&amp;&amp; f&#39;(t) \text{ is continuous} \\
\tag{2.19}
\end{align*}
</p>
</blockquote>
<p>So we can see that quadratic variation is not very important for most functions
we are used to seeing i.e., ones with continuous derivatives.  In cases where
this is not true, we cannot use the mean value theorem to simplify quadratic
variation and we potentially will get something that is non-zero.</p>
<p>For the Wiener process in particular, we do not have a continuous derivative
and cannot use the mean value theorem as in Equation 2.19, so we end up with
non-zero quadratic variation.  To see this, let&#39;s take a look at the absolute
value function <span>\(f(t) = |t|\)</span> in Figure 3.  On the interval <span>\((-2, 5)\)</span>,
the slope between the two points is <span>\(\frac{3}{7}\)</span>, but nowhere in this
interval is the slope of the absolute value function <span>\(\frac{3}{7}\)</span> (it&#39;s
either constant 1 or constant -1 or undefined).</p>
<div>
<p><img alt="Mean value theorem does not apply on functions without derivatives" src="https://bjlkeng.io/images/stochastic_calculus_mvt.png"/></p><p><strong>Figure 3: Mean value theorem does not apply on functions without derivatives</strong> (<a href="https://people.math.sc.edu/meade/Bb-CalcI-WMI/Unit3/HTML-GIF/MeanValueTheorem.html">source</a>)</p>
</div>
<p>Recall, this is a similar situation to what we had for the scaled symmetric
random walk -- in between each of the discrete points, we used a linear
interpolation.  As we increase <span>\(n\)</span>, this &#34;pointy&#34; behaviour persists and
is inherited by the Wiener process where we no longer have a continuous
derivative.  Thus, we need to deal with this situation where we have a function
that is continuous everywhere, but differentiable nowhere.  This is one of the
key reasons why we need stochastic calculus, otherwise we could just use the
standard rules for calculus that we all know and love.</p>
<div>
<p><strong>Theorem 1</strong></p>
<p><em>For the Wiener process</em> <span>\(W\)</span>, <em>the quadratic variation is</em> <span>\([W,W](T) = T\)</span>
<em>for all</em> <span>\(T\geq 0\)</span> <em>almost surely.</em></p>
<p><strong>Proof</strong></p>
<p>Define the sampled quadratic variation for partition as above (Equation 2.18):</p>
<p>
\begin{equation*}
Q_{\Pi} = \sum_{j=0}^{n-1}\big( W(t_{j+1}) - W(t_j) \big)^2 \tag{2.20}
\end{equation*}
</p>
<p>This quantity is a random variable since it depends on the particular
&#34;outcome&#34; path of the Wiener process (recall quadratic variation is with
respect to a particular realized path).</p>
<p>To prove the theorem, we need to show that the sampled quadratic variation
converges to <span>\(T\)</span> as <span>\(||\Pi|| \to 0\)</span>.  This can be accomplished
by showing <span>\(E[Q_{\Pi}] = T\)</span> and <span>\(Var[Q_{\Pi}] = 0\)</span>, which says
that we will converge to <span>\(T\)</span> regardless of the path taken.</p>
<p>We know that each increment in the Wiener process is independent, thus
their sums are the sums of the respective means and variances of each
increment.  So given that we have:</p>
<p>
\begin{align*}
E[(W(t_{j+1})-W(t_j))^2] &amp;= E[(W(t_{j+1})-W(t_j))^2] - 0 \\
                         &amp;= E[(W(t_{j+1})-W(t_j))^2] - E[W(t_{j+1})-W(t_j)]^2 &amp;&amp; \text{definition of the Wiener process}\\
                         &amp;= Var[W(t_{j+1})-W(t_j)]  \\
                         &amp;= t_{j+1} -  t_j &amp;&amp; \text{definition of the Wiener process}\\
                         \tag{2.21}
\end{align*}
</p>
<p>We can easily compute <span>\(E[Q_{\Pi}]\)</span> as desired:</p>
<p>
\begin{align*}
&amp;E[Q_{\Pi}] \\
&amp;= E[ \sum_{j=0}^{n-1}\big( W(t_{j+1}) - W(t_j) \big)^2 ] \\
&amp;= \sum_{j=0}^{n-1} E[W(t_{j+1}) - W(t_j)]^2 \\
&amp;= \sum_{j=0}^{n-1} (t_{j+1} - t_j)  &amp;&amp; \text{Equation } 2.21 \\
&amp;= T \\
\tag{2.22}
\end{align*}
</p>
<p>From here, we use the <a href="https://math.stackexchange.com/questions/1917647/proving-ex4-3%CF%834">fact</a>
that the expected value of the fourth moment of a normal random variable
with zero mean is three times its variance.  Anticipating the quantity
we&#39;ll need to compute the variance, we have:</p>
<p>
\begin{equation*}
E\big[(W(t_{j+1})-W(t_j))^4 \big] = 3Var[(W(t_{j+1})-W(t_j)] = 3(t_{j+1} - t_j)^2 \tag{2.23}
\end{equation*}
</p>
<p>Computing the variance of the quadratic variation for each increment:</p>
<p>
\begin{align*}
&amp;Var\big[(W(t_{j+1})-W(t_j))^2 \big] \\
&amp;= E\big[\big( (W(t_{j+1})-W(t_j))^2 -  E[(W(t_{j+1})-W(t_j))^2] \big)^2\big] &amp;&amp; \text{definition of variance} \\
&amp;= E\big[\big( (W(t_{j+1})-W(t_j))^2 -  (t_{j+1} - t_j) \big)^2\big] &amp;&amp; \text{Equation } 2.21 \\
&amp;= E[(W(t_{j+1})-W(t_j))^4] - 2(t_{j+1}-t_j)E[(W(t_{j+1})-W(t_j))^2] + (t_{j+1} - t_j)^2 \\
&amp;= 3(t_{j+1}-t_j)^2 - 2(t_{j+1}-t_j)^2 + (t_{j+1} - t_j)^2 &amp;&amp; \text{Equation } 2.21/2.23 \\
&amp;= 2(t_{j+1}-t_j)^2 \\
\tag{2.24}
\end{align*}
</p>
<p>From here, we can finally compute the variance:</p>
<p>
\begin{align*}
Var[Q_\Pi] &amp;= \sum_{j=0}^{n-1} Var\big[ (W(t_{j+1} - W(t_j)))^2 \big]  \\
           &amp;= \sum_{j=0}^{n-1} 2(t_{j+1}-t_j)^2  &amp;&amp; \text{Equation } 2.24 \\
           &amp;\leq  \sum_{j=0}^{n-1} 2 ||\Pi|| (t_{j+1}-t_j)  \\
           &amp;= 2 ||\Pi|| T &amp;&amp; \text{Equation } 2.22 \\
           \tag{2.25}
\end{align*}
</p>
<p>As <span>\(\lim_{||\Pi|| \to 0} Var[Q_\Pi] = 0\)</span>, therefore we have shown that
<span>\(\lim_{||\Pi|| \to 0} Q_\Pi = T\)</span> as required.</p>
</div>
<p>The term <a href="https://en.wikipedia.org/wiki/Almost_surely">almost surely</a>  is a
technical term meaning with probability 1.  This is another unintuitive idea
when dealing with infinities.  The theorem doesn&#39;t say that there are no paths
with different quadratic variation, it only says those paths are negligible in
size with respect to the infinite number of paths, and thus have probability
zero.</p>
<p>Taking a step back, this is quite a profound result: if you take <em>any</em> realized
path of the Wiener process, sum the infinitesimally small squared increments of
that paths, it equals the length of the interval almost surely. In other words,
<em>the Wiener process accumulates quadratic variation at a rate of one unit per
time</em>.</p>
<p>This is perhaps surprising result because it can be <em>any</em> path.  It doesn&#39;t
matter how the &#34;infinitely fast&#34; coin flips land, the sum of the square
increments will always approach the length of the interval.  The fact
that it&#39;s also non-zero is surprising too despite the path being continuous (but
without a continuous derivative) as we discussed above.</p>
<p>We often will informally write:</p>
<p>
\begin{equation*}
dW(t)dW(t) = dt \tag{2.26}
\end{equation*}
</p>
<p>To describe the accumulation of quadratic variation at one unit per time.
However, this should not be interpreted to be true for each infinitesimally
small increment.  Recall each increment of <span>\(W(t)\)</span> is normally distributed, so the
LHS of Equation 2.26 is actually distributed as the square of a normal
distribution.  We only get the result of Theorem 1 when we sum a large number
of them (see [1] for more details).</p>
<p>We can also use this informal notation to describe a few other related concepts.
The cross variation (Equation 2.27) and quadratic of variation for the time
variable (Equation 2.28) respectively:</p>
<p>
\begin{align*}
dW(t)dt &amp;= 0 \tag{2.27} \\
dtdt &amp;= 0 \tag{2.28}
\end{align*}
</p>
<p>The quadratic variation for time can use the same definition from Equation 2.18
above, and the cross variation just uses two different function (<span>\(W(t)\)</span>
and <span>\(t\)</span>) instead of the same function.  Intuitively, both of these are
zero because the time increment (<span>\(\Pi\)</span>) goes to zero in the limit by
definition, thus so do these two variations.  This can be shown more formally
using similar arguments as the quadratic variation above (see [1] for more details).</p>
</div>
<div id="first-passage-time-for-wiener-process">
<h4>
<span>2.4.4</span> First Passage Time for Wiener Process</h4>
<p>We digress here to show a non-intuitive property of the Wiener process: it will
<em>eventually</em> be equal to a given level <span>\(m\)</span>.</p>
<div>
<p><strong>Theorem 2</strong></p>
<p><em>For</em> <span>\(m \in \mathbb{R}\)</span>, <em>the first passage time</em> <span>\(\tau_m\)</span> <em>of
the Wiener process to level</em> <span>\(m\)</span> <em>is finite almost surely, i.e.,</em>
<span>\(P(\tau_m &lt; \infty) = 1\)</span>.</p>
</div>
<p>This basically says that the Wiener process is almost certain to reach whatever
finite level within some finite time <span>\(\tau_m\)</span>.  Again, there is a
possible realized path of the Wiener process that does not exceed a given level
<span>\(m\)</span> but they collectively are so infinitesimally small that they are
assigned probability 0 (almost surely).  Working with infinities
can be unintuitive.</p>
</div>
</div>
<div id="the-relationship-between-the-wiener-process-and-white-noise">
<h3><a href="#id12"><span>2.5</span> The Relationship Between the Wiener Process and White Noise</a></h3>
<p>The Wiener process can be characterized in several equivalent ways with the
definition above being one of the most common.  Another common way to define
it is from the white noise we discussed in the motivation section.  In this
definition, the Wiener process is the definite integral of Gaussian white
noise, or equivalently, Gaussian white noise is the derivative of the Wiener
process:</p>
<p>
\begin{align*}
W(t) &amp;= \int_0^t \eta(s)ds \tag{2.29} \\
\frac{dW(t)}{dt} &amp;= \eta(s) \tag{2.30}
\end{align*}
</p>
<p>To understand why this relationship is true, let&#39;s first define the derivative
of a stochastic process from [4]:</p>
<blockquote>
<p>A stochastic process <span>\(X(t)\)</span>, <span>\(t \in \mathbb{R}\)</span>, is said to be
differentiable in quadratic mean with derivative <span>\(X&#39;(t)\)</span> if</p>
<p>
\begin{align*}
\frac{X(t+h) - X(t)}{h} &amp;\to X&#39;(t) \\
E\big[(\frac{X(t+h) - X(t)}{h} - X&#39;(t))^2 \big] &amp;\to 0 \\
\tag{2.31}
\end{align*}
</p>
<p>when <span>\(h \to 0\)</span>.</p>
</blockquote>
<p>We can see that the definition is basically the same as regular calculus
except that we require the expectation to go to zero with a weaker squared
convergence, which we&#39;ll see appear again in the next section.</p>
<p>From this definition, we can calculate the mean of the derivative of <span>\(W(t)\)</span> as:</p>
<p>
\begin{align*}
E[\frac{dW(t)}{dt}] &amp;= E[\lim_{h\to 0} \frac{W(t+h) - W(t)}{h}] \\
&amp;= \lim_{h\to 0} \frac{E[W(t+h)] - E[W(t)]}{h} \\
&amp;= \lim_{h\to 0} \frac{0 - 0}{h} \\
&amp;= 0\\
\tag{2.32}
\end{align*}
</p>
<p>Similarly, we can show a general property about the time correlation of a
derivative of a stochastic process:</p>
<p>
\begin{align*}
C_{X&#39;}(t_1, t_2) &amp;= E\big[
    \lim_{k\to 0} \frac{X(t_1 + k) - X(t_1)}{k}
    \lim_{h\to 0} \frac{X(t_2 + h) - X(t_2)}{h}
\big]\\
&amp;= \lim_{h\to 0} \frac{1}{h}
   \lim_{k\to 0} E\big[\frac{(X(t_1 + k) - X(t_1))(X(t_2 + h) - X(t_2))}{k}\big] \\
&amp;= \lim_{h\to 0} \frac{1}{h}
   \lim_{k\to 0}\big( \frac{E[X(t_1 + k)X(t_2+h)] - E[X(t_1+k)X(t_2)]
                            -E[X(t_1)X(t_2+h)] + E[X(t_1)X(t_2)]}{k}\big) \\
&amp;= \lim_{h\to 0} \frac{1}{h}
   \lim_{k\to 0}\big( \frac{C_X(t_1 + k, t_2+h) -C_X(t_1, t_2+h)}{k}
                      - \frac{C_X(t_1+k, t_2) - C_X(t_1, t_2)}{k}\big) \\
&amp;= \lim_{h\to 0} \frac{1}{h}
   \big( \frac{\partial C_X(t_1, t_2+h)}{\partial t_1} -
         \frac{\partial C_X(t_1, t_2)}{\partial t_1} \big) \\
&amp;= \frac{\partial C_X(t_1, t_2)}{\partial t_1 \partial t_2} \tag{2.33}
\end{align*}
</p>
<p>Thus we have shown that the time correlation of the derivative of a stochastic
process is the mixed second-order partial derivative.  Now all we have to do
is evaluate it for the Wiener process.</p>
<p>First, assuming <span>\(t_1 &lt; t_2\)</span> the Wiener process time correlation is given by
(see this <a href="https://math.stackexchange.com/questions/884299/autocorrelation-of-a-wiener-process-proof">StackExchange answer</a>
for more details):</p>
<p>
\begin{align*}
0 &amp;= E[W(t_1)(W(t_2) - W(t_1))] &amp;&amp; \text{independent increments} \\
&amp;= E[W(t_1)W(t_2)] - E[(W(t_1))^2] \\
&amp;= E[W(t_1)W(t_2)] - t_1 &amp;&amp; Var(W(t_1)) = t_1 \\
C_W(t_1, t_2) &amp;= E[W(t_1)W(t_2)] = t_1 = \min(t_1, t_2) \\
\tag{2.34}
\end{align*}
</p>
<p>We get the same result if <span>\(t_2 &lt; t_1\)</span>, thus <span>\(C_W(t_1, t_2) = \min(t_1, t_2)\)</span>.
Now we have to figure out how to take the second order partial derivatives.
The first partial derivative is easy as long as <span>\(t_1 \neq t_2\)</span>
(see this <a href="https://math.stackexchange.com/questions/150960/derivative-of-the-fx-y-minx-y">answer</a> on StackExchange):</p>
<p>
\begin{align*}
\frac{\partial \min(t_1, t_2)}{\partial t_1} &amp;= \begin{cases}
1 &amp; \text{if } t_1 \lt t_2 \\
0 &amp; \text{if } t_2 \gt t_1
\end{cases} \\
&amp;= H(t_2 - t_1) &amp;&amp; \text{everywhere except } t_1=t_2 \\
\tag{2.35}
\end{align*}
</p>
<p>where <span>\(H(x)\)</span> is the
<a href="https://en.wikipedia.org/wiki/Heaviside_step_function">Heaviside step function</a>.
But we know the derivative of this step function is just the Dirac delta
function (even with the missing point), so:</p>
<p>
\begin{equation*}
C_{W&#39;}(t_1, t_2) = \frac{\partial \min(t_1, t_2)}{\partial t_1\partial t_2}
= \frac{\partial H(t_2-t_1)}{\partial{t_2}} = \delta(t_2-t_1) \tag{2.36}
\end{equation*}
</p>
<p>From Equation 2.32 and 2.36, we see we have the same statistics as the white noise
we defined in the motivation section above in Equation 1.4.  Since the mean
is also zero, the covariance is equal to the time correlation too:
<span>\(Cov_{W&#39;}(t_1, t_2) = C_{W&#39;}(t_1, t_2)\)</span></p>
<p>Now all we have to show is that it is also normally distributed.  By definition
(given above) the Wiener stochastic process has derivative:</p>
<p>
\begin{equation*}
\frac{dW(t)}{dt} = \lim_{h\to 0} \frac{W(t + h) - W(t)}{h} \tag{2.37}
\end{equation*}
</p>
<p>But since each increment of the Wiener process is normally distributed (and independent),
the derivative from Equation 2.37 is also normally distributed since the difference of two
independent normals is normally distributed.
This implies the derivative of the Wiener process is a Gaussian process with
zero mean and delta time correlation, which is the standard definition of
Gaussian white noise.  Thus, we have shown the relationship in Equation 2.29 /
2.30.</p>
</div>
<div id="the-importance-of-the-wiener-process">
<h3><a href="#id13"><span>2.6</span> The Importance of the Wiener Process</a></h3>
<p>One question that you might ask (especially after reading the next section) is
why is there so much focus on the Wiener process?  It turns out that the Wiener
process is the <em>only</em> (up to a scaling factor and drift term) continuous
process with stationary independent increments [5].  Let&#39;s be more precise.</p>
<p>A stochastic process is said to have independent increments if <span>\(X(t) - X(s)\)</span>
is independent of <span>\(\{X(u)\}_{u\leq s}\)</span> for all <span>\(s\leq t\)</span>.  If
the distribution of the increments don&#39;t depend on <span>\(s\)</span> or <span>\(t\)</span>
directly (but can depend on <span>\(t-s\)</span>), then the increments are called
stationary.  This leads us to this important result:</p>
<div>
<p><strong>Theorem 3</strong></p>
<p>Any continuous real-valued process <span>\(X\)</span> with stationary independent
increments can be written as:</p>
<p>
\begin{equation*}
X(t) = X(0) + bt + \sigma W(t) \tag{2.38}
\end{equation*}
</p>
<p>where <span>\(b, \sigma\)</span> are constants.</p>
</div>
<p>Equation 2.38 is the generalized Wiener process that includes a potentially
non-zero initial value <span>\(X(0)\)</span>, deterministic drift term <span>\(bt\)</span>, and
scaling factor <span>\(\sigma\)</span>.</p>
<p>The intuition behind Theorem 3 follows directly from the central limit theorem.
For a given interval <span>\([s, t]\)</span>, the value of <span>\(X(t) - X(s)\)</span> is the sum
of infinitesimally small independent, identically distributed partitions,
or in other words IID random variables (doesn&#39;t have to be normally
distributed).  Thus, we can apply the central limit theorem and get a normal
distribution (under some mild conditions).</p>
<p>Processes with independent increments appear in many contexts.  For example,
the random displacement of a macro particle moving through a fluid caused by the
random interactions with the fluid molecules is naturally modelled using the
Wiener process.  Similarly, the variability of the return of a stock price in a
very short period of time is approximately the same regardless of the price,
thus can also be modelled using a Wiener process.  We&#39;ll look at both of these
examples more closely later on in the post.</p>
</div>
</div><div id="stochastic-calculus">
<h2><a href="#id14"><span>3</span> Stochastic Calculus</a></h2>
<p>One of the main goals of stochastic calculus is to make sense of the following integral:</p>
<p>
\begin{equation*}
\int_0^t H(s) dX(s) \tag{3.1}
\end{equation*}
</p>
<p>where <span>\(X(t)\)</span> and <span>\(H(t)\)</span> are two special types of stochastic
processes.  A few questions immediately come to mind:</p>
<ol>
<li><p><em>What &#34;thing&#34; do we get out of the stochastic integral?</em>  This is pretty
simple, it&#39;s another stochastic process, although it&#39;s not immediately clear
that should be case, but rather something that becomes more obvious once we
see the definition.</p></li>
<li><p><em>How do we deal with the limits of integration being in terms of
time</em> <span>\(t\)</span> <em>but the integrand and integrator being stochastic processes
with time index set</em> <span>\(t\)</span>?  We&#39;ll see below that the definition of the
integral is conceptually not too different from a plain old <a href="https://en.wikipedia.org/wiki/Riemann_integral">Riemannian integral</a> that we learn in
regular calculus, but with some key differences due to the nature of
the stochastic processes we use (e.g., Wiener process).</p></li>
<li><p><em>How do we deal with the case of a non-continuous derivative of the
integrator (e.g., Wiener process), which manifests itself with non-zero
quadratic variation?</em> We&#39;ll see that this results in one of the big
differences with regular calculus.  Choices that didn&#39;t matter, suddenly
matter, and the result produces different outputs from the usual integration
operation.</p></li>
</ol>
<p>All the depth we went into previously is about to pay off!  We&#39;ll have to use
all of those ideas in order to properly define Equation 3.1.  We&#39;ll start with
defining the simpler cases where <span>\(X(t)\)</span> is a Wiener process, and
generalize it to be any It√¥ process, and then introduce the key result called
It√¥&#39;s lemma, a conceptual form of the chain rule, which will allow us to solve
many more interesting problems.</p>
<div id="stochastic-integrals-with-brownian-motion">
<h3><a href="#id15"><span>3.1</span> Stochastic Integrals with Brownian Motion</a></h3>
<p>To begin, we&#39;ll start with the simplest case when the integrator (<span>\(dX(t)\)</span>
in Equation 3.1) is the Wiener process.  For this simple case, we can define
the integral as:</p>
<p>
\begin{equation*}
\int_0^t H(s) dW(s) := \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} H(s_j)[W(t_{j+1}) - W(t_j)] \tag{3.2}
\end{equation*}
</p>
<p>where <span>\(t_j \leq s_j \leq t_{j+1}\)</span>, and <span>\(||\Pi||\)</span> is the mesh (or
maximum partition) that goes to zero while the number of partitions goes to infinity
like in Equation 2.18 (and standard Riemannian integrals).</p>
<p>From a high level, Equation 3.2 is not too different from our usual Riemannian
integrals.  However, we have to note that instead of having a <span>\(dt\)</span>, we
have a <span>\(dW(s)\)</span>.  This makes the results more volatile than a regular
integral.  Let&#39;s contrast the difference between approximating a regular
and stochastic integral for a small step size <span>\(\Delta t\)</span> starting
from <span>\(t\)</span>:</p>
<p>
\begin{align*}
R(t + \Delta t) &amp;:= \int_0^{t+\Delta t} H(s) ds \approx R(t) + H(t)\Delta t \tag{3.3} \\
I(t + \Delta t) &amp;:= \int_0^{t+\Delta t} H(s) dW(s) \approx I(t) + H(t)(W(t + \Delta t) - W(t)) \tag{3.4}
\end{align*}
</p>
<p><span>\(R(t)\)</span> changes more predictably than <span>\(I(t)\)</span> since we know that each
increment changes by <span>\(H(s)\Delta t\)</span>.  Note that <span>\(H(s)\)</span> can still be
a random (and <span>\(R(t)\)</span> can be random as well) but its change is multiplied by a
deterministic <span>\(\Delta t\)</span>.  This is in contrast to <span>\(I(t)\)</span> which changes
by <span>\(W(t + \Delta t) - W(t)\)</span>.  Recall that each increment of the Wiener process
is independent and distributed normally with <span>\(\mathcal{N}(0, \Delta t)\)</span>.
Thus <span>\(H(t)(W(t + \Delta t) - W(t))\)</span> changes much more randomly and erratically because
our increments follow an <em>independent</em> normal distribution versus just a
<span>\(\Delta t\)</span>.  This is one of the key intuitions why we need to define a
new type of calculus.</p>
<p>To ensure that the stochastic integral in Equation 3.2 is well defined, we need
a few conditions, which I will just quickly summarize:</p>
<ol>
<li><p>The choice of <span>\(s_j\)</span> is quite important (unlike regular integrals).
The <a href="https://en.wikipedia.org/wiki/Stochastic_calculus#It%C3%B4_integral">It√¥ integral</a>
uses <span>\(s_j = t_j\)</span>, which is more common in finance; the
<a href="https://en.wikipedia.org/wiki/Stochastic_calculus#Stratonovich_integral">Stratonovich integral</a>
uses <span>\(s_j = \frac{(t_j + t_{j+1})}{2}\)</span>, which is more common in physics.
We&#39;ll be using the It√¥ integral for most of this post, but will show the difference
in the example below.</p></li>
<li><p><span>\(H(t)\)</span> must be adapted to the same process as our integrator
<span>\(X(t)\)</span>, otherwise we would be allowing it to &#34;see into the
future&#34;.  For most of our applications, this is a very reasonable assumption.</p></li>
<li><p>The integrand needs to have square-integrability: <span>\(E[\int_0^T H^2(t)dt] &lt; \infty\)</span>.</p></li>
<li>
<p>We ideally want to ensure that each sample point of the integrand
<span>\(H(s_j)\)</span> from Equation 3.2 converges in the limit to <span>\(H(s)\)</span> with
probability one (remember we&#39;re still working with stochastic processes here).
That&#39;s a pretty strong condition, so we&#39;ll actually use a weaker
squared convergence as:</p>
<p>
\begin{equation*}
\lim_{n \to \infty} E\big[\int_0^T |H_n(t) - H(t)|^2 dt\big] = 0 \tag{3.5}
\end{equation*}
</p>
<p>where we define  <span>\(H_n(s) := H(t_j)\)</span> for <span>\(t_j \leq s &lt; t_{j+1}\)</span>
i.e., it&#39;s the constant piece-wise approximation for <span>\(H(t)\)</span> using the
left most point for the interval.</p>
</li>
</ol>
<div>
<p>Example 6: A Simple Stochastic Integral in Two Ways</p>
<p>Let&#39;s work through the simple integral where the integrand and integrator are
both the Wiener process:</p>
<p>
\begin{equation*}
\int_0^t W(s) dW(s) = \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} W(s_j)[W(t_{j+1}) - W(t_j)] \tag{3.6}
\end{equation*}
</p>
<p>First, we&#39;ll work through it using the It√¥ convention where <span>\(s_j=t_j\)</span>:</p>
<p>
\begin{align*}
\int_0^t W(s) dW(s) &amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} W(t_j)[W(t_{j+1}) - W(t_j)] \\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} \big[W(t_j)W(t_{j+1}) - W(t_j)^2 + \frac{1}{2}W(t_{j+1})^2 - \frac{1}{2}W(t_{j+1})^2 \big]\\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}
\big[\frac{1}{2}W(t_{j+1})^2 - \frac{1}{2}W(t_j)^2
- \frac{1}{2}W(t_{j+1})^2 + W(t_j)W(t_{j+1}) - \frac{1}{2}W(t_j)^2 \big]\\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}
\frac{1}{2}[W(t_{j+1})^2 - W(t_j)^2] - \frac{1}{2}[W(t_{j+1}) - W(t_{j})]^2 \\
\tag{3.7}
\end{align*}
</p>
<p>The first term is just a telescoping sum, which has massive cancellation:</p>
<p>
\begin{equation*}
\lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} \frac{1}{2}[W(t_{j+1})^2 - W(t_j)^2] = \frac{1}{2}(W(t)^2 - W(0)^2)
= \frac{1}{2} W(t)^2 - 0 = \frac{W(t)^2}{2}  \tag{3.8}
\end{equation*}
</p>
<p>The second term you&#39;ll notice is precisely the quadratic variance from Theorem 1,
which we knows equals the interval <span>\(t\)</span>.  Putting it together, we have:</p>
<p>
\begin{equation*}
\int_0^t W(s) dW(s) =  \frac{W(t)^2}{2} - \frac{t}{2} \tag{3.9}
\end{equation*}
</p>
<p>We&#39;ll notice that this <em>almost</em> looks like the result from calculus i.e.,
<span>\(\int x dx = \frac{x^2}{2}\)</span>, except with an extra term.  As we saw
above the extra term comes in precisely because we have non-zero quadratic
variation.  If the Wiener process had continuous differentiable paths, then
we wouldn&#39;t need all this extra work with stochastic integrals.</p>
<hr/>
<p>Now let&#39;s look at what happens when we use the Stratonovich convention
(using the <span>\(\circ\)</span> operator to denote it) with <span>\(s_j = \frac{t_j + t_{j+1}}{2}\)</span>:</p>
<p>
\begin{align*}
&amp;\int_0^t W(s) \circ dW(s) \\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} W(s_j)[W(t_{j+1}) - W(t_j)] \\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} \big[W(s_j)W(t_{j+1}) - W(s_j)W(t_j) +  W(t_j)W(s_j) - W(t_j)W(s_j) \\
&amp;+ W(t_j)^2 - W(t_j)^2 + W(s_j)^2 - W(s_j)^2 \big] \\
&amp;= \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} \big[W(t_j)(W(s_j) - W(t_j)) + W(s_j)(W(t_{j+1}) - W(s_j)) \big]  \\
&amp;+ \sum_{j=0}^{n-1}\big[ W(s_j) - W(t_j) \big]^2 \\
&amp;= \int_0^t W(s) dW(s) + \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}\big[ W(s_j) - W(t_j) \big]^2
&amp;&amp; \text{It√¥ integral with partitions } t_0, s_0, t_1, s_1, \ldots \\
&amp;= \frac{W(t)^2}{2} - \frac{t}{2} + \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1}\big[ W(s_j) - W(t_j) \big]^2
&amp;&amp; \text{Equation 3.9} \\
&amp;= \frac{W(t)^2}{2} - \frac{t}{2} + \frac{t}{2} &amp;&amp; \text{Half-sample quadratic variation} \\
&amp;= \frac{W(t)^2}{2} \\
\tag{3.10}
\end{align*}
</p>
<p>We use the fact that the half-sample quadratic variation is equal to
<span>\(\frac{t}{2}\)</span> using a similar proof to Theorem 1.</p>
<p>What we see here is that the Stratonovich integral actually follows our
regular rules of calculus more closely, which is the reason it&#39;s used
in certain domains.  However in many domains such as finance, it is not
appropriate.  This is because the integrand represents a decision
we are making for a time interval <span>\([t_j, t_{j+1}]\)</span>, such as a
position in an asset, and we have to decide that <em>before</em> that interval starts,
not mid-way through.  That&#39;s analogous to deciding in the middle of the day
that I should have actually bought more of a stock at the start of the day
for a stock that went up in price.</p>
</div>
<div id="quadratic-variation-of-stochastic-integrals-with-brownian-motion">
<h4>
<span>3.1.1</span> Quadratic Variation of Stochastic Integrals with Brownian Motion</h4>
<p>Let&#39;s look at the quadratic variation (or sum of squared incremental
differences) along a particular path for the stochastic integral we
just defined above, and a related property.  Note: the &#34;output&#34; of the
stochastic integral is a stochastic process.</p>
<div>
<p><strong>Theorem 3</strong></p>
<p><em>The quadratic variation accumulated up to time</em> <span>\(t\)</span> <em>by the It√¥ integral
with the Wiener process</em> (<em>denoted by</em> <span>\(I\)</span>) <em>from Equation 3.2 is</em>:</p>
<p>
\begin{equation*}
[I, I] = \int_0^t H^2(s) ds \tag{3.11}
\end{equation*}
</p>
</div>
<div>
<p><strong>Theorem 4 (It√¥ isometry)</strong></p>
<p><em>The It√¥ integral with the Wiener process from Equation 3.2 satisfies</em>:</p>
<p>
\begin{equation*}
Var(I(t)) = E[I^2(t)] = E\big[\int_0^t H^2(s) ds\big] \tag{3.12}
\end{equation*}
</p>
</div>
<p>A couple things to notice.  First, the quadratic variation is &#34;scaled&#34; by the
underlying integrand <span>\(H(t)\)</span> as opposed to accumulating quadratic
variation at one unit per time from the Wiener process.</p>
<p>Second, we start to see the difference between the path-dependent quantity
of quadratic variation and variance.  The former depends on the path taken
by <span>\(H(t)\)</span> up to time <span>\(t\)</span>.  If it&#39;s large, then the quadratic
variation will be large, and similarly small with small values.  Variance
on the other hand is a fixed quantity up to time <span>\(t\)</span> that is averaged
over all paths and does not change (given the underlying distribution).</p>
<p>Finally, let&#39;s gain some intuition on the quadratic variation by utilizing
the informal differential notation from Equation 2.26-2.28.  We can re-write
our stochastic integral from Equation 3.2:</p>
<p>
\begin{equation*}
I(t) = \int_0^t H(s) dW(s) \tag{3.13}
\end{equation*}
</p>
<p>as:</p>
<p>
\begin{equation*}
dI(t) = H(t)dW(t) \tag{3.14}
\end{equation*}
</p>
<p>Equation 3.13 is the <em>integral form</em> while Equation 3.14 is the <em>differential form</em>,
and they have identical meaning.</p>
<p>The differential form is a bit easier to understand intuitively.  We can see
that it matches the approximation (Equation 3.4) that we discussed in the previous
subsection.  Using this differential notation and the informal notation we defined
above in Equation 2.26-2.28, we can &#34;calculate&#34; the quadratic variation as:</p>
<p>
\begin{equation*}
dI(t)dI(t) = H^2(t)dW(t)dW(t) = H^2(t)dt \tag{3.15}
\end{equation*}
</p>
<p>using the fact that the quadratic variation for the Wiener process accumulates at
one unit per time (<span>\(dW(t)dW(t) = dt\)</span>) from Theorem 1.  We&#39;ll utilize
this differential notation more in the following subsections as we move
into stochastic differential equations.</p>
</div>
</div>
<div id="ito-processes-and-integrals">
<h3><a href="#id16"><span>3.2</span> It√¥ Processes and Integrals</a></h3>
<p>In the previous subsections, we only allowed integrators that were Wiener processes
but we&#39;d like to extend that to a more general class of stochastic processes
called It√¥ processes <a href="#id5" id="id3">2</a>:</p>
<blockquote>
<p>Let <span>\(W(t)\)</span>, <span>\(t\geq 0\)</span>, be a Wiener process with an associated
filtration <span>\(\mathcal{F}(t)\)</span>.  An <strong>It√¥ process</strong> is a stochastic
process of the form:</p>
<p>
\begin{equation*}
X(t) = X(0) + \int_0^t \mu(s) ds + \int_0^t \sigma(s) dW(s) \tag{3.16}
\end{equation*}
</p>
<p>where <span>\(X(0)\)</span> is nonrandom and <span>\(\sigma(s)\)</span> and <span>\(\mu(s)\)</span>
are adapted stochastic processes.</p>
</blockquote>
<p>Equation 3.16 can also be written in its more natural (informal) differential form:</p>
<p>
\begin{equation*}
dX(t) = \mu(t)dt + \sigma(t)dW(t) \tag{3.17}
\end{equation*}
</p>
<p>A large class of stochastic processes are It√¥ processes.  In fact, any
stochastic process that is square integrable measurable with respect to a
filtration generated by a Wiener process can be represented by
Equation 3.16
(see the <a href="https://en.wikipedia.org/wiki/Martingale_representation_theorem">martingale representation theorem</a>).
Thus, many different types of stochastic processes that we practically care
about are It√¥ processes.</p>
<p>Using our differential notation, we can take Equation 3.17
and take the expectation and variance to get more insight:</p>
<p>
\begin{align*}
E[dX(t)] &amp;= E[\mu(t)dt + \sigma(t)dW(t)] \\
&amp;= E[\mu(t)dt] + E[\sigma(t)dW(t)] \\
&amp;= E[\mu(t)dt] + E[\sigma(t)]E[dW(t)] &amp;&amp; \sigma(t) \text{ and } dW(t) \text{ independent } \\
&amp;\approx \mu(t)dt &amp;&amp; \mu(t) \text{ approx. const for small } dt \tag{3.18} \\
\\
Var[dX(t)] &amp;= Var[\mu(t)dt + \sigma(t)dW(t)] \\
&amp;= E[(\mu(t)dt + \sigma(t)dW(t))^2] - (E[dX(t)])^2 \\
&amp;\approx E[\sigma^2(t)(dW(t))^2] - (\mu(t)dt)^2 &amp;&amp; \text{Equation 2.27/2.28} \\
&amp;= E[\sigma^2(t)dt] &amp;&amp; \text{Equation 2.26} \\
&amp;\approx \sigma^2(t)dt &amp;&amp; \text{ approx. const for small } dt \\
\tag{3.19}
\end{align*}
</p>
<p>In Equation 3.18, <span>\(\sigma(t)\)</span> and <span>\(dW(t)\)</span> are independent because <span>\(\sigma(t)\)</span>
is adapted to <span>\(W(t)\)</span>, thus the <span>\(dW(t)\)</span> increment is in the &#34;future&#34; of the
current value of <span>\(\sigma(t)\)</span>.  This reasoning only works because of the choice
of the <span>\(s_j=t_j\)</span> in Equation 3.2 for the It√¥ integral.</p>
<p>In fact, this result actually holds if we convert to our integral notation:</p>
<p>
\begin{align*}
E[X(t)] = \int_0^t \mu(s)ds \tag{3.20} \\
Var[X(t)] = \int_0^t \sigma^2(s)ds \tag{3.21} \\
\end{align*}
</p>
<p>So the notation of using <span>\(\mu\)</span> and <span>\(\sigma\)</span> makes more sense.
The regular time integral contributes to the mean of the It√¥ process,
while the stochastic integral contributes to the variance.  We&#39;ll see how we
can practically manipulate them in the next section.</p>
<p>Lastly as with our other processes, we would like to know its quadratic
variation.  Informally we can compute quadratic variation as:</p>
<p>
\begin{align*}
dX(t)dX(t) &amp;= \sigma^2(t)dW(t)dW(t) + 2\sigma(t)\mu(t)dW(t)dt + \mu^2(t)dtdt \\
&amp;= \sigma^2(t)dW(t)dW(t) &amp;&amp; \text{Eqn. 2.27/2.28} \\
&amp;= \sigma^2(t)dt &amp;&amp; \text{Quadratic variation of Wiener process} \\
\tag{3.22}
\end{align*}
</p>
<p>which is essentially the same computation we used in Equation 3.19 above (and
the same as the variance).  In fact, we get the same result as with the simpler
Wiener process where we accumulate quadratic variation with
<span>\(H^2(t)\)</span> per unit time.  The reason is that the cross variation
(Equation 2.27) and time quadratic variation (Equation 2.28) are zero and don&#39;t
contribute to the final expression.</p>
<p>Finally, let&#39;s see how to compute an integral of an It√¥ process <span>\(X(t)\)</span>
using our informal differential notation:</p>
<p>
\begin{align*}
\int_0^t F(s) dX(s) &amp;= \int_0^t F(s) (\sigma(s)dW(s) + \mu(s)ds) \\
&amp;= \int_0^t [F(s)\sigma(s)dW(s) + F(s)\mu(s)ds] \\
&amp;= \int_0^t F(s)\sigma(s)dW(s) + \int_0^t F(s)\mu(s)ds \\
\tag{3.23}
\end{align*}
</p>
<p>As we can see, it&#39;s just a sum of a simple Wiener process stochastic integral
and a regular time integral.</p>
<div>
<p>Example 7: A Simple It√¥ Integral</p>
<p>Starting with our It√¥ process:</p>
<p>
\begin{equation*}
X(t) = X(0) + \int_0^t A dt + \int_0^t B dW(s) \tag{3.24}
\end{equation*}
</p>
<p>where <span>\(A, B\)</span> are constant.  Now calculate a simple integral using it as the integrator:</p>
<p>
\begin{align*}
I(t) = \int_0^t C dX(s) &amp;= \int_0^t AC ds + \int_0^t BC dW(s) \\
     &amp;= AC t + \lim_{||\Pi|| \to 0} \sum_{j=0}^{n-1} BC[W(t_{i+1}) - W(t_i)] &amp;&amp; \text{defn. of stochastic integral} \\
     &amp;= AC t + \lim_{||\Pi|| \to 0} BC[W(t) - W(0)] &amp;&amp; \text{telescoping sum} \\
     &amp;= AC t + BC W(t) &amp;&amp; W(0) = 0 \\
\tag{3.25}
\end{align*}
</p>
<p>where <span>\(C\)</span> is constant.  From there, we can see that the mean and
variance of this process can be calculated in a straight forward manner
since <span>\(W(t)\)</span> is the only random component:</p>
<p>
\begin{align*}
E[I(t)] &amp;= E[AC t + BC W(t)] \\
 &amp;= AC t + BC E[W(t)] \\
 &amp;= AC t &amp;&amp; E[W(t)] = 0 \tag{3.26}\\
\\
Var[I(t)] &amp;= E[(I(t) - E[I(t)])^2] \\
 &amp;= E[(BC W(t))^2] \\
 &amp;= (BC)^2 t &amp;&amp; Var(W(t)) = E[W^2(t)] = t \tag{3.27}
\end{align*}
</p>
<p>Which is the same result as if we just directly computed Equation 3.20/3.21.
The final result is a simple stochastic process that is essentially
a Wiener process but that drifts up by <span>\(AC\)</span> over time.</p>
</div>
</div>
<div id="ito-s-lemma">
<h3><a href="#id17"><span>3.3</span> It√¥&#39;s Lemma</a></h3>
<p>Although many stochastic processes can be written as It√¥ processes, often times
the process under consideration is not in the form of Equation 3.16/3.17.
A common situation is where our target stochastic process <span>\(Y(t)\)</span> is a
deterministic function <span>\(f(\cdot)\)</span> of a simpler It√¥ process <span>\(X(t)\)</span>:</p>
<p>
\begin{equation*}
Y(t) = f(t, X(t)) \tag{3.28}
\end{equation*}
</p>
<p>In these situations, we&#39;ll want a method to simplify this so we can get it into
the simpler form of Equation 3.16/3.17 with a single <span>\(dt\)</span> and a single
<span>\(dW(s)\)</span> term.  This technique is known as It√¥&#39;s lemma.</p>
<div>
<p><strong>It√¥&#39;s Lemma</strong></p>
<p><em>Let</em> <span>\(X(t)\)</span> <em>be an It√¥ process as described in Equation 3.16/3.17, and let</em>
<span>\(f(t, x)\)</span> <em>be a function for which the partial derivatives</em>
<span>\(\frac{\partial f}{\partial t}, \frac{\partial f}{\partial x},
\frac{\partial^2 f}{\partial x^2}\)</span> <em>are defined and continuous.  Then for</em>
<span>\(T\geq 0\)</span>:</p>
<p>
\begin{align*}
&amp;f(T, X(T)) \\
 &amp;= f(0, X(0)) + \int_0^T \frac{\partial f(t, X(t))}{\partial t} dt
    + \int_0^T \frac{\partial f(t, X(t))}{\partial x} dX(t) \\
 &amp;\quad + \frac{1}{2} \int_0^T \frac{\partial^2 f(t, X(t))}{\partial x^2} dX(t)dX(t)\\
 &amp;= f(0, X(0)) + \int_0^T \frac{\partial f(t, X(t))}{\partial t} dt
  + \int_0^T \frac{\partial f(t, X(t))}{\partial x} \mu(t) dt \\
 &amp;\quad + \int_0^T \frac{\partial f(t, X(t))}{\partial x} \sigma(t) dW(t)
 + \frac{1}{2} \int_0^T \frac{\partial^2 f(t, X(t))}{\partial x^2} \sigma^2(t) dt\\
 \tag{3.29}
\end{align*}
</p>
<p><em>Or using differential notation, we can re-write the first equation more simply as:</em></p>
<p>
\begin{align*}
df(t, X(t)) &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x}dX(t)
+ \frac{1}{2} \frac{\partial^2 f}{\partial x^2}dX(t)dX(t) \\
&amp;= \big(\frac{\partial f}{\partial t} +
 \mu(t)\frac{\partial f}{\partial x} +
 \frac{\sigma^2(t)}{2}\frac{\partial^2 f}{\partial x^2}\big)dt +
 \frac{\partial f}{\partial x} \sigma(t) dW(t)   \\
\tag{3.30}
\end{align*}
</p>
<p><strong>Informal Proof</strong></p>
<p>Expand <span>\(f(t, x)\)</span> as a Taylor series:</p>
<p>
\begin{equation*}
df(t, x) = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x}dx
+ \frac{1}{2} \frac{\partial^2 f}{\partial x^2}dx^2 + \ldots \tag{3.31}
\end{equation*}
</p>
<p>Substitute <span>\(X(t)\)</span> for <span>\(x\)</span> and <span>\(\mu(t)dt + \sigma(t)dW(s)\)</span> for <span>\(dx\)</span>:</p>
<p>
\begin{align*}
&amp;df(t, X(s)) \\
&amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x}dX(t)
+ \frac{1}{2} (\frac{\partial^2 f}{\partial x^2})^2 dX(t)dX(t) + \ldots  \\
&amp;=\frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x}(\mu(t)dt + \sigma(t)dW(s)) \\
&amp;\quad+ \frac{1}{2} \frac{\partial^2 f}{\partial x^2}^2 (\mu(t)^2dt^2 + 2\mu(t)\sigma(t)dtdW(s) + \sigma^2(t)dW(s)dW(s)) + \ldots\\
&amp;=\frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x}(\mu(t)dt + \sigma(t)dW(s))
+ \frac{\sigma^2(t)}{2} \frac{\partial^2 f}{\partial x^2}^2 dW(s)dW(s) &amp;&amp; \text{since } dt^2=0 \text{ and } dtdW(t) = 0 \\
&amp;= \big(\frac{\partial f}{\partial t} +
 \mu(t)\frac{\partial f}{\partial x} +
 \frac{\sigma^2(t)}{2}\frac{\partial^2 f}{\partial x^2}\big)dt +
 \frac{\partial f}{\partial x} \sigma(t) dW(t) &amp;&amp;  \text{since } dW(s)dW(s) = dt \\
\tag{3.32}
\end{align*}
</p>
</div>
<p>As you can see, we can re-write the above stochastic process from Equation 3.28
in terms of a single <span>\(dt\)</span> and single <span>\(dW(s)\)</span> term (using
differential notation).  This can be thought of as a form of the
<a href="https://en.wikipedia.org/wiki/Total_derivative#Example:_Differentiation_with_direct_dependencies">chain rule for total derivatives</a>,
except now that we have a non-zero quadratic variation, we need to include the
extra second order term involving <span>\(dW(s)dW(s)\)</span>.</p>
<p>It√¥&#39;s lemma is an incredibly important result because most applications of
stochastic calculus are &#34;little more than repeated use of this formula in a
variety of situations&#34; [1].  In fact, based on what I can tell, many
introductory courses to stochastic calculus skip over a lot of the theoretical
material and simply just jump directly into applications of It√¥&#39;s lemma because that&#39;s
mostly what you need.</p>
<div>
<p>Example 7: It√¥&#39;s Lemma</p>
<p>Given the It√¥ process <span>\(X(t)\)</span> as given by Equation 3.16, consider
the stochastic process <span>\(Y(t)\)</span>:</p>
<p>
\begin{equation*}
Y(t) = f(t, X(t)) = X^2(t) + t^2 \tag{3.33}
\end{equation*}
</p>
<p>Using It√¥&#39;s Lemma, we can re-write <span>\(Y(t)\)</span> as
(in the differential form since it&#39;s cleaner):</p>
<p>
\begin{align*}
dY(t) &amp;= df(t, X(S)) = \\
&amp;= \big(\frac{\partial f}{\partial t} +
 \mu(t)\frac{\partial f}{\partial x} +
 \frac{\sigma^2(t)}{2}\frac{\partial^2 f}{\partial x^2}\big)dt +
 \frac{\partial f}{\partial x} \sigma(t) dW(t)   \\
&amp;= \big(2t + \sigma^2(t) + 2\mu(t)X(t) \big)dt + 2\sigma(t) X(t) dW(t) \\
\tag{3.34}
\end{align*}
</p>
<p>Which specifies <span>\(Y(t)\)</span> in a simpler form of just a <span>\(dt\)</span> and
<span>\(dW\)</span> term.</p>
</div>
</div>
<div id="stochastic-differential-equations">
<h3><a href="#id18"><span>3.4</span> Stochastic Differential Equations</a></h3>
<p>One of the most common problems we want to use stochastic calculus for is
solving stochastic differential equations (SDE). Similar to their non-stochastic
counterpart, they appear in many different phenomenon (a couple of which we
will see in the next section) and are usually very natural to write,
but not necessarily to easy solve.</p>
<p>Starting with the definition:</p>
<blockquote>
<p>A <strong>stochastic differential equation</strong> is an equation of the form:</p>
<p>
\begin{align*}
dX(t) &amp;= \mu(t, X(t))dt + \sigma(t, X(t)) dW(t) &amp;&amp; \text{differential form}\tag{3.35} \\
X(T) &amp;= X(t) + \int_t^T \mu(u, X(u))du + \int_t^T \sigma(u, X(u)) dW(u) &amp;&amp; \text{integral form} \tag{3.36}
\end{align*}
</p>
<p>where <span>\(\mu(t, x)\)</span> and <span>\(\sigma(t, x)\)</span> are given functions called
the <em>drift</em> and <em>diffusion</em> respectively.  Additionally, we are given
an initial condition <span>\(X(t) = x\)</span> for <span>\(t\geq 0\)</span>.  The problem is
to then find the stochastic process <span>\(X(T)\)</span> for <span>\(T\geq t\)</span>.</p>
</blockquote>
<p>Notice that <span>\(X(t)\)</span> appears on both sides making it difficult to solve for
explicitly.  A nice property though is that under mild conditions on
<span>\(\mu(t, x)\)</span> and <span>\(\sigma(t, x)\)</span>, there exists a unique process
<span>\(X(T)\)</span> that satisfies the above.  As you might also guess,
one-dimensional, linear SDEs can be solved for explicitly.</p>
<p>SDEs can add similar complexities as their non-stochastic counterparts such as
non-linearities, systems of SDEs, and multidimensional SDEs (with multiple
associated Wiener processes) etc.  Generally, SDEs won&#39;t have explicit closed
form solutions so you&#39;ll have to use numerical methods to solve them.</p>
<p>The two popular methods are Monte Carlo simulation and numerically solving
a partial differential equation (PDE).  Roughly, Monte Carlo simulation for
differential equations involve simulating many different paths of the
underlying process and using these paths to compute the associated statistics
(e.g., mean, variance etc.).  Given enough paths (and associated time), you
generally can get as accurate as you like.</p>
<p>The other method is to numerically solve a PDE.  An SDE can be recast to as a
PDE problem (at least in finance applications, not sure about others), and from
the PDEs you can use the plethora of numerical methods to solve them.
How both of these methods work is beyond the scope of this post (and how far I
wanted to dig into this subject), but there is a lot of literature online about
it.</p>
</div>
</div><div id="applications-of-stochastic-calculus">
<h2><a href="#id19"><span>4</span> Applications of Stochastic Calculus</a></h2>
<p><em>(Note: In this section, we&#39;ll forgo the explicit parameterization of the
stochastic processes to simplify the notation.)</em></p>
<div id="black-scholes-merton-model-for-options-pricing">
<h3><a href="#id20"><span>4.1</span> Black-Scholes-Merton Model for Options Pricing</a></h3>
<p>The rigorous math to get to the Black-Scholes-Merton model for options pricing
is quite in depth so instead I&#39;ll just present a quick overview of some of
the main concepts and intuition (following [6] closely).  See [6] for a
lighter but more intuitive treatment, and [1] for all the gory details.</p>
<div id="the-process-for-a-stock-price">
<h4>
<span>4.1.1</span> The Process for a Stock Price</h4>
<p>Stock prices are probably one of the most natural places where one would think
about using stochastic processes.  We might be tempted to directly use an
It√¥ process with constant <span>\(\mu\)</span> and <span>\(\sigma\)</span>.  However, this
translates to a linear growth in the stock price, which isn&#39;t quite right.
Instead, investors are typically expecting the same <em>percent return</em> regardless
of the current price vs. fixed linear growth.  For example, if a stock&#39;s price
is expected to grow at 10%, it should grow at that rate regardless of whether
the price is 10 or 100.  The naturally leads to this differential equation for stock
price <span>\(S\)</span> and constant return <span>\(\mu\)</span> (a pretty big assumption):</p>
<p>
\begin{equation*}
dS = \mu S dt \tag{4.1}
\end{equation*}
</p>
<p>The change in growth of the stock price (<span>\(dS\)</span>) is equal to the percent
return of the current price (<span>\(\mu S dt\)</span>).  This yields the solution at
time <span>\(t\)</span> by dividing by <span>\(S\)</span> and integrating both sides:</p>
<p>
\begin{equation*}
S(t) = S_0 e^{\mu t} \tag{4.2}
\end{equation*}
</p>
<p>Of course, this simplistic model has no random component.  We would expect that
the return is uncertain over a time period.  A (perhaps) reasonable assumption
to make is that for small time periods, the variability in the return is the same
regardless of the stock price.  That is, we are similarly unsure (as a percent of
the stock) of the returns whether it&#39;s at 10 or 100.  Using a Wiener process,
we can add this assumption to Equation 4.1 as:</p>
<p>
\begin{equation*}
dS = \mu S dt + \sigma S dW \tag{4.3}
\end{equation*}
</p>
<p>This results in a stochastic differential equation called <strong>geometric Brownian motion</strong> (GBM).</p>
<p>Fortunately, GBM has a closed form solution that we can derive by using It√¥&#39;s lemma
on <span>\(f(s) = \log s\)</span>:</p>
<p>
\begin{align*}
d(\log S) &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial s}dS
+ \frac{1}{2} \frac{\partial^2 f}{\partial s^2}dSdS \\
&amp;= 0 + \frac{dS}{S} - \frac{1}{2}\frac{1}{S^2} dS dS \\
&amp;= \frac{\mu S dt + \sigma S dW}{S} - \frac{1}{2}\frac{1}{S^2}\big(\mu S dt + \sigma S dW\big)\big(\mu S dt + \sigma S dW\big)  &amp;&amp; \text{Eq. 4.3} \\
&amp;= \mu dt + \sigma dW - \frac{\sigma^2}{2}dt &amp;&amp; \text{Eq. 2.27/2.28} \\
&amp;= (\mu - \frac{\sigma^2}{2})dt + \sigma dW \\
\tag{4.4}
\end{align*}
</p>
<p>From that, we know the <span>\(\log S\)</span> process between increment <span>\([0, t]\)</span>
is normally distributed with mean <span>\((\mu - \frac{\sigma^2}{2})t\)</span> (due to non-zero mean)
and variance <span>\(\sigma^2t\)</span> telling us that:</p>
<p>
\begin{equation*}
\log S \sim \mathcal{N}(\log S(0) + (\mu - \frac{\sigma^2}{2})t, \sigma^2 t) \tag{4.5}
\end{equation*}
</p>
<p>Meaning <span>\(S\)</span> is <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normally</a>
distributed with the above statistics.</p>
</div>
<div id="black-scholes-merton-differential-equation">
<h4>
<span>4.1.2</span> Black-Scholes-Merton Differential Equation</h4>
<p>The BSM model is probably the most famous equation in quantitative finance, but
it actually is quite complex to derive requiring all the stochastic calculus
that we have covered so far.  At the heart of the model is the BSM differential
equation, which we will presently derive and discuss.</p>
<p>The first thing to understand is the &#34;no arbitrage&#34; condition.  In the case of
a financial derivative (e.g., call or put option) and the underlying stock, the
price of the derivative should never allow one to make a portfolio of the two
such that you are guaranteed to make money i.e., arbitrage.  In this
theoretical portfolio you can be &#34;long&#34;, or buying and <em>owning</em> the financial
security, or &#34;short&#34;, <em>owing</em> the financial security but not owning it
(implemented by borrowing the security, selling it, buying it at some later
date, and returning the borrowed security).  A theoretical &#34;short&#34; is
essentially the opposite of buying and owning the asset where you benefit if
the asset goes down.</p>
<p>To build this no arbitrage or &#34;riskless&#34; portfolio, we will want to go long/short the
underlying stock and go short/long the derivative in exact proportion to the
relative change in the asset prices of the two.  This proportion between the
two only exists for a short period of time under that exact condition, and will
need to be rebalanced as market conditions change.</p>
<p>The other key idea is that once you have a &#34;riskless&#34; portfolio set up, it
should return the &#34;risk free&#34; rate (within the short period of time the balance
is maintained).  The risk free rate is an asset that is virtually guaranteed to
receive that given rate (think: a savings account, or more commonly a treasury bond).
With these few conditions and some additional idealized assumptions (e.g.,
stock prices follow the model we developed, no transaction costs, no dividends,
perfect &#34;shorting&#34; etc.), we can formulate the BSM differential equation.</p>
<p>Translating the above into concrete equations, we begin by assuming that stock
prices of a security follow geometric Brownian motion from Equation 4.3:</p>
<p>
\begin{equation*}
dS = \mu S dt + \sigma S dW \tag{4.6}
\end{equation*}
</p>
<p>An option on that security is some function <span>\(f(S, t)\)</span> of the current
stock price <span>\(S\)</span> and the time <span>\(t\)</span>, using It√¥&#39;s Lemma we get:</p>
<p>
\begin{align*}
df = \big(\frac{\partial f}{\partial t} +
              \mu \frac{\partial f}{\partial S} S  +
              \frac{\sigma^2 }{2}\frac{\partial^2 f}{\partial S^2}S^2 \big)dt +
              \frac{\partial f}{\partial S} \sigma S dW \\
              \tag{4.7}
\end{align*}
</p>
<p>Equations 4.6/4.7 describe infinitesimal changes in (a) the underlying stock
(<span>\(dS\)</span>), and (b) the change in the underlying financial derivative
(<span>\(df\)</span>).  Notice the Wiener process associated with both is the
same because <span>\(f\)</span> is derived from <span>\(S\)</span>, which can be seen in the
derivation of It√¥&#39;s Lemma.</p>
<p>With these two equations, we now have SDEs for both the stock price <span>\(S\)</span>
and the price of an option <span>\(f(S, t)\)</span>.  Our goal is to select a portfolio
of the two (at a given time instant and price <span>\(S\)</span>) that doesn&#39;t change
regardless of the random fluctuations in price of the underlying stock.  This
can be accomplished by ensuring that the stochastic components (<span>\(dW\)</span>
terms in each SDE) cancel out.  Since the <span>\(dW\)</span> terms are the only source
of randomness, when they are cancelled we can derive an expression for the
portfolio that deterministically changes with time.</p>
<p>Cancelling the stochastic terms is done simply by equating the two <span>\(dW\)</span>
terms in Equations 4.6 and 4.7, which results in taking proportions of
<span>\(-1\)</span> of the financial derivative and <span>\(\frac{\partial f}{\partial S}\)</span>
shares of the underlying stock.  In other words, the portfolio is <em>short</em> one
derivative and long <span>\(\frac{\partial f}{\partial S}\)</span> shares.  Defining our
portfolio value as <span>\(\Pi\)</span>, we get:</p>
<p>
\begin{equation*}
\Pi = -f + \frac{\partial f}{\partial S} S \tag{4.8}
\end{equation*}
</p>
<p>Taking the differentials, applying It√¥&#39;s lemma, and plugging in Equation 4.6/4.7:</p>
<p>
\begin{align*}
d\Pi &amp;= -df + \frac{\partial f}{\partial S} dS \\
     &amp;= -\big(\frac{\partial f}{\partial t} +
               \mu \frac{\partial f}{\partial S}S  +
               \frac{\sigma^2 }{2}\frac{\partial^2 f}{\partial S^2}S^2\big)dt
         - \frac{\partial f}{\partial S} \sigma S dW
       + \frac{\partial f}{\partial S}(\mu S dt + \sigma S dW) \\
     &amp;= -\big(\frac{\partial f}{\partial t} +
               \mu \frac{\partial f}{\partial S} S +
               \frac{\sigma^2 }{2}\frac{\partial^2 f}{\partial S^2}S^2\big)dt
         +\mu \frac{\partial f}{\partial S}S dt &amp;&amp; dW(s) \text{ terms cancel} \\
     &amp;= \big(-\frac{\partial f}{\partial t} -
               \frac{\sigma^2 }{2}\frac{\partial^2 f}{\partial S^2}S^2\big)dt \\
     \tag{4.9}
\end{align*}
</p>
<p>By construction (with our assumptions), <span>\(\Pi\)</span> is a riskless portfolio
(at time instant <span>\(t\)</span>) that deterministically changes with <span>\(t\)</span>.
The assumption of a no arbitrage situation implies that this portfolio must
make the risk free rate.  If this portfolio earns more than the risk free rate
you can just borrow money at the risk free rate and earn the difference between
the two.  If it earns less than the risk free rate then you can just short the
portfolio (and pay the associated lower interest rate) and buy risk free
securities and make the difference.</p>
<p>From this, we expect <span>\(\Pi\)</span> to earn the risk free rate for the
infinitesimal time in which our portfolio is perfectly balanced.
Using Equation 4.9 we can construct an SDE:</p>
<p>
\begin{align*}
d\Pi &amp;= r\Pi dt \\
\big(-\frac{\partial f}{\partial t} -
               \frac{\sigma^2 }{2}\frac{\partial^2 f}{\partial S^2}S^2\big)
     &amp;= r(-f + \frac{\partial f}{\partial S} S) dt \\
\frac{\partial f}{\partial t} + rS \frac{\partial f}{\partial S} +
        \frac{\sigma^2}{2}S^2 \frac{\partial^2 f}{\partial S^2}
     &amp;= rf \\
 \tag{4.10}
\end{align*}
</p>
<p>Equation 4.10 defines the Black-Scholes-Merton differential equation.  Notice
that this is a <em>deterministic</em> differential equation in <span>\(f(S, t)\)</span> because
we have cancelled away the stochastic Wiener process and <span>\(S, t\)</span> are given
with respect to <span>\(f(S, t)\)</span>.  It also has many solutions corresponding to the
<a href="https://en.wikipedia.org/wiki/Boundary_value_problem">boundary conditions</a>
placed on <span>\(f(S, t)\)</span>.  For example,
<a href="https://www.investopedia.com/terms/e/europeanoption.asp">European call and put options</a>
have these associated boundary conditions for strike price <span>\(K\)</span> and
expiry time <span>\(T\)</span>:</p>
<p>
\begin{align*}
f(S, t) &amp;= \max(S-K, 0) \text{ when } t = T \tag{4.11} &amp;&amp; \text{European call} \\
f(S, t) &amp;= \max(K-S, 0) \text{ when } t = T \tag{4.12} &amp;&amp; \text{European put}
\end{align*}
</p>
<p>In other words, when the call option contract expires, it is worth precisely the
difference between the stock price and strike price or zero if negative
(similarly in reverse for put options).</p>
<p>Solving this differential equation with these boundary conditions results
in the most famous formulas that you&#39;ll find when searching for BSM (see
<a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model#Black%E2%80%93Scholes_formula">here</a> for more details).
I won&#39;t go into all the details since that&#39;s not the focus of this post, but
the fact that it has a closed form solution is a big plus.  There are many
more complex quantitative finance models that do not have closed form solutions,
and even ones that go beyond It√¥ processes (see <a href="https://en.wikipedia.org/wiki/Jump_process">Jump Processes</a>).  These models require approximate solutions as discussed in
section 3.4.</p>
</div>
</div>
<div id="langevin-equation">
<h3><a href="#id21"><span>4.2</span> Langevin Equation</a></h3>
<p>A <a href="https://en.wikipedia.org/wiki/Langevin_equation">Langevin equation</a> is a
well known stochastic differential equation that describes how a system evolves
when subjected to a combination of deterministic and fluctuating forces.  The
original equation was developed well before stochastic calculus was discovered
in the context of the apparent random movement of a particle through a fluid,
which describes the physical phenomenon of
<a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>.  Since the
Wiener process and Brownian motion are so related, they are sometimes used
interchangeably to describe the underlying stochastic process.</p>
<p>Many people contributed to the discovery of Brownian motion (including
Einstein) but the stochastic differential equation was derived several years
after by Langevin (hence the name) in 1908.  Interestingly, since Langevin
did not approach his stochastic differential equation with much rigour (by
mathematician standards), this gave rise to the field of stochastic analysis to
answer some of the issues with Langevin&#39;s approach.</p>
<p>In this section, I&#39;m going to give a brief overview of the Langevin equation
in the context of Brownian motion, glossing over many of the usual analyses one
would do in a physics class.  Additionally, I&#39;m going to approach it using It√¥
calculus, which is not the typical approach (not the one originally used).
Finally, I&#39;ll briefly mention its relationship to a financial application.</p>
<div id="brownian-motion-and-the-langevin-equation">
<h4>
<span>4.2.1</span> Brownian Motion and the Langevin Equation</h4>
<p>The original Langevin equation describes the random movement of a (usually much
larger) particle suspended in a fluid due to collisions with the molecules of
the fluid:</p>
<p>
\begin{equation*}
m\frac{d{\bf v_t}}{dt} = -\lambda {\bf v_t} + {\bf \eta}(t) \tag{4.13}
\end{equation*}
</p>
<p>where <span>\(m\)</span> is the mass, <span>\(\bf v_t\)</span> is the velocity,
<span>\(\frac{d{\bf v_t}}{dt}\)</span> is the acceleration (the time derivative of velocity),
and <span>\(\bf \eta\)</span> is a white noise term with zero mean and flat frequency spectrum
(the same one we discussed in Section 2.5).
The easiest way to interpret this equation is using
<a href="https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion#Second">Newton&#39;s second law</a> of motion:
the net force on an object is equal to its mass times acceleration (<span>\(F_{net} = ma\)</span>).
The right hand side is the net force, and the left hand side is the product of
mass and acceleration.</p>
<p>Breaking it down further, there are two types of forces acting on our particle
suspended in a fluid: (a) a <a href="https://en.wikipedia.org/wiki/Stokes%27_law">drag force</a>
of the fluid that is proportional to its velocity (think something analogous to air
resistance), and (b) a noise term representing the effect of random collisions
with the small fluid molecules.  This is a bit strange because we&#39;re combining the
microscopic (drag force acting on the particle) with a seemingly macroscopic
average from the noise.  This needs a bit of explanation.</p>
<p>The noise term is an approximation of sorts.  For any given time instant, there
(theoretically) are specific molecules colliding with our target particle so
why are we considering this noise term <span>\(\bf \eta\)</span>?  Besides simplifying
the math, the justification is that it is a good approximation for the
<em>average</em> force within a small time instant because of the scale of our
observations.  Our instruments do not have infinite precision and only measure
finitely small time intervals, this means the resulting observations are really
an average over these small finite time intervals and look a lot like the white
noise term in Equation 4.13.  So while not exact (like any model), it provides
a pretty good approximation for this phenomenon (and many others with some
variations on the basic equation).</p>
<p>Interestingly, the noise term was not precisely defined (i.e., mathematically
rigorous) when Langevin wrote his original equations.  However with the advent
of stochastic calculus, we can write an equivalent stochastic differential
equation, which is often referred to as the
<a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">Ornstein-Uhlenbeck process</a>
as:</p>
<p>
\begin{equation*}
d{\bf v_t} = -\theta v_t dt + \sigma dW \tag{4.14}
\end{equation*}
</p>
<p>where <span>\(\theta, \sigma\)</span> are constants, and assume <span>\(\eta(t) =
\frac{dW}{dt}\)</span>.  As an aside, technically, the Wiener process is nowhere
differentiable, so <span>\(\frac{dW}{dt}\)</span> does not have a precise meaning, which
is why we rarely write it in this form and instead use the differential form of
Equation 4.14.</p>
<p>Equation 4.14 is complicated by the fact that we have our target process
<span>\(v_t\)</span> mixed in with differentials and non-differentials
i.e., a stochastic differential equation.  Since this is a relatively
simple SDE, we can use similar techniques to solving their non-stochastic
counterparts along with It√¥&#39;s lemma to compute the differential.</p>
<p>Without going into all of the reasoning behind it, we&#39;ll start with the function
<span>\(f(t, v_t) = v_t e^{\theta t}\)</span>, write down its differential and its Taylor
expansion similar to our (informal) derivation of It√¥&#39;s lemma:</p>
<p>
\begin{align*}
df(v_t, t) &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial v_t} dv_t
             + \frac{1}{2} (\frac{\partial^2 f}{\partial v_t^2})^2 dv_t dv_t \\
&amp;= \theta v_t e^{\theta t} dt + e^{\theta t} dv_t + (0)dv_t dv_t \\
&amp;= \theta v_t e^{\theta t} dt + e^{\theta t} (-\theta v_t dt + \sigma dW) &amp;&amp; \text{Equation 4.14} \\
&amp;= \sigma e^{\theta t} dW \\
\int_0^t df &amp;= v_0 +  \int_0^t \sigma e^{\theta s} dW \\
v_t e^{\theta t} &amp;= v_0 + \int_0^t \sigma e^{\theta s} dW \\
v_t &amp;= v_0e^{-\theta t} + \sigma \int_0^t e^{-\theta (t-s)} dW \\ \tag{4.15}
\end{align*}
</p>
<p>Which shows the general solution to our stochastic differential equation.
We can characterize this stochastic process by evaluating its mean and variance.
First, we use the fact that an
<a href="https://quant.stackexchange.com/questions/53212/integration-of-a-deterministic-function-w-r-t-a-brownian-motion">It√¥ integral of a deterministic integrand is normally distributed</a>,
thus the second term in Equation 4.15 has zero mean, and so (assuming a non-random initial velocity <span>\(v_0\)</span>):</p>
<p>
\begin{align*}
E[v_t] &amp;= E[v_0e^{-\theta t}] + E[\sigma \int_0^t e^{-\theta (t-s)} dW] \\
 &amp;= v_0e^{-\theta t} \\
 \tag{4.16}
\end{align*}
</p>
<p>Showing the average velocity vanishes relatively quickly over time.  Next, we
can compute the variance using It√¥&#39;s isometry (Theorem 4 above):</p>
<p>
\begin{align*}
Var(v_t) &amp;= E[(v_t - E[v_t])(v_t - E[v_t])] \\
&amp;= E\big[(\sigma \int_0^t e^{-\theta (t-s)} dW)^2] \\
&amp;= \sigma^2 E\big[\int_0^t (e^{-\theta (t-s)})^2 ds] &amp;&amp; \text{It√¥&#39;s isometry} \\
&amp;= \frac{\sigma^2}{2\theta}(1 - e^{-2\theta t}) \\
\tag{4.17}
\end{align*}
</p>
<p>From the mean and variance, and the fact the integrator in <span>\(v_t\)</span> is normally distributed,
we can infer that the stochastic process is one with a scaled and shifted Wiener process:</p>
<p>
\begin{equation*}
v_t = v_0 e^{-\theta t} + \frac{\sigma}{\sqrt{2\theta}}W(1-e^{-2\theta t}) \tag{4.18}
\end{equation*}
</p>
<p>Similarly, we can compute the displacement <span>\(x_t\)</span> by integrating
<span>\(v_t\)</span> with respect to time:</p>
<p>
\begin{align*}
x_t &amp;= \int_0^t v_t dt \\
    &amp;= \int_0^t v_0 e^{-\theta s} ds + \int_0^t \frac{\sigma}{\sqrt{2\theta}}W(1-e^{-2\theta s}) ds \\
    &amp;= \frac{v_0}{\theta} (1 - e^{-\theta t}) + \int_0^t \frac{\sigma}{\sqrt{2\theta}}W(1-e^{-2\theta s}) ds \\
    \tag{4.19}
\end{align*}
</p>
<p>I won&#39;t expand the last time integral, but you can calculate it using the explanation in <a href="https://quant.stackexchange.com/questions/29504/integral-of-brownian-motion-w-r-t-time">this
StackExchange answer</a>,
which has a zero mean.  Thus, the average displacement asymptotes to <span>\(\frac{v_0}{\theta}\)</span> over time.
See the following
<a href="https://en.wikipedia.org/wiki/Langevin_equation#Trajectories_of_free_Brownian_particles">Wikipedia article</a>
for more details on the physics of it all.</p>
</div>
</div>
</div><div id="conclusion">
<h2><a href="#id22"><span>5</span> Conclusion</a></h2>
<p>Well I did it again!  I went down a rabbit hole, and before I knew it I was
in over my head on this topic.  In a lot of ways it&#39;s nice learning on your own time
because you can meander.  I will say I had no idea what I was getting myself into
when I started to write this post.  Little did I know I would have to learn
more about measure theoretic probability theory (something that was never a
priority for me), nor that stochastic calculus needed so technical depth in order
to intuitively understand the underlying math (vs. just symbol manipulation).
In any case, I&#39;m glad I dug into it but I&#39;ll be happy when I can get back on
track to more standard ML topics.  Until next time!</p>
</div><div id="references">
<h2><a href="#id23"><span>6</span> References</a></h2>
<ul>
<li><p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Stochastic_process#Stochastic_process">Stochastic Processes</a>, <a href="https://en.wikipedia.org/wiki/Adapted_process">Adapted Stochastic Process</a></p></li>
<li><p>[1] Steven E. Shreve, &#34;Stochastic Calculus for Finance II: Continuous Time Models&#34;, Springer, 2004.</p></li>
<li><p>[2] Michael Kozdron, &#34;<a href="https://uregina.ca/~kozdron/Teaching/Regina/862Winter06/Handouts/revised_lecture1.pdf">Introduction to Stochastic Processes Notes</a>&#34;, Stats 862, University of Regina, 2006.</p></li>
<li><p>[3] &#34;<a href="https://canvas.harvard.edu/courses/669/files/431355/download?verifier=3LjaEzjDCgXxHFzoeTjmUv6u3VfY60yVh9y6xKSP&amp;wrap=1">Introduction to Stochastic Differential Equations</a>&#34;, Harvard, 2007.</p></li>
<li><p>[4] Maria Sandsten, &#34;<a href="https://canvas.education.lu.se/courses/5687/pages/differentiation-and-introduction-to-ar-and-ma-processes?module_item_id=130256">Differentiation of stationary stochastic processes</a>&#34;, 2020.</p></li>
<li><p>[5] George Lowther, <a href="https://almostsuremath.com/2010/06/16/continuous-processes-with-independent-increments/">Continuous Processes with Independent Increments</a></p></li>
<li><p>[6] John C. Hull, &#34;Options, Futures, and Other Derivatives&#34;, Pearson, 2018.</p></li>
</ul>
</div><div id="appendix-a-event-space-and-probability-measure-for-a-bernoulli-process">
<h2><a href="#id24"><span>7</span> Appendix A: Event Space and Probability Measure for a Bernoulli Process</a></h2>
<p>As mentioned the sample space for the Bernoulli process is all infinite
sequences of heads and tails: <span>\(\Omega = \{ (a_n)_1^{\infty} : a_n \in {H, T} \}\)</span>.
The first thing to mention about this sample space is that it is
<a href="https://en.wikipedia.org/wiki/Uncountable_set">uncountable</a>,
which basically means it is &#34;larger&#34; than the natural numbers.
Reasoning in infinities is quite unnatural but the two frequent &#34;infinities&#34;
that usually pop up are sets that have the same
<a href="https://en.wikipedia.org/wiki/Cardinality">cardinality</a> (&#34;size&#34;) as
(a) the natural numbers, and (b) the real numbers.
Our sample space has the same cardinality as the latter.
Cantor&#39;s original <a href="https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument">diagonalization argument</a>
actually used a variation of this sample space (with <span>\(\{0, 1\}\)</span>&#39;s), and
the proof is relatively intuitive.
In any case, this complicates things because a lot of our intuition falls apart
when we work with infinites, and especially with infinities that have the cardinality of the
real numbers.</p>
<p><em>(This construction was taken from [1], which is a dense, but informative reference for all the topics in this post.)</em></p>
<p>Now we will construct the event space (<span>\(\sigma\)</span>-algebra) and probability
measure for the Bernoulli process.  We&#39;ll do it iteratively.  First, let&#39;s define
<span>\(P(\emptyset) = 0\)</span> and <span>\(P(\Omega) = 1\)</span>, and the corresponding (trivial)
event space:</p>
<p>
\begin{equation*}
\mathcal{F}_0 = \{\emptyset, \Omega\} \tag{A.1}
\end{equation*}
</p>
<p>Notice that <span>\(\mathcal{F}_0\)</span> is a <span>\(\sigma\)</span>-algebra.  Next, let&#39;s
define two sets:</p>
<p>
\begin{align*}
A_H &amp;= \text{the set of all sequences beginning with } H = \{\omega: \omega_1 = H\} \\
A_T &amp;= \text{the set of all sequences beginning with } T = \{\omega: \omega_1 = T\} \\
\tag{A.2}
\end{align*}
</p>
<p>And set the intuitive definition of the corresponding probability measure:
<span>\(P(A_H) = p\)</span> and <span>\(P(A_T) = 1-p\)</span>.  That is, the probability of
seeing an H on the first toss is <span>\(p\)</span>, otherwise <span>\(1-p\)</span>.
Since these two sets are compliments of each other (<span>\(A_H = A_T^c\)</span>),
this defines another <span>\(\sigma\)</span>-algebra:</p>
<p>
\begin{equation*}
\mathcal{F}_1 = \{\emptyset, \Omega, A_H, A_T\} \tag{A.3}
\end{equation*}
</p>
<p>We can repeat this process again but for the first two tosses, define sets:</p>
<p>
\begin{align*}
A_{HH} &amp;= \text{the set of all sequences beginning with } HH = \{\omega: \omega_1\omega_2 = HH\} \\
A_{HT} &amp;= \text{the set of all sequences beginning with } HT = \{\omega: \omega_1\omega_2 = HT\} \\
A_{TH} &amp;= \text{the set of all sequences beginning with } TH = \{\omega: \omega_1\omega_2 = TH\} \\
A_{TT} &amp;= \text{the set of all sequences beginning with } TT = \{\omega: \omega_1\omega_2 = TT\} \\
\tag{A.4}
\end{align*}
</p>
<p>Similarly, we can extend our probability measure with the definition we would expect:
<span>\(P(A_{HH}) = p^2, P(A_{HT}) = p(1-p), P(A_{TH}) = p(1-p), P(A_{TT}) = (1-p)^2\)</span>.
Now we have to do a bit more analysis, but if one works out every possible set we can
create either from compliments or unions of any of the above sets, we&#39;ll find
that we have 16 in total.  For each one of them, we can compute its probability
measure by using one of the above definitions or by the fact that <span>\(P(A) = 1-P(A)\)</span>
or <span>\(P\big(\bigcup_{n=1}^{N} A_N \big) = \sum_{n=1}^{N} P(A_N)\)</span> if the sets
are disjoint.  These 16 sets define our next <span>\(\sigma\)</span>-algebra:</p>
<p>
\begin{equation*}
\mathcal{F}_2 = \left. \begin{cases}
        \emptyset, \Omega, A_H, A_T, A_{HH}, A_{HT}, A_{TH}, A_{TT}, A_{HH}^c, A_{HT}^c, A_{TH}^c, A_{TT}^c \\
        A_{HH} \bigcup A_{TH}, A_{HH} \bigcup A_{TT}, A_{HT} \bigcup A_{TH}, A_{HT} \bigcup A_{TT}
    \end{cases} \right\} \tag{A.5}
\end{equation*}
</p>
<p>As you can imagine, we can continue this process and define the probability (and associated
<span>\(\sigma\)</span>-algebra) for every set in terms of finitely many tosses.  Let&#39;s call
this set <span>\(\mathcal{F}_\infty\)</span>, which contains all of the sets that can be described
by <em>any number</em> of finitely many coin tosses using the procedure above, and then adding in all the
other ones using the compliment or union operator.  This turns out to be precisely
the <span>\(\sigma\)</span>-algebra of the Bernoulli process.  And by the construction,
we also have defined the associated probability measure for each one of the events
in <span>\(\mathcal{F}_\infty\)</span>.</p>
<p>Now we could leave it there, but let&#39;s take a look at the non-intuitive things that go
on when we work with infinities.  This definition implicitly includes sequences
that weren&#39;t explicitly defined by us, for example, the sequence of all heads:
<span>\(H, H, H, H, \ldots\)</span>.  But we can see this sequence is included in
<span>\(A_H, A_{HH}, A_{HHH}, \ldots\)</span>.  Further, we have:</p>
<p>
\begin{equation*}
P(A_H) = p, P(A_{HH})=p^2, P(A_{HHH})=p^3, \ldots \tag{A.6}
\end{equation*}
</p>
<p>so this implies the probability of <span>\(P(\text{sequence of all heads}) = 0\)</span>.
This illustrates an important non-intuitive result: all (infinite) sequences in our sample
space have probability <span>\(0\)</span>.  Importantly, it doesn&#39;t mean they can never occur,
just that they occur &#34;infinitesimally&#34;.  Similarly, the complement (&#34;sequences
of at least one tails&#34;) happens with probability <span>\(1\)</span>.
Mathematicians have a name for this probability equals to <span>\(1\)</span> event: <em>almost
surely</em>.  So any infinite sequence of coin flips <em>almost surely</em> has at least one
tail.  For finite event spaces, there is not difference between surely (always
happens) and almost surely.</p>
<p>This definition also includes sets of sequences that cannot be easily defined such
as:</p>
<p>
\begin{equation*}
\lim_{n\to \infty} \frac{H_n(\omega_1\ldots\omega_n)}{n} = \frac{1}{2} \tag{A.7}
\end{equation*}
</p>
<p>where <span>\(H_n\)</span> denotes the number of heads in the <span>\(n\)</span> tosses.  This
can be implicitly constructed by taking (countably infinite) unions and intersections
of sets that we have defined in our <span>\(A_\ldots\)</span> event space.  See Example
1.1.4 from [1] for more details.</p>
<p>Finally, although it may seem that we will have defined every subset of our
sample space, there does exist sequences that are not in
<span>\(\mathcal{F}_\infty\)</span>.  But it&#39;s extremely hard to produce such a set
(and don&#39;t ask me how :p).</p>

</div></div>
  </body>
</html>
