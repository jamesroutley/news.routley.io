<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arstechnica.com/ai/2026/01/google-removes-some-ai-health-summaries-after-investigation-finds-dangerous-flaws/">Original</a>
    <h1>Google removes AI health summaries after investigation finds dangerous flaws</h1>
    
    <div id="readability-page-1" class="page"><div>

        
        <div>
                      
                      
          <p>On Sunday, Google <a href="https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation">removed</a> some of its <a href="https://arstechnica.com/information-technology/2024/05/googles-ai-overview-can-give-false-misleading-and-dangerous-answers/">AI Overviews</a> health summaries after a Guardian investigation found people were being put at risk by false and misleading information. The removals came after the newspaper found that Google’s generative AI feature delivered inaccurate health information at the top of search results, potentially leading seriously ill patients to mistakenly conclude they are in good health.</p>
<p>Google disabled specific queries, such as “what is the normal range for liver blood tests,” after experts contacted by The Guardian flagged the results as dangerous. The report also highlighted a critical error regarding pancreatic cancer: The AI suggested patients avoid high-fat foods, a recommendation that contradicts standard medical guidance to maintain weight and could jeopardize patient health. Despite these findings, Google only deactivated the summaries for the liver test queries, leaving other potentially harmful answers accessible.</p>
<p>The investigation revealed that searching for liver test norms generated raw data tables (listing specific enzymes like ALT, AST, and alkaline phosphatase) that lacked essential context. The AI feature also failed to adjust these figures for patient demographics such as age, sex, and ethnicity. Experts warned that because the AI model’s definition of “normal” often differed from actual medical standards, patients with serious liver conditions might mistakenly believe they are healthy and skip necessary follow-up care.</p>
<p>Vanessa Hebditch, director of communications and policy at the British Liver Trust, told The Guardian that a liver function test is a collection of different blood tests and that understanding the results “is complex and involves a lot more than comparing a set of numbers.” She added that the AI Overviews fail to warn that someone can get normal results for these tests when they have serious liver disease and need further medical care. “This false reassurance could be very harmful,” she said.</p>
<p>Google declined to comment on the specific removals to The Guardian. A company spokesperson <a href="https://www.theverge.com/news/860356/google-pulls-alarming-dangerous-medical-ai-overviews">told</a> The Verge that Google invests in the quality of AI Overviews, particularly for health topics, and that “the vast majority provide accurate information.” The spokesperson added that the company’s internal team of clinicians reviewed what was shared and “found that in many instances, the information was not inaccurate and was also supported by high-quality websites.”</p>

          
                      
                  </div>

              </div></div>
  </body>
</html>
