<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://trieve.ai/building-blazingly-fast-typo-correction-in-rust/">Original</a>
    <h1>300μs typo detection for 1.3M words</h1>
    
    <div id="readability-page-1" class="page"><div> <p>We launched our <a href="https://hn.trieve.ai">Hacker News search and RAG engine</a> with a half-baked typo correction system. Our first draft took 30+ms for correctly spelled queries which was slow enough that we defaulted it to off. Our latest version is 100 times faster, 300μs for correctly spelled queries and ~5ms/word for misspellings. We explain how it was accomplished in this post!</p>
<p><img src="https://cdn.trieve.ai/blog/building-30%CE%BCs-typo-tolerance-for-1.3M-words%20using%20Rust/typo-tolerance-demo.gif" alt="video demo of spellcheck"/></p>
<h2 id="sample-queries-you-can-try">Sample Queries You Can Try<a href="#sample-queries-you-can-try"><span>#</span></a></h2>
<p>Click the links to try the typo correction system out yourself on <a href="https://hn.trieve.ai">hn.trieve.ai</a>.</p>
<ul>
<li><a href="https://hn.trieve.ai/?score_threshold=5&amp;page_size=30&amp;prefetch_amount=30&amp;rerank_type=none&amp;highlight_delimiters=+%2C-%2C_%2C.%2C%2C&amp;highlight_threshold=0.85&amp;highlight_max_length=50&amp;highlight_max_num=50&amp;highlight_window=0&amp;recency_bias=0&amp;highlight_results=true&amp;use_quote_negated_terms=true&amp;q=OpnAi&amp;storyType=story&amp;matchAnyAuthorNames=&amp;matchNoneAuthorNames=&amp;popularityFilters=%7B%7D&amp;sortby=relevance&amp;dateRange=all&amp;searchType=fulltext&amp;page=1&amp;getAISummary=false">OpnAI</a></li>
<li><a href="https://hn.trieve.ai/?score_threshold=5&amp;page_size=30&amp;prefetch_amount=30&amp;rerank_type=none&amp;highlight_delimiters=+%2C-%2C_%2C.%2C%2C&amp;highlight_threshold=0.85&amp;highlight_max_length=50&amp;highlight_max_num=50&amp;highlight_window=0&amp;recency_bias=0&amp;highlight_results=true&amp;use_quote_negated_terms=true&amp;q=Cnva+devloper+platfirm&amp;storyType=story&amp;matchAnyAuthorNames=&amp;matchNoneAuthorNames=&amp;popularityFilters=%7B%7D&amp;sortby=relevance&amp;dateRange=all&amp;searchType=fulltext&amp;page=1&amp;getAISummary=false">Cnva devloper platfirm</a></li>
<li><a href="https://hn.trieve.ai/?score_threshold=5&amp;page_size=30&amp;prefetch_amount=30&amp;rerank_type=none&amp;highlight_delimiters=+%2C-%2C_%2C.%2C%2C&amp;highlight_threshold=0.85&amp;highlight_max_length=50&amp;highlight_max_num=50&amp;highlight_window=0&amp;recency_bias=0&amp;highlight_results=true&amp;use_quote_negated_terms=true&amp;q=prviacy+focsed+email&amp;storyType=story&amp;matchAnyAuthorNames=&amp;matchNoneAuthorNames=&amp;popularityFilters=%7B%7D&amp;sortby=relevance&amp;dateRange=all&amp;searchType=fulltext&amp;page=1&amp;getAISummary=false">prviacy focsed email</a></li>
</ul>
<h2 id="creating-a-dictionary-of-words-and-frequencies">Creating a dictionary of Words and Frequencies<a href="#creating-a-dictionary-of-words-and-frequencies"><span>#</span></a></h2>
<p>For small datasets, this is an easy task. You can scroll ~1000 HN post size text blobs in 10 seconds with one worker and basic word splitting. However, as you scale to the size of our <a href="https://hn.trieve.ai">Hacker News Demo (38M+ posts)</a>, work needs to be distributed.</p>
<p>Eventually, we decided on 2 distinct workers for dictionary building:</p>
<ol>
<li><a href="https://github.com/devflowinc/trieve/blob/main/server/src/bin/word-id-cronjob.rs">Cronjob</a> to scroll all of the documents present in each of our users’ search indices and add chunk ids from our database into a Redis queue 500 at a time.</li>
<li><a href="https://github.com/devflowinc/trieve/blob/main/server/src/bin/word-worker.rs">Word worker</a> that pops off the queue and procesesses 500 chunks at a time. Text for each chunk is pulled, split into words, and each word is then loaded into Clickhouse.</li>
</ol>
<p>We chose <a href="https://clickhouse.com/">ClickHouse</a> to store the dictionary as we ran into deadlock and performance issues with Postgres writes as we scaled the number of workers. ClickHouse’s async inserts are fantastic for this task and allowed us to ingest the entire 38M+ document dataset in &lt; 1hr.</p>
<h2 id="using-a-bktree-data-structure-to-identify-and-correct-typos">Using a BKTree data structure to identify and correct typos<a href="#using-a-bktree-data-structure-to-identify-and-correct-typos"><span>#</span></a></h2>
<p>We take the <a href="https://nullwords.wordpress.com/2013/03/13/the-bk-tree-a-data-structure-for-spell-checking/">standard approach to typo correction</a> and build per-dataset Burkhard-Keller Trees (BKTrees) for efficient comparision of words in the search query and the dataset’s dictionary in O(log N) time complexity. Explaining this data structure in depth is outside the scope of this blog, but you can read our <a href="(https://github.com/devflowinc/trieve/blob/6e114abdca5683440e2834eccacf3f850dff810f/server/src/operators/typo_operator.rs#L35-112)">Rust implementation here</a> or its <a href="https://en.wikipedia.org/wiki/BK-tree">wiki</a> for more information.</p>
<p>We utilized a third<a href="https://github.com/devflowinc/trieve/blob/main/server/src/bin/bktree-worker.rs"> bktree-worker</a> to build the BKTrees. It takes datasets with completed dictonaries stored in Clickhouse then uses their words and frequencies to construct a tree.</p>
<p>Once the BKTree is constructed, the worker then stores it in Redis such that it can be efficiently loaded into the API server’s memory when needed at first query time for a given dataset.</p>
<p>This was challenging for larger datasets where the tree was hundreds of megabytes large and timed out redis on write and read. We developed a <a href="https://github.com/devflowinc/trieve/blob/6e114abdca5683440e2834eccacf3f850dff810f/server/src/operators/typo_operator.rs#L40-112">serialization method</a> which flattens and gzips to reduce the size in redis as well as the latency when pulling and pushing from it.</p>
<h2 id="writing-the-business-logic-to-perform-typo-corrections">Writing the Business Logic to Perform Typo Corrections<a href="#writing-the-business-logic-to-perform-typo-corrections"><span>#</span></a></h2>
<p>On the API server side, in our <a href="https://github.com/devflowinc/trieve/blob/main/server/src/operators/typo_operator.rs">typo_operator</a>, we optimized to reduce time required for corrections down to ~300μs for correctly spelled queries and ~10ms/word for mispelled queries.</p>
<p><img src="https://trieve.b-cdn.net/blog/building-30%CE%BCs-typo-tolerance-for-1.3M-words%20using%20Rust/typo-operator-graph.webp" alt="typo-operator-graph"/></p>
<h3 id="pulling-from-redis">Pulling from Redis<a href="#pulling-from-redis"><span>#</span></a></h3>
<p>Pulling a massive data structure, like the BKTree for HN, from Redis takes 300+μs. This is <strong>nonviable</strong> to do on each search, so we developed a cache layer server-side to store BKTrees after they had been pulled once using <code>lazy_static!</code></p>
<pre tabindex="0" data-language="rust"><code><span><span>lazy_static!</span><span> {</span></span>
<span><span>    static</span><span> ref</span><span> BKTREE_CACHE</span><span>:</span><span> BKTreeCache</span><span> =</span><span> BKTreeCache</span><span>::</span><span>new</span><span>();</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p>On the first search with typo-tolerance enabled, we initiate a ~200-400ms cold start to pull the BKTree for the dataset being queried from Redis into server memory. Searches following this operation then use the BKTree to check for typos which only takes 100-300μs.</p>
<h3 id="identifying-english-words">Identifying English Words<a href="#identifying-english-words"><span>#</span></a></h3>
<h4 id="1-preliminary-english-word-identification">1. Preliminary English Word Identification<a href="#1-preliminary-english-word-identification"><span>#</span></a></h4>
<p>Since our BKTrees are constructed solely from dataset-specific dictionaries, they may not have all valid English words. To prevent inaccurate corrections of legitimate words absent from our trees, we use a preliminary English word identification step:</p>
<ul>
<li>We maintain an in-memory hashset of approximately 400,000 English words, stored using <code>lazy_static!</code>.</li>
</ul>
<pre tabindex="0" data-language="rust"><code><span><span>static</span><span> ref</span><span> ENGLISH_WORDS</span><span>:</span><span> HashSet</span><span>&lt;</span><span>String</span><span>&gt; </span><span>=</span><span> {</span></span>
<span><span>        include_str!</span><span>(</span><span>&#34;../words.txt&#34;</span><span>)</span></span>
<span><span>            .</span><span>lines</span><span>()</span></span>
<span><span>            .</span><span>map</span><span>(</span><span>|</span><span>s</span><span>|</span><span> s</span><span>.</span><span>to_lowercase</span><span>())</span></span>
<span><span>            .</span><span>collect</span><span>()</span></span>
<span><span>    };</span></span>
<span></span></code></pre>
<h4 id="2-affix-analysis">2. Affix Analysis<a href="#2-affix-analysis"><span>#</span></a></h4>
<p>We then check for if the word is just an english word with a prefix or suffix:</p>
<ul>
<li>We construct separate Tries for common prefixes and suffixes.</li>
</ul>
<pre tabindex="0" data-language="rust"><code><span><span>static</span><span> ref</span><span> PREFIX_TRIE</span><span>:</span><span> Trie</span><span> =</span><span> {</span></span>
<span><span>        let</span><span> prefixes </span><span>=</span><span> vec!</span><span>[</span></span>
<span><span>            &#34;anti&#34;</span><span>, </span><span>&#34;auto&#34;</span><span>, </span><span>&#34;de&#34;</span><span>, </span><span>&#34;dis&#34;</span><span>, </span><span>&#34;down&#34;</span><span>, </span><span>&#34;extra&#34;</span><span>, </span><span>&#34;hyper&#34;</span><span>, </span><span>&#34;il&#34;</span><span>, </span><span>&#34;im&#34;</span><span>, </span><span>&#34;in&#34;</span><span>, </span><span>&#34;ir&#34;</span><span>, </span><span>&#34;inter&#34;</span><span>,</span></span>
<span><span>            &#34;mega&#34;</span><span>, </span><span>&#34;mid&#34;</span><span>, </span><span>&#34;mis&#34;</span><span>, </span><span>&#34;non&#34;</span><span>, </span><span>&#34;over&#34;</span><span>, </span><span>&#34;out&#34;</span><span>, </span><span>&#34;post&#34;</span><span>, </span><span>&#34;pre&#34;</span><span>, </span><span>&#34;pro&#34;</span><span>, </span><span>&#34;re&#34;</span><span>, </span><span>&#34;semi&#34;</span><span>, </span><span>&#34;sub&#34;</span><span>,</span></span>
<span><span>            &#34;super&#34;</span><span>, </span><span>&#34;tele&#34;</span><span>, </span><span>&#34;trans&#34;</span><span>, </span><span>&#34;ultra&#34;</span><span>, </span><span>&#34;un&#34;</span><span>, </span><span>&#34;under&#34;</span><span>, </span><span>&#34;up&#34;</span><span>,</span></span>
<span><span>        ];</span></span>
<span><span>        Trie</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>prefixes)</span></span>
<span><span>    };</span></span>
<span><span>    static</span><span> ref</span><span> SUFFIX_TRIE</span><span>:</span><span> Trie</span><span> =</span><span> {</span></span>
<span><span>        let</span><span> suffixes </span><span>=</span><span> vec!</span><span>[</span></span>
<span><span>            &#34;able&#34;</span><span>, </span><span>&#34;al&#34;</span><span>, </span><span>&#34;ance&#34;</span><span>, </span><span>&#34;ation&#34;</span><span>, </span><span>&#34;ative&#34;</span><span>, </span><span>&#34;ed&#34;</span><span>, </span><span>&#34;en&#34;</span><span>, </span><span>&#34;ence&#34;</span><span>, </span><span>&#34;ent&#34;</span><span>, </span><span>&#34;er&#34;</span><span>, </span><span>&#34;es&#34;</span><span>, </span><span>&#34;est&#34;</span><span>,</span></span>
<span><span>            &#34;ful&#34;</span><span>, </span><span>&#34;ian&#34;</span><span>, </span><span>&#34;ible&#34;</span><span>, </span><span>&#34;ic&#34;</span><span>, </span><span>&#34;ing&#34;</span><span>, </span><span>&#34;ion&#34;</span><span>, </span><span>&#34;ious&#34;</span><span>, </span><span>&#34;ise&#34;</span><span>, </span><span>&#34;ish&#34;</span><span>, </span><span>&#34;ism&#34;</span><span>, </span><span>&#34;ist&#34;</span><span>, </span><span>&#34;ity&#34;</span><span>,</span></span>
<span><span>            &#34;ive&#34;</span><span>, </span><span>&#34;ize&#34;</span><span>, </span><span>&#34;less&#34;</span><span>, </span><span>&#34;ly&#34;</span><span>, </span><span>&#34;ment&#34;</span><span>, </span><span>&#34;ness&#34;</span><span>, </span><span>&#34;or&#34;</span><span>, </span><span>&#34;ous&#34;</span><span>, </span><span>&#34;s&#34;</span><span>, </span><span>&#34;sion&#34;</span><span>, </span><span>&#34;tion&#34;</span><span>, </span><span>&#34;ty&#34;</span><span>,</span></span>
<span><span>            &#34;y&#34;</span><span>,</span></span>
<span><span>        ];</span></span>
<span><span>        Trie</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>suffixes)</span></span>
<span><span>    };</span></span>
<span></span></code></pre>
<ol>
<li>For each word in the query, we search these Tries to identify the longest matching prefix and suffix.</li>
<li>We then strip these affixes from the word, leaving us with the root.</li>
<li>After stripping, we perform a final dictionary check:</li>
<li>The stripped root is searched against the english word corpus.</li>
</ol>
<pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> is_likely_english_word</span><span>(word</span><span>:</span><span> &amp;</span><span>str</span><span>) </span><span>-&gt;</span><span> bool</span><span> {</span></span>
<span><span>    if</span><span> ENGLISH_WORDS</span><span>.</span><span>contains</span><span>(</span><span>&amp;</span><span>word</span><span>.</span><span>to_lowercase</span><span>()) {</span></span>
<span><span>        return</span><span> true</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Check for prefix</span></span>
<span><span>    if</span><span> let</span><span> Some</span><span>(prefix_len) </span><span>=</span><span> PREFIX_TRIE</span><span>.</span><span>longest_prefix</span><span>(word) {</span></span>
<span><span>        if</span><span> ENGLISH_WORDS</span><span>.</span><span>contains</span><span>(</span><span>&amp;</span><span>word[prefix_len</span><span>..</span><span>]</span><span>.</span><span>to_lowercase</span><span>()) {</span></span>
<span><span>            return</span><span> true</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Check for suffix</span></span>
<span><span>    if</span><span> let</span><span> Some</span><span>(suffix_len) </span><span>=</span><span> SUFFIX_TRIE</span><span>.</span><span>longest_suffix</span><span>(word) {</span></span>
<span><span>        if</span><span> ENGLISH_WORDS</span><span>.</span><span>contains</span><span>(</span><span>&amp;</span><span>word[</span><span>..</span><span>word</span><span>.</span><span>len</span><span>() </span><span>-</span><span> suffix_len]</span><span>.</span><span>to_lowercase</span><span>()) {</span></span>
<span><span>            return</span><span> true</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Check for compound words</span></span>
<span><span>    if</span><span> word</span><span>.</span><span>contains</span><span>(</span><span>&#39;-&#39;</span><span>) {</span></span>
<span><span>        let</span><span> parts</span><span>:</span><span> Vec</span><span>&lt;</span><span>&amp;</span><span>str</span><span>&gt; </span><span>=</span><span> word</span><span>.</span><span>split</span><span>(</span><span>&#39;-&#39;</span><span>)</span><span>.</span><span>collect</span><span>();</span></span>
<span><span>        if</span><span> parts</span></span>
<span><span>            .</span><span>iter</span><span>()</span></span>
<span><span>            .</span><span>all</span><span>(</span><span>|</span><span>part</span><span>|</span><span> ENGLISH_WORDS</span><span>.</span><span>contains</span><span>(</span><span>&amp;</span><span>part</span><span>.</span><span>to_lowercase</span><span>()))</span></span>
<span><span>        {</span></span>
<span><span>            return</span><span> true</span><span>;</span></span>
<span><span>        }</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    false</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<h4 id="4-bktree-search-for-non-dictionary-words">4. BKTree Search for Non-Dictionary Words<a href="#4-bktree-search-for-non-dictionary-words"><span>#</span></a></h4>
<p>For words that don’t pass our English word checks, we initiate a BKTree search:</p>
<ol>
<li>Query the BKTree to find the closest matching words.</li>
<li>Generate a set of candidate corrections for each non-dictionary word.</li>
</ol>
<pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> best_correction </span><span>=</span><span> None</span><span>;</span></span>
<span><span>            let</span><span> mut</span><span> best_score </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span>            for</span><span> ((correction, freq), distance) </span><span>in</span><span> tree</span><span>.</span><span>find</span><span>(word</span><span>.</span><span>to_string</span><span>(), max_distance) {</span></span>
<span><span>                if</span><span> distance </span><span>==</span><span> 0</span><span> {</span></span>
<span><span>                    best_correction </span><span>=</span><span> None</span><span>;</span></span>
<span><span>                    break</span><span>;</span></span>
<span><span>                }</span></span>
<span><span>                if</span><span> !</span><span>is_best_correction</span><span>(word, correction) {</span></span>
<span><span>                    continue</span><span>;</span></span>
<span><span>                }</span></span>
<span></span>
<span><span>                let</span><span> score </span><span>=</span><span> (max_distance </span><span>-</span><span> distance) </span><span>*</span><span> 1000</span><span> +</span><span> *</span><span>freq </span><span>as</span><span> isize</span><span>;</span></span>
<span></span>
<span><span>                if</span><span> score </span><span>&gt;</span><span> best_score </span><span>||</span><span> best_correction</span><span>.</span><span>is_none</span><span>() {</span></span>
<span><span>                    best_correction </span><span>=</span><span> Some</span><span>(correction);</span></span>
<span><span>                    best_score </span><span>=</span><span> score;</span></span>
<span><span>                }</span></span>
<span><span>            }</span></span>
<span></span>
<span><span>            if</span><span> let</span><span> Some</span><span>(correction) </span><span>=</span><span> best_correction {</span></span>
<span><span>                corrections</span><span>.</span><span>insert</span><span>(word, correction</span><span>.</span><span>to_string</span><span>());</span></span>
<span><span>            }</span></span>
<span></span></code></pre>
<h4 id="5-correction-selection">5. Correction Selection<a href="#5-correction-selection"><span>#</span></a></h4>
<p>From the set of correction candidates, we use a scoring algorithm to select the best correction:</p>
<p>Our algorithm prioritizes prefix matches and factors in the frequency of each candidate word within the dataset.</p>
<pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> is_best_correction</span><span>(word</span><span>:</span><span> &amp;</span><span>str</span><span>, correction</span><span>:</span><span> &amp;</span><span>str</span><span>) </span><span>-&gt;</span><span> bool</span><span> {</span></span>
<span><span>    // Length-based filter</span></span>
<span><span>    let</span><span> len_diff </span><span>=</span><span> (word</span><span>.</span><span>len</span><span>() </span><span>as</span><span> i32</span><span> -</span><span> correction</span><span>.</span><span>len</span><span>() </span><span>as</span><span> i32</span><span>)</span><span>.</span><span>abs</span><span>();</span></span>
<span><span>    if</span><span> len_diff </span><span>&gt;</span><span> 2</span><span> {</span></span>
<span><span>        return</span><span> false</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Prefix matching (adjust the length as needed)</span></span>
<span><span>    let</span><span> prefix_len </span><span>=</span><span> std</span><span>::</span><span>cmp</span><span>::</span><span>min</span><span>(</span><span>1</span><span>, </span><span>std</span><span>::</span><span>cmp</span><span>::</span><span>min</span><span>(word</span><span>.</span><span>len</span><span>(), correction</span><span>.</span><span>len</span><span>()));</span></span>
<span><span>    if</span><span> word[</span><span>..</span><span>prefix_len] </span><span>!=</span><span> correction[</span><span>..</span><span>prefix_len] {</span></span>
<span><span>        return</span><span> false</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Character set comparison</span></span>
<span><span>    let</span><span> word_chars</span><span>:</span><span> HashSet</span><span>&lt;</span><span>char</span><span>&gt; </span><span>=</span><span> word</span><span>.</span><span>chars</span><span>()</span><span>.</span><span>collect</span><span>();</span></span>
<span><span>    let</span><span> correction_chars</span><span>:</span><span> HashSet</span><span>&lt;</span><span>char</span><span>&gt; </span><span>=</span><span> correction</span><span>.</span><span>chars</span><span>()</span><span>.</span><span>collect</span><span>();</span></span>
<span><span>    let</span><span> common_chars </span><span>=</span><span> word_chars</span><span>.</span><span>intersection</span><span>(</span><span>&amp;</span><span>correction_chars)</span><span>.</span><span>count</span><span>();</span></span>
<span><span>    let</span><span> similarity_ratio </span><span>=</span></span>
<span><span>        common_chars </span><span>as</span><span> f32</span><span> /</span><span> word_chars</span><span>.</span><span>len</span><span>()</span><span>.</span><span>max</span><span>(correction_chars</span><span>.</span><span>len</span><span>()) </span><span>as</span><span> f32</span><span>;</span></span>
<span></span>
<span><span>    similarity_ratio </span><span>&gt;=</span><span> 0.8</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<h2 id="unexpected-benefits">Unexpected Benefits<a href="#unexpected-benefits"><span>#</span></a></h2>
<p><a href="https://news.ycombinator.com/item?id=41396655">Levitating commented on HN</a> that a query for <code>FreeBSD</code> sorted by points returned irrelevant results. Our tokenizer splits on camel case so <code>FreeBSD</code> got turned into <code>Free BSD FreeBSD</code> and stories only containing the word “Free” had more points than anything containing “FreeBSD” and thus ranked higher. Being able to effectively check for full words on a dataset-level allowed us to automatically require non-English words such that a query for <code>FreeBSD</code> turned into <code>&#34;FreeBSD&#34;</code>.</p>
<p><img src="https://cdn.trieve.ai/blog/building-30%CE%BCs-typo-tolerance-for-1.3M-words%20using%20Rust/auto-required-word-demo.gif" alt="auto required word demo"/></p>
<h2 id="future-ideas">Future Ideas<a href="#future-ideas"><span>#</span></a></h2>
<p>We plan to leverage this same system to implement query splitting and concatenation as those features share the same requirement of quickly looking up words in a dictionary.</p>
<p>Trieve will always pursue the best possible relevance out of the box! Try it on our <a href="https://hn.trieve.ai">HN search engine</a>, <a href="https://dashboard.trieve.ai">sign up for a free cloud account</a>, or <a href="https://docs.trieve.ai/self-hosting/aws">see our self-hosting guides</a>.</p> </div></div>
  </body>
</html>
