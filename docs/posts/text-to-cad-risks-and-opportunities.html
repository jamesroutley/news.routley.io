<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thegradient.pub/text-to-cad/">Original</a>
    <h1>Text-to-CAD: Risks and Opportunities</h1>
    
    <div id="readability-page-1" class="page"><div>
          <p>The dust has hardly formed, much less settled, when it comes to AI-powered <a href="https://medium.com/@mechanism_nice/ai-and-the-genocide-of-the-creative-class-213a7a7315f7">text-to-image generation</a>. Yet the result is already clear: a tidal wave of crummy images. There is some quality in the mix, to be sure, but not nearly enough to justify the damage done to the signal-to-noise ratio – for every artist who benefits from a Midjourney-generated album cover, there are fifty people duped by a Midjourney-generated deepfake. And in a world where declining signal-to-noise ratios are the root cause of so many ills (think scientific research, journalism, government accountability), this is not good.</p><p>It’s now necessary to view all images with suspicion. (This has admittedly long been the case, but the increasing incidence of deepfakes warrants a proportional increase in vigilance, which, apart from being simply unpleasant, is cognitively taxing.) Constant suspicion - or failing that, frequent misdirection - seems a high price to pay for a digital bauble that no one asked for, and offers as yet little in the way of upside. Hopefully - or perhaps more aptly, prayerfully - the cost-to-benefit ratio will soon enter saner territory.</p><p>But in the meantime, we should be aware of a new phenomenon in the generative AI world: AI-powered text-to-CAD generation. The premise is similar to that of text-to-image programs, just instead of an image, the programs return a 3D CAD model.</p><figure><img src="https://lh6.googleusercontent.com/cIRxCIswC2-eJJ_DMqvqDKtqMOQ2uZACnpdtsMIP__I6D-VhaDd5JcCO5QEBU1Ikfy4N9PuiqWpqBcZOSbFk4kEFJP9JO8dpfIWuQrymShT-ssfBoYePCJf7vzxZZMfyXJiabJo8aNHgSzDNt9telw" alt="" loading="lazy" width="624" height="265"/><figcaption>Asking AI to give me &#34;Mona Lisa, but wearing Balenciaga&#34; yields a half-decent image, which AI then converts into 3D</figcaption></figure><p>A few definitions are in order here. First, Computer Aided Design (CAD) refers to software tools wherein users create digital models of physical objects - things like cups, cars, and bridges. (Models in the context of CAD have nothing to do with deep learning models; a Toyota Camry ≠ a recurrent neural network.) Also, CAD is important; try to think of the last time you were not within sight of a CAD-designed object.</p><p>Definitions behind us, let’s turn now to the big players who want in to the text-to-CAD world: Autodesk (<a href="https://github.com/AutodeskAILab/Clip-Forge">CLIP-Forge</a>), Google (<a href="https://dreamfusion3d.github.io/">DreamFusion</a>), OpenAI (<a href="https://github.com/openai/point-e">Point-E</a>), and NVIDIA (<a href="https://research.nvidia.com/labs/dir/magic3d/">Magic3D</a>). Example of each are shown below:</p><figure><img src="https://lh5.googleusercontent.com/3rjGi-xoN10Dxqwdo2WTG9TNY2aVnRMxzV9cnOoVGzjRMfeKo-Zgrt99cm5VfJLKLSrb8_MqresJ0k2zEbXIl7n38yy0lbDM-fSR8zr0sDVuPanPzievM2Nx9yZP1RLst0F4hN-yrfxW8yhis1W1Yw" alt="" loading="lazy" width="522" height="903"/></figure><p>Major players have not deterred startups from popping up at the rate of nearly one a month, as of early 2023, among whom <a href="https://www.csm.ai/">CSM</a> and <a href="https://www.sloyd.ai/">Sloyd</a> are perhaps the most promising.</p><p>In addition, there are a number of fantastic tools that might be termed 2.5-D, as their output is somewhere between 2- and 3-D. The idea with these is that the user uploads an image, and AI then makes a good guess as to how the image would look in 3D.</p><figure><img src="https://lh3.googleusercontent.com/LPOu_N5n5H2HNZVfCAfOEm2TO5b6E1vBRLxjiQiboLhj8xBfC2wz-FN9BI57zpngBNrjmqXRXE6A82uolpX4YBvNfMjn1M0k7nomd_RKM90erTaojZkO8FjZh6jgAuvGU1qei0L0sPFBSmBYTOZlvg" alt="" loading="lazy" width="624" height="560"/><figcaption>This <a href="https://en.wikipedia.org/wiki/Pythagorean_cup">greedy cup</a> used AI to turn <a href="https://www.dropbox.com/scl/fi/gxwom1agwt5copw2hwm7z/freize-in-color.png?rlkey=giwunnb486uapy7tc79ob1aco&amp;dl=0">an image</a> of Sam Bankman-Fried (depicted as a wolf in sheep’s clothing-cum-pied piper) into a bas-relief (Credit: Reggie Raye / <a href="http://www.tomo.love">TOMO</a>)</figcaption></figure><p>Open source animation and modeling platform Blender is, unsurprisingly, a leader in this space. And the CAD modeling software Rhino now has plugins such as<a href="https://www.food4rhino.com/en/app/surfacerelief"> SurfaceRelief</a> and <a href="https://www.food4rhino.com/en/app/ambrosinus-toolkit?lang=en">Ambrosinus Toolkit</a> which do a great job of generating 3D depth maps from plain images.</p><p>All of this, it should first be said, is exciting and cool and novel. As a CAD designer myself, I eagerly anticipate the potential benefits. And engineers, 3D printing hobbyists, and video game designers, among many others, likewise stand to benefit.</p><p>However, there are many downsides to text-to-CAD, many of them severe. A brief listing might include:</p><ul><li>Opening the door to mass creation of weapons, and racist or otherwise objectionable material</li><li>Unleashing a tidal wave of crummy models, which then go on to pollute model repos</li><li>Violating the rights of content creators, whose work is copyrighted</li><li>Digital colonialism: amplifying very-online western design at the expense of non-western design traditions</li></ul><p>In any event, text-to-CAD is coming whether we want it or not. But, thankfully, there are a number of steps technologists can take to improve their program’s output and reduce their negative impacts. We’ve identified three key areas where such programs can level up: dataset curation, a pattern language for usability, and filtering.</p><p>To our knowledge, these areas remain largely unexplored in the text-to-CAD context. The idea of a pattern language for usability will receive special attention, given its potential to dramatically improve output. Notably, this potential isn’t limited to CAD; it can improve outcomes in most generative AI domains, such as text and image.</p><h3 id="dataset-curation"><strong>Dataset Curation</strong></h3><p><em>Passive Curation</em></p><p>While not all approaches to text-to-CAD rely on a training set of 3D models (Google’s DreamFusion is one exception), curating a model dataset is still the most common approach. The key here, it scarcely bears mentioning, is to curate an awesome set of models for training.</p><p>And the key to doing that is twofold. First, technologists ought to avoid the obvious model sources: Thingiverse, Cults3D, MyMiniFactory. While high quality models are present there (<a href="https://cults3d.com/en/users/TOMO/creations">mine among them</a> ;) the vast majority are junk. (The Reddit thread ‘<a href="https://www.reddit.com/r/3Dprinting/comments/pboa8c/why_is_thingiverse_so_shit/">Why is Thingiverse so shit?</a>’ is one of many that speak to this problem.) Second, super high-quality model repos should be sought out. (<a href="https://www.myminifactory.com/scantheworld/full-collection">Scan the World</a> is perhaps the world&#39;s best.)</p><p>Next, model sources can be weighted according to quality. Master of Fine Arts (MFA) students would likely jump at the chance to do this kind of labeling - and, due to the inequities of the labor market, for peanuts.</p><p><em>Active Curation</em></p><p>Curation can and should take a more active role. Many museums, private collections, and design firms would gladly have their industrial design collections 3D scanned. Plus, in addition to producing a rich corpus, scanning would create a robust record of our all-too-fragile culture.</p><figure><img src="https://lh4.googleusercontent.com/a_ptbr-ksuh4FbnIyD1OailyL7YmVxFrIHmRUhPZ8gkuwO9Lf6NBpDJr5fSJlV6iD_MNkwSi5PKWvto4co_aOpwtl77_mtB9xVFUkcDx7ZdArLnqTkefq5ow80DSySk7DvD4eQI4YLVwFU76IsBAnw" alt="" loading="lazy" width="624" height="387"/><figcaption>The only way the French have been able to rebuild Notre Dame after its catastrophic fire was thanks to the 3D scanning of a <a href="https://www.theatlantic.com/technology/archive/2019/04/laser-scans-could-help-rebuild-notre-dame-cathedral/587230/">single American</a>. Credit: Andrew Tallon / Vassar College</figcaption></figure><p><em>Data Enrichment</em></p><p>In the process of creating a high quality corpus, technologists must think hard about what they want the data to do. At first glance, the main use case might seem to be ‘empowering managers at hardware companies to move a few sliders that output blueprints for a desired product, which can then be manufactured’. If the <a href="https://dave-sloan.medium.com/what-ever-happened-to-mass-customization-8f057e73fb5">failure-rich</a> <a href="https://www.inc.com/tracy-leigh-hazzard/made-to-order-for-masses-idyllic-or-impractical.html">history</a> of mass customization is any guide, however, this approach is likely to flounder.</p><p>A more effective use case, in our view, would be ‘empowering domain experts - people like industrial designers at product design firms - to prompt engineer until they get a suitable output, which they then fine-tune to completion’.</p><p>Such a use case would require a number of things which are perhaps non-obvious at first glance. For example, domain experts need to be able to upload images of reference products, as in Midjourney, which they then tag according to their target attributes - style, material, kinetics, etc. It might be tempting to adopt a faceting approach here, where experts select dropdowns for style type, material type, etc. But experience suggests that enriching datasets so as to create attribute buckets is a bad idea. This manual approach was favored by the music streaming service Pandora, which was ultimately steamrolled by Spotify, which relies on neural nets.</p><figure><img src="https://lh6.googleusercontent.com/rNKyDoeZk7sSAXz7kwyiz9bu4JsrRgIpbiozUdIpZ-y4PU4bI8EcHqOZFO21gWHvCTiE8eW2E4F9JibMUrBA14Kbv1PC5Vj3e75yCkWUp_zR450GpPI0dB3g2w5XLUz6BhFtIX6glOdoyGV5AaWNqw" alt="" loading="lazy" width="624" height="401"/><figcaption>Faceting is tempting, but likely to hit a dead end, as illustrated by this failed startup by the author.</figcaption></figure><p><em>Takeaways</em></p><p>Rigorous dataset curation is an area where (with a few <a href="https://deep-geometry.github.io/abc-dataset/">exceptions</a>) little has been done and, hence, much is to be gained. This should be a prime target for companies and entrepreneurs seeking a competitive advantage in the text-to-CAD wars. A large, enriched dataset is hard to make and hard to imitate - the best kind of mote.</p><p>On a less corporatist note, thoughtful dataset curation is the ideal way to drive the creation of products that are <em>beautiful</em>. Reflecting the priorities of their creators, generative AI tools to date have been, to put it lightly, taste-agnostic. But we ought to take a stand for the importance of beauty. We ought to care about whether what we bring into this world will enchant users and stand the test of time. We ought to push back against the mediocre products being heaped onto mediocre bandwagons.</p><p>If beauty as an end in itself is insufficient to some, perhaps they will be persuaded by two data points: sustainability and profit.</p><p>The most iconic products of the past hundred years - the Eames chairs, Leica cameras, Vespa scooters - are treasured by their users. Vibrant fandoms restore them, sell them, and continue to use them. Perhaps the intricacy of their design required 20% more emissions than rival products of their day. No matter. That their lifespans are measured in quarter centuries and not in years means that they led to less consumption and less emissions.</p><figure><img src="https://lh5.googleusercontent.com/jqD58ugU346vsq8_snKCzgsDdzYyER8s9AeLHlKPxq_cUAv_ZtSVmORybTqBa8qViAZI3ihQmVBezWyMcx1WLZhD-g9fT5bASCISOhbTKw3OgskP09NYXkdaI2R18TbfjTBzzH56Bmxe4K3vkM83Cw" alt="" loading="lazy" width="624" height="400"/><figcaption>Beautiful products get more love. A 1963 Vespa GS 160 selling for $13,000 in 2023</figcaption></figure><p>As for profit, it’s no secret that beautiful products command a price premium. iPhone specs have never been comparable to Samsungs’. Yet Apple can charge 25% more than Samsung. The adorable Fiat 500 subcompact gets worse gas mileage than an F-150. No matter. Fiat wagered, correctly, that yuppies would gladly pay an extra $5K for cuteness.</p><h3 id="a-pattern-language-for-usability"><strong>A Pattern Language for Usability</strong></h3><p><em>Overview</em></p><p>Pattern languages were pioneered in the 1970s by polymath Christopher Alexander. They are defined as a mutually-reinforcing set of patterns, each of which describes a design problem and its solution. While Alexander’s first pattern language was targeted at architecture, they have been profitably applied to many domains (most famously in programming) and stand to be at least as useful in the domain of generative design.</p><p>In the context of text-to-CAD, a pattern language would consist of a set of patterns; for example, one for moving parts, one for hinges (a subset of moving parts, hence one layer of abstraction down), and one for friction hinges (another layer of abstraction down). The format for a friction hinge pattern might look like this:<br/></p><!--kg-card-begin: html--><table><colgroup><col width="180"/><col width="444"/></colgroup><tbody><tr><td><p dir="ltr"><span>Pattern Name</span></p></td><td><p dir="ltr"><span>Friction Hinge</span></p></td></tr><tr><td><p dir="ltr"><span>Pattern Description</span></p></td><td><p dir="ltr"><span>The Friction Hinge pattern addresses the need for adjustable friction in hinges so as to provide tuneable resistance, but without compromising smooth movement. By allowing customization of the level of friction, this pattern enhances usability. This pattern may be used in the design of consumer electronics, automotive interior components, medical equipment, and folding furniture, among others.</span></p></td></tr><tr><td><p dir="ltr"><span>Consider These Patterns First</span></p></td><td><p dir="ltr"><span>Ergonomics for Hand-held Devices, Hinges, Load and Force Analysis, Safety Locking, Lubrication and Wear Resistance</span></p></td></tr><tr><td><p dir="ltr"><span>Problem Statement</span></p></td><td><p dir="ltr"><span>Folding devices require hinges with adjustable friction, lack of which may result in either excessive resistance or insufficient support for the object attached to the hinge.</span></p></td></tr><tr><td><p dir="ltr"><span>Solution</span></p></td><td><p dir="ltr"><span>Friction Adjustability: Integrate a mechanism into a generic barrel hinge that enables friction calibration per use case requirements. This adjustment can be achieved through various means, such as a tensioning screw or a friction pad with different settings. </span><span></span></p><p dir="ltr"><span>Smooth Transition: Ensure that the friction adjustment mechanism allows for smooth and incremental changes, without sudden jumps or unintended collisions with other design elements.</span></p></td></tr><tr><td><p dir="ltr"><span>Used In</span></p></td><td><p dir="ltr"><span>Laptops, adjustable stands and mounts, folding tables, cabinet doors, exercise incline benches, medical examination tables</span></p></td></tr><tr><td><p dir="ltr"><span>Consider These Patterns Next</span></p></td><td><p dir="ltr"><span>Adaptive Friction Control, Sealed Friction Mechanism, Safety Release, Indexed Folding Mechanism</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>In common with natural language, pattern languages comprise a vocabulary (the set of design solutions), syntax (where a solution fits into the language), and grammar (rules for which patterns may solve a problem). Note that the above pattern ‘Friction Hinge’ is one node in a hierarchical network, which can be visualized by a directed network graph. </p><p>Embodied in these patterns would be best practices with respect to design fundamentals - human factors, functionality, aesthetics, etc. The output of such patterns would thereby be more usable, more understandable (avoiding the black box problem), and easier to fine-tune.</p><p>Crucially, unless text-to-CAD programs account for design fundamentals, their output will amount to little less than junk. Better nothing at all than a text-to-CAD-generated laptop whose screen doesn’t stay upright.</p><p><strong>Perhaps the most important of all these fundamentals - and the most difficult to account for - is design for human factors.</strong> To get a useful product, the number of human factors considerations verges on the infinite. The AI must recognize and design around pinch points, finger entrapment, ill-placed sharp edges, ergonomic proportions, etc.</p><p><em>Implementation</em></p><p>Let’s look at a practical example. Suppose Jane is an industrial designer at Design Studio ABC, which has a commission to design a futuristic gaming laptop. The state of the art now would be for Jane to turn to a CAD program like <a href="https://www.autodesk.com/products/fusion-360/overview?term=1&amp;tab=subscription">Fusion 360</a>, enter Fusion’s generative design workspace, and spend the rest of the week (or month) working with her team to specify all relevant constraints: loads, conditions, objectives, material properties, etc.</p><p>But however powerful Fusion’s generative design workspace is (and we know from experience that it’s powerful) it can never get around one key fact: a user must have lots of domain expertise, CAD ability, and time.</p><p>A more pleasant user experience would be to simply prompt a text-to-CAD program until its output meets ones’ requirements. Such a pattern design-centric workflow might look like the following:</p><p>Jane prompts her text-to-CAD program: “Show me some examples of a futuristic gaming laptop. Use for inspiration the form factor of the <a href="https://www.tomo.love/laptop-stand">TOMO laptop stand</a> and the surface texture of a <a href="https://www.google.com/search?q=shiny+black+cobra&amp;tbm=isch&amp;sxsrf=AB5stBhpkhX4hP5jtJf8VPX1YDPq6mQefA%3A1688670685278&amp;source=hp&amp;biw=1429&amp;bih=738&amp;ei=3RGnZMiGD6SJptQPnoOcIA&amp;iflsig=AD69kcEAAAAAZKcf7X897KGgl6S3Qx9N-2g2q1RR8JOo&amp;ved=0ahUKEwiI6djC5Pr_AhWkhIkEHZ4BBwQQ4dUDCAc&amp;uact=5&amp;oq=shiny+black+cobra&amp;gs_lcp=CgNpbWcQAzoECCMQJzoFCAAQgAQ6CAgAEIAEELEDOgcIIxDqAhAnOgsIABCABBCxAxCDAToGCAAQCBAeUABY7xlg0RpoA3AAeAGAAYwBiAHfC5IBBDE4LjKYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABCg&amp;sclient=img#imgrc=9f4plDvMpTtDnM">king cobra</a>”.</p><figure><img src="https://lh5.googleusercontent.com/22XyQTsHAdz5wzj8fiHD5aNuqIM4BeIgfnFRqVp_1hFykPTrMGgG6sH_f164t2_cxE86MJd6UvIejQY_BV4IfNrXQT5rBsnbxUbYbDYlRbgUXv9cm3xa1jrKwYO86UvBEjF62aB5sbpkvYnRIZ5csw" alt="" loading="lazy" width="624" height="469"/><figcaption>Fully realized text-to-CAD will close the loop from image to manufacturable product.</figcaption></figure><p>The program outputs six concept images, each informed by patterns such as “Keyboard Layout”, “Hinged Mechanisms”, and “Port Layout for Consumer Electronics”</p><p>She replies “Give me some variations of image 2. Make the screen more restrained and the keyboard more textured.”</p><p>Jane: “I like the third one. What parameters do we have on that one?”</p><p>The system, drawing on the ‘Solution’ fields of the patterns it finds most relevant, lists 20 parameters - length, width, monitor height, key density, etc.</p><p>Jane notes that the hinge type is not specified, so types “add a hinge type parameter to that list and output the CAD model”.</p><p>She opens the model in Fusion 360 and is pleased to see that an appropriate friction hinge has been added. As the hinge has come parameterized, she increases the width parameter, knowing that Studio ABC’s client will want the screen to hold up to a lot of abuse.</p><p>Jane continues making adjustments until she’s fully satisfied with the form and function. This done, she can pass it off to her colleague Joe, a mechanical engineer, who will inspect it to see which custom components might be replaced by stock versions.</p><p>In the end, management at Studio ABC is happy because the laptop design process went from an average of six months to just one. They are doubly pleased because, thanks to parameterization, any revisions requested by their client can be quickly satisfied without a redesign.</p><h3 id="thorough-filtering"><strong>Thorough Filtering</strong></h3><p>As AI ethicist Irene Solaiman recently pointed out in a <a href="https://thegradientpub.substack.com/p/irene-solaiman-ai-policy-and-social#details">poignant interview</a>, generative AI is sorely in need of thorough guardrails. Even with the benefit of a pattern language approach, there’s nothing inherent in generative AI to prevent generation of undesirable output. This is where guardrails come in.</p><p>We need to be capable of detecting and denying prompts that request weapons, gore, child sexual abuse material (CSAM), and other objectionable content. Technologists wary of lawsuits might add to this list products under copyright. But if experience is any guide, objectionable prompts are likely to make up a significant portion of queries.</p><p>Alas, once text-to-CAD models get open-sourced or leaked, many of these queries will be satisfied without compunction. (And if the saga of <a href="https://www.washingtontimes.com/news/2018/jul/22/defense-distributed-wins-settlement-can-post-firea/">Defense Distributed</a> has taught us anything, it’s that the genie will never go back into the bottle; thanks to a <a href="https://www.washingtontimes.com/news/2018/jul/22/defense-distributed-wins-settlement-can-post-firea/">recent ruling</a> in Texas, it’s now legal for an American to download an AR-15, 3D print it, and then - should he feel threatened - shoot someone with it.)</p><p>In addition, we need widely-shared performance benchmarks, analogous to those that have cropped up around LLMs. After all, if you can’t measure it, you can’t improve it.</p><p>____</p><p>In conclusion, the emergence of AI-powered text-to-CAD generation presents both risks and opportunities, the ratio of which is still very much undecided. The proliferation of low-quality CAD models and toxic content are just a few things that require immediate attention.</p><p>There are several neglected areas where technologists might profitably train their attention. Dataset curation is crucial: we need to track down high-quality models from high-quality sources, and explore alternatives such as scanning of industrial design collections. A pattern language for usability could provide a powerful framework for incorporating design best practices. Further, a pattern language will provide a robust framework for generating CAD model parameters that can be fine-tuned until a model meets the requirements of its use case. Finally, thorough filtering techniques must be developed to prevent the generation of dangerous content.</p><p>We hope the ideas presented here will help technologists avoid the pitfalls that have plagued generative AI to date, and also enhance the ability of text-to-CAD to deliver delightful models that benefit the many people who will soon be turning to them.</p><hr/><p><a href="https://www.linkedin.com/in/reggieraye/">Reggie Raye</a> is a teaching artist with a background in industrial design and fabrication. He is the founder of design studio <a href="http://www.tomo.love">TOMO</a>.</p><p><a href="https://kalexandriabond.github.io/">K. Alexandria Bond, PhD</a> is a neuroscientist focusing on the rules driving learning dynamics. She studied cognitive computational neuroscience at Carnegie Mellon. She currently develops machine learning methods for precision diagnosis of psychiatric conditions at Yale. </p><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><blockquote>Reggie Raye and K. Alexandria Bond, &#34;Text-to-CAD: Risks and Opportunities&#34;, The Gradient, 2023.</blockquote><p>Bibtex citation:</p><!--kg-card-begin: markdown--><pre><code>@article{raye2023texttocad,
    author = {Raye, Reggie and Bond, K. Alexandria},
    title = {Text-to-CAD: Risks and Opportunities},
    journal = {The Gradient},
    year = {2023},
    howpublished = {\url{https://thegradient.pub/text-to-cad},
}
</code></pre>
<!--kg-card-end: markdown-->
        </div></div>
  </body>
</html>
