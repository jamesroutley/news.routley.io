<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ace-step/ACE-Step">Original</a>
    <h1>ACE-Step: A step towards music generation foundation model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">
    <a href="https://ace-step.github.io/" rel="nofollow">Project</a> |
    <a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B" rel="nofollow">Hugging Face</a> |
    <a href="https://modelscope.cn/models/ACE-Step/ACE-Step-v1-3.5B" rel="nofollow">ModelScope</a> |
    <a href="https://huggingface.co/spaces/ACE-Step/ACE-Step" rel="nofollow">Space Demo</a> |
     <a href="https://discord.gg/rjAZz2xBdG" rel="nofollow">Discord</a> 
</p>
<hr/>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/ace-step/ACE-Step/blob/main/fig/orgnization_logos.png"><img src="https://github.com/ace-step/ACE-Step/raw/main/fig/orgnization_logos.png" width="100%" alt="StepFun Logo"/></a>
</p>

<ul dir="auto">
<li><a href="#-features">Features</a></li>
<li><a href="#-installation">Installation</a></li>
<li><a href="#-usage">Usage</a></li>
<li><a href="#-train">Train</a></li>
</ul>

<ul dir="auto">
<li>ğŸš€ 2025.05.06: Open source demo code and model</li>
</ul>

<ul>
<li> Release training code ğŸ”¥</li>
<li> Release LoRA training code ğŸ”¥</li>
<li> Release RapMachine lora ğŸ¤</li>
<li> Release ControlNet training code ğŸ”¥</li>
<li> Release Singing2Accompaniment controlnet ğŸ®</li>
<li> Release evaluation performance and technical report  ğŸ“„</li>
</ul>

<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/ace-step/ACE-Step/blob/main/fig/ACE-Step_framework.png"><img src="https://github.com/ace-step/ACE-Step/raw/main/fig/ACE-Step_framework.png" width="100%" alt="ACE-Step Framework"/></a>
</p>

<p dir="auto">We introduce ACE-Step, a novel open-source foundation model for music generation that overcomes key limitations of existing approaches and achieves state-of-the-art performance through a holistic architectural design. Current methods face inherent trade-offs between generation speed, musical coherence, and controllability. For instance, LLM-based models (e.g., Yue, SongGen) excel at lyric alignment but suffer from slow inference and structural artifacts. Diffusion models (e.g., DiffRhythm), on the other hand, enable faster synthesis but often lack long-range structural coherence.</p>
<p dir="auto">ACE-Step bridges this gap by integrating diffusion-based generation with Sanaâ€™s Deep Compression AutoEncoder (DCAE) and a lightweight linear transformer. It further leverages MERT and m-hubert to align semantic representations (REPA) during training, enabling rapid convergence. As a result, our model synthesizes up to 4 minutes of music in just 20 seconds on an A100 GPUâ€”15Ã— faster than LLM-based baselinesâ€”while achieving superior musical coherence and lyric alignment across melody, harmony, and rhythm metrics. Moreover, ACE-Step preserves fine-grained acoustic details, enabling advanced control mechanisms such as voice cloning, lyric editing, remixing, and track generation (e.g., lyric2vocal, singing2accompaniment).</p>
<p dir="auto">Rather than building yet another end-to-end text-to-music pipeline, our vision is to establish a foundation model for music AI: a fast, general-purpose, efficient yet flexible architecture that makes it easy to train sub-tasks on top of it. This paves the way for developing powerful tools that seamlessly integrate into the creative workflows of music artists, producers, and content creators. In short, we aim to build the Stable Diffusion moment for music.</p>

<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/ace-step/ACE-Step/blob/main/fig/application_map.png"><img src="https://github.com/ace-step/ACE-Step/raw/main/fig/application_map.png" width="100%" alt="ACE-Step Framework"/></a>
</p>

<div dir="auto"><h4 tabindex="-1" dir="auto">ğŸŒˆ Diverse Styles &amp; Genres</h4><a id="user-content--diverse-styles--genres" aria-label="Permalink: ğŸŒˆ Diverse Styles &amp; Genres" href="#-diverse-styles--genres"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>ğŸ¸ Supports all mainstream music styles with various description formats including short tags, descriptive text, or use-case scenarios</li>
<li>ğŸ· Capable of generating music across different genres with appropriate instrumentation and style</li>
</ul>

<ul dir="auto">
<li>ğŸ—£ï¸ Supports 19 languages with top 10 well-performing languages including:
<ul dir="auto">
<li>ğŸ‡ºğŸ‡¸ English, ğŸ‡¨ğŸ‡³ Chinese, ğŸ‡·ğŸ‡º Russian, ğŸ‡ªğŸ‡¸ Spanish, ğŸ‡¯ğŸ‡µ Japanese, ğŸ‡©ğŸ‡ª German, ğŸ‡«ğŸ‡· French, ğŸ‡µğŸ‡¹ Portuguese, ğŸ‡®ğŸ‡¹ Italian, ğŸ‡°ğŸ‡· Korean</li>
</ul>
</li>
<li><g-emoji alias="warning">âš ï¸</g-emoji> Due to data imbalance, less common languages may underperform</li>
</ul>

<ul dir="auto">
<li>ğŸ¹ Supports various instrumental music generation across different genres and styles</li>
<li>ğŸº Capable of producing realistic instrumental tracks with appropriate timbre and expression for each instrument</li>
<li>ğŸ¼ Can generate complex arrangements with multiple instruments while maintaining musical coherence</li>
</ul>

<ul dir="auto">
<li>ğŸ™ï¸ Capable of rendering various vocal styles and techniques with good quality</li>
<li>ğŸ—£ï¸ Supports different vocal expressions including various singing techniques and styles</li>
</ul>

<div dir="auto"><h4 tabindex="-1" dir="auto">ğŸ”„ Variations Generation</h4><a id="user-content--variations-generation" aria-label="Permalink: ğŸ”„ Variations Generation" href="#-variations-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>âš™ï¸ Implemented using training-free, inference-time optimization techniques</li>
<li>ğŸŒŠ Flow-matching model generates initial noise, then uses trigFlow&#39;s noise formula to add additional Gaussian noise</li>
<li>ğŸšï¸ Adjustable mixing ratio between original initial noise and new Gaussian noise to control variation degree</li>
</ul>

<ul dir="auto">
<li>ğŸ–Œï¸ Implemented by adding noise to the target audio input and applying mask constraints during the ODE process</li>
<li>ğŸ” When input conditions change from the original generation, only specific aspects can be modified while preserving the rest</li>
<li>ğŸ”€ Can be combined with Variations Generation techniques to create localized variations in style, lyrics, or vocals</li>
</ul>

<ul dir="auto">
<li>ğŸ’¡ Innovatively applies flow-edit technology to enable localized lyric modifications while preserving melody, vocals, and accompaniment</li>
<li>ğŸ”„ Works with both generated content and uploaded audio, greatly enhancing creative possibilities</li>
<li>â„¹ï¸ Current limitation: can only modify small segments of lyrics at once to avoid distortion, but multiple edits can be applied sequentially</li>
</ul>


<ul dir="auto">
<li>ğŸ”Š Based on a LoRA fine-tuned on pure vocal data, allowing direct generation of vocal samples from lyrics</li>
<li>ğŸ› ï¸ Offers numerous practical applications such as vocal demos, guide tracks, songwriting assistance, and vocal arrangement experimentation</li>
<li>â±ï¸ Provides a quick way to test how lyrics might sound when sung, helping songwriters iterate faster</li>
</ul>

<ul dir="auto">
<li>ğŸ›ï¸ Similar to Lyric2Vocal, but fine-tuned on pure instrumental and sample data</li>
<li>ğŸµ Capable of generating conceptual music production samples from text descriptions</li>
<li>ğŸ§° Useful for quickly creating instrument loops, sound effects, and musical elements for production</li>
</ul>


<ul dir="auto">
<li>ğŸ”¥ Fine-tuned on pure rap data to create an AI system specialized in rap generation</li>
<li>ğŸ† Expected capabilities include AI rap battles and narrative expression through rap</li>
<li>ğŸ“š Rap has exceptional storytelling and expressive capabilities, offering extraordinary application potential</li>
</ul>

<ul dir="auto">
<li>ğŸšï¸ A controlnet-lora trained on multi-track data to generate individual instrument stems</li>
<li>ğŸ¯ Takes a reference track and specified instrument (or instrument reference audio) as input</li>
<li>ğŸ¹ Outputs an instrument stem that complements the reference track, such as creating a piano accompaniment for a flute melody or adding jazz drums to a lead guitar</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">ğŸ¤ Singing2Accompaniment</h4><a id="user-content--singing2accompaniment" aria-label="Permalink: ğŸ¤ Singing2Accompaniment" href="#-singing2accompaniment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>ğŸ”„ The reverse process of StemGen, generating a mixed master track from a single vocal track</li>
<li>ğŸµ Takes a vocal track and specified style as input to produce a complete vocal accompaniment</li>
<li>ğŸ¸ Creates full instrumental backing that complements the input vocals, making it easy to add professional-sounding accompaniment to any vocal recording</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">ğŸ–¥ï¸ Hardware Performance</h2><a id="user-content-ï¸-hardware-performance" aria-label="Permalink: ğŸ–¥ï¸ Hardware Performance" href="#ï¸-hardware-performance"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We have evaluated ACE-Step across different hardware setups, yielding the following throughput results:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Device</th>
<th>RTF (27 steps)</th>
<th>Time to render 1 min audio (27 steps)</th>
<th>RTF (60 steps)</th>
<th>Time to render 1 min audio (60 steps)</th>
</tr>
</thead>
<tbody>
<tr>
<td>NVIDIA RTX 4090</td>
<td>34.48 Ã—</td>
<td>1.74 s</td>
<td>15.63 Ã—</td>
<td>3.84 s</td>
</tr>
<tr>
<td>NVIDIA A100</td>
<td>27.27 Ã—</td>
<td>2.20 s</td>
<td>12.27 Ã—</td>
<td>4.89 s</td>
</tr>
<tr>
<td>NVIDIA RTX 3090</td>
<td>12.76 Ã—</td>
<td>4.70 s</td>
<td>6.48 Ã—</td>
<td>9.26 s</td>
</tr>
<tr>
<td>MacBook M2 Max</td>
<td>2.27 Ã—</td>
<td>26.43 s</td>
<td>1.03 Ã—</td>
<td>58.25 s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">We use RTF (Real-Time Factor) to measure the performance of ACE-Step. Higher values indicate faster generation speed. 27.27x means to generate 1 minute of music, it takes 2.2 seconds (60/27.27). The performance is measured on a single GPU with batch size 1 and 27 steps.</p>


<ul dir="auto">
<li>Make sure you have Python installed. You can download it from <a href="https://www.python.org/" rel="nofollow">python.org</a>.</li>
<li>You will also need either <code>Conda</code> (recommended) or <code>venv</code>.</li>
</ul>

<p dir="auto">It is highly recommended to use a virtual environment to manage project dependencies and avoid conflicts. Choose <strong>one</strong> of the following methods (Conda or venv):</p>

<ol dir="auto">
<li>
<p dir="auto"><strong>Create the environment</strong> named <code>ace_step</code> with Python 3.10:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n ace_step python=3.10 -y"><pre>conda create -n ace_step python=3.10 -y</pre></div>
</li>
<li>
<p dir="auto"><strong>Activate the environment:</strong></p>

</li>
</ol>

<ol dir="auto">
<li>
<p dir="auto"><strong>Ensure you are using the correct Python version.</strong></p>
</li>
<li>
<p dir="auto"><strong>Create the virtual environment</strong> (commonly named <code>venv</code>):</p>

</li>
<li>
<p dir="auto"><strong>Activate the environment:</strong></p>
<ul dir="auto">
<li><strong>On Windows (cmd.exe):</strong>
<div dir="auto" data-snippet-clipboard-copy-content="venv\Scripts\activate.bat"><pre>venv<span>\S</span>cripts<span>\a</span>ctivate.bat</pre></div>
</li>
<li><strong>On Windows (PowerShell):</strong>
<div dir="auto" data-snippet-clipboard-copy-content=".\venv\Scripts\Activate.ps1 "><pre>.\venv\Scripts\Activate.ps1 </pre></div>
<em>(If you encounter execution policy errors, you might need to run <code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process</code> first)</em></li>
<li><strong>On Linux / macOS (bash/zsh):</strong>

</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Install dependencies</strong> from the <code>requirements.txt</code> file:</p>
<p dir="auto">for macOS/Linux users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">for Windows users:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install PyTorch, TorchAudio, and TorchVision for Windows
# replace cu126 with your CUDA version
# replace torchvision and torchaudio with your version
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# then install other dependencies
pip install -r requirements.txt"><pre><span><span>#</span> Install PyTorch, TorchAudio, and TorchVision for Windows</span>
<span><span>#</span> replace cu126 with your CUDA version</span>
<span><span>#</span> replace torchvision and torchaudio with your version</span>
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

<span><span>#</span> then install other dependencies</span>
pip install -r requirements.txt</pre></div>
</li>
</ol>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ace-step/ACE-Step/blob/main/fig/demo_interface.png"><img src="https://github.com/ace-step/ACE-Step/raw/main/fig/demo_interface.png" alt="Demo Interface"/></a></p>



<div dir="auto" data-snippet-clipboard-copy-content="python app.py --checkpoint_path /path/to/checkpoint --port 7865 --device_id 0 --share true --bf16 true"><pre>python app.py --checkpoint_path /path/to/checkpoint --port 7865 --device_id 0 --share <span>true</span> --bf16 <span>true</span></pre></div>
<p dir="auto">If you are using MacOS, please use <code>--bf16 false</code> to avoid errors.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">ğŸ› ï¸ Command Line Arguments</h4><a id="user-content-ï¸-command-line-arguments" aria-label="Permalink: ğŸ› ï¸ Command Line Arguments" href="#ï¸-command-line-arguments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--checkpoint_path</code>: Path to the model checkpoint (default: downloads automatically)</li>
<li><code>--server_name</code>: IP address or hostname for the Gradio server to bind to (default: &#39;127.0.0.1&#39;). Use &#39;0.0.0.0&#39; to make it accessible from other devices on the network.</li>
<li><code>--port</code>: Port to run the Gradio server on (default: 7865)</li>
<li><code>--device_id</code>: GPU device ID to use (default: 0)</li>
<li><code>--share</code>: Enable Gradio sharing link (default: False)</li>
<li><code>--bf16</code>: Use bfloat16 precision for faster inference (default: True)</li>
<li><code>--torch_compile</code>: Use <code>torch.compile()</code> to optimize the model, speeding up inference (default: False). <strong>Not Supported on Windows</strong></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">ğŸ“± User Interface Guide</h2><a id="user-content--user-interface-guide" aria-label="Permalink: ğŸ“± User Interface Guide" href="#-user-interface-guide"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The ACE-Step interface provides several tabs for different music generation and editing tasks:</p>

<ol dir="auto">
<li>
<p dir="auto"><strong>ğŸ“‹ Input Fields</strong>:</p>
<ul dir="auto">
<li><strong>ğŸ·ï¸ Tags</strong>: Enter descriptive tags, genres, or scene descriptions separated by commas</li>
<li><strong>ğŸ“œ Lyrics</strong>: Enter lyrics with structure tags like [verse], [chorus], and [bridge]</li>
<li><strong>â±ï¸ Audio Duration</strong>: Set the desired duration of the generated audio (-1 for random)</li>
</ul>
</li>
<li>
<p dir="auto"><strong>âš™ï¸ Settings</strong>:</p>
<ul dir="auto">
<li><strong>ğŸ”§ Basic Settings</strong>: Adjust inference steps, guidance scale, and seeds</li>
<li><strong>ğŸ”¬ Advanced Settings</strong>: Fine-tune scheduler type, CFG type, ERG settings, and more</li>
</ul>
</li>
<li>
<p dir="auto"><strong>ğŸš€ Generation</strong>: Click &#34;Generate&#34; to create music based on your inputs</p>
</li>
</ol>

<ul dir="auto">
<li>ğŸ² Regenerate music with slight variations using different seeds</li>
<li>ğŸšï¸ Adjust variance to control how much the retake differs from the original</li>
</ul>

<ul dir="auto">
<li>ğŸ–Œï¸ Selectively regenerate specific sections of the music</li>
<li>â±ï¸ Specify start and end times for the section to repaint</li>
<li>ğŸ” Choose the source audio (text2music output, last repaint, or upload)</li>
</ul>

<ul dir="auto">
<li>ğŸ”„ Modify existing music by changing tags or lyrics</li>
<li>ğŸ›ï¸ Choose between &#34;only_lyrics&#34; mode (preserves melody) or &#34;remix&#34; mode (changes melody)</li>
<li>ğŸšï¸ Adjust edit parameters to control how much of the original is preserved</li>
</ul>

<ul dir="auto">
<li>â• Add music to the beginning or end of an existing piece</li>
<li>ğŸ“ Specify left and right extension lengths</li>
<li>ğŸ” Choose the source audio to extend</li>
</ul>

<p dir="auto">The <code>examples/input_params</code> directory contains sample input parameters that can be used as references for generating music.</p>


<ol dir="auto">
<li>
<p dir="auto">Prepare the environment as described in the installation section.</p>
</li>
<li>
<p dir="auto">If you plan to train a LoRA model, install the PEFT library:</p>

</li>
<li>
<p dir="auto">Prepare your dataset in Huggingface format (<a href="https://huggingface.co/docs/datasets/index" rel="nofollow">Huggingface Datasets documentation</a>). The dataset should contain the following fields:</p>
<ul dir="auto">
<li><code>keys</code>: Unique identifier for each audio sample</li>
<li><code>tags</code>: List of descriptive tags (e.g., <code>[&#34;pop&#34;, &#34;rock&#34;]</code>)</li>
<li><code>norm_lyrics</code>: Normalized lyrics text</li>
<li>Optional fields:
<ul dir="auto">
<li><code>speaker_emb_path</code>: Path to speaker embedding file (use empty string if not available)</li>
<li><code>recaption</code>: Additional tag descriptions in various formats</li>
</ul>
</li>
</ul>
</li>
</ol>
<p dir="auto">Example dataset entry:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;keys&#34;: &#34;1ce52937-cd1d-456f-967d-0f1072fcbb58&#34;,
  &#34;tags&#34;: [&#34;pop&#34;, &#34;acoustic&#34;, &#34;ballad&#34;, &#34;romantic&#34;, &#34;emotional&#34;],
  &#34;speaker_emb_path&#34;: &#34;&#34;,
  &#34;norm_lyrics&#34;: &#34;I love you, I love you, I love you&#34;,
  &#34;recaption&#34;: {
    &#34;simplified&#34;: &#34;pop&#34;,
    &#34;expanded&#34;: &#34;pop, acoustic, ballad, romantic, emotional&#34;,
    &#34;descriptive&#34;: &#34;The sound is soft and gentle, like a tender breeze on a quiet evening. It&#39;s soothing and full of longing.&#34;,
    &#34;use_cases&#34;: &#34;Suitable for background music in romantic films or during intimate moments.&#34;,
    &#34;analysis&#34;: &#34;pop, ballad, piano, guitar, slow tempo, romantic, emotional&#34;
  }
}"><pre>{
  <span>&#34;keys&#34;</span>: <span><span>&#34;</span>1ce52937-cd1d-456f-967d-0f1072fcbb58<span>&#34;</span></span>,
  <span>&#34;tags&#34;</span>: [<span><span>&#34;</span>pop<span>&#34;</span></span>, <span><span>&#34;</span>acoustic<span>&#34;</span></span>, <span><span>&#34;</span>ballad<span>&#34;</span></span>, <span><span>&#34;</span>romantic<span>&#34;</span></span>, <span><span>&#34;</span>emotional<span>&#34;</span></span>],
  <span>&#34;speaker_emb_path&#34;</span>: <span><span>&#34;</span><span>&#34;</span></span>,
  <span>&#34;norm_lyrics&#34;</span>: <span><span>&#34;</span>I love you, I love you, I love you<span>&#34;</span></span>,
  <span>&#34;recaption&#34;</span>: {
    <span>&#34;simplified&#34;</span>: <span><span>&#34;</span>pop<span>&#34;</span></span>,
    <span>&#34;expanded&#34;</span>: <span><span>&#34;</span>pop, acoustic, ballad, romantic, emotional<span>&#34;</span></span>,
    <span>&#34;descriptive&#34;</span>: <span><span>&#34;</span>The sound is soft and gentle, like a tender breeze on a quiet evening. It&#39;s soothing and full of longing.<span>&#34;</span></span>,
    <span>&#34;use_cases&#34;</span>: <span><span>&#34;</span>Suitable for background music in romantic films or during intimate moments.<span>&#34;</span></span>,
    <span>&#34;analysis&#34;</span>: <span><span>&#34;</span>pop, ballad, piano, guitar, slow tempo, romantic, emotional<span>&#34;</span></span>
  }
}</pre></div>


<ul dir="auto">
<li><code>--dataset_path</code>: Path to your Huggingface dataset (required)</li>
<li><code>--checkpoint_dir</code>: Directory containing the base model checkpoint</li>
<li><code>--learning_rate</code>: Learning rate for training (default: 1e-4)</li>
<li><code>--max_steps</code>: Maximum number of training steps (default: 2000000)</li>
<li><code>--precision</code>: Training precision, e.g., &#34;bf16-mixed&#34; (default) or &#34;fp32&#34;</li>
<li><code>--devices</code>: Number of GPUs to use (default: 1)</li>
<li><code>--num_nodes</code>: Number of compute nodes to use (default: 1)</li>
<li><code>--accumulate_grad_batches</code>: Gradient accumulation steps (default: 1)</li>
<li><code>--num_workers</code>: Number of data loading workers (default: 8)</li>
<li><code>--every_n_train_steps</code>: Checkpoint saving frequency (default: 2000)</li>
<li><code>--every_plot_step</code>: Frequency of generating evaluation samples (default: 2000)</li>
<li><code>--exp_name</code>: Experiment name for logging (default: &#34;text2music_train_test&#34;)</li>
<li><code>--logger_dir</code>: Directory for saving logs (default: &#34;./exps/logs/&#34;)</li>
</ul>

<p dir="auto">Train the base model with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python trainer.py --dataset_path &#34;path/to/your/dataset&#34; --checkpoint_dir &#34;path/to/base/checkpoint&#34; --exp_name &#34;your_experiment_name&#34;"><pre>python trainer.py --dataset_path <span><span>&#34;</span>path/to/your/dataset<span>&#34;</span></span> --checkpoint_dir <span><span>&#34;</span>path/to/base/checkpoint<span>&#34;</span></span> --exp_name <span><span>&#34;</span>your_experiment_name<span>&#34;</span></span></pre></div>

<p dir="auto">For LoRA training, you need to provide a LoRA configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python trainer.py --dataset_path &#34;path/to/your/dataset&#34; --checkpoint_dir &#34;path/to/base/checkpoint&#34; --lora_config_path &#34;path/to/lora_config.json&#34; --exp_name &#34;your_lora_experiment&#34;"><pre>python trainer.py --dataset_path <span><span>&#34;</span>path/to/your/dataset<span>&#34;</span></span> --checkpoint_dir <span><span>&#34;</span>path/to/base/checkpoint<span>&#34;</span></span> --lora_config_path <span><span>&#34;</span>path/to/lora_config.json<span>&#34;</span></span> --exp_name <span><span>&#34;</span>your_lora_experiment<span>&#34;</span></span></pre></div>
<p dir="auto">Example LoRA configuration file (lora_config.json):</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;r&#34;: 16,
  &#34;lora_alpha&#34;: 32,
  &#34;target_modules&#34;: [
    &#34;speaker_embedder&#34;,
    &#34;linear_q&#34;,
    &#34;linear_k&#34;,
    &#34;linear_v&#34;,
    &#34;to_q&#34;,
    &#34;to_k&#34;,
    &#34;to_v&#34;,
    &#34;to_out.0&#34;
  ]
}"><pre>{
  <span>&#34;r&#34;</span>: <span>16</span>,
  <span>&#34;lora_alpha&#34;</span>: <span>32</span>,
  <span>&#34;target_modules&#34;</span>: [
    <span><span>&#34;</span>speaker_embedder<span>&#34;</span></span>,
    <span><span>&#34;</span>linear_q<span>&#34;</span></span>,
    <span><span>&#34;</span>linear_k<span>&#34;</span></span>,
    <span><span>&#34;</span>linear_v<span>&#34;</span></span>,
    <span><span>&#34;</span>to_q<span>&#34;</span></span>,
    <span><span>&#34;</span>to_k<span>&#34;</span></span>,
    <span><span>&#34;</span>to_v<span>&#34;</span></span>,
    <span><span>&#34;</span>to_out.0<span>&#34;</span></span>
  ]
}</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Advanced Training Options</h3><a id="user-content-advanced-training-options" aria-label="Permalink: Advanced Training Options" href="#advanced-training-options"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>--shift</code>: Flow matching shift parameter (default: 3.0)</li>
<li><code>--gradient_clip_val</code>: Gradient clipping value (default: 0.5)</li>
<li><code>--gradient_clip_algorithm</code>: Gradient clipping algorithm (default: &#34;norm&#34;)</li>
<li><code>--reload_dataloaders_every_n_epochs</code>: Frequency to reload dataloaders (default: 1)</li>
<li><code>--val_check_interval</code>: Validation check interval (default: None)</li>
</ul>

<p dir="auto">This project is licensed under <a href="https://github.com/ace-step/ACE-Step/blob/main/LICENSE">Apache License 2.0</a></p>
<p dir="auto">ACE-Step enables original music generation across diverse genres, with applications in creative production, education, and entertainment. While designed to support positive and artistic use cases, we acknowledge potential risks such as unintentional copyright infringement due to stylistic similarity, inappropriate blending of cultural elements, and misuse for generating harmful content. To ensure responsible use, we encourage users to verify the originality of generated works, clearly disclose AI involvement, and obtain appropriate permissions when adapting protected styles or materials. By using ACE-Step, you agree to uphold these principles and respect artistic integrity, cultural diversity, and legal compliance. The authors are not responsible for any misuse of the model, including but not limited to copyright violations, cultural insensitivity, or the generation of harmful content.</p>

<p dir="auto">This project is co-led by ACE Studio and StepFun.</p>

<p dir="auto">If you find this project useful for your research, please consider citing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{gong2025acestep,
  title={ACE-Step: A Step Towards Music Generation Foundation Model},
  author={Junmin Gong, Wenxiao Zhao, Sen Wang, Shengyuan Xu, Jing Guo}, 
  howpublished={\url{https://github.com/ace-step/ACE-Step}},
  year={2025},
  note={GitHub repository}
}"><pre><span>@misc</span>{<span>gong2025acestep</span>,
  <span>title</span>=<span><span>{</span>ACE-Step: A Step Towards Music Generation Foundation Model<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Junmin Gong, Wenxiao Zhao, Sen Wang, Shengyuan Xu, Jing Guo<span>}</span></span>, 
  <span>howpublished</span>=<span><span>{</span>\url{https://github.com/ace-step/ACE-Step}<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
  <span>note</span>=<span><span>{</span>GitHub repository<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
