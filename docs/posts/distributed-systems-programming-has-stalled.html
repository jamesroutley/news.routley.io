<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.shadaj.me/writing/distributed-programming-stalled">Original</a>
    <h1>Distributed Systems Programming Has Stalled</h1>
    
    <div id="readability-page-1" class="page"><div><p>Over the last decade, we’ve seen great advancements in distributed systems, but the way we program them has seen few fundamental improvements. While we can sometimes abstract away distribution (Spark, Redis, etc.), developers still struggle with challenges like concurrency, fault tolerance, and versioning.</p>
<p>There are lots of people (and startups) working on this. But nearly all focus on tooling to help analyze distributed systems written in classic (sequential) programming languages. Tools like <a href="https://jepsen.io/">Jepsen</a> and <a href="https://antithesis.com/">Antithesis</a> have advanced the state-of-the-art for verifying correctness and fault tolerance, but tooling is no match for programming models that natively surface fundamental concepts. We’ve already seen this with Rust, which provides memory safety guarantees that are far richer than C++ with AddressSanitizer.</p>
<p>If you look online, there are tons of frameworks for writing distributed code. In this blog post, I’ll make the case that they only offer band-aids and sugar over three fixed underlying paradigms: <strong>external-distribution</strong>, <strong>static-location</strong>, and <strong>arbitrary-location</strong>. We’re still missing a programming model that is <em>native</em> to distributed systems. We’ll walk through these paradigms then reflect on what’s missing for a truly distributed programming model.</p>
<hr/>
<p><strong>External-distribution</strong> architectures are what the vast majority of “distributed” systems look like. In this model, software is written as <em>sequential logic</em> that runs against a state management system with <em>sequential semantics</em>:</p>
<ul>
<li>Stateless Services with a Distributed Database (Aurora DSQL, Cockroach)</li>
<li>Services using gossiped CRDT state (Ditto, ElectricSQL, Redis Enterprise)<sup>1</sup>1<span>1<!-- -->. <!-- -->This may come as a surprise. CRDTs are often marketed as a silver bullet for all distributed systems, but another perspective is they simply <a href="https://www.vldb.org/pvldb/vol16/p856-power.pdf">accelerate distributed transactions</a>. Software running over CRDTs is still sequential. <a href="#user-content-fnref-this%20may%20come%20as%20a%20surprise.%20crdts%20are%20often%20marketed%20as%20a%20silver%20bullet%20for%20all%20distributed%20systems,%20but%20another%20perspective%20is%20they%20simply%20accelerate%20distributed%20transactions.%20software%20running%20over%20crdts%20is%20still%20sequential." data-footnote-backref="" aria-label="Back to reference 1">↩</a></span><span><span>This may come as a surprise. CRDTs are often marketed as a silver bullet for all distributed systems, but another perspective is they simply <a href="https://www.vldb.org/pvldb/vol16/p856-power.pdf">accelerate distributed transactions</a>. Software running over CRDTs is still sequential. <a href="#user-content-fnref-this%20may%20come%20as%20a%20surprise.%20crdts%20are%20often%20marketed%20as%20a%20silver%20bullet%20for%20all%20distributed%20systems,%20but%20another%20perspective%20is%20they%20simply%20accelerate%20distributed%20transactions.%20software%20running%20over%20crdts%20is%20still%20sequential." data-footnote-backref="" aria-label="Back to reference 1">↩</a></span></span></li>
<li>Workflows and Step Functions</li>
</ul>
<p>These architectures are easy to write software in, because none of the underlying distribution is exposed<sup>2</sup>2<span>2<!-- -->. <!-- -->Well that’s the idea, at least. Serializability typically isn’t the default (<a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a> is), so concurrency bugs can sometimes be exposed. <a href="#user-content-fnref-well%20that%E2%80%99s%20the%20idea,%20at%20least.%20serializability%20typically%20isn%E2%80%99t%20the%20default%20(snapshot%20isolation%20is),%20so%20concurrency%20bugs%20can%20sometimes%20be%20exposed." data-footnote-backref="" aria-label="Back to reference 2">↩</a></span><span><span>Well that’s the idea, at least. Serializability typically isn’t the default (<a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a> is), so concurrency bugs can sometimes be exposed. <a href="#user-content-fnref-well%20that%E2%80%99s%20the%20idea,%20at%20least.%20serializability%20typically%20isn%E2%80%99t%20the%20default%20(snapshot%20isolation%20is),%20so%20concurrency%20bugs%20can%20sometimes%20be%20exposed." data-footnote-backref="" aria-label="Back to reference 2">↩</a></span></span> to the developer! Although this architecture results in a distributed <em>system</em>, we do not have a distributed <em>programming model</em>.</p>
<p>There is little need to reason about fault-tolerance or concurrency bugs (other than making sure to opt into the right consistency levels for CRDTs). So it’s clear why developers opt for this option, since it hides the distributed chaos under a clean, sequential semantics. But this comes at a clear cost: performance and scalability.</p>
<p>Serializing everything is tantamount to <em>emulating</em> a non-distributed system, but with expensive coordination protocols. The database forms a single point of failure in your system; you either hope that us-east-1 doesn’t go down or switch to a multi-writer system like Cockroach that comes with its own performance implications. Many applications are at sufficiently low scale to tolerate this, but you wouldn’t want to <a href="https://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2">implement a counter</a> like this.</p>
<hr/>
<p><strong>Static-location</strong> architectures are the classic way to write distributed code. You compose several units—each written as <em>local (single-machine)</em> code that communicates with other machines using asynchronous network calls:</p>
<ul>
<li>Services communicating with API calls, possibly using async / await (gRPC, REST)</li>
<li>Actors (Akka, Ray, Orleans)</li>
<li>Services polling and pushing to a shared pub/sub (Kafka)</li>
</ul>
<p>These architectures give us full, low-level control. We’re writing a bunch of sequential, single-machine software with network calls. This is great for performance and fault-tolerance because we control what gets run where and when.</p>
<p>But the boundaries between networked units are <strong>rigid and opaque</strong>. Developers must make one-way decisions on how to break up their application. These decisions have a wide impact on correctness; retries and message ordering are controlled by the sender and unknown to the recipient. Furthermore, the language and tooling have limited insight into how units are composed. Jump-to-definition is often unavailable, and serialization mismatches across services can easily creep in.</p>
<p>Most importantly, this approach to distributed systems fundamentally eliminates <strong>semantic co-location and modularity.</strong> In sequential code, things that happen one after the other are <em>textually placed</em> one after the other and function calls encapsulate <em>entire algorithms</em>. But with static-location architectures, developers are coerced to modularize code on machine boundaries, rather than on semantic boundaries. In these architectures there is simply no way to encapsulate a distributed algorithm as a single, unified semantic unit.</p>
<p>Although static-location architectures offer developers the most low-level control over their system, in practice they are difficult to implement robustly without distributed systems expertise. There is a fundamental mismatch between <em>implementation</em> and <em>execution</em>: static-location software is written as single-machine code, but the correctness of the system requires reasoning about the fleet of machines as a whole. Teams building such systems often live in fear of concurrency bugs and failures, leading to mountains of legacy code that are too critical to touch.</p>
<hr/>
<p><strong>Arbitrary-location</strong> architectures are the foundation of most “modern” approaches to distributed systems. These architectures simplify distributed systems by letting to write code as if it were running on a single machine, but at runtime the software is <em>dynamically</em> executed <em>across several machines</em><sup>3</sup>3<span>3<!-- -->. <!-- -->Actor frameworks don’t <em>really</em> count even if they support migration, since the developer still has to explicitly define the boundaries of an actor and specify where message passing happens <a href="#user-content-fnref-actor%20frameworks%20don%E2%80%99t%20really%20count%20even%20if%20they%20support%20migration,%20since%20the%20developer%20still%20has%20to%20explicitly%20define%20the%20boundaries%20of%20an%20actor%20and%20specify%20where%20message%20passing%20happens" data-footnote-backref="" aria-label="Back to reference 3">↩</a></span><span><span>Actor frameworks don’t <em>really</em> count even if they support migration, since the developer still has to explicitly define the boundaries of an actor and specify where message passing happens <a href="#user-content-fnref-actor%20frameworks%20don%E2%80%99t%20really%20count%20even%20if%20they%20support%20migration,%20since%20the%20developer%20still%20has%20to%20explicitly%20define%20the%20boundaries%20of%20an%20actor%20and%20specify%20where%20message%20passing%20happens" data-footnote-backref="" aria-label="Back to reference 3">↩</a></span></span><strong>:</strong></p>
<ul>
<li>Distributed SQL Engines</li>
<li>MapReduce Frameworks (Hadoop, Spark)</li>
<li>Stream Processing (Flink, Spark Streaming, Storm)</li>
<li>Durable Execution (Temporal, DBOS, Azure Durable Functions)</li>
</ul>
<p>These architectures elegantly handle the co-location problem since there are no explicit network boundaries in the language/API to split your code across. But this simplicity comes at a significant cost: <strong>control</strong>. By letting the runtime decide how the code is distributed, we lose the ability to make decisions about how the application is scaled, where the fault domains lie, and when data is sent over the network.</p>
<p>Just like the external-distribution model, arbitrary-location architectures often come with a performance cost. Durable execution systems typically snapshot their state to a persistent store between every step<sup>4</sup>4<span>4<!-- -->. <!-- -->With some optimizations when a step is a pure, deterministic function <a href="#user-content-fnref-with%20some%20optimizations%20when%20a%20step%20is%20a%20pure,%20deterministic%20function" data-footnote-backref="" aria-label="Back to reference 4">↩</a></span><span><span>With some optimizations when a step is a pure, deterministic function <a href="#user-content-fnref-with%20some%20optimizations%20when%20a%20step%20is%20a%20pure,%20deterministic%20function" data-footnote-backref="" aria-label="Back to reference 4">↩</a></span></span>. Stream processing systems may dynamically persist data and are free to introduce asynchrony across steps. SQL users are at the mercy of the query optimizer, to which they at best can only give “hints” on distribution decisions.</p>
<p>We often need low-level control over where individual logic is placed for performance and correctness. Consider implementing Two-Phase Commit. This protocol has explicit, asymmetric roles for a leader that broadcasts proposals and workers that acknowledge them. To correctly implement such a protocol, we need to explicitly assign specific logic to these roles, since quorums must be determined on a single leader and each worker must atomically decide to accept or reject a proposal. It’s simply not possible to implement such a protocol in an arbitrary-location architecture without introducing unnecessary networking and coordination overhead.</p>

<p>If you’ve been following the “agentic” LLM space, you might be wondering: “Are any of these issues relevant in a world where my software is being written by an LLM?” If the static-location model is sufficiently rich to express all distributed systems, who cares if it’s painful to program in!</p>
<p>I’d argue that LLMs actually are a great argument why we <em>need</em> a new programming model. These models famously struggle under scenarios where contextually-relevant information is scattered across large bodies of text<sup>5</sup>5<span>5<!-- -->. <!-- -->See the <a href="https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it">Needle in a Haystack Test</a>; reasoning about distributed systems is even harder. <a href="#user-content-fnref-see%20the%20needle%20in%20a%20haystack%20test;%20reasoning%20about%20distributed%20systems%20is%20even%20harder." data-footnote-backref="" aria-label="Back to reference 5">↩</a></span><span><span>See the <a href="https://cloud.google.com/blog/products/ai-machine-learning/the-needle-in-the-haystack-test-and-how-gemini-pro-solves-it">Needle in a Haystack Test</a>; reasoning about distributed systems is even harder. <a href="#user-content-fnref-see%20the%20needle%20in%20a%20haystack%20test;%20reasoning%20about%20distributed%20systems%20is%20even%20harder." data-footnote-backref="" aria-label="Back to reference 5">↩</a></span></span>. LLMs do best when semantically-relevant information is co-located.</p>
<p>The static-location model forces us to split up our semantically-connected distributed logic across several modules. LLMs aren’t great yet at correctness on a single machine; it is well beyond their abilities to compose several single-machine programs that work together correctly. Furthermore, LLMs make decisions sequentially; splitting up distributed logic across several networked modules is inherently challenging to the very structure of AI models.</p>
<p>LLMs would do far better with a programming model that retains “semantic locality”. In a hypothetical programming model where code that spans several machines can be co-located, this problem becomes trivial. All the relevant logic for a distributed algorithm would be right next to each other, and the LLM can generate distributed logic in a straight-line manner.</p>
<p>The other piece of the puzzle is correctness. LLMs make mistakes, and our best bet is to combine them with tools that can automatically discover them<sup>6</sup>6<span>6<!-- -->. <!-- -->Lean is a great example of this in action. Teams including <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">Google</a> and <a href="https://arxiv.org/pdf/2405.14333">Deepseek</a> have been using it for some time. <a href="#user-content-fnref-lean%20is%20a%20great%20example%20of%20this%20in%20action.%20teams%20including%20google%20and%20deepseek%20have%20been%20using%20it%20for%20some%20time." data-footnote-backref="" aria-label="Back to reference 6">↩</a></span><span><span>Lean is a great example of this in action. Teams including <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">Google</a> and <a href="https://arxiv.org/pdf/2405.14333">Deepseek</a> have been using it for some time. <a href="#user-content-fnref-lean%20is%20a%20great%20example%20of%20this%20in%20action.%20teams%20including%20google%20and%20deepseek%20have%20been%20using%20it%20for%20some%20time." data-footnote-backref="" aria-label="Back to reference 6">↩</a></span></span>. Sequential models have no way to reason about the ways distributed execution might cause trouble. But a sufficiently rich distributed programming model could surface issues arising from network delays and faults (think a borrow-checker, but for distributed systems).</p>

<p>Although the programming models we’ve discussed each have several limitations, they also demonstrate desirable features that a native programming model for distributed systems should support. What can we learn from each model?</p>
<p>I’m going to skip over external-distribution, which as we discussed is not quite distributed. For applications that can tolerate the performance and semantic restrictions of this model, this is the way to go. But for a general distributed programming model, we can’t keep networking and concurrency hidden from the developer.</p>
<p>The static-location model seems like the right place to start, since it is at least capable of <em>expressing</em> all the types of distributed systems we might want to implement, even if the programming model offers us little help in <em>reasoning</em> about the distribution. We were missing two things that the arbitrary-location model offered:</p>
<ul>
<li>Writing logic that spans several machines right next to each other, in a single function</li>
<li>Surfacing semantic information on distributed behavior such as message reordering, retries, and serialization formats across network boundaries</li>
</ul>
<p>Each of these points have a dual, something we don’t want to give up:</p>
<ul>
<li>Explicit control over placement of logic on machines, with the ability to perform local, atomic computations</li>
<li>Rich options for fault tolerance guarantees and network semantics, without the language locking us into global coordination and recovery protocols</li>
</ul>
<p>It’s time for a <em>native</em> programming model—a Rust-for-distributed systems, if you will—that addresses all of these.</p>
<p><em>Thanks to</em> <a href="https://tylerhou.com/"><em>Tyler Hou</em></a><em>,</em> <a href="https://dsf.berkeley.edu/jmh/"><em>Joe Hellerstein</em></a><em>, and</em> <a href="https://www.ramnivas.com/about"><em>Ramnivas Laddad</em></a> <em>for feedback on this post!</em></p>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>
  </body>
</html>
