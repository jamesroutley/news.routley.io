<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://wattenberger.com/thoughts/fish-eye">Original</a>
    <h1>LLM abstraction levels inspired by fish eye lens</h1>
    
    <div id="readability-page-1" class="page"><div>





<article id="fish-eye">

<div>


	
	

	
	
	
	

	<a href="https://liquidbrain.net/"><svg style="width: min(8vw, 8vh)" viewBox="0 0 245 213" fill="none" xmlns="http://www.w3.org/2000/svg"><g style="mix-blend-mode:multiply"><path fill-rule="evenodd" clip-rule="evenodd" d="M122.692 0.712992L163.395 71.213L163.394 71.2143L204.097 141.713H122.7L122.692 141.713L122.69 141.713H41.2939L81.993 71.2201L81.9889 71.213L122.692 0.712992Z" fill="#102A3D22"></path></g><g style="mix-blend-mode:multiply"><path fill-rule="evenodd" clip-rule="evenodd" d="M81.9889 212.213L122.692 141.713L122.691 141.712L163.394 71.213H81.9972L81.9889 71.2129L81.9873 71.213H0.590759L41.2898 141.706L41.2857 141.713L81.9889 212.213Z" fill="#102A3D22"></path></g><g style="mix-blend-mode:multiply"><path fill-rule="evenodd" clip-rule="evenodd" d="M163.39 212.213L204.094 141.713L204.093 141.712L244.795 71.213H163.399L163.39 71.2129L163.389 71.213H81.9922L122.691 141.706L122.687 141.713L163.39 212.213Z" fill="#102A3D22"></path></g></svg></a>

	

	
	

	<p>We often think of the same information at different levels or abstraction. Here&#39;s a simple example:
	</p>

	<div><div><p>Here&#39;s a close-up of a fish— we can see the scales, the texture of the fins,
			the faint reflection in the eye.
		</p>

		<p><img src="https://liquidbrain.net/thoughts/fish-eye/fish-zoom1.png" alt="An illustration of a fish, close up"/></p></div>

	<div><p>Zoomed out a bit, the details are faded slightly. The
			surrounding ocean comes into view.
		</p>
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/fish-zoom2.png" alt="An illustration of a fish, medium zoom"/></p></div>	

	<div><p>Zooming out more, the fish is just a few splashes of color amongst a school of fish.
		</p>
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/fish-zoom3.png" alt="An illustration of a fish, far away in a school of fish"/></p></div></div>

	

	<p>Maps work in a similar way.</p>

	<div><div><p>Looking at a section of the National Zoo, we can see paths, buildings, and even animal enclosures!
		</p>
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/map-zoom1.png" alt="A zoomed in Google map of the Reptile Discovery Center at the National Zoo"/></p></div>

		<div><p>Zoomed out a little, and now it’s about neighborhoods—parks, stores, and streets.
		</p>
		
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/map-zoom2.png" alt="A few zooms out on a Google map, looking at a few neighborhoods in DC"/></p></div>	

	<div><p>Keep zooming out and you’re looking at a city, with just the highways and terrain.
		</p>
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/map-zoom3.png" alt="A further zoomed out Google map of the greater DC area"/></p></div></div>

	<p>Each level gives you completely different information, depending on what Google thinks
		the user might be interested in. Maps are a true masterclass for visualizing the same information in a variety of ways.
	</p>

	<p>So far, these are familiar examples. What if we applied the same principles to text?</p>

	<div><div><p>Let&#39;s take a look at the opening paragraph of The Metamorphosis.
		</p>
		<p>As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed
			into a gigantic insect. He was laying on his hard, as it were armor-plated, back and when he
			lifted his head a little he could see his domelike brown belly divided into stiff arched
			segments on top of which the bed quilt could hardly keep in position and was about to slide
			off completely. His numerous legs, which were pitifully thin compared to the rest of his bulk,
			waved helplessly before his eyes.
		</p></div>

	<div><p>Abstracted a little, we can remove some of the detail.
		</p>

		<p>Gregor Samsa wakes from troubled sleep to find he&#39;s become a giant insect. He lies trapped on
			his hard shell-like back, observing his new segmented belly and thin legs moving helplessly.
		</p></div>

	<div><p>And fully zoomed out, we can summarize the paragraph in a single sentence.
		</p>
		<p>A man unexpectedly transforms into an insect while sleeping.
		</p></div></div>

	<p>See how we can view even text at different levels of abstraction? For the interested, there&#39;s a little more detail in my <a href="https://www.youtube.com/watch?v=PAy_GHUAICw" target="_blank" rel="noopener noreferrer">AI Engineer Summit</a> talk from 2023.
	</p>

	

	<h2>Showing multiple levels at once</h2>

	<p>Viewing the same text at different levels of
		abstraction is powerful, but what, instead of switching between them, we could see multiple levels at the same time? How might that work?
	</p>

	<p>Let&#39;s look at an analogy from photography. A portrait lens brings a single subject into focus, isolating it from the background to draw all attention to its details. A wide-angle lens captures more of the scene, showing how the subject relates to its surroundings. And then there’s the fish eye lens—a tool that does both, pulling the center close while curving the edges to reveal the full context.
	</p>

	

	<p>A fish eye lens doesn’t ask us to choose between focus and context—it lets us experience both simultaneously. It’s good inspiration for how to offer detailed answers while revealing the surrounding connections and structures.
	</p>

	<p>Often, we’re handed a single piece of text, stripped from its surroundings: a quote from a book, a paragraph from a Wikipedia article, or a sentence from a movie script. You&#39;re left filling in the context on your own, and often incorrectly.
	</p>

	<p>Take this quote from The Game of Thrones:
</p>
<p>Winter is coming.
	<svg width="1em" viewBox="0 0 1 1"><path d="M0 1 Q 1 1 1 0" fill="none" stroke="white" stroke-width="6" vector-effect="non-scaling-stroke"></path></svg></p>
	
	<p>Without the backdrop of the looming threat in Westeros, this is just a weather report. Within the series, it’s an ominous warning of existential danger. Viewing quotes out of context is like trying to understand a movie by watching one scene: inherently incomplete.
	</p>

	

	<p>Imagine you’re reading The Elves and the Shoemaker by The Brothers Grimm. You come across a single paragraph describing the shoemaker discovering the tiny, perfectly crafted shoes left by the elves. Without context, the paragraph is just an intriguing moment.
	</p>
	
	<p>Now, what if instead of reading the whole book, you could hover over this paragraph and instantly access a layered view of the story? The immediate layer might summarize the events leading up to this moment: the shoemaker, struggling in poverty, left his last bit of leather out overnight. Another layer could give you a broader view of the story so far: the shoemaker’s business is mysteriously revitalized thanks to these tiny benefactors. Beyond that, an even higher-level summary might preview how the tale concludes, with the shoemaker and his wife crafting clothes for the elves to thank them.
	</p>

	

	<p>This approach allows you to orient yourself without having to piece everything together by reading linearly. You get the detail of the paragraph itself, but with the added richness of understanding how it fits into the larger story.
	</p>

	<p>This specific example isn&#39;t the best, but alas there are only so many hours a day to play with code with a baby around. But I can imagine variations on it that would be more compelling. Even just a &#34;grab a quote&#34; interaction for articles that inserts some context around the quote would be really useful.
	</p>

	

	<p>Think about how we typically learn. Pick up a book, and context surrounds every piece of information. Chapters give structure, connecting each idea to the ones that came before and after. A good author sets the stage, immersing you with anecdotes, historical background, or thematic threads that help you make sense of the details. Even the act of flipping through a book—a glance at the cover, the table of contents, a few highlighted sections—anchors you in a broader narrative.
	</p>

	<p>Think about learning something from a person. Let’s say a friend tells you about fancy goldfish. Their explanation doesn’t stop at naming the Oranda—they might tell you where they first saw one, why they think it’s fascinating, or how it’s a beloved species in Chinese culture, symbolizing luck and prosperity. The context of who is telling you the information—their expertise, interests, or personal connection—colors how you understand it.
	</p>
	
	<p>Or take museums: when you see a fish displayed in an exhibit, the plaque beneath it doesn’t just name the species. It tells you where the fish lives, how it evolved, and maybe even its significance in art or mythology. The exhibit places the fish in an ecosystem of knowledge, helping you understand it in a way that goes beyond just a name.
	</p>

	<h2>Zooming Beyond the Answer
	</h2>

	<p>But when you ask an LLM chatbot, you don’t get any of that. It’s as if you opened a book to one random sentence, read it, and closed the book again. Sure, the answer might be accurate, even helpful—but it’s stripped of all the surrounding context that makes learning feel meaningful and complete.
			</p>
		
	<p>Let&#39;s say you take a photo of a fish on vacation and want to know what it is.
	</p>

	<div><div><p>What fish is this?
				<svg width="1em" viewBox="0 0 1 1"><path d="M0 1 Q 1 1 1 0" fill="none" stroke="white" stroke-width="6" vector-effect="non-scaling-stroke"></path></svg></p>
			<p><img src="https://liquidbrain.net/thoughts/fish-eye/goldfish.png" alt="An illustration of a fish, close up"/></p></div>
		<p>This appears to be a type of fancy goldfish, likely an Oranda or Fantail Goldfish, recognized for its flowing fins and vibrant coloration.

			<svg width="1em" viewBox="0 0 1 1"><path d="M1 1 Q 0 1 0 0" fill="none" stroke="white" stroke-width="6" vector-effect="non-scaling-stroke"></path></svg></p></div>

	<p>Not a bad answer, really! But we&#39;re fully zoomed in on just the answer to our question. Sure, you can ask follow-up questions—what does the Oranda eat, where does it live, how does it compare to other goldfish?—but the onus is entirely on you to know what to ask next. The chatbot is reactive, not proactive, and unless you’re already an ichthyology enthusiast, you might not know the right questions to even start exploring deeper.
	</p>

	<p>Let&#39;s reimagine a Wikipedia a bit. In the center of the
		page, you see a detailed article about fancy goldfish—their habitat, types, and role in the food chain.
		Surrounding this are broader topics like ornamental fish, similar topics like Koi fish, more specific topics like the Oranda goldfish, and related people like the designer who popularized them.
	</p>

	<p>Clicking on another topic shifts it to the center, expanding into full detail while its
		context adjusts around it. It’s dynamic, engaging, and most importantly, it keeps you connected
		to the web of knowledge.
	</p>

	
	
	<p>You’re learning around a concept, immersing yourself in a connected web of ideas where one answer sparks curiosity about the next.
	</p>

	

	<h2>What else could this apply to?</h2>

	<p>Where else could we apply a fish eye lens?
	</p>

	<ul><li><img src="https://liquidbrain.net/thoughts/fish-eye/fish13.png" alt="An illustration of a fish"/>
			<strong>Timelines</strong>
			</li>

		<li><img src="https://liquidbrain.net/thoughts/fish-eye/fish1.png" alt="An illustration of a fish"/>
			<strong>Code</strong>
			</li>

		<li><img src="https://liquidbrain.net/thoughts/fish-eye/fish17.png" alt="An illustration of a fish"/>
			<strong>Education</strong>
			</li>

		<li><img src="https://liquidbrain.net/thoughts/fish-eye/fish8.png" alt="An illustration of a fish"/>
			<strong>Task management</strong>
			</li></ul>

	<p>The beauty of a fish eye lens for text is how naturally it fits with the way we process the world. We’re wired to see the details of a single flower while still noticing the meadow it grows in, to focus on a conversation while staying aware of the room around us. Facts and ideas are never meaningful in isolation; they only gain depth and relevance when connected to the broader context.
	</p>
	
	<p>This concept isn’t new—it’s foundational to fields like data visualization. A single number on its own might tell you something, but it’s the trends, comparisons, and relationships that truly reveal its story. Is 42 a high number? A low one? Without context, it’s impossible to say. Context is what turns raw data into understanding, and it’s what makes any fact—or paragraph, or answer—gain meaning.
</p>

<p>The fish eye lens takes this same principle and applies it to how we explore knowledge. It’s not just about seeing the big picture or the fine print—it’s about navigating between them effortlessly. By mirroring the way we naturally process detail and context, it creates tools that help us think not only more clearly but also more humanly.
	</p>

	<div>
		<p><img src="https://liquidbrain.net/thoughts/fish-eye/footer4.png" alt="An illustration of coral."/></p></div>

	
</div>
</article>


			
			
		</div></div>
  </body>
</html>
