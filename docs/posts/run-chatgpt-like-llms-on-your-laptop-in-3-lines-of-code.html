<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/amaiya/onprem">Original</a>
    <h1>Run ChatGPT-like LLMs on your laptop in 3 lines of code</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">

<blockquote>
<p dir="auto">A tool for running large language models on-premises using non-public
data</p>
</blockquote>
<p dir="auto"><strong>OnPrem.LLM</strong> is a simple Python package that makes it easier to run
large language models (LLMs) on non-public or sensitive data and on
machines with no internet connectivity (e.g., behind corporate
firewalls). Inspired by the
<a href="https://github.com/imartinez/privateGPT">privateGPT</a> GitHub repo and
Simon Willison’s <a href="https://pypi.org/project/llm/" rel="nofollow">LLM</a> command-line
utility, <strong>OnPrem.LLM</strong> is designed to help integrate local LLMs into
practical applications.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-install" aria-hidden="true" tabindex="-1" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Install</h2>
<p dir="auto">Once <a href="https://pytorch.org/get-started/locally/" rel="nofollow">installing PyTorch</a>, you
can install <strong>OnPrem.LLM</strong> with:</p>

<p dir="auto">For fast GPU-accelerated inference, see additional instructions below.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-how-to-use" aria-hidden="true" tabindex="-1" href="#how-to-use"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to use</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" tabindex="-1" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h3>
<div dir="auto" data-snippet-clipboard-copy-content="from onprem import LLM

llm = LLM()"><pre><span>from</span> <span>onprem</span> <span>import</span> <span>LLM</span>

<span>llm</span> <span>=</span> <span>LLM</span>()</pre></div>
<p dir="auto">By default, a 7B-parameter model is used. If <code>use_larger=True</code>, a
13B-parameter is used. You can also supply the URL to an LLM of your
choosing to <a href="https://amaiya.github.io/onprem/core.html#llm" rel="nofollow"><code>LLM</code></a> (see
code generation section below for an example). Currently, only models in
GGML format are supported. Future versions of <strong>OnPrem.LLM</strong> will
transition to the newer GGUF format.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-send-prompts-to-the-llm-to-solve-problems" aria-hidden="true" tabindex="-1" href="#send-prompts-to-the-llm-to-solve-problems"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Send Prompts to the LLM to Solve Problems</h3>
<p dir="auto">This is an example of few-shot prompting, where we provide an example of
what we want the LLM to do.</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &#34;&#34;&#34;Extract the names of people in the supplied sentences. Here is an example:
Sentence: James Gandolfini and Paul Newman were great actors.
People:
James Gandolfini, Paul Newman
Sentence:
I like Cillian Murphy&#39;s acting. Florence Pugh is great, too.
People:&#34;&#34;&#34;

saved_output = llm.prompt(prompt)"><pre><span>prompt</span> <span>=</span> <span>&#34;&#34;&#34;Extract the names of people in the supplied sentences. Here is an example:</span>
<span>Sentence: James Gandolfini and Paul Newman were great actors.</span>
<span>People:</span>
<span>James Gandolfini, Paul Newman</span>
<span>Sentence:</span>
<span>I like Cillian Murphy&#39;s acting. Florence Pugh is great, too.</span>
<span>People:&#34;&#34;&#34;</span>

<span>saved_output</span> <span>=</span> <span>llm</span>.<span>prompt</span>(<span>prompt</span>)</pre></div>
<div data-snippet-clipboard-copy-content="Cillian Murphy, Florence Pugh"><pre><code>Cillian Murphy, Florence Pugh
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-talk-to-your-documents" aria-hidden="true" tabindex="-1" href="#talk-to-your-documents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Talk to Your Documents</h3>
<p dir="auto">Answers are generated from the content of your documents.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-step-1-ingest-the-documents-into-a-vector-database" aria-hidden="true" tabindex="-1" href="#step-1-ingest-the-documents-into-a-vector-database"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 1: Ingest the Documents into a Vector Database</h4>
<div dir="auto" data-snippet-clipboard-copy-content="llm.ingest(&#39;./sample_data&#39;)"><pre><span>llm</span>.<span>ingest</span>(<span>&#39;./sample_data&#39;</span>)</pre></div>
<div data-snippet-clipboard-copy-content="2023-09-03 16:30:54.459509: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading new documents: 100%|██████████████████████| 2/2 [00:00&lt;00:00, 17.16it/s]

Creating new vectorstore
Loading documents from ./sample_data
Loaded 11 new documents from ./sample_data
Split into 62 chunks of text (max. 500 tokens each)
Creating embeddings. May take some minutes...
Ingestion complete! You can now query your documents using the LLM.ask method"><pre><code>2023-09-03 16:30:54.459509: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading new documents: 100%|██████████████████████| 2/2 [00:00&lt;00:00, 17.16it/s]

Creating new vectorstore
Loading documents from ./sample_data
Loaded 11 new documents from ./sample_data
Split into 62 chunks of text (max. 500 tokens each)
Creating embeddings. May take some minutes...
Ingestion complete! You can now query your documents using the LLM.ask method
</code></pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-step-2-answer-questions-about-the-documents" aria-hidden="true" tabindex="-1" href="#step-2-answer-questions-about-the-documents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 2: Answer Questions About the Documents</h4>
<div dir="auto" data-snippet-clipboard-copy-content="question = &#34;&#34;&#34;What is  ktrain?&#34;&#34;&#34; 
answer, docs = llm.ask(question)
print(&#39;\n\nReferences:\n\n&#39;)
for i, document in enumerate(docs):
    print(f&#34;\n{i+1}.&gt; &#34; + document.metadata[&#34;source&#34;] + &#34;:&#34;)
    print(document.page_content)"><pre><span>question</span> <span>=</span> <span>&#34;&#34;&#34;What is  ktrain?&#34;&#34;&#34;</span> 
<span>answer</span>, <span>docs</span> <span>=</span> <span>llm</span>.<span>ask</span>(<span>question</span>)
<span>print</span>(<span>&#39;<span>\n</span><span>\n</span>References:<span>\n</span><span>\n</span>&#39;</span>)
<span>for</span> <span>i</span>, <span>document</span> <span>in</span> <span>enumerate</span>(<span>docs</span>):
    <span>print</span>(<span>f&#34;<span>\n</span><span><span>{</span><span>i</span><span>+</span><span>1</span><span>}</span></span>.&gt; &#34;</span> <span>+</span> <span>document</span>.<span>metadata</span>[<span>&#34;source&#34;</span>] <span>+</span> <span>&#34;:&#34;</span>)
    <span>print</span>(<span>document</span>.<span>page_content</span>)</pre></div>
<div data-snippet-clipboard-copy-content=" Ktrain is a low-code machine learning library designed to augment human
engineers in the machine learning workow by automating or semi-automating various
aspects of model training, tuning, and application. Through its use, domain experts can
leverage their expertise while still benefiting from the power of machine learning techniques.

References:



1.&gt; ./sample_data/ktrain_paper.pdf:
lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-
tomation and instead focuses on either partially or fully automating other aspects of the
machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-
2

2.&gt; ./sample_data/ktrain_paper.pdf:
possible, ktrain automates (either algorithmically or through setting well-performing de-
faults), but also allows users to make choices that best ﬁt their unique application require-
ments. In this way, ktrain uses automation to augment and complement human engineers
rather than attempting to entirely replace them. In doing so, the strengths of both are
better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai

3.&gt; ./sample_data/ktrain_paper.pdf:
with custom models and data formats, as well.
Inspired by other low-code (and no-
code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig
(Molino et al., 2019), ktrain is intended to help further democratize machine learning by
enabling beginners and domain experts with minimal programming or data science experi-
4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups
6

4.&gt; ./sample_data/ktrain_paper.pdf:
ktrain: A Low-Code Library for Augmented Machine Learning
toML platform and more of what might be called a “low-code” ML platform. Through
automation or semi-automation, ktrain facilitates the full machine learning workﬂow from
curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,
tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain
experts who may have less experience with machine learning and software coding. Where"><pre><code> Ktrain is a low-code machine learning library designed to augment human
engineers in the machine learning workow by automating or semi-automating various
aspects of model training, tuning, and application. Through its use, domain experts can
leverage their expertise while still benefiting from the power of machine learning techniques.

References:



1.&gt; ./sample_data/ktrain_paper.pdf:
lection (He et al., 2019). By contrast, ktrain places less emphasis on this aspect of au-
tomation and instead focuses on either partially or fully automating other aspects of the
machine learning (ML) workﬂow. For these reasons, ktrain is less of a traditional Au-
2

2.&gt; ./sample_data/ktrain_paper.pdf:
possible, ktrain automates (either algorithmically or through setting well-performing de-
faults), but also allows users to make choices that best ﬁt their unique application require-
ments. In this way, ktrain uses automation to augment and complement human engineers
rather than attempting to entirely replace them. In doing so, the strengths of both are
better exploited. Following inspiration from a blog post1 by Rachel Thomas of fast.ai

3.&gt; ./sample_data/ktrain_paper.pdf:
with custom models and data formats, as well.
Inspired by other low-code (and no-
code) open-source ML libraries such as fastai (Howard and Gugger, 2020) and ludwig
(Molino et al., 2019), ktrain is intended to help further democratize machine learning by
enabling beginners and domain experts with minimal programming or data science experi-
4. http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups
6

4.&gt; ./sample_data/ktrain_paper.pdf:
ktrain: A Low-Code Library for Augmented Machine Learning
toML platform and more of what might be called a “low-code” ML platform. Through
automation or semi-automation, ktrain facilitates the full machine learning workﬂow from
curating and preprocessing inputs (i.e., ground-truth-labeled training data) to training,
tuning, troubleshooting, and applying models. In this way, ktrain is well-suited for domain
experts who may have less experience with machine learning and software coding. Where
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-code-generation" aria-hidden="true" tabindex="-1" href="#text-to-code-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text to Code Generation</h3>
<p dir="auto">We’ll use the CodeUp LLM by supplying the URL and employ the particular
prompt format this model expects.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from onprem import LLM
url = &#39;https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML/resolve/main/codeup-llama-2-13b-chat-hf.ggmlv3.q4_1.bin&#39;
llm = LLM(url, n_gpu_layers=43) # see below for GPU information"><pre><span>from</span> <span>onprem</span> <span>import</span> <span>LLM</span>
<span>url</span> <span>=</span> <span>&#39;https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML/resolve/main/codeup-llama-2-13b-chat-hf.ggmlv3.q4_1.bin&#39;</span>
<span>llm</span> <span>=</span> <span>LLM</span>(<span>url</span>, <span>n_gpu_layers</span><span>=</span><span>43</span>) <span># see below for GPU information</span></pre></div>
<p dir="auto">Setup the prompt based on what <a href="https://huggingface.co/TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML#prompt-template-alpaca" rel="nofollow">this model
expects</a>
(this is important):</p>
<div dir="auto" data-snippet-clipboard-copy-content="template = &#34;&#34;&#34;
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{prompt}

### Response:&#34;&#34;&#34;"><pre><span>template</span> <span>=</span> <span>&#34;&#34;&#34;</span>
<span>Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>
<span></span>
<span>### Instruction:</span>
<span>{prompt}</span>
<span></span>
<span>### Response:&#34;&#34;&#34;</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="answer = llm.prompt(&#39;Write Python code to validate an email address.&#39;, prompt_template=template)"><pre><span>answer</span> <span>=</span> <span>llm</span>.<span>prompt</span>(<span>&#39;Write Python code to validate an email address.&#39;</span>, <span>prompt_template</span><span>=</span><span>template</span>)</pre></div>
<div data-snippet-clipboard-copy-content="Here is an example of Python code that can be used to validate an email address:
```
import re

def validate_email(email):
    # Use a regular expression to check if the email address is in the correct format
    pattern = r&#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$&#39;
    if re.match(pattern, email):
        return True
    else:
        return False

# Test the validate_email function with different inputs
print(&#34;Email address is valid:&#34;, validate_email(&#34;example@example.com&#34;))  # Should print &#34;True&#34;
print(&#34;Email address is invalid:&#34;, validate_email(&#34;example@&#34;))  # Should print &#34;False&#34;
print(&#34;Email address is invalid:&#34;, validate_email(&#34;example.com&#34;))  # Should print &#34;False&#34;
```
The code defines a function `validate_email` that takes an email address as input and uses a regular expression to check if the email address is in the correct format. The regular expression checks for an email address that consists of one or more letters, numbers, periods, hyphens, or underscores followed by the `@` symbol, followed by one or more letters, periods, hyphens, or underscores followed by a `.` and two to three letters.
The function returns `True` if the email address is valid, and `False` otherwise. The code also includes some test examples to demonstrate how to use the function."><pre><code>Here is an example of Python code that can be used to validate an email address:
```
import re

def validate_email(email):
    # Use a regular expression to check if the email address is in the correct format
    pattern = r&#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$&#39;
    if re.match(pattern, email):
        return True
    else:
        return False

# Test the validate_email function with different inputs
print(&#34;Email address is valid:&#34;, validate_email(&#34;example@example.com&#34;))  # Should print &#34;True&#34;
print(&#34;Email address is invalid:&#34;, validate_email(&#34;example@&#34;))  # Should print &#34;False&#34;
print(&#34;Email address is invalid:&#34;, validate_email(&#34;example.com&#34;))  # Should print &#34;False&#34;
```
The code defines a function `validate_email` that takes an email address as input and uses a regular expression to check if the email address is in the correct format. The regular expression checks for an email address that consists of one or more letters, numbers, periods, hyphens, or underscores followed by the `@` symbol, followed by one or more letters, periods, hyphens, or underscores followed by a `.` and two to three letters.
The function returns `True` if the email address is valid, and `False` otherwise. The code also includes some test examples to demonstrate how to use the function.
</code></pre></div>
<p dir="auto">Let’s try out the code generated above.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import re
def validate_email(email):
    # Use a regular expression to check if the email address is in the correct format
    pattern = r&#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$&#39;
    if re.match(pattern, email):
        return True
    else:
        return False
print(validate_email(&#39;sam@@openai.com&#39;)) # bad email address
print(validate_email(&#39;sam@openai&#39;))      # bad email address
print(validate_email(&#39;sam@openai.com&#39;))  # good email address"><pre><span>import</span> <span>re</span>
<span>def</span> <span>validate_email</span>(<span>email</span>):
    <span># Use a regular expression to check if the email address is in the correct format</span>
    <span>pattern</span> <span>=</span> <span>r&#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$&#39;</span>
    <span>if</span> <span>re</span>.<span>match</span>(<span>pattern</span>, <span>email</span>):
        <span>return</span> <span>True</span>
    <span>else</span>:
        <span>return</span> <span>False</span>
<span>print</span>(<span>validate_email</span>(<span>&#39;sam@@openai.com&#39;</span>)) <span># bad email address</span>
<span>print</span>(<span>validate_email</span>(<span>&#39;sam@openai&#39;</span>))      <span># bad email address</span>
<span>print</span>(<span>validate_email</span>(<span>&#39;sam@openai.com&#39;</span>))  <span># good email address</span></pre></div>

<p dir="auto">The generated code may sometimes need editing, but this one worked
out-of-the-box.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-speeding-up-inference-using-a-gpu" aria-hidden="true" tabindex="-1" href="#speeding-up-inference-using-a-gpu"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Speeding Up Inference Using a GPU</h3>
<p dir="auto">The above example employed the use of a CPU.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-step-1-install-llama-cpp-python-with-cublas-support" aria-hidden="true" tabindex="-1" href="#step-1-install-llama-cpp-python-with-cublas-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 1: Install <code>llama-cpp-python</code> with CUBLAS support</h4>
<div dir="auto" data-snippet-clipboard-copy-content="CMAKE_ARGS=&#34;-DLLAMA_CUBLAS=on&#34; FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir"><pre>CMAKE_ARGS=<span><span>&#34;</span>-DLLAMA_CUBLAS=on<span>&#34;</span></span> FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir</pre></div>
<p dir="auto">It is important to use the specific version shown above due to library
incompatibilities.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-step-2-use-the-n_gpu_layers-argument-with-llm" aria-hidden="true" tabindex="-1" href="#step-2-use-the-n_gpu_layers-argument-with-llm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step 2: Use the <code>n_gpu_layers</code> argument with <a href="https://amaiya.github.io/onprem/core.html#llm" rel="nofollow"><code>LLM</code></a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="llm = LLM(model_name=os.path.basename(url), n_gpu_layers=128)"><pre><span>llm</span> <span>=</span> <span>LLM</span>(<span>model_name</span><span>=</span><span>os</span>.<span>path</span>.<span>basename</span>(<span>url</span>), <span>n_gpu_layers</span><span>=</span><span>128</span>)</pre></div>
<p dir="auto">With the steps above, calls to methods like <code>llm.prompt</code> will offload
computation to your GPU and speed up responses from the LLM.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-faq" aria-hidden="true" tabindex="-1" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQ</h2>
<ol dir="auto">
<li>How do I use other models with <strong>OnPrem.LLM</strong>?</li>
</ol>
<blockquote>
<p dir="auto">You supply the URL to other models to the
<a href="https://amaiya.github.io/onprem/core.html#llm" rel="nofollow"><code>LLM</code></a> constructor, as
we did above in the code generation example.</p>
</blockquote>
<ol start="2" dir="auto">
<li>I’m behind a corporate firewall and receiving an SSL error when
trying to download the model?</li>
</ol>
<blockquote>
<p dir="auto">Try this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from onprem import LLM
LLM.download_model(url, ssl_verify=False)"><pre><span>from</span> <span>onprem</span> <span>import</span> <span>LLM</span>
<span>LLM</span>.<span>download_model</span>(<span>url</span>, <span>ssl_verify</span><span>=</span><span>False</span>)</pre></div>
</blockquote>
<ol start="3" dir="auto">
<li>Which models can I use with this?</li>
</ol>
<blockquote>
<p dir="auto">We currently support models in GGML format. However, the GGML format
has now been superseded by GGUF. As of August 21st 2023, llama.cpp no
longer supports GGML models, which is why we are pinning to an older
version of all dependencies.</p>
<p dir="auto">Future versions of <strong>OnPrem.LLM</strong> will use the newer GGUF format.</p>
</blockquote>
</article>
          </div></div>
  </body>
</html>
