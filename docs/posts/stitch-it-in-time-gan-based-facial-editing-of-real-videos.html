<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stitch-time.github.io/">Original</a>
    <h1>Stitch It in Time: GAN-Based Facial Editing of Real Videos</h1>
    
    <div id="readability-page-1" class="page">

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
<!--</nav>-->


<section>
  <div>
    <div>
      <div>
        <div>
          
          

          <p><span>Tel Aviv University</span>
          </p>

          <div>
            <div>
              <!-- PDF Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2108.00946"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <p><span>
                <a href="https://arxiv.org/abs/2201.08361">
                  <span>
                      <i></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span></p><!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <p><span>
                <a href="https://github.com/rotemtzaban/STIT">
                  <span>
                      <i></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span></p><p>
              This page contains many wide videos which may not display well on a cellphone. Viewing on browser is recommended
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section>
  <div>
    <p>
      <video id="teaser" autoplay="" controls="" muted="" loop="" height="100%">
        <source src="static/videos/kamala.mp4" type="video/mp4"/>
      </video>
      <h2>
        Our method can apply semantic manipulations to real facial videos without requiring any temporal components.
      </h2>
    </p>
  </div>
</section>

<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel-face" class="carousel results-carousel">-->
<!--        <div class="item item-car1-vid-1">-->
<!--          <div class="carousel-content">-->
<!--                  <video id="carousel_1_vid_1" autoplay controls muted loop height="100%">-->
<!--                    <source src="static/videos/obama.mp4"-->
<!--                            type="video/mp4">-->
<!--                  </video>-->
<!--          </div>-->
<!--        </div>-->
<!--        <div class="item item-car1-vid-1">-->
<!--          <div class="carousel-content">-->
<!--                  <video id="carousel_1_vid_2" autoplay controls muted loop height="100%">-->
<!--                    <source src="static/videos/emma.mp4"-->
<!--                            type="video/mp4">-->
<!--                  </video>-->
<!--          </div>-->
<!--        </div>-->
<!--        <div class="item item-car1-vid-2">-->
<!--          <div class="carousel-content">-->
<!--                  <video id="carousel_1_vid_3" autoplay controls muted loop height="100%">-->
<!--                    <source src="static/videos/kamala.mp4"-->
<!--                            type="video/mp4">-->
<!--                  </video>-->
<!--          </div>-->
<!--        </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->

<section>
      
</section>


<section>
  <div>
    <div>
    <div>
        <h2>Don&#39;t have time? Skip to the <a href="#tldr">tl;dr</a></h2>
    </div>
    </div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
          The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing.
          However, replicating their success with videos has proven challenging. Sets of high-quality facial videos are lacking, and working with videos introduces a fundamental barrier to overcome - temporal coherency.
          We propose that this barrier is largely artificial. The source video is already temporally coherent, and deviations from this state arise in part due to careless treatment of individual components in the editing pipeline.
          We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low frequency functions, and demonstrate that they provide a strongly consistent prior.
          We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art.
          Our method produces meaningful face manipulations, maintains a higher degree of temporal consistency, and can be applied to challenging, high quality, talking head videos which current methods struggle with.
        </p>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section>
  <div>
    <div>
        <p>
          <h2>So, what&#39;s the gist?</h2>
        </p>
        <div>
          <div>
            <p>
                  <video id="pipeline" autoplay="" controls="" muted="" loop="" height="100%">
                    <source src="static/videos/pipeline.mp4" type="video/mp4"/>
                  </video>
                <h2>Video outputs at different stages of our editing pipeline</h2>
            </p>
          </div>
        </div>
        <p>
          We&#39;re using the same StyleGAN image-based editing techniques you all know and love, and we&#39;re doing it at the frame level - without any components that try to enforce temporal consistency!
          How come it&#39;s so smooth? Why is there so little jitter? The answer is simple - the original video is already temporally consistent.
          Instead of using the wrong tools, ruining consistency and working hard to restore it, we analyze the different components of a GAN editing pipeline, determine which ones are consistent, and just use those!
        </p>
        </div>
    </div>
  
  </section>

<section>
  <div>
    <div>
        <p>
          <h2>What does it look like, compared to the alternatives?</h2>
        </p>
        <p>
          Even without optical-flow or other temporal components, our method can tackle complex scenes with motion that destabilizes the state-of-the-art.
        </p>
      </div>
  </div>
</section>

<section>
      <div>
        <div>
          <div>
            <p>
              <video id="domain_1" autoplay="" controls="" loop="" muted="" height="100%">
               <source src="static/videos/domain.mp4" type="video/mp4"/>
              </video>
              <h2>Our method can be applied not only to real videos, but also to animated media! </h2>
            </p>
          </div>
        </div>
      </div>
</section>

<section>
  
</section>

<section id="BibTeX">
  <div>
    <h2>BibTeX</h2>
    <p>If you find our work useful, please cite our paper:</p>
    <pre><code>@misc{tzaban2022stitch,
      title={Stitch it in Time: GAN-Based Facial Editing of Real Videos},
      author={Rotem Tzaban and Ron Mokady and Rinon Gal and Amit H. Bermano and Daniel Cohen-Or},
      year={2022},
      eprint={2201.08361},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre>
  </div>
</section>




</div>
  </body>
</html>
