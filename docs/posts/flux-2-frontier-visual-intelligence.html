<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bfl.ai/blog/flux-2">Original</a>
    <h1>FLUX.2: Frontier Visual Intelligence</h1>
    
    <div id="readability-page-1" class="page"><div><p>FLUX.2 is designed for real-world creative workflows, not just demos or party tricks. It generates high-quality images while maintaining character and style consistency across multiple reference images, following structured prompts, reading and writing complex text, adhering to brand guidelines, and reliably handling lighting, layouts, and logos. FLUX.2 can edit images at up to 4 megapixels while preserving detail and coherence.</p><h2><strong>Black Forest Labs: Open Core</strong></h2><p>We believe visual intelligence should be shaped by researchers, creatives, and developers everywhere, not just a few. That’s why we pair frontier capability with open research and open innovation, releasing powerful, inspectable, and composable open-weight models for the community, alongside robust, production-ready endpoints for teams that need scale, reliability, and customization.</p><p>When we launched Black Forest Labs in 2024, we set out to make open innovation sustainable, building on our experience developing some of the world’s most popular open models. We’ve combined open models like FLUX.1 [dev]—<a target="_blank" rel="noindex nofollow" href="https://huggingface.co/models?sort=likes">the most popular open image model globally</a>—with professional-grade models like FLUX.1 Kontext [pro], which powers teams from Adobe to Meta and beyond. Our open core approach drives experimentation, invites scrutiny, lowers costs, and ensures that we can keep sharing open technology from the Black Forest and the Bay into the world.</p><h2><strong>From FLUX.1 to FLUX.2</strong></h2><p>Precision, efficiency, control, extreme realism - where FLUX.1 showed the potential of media models as powerful creative tools, FLUX.2 shows how frontier capability can transform production workflows. By radically changing the economics of generation, FLUX.2 will become an indispensable part of our creative infrastructure.</p><p><img alt="" draggable="false" loading="lazy" width="1578" height="800" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3d8b43142639897e0f0e4a5c073ad7202c2c2fea-1578x800.jpg&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3d8b43142639897e0f0e4a5c073ad7202c2c2fea-1578x800.jpg&amp;w=3840&amp;q=75 2x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3d8b43142639897e0f0e4a5c073ad7202c2c2fea-1578x800.jpg&amp;w=3840&amp;q=75"/></p><p><em><strong>Output Versatility</strong>: FLUX.2 is capable of generating highly detailed, photoreal images along with infographics with complex typography, all at resolutions up to 4MP</em></p><h2><strong>What’s New</strong></h2><ul><li><span><strong>Multi-Reference Support</strong>: Reference up to 10 images simultaneously with the best character / product / style consistency available today.</span></li><li><span><strong>Image Detail &amp; Photorealism</strong>: Greater detail, sharper textures, and more stable lighting suitable for product shots, visualization, and photography-like use cases.</span></li><li><span><strong>Text Rendering</strong>: Complex typography, infographics, memes and UI mockups with legible fine text now work reliably in production.</span></li><li><span><strong>Enhanced Prompt Following</strong>: Improved adherence to complex, structured instructions, including multi-part prompts and compositional constraints.</span></li><li><span><strong>World Knowledge</strong>: Significantly more grounded in real-world knowledge, lighting, and spatial logic, resulting in more coherent scenes with expected behavior.</span></li><li><span><strong>Higher Resolution &amp; Flexible Input/Output Ratios:</strong> Image editing on resolutions up to 4MP.</span></li></ul><p><img alt="" draggable="false" loading="lazy" width="3721" height="2798" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798.png&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798.png&amp;w=3840&amp;q=75"/></p><p><em>All variants of FLUX.2 offer image editing from text and multiple references in one model.</em></p><h2><strong>Available Now</strong></h2><p>The FLUX.2 family covers a spectrum of model products, from fully managed, production-ready APIs to open-weight checkpoints developers can run themselves. The overview graph below shows how FLUX.2 [pro], FLUX.2 [flex], FLUX.2 [dev], and FLUX.2 [klein] balance performance, and control</p><ul><li><span><strong>FLUX.2 [pro]:</strong> State-of-the-art image quality that rivals the best closed models, matching other models for prompt adherence and visual fidelity while generating images faster and at lower cost. No compromise between speed and quality. → Available now at <a target="_blank" rel="noindex nofollow" href="http://bfl.ai/play">BFL Playground</a>, the <a target="_blank" rel="noindex nofollow" href="http://docs.bfl.ai/flux_2/">BFL API</a> and via our launch partners.</span></li><li><span><strong>FLUX.2 [flex]</strong>: Take control over model parameters such as the number of steps and the guidance scale, giving developers full control over quality, prompt adherence and speed. This model excels at rendering text and fine details. → Available now at <a target="_blank" rel="noindex nofollow" href="http://bfl.ai/play">bfl.ai/play</a> , the <a target="_blank" rel="noindex nofollow" href="http://docs.bfl.ai/flux_2/">BFL API</a> and via our launch partners.</span></li><li><span><strong>FLUX.2 [dev]:</strong> 32B open-weight model, derived from the FLUX.2 base model. The most powerful open-weight image generation and editing model available today, combining text-to-image synthesis and image editing with multiple input images in a single checkpoint. FLUX.2 [dev] weights are available on <a target="_blank" rel="noindex nofollow" href="https://huggingface.co/black-forest-labs/FLUX.2-dev">Hugging Face</a> and can now be used locally using our <a target="_blank" rel="noindex nofollow" href="https://github.com/black-forest-labs/flux2">reference inference code</a>. On consumer grade GPUs like GeForce RTX GPUs you can use an optimized fp8 reference implementation of FLUX.2 [dev], created in collaboration with <a target="_blank" rel="noindex nofollow" href="https://blogs.nvidia.com/blog/rtx-ai-garage-flux.2-comfyui">NVIDIA</a> and <a target="_blank" rel="noindex nofollow" href="https://blog.comfy.org/p/flux2-state-of-the-art-visual-intelligence">ComfyUI</a>. You can also sample Flux.2 [dev] via API endpoints on <a target="_blank" rel="noindex nofollow" href="https://fal.ai/models/fal-ai/flux-2/">FAL</a>, <a target="_blank" rel="noindex nofollow" href="https://replicate.com/black-forest-labs/flux-2-dev">Replicate</a>, <a target="_blank" rel="noindex nofollow" href="https://runware.ai/models#image-flux">Runware</a>, <a target="_blank" rel="noindex nofollow" href="https://verda.com/managed-endpoints/flux-2">Verda</a>, <a target="_blank" rel="noindex nofollow" href="http://www.together.ai/models/flux-2-dev">TogetherAI</a>, <a target="_blank" rel="noindex nofollow" href="https://blog.cloudflare.com//flux-2-workers-ai">Cloudflare</a>, <a target="_blank" rel="noindex nofollow" href="https://deepinfra.com/black-forest-labs/FLUX-2-dev">DeepInfra</a>. For a commercial license, visit our <a target="_blank" rel="noindex nofollow" href="https://bfl.ai/licensing">website</a>.</span></li><li><span><strong>FLUX.2 [klein] (<em>coming soon</em>): </strong>Open-source, Apache 2.0 model, size-distilled from the FLUX.2 base model. More powerful &amp; developer-friendly than comparable models of the same size trained from scratch, with many of the same capabilities as its teacher model. <a target="_blank" rel="noindex nofollow" href="https://docs.google.com/forms/d/e/1FAIpQLScOIvOkHN2fPbD8cFsAf7MQJfqu2bnEmoNb0x1k3ismTLLm-Q/viewform">Join the beta</a></span></li><li><span><strong>FLUX.2 - VAE:</strong> A new variational autoencoder for latent representations that provide an optimized trade-off between learnability, quality and compression rate. This model provides the foundation for all FLUX.2 flow backbones, and an in-depth report describing its technical properties is available <a target="_blank" rel="noindex nofollow" href="https://bfl.ai/research/representation-comparison">here</a>. <a target="_blank" rel="noindex nofollow" href="https://huggingface.co/black-forest-labs/FLUX.2-dev">The FLUX.2 - VAE is available on HF under an Apache 2.0 license</a>.</span></li></ul><p><img alt="" draggable="false" loading="lazy" width="3007" height="1690" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F7df1d6ae0febf5777b91f2520ffe58abb18add99-3007x1690.jpg&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F7df1d6ae0febf5777b91f2520ffe58abb18add99-3007x1690.jpg&amp;w=3840&amp;q=75"/></p><p><img alt="" draggable="false" loading="lazy" width="3626" height="704" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3e6970864309fcba9e66a189f9a4e2d1edb25922-3626x704.jpg&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3e6970864309fcba9e66a189f9a4e2d1edb25922-3626x704.jpg&amp;w=3840&amp;q=75"/></p><p><em><strong>Generating designs with variable steps: </strong>FLUX.2 [flex] provides a “steps” parameter, trading off typography accuracy and latency. From left to right: 6 steps, 20 steps, 50 steps.</em></p><p><img alt="" draggable="false" loading="lazy" width="3840" height="768" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F2952776af632e4c98278d36c19dd82fb7a88e16c-3840x768.jpg&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F2952776af632e4c98278d36c19dd82fb7a88e16c-3840x768.jpg&amp;w=3840&amp;q=75"/></p><p><em><strong>Controlling image detail with variable steps: </strong>FLUX.2 [flex] provides a “steps” parameter, trading off image detail and latency. From left to right: 6 steps, 20 steps, 50 steps.</em></p><p><img alt="" draggable="false" loading="lazy" width="5000" height="3750" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F9ce64429276aac68efa5bbf66e584bb6fc080f4c-5000x3750.png&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F9ce64429276aac68efa5bbf66e584bb6fc080f4c-5000x3750.png&amp;w=3840&amp;q=75"/></p><p><img alt="" draggable="false" loading="lazy" width="5000" height="2500" decoding="async" data-nimg="1" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3742dec4aa779c98c92e4baf81e60b1959498f02-5000x2500.png&amp;w=3840&amp;q=75 1x" src="http://tinylogger.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F2gpum2i6%2Fproduction%2F3742dec4aa779c98c92e4baf81e60b1959498f02-5000x2500.png&amp;w=3840&amp;q=75"/></p><p>The FLUX.2 model family delivers state-of-the-art image generation quality at extremely competitive prices, offering the best value across performance tiers.</p><p>For open-weights image models, FLUX.2 [dev] sets a new standard, achieving leading performance across text-to-image generation, single-reference editing, and multi-reference editing, consistently outperforming all open-weights alternatives by a significant margin.</p><p>Whether open or closed, we are committed to the <a target="_blank" rel="noindex nofollow" href="https://huggingface.co/black-forest-labs/FLUX.2-dev">responsible development </a>of these models and services before, during, and after every release.</p><h2><strong>How It Works</strong></h2><p>FLUX.2 builds on a latent flow matching architecture, and combines image generation and editing in a single architecture. The model couples the <a target="_blank" rel="noindex nofollow" href="https://docs.mistral.ai/models/mistral-small-3-2-25-06">Mistral-3 24B parameter vision-language model </a>with a rectified flow transformer. The VLM brings real world knowledge and contextual understanding, while the transformer captures spatial relationships, material properties, and compositional logic that earlier architectures could not render.</p><p>FLUX.2 now provides multi-reference support, with the ability to combine up to 10 images into a novel output, an output resolution of up to 4MP, substantially better prompt adherence and world knowledge, and significantly improved typography. We re-trained the model’s latent space from scratch to achieve better learnability and higher image quality at the same time, a step towards solving the “Learnability-Quality-Compression” trilemma. Technical details can be found in the <a target="_blank" rel="noindex nofollow" href="https://bfl.ai/research/representation-comparison">FLUX.2 VAE blog post</a>.</p><h2><strong>More Resources:</strong></h2><ul><li><span><a target="_blank" rel="noindex nofollow" href="http://docs.bfl.ai/flux_2/">FLUX.2 Documentation</a></span></li><li><span><a target="_blank" rel="noindex nofollow" href="http://docs.bfl.ai/guides/prompting_guide_flux2">FLUX.2 Prompting Guide</a></span></li><li><span><a target="_blank" rel="noindex nofollow" href="https://github.com/black-forest-labs/flux2">FLUX.2 Open Weights / Inference Code</a></span></li><li><span><a target="_blank" rel="noindex nofollow" href="https://playground.bfl.ai">FLUX Playground</a></span></li></ul><h2><strong>Into the New</strong></h2><p>We&#39;re building foundational infrastructure for visual intelligence, technology that transforms how the world is seen and understood. FLUX.2 is a step closer to multimodal models that unify perception, generation, memory, and reasoning, in an open and transparent way.</p><p>Join us on this journey. We&#39;re hiring in Freiburg (HQ) and San Francisco. <a target="_blank" rel="noindex nofollow" href="https://bfl.ai/careers"><strong>View open roles</strong></a>.</p></div></div>
  </body>
</html>
