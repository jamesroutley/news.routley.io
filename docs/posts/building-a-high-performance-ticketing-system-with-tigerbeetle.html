<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://renerocks.ai/blog/2025-11-02--tigerfans/">Original</a>
    <h1>Building a high-performance ticketing system with TigerBeetle</h1>
    
    <div id="readability-page-1" class="page"><div id="docs">
    <div id="post-body"><p><strong>A journey from simple curiosity to 977 tickets per second</strong></p><figure><img src="https://renerocks.ai/demo.gif"/>
<figcaption>TigerFans demo showing ticket checkout and payment flow</figcaption></figure><h2>What Started Everything</h2><p><strong>‚Äù<a href="https://x.com/jorandirkgreef/status/1959268005407252929" target="_blank">Too easy: TigerBeetle.</a>‚Äù</strong></p><p>That was Joran Dirk Greef‚Äôs response when someone on Twitter asked how you‚Äôd build a ticketing solution for an Oasis-scale concert‚Äîhundreds of thousands of people flooding your website at once, where you need to guarantee no ticket gets sold twice and everyone who pays gets a ticket. Joran is the CEO and founder of TigerBeetle.</p><p>He was right. Everyone who knows TigerBeetle would give the same advice. TigerBeetle is a financial transactions database designed for exactly this kind of problem: counting resources with absolute correctness under extreme load.</p><p>But I wanted to understand the concrete implementation. Not just conceptually‚ÄîI needed to see the actual code.</p><p>How do you model ticket transactions as financial transactions? What does the account structure look like? How do the transfers flow through a realistic booking system with payment providers? What about pending reservations that timeout? What about idempotency when webhooks retry?</p><p>The best way to learn: build it myself.</p><p>So I built it. Three days later, I had a working demo. What started as an educational project became a 19-day optimization journey that pushed the system to 977 ticket reservations per second‚Äî15x faster than the Oasis baseline. All in Python.</p><h2>Building It: The HOW</h2><p>The goal was clear: build a working demo that answers the HOW question. Not a toy example‚Äîa realistic booking flow with all the messy complexities of real payment systems.</p><p>The first challenge: TigerBeetle isn‚Äôt a general-purpose database. It‚Äôs a financial transactions database. That means it forces you to think in double-entry accounting primitives‚Äîaccounts, transfers, debits, and credits.</p><p>So the question became: how do you model tickets as financial transactions?</p><p>Think about how banks handle money. They use double-entry bookkeeping‚Äîa system that‚Äôs been used for hundreds of years because it provides built-in error detection and perfect audit trails. Every transaction affects at least two accounts: one is debited, another is credited. Debits always equal credits, so the system is always balanced and errors are immediately obvious.</p><p>TigerBeetle demanded we do the same with tickets.</p><p>For each resource‚ÄîClass A tickets, Class B tickets, those limited-edition t-shirts‚Äîwe set up three TigerBeetle accounts: an Operator account that holds all available inventory, a Budget account that represents what‚Äôs available to sell, and a Spent account for consumed inventory.</p><p>First, we initialize by funding the Budget account from the Operator account:</p><p><img src="https://renerocks.ai/THE_WRITEUP_1.png"/></p><p>When someone starts checkout, we create a pending transfer from Budget to Spent with a five-minute timeout. The crucial part: TigerBeetle‚Äôs <code>DEBITS_MUST_NOT_EXCEED_CREDITS</code> constraint flag. This makes overselling architecturally impossible‚Äînot prevented through careful programming, but impossible by design. The database enforces correctness.</p><p><img src="https://renerocks.ai/THE_WRITEUP_2.png"/></p><p>When payment succeeds, we post the pending transfer to make it permanent:</p><p><img src="https://renerocks.ai/THE_WRITEUP_3.png"/></p><p>When payment fails or times out, we void the transfer. It just vanishes. No cleanup jobs, no race conditions, perfect audit trail.</p><p>The demo stack was deliberately simple. FastAPI‚Äîthe Python async web framework‚Äîbecause it‚Äôs quick to build and easy to understand. SQLite because that‚Äôs one less process to manage. TigerBeetle in dev mode. And MockPay‚Äîa simulated payment provider that mimicked the real webhook flow you‚Äôd get from Stripe or similar.</p><p>I wrote the code, built the UI, documented the account model. Everything worked. The two-phase checkout flow with webhooks, the pending reservations that auto-expire, the whole thing.</p><p>You can try the live demo yourself at <a href="https://tigerfans.io" target="_blank">tigerfans.io</a> ‚Äî it‚Äôs still running, complete with the MockPay simulator.</p><p>Mission accomplished. I understood the HOW.</p><h2>Reaching Out to TigerBeetle</h2><p>Before sharing the demo with the world, I wanted to show it to Joran and the TigerBeetle team first. The email I sent was careful:</p><blockquote><p>‚ÄúBefore sharing it with the world, (posting about it), I wanted to show it to you. I don‚Äôt want to spread ‚Äòanti-patterns‚Äô.‚Äù</p></blockquote><p>After all, I had made it all up. Maybe there is a better, more accounting-like, way of modeling tickets with TigerBeetle. I didn‚Äôt want my educational project become a bad example.</p><p>Joran was traveling, stuck dealing with flight delays somewhere, but he took the time to reply. The response was warm and encouraging. Rafael Batiati, one of TigerBeetle‚Äôs core developers, joined in with a note of caution: people would inevitably start benchmarking it once released. Oh. Right. Thinking about it, they probably would.</p><p>But then Joran turned it into a friendly challenge. He mentioned the Oasis ticket sale‚Äîapproximately 1.4 million tickets over six hours, or roughly 65 tickets per second. Then came the kicker: <em>‚ÄúIt would be pretty sweet if you could do better than six hours.‚Äù</em></p><p>That number‚Äî65 tickets per second‚Äîbecame my new baseline.</p><h2>Performance Journey</h2><p>What started as an educational demo shifted into something else entirely. The patterns were proven. The implementation was correct. But now a different question emerged: how many hidden inefficiencies could we eliminate?</p><p>Not the fundamental stuff‚ÄîPython is Python, and there‚Äôs only so fast an interpreted language can go. But the bottlenecks we could actually fix. The architectural inefficiencies. How well could we really utilize TigerBeetle?</p><figure><img src="https://renerocks.ai/progression.svg"/>
<figcaption>Performance progression for ticket reservations from 115 to 977 ops/s</figcaption></figure><hr/><h3>Initial Infrastructure Upgrades</h3><p>To prepare for performance testing, I needed to eliminate database bottlenecks. SQLite‚Äôs blocking I/O would serialize request handling in FastAPI‚Äôs event loop, while PostgreSQL‚Äôs async drivers would allow true concurrent processing. So I switched from SQLite to PostgreSQL.</p><p>This meant upgrading the infrastructure‚Äîfrom a tiny 2-vCPU spot instance to a proper c7g.xlarge EC2 machine with 4 vCPUs and 8GB of RAM. PostgreSQL runs as its own process, so I wanted one vCPU for the OS, one for the HTTP workers, one for the database‚Äîwith room to experiment with multiple workers later.</p><p>I also optimized the SQL queries, tuned transaction handling, and threw in uvloop for a faster event loop to have a good baseline for performance tests.</p><p>The result: 115 tickets per second. Better than the Oasis baseline.</p><p>I sent the numbers to Joran with a bit of playful humor: <em>‚ÄúWe already redefined TPS as tickets per second. Why not redefine big O notation as big Oasis notation? O(1) = 65 TPS. We‚Äôre currently at ~O(1.7).‚Äù</em></p><p>Joran‚Äôs response was encouraging as always, but also included a reality check: <em>‚ÄúI was surprised the TPS is so low. It should be at least approximately 10K.‚Äù</em></p><p>Ten thousand. We were at 115.</p><p>TigerBeetle is famous for its &gt;1000X performance. It‚Äôs built for this. So why was the whole system so sluggish?</p><hr/><h3>Understanding the Bottleneck</h3><p>I sat down and drew out the complete sequence diagram. Every API request, every database roundtrip, every operation. The exercise was revealing:</p><p><strong>Checkout flow:</strong></p><ol><li>Browser ‚Üí Server (ticket class, email)</li><li>Server ‚Üí TigerBeetle: Create PENDING transfers</li><li>Server ‚Üí PostgreSQL: INSERT Order + PaymentSession</li><li>Server ‚Üí Browser: Redirect to MockPay</li></ol><p><strong>Webhook flow (payment confirmation):</strong></p><ol><li>MockPay ‚Üí Server: Payment succeeded</li><li>Server ‚Üí PostgreSQL: Check idempotency</li><li>Server ‚Üí TigerBeetle: POST pending transfers</li><li>Server ‚Üí PostgreSQL: BEGIN TX, INSERT idempotency keys, UPDATE order, COMMIT TX</li></ol><p>Every single API request was hitting PostgreSQL 2-4 times. PostgreSQL was in the critical path. Always.</p><p>To understand the bottleneck better, I split the measurements into two phases:</p><ul><li><strong>Phase 1</strong>: Checkout/Reservations (create holds, save sessions)</li><li><strong>Phase 2</strong>: Payment Confirmations/Webhooks (commit/cancel, persist orders)</li></ul><p>The results confirmed the problem:</p><ul><li><strong>Phase 1 (Reservations)</strong>: ~150 ops/s</li><li><strong>Phase 2 (Webhooks)</strong>: ~130 ops/s</li></ul><p>PostgreSQL was slow in BOTH phases. It was the bottleneck everywhere.</p><hr/><h3>The Redis Experiment</h3><p>After seeing PostgreSQL as the bottleneck in both phases, I started questioning our architecture. Do we even need a relational database when we don‚Äôt really do anything relational in it? We‚Äôre basically just storing orders and idempotency keys.</p><p>I implemented Redis as a complete DATABASE_URL replacement. The system now supported three swappable backends: SQLite, PostgreSQL, or Redis. It was the same interface, just different storage. I replaced ALL of PostgreSQL with Redis‚Äînot just sessions, but everything.</p><p>I ran the benchmarks with Redis in everysec fsync mode‚Äîbalancing performance with some durability.</p><p>The results were impressive:</p><ul><li><strong>Phase 1 (Reservations)</strong>: <strong>930 ops/s</strong> (6x improvement!)</li><li><strong>Phase 2 (Webhooks)</strong>: <strong>450 ops/s</strong> (3.4x improvement!)</li></ul><p>The numbers were exciting. But there was a problem: Redis in everysec mode could lose up to 1 second of orders on crash. The faster Redis gets, the worse this becomes. I had replaced ALL of PostgreSQL with Redis, including the durable orders. That‚Äôs not acceptable, even for a demo.</p><p>I sent the impressive benchmark results (and the durability concern) to Rafael.</p><hr/><h3>Rafael‚Äôs Hot/Cold Path Compromise</h3><p>Rafael‚Äôs response brought the architectural insight that would transform the system. He appreciated the detailed benchmarks but cautioned against gambling with durability even for a demo. His insight: separate ephemeral session data from durable order records.</p><p>This was the hot/cold path insight‚Äîa compromise between my speed experiment and proper durability. Instead of replacing ALL of PostgreSQL with Redis (which sacrificed durability), use Redis ONLY for payment sessions (hot path), NOT for orders (cold path).</p><p>Payment sessions are ephemeral. They only matter for a few minutes while the user is paying. Once payment succeeds or fails, we don‚Äôt need the session anymore. Same with idempotency keys‚Äîtemporary deduplication data that prevents double-charging when webhooks retry.</p><p>But orders? Those need to be durable forever. We need PostgreSQL for that.</p><p>The insight was brilliant: not all data needs immediate durability!</p><p>What a great idea!</p><p>Any crash or abandoned cart would eventually get reverted by TigerBeetle‚Äôs timeout mechanism. In a real application, if a webhook callback is not found in Redis or already expired in TigerBeetle, you‚Äôd reverse the payment with the payment gateway.</p><p>The architecture clicked into place:</p><ul><li><strong>Hot path (reservations)</strong>: TigerBeetle + Redis (in-memory, fast)</li><li><strong>Cold path (confirmed payments)</strong>: TigerBeetle + PostgreSQL (durable, slower)</li></ul><hr/><h3>Correct Hot/Cold Implementation</h3><p>I rebuilt the system with the correct separation: Redis for payment sessions, TigerBeetle for accounting, PostgreSQL for durable orders.</p><p>The hot path became truly hot‚ÄîRedis and TigerBeetle handling every request. PostgreSQL only gets written to when payment actually succeeds. Failed checkouts never touch the database at all.</p><p><img src="https://renerocks.ai/THE_WRITEUP_4.png"/></p><p>The impact was immediate. Throughput jumped to <strong>865 tickets per second</strong>. Moving PostgreSQL out of the critical path unlocked massive performance gains.</p><p>This was the working hot/cold architecture‚Äîfast where it needed to be fast, durable where it needed to be durable.</p><hr/><h3>Three Configuration Levels</h3><p>With the hot/cold architecture working, Joran suggested something interesting: could we show ‚Äúthe before and after‚Äù? What if we used PostgreSQL for everything? What if we were smart and just added Redis? People would want to see what TigerBeetle actually makes possible.</p><p>He was right. The progression itself would tell the story‚Äîshowing exactly where the performance gains came from, while also checking our assumptions about what was actually slow. As Mark Twain said: <em>‚ÄúIt isn‚Äôt what you don‚Äôt know that gets you; it‚Äôs what you know for sure that just ain‚Äôt so.‚Äù</em> <strong>Unchecked assumptions are silent killers.</strong></p><p>This required restructuring the code to support swappable backends, allowing fair comparisons between configurations.</p><p>I built three distinct configurations:</p><ul><li><strong>Level 1 (PG Only)</strong>: PostgreSQL for everything‚Äîsessions, accounting, orders</li><li><strong>Level 2 (PG + Redis)</strong>: Redis for sessions, PostgreSQL for accounting and orders</li><li><strong>Level 3 (TB + Redis)</strong>: Redis for sessions, TigerBeetle for accounting, PostgreSQL for orders</li></ul><p>The refactoring itself cleaned up some inefficiencies‚ÄîLevel 3 maintained its strong performance.</p><p>The results told a clear story:</p><ul><li><strong>Level 1</strong>: 175 ops/s (reservations), 34 ops/s (webhooks)</li><li><strong>Level 2</strong>: 263 ops/s (1.5x better), 245 ops/s (7.3x better!)</li><li><strong>Level 3</strong>: 900 ops/s (5.1x vs L1), 313 ops/s (9.3x vs L1)</li></ul><p>TigerBeetle speeds up everything.</p><hr/><h3>Comprehensive Testing Infrastructure</h3><p>With the 3-level comparison structure in place, I built comprehensive testing infrastructure to understand exactly where time was being spent.</p><p>I had already split measurements into two phases earlier (during the bottleneck investigation):</p><ul><li><strong>Phase 1</strong>: Checkout/Reservations (create holds, save sessions)</li><li><strong>Phase 2</strong>: Payment Confirmations/Webhooks (commit/cancel, persist orders)</li></ul><p>Now I added isolated endpoints that measured just the core TigerBeetle operations: raw checkout (just the reservation accounting), raw webhook (just the commit accounting). Strip away everything else and see what TigerBeetle itself could do.</p><p>But I wanted more. The HTTP request timings were real-world relevant, but they didn‚Äôt show POTENTIAL. If, hypothetically, PostgreSQL took 4ms, Redis 2ms, TigerBeetle 1ms, and you had 2 operations per request, you‚Äôd see a 2x speedup when switching from PostgreSQL+Redis to TigerBeetle+Redis‚Äîwhen TigerBeetle was actually 4x faster. The Redis offset would hide the true speed in this hypothetical.</p><p>The solution: instrument transaction times inside the server. Query them after tests complete. Compare actual operation times, not just request-to-response latency.</p><p>Then I built TigerBench‚Äîan interactive visualization showing the complete progression across three configurations:</p><ul><li><strong>Level 1 (PG Only)</strong>: PostgreSQL for everything‚Äîsessions, accounting, orders</li><li><strong>Level 2 (PG + Redis)</strong>: Redis for sessions, PostgreSQL for accounting and orders</li><li><strong>Level 3 (TB + Redis)</strong>: Redis for sessions, TigerBeetle for accounting, PostgreSQL for orders</li></ul><p>The page shows both phases, both isolated operations, all the timings broken down by component. You can see exactly where the time goes in each configuration.</p><figure><img src="https://renerocks.ai/tigerbench.png"/>
<figcaption>TigerBench visualization showing performance comparison across three configurations</figcaption></figure><p>You can explore it yourself: <a href="https://tigerfans.io/bench" target="_blank">tigerfans.io/bench</a></p><hr/><h3>Discovering the Batching Problem</h3><p>Then I added more instrumentation to see what was actually happening inside TigerBeetle calls. The numbers told a strange story: we were sending TigerBeetle batches of size 1.</p><p>One transfer per request. Every single time.</p><p>TigerBeetle has an interface created for batching. Yet I was using it with many concurrent requests, each creating a batch size of 1.</p><p>But TigerBeetle can handle 8190 operations per request. That‚Äôs where its performance shines. Yet FastAPI‚Äôs request-oriented design means every <code>await</code> fires off immediately, creating an interface impedance mismatch. We‚Äôre flying a 747 to deliver individual passengers.</p><p>The solution was a custom batching layer‚Äîthe LiveBatcher. It sits between the application and TigerBeetle, collecting concurrent requests as they arrive and packing them into efficient batches. While a batch is being processed, new requests queue up. When processing completes, queued requests are immediately packed and sent, continuously chaining to keep the pipeline full.</p><p><img src="https://renerocks.ai/THE_WRITEUP_5.png"/></p><p>Result: batch sizes averaging 5-6 transfers. Throughput jumped to 977 reservations per second‚Äî15x faster than the Oasis baseline.</p><p>I added the batching metrics to TigerBench too‚Äîyou can see the batch size distributions across different concurrency levels, watch how batching efficiency changes the performance curve.</p><figure><img src="https://renerocks.ai/tb.batch_size_line.png"/>
<figcaption>Line chart showing TigerBeetle batch size distribution across concurrency levels</figcaption></figure><hr/><h3>When More Is Less</h3><p>Then came the most counter-intuitive discovery of the entire project.</p><p>For this test, I upgraded to c7g.2xlarge (8 vCPU, 16 GB RAM).</p><p>I tried running with multiple workers. Standard practice, right? Eight CPUs, so run multiple workers. Utilize all the cores.</p><p>The hypothesis: more workers = more throughput?</p><p><strong>Result</strong>: <strong>NO</strong>.</p><p>Testing 1,000 reservations:</p><ul><li><strong>1 worker</strong>: 977 ops/s, average batch size 5.3</li><li><strong>2 workers</strong>: 966 ops/s (1% slower), average batch size 3.9</li><li><strong>3 workers</strong>: 770 ops/s (21% slower!), average batch size 2.9</li></ul><p>The measurements were clear, but they made no sense. Until I understood what was happening.</p><p>Multiple workers fragment batches across event loops. The load balancer sends request 1 to worker 1, request 2 to worker 2, request 3 to worker 3. Each worker‚Äôs batcher only sees a fraction of the concurrent load. Smaller batches. And since TigerBeetle processes those batches sequentially anyway, the fragmentation doesn‚Äôt buy us parallelism‚Äîjust overhead.</p><p>When batching efficiency is critical, consolidation beats parallelism. It‚Äôs Amdahl‚Äôs Law in action.</p><figure><img src="https://renerocks.ai/workers.svg"/>
<figcaption>Single worker vs multi-worker batch fragmentation</figcaption></figure><hr/><h3>Final Performance Results</h3><p>After weeks of iteration and measurement, the system hit 977 ticket reservations per second. Fifteen times faster than the Oasis baseline. All in Python.</p><p>Median latency: 11 milliseconds. Even at the 99th percentile, requests completed in 26 milliseconds. The batch sizes averaged 5-6 transfers‚Äînot close to TigerBeetle‚Äôs theoretical optimum, but good enough to unlock real performance.</p><p>The limiting factor was clear: Python‚Äôs event loop overhead, about 5 milliseconds per request. That‚Äôs 45% of the total time. Even with infinitely fast TigerBeetle, you can‚Äôt get much faster without ditching Python.</p><p>But that was the point. If it works this well with Python‚Äôs overhead, the architecture is sound.</p><hr/><h2>TigerBeetle Ticket Challenge</h2><p>The recipe is proven. This implementation‚Äîin Python, with all its overhead‚Äîachieves 977 reservations per second. The architecture is documented, the patterns are explained, the lessons are captured.</p><p>Imagine this same architecture in <strong>Go</strong>, where removing Python‚Äôs 5ms overhead could yield <strong>10-30x better throughput</strong>. Or in <strong>Zig</strong>, where manual optimization might push it to <strong>50-100x faster</strong>.</p><p><strong>The TTC challenge is simple</strong>: Build your version, any language, any stack, and share your results. Let‚Äôs see how fast ticketing can be when TigerBeetle‚Äôs batch-oriented design meets systems programming languages.</p><p><strong>Resources available</strong>:</p><ul><li>Live demo: <a href="https://tigerfans.io" target="_blank">tigerfans.io</a></li><li>TigerBench visualization: <a href="https://tigerfans.io/bench" target="_blank">tigerfans.io/bench</a></li><li>Source code: <a href="https://github.com/renerocksai/tigerfans" target="_blank">github.com/renerocksai/tigerfans</a></li><li>Technical deep-dives on all four patterns <em>(see menu at the end)</em></li><li>Reproducible benchmarks</li></ul><h2>Gratitude</h2><p>Thank you to <strong>Joran Dirk Greef</strong> for creating TigerBeetle, for the ‚Äúbenchmark would be nice‚Äù challenge that started the optimization journey, and for being so encouraging throughout.</p><p>Thank you to <strong>Rafael Batiati</strong> for the crucial hot/cold path compromise, the perfect balance of speed and durability, the deep code reviews, and diving into TigerBeetle‚Äôs Python batching behavior to help unlock the final performance tier.</p><p>Thank you to the entire <strong>TigerBeetle team</strong> for building an amazing database and being so generous with their knowledge.</p><p>As the final commit message said:</p><p><strong>‚ÄúI had the time of my life working on this üòä!‚Äù</strong></p><blockquote><p><strong>Sometimes the best projects aren‚Äôt the ones you plan. They‚Äôre the ones that grab you, challenge you, teach you, and refuse to let go until you‚Äôve explored every corner, answered every question, and squeezed every last drop of insight from the problem.</strong></p></blockquote><hr/><h2>Related Documents</h2><p><strong>Overview</strong>: <a href="https://renerocks.ai/projects/tigerfans/">Executive Summary</a></p><p><strong>Technical Details</strong>:</p><ul><li><a href="https://renerocks.ai/projects/tigerfans/DEEPDIVE_RESOURCE_MODELING/">Resource Modeling with Double-Entry Accounting</a></li><li><a href="https://renerocks.ai/projects/tigerfans/DEEPDIVE_HOT_COLD_PATH/">Hot/Cold Path Architecture</a></li><li><a href="https://renerocks.ai/projects/tigerfans/DEEPDIVE_AUTO_BATCHING/">Auto-Batching</a></li><li><a href="https://renerocks.ai/projects/tigerfans/DEEPDIVE_SINGLE_WORKER_PARADOX/">The Single-Worker Paradox</a></li><li><a href="https://renerocks.ai/projects/tigerfans/ANALYSIS_AMDAHL_AND_PLANES/">Amdahl‚Äôs Law Analysis</a></li></ul><p><strong>Resources</strong>:</p><ul><li><a href="https://tigerfans.io" target="_blank">Live Demo</a></li><li><a href="https://tigerfans.io/bench" target="_blank">TigerBench Visualization</a></li><li><a href="https://github.com/renerocksai/tigerfans" target="_blank">Source Code</a></li></ul></div>
  </div></div>
  </body>
</html>
