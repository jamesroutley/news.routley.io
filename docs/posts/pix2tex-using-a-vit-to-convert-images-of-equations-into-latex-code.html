<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lukas-blecher/LaTeX-OCR">Original</a>
    <h1>Pix2tex: Using a ViT to convert images of equations into LaTeX code</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://github.com/lukas-blecher/LaTeX-OCR"><img src="https://camo.githubusercontent.com/e26e90c96a0fee3de5b53a7232dfc0fd0f49f5ea6c9ec1449563dd40aefee496/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6c756b61732d626c65636865722f4c615465582d4f4352" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR"/></a> <a href="https://pix2tex.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/a1e97c43de4d4103a26c25283713f88502716ee63268614479f996d7270d0f48/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f706978327465782f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/pix2tex/badge/?version=latest"/></a> <a href="https://pypi.org/project/pix2tex" rel="nofollow"><img src="https://camo.githubusercontent.com/00f77be6f9cb1f78be8f53d85089b77df578e3e516777c949f781edd45a0e715/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f706978327465783f6c6f676f3d70797069" alt="PyPI" data-canonical-src="https://img.shields.io/pypi/v/pix2tex?logo=pypi"/></a> <a href="https://pypi.org/project/pix2tex" rel="nofollow"><img src="https://camo.githubusercontent.com/8013cfd657c31171c1946c0b6b2b40a1c90d1f7c8d842dde209185d307e922b3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f706978327465783f6c6f676f3d70797069" alt="PyPI - Downloads" data-canonical-src="https://img.shields.io/pypi/dm/pix2tex?logo=pypi"/></a> <a href="https://github.com/lukas-blecher/LaTeX-OCR/releases"><img src="https://camo.githubusercontent.com/a369d922b436161c9e517492585f9f424cba04cfd4d20b7815fed921d02cf3b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6c756b61732d626c65636865722f4c615465582d4f43522f746f74616c3f636f6c6f723d626c7565266c6f676f3d676974687562" alt="GitHub all releases" data-canonical-src="https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;logo=github"/></a> <a href="https://hub.docker.com/r/lukasblecher/pix2tex" rel="nofollow"><img src="https://camo.githubusercontent.com/c1b9289285efdec353acaa08d4f0e55791c3c832f08106b99ce54850ed14df14/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6c756b6173626c65636865722f706978327465783f6c6f676f3d646f636b6572" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker"/></a> <a href="https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a> <a href="https://huggingface.co/spaces/lukbl/LaTeX-OCR" rel="nofollow"><img src="https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" alt="Hugging Face Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>
<p dir="auto">The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png"><img src="https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png" alt="header"/></a></p>
<h2 tabindex="-1" id="user-content-using-the-model" dir="auto"><a href="#using-the-model">Using the model<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">To run the model you need Python 3.7+</p>
<p dir="auto">If you don&#39;t have PyTorch installed. Follow their instructions <a href="https://pytorch.org/get-started/locally/" rel="nofollow">here</a>.</p>
<p dir="auto">Install the package <code>pix2tex</code>:</p>
<div data-snippet-clipboard-copy-content="pip install &#34;pix2tex[gui]&#34;"><pre><code>pip install &#34;pix2tex[gui]&#34;
</code></pre></div>
<p dir="auto">Model checkpoints will be downloaded automatically.</p>
<p dir="auto">There are three ways to get a prediction from an image.</p>
<ol dir="auto">
<li>
<p dir="auto">You can use the command line tool by calling <code>pix2tex</code>. Here you can parse already existing images from the disk and images in your clipboard.</p>
</li>
<li>
<p dir="auto">Thanks to <a href="https://github.com/katie-lim">@katie-lim</a>, you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with <code>latexocr</code>. From here you can take a screenshot and the predicted latex code is rendered using <a href="https://www.mathjax.org/" rel="nofollow">MathJax</a> and copied to your clipboard.</p>
<p dir="auto">Under linux, it is possible to use the GUI with <code>gnome-screenshot</code> (which comes with multiple monitor support) if <code>gnome-screenshot</code> was installed beforehand. For Wayland, <code>grim</code> and <code>slurp</code> will be used when they are both available. Note that <code>gnome-screenshot</code> is not compatible with wlroots-based Wayland compositors. Since <code>gnome-screenshot</code> will be preferred when available, you may have to set the environment variable <code>SCREENSHOT_TOOL</code> to <code>grim</code> in this case (other available values are <code>gnome-screenshot</code> and <code>pil</code>).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif"><img src="https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif" alt="demo" data-animated-image=""/></a></p>
<p dir="auto">If the model is unsure about the what&#39;s in the image it might output a different prediction every time you click &#34;Retry&#34;. With the <code>temperature</code> parameter you can control this behavior (low temperature will produce the same result).</p>
</li>
<li>
<p dir="auto">You can use an API. This has additional dependencies. Install via <code>pip install -U &#34;pix2tex[api]&#34;</code> and run</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pix2tex.api.run"><pre>python -m pix2tex.api.run</pre></div>
<p dir="auto">to start a <a href="https://streamlit.io/" rel="nofollow">Streamlit</a> demo that connects to the API at port 8502. There is also a docker image  available for the API: <a href="https://hub.docker.com/r/lukasblecher/pix2tex" rel="nofollow">https://hub.docker.com/r/lukasblecher/pix2tex</a> <a href="https://hub.docker.com/r/lukasblecher/pix2tex" rel="nofollow"><img src="https://camo.githubusercontent.com/5c27282a4120785ec54bf7b85a9e3dd4da3b73758535c8865a3ea4fd06cc4a4e/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6c756b6173626c65636865722f706978327465783f6c6f676f3d646f636b6572" alt="Docker Image Size (latest by date)" data-canonical-src="https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker"/></a></p>
<div data-snippet-clipboard-copy-content="docker pull lukasblecher/pix2tex:api
docker run --rm -p 8502:8502 lukasblecher/pix2tex:api"><pre><code>docker pull lukasblecher/pix2tex:api
docker run --rm -p 8502:8502 lukasblecher/pix2tex:api
</code></pre></div>
<p dir="auto">To also run the streamlit demo run</p>
<div data-snippet-clipboard-copy-content="docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py"><pre><code>docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py
</code></pre></div>
<p dir="auto">and navigate to <a href="http://localhost:8501/" rel="nofollow">http://localhost:8501/</a></p>
</li>
<li>
<p dir="auto">Use from within Python</p>
<div dir="auto" data-snippet-clipboard-copy-content="from PIL import Image
from pix2tex.cli import LatexOCR

img = Image.open(&#39;path/to/image.png&#39;)
model = LatexOCR()
print(model(img))"><pre><span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>from</span> <span>pix2tex</span>.<span>cli</span> <span>import</span> <span>LatexOCR</span>

<span>img</span> <span>=</span> <span>Image</span>.<span>open</span>(<span>&#39;path/to/image.png&#39;</span>)
<span>model</span> <span>=</span> <span>LatexOCR</span>()
<span>print</span>(<span>model</span>(<span>img</span>))</pre></div>
</li>
</ol>
<p dir="auto">The model works best with images of smaller resolution. That&#39;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#39;s not perfect and might not be able to handle huge images optimally, so don&#39;t zoom in all the way before taking a picture.</p>
<p dir="auto">Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.</p>
<p dir="auto"><strong>Want to use the package?</strong></p>
<p dir="auto">I&#39;m trying to compile a documentation right now.</p>
<p dir="auto">Visit here: <a href="https://pix2tex.readthedocs.io/" rel="nofollow">https://pix2tex.readthedocs.io/</a></p>
<h2 tabindex="-1" id="user-content-training-the-model-" dir="auto"><a href="#training-the-model-">Training the model </a><a href="https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></h2>
<p dir="auto">Install a couple of dependencies <code>pip install &#34;pix2tex[train]&#34;</code>.</p>
<ol dir="auto">
<li>First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run</li>
</ol>
<div data-snippet-clipboard-copy-content="python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl"><pre><code>python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl
</code></pre></div>
<p dir="auto">To use your own tokenizer pass it via <code>--tokenizer</code> (See below).</p>
<p dir="auto">You can find my generated training data on the <a href="https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO" rel="nofollow">Google Drive</a> as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.</p>
<ol start="2" dir="auto">
<li>Edit the <code>data</code> (and <code>valdata</code>) entry in the config file to the newly generated <code>.pkl</code> file. Change other hyperparameters if you want to. See <code>pix2tex/model/settings/config.yaml</code> for a template.</li>
<li>Now for the actual training run</li>
</ol>
<div data-snippet-clipboard-copy-content="python -m pix2tex.train --config path_to_config_file"><pre><code>python -m pix2tex.train --config path_to_config_file
</code></pre></div>
<p dir="auto">If you want to use your own data you might be interested in creating your own tokenizer with</p>
<div data-snippet-clipboard-copy-content="python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json"><pre><code>python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json
</code></pre></div>
<p dir="auto">Don&#39;t forget to update the path to the tokenizer in the config file and set <code>num_tokens</code> to your vocabulary size.</p>
<h2 tabindex="-1" id="user-content-model" dir="auto"><a href="#model">Model<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The model consist of a ViT [<a href="#References">1</a>] encoder with a ResNet backbone and a Transformer [<a href="#References">2</a>] decoder.</p>
<h3 tabindex="-1" id="user-content-performance" dir="auto"><a href="#performance">Performance<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<table>
<thead>
<tr>
<th>BLEU score</th>
<th>normed edit distance</th>
<th>token accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.88</td>
<td>0.10</td>
<td>0.60</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-data" dir="auto"><a href="#data">Data<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. <a href="https://www.wikipedia.org" rel="nofollow">wikipedia</a>, <a href="https://www.arxiv.org" rel="nofollow">arXiv</a>. We also use the formulae from the <a href="https://zenodo.org/record/56198#.V2px0jXT6eA" rel="nofollow">im2latex-100k</a> [<a href="#References">3</a>] dataset.
All of it can be found <a href="https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO" rel="nofollow">here</a></p>
<h3 tabindex="-1" id="user-content-dataset-requirements" dir="auto"><a href="#dataset-requirements">Dataset Requirements<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">In order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools:</p>
<ul dir="auto">
<li><a href="https://www.ctan.org/pkg/xetex" rel="nofollow">XeLaTeX</a></li>
<li><a href="https://imagemagick.org/" rel="nofollow">ImageMagick</a> with <a href="https://www.ghostscript.com/index.html" rel="nofollow">Ghostscript</a>. (for converting pdf to png)</li>
<li><a href="https://nodejs.org/" rel="nofollow">Node.js</a> to run <a href="https://github.com/KaTeX/KaTeX">KaTeX</a> (for normalizing Latex code)</li>
<li>Python 3.7+ &amp; dependencies (specified in <code>setup.py</code>)</li>
</ul>
<h3 tabindex="-1" id="user-content-fonts" dir="auto"><a href="#fonts">Fonts<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math</p>
<h2 tabindex="-1" id="user-content-todo" dir="auto"><a href="#todo">TODO<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul>
<li> add more evaluation metrics</li>
<li> create a GUI</li>
<li> add beam search</li>
<li> support handwritten formulae (kinda done, see training colab notebook)</li>
<li> reduce model size (distillation)</li>
<li> find optimal hyperparameters</li>
<li> tweak model structure</li>
<li> fix data scraping and scrape more data</li>
<li> trace the model (<a href="https://github.com/lukas-blecher/LaTeX-OCR/issues/2" data-hovercard-type="issue" data-hovercard-url="/lukas-blecher/LaTeX-OCR/issues/2/hovercard">#2</a>)</li>
</ul>
<h2 tabindex="-1" id="user-content-contribution" dir="auto"><a href="#contribution">Contribution<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">Contributions of any kind are welcome.</p>
<h2 tabindex="-1" id="user-content-acknowledgment" dir="auto"><a href="#acknowledgment">Acknowledgment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">Code taken and modified from <a href="https://github.com/lucidrains">lucidrains</a>, <a href="https://github.com/rwightman/pytorch-image-models">rwightman</a>, <a href="https://github.com/harvardnlp/im2markup">im2markup</a>, <a href="https://github.com/soskek/arxiv_leaks">arxiv_leaks</a>, <a href="https://github.com/pkra/MathJax-single-file">pkra: Mathjax</a>, <a href="https://github.com/harupy/snipping-tool">harupy: snipping tool</a></p>
<h2 tabindex="-1" id="user-content-references" dir="auto"><a href="#references">References<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">[1] <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">An Image is Worth 16x16 Words</a></p>
<p dir="auto">[2] <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need</a></p>
<p dir="auto">[3] <a href="https://arxiv.org/abs/1609.04938v2" rel="nofollow">Image-to-Markup Generation with Coarse-to-Fine Attention</a></p>
</article>
          </div></div>
  </body>
</html>
