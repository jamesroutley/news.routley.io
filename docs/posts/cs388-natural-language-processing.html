<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html">Original</a>
    <h1>CS388: Natural Language Processing</h1>
    
    <div id="readability-page-1" class="page"><p>These are the course materials for an online masters course in NLP. All lectures are videos available on YouTube.</p><p><b>Note on enrollment for on-campus students:</b> This course is listed in the course catalog as &#34;Natural Language Processing-WB&#34;. It is a partially asynchronous
course taught for certain online masters programs at UT (&#34;Option III&#34; programs, as the university calls them). If you are a student enrolled on-campus at UT Austin,
you are <b>not</b> eligible to take this course. This is a hard requirement from
the university due to the fact that this course is part of an Option III program. There is an on-campus version of CS388 that is typically
taught once per year by either me, Eunsol Choi, or Ray Mooney, which you are eligible to take (or CS371N if you&#39;re an undergraduate student). Regardless, you are free to consult the materials here!

</p><div bordercolor="#111111" id="AutoNumber5" width="800">
  <tbody>
    <tr>
      <td><b>Topics and Videos</b></td>
      <td><b>Readings</b></td>
    </tr>
    <tr><td colspan="2"><b>Week 1: Intro and Linear Classification</b></td></tr>
    <tr>
     <td><a href="https://youtu.be/Mz8-LTednt4">Course Preview</a>
     </td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/k5p8teUNHX4">Introduction</a>
      </td>
      <td><p>Note: this introduction video is from an older run of the class and references an outdated schedule. Please refer
          to the new course structure here.
          </p>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DVxR3AwdxoA">Linear Binary
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          2.0-2.5, 4.2-4.4.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0jSElGFUxro">
          Sentiment Analysis and Basic Feature Extraction</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          4.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/_We4tlPkaj0">Basics of
          Learning, Gradient Descent</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tMGv5ZcuVP4">Perceptron</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hhTkyP7EzGw">Perceptron as
          Minimizing Loss</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0naHFT07ja8">Logistic
          Regression</a>
      </td>
      <td><a href="https://bytes.zone/micro/stopping-thing-a-month/perc-lr-connections.pdf">Perceptron and LR connections</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/cKbnEmjxnOY">Sentiment
          Analysis</a>
      </td>
      <td><a href="https://www.aclweb.org/anthology/W02-1011/" target="_blank">Thumbs up? Sentiment Classification using
          Machine Learning Techniques</a> Bo Pang et al., 2002</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/65ui-GdtY0Q">Optimization
          Basics</a>
      </td>
      <td></td>
    </tr>

    <tr><td colspan="2"><b>Week 2: Multiclass and Neural Classification</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/My6GaGhqxdI">Multiclass
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 4.2</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/EA627DC7k6M">Multiclass
          Perceptron and Logistic Regression</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/va2i7LXt9zI">Multiclass
          Classification Examples</a></td>
      <td><a href="https://www.aclweb.org/anthology/D15-1075/" target="_blank">A large annotated corpus for learning
          natural language inference</a> Sam Bowman et al., 2015</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/N4f2-S19LME">Fairness in
          Classification</a></td>
      <td><a href="https://arxiv.org/pdf/1811.10104.pdf" target="_blank">50 Years of Test (Un)fairness: Lessons for
          Machine Learning</a> Ben Hutchinson and Margaret Mitchell, 2018</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DU_p-RBy5gM">Neural
          Networks</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rdohzaGa8aE">Neural Network
          Visualization</a></td>
      <td><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">[Blog] Neural Networks,
          Manifolds, and Topology</a> Chris Olah</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/8WhPYIWyR5g">Feedforward
          Neural Networks, Backpropagation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          Chapter 3.1-3.3</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/IRZCQO18QAI">Neural Net
          Implementation</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KPZb2rYS4BE">Neural Net
          Training, Optimization</a></td>
      <td><a href="https://dl.acm.org/doi/10.5555/2627435.2670313">Dropout: a simple way to prevent neural networks from
          overfitting</a> Nitish Srivastava et al., 2014 </td>
    </tr>

    <tr><td colspan="2"><b>Week 3: Word Embeddings</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/8EqQROdVPyM">Word
          Embeddings</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hznxqCIrzSQ">Skip-gram</a>
      </td>
      <td><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed
          Representations of Words and Phrases and their Compositionality</a> Tomas Mikolov et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/gpP-depOUwg">Other Word
          Embedding Methods</a></td>
      <td><a href="https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html" target="_blank">A Scalable Hierarchical Distributed Language Model</a> Andriy Mnih and Geoff Hinton, 2008</td></tr>

    <tr>
      <td><a href="https://youtu.be/J_227g77Jqg">Bias in Word
          Embeddings</a></td>
      <td><a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" target="_blank">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
          Embeddings</a> Tolga Bolukbasi et al., 2016</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/3pwwdHuH0I4">Applying
          Embeddings, Deep Averaging Networks</a></td>
      <td><a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals
          Syntactic Methods for Text Classification</a> Mohit Iyyer et al., 2015</td>
    </tr>


    <tr><td colspan="2"><b>Week 4: Language Modeling and Self-Attention</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/J-yHbD8LYCM">
          n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Yfug5eIQh5w">
          Smoothing in n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.2</a></td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/ImW4vJ5XZQc">
          LM Evaluation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.4</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/59NrmwAdOWA">
          Neural Language Models</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/xvnnA04JVQo">
          RNNs and their Shortcomings</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.3</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q7HY7tpWWi8">
          Attention</a></td>
      <td><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate
</a> Dzmitry Bahdanau et al., 2015</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/10l2NXStROU">
          Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/nHXrdLMo8Uk">
          Multi-Head Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a8sTGth7PoU">
          Position Encodings</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr><td colspan="2"><b>Week 5: Transformers and Decoding</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/sLsUD-RcDqg">
          Transformer Architecture</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/1Efx04lHa7w">
          Using Transformers</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/htyspM3FrMg">
          Transformer Language Modeling</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DPvDL8L4Dqo">
          Transformer Extensions</a></td>
      <td>
        <a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a> Jared Kaplan et al., 2020</td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/wltqDbhlcJ0">
          Beam Search</a></td>
      <td>
      </td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/JETxaSaj6_k">
          Nucleus Sampling</a></td>
      <td>
         <a href="https://arxiv.org/abs/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a> Ari Holtzman et al., 2019
      </td>
    </tr>


    <tr><td colspan="2"><b>Week 6: Pre-training, seq2seq LMs</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/dya_QNFvtiQ">
          BERT: Masked Language Modeling</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/g96oi4ihc_E">
          BERT: Model and Applications</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019</td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/TKZkvqb-qpM">
          Seq2seq Models</a></td>
      <td>
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/M9L3gk4ITec">
          BART</a></td>
      <td><a href="https://arxiv.org/abs/1910.13461" target="_blank">BART: Denoising Sequence-to-Sequence Pre-training
          for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et al., 2019
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/b6KFaT8mK4g">
          T5</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">Exploring the Limits of Transfer Learning with a
          Unified Text-to-Text Transformer</a> Colin Raffel et al., 2020</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/WA16JelEkkg">
          Word Piece and Byte Pair Encoding</a></td>
      <td><a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank">Neural Machine Translation of Rare Words with
          Subword Units</a> Rico Sennrich et al., 2016</td>
    </tr>
    <tr><td colspan="2"><b>Week 7-8: Structured Prediction: Part-of-speech, Syntactic Parsing</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/Llw6qfeAWDs">Part-of-Speech
          Tagging</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 8.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/yQZ0mDW-U3g">Sequence
          Labeling, Tagging with Classifiers</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/FeLtLLbn4qU">Hidden Markov
          Models</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dVF7LZkbl9g">
          HMMs: Parameter Estimation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Ks7IrsjhqSo">
          HMMs: Viterbi Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.3</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/wijpAX_LLXo">
          HMMs for POS Tagging</a></td>
      <td><a href="https://arxiv.org/abs/cs/0003055" target="_blank">TnT - A Statistical Part-of-Speech Tagger</a>
        Thorsten Brants, 2000</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/zDPUKQKDaMM">
          Constituency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.1-10.2</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q3dLP9YQLPA">
          Probabilistic Context-Free Grammars</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3-10.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QeDb6mSDSqs">
          CKY Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/f1o1_bPWzM0">
          Refining Grammars</a></td>
      <td><a href="https://www.aclweb.org/anthology/P03-1054/" target="_blank">Accurate Unlexicalized Parsing</a> Dan Klein
        and Chris Manning, 2003</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dbDjKCc4R3E">
          Dependencies</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ypoaw7lJ6Rk">
          Transition-based Dependency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.3</a>
      </td>
    </tr>
    <tr><td colspan="2"><b>Week 9: Modern Large Language Models</b></td></tr>

    <tr>
      <td><a href="https://youtu.be/jn41DLgnqek">
          GPT-3</a></td>
      <td>
       <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> Alec Radford et al., 2019</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YCq6b31Jb6E">
          Zero-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2212.04037">Demystifying Prompts in Language Models via Perplexity Estimation</a> Hila Gonen et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/JSBjj09xJeM">
          Few-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> Tony Z. Zhao et al., 2021</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/mUthsZ_Aivo">
          Understanding ICL: Induction Heads</a></td>
      <td>
        <a href="https://arxiv.org/abs/2209.11895">In-context Learning and Induction Heads</a> Catherine Olsson et al., 2022
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/YT3VSlDjrVU">
          Instruction Tuning</a></td>
      <td>
        <a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> Victor Sanh et al., 2021</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DwAdhx6GFh8">
          Reinforcement Learning from Human Feedback (RLHF)</a></td>
      <td>
        <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> Long Ouyang et al., 2022</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bQZvmQUlqcs">
          Factuality of LLMs</a></td>
      <td>
        <a href="https://arxiv.org/abs/2212.07981">Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation</a> Yixin Liu et al., 2023</td>
    </tr>
    <tr><td colspan="2"><b>Week 10: Explanations</b></td></tr>


    <tr>
      <td><a href="https://youtu.be/Nr0_xYEso-4">
          Explainability in NLP</a></td>
      <td><a href="https://arxiv.org/pdf/1606.03490.pdf" target="_blank">The Mythos of Model Interpretability</a> Zach Lipton,
        2016</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ZVElc4CvHpk">
          Local Explanations: Highlights</a></td>
      <td><a href="https://arxiv.org/pdf/1602.04938.pdf" target="_blank">&#34;Why Should I Trust You?&#34; Explaining the
          Predictions of Any Classifier</a> Marco Tulio Ribeiro et al., 2016</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a6u6WM5wcLQ">
          Model Probing</a></td>
      <td><a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank">BERT Rediscovers the Classical NLP Pipeline</a>
        Ian Tenney et al., 2019</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/RXYaMZcDIWU">
          Annotation Artifacts</a></td>
      <td><a href="https://www.aclweb.org/anthology/N18-2017/" target="_blank">Annotation Artifacts in Natural Language
          Inference Data</a> Suchin Gururangan et al., 2018</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/bXHM5t_ejsc">
          Text Explanations</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1603.08507.pdf" target="_blank">Generating Visual Explanations</a> Lisa-Anne Hendricks et
        al., 2016</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tNGu3EqJbKc">
          Chain-of-thought</a></td>
      <td>
        <a href="https://arxiv.org/abs/1705.04146" target="_blank">Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems</a> Wang Ling et al., 2017</td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/9sFyzMywKmo">
          Chain-of-thought: Extensions and Analysis</a></td>
      <td>
        <a href="https://arxiv.org/pdf/2211.13892.pdf" target="_blank">Complementary Explanations for Effective In-Context Learning</a> Xi Ye et al., 2023</td>
    </tr>

    <tr><td colspan="2"><b>Week 11: Question Answering, Dialogue Systems</b></td></tr>
    
    <tr>
      <td><a href="https://youtu.be/gnUSE0fCbso">
          Reading comprehension intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JRI3RwRBnMY">
          Reading comprehension: setup and baselines</a></td>
      <td><a href="https://www.aclweb.org/anthology/D13-1020.pdf" target="_blank">MCTest: A Challenge Dataset for the
          Open-Domain Machine Comprehension of Text</a> Matthew Richardson et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/F8hWZ4xaVkA">
          BERT for QA</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tCvAHmrxPvY">
          Problems with Reading Comprehension</a></td>
      <td><a href="https://www.aclweb.org/anthology/D17-1215/" target="_blank">Adversarial Examples for Evaluating
          Reading Comprehension Systems</a> Robin Jia and Percy Liang, 2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/P-j_zeS0Pa8">
          Open-domain QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1704.00051" target="_blank">Reading Wikipedia to Answer Open-Domain Questions</a> Danqi Chen et al., 2017</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/jpRwa2iE_z8">
          Multi-hop QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1809.09600" target="_blank">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
             </a> Zhilin Yang et al., 2018</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/vAZ7VlLXReE">
          Dialogue: Chatbots</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/JXfAkX7kvnM">
          Task-Oriented Dialogue</a></td>
      <td><a href="https://arxiv.org/pdf/1811.01241.pdf" target="_blank">Wizards of Wikipedia: Knowledge-Powered
          Conversational Agents</a> Emily Dinan et al., 2019</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hc7P3QukmJk" target="_blank">
          Neural Chatbots</a></td>
      <td>
        <a href="https://arxiv.org/abs/1506.06714">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a> Alessandro Sordoni et al., 2015</td>
          
    </tr>

    <tr><td colspan="2"><b>Week 12: Machine Translation, Summarization</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/9KAZ4-gKj9g">
          Machine Translation Intro</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Oup0DEYJXEQ">
          MT: Framework and Evaluation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dzOuPhBmFtE">
          MT: Word alignment</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/mbtk3VCG_2A">
          MT: IBM Models</a></td>
      <td><a href="https://www.aclweb.org/anthology/C96-2141.pdf" target="_blank">HMM-Based Word Alignment in
          Statistical Translation</a> Stephan Vogel et al., 1996</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0k8b5jGk-h4">
          Phrase-based Machine Translation</a></td>
      <td><a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/pharaoh-amta2004.pdf" target="_blank">Pharaoh: A
          Beam Search Decoder for Phrase-Based Statistical Machine Translation Models</a> Philipp Koehn, 2004</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bcP4b_4HQ8A">
          Neural and Pre-Trained Machine Translation</a></td>
      <td>
        <a href="https://arxiv.org/abs/1905.11901" target="_blank">Revisiting Low-Resource Neural Machine Translation: A Case Study</a> Rico Sennrich and Biao Zhang, 2019</td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/lBDI1CBNe_U">
          Summarization Intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QWt2E3m00kA">
          Extractive Summarization</a></td>
      <td><a href="https://dl.acm.org/doi/10.1145/290941.291025" target="_blank">The use of MMR, diversity-based
          reranking for reordering documents and producing summaries</a> Jaime Carbonell and Jade Goldstein, 1998</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/feLTtTilycY">
          Pre-trained Summarization and Factuality</a></td>
      <td><a href="https://www.aclweb.org/anthology/2020.acl-main.703/" target="_blank">BART: Denoising
          Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et
        al., 2019</td>
    </tr>

    <tr><td colspan="2"><b>Week 13-14: Multilinguality, Language Grounding, Ethical Issues</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/ettP9Ayrho8">
          Morphology</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/rTSCLfxdxrI">
          Cross-lingual Tagging and Parsing</a></td>
      <td><a href="https://www.aclweb.org/anthology/P11-1061/" target="_blank">Unsupervised Part-of-Speech Tagging with
          Bilingual Graph-Based Projections</a> Dipanjan Das and Slav Petrov, 2011</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/KNBRb8sjzOA">
          Cross-lingual Pre-training</a></td>
      <td><a href="https://arxiv.org/pdf/1602.01925.pdf" target="_blank">Massively Multilingual Word Embeddings</a>
        Waleed Ammar et al., 2016</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/ayNoMmoXnd8">
          Language Grounding</a></td>
      <td>
        <a href="https://aclanthology.org/2020.acl-main.463/">Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data</a> Emily Bender and Alexander Koller, 2020</td>
    </tr>
    <tr>
      <td><a href="https://www.youtube.com/watch?v=pkdV-iddZxk">
          Language and Vision</a></td>
      <td>
       <a href="https://arxiv.org/abs/1505.00468" target="_blank">VQA: Visual Question Answering</a> Aishwarya Agrawal et al., 2015</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/tTkAjNkXvH4">
          Ethics: Bias</a></td>
      <td>
        <a href="https://aclanthology.org/P16-2096.pdf" target="_blank">The Social Impact of Natural Language Processing</a> Dirk Hovy and Shannon Spruit, 2016</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/0haVbW2ouzw">
          Ethics: Exclusion</a></td>
      <td>
        <a href="https://arxiv.org/abs/2205.12247" target="_blank">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</a> Da Yin et al., 2022</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/2PSzHb08Xm4">
          Ethics: Dangers of Automation</a></td>
      <td>
       <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a> Emily Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell, 2021</td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/Hp8qfbsp57M">
          Ethics: Unethical Use and Paths Forward</a></td>
      <td>
       <a href="https://arxiv.org/pdf/1803.09010.pdf">Datasheets for Datasets</a> Timnit Gebru et al., 2018</td>
    </tr>

  </tbody>
</div></div>
  </body>
</html>
