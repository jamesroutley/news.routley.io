<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.humprog.org/~stephen/blog/2024/08/27/#how-to-wrap-cc-really">Original</a>
    <h1>How to wrap a C compiler and preprocessor, really</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>Diverting trains of thought, wasting precious time</p>
<!--<p>27 08 2024</p>-->
        <h2>Tue, 27 Aug 2024</h2>
<a name="how-to-wrap-cc-really"><h3>How to really wrap a C compiler and preprocessor, really* </h3>

<p>* really</p>

<p>Suppose we want to interfere with how a vaguely Unix-style C compiler does its job,
and that we want to try compiling existing software with this modified compiler.
Assuming the build system will let us do something like:</p>

<pre>CC=/path/to/my/wrapper make
</pre>

<p>or</p>

<pre>CC=/path/to/my/wrapper ./configure &amp;&amp; make
</pre>

<p>... then we&#39;d like something that is a drop-in replacement at the command-line level.
Let&#39;s also assume that our modifications are black-box: we want to do
“what the stock compiler was doing”, modulo some tweaks at the edges.
The tweaks might be running extra tools, munging the compiler command-line options,
doing source-to-source rewrites on some input files,
postprocessing output files, and so on.
Although I say “tweaks”, the extent of our interference could be arbitrary,
once we get into rewriting the input or output programs.
But we are still making at least some use of the original compiler.</p>

<p>This requirement is common if you do research with compilers.
The time-honoured way is to write a
Python script
or Perl script
or shell script
that parses a command line,
delegates to the real compiler
while also doing the desired extra stuff.
Then you build the existing software using this wrapper script, e.g. by
setting <tt>CC</tt> to point this wrapper script.</p>

<p>There are some problems with this.</p>

</a><p><a name="how-to-wrap-cc-really">To do any non-trivial interference
you probably want to divert the input and/or output of some stage(s) of compilation.
So you have to be able to identify
the input and output files on the command line,
from an understanding of its syntax and semantics.
Finding the output filename is nicely syntactic if <tt>-o</tt> is given,
but gets a bit fiddly and semantic in other cases: it&#39;s <tt>a.out</tt> if we&#39;re linking,
    otherwise it&#39;s a substitution on the input filename.
Input files are hard to detect too:
    there might be many of them (hence many output files too)
    and we need to know that if we see a pathological argument sequence like
    <tt>-I foo.c</tt>, <tt>foo.c</tt> in fact names a directory.
As well as “obvious”
input and outputs, there may be other outputs generated by options
    like <tt>-M</tt>, <tt>-MF</tt> and <tt>-MD</tt>.
Sometimes these are <em>in addition to</em> the named output file,
    but sometimes they are <em>instead of</em> it and </a><a href="https://github.com/stephenrkell/toolsub/issues/7#issuecomment-1747444526">the named output file will be ignored
    or left empty</a>.</p>

<p>We know that a single “compiler” command
is really a “driver” command.
The driver may run a selection of tools or “subcommands”:
the preprocessor, the compiler proper, the assembler and/or the linker.
The selection of these depends on the driver command.
To interfere with specific parts of compilation,
our script needs to be able to deconstruct a driver invocation
into the many subcommands.
It will then mess with the relevant ones of these,
but leave the others undisturbed.</p>

<p>Doing the deconstruction into subcommands is non-trivial.
It&#39;s what the driver exists for, after all.
To duplicate this ourselves, there&#39;s a lot we have to get right.
Which arguments to the driver need to go with which of the subcommands?
I have <a href="https://github.com/stephenrkell/toolsub/tree/master/compilerwrapper">my own Python script</a> that can usually sort these out,
and I&#39;m sure dozens of others are out there.
It works most of the time... but some commands will defeat it.
It also adds Python to the build dependencies of anything we want to build;
not a big deal, but this caused me some annoyance during the infamous Python-2-to-3 transition.
<a href="https://cil-project.github.io/cil/doc/html/cil/cilly.html#sec7">cilly</a> is another such script, written using Perl and even backing onto both GCC and MSVC.
As with many such scripts, it has annoyingly similar but not-quite-the-same options
to both GCC&#39;s and MSVC&#39;s. For example,
it has <tt>--save-temps</tt> whereas GCC has <tt>-save-temps</tt>,
so it is not quite a drop-in replacement.</p>

<p>These scripts are taking what I call the “new front door” approach.
They give the compiler driver a whole new entry point.
This is doable, but it quickly turns into reimplementing the driver (and doing so badly, in practice).</p>

<!--
It needs to replicate (ideally) the whole command line language of the targeted compiler(s),
and it needs to understand how these commands are deconstructed into
subcommands, in order to perform its manipulations.
-->

<p>I want instead a wrapper script that is simpler, more robust and more generic.
I want to get away with as shallow as possible an awareness of the command line syntax and semantics.
In particular, I don&#39;t want to reimplement the driver&#39;s deconstruction to subcommands.
I also want my script to be generic, in the sense that I could
straightforwardly re-use the same wrapper script for many purposes.
This contrasts with, say, <tt>cilly</tt> that is specialised to running <a href="https://cil-project.github.io/">CIL</a>
and tools based on it.</p>

<p>I&#39;ve been through a few iterations with this now.
What I&#39;ve ended up with is a “back-room” approach that
contrasts with the “front door” approach.
I&#39;ve made life simpler by only caring about GCC and Clang,
although it could probably work with
any compiler that follows vaguely the same historical command-line conventions
(which I suspect date back at least to <tt>pcc</tt>, although I haven&#39;t verified that).
GCC has some options that appear handy for a “back-room” approach.</p>

<pre>       -wrapper
           Invoke all subcommands under a wrapper program.  The name of the wrapper program and
           its parameters are passed as a comma separated list.

                   gcc -c t.c -wrapper gdb,--args

           This invokes all subprograms of gcc under gdb --args, thus the invocation of cc1 is
           gdb --args cc1 ....
...
       -v  Print (on standard error output) the commands executed to run the stages of
           compilation.  Also print the version number of the compiler driver program and of the
           preprocessor and the compiler proper.

       -###
           Like -v except the commands are not executed and arguments are quoted unless they
           contain only alphanumeric characters or &#34;./-_&#34;.  This is useful for shell scripts to
           capture the driver-generated command lines.
</pre>

<p>In short, <tt>-wrapper</tt> will prefix each of the subcommands with a user-supplied wrapper program,
while <tt>-###</tt> simply prints out the subcommands that the driver
<em>would</em> run if you ran it without that option.
<tt>clang</tt> supports the latter of these but not the former.</p>

<p>On my system, the GCC driver run on a hello-world C program generates the following subcommands.</p>

<pre>$ gcc -### -o hello hello.c 
(snipped various less interesting lines)
/usr/lib/gcc/x86_64-linux-gnu/10/cc1 -quiet -imultiarch x86_64-linux-gnu hello.c -quiet -dumpbase hello.c &#34;-mtune=generic&#34; &#34;-march=x86-64&#34; -auxbase hello -fasynchronous-unwind-tables -o /tmp/ccXlWTFa.s
as --64 -o /tmp/ccNhuw5L.o /tmp/ccXlWTFa.s
/usr/lib/gcc/x86_64-linux-gnu/10/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/10/liblto_plugin.so &#34;-plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/10/lto-wrapper&#34; &#34;-plugin-opt=-fresolution=/tmp/cc77VLgc.res&#34; &#34;-plugin-opt=-pass-through=-lgcc&#34; &#34;-plugin-opt=-pass-through=-lgcc_s&#34; &#34;-plugin-opt=-pass-through=-lc&#34; &#34;-plugin-opt=-pass-through=-lgcc&#34; &#34;-plugin-opt=-pass-through=-lgcc_s&#34; --build-id --eh-frame-hdr -m elf_x86_64 &#34;--hash-style=gnu&#34; --as-needed -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie -o hello /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/Scrt1.o /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/10/crtbeginS.o -L/usr/lib/gcc/x86_64-linux-gnu/10 -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/
</pre>

<p>That&#39;s already quite a mouthful.
But the <tt>-wrapper</tt> or <tt>-###</tt> options can take care of this “deconstructing” thing for us.
That&#39;s the back-room approach: our tool needs to work only with the resulting
split-out subcommands. This makes it less error-prone than if we
tried to deduce the subcommands (or equivalents-we-hope) from the driver command.
<!--
For example, to do analysis or instrumentation of preprocessed C source code,
although you might cobble together a script
that's capable of inferring “this looks like a driver command that does
preprocessing—let's just run <tt>cpp</tt>” separately.
But it's easy to get that wrong, say in the presence of options like <tt>-x c</tt>
(“treat subsequent input files as C source”).
-->
<!--For example, when it runs the C compiler proper, we know that the input is preprocessed C source code.-->
Despite all this, we still have a fair number of problems to solve.</p>

<h3>De-integrating preprocessing from compilation</h3>

<p>In modern versions of both GCC and Clang,
the compiler and preprocessor are integrated by default.
You can see that in the above: there is only a single <tt>cc1</tt> command,
no separate preprocessing stage.
So, if we want to interfere with the source code after preprocessing, we appear to be stuck.
With GCC we can ask nicely by adding <tt>-no-integrated-cpp</tt> and get back a separate preprocessing
subcommand.</p>

<pre>$ gcc -### -no-integrated-cpp -o hello hello.c 
/usr/lib/gcc/x86_64-linux-gnu/10/cc1 -E -quiet -imultiarch x86_64-linux-gnu hello.c &#34;-mtune=generic&#34; &#34;-march=x86-64&#34; -fasynchronous-unwind-tables -o /tmp/ccdz9uxk.i
/usr/lib/gcc/x86_64-linux-gnu/10/cc1 -fpreprocessed /tmp/ccdz9uxk.i -quiet -dumpbase hello.c &#34;-mtune=generic&#34; &#34;-march=x86-64&#34; -auxbase hello -fasynchronous-unwind-tables -o /tmp/ccisTsYo.s
as --64 -o /tmp/ccALTbUH.o /tmp/ccisTsYo.s
/usr/lib/gcc/x86_64-linux-gnu/10/collect2 -plugin /usr/lib/gcc/x86_64-linux-gnu/10/liblto_plugin.so &#34;-plugin-opt=/usr/lib/gcc/x86_64-linux-gnu/10/lto-wrapper&#34; &#34;-plugin-opt=-fresolution=/tmp/ccx8FfYe.res&#34; &#34;-plugin-opt=-pass-through=-lgcc&#34; &#34;-plugin-opt=-pass-through=-lgcc_s&#34; &#34;-plugin-opt=-pass-through=-lc&#34; &#34;-plugin-opt=-pass-through=-lgcc&#34; &#34;-plugin-opt=-pass-through=-lgcc_s&#34; --build-id --eh-frame-hdr -m elf_x86_64 &#34;--hash-style=gnu&#34; --as-needed -dynamic-linker /lib64/ld-linux-x86-64.so.2 -pie -o hello /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/Scrt1.o /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/10/crtbeginS.o -L/usr/lib/gcc/x86_64-linux-gnu/10 -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/10/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/10/../../.. /tmp/ccALTbUH.o -lgcc --push-state --as-needed -lgcc_s --pop-state -lc -lgcc --push-state --as-needed -lgcc_s --pop-state /usr/lib/gcc/x86_64-linux-gnu/10/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/10/../../../x86_64-linux-gnu/crtn.o
</pre>

<p>With Clang we can&#39;t ask directly, but it
turns out that there is a hack that works in the versions I&#39;ve tried.
If we add <tt>-traditional-cpp</tt> to the arguments,
it will split the two phases apart,
presumably since the integrated preprocessor can&#39;t do the
traditional emulations. Compare the following.</p>

<pre>$ clang-13 -### -o hello hello.c
&#34;/usr/lib/llvm-13/bin/clang&#34; &#34;-cc1&#34; &#34;-triple&#34; &#34;x86_64-pc-linux-gnu&#34; &#34;-emit-obj&#34; &#34;-mrelax-all&#34; &#34;--mrelax-relocations&#34; &#34;-disable-free&#34; &#34;-disable-llvm-verifier&#34; &#34;-discard-value-names&#34; &#34;-main-file-name&#34; &#34;hello.c&#34; &#34;-mrelocation-model&#34; &#34;static&#34; &#34;-mframe-pointer=all&#34; &#34;-fmath-errno&#34; &#34;-fno-rounding-math&#34; &#34;-mconstructor-aliases&#34; &#34;-munwind-tables&#34; &#34;-target-cpu&#34; &#34;x86-64&#34; &#34;-tune-cpu&#34; &#34;generic&#34; &#34;-debugger-tuning=gdb&#34; &#34;-fcoverage-compilation-dir=/tmp&#34; &#34;-resource-dir&#34; &#34;/usr/lib/llvm-13/lib/clang/13.0.1&#34; &#34;-internal-isystem&#34; &#34;/usr/lib/llvm-13/lib/clang/13.0.1/include&#34; &#34;-internal-isystem&#34; &#34;/usr/local/include&#34; &#34;-internal-isystem&#34; &#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/12/../../../../x86_64-linux-gnu/include&#34; &#34;-internal-externc-isystem&#34; &#34;/usr/include/x86_64-linux-gnu&#34; &#34;-internal-externc-isystem&#34; &#34;/include&#34; &#34;-internal-externc-isystem&#34; &#34;/usr/include&#34; &#34;-fdebug-compilation-dir=/tmp&#34; &#34;-ferror-limit&#34; &#34;19&#34; &#34;-fgnuc-version=4.2.1&#34; &#34;-fcolor-diagnostics&#34; &#34;-faddrsig&#34; &#34;-D__GCC_HAVE_DWARF2_CFI_ASM=1&#34; &#34;-o&#34; &#34;/tmp/hello-e8c787.o&#34; &#34;-x&#34; &#34;c&#34; &#34;hello.c&#34;
&#34;/usr/bin/ld&#34; &#34;--hash-style=both&#34; &#34;--build-id&#34; &#34;--eh-frame-hdr&#34; &#34;-m&#34; &#34;elf_x86_64&#34; &#34;-dynamic-linker&#34; &#34;/lib64/ld-linux-x86-64.so.2&#34; &#34;-o&#34; &#34;hello&#34; &#34;/usr/lib/x86_64-linux-gnu/crt1.o&#34; &#34;/usr/lib/x86_64-linux-gnu/crti.o&#34; &#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/12/crtbegin.o&#34; &#34;-L/usr/bin/../lib/gcc/x86_64-linux-gnu/12&#34; &#34;-L/usr/bin/../lib/gcc/x86_64-linux-gnu/12/../../../../lib64&#34; &#34;-L/lib/x86_64-linux-gnu&#34; &#34;-L/lib/../lib64&#34; &#34;-L/usr/lib/x86_64-linux-gnu&#34; &#34;-L/usr/lib/../lib64&#34; &#34;-L/usr/lib/llvm-13/bin/../lib&#34; &#34;-L/lib&#34; &#34;-L/usr/lib&#34; &#34;/tmp/hello-e8c787.o&#34; &#34;-lgcc&#34; &#34;--as-needed&#34; &#34;-lgcc_s&#34; &#34;--no-as-needed&#34; &#34;-lc&#34; &#34;-lgcc&#34; &#34;--as-needed&#34; &#34;-lgcc_s&#34; &#34;--no-as-needed&#34; &#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/12/crtend.o&#34; &#34;/usr/lib/x86_64-linux-gnu/crtn.o&#34;
</pre>

<p>... versus ...</p>

<pre>$ clang-13 -### -traditional-cpp -o hello hello.c
clang: error: the clang compiler does not support &#39;-traditional-cpp&#39;
 &#34;/usr/lib/llvm-13/bin/clang&#34; &#34;-cc1&#34; &#34;-triple&#34; &#34;x86_64-pc-linux-gnu&#34; &#34;-E&#34; &#34;-disable-free&#34; &#34;-disable-llvm-verifier&#34; &#34;-discard-value-names&#34; &#34;-main-file-name&#34; &#34;hello.c&#34; &#34;-mrelocation-model&#34; &#34;static&#34; &#34;-mframe-pointer=all&#34; &#34;-fmath-errno&#34; &#34;-fno-rounding-math&#34; &#34;-mconstructor-aliases&#34; &#34;-munwind-tables&#34; &#34;-target-cpu&#34; &#34;x86-64&#34; &#34;-tune-cpu&#34; &#34;generic&#34; &#34;-debugger-tuning=gdb&#34; &#34;-fcoverage-compilation-dir=/tmp&#34; &#34;-resource-dir&#34; &#34;/usr/lib/llvm-13/lib/clang/13.0.1&#34; &#34;-internal-isystem&#34; &#34;/usr/lib/llvm-13/lib/clang/13.0.1/include&#34; &#34;-internal-isystem&#34; &#34;/usr/local/include&#34; &#34;-internal-isystem&#34; &#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/12/../../../../x86_64-linux-gnu/include&#34; &#34;-internal-externc-isystem&#34; &#34;/usr/include/x86_64-linux-gnu&#34; &#34;-internal-externc-isystem&#34; &#34;/include&#34; &#34;-internal-externc-isystem&#34; &#34;/usr/include&#34; &#34;-fdebug-compilation-dir=/tmp&#34; &#34;-ferror-limit&#34; &#34;19&#34; &#34;-fgnuc-version=4.2.1&#34; &#34;-fcolor-diagnostics&#34; &#34;-traditional-cpp&#34; &#34;-faddrsig&#34; &#34;-D__GCC_HAVE_DWARF2_CFI_ASM=1&#34; &#34;-o&#34; &#34;/tmp/hello-8e16c8.i&#34; &#34;-x&#34; &#34;c&#34; &#34;hello.c&#34;
 &#34;/usr/lib/llvm-13/bin/clang&#34; &#34;-cc1&#34; &#34;-triple&#34; &#34;x86_64-pc-linux-gnu&#34; &#34;-emit-obj&#34; &#34;-mrelax-all&#34; &#34;--mrelax-relocations&#34; &#34;-disable-free&#34; &#34;-disable-llvm-verifier&#34; &#34;-discard-value-names&#34; &#34;-main-file-name&#34; &#34;hello.c&#34; &#34;-mrelocation-model&#34; &#34;static&#34; &#34;-mframe-pointer=all&#34; &#34;-fmath-errno&#34; &#34;-fno-rounding-math&#34; &#34;-mconstructor-aliases&#34; &#34;-munwind-tables&#34; &#34;-target-cpu&#34; &#34;x86-64&#34; &#34;-tune-cpu&#34; &#34;generic&#34; &#34;-debugger-tuning=gdb&#34; &#34;-fcoverage-compilation-dir=/tmp&#34; &#34;-resource-dir&#34; &#34;/usr/lib/llvm-13/lib/clang/13.0.1&#34; &#34;-fdebug-compilation-dir=/tmp&#34; &#34;-ferror-limit&#34; &#34;19&#34; &#34;-fgnuc-version=4.2.1&#34; &#34;-fcolor-diagnostics&#34; &#34;-faddrsig&#34; &#34;-D__GCC_HAVE_DWARF2_CFI_ASM=1&#34; &#34;-o&#34; &#34;/tmp/hello-c08823.o&#34; &#34;-x&#34; &#34;cpp-output&#34; &#34;/tmp/hello-8e16c8.i&#34;
 &#34;/usr/bin/ld&#34; &#34;--hash-style=both&#34; &#34;--build-id&#34; &#34;--eh-frame-hdr&#34; &#34;-m&#34; &#34;elf_x86_64&#34; &#34;-dynamic-linker&#34; &#34;/lib64/ld-linux-x86-64.so.2&#34; &#34;-o&#34; &#34;hello&#34; &#34;/usr/lib/x86_64-linux-gnu/crt1.o&#34; &#34;/usr/lib/x86_64-linux-gnu/crti.o&#34; &#34;/usr/bin/../lib/gcc/x86_64-linux-gnu/12/crtbegin.o&#34; &#34;-L/usr/bin/../lib/gcc/x86_64-linux-gnu/12&#34; &#34;-L/usr/bin/../lib/gcc/x86_64-linux-gnu/12/../../../../lib64&#34; &#34;-L/lib/x86_64-linux-gnu&#34; &#34;-L/lib/../lib64&#34; &#34;-L/usr/lib/x86_64-linux-gnu&#34; &#34;-L/usr/lib/../lib64&#34; &#34;-L/usr/lib/llvm-13/bin/../lib&#34; &#34;-L/lib&#34; &#34;-L/usr/lib&#34; &#34;/tmp/hello-c08823.o&#34; &#34;-lgcc&#34; &#34;--as-needed&#34; &#34;-lgcc_s&#34; &#34;--no-as-needed&#34; &#34;-lc&#34; &#34;-lgcc&#34; &#34;--as-needed&#34; &#34;-lgcc_s&#34; &#34;--no-as-nee
</pre>

<p>So our wrapper can add <tt>-traditional-cpp</tt> if it&#39;s absent,
then, if we added it, strip it out of the preprocessing-only command that emerges.
This is nasty and may stop working, of course.
In fact, in a way, it already has! Notice that although the Clang driver generated the subcommands,
it has also generated an error message saying they won&#39;t work. And they don&#39;t!
Even though <tt>-traditional-cpp</tt> is listed in the <tt>--help</tt> output of Clang 13,
the actual code seems to have been removed from the preprocessor.
This is again illustrating the distinction between the driver command (which does support the option)
and the subcommands (which don&#39;t).
Luckily, we only need the driver part:
we make it create these commands in order to then <em>delete</em> <tt>-traditional-cpp</tt>
from them.
That makes them work again! And most importantly,
there&#39;s now two of them: preprocessing is now separate from compilation.</p>

<h3>Precise scanning, a.k.a. distinguishing option and non-option arguments</h3>

<p>Even though we&#39;re not supplying a whole new front door,
there&#39;s still no getting away from it:
to be robust, our script needs complete knowledge of each subcommand&#39;s
command-line syntax, <em>up to option arguments</em>.
This is a problem I call <em>precise scanning</em>, because
scanning a.k.a. lexing is about chunking things up.
Here we&#39;re chunking command-line arguments, so that options (like <tt>-o</tt>) are
correctly chunked together
with their argument (the output filename).
Front-door approaches need this too:
although they only need to know the driver&#39;s options, not subcommands&#39;,
they also need to know the semantics (i.e. which subcommands are implied),
not just the syntax (how to scan the arguments).</p>

<p>Without precise scanning, we cannot reliably identify input and output files,
nor be sure that we&#39;re seeing all options we may be interested in
(and not seeing them spuriously).
Consider the slightly wacky command <tt>cc -o -c /tmp/hello.c</tt>.
If we run it, a fully-linked output file called “<tt>-c</tt>” will appear.
If I naively scanned the command line,
I&#39;d have seen <tt>-c</tt> and inferred
that the command is not doing linking, but that would be wrong.
In general, while (almost) all options begin with
a <tt>-</tt>,
the converse is not true: arguments beginning with <tt>-</tt>
need not be options.
Note also that the bare <tt>-</tt> is not an option but a filename, denoting standard input or output.
Now consider that there are hundreds of options that take arguments:
<tt>-o</tt> is well-known but there&#39;s plenty that aren&#39;t.
The approach I&#39;ve taken is based on scraping the help text of
GCC and Clang,
since this is mechanically generated and fairly easy to de-generate.
The scrapings are cached in the script, but the script tells you how to do a re-scrape
using your actual live compiler version, if it sees options it doesn&#39;t understand.</p>

<p>It turns out that this scraping exercise is a good way to find ways in which the online help is not complete.
Some options are also manually added to the script.
For GCC these include our friends
<tt>-wrapper</tt> and <tt>-no-integrated-cpp</tt>, but also
the <tt>-M*</tt>, <tt>-D*</tt>, <tt>-L*</tt> and <tt>-A*</tt> options.
Each of these omissions is arguably reportable bug, although I haven&#39;t reported any myself yet.
(The help also omits most of the <tt>-O*</tt> and <tt>-g*</tt> ones, although
these are covered by the spiel at the bottom, 
saying that options “starting with -g, -f, -m, -O, -W, or --param are automatically
passed on to the various sub-processes”. This list is not exhaustive!)</p>

<h3>Interpreting compiler-internal subcommands, namely <tt>cc1</tt></h3>

<p>Preprocessing and/or compiling
use an internal and relatively undocumented subcommand, <tt>cc1</tt>.
GCC actually has a separate <tt>cc1</tt> program.
In <tt>clang</tt>&#39;s case, it&#39;s simply the same <tt>clang</tt> binary
invoked with the <tt>-cc1</tt> option.
The effect is the same.
Note that
<tt>clang</tt>&#39;s <tt>cc1</tt> options are different from GCC&#39;s.</p>

<p>As part of my “generic” requirement, I want an interface
that lets a client say “wrap preprocessing with the following tool”.
What options will this tool receive?
I could just say “it will receive <tt>cc1</tt> options; suck it up!”
But then the tool is coupled to a particular compiler.
Given that our script already needs complete knowledge of the command-line syntax,
I&#39;d rather that <em>only</em> the script need know this,
and that we somehow find a generic way to write the tool.
For now I&#39;ll just tackle <tt>cc1</tt>-related issues,
but we&#39;ll tackle a strong version of this “generic” requirement later.</p>

<p>The options to <tt>cc1</tt> are mostly the same as ones accepted by the compiler driver
that have to do with compilation proper—but not always.
There are several gotchas.
Some driver arguments, like <tt>-c</tt>, are not accepted by the compiler proper,
as they would make no sense.
Some compiler-proper arguments are not understood by the driver;
one example is that
<tt>cc1</tt> as invoked by <tt>gcc</tt> is usually given an extra
argument <tt>-auxbase &lt;arg&gt;</tt> that the driver does not accept.
And finally,
some arguments have different semantics and/or different syntax
between the two.
For example, <tt>-MD</tt> takes no arguments as an option to the driver
but
it <em>does</em> take an argument when given to
<tt>cc1</tt> as invoked by <tt>gcc</tt> (it&#39;s like <tt>-MD -MF</tt>).
The wrapper script needs to handle all this nastiness,
so that tools don&#39;t have to.
It contains a scanner for <tt>cc1</tt> options,
generated by the same scraping method as before,
and it contains some logic for rewriting specific options
such as <tt>-MD</tt>.
More generally, we can think of the rewriting it does as “lifting” back to a driver command.</p>

<h3>Providing tools with a “lifted” driver-like command interface</h3>

<!--
We will have more than one tool, but want only one wrapper script.
So what should the tool receive as its arguments?
For example, 
suppose we want to wrap preprocessing.
We know that compiler drivers don't call <tt>cpp</tt> any more.
Using the hacks above, we can make them call <tt>cc1</tt> twice.
But owing to the aforementioned nastiness, 
we don't want our tools to emulate <tt>cc1</tt>.
What command-line interface should a tool expose?
-->

<p>We&#39;ve deciding that giving our own tools a <tt>cc1</tt>-style command line is not generic enough.
In the case of preprocessing,
we might therefore think instead about lifting to a <tt>cpp</tt> invocation.
<tt>cpp</tt> is still a vaguely standard tool
with a documented interface.
It seems appealing that our wrapper could translate to this from the <tt>cc1</tt> command,
and our tool to emulate <tt>cpp</tt>.
I went a long way down that road, but it was a mistake.
Things start going wrong when we consider: which <tt>cpp</tt> should we emulate?
The tool will probably run this “real” <tt>cpp</tt>,
with somehow modified input and/or modifying its output.
<em>Which</em> <tt>cpp</tt> is the real <tt>cpp</tt>?
Since a modern compiler never actually calls <tt>cpp</tt> at all,
guesswork is required.
There can well be many versions of <tt>cpp</tt> on the system.
Since older ones will have an earlier default C standard, we
        may get subtly wrong preprocessing if we choose wrongly.</p>

<p>There is a real example here, of a rather self-applicative kind.
Older versions of GCC&#39;s own source code contain token sequences that look like <tt>U&#34;somestring&#34;</tt>,
where <tt>U</tt> is supposed to get macro-expanded.</p>

<pre>cpphash.h:
#define U (const U_CHAR *)  /* Intended use: U&#34;string&#34; */
</pre>

<p>In cpplex.c and cpplib.c there are uses of this form.
These files are supposed to be compiled with a C99 or earlier compiler.</p>

<p>In more recent preprocessors, this token sequence is interpreted as a single Unicode literal
token, causing a downstream error when doing compilation proper (of the compiler).</p>

<pre>$ echo &#39;U&#34;blah&#34;&#39; | cpp-4.9 -DU=X -
# 1 &#34;&lt;stdin&gt;&#34;
# 1 &#34;&lt;built-in&gt;&#34;
# 1 &#34;&lt;command-line&gt;&#34;
# 1 &#34;/usr/include/stdc-predef.h&#34; 1 3 4
# 1 &#34;&lt;command-line&gt;&#34; 2
# 1 &#34;&lt;stdin&gt;&#34;
X &#34;blah&#34;

$ echo &#39;U&#34;blah&#34;&#39; | cpp -DU=X -
# 1 &#34;&lt;stdin&gt;&#34;
# 1 &#34;&lt;built-in&gt;&#34;
# 1 &#34;&lt;command-line&gt;&#34;
# 1 &#34;/usr/include/stdc-predef.h&#34; 1 3 4
# 1 &#34;&lt;command-line&gt;&#34; 2
# 1 &#34;&lt;stdin&gt;&#34;
U&#34;blah&#34;

$ echo &#39;U&#34;blah&#34;&#39; | cpp -DU=X -
# 1 &#34;&lt;stdin&gt;&#34;
# 1 &#34;&lt;built-in&gt;&#34;
# 1 &#34;&lt;command-line&gt;&#34;
# 1 &#34;/usr/include/stdc-predef.h&#34; 1 3 4
# 1 &#34;&lt;command-line&gt;&#34; 2
# 1 &#34;&lt;stdin&gt;&#34;
U&#34;blah&#34;
</pre>

<p>The change was between C99 and C11. Older versions of GNU CPP have a pre-C11 default standard,
but newer ones use C11 and will mispreprocess the above code.
How can we know to select the older <tt>cpp</tt> as our underlying <tt>cpp</tt>, in only
these cases?
We could at best resort to substitutions on the driver&#39;s executable
name, like <tt>s/gcc/cpp/</tt> or <tt>s/clang/clang-cpp/</tt>,
(while making sure we pass along <tt>-std=</tt> if it&#39;s present).
But that is not reliable.</p>

<p>Instead I&#39;ve abandoned the idea of lifting to <tt>cpp</tt>.
Instead I lift uniformly to <em>single-function driver invocations</em>,
    i.e. <em>either</em> a <tt>cc -E</tt> command or a
    <tt>cc -S</tt> command, or one that just runs the assembler,
    or one that just runs the linker.
Whereas we might not know which <tt>cpp</tt> to run,
we <em>do</em> always know which driver to run,
    because we, the wrapper script, are called by a specific driver.
(In practice, recovering this still gets a little ugly: one way is to scrape the parent PID&#39;s command line
from Linux&#39;s <tt>/proc</tt> or similar.
We also accept it explicitly, via the <tt>CC_DRIVER</tt> variable,
to stop our script becoming an untestable Heisenmess.)</p>

<p>This approach means our wrapper script has to lift <tt>cc1</tt>&#39;s special options
back into the driver&#39;s command-line language.
For example, we have to translate <tt>cc1</tt>&#39;s non-standard
<tt>-MD</tt> option back into the <tt>-MD -MF</tt> that is the driver&#39;s
        interface for this purpose.
We&#39;ll come to this shortly.</p>

<h3>Digression: <tt>cc -E</tt> is not <tt>cpp</tt>!</h3>

<p>Even <tt>make</tt> makes this conflation: GNU Make on my system
defaults <tt>CPP</tt> to <tt>cc -E</tt>.
But in general a system&#39;s C preprocessor can and does take arguments
that the compiler driver does not.
That is the entire motivation for <tt>-Xpreprocessor</tt> and <tt>-Wp</tt>, after all.
However, the real smoking gun that shows the command lines are
not compatible is <tt>infile outfile</tt>:
the preprocessor lets you name the output file directly on the command line, without <tt>-o</tt>.
One could easily imagine writing a <tt>%.i: %.c</tt> Make rule
that tried using <tt>CPP</tt> with this <tt>infile outfile</tt>
syntax, and then getting very confused about why it didn&#39;t work.</p>

<h3>Not just <tt>cc1</tt>...</h3>

<p>There is the assembler, of course, which is
straightforward.
But at least with GCC, linking is less straightforward:
it is done via a non-standard <tt>collect2</tt> tool,
which exists to support constructors and destructors
(not supported natively by many loaders).
It can end up running the real linker twice.
However, it is a very thin wrapper around the real linker and
its command line is very nearly the same.
It also helpfully makes it easy to override its choice of real linker,
so we can easily supply a wrapper <tt>ld</tt> rather than a wrapper <tt>collect2</tt>.
I&#39;ll skip details of this here.</p>

<h3>Expanding the command language, while avoiding in-tool scanning</h3>

<p>Suppose our tool is a replacement preprocessor—or, as we&#39;ve covered,
a replacement for <tt>cc -E</tt>.
Internally, it calls a “real” preprocessor somehow.
So there are now two “preprocessors”: the real one and our tool that wraps it.</p>

<p>We might want there to be options that our tool takes but the real preprocessor doesn&#39;t, and vice-versa.
For example, my <tt>cilpp</tt> is a preprocessor that supports plugins for source-to-source transformation after
“normal” preprocessing, using a <tt>-plugin</tt> option that no ordinary preprocessor supports.
But also, there&#39;s a ton of options we might want, orthogonally, to pass to the underlying preprocessor,
such as <tt>-dD</tt> which will leave the effective <tt>#define</tt> directives
in the preprocessed output, and which is handy for debugging.</p>

<p>So, the command-line language that our tool must accept is some kind of composite.
This is the stronger version of the “generic” “script does the scanning” problem
we set for ourselves earlier.
We don&#39;t want our tool to have to reimplement scanning of the “standard”
command lines,
a tool may want to extend that command-line language.
How can we arrange that the tool itself doesn&#39;t have to replicate all that painful precise scanning logic internally?
We know how important it is to be able to scan command lines precisely,
in order to identify options and input files accurately.</p>

<p>Since we can&#39;t bring the standard scanning to the tool, our answer is:
take the tool&#39;s scanning to the same place as the standard scanning, i.e. to our
overall wrapper script.
We make our tool declare which options it&#39;s interested in,
and we have our wrapper script scan them for it.</p>

<p>This is plumbed through by environment variables: the tool sets <tt>CC_IDENTIFY_ARGS</tt>
with a vaguely <tt>getopt</tt>-like spec (but uglier) of the options it cares about.
The wrapper
script responds by setting <tt>CC_IDENTIFIED_ARGS</tt>, at the time the tool is invoked,
to encode the positions of these arguments on the command line.
The tool then doesn&#39;t need to parse its own command line; it can just look at
this environment variable, since the work has already been done.
The tool might additionally make a best-effort attempt at scanning the command line, since
invoking it directly may be convenient. (But it should give up, with a warning,
when it sees an option it doesn&#39;t understand, because precise scanning is
impossible from that point onwards.
Clang tools take the approach of requiring their arguments to be left of a <tt>--</tt>, with
general compiler arguments to the right.
That is similar to what I&#39;m doing here, but results in a narrower interface.
My demo <tt>cilpp</tt>, that I&#39;ll present shortly,
also accepts <tt>--</tt> with similar meaning, i.e. the tool can conclude
“no more options for me past this point” and stop scanning, much as if it had
seen any other unrecognised argument but without issuing a warning.)</p>

<p>(How does a tool like <tt>cilpp</tt> set up <tt>CC_IDENTIFY_ARGS</tt> before the wrapper is run, as it must do?
If the wrapper hasn&#39;t run yet, surely the tool itself hasn&#39;t run yet?
Well, we have to change that.
In fact we might already have reason to invoke the compiler with help from some custom tool, maybe
as something like <tt>cc `my-tool-cflags`</tt>,
as a shorthand for <tt>cc -wrapper my-wrapper -no-integrated-cpp</tt>.
That seems useful, except that
obviously, in this scenario a <tt>my-tool-cflags</tt> script will be powerless to
set the environment variables for the <tt>cc</tt> command itself, and hence also cannot influence the
environment of the wrapper script that it invokes.
I thought about making our compiler invocation something like
<tt>-wrapper &#34;env CC_IDENTIFY_ARGS=optspec /path/to&#34;</tt>
but this also doesn&#39;t work: <tt>-wrapper</tt> is not subject to word-splitting.
(ACTUALLY: as the <tt>gdb</tt> example above shows,
it <em>is</em> subject to comma-splitting! I missed that, so the following is probably unnecessary.)
Instead we have to introduce another level of indirection:
our tool provides its own wrapper tool, that 
sets <tt>CC_IDENTIFY_ARGS</tt> and then runs the generic wrapper
(found using <tt>WRAPPER</tt> or by pathname guesswork).</p>

<h3>Recapping what we&#39;ve achieved</h3>

<p>The net effect of all the above is that using a command like</p>

<pre>cc -wrapper /path/to/tool/wrapper ...
</pre>

<p>we can extend the behaviour of the compiler,
as a black-box intervention around each of its subcommands
(preprocess, compile, assemble, link),
while also extending its command-line option syntax.
We do this in way that <tt>tool/wrapper</tt> defines,
but with the argument processing done by a generic underlying
<tt>wrapper</tt> script that <em>many</em> such tools can share.</p>

<h3>Lifting in the presence of tool-specific tool-directed options</h3>

<p>We mentioned that our wrapper script must “lift” from
<tt>cc1</tt> compiler-private command lines back to
<tt>cc</tt> “driver” command lines, that are relatively more documented and standardised.
There are two problems.
Firstly, how do we translate the options in general?
Some are not understood by the driver (e.g. <tt>-auxbase</tt>); what should we do with those?
Secondly, how does this interact with options like <tt>-Xpreprocessor</tt> or <tt>-Wp,...</tt>,
whose role is to pass options <em>through</em>
to preprocessors, assemblers and the like?
Of course there are also <tt>-Xassembler</tt> or <tt>-Wa,...</tt> and
<tt>-Xlinker</tt> or <tt>-Wl,...</tt> (but, curiously, no <tt>-Xcompiler</tt>)
but this lifting is only necessary for <tt>-Xpreprocessor</tt> because
there is no <tt>cc1</tt> equivalent for the assembler or linker.
(But see above about <tt>collect2</tt>.)</p>

<p>This “tool-directed options” feature
complements our previous idea of extending the command-line syntax
in a per-tool fashion.
Our extended preprocessor might support a special option, and
we&#39;d use <tt>-Xpreprocessor</tt> to pass it through.</p>

<p>The problem is that when we see a <tt>cc1</tt> command, the <tt>-Xpreprocessor</tt> options and the like
have already been stripped out by the driver.
If we are to lift the <tt>cc1</tt> command back up to a driver command,
we need to reinstate them somehow.
Otherwise we will pass the driver <tt>cc1</tt>- or tool-specific options in unqualified form,
and it will choke.
That creates some guesswork: how do we know which <tt>cc1</tt> options need this treatment?
As a pathological example, suppose our driver command is</p>

<pre>cc -wrapper mywrapper -Xpreprocessor -blah -o -blah blah.c
</pre>

<p>... where we are asking the compiler to output a file called <tt>-blah</tt>.
When we get the <tt>cc1</tt> preprocessing command it will say something including (leaving out the usual <tt>cc1</tt> noise)</p>

<pre>cc1 -E -blah -o -blah blah.c
</pre>

<p>... and we know that we want to lift this back to:</p>

<pre>cc -E -Xpreprocessor -blah -o -blah blah.c
</pre>

<p>... but it&#39;s not as straightforward as always prefixing our known-special option
<tt>-blah</tt> with <tt>-Xpreprocessor</tt>.
Again, we&#39;re back to precise scanning.
Firstly we assume we can scan the driver command itself precisely.
Secondly we assume that only options, not input files, make the driver choke, and that we can identify these
as hyphen-prefixed words at the head of a scanned chunk (but not a plain <tt>-</tt> by itself, which is a filename).
In short, any option we see with <tt>cc1</tt> that appeared at the head of a chunk
on the plain driver command, where that chunk was not qualified
by  <tt>-Xpreprocessor</tt>, we know should pass through fine.
Or else if it was so qualified,
we add the qualifier back (perhaps many times—it needs repeating it before any argument
that follows within the chunk).</p>

<p>What if the <tt>cc1</tt> option just didn&#39;t appear in the driver command?
This is a common case!
For example, GCC&#39;s <tt>cc1</tt> options <tt>-quiet</tt> and <tt>-auxbase</tt> are not understood by the driver.
They were <em>generated by</em>
the driver, but we don&#39;t know which of the driver&#39;s options caused them to be generated.
This is the nastiest semantic issue we face.
We have a few choices.
If preprocessing, we could conservatively always prefix <tt>-Xpreprocessor</tt>
since we know that&#39;ll make it come through to <tt>cc1</tt> again.
Or to be more refined,
we could use our scanner for driver&#39;s command-line options: if the option would
be recognised according to those, we let it through unprefixed, otherwise we prefix.
(This is still assuming the option means the same thing to
the driver as it does to <tt>cc1</tt>! So we must do this only after normalising <tt>-MD</tt> away.
We also risk introducing duplicates, if the driver re-generates another copy of the option
and that matters semantically. I&#39;m not currently aware of any issues of this kind.)</p>

<p>This prefixing is all very well, but we don&#39;t have an equivalent for compilation proper.
We could alternatively assume that the driver will <em>always</em> regenerate whatever
<tt>cc1</tt>-specific option we&#39;re seeing, so just drop it from the driver command we build.
That is unsafe in general but it&#39;s what I currently do.
(For preprocessing, the “refined” version described above is probably
the right fix.
Then to address the lack of <tt>-Xcompiler</tt>
we could write a “sub-wrapper”, used only while doing our own single-function
compilation-proper driver invocations.
The sub-wrapper would slurp extra options from an environment variable.
It could add them back in a “no duplicates” way, so that if the
driver created the option of its own accord, it won&#39;t be added twice.
The main wrinkle is some guesswork about when this no-duplication semantics is necessary.)</p>

<h3>Tying it all together</h3>

<p>The <tt>-wrapper</tt> and <tt>-###</tt> interfaces follow a depressingly familiar pattern:
they are extension interfaces that are undoubtedly
“useful”, but impossible to use reliably, because
using them needs too much change-prone, implementation-specific knowledge.
This built-in unreliability could largely be solved if GCC and Clang provided two things:
firstly, online help that is complete with respect to their command line and intentionally machine-readable,
and secondly, instead of ad-hoc options like <tt>-###</tt> or <tt>-no-integrated-cpp</tt>,
a systematic interface for boiling down their command-line complexity
by normalisation—possibly as an extension to the machine-readable interface description,
and possibly based on the exact “single-function driver invocations” idea
that I&#39;ve motivated here.
Currently, thanks to the opaque mapping between driver and
<tt>cc1</tt> options, and the lack of <tt>-Xcompiler</tt>, I could only implement it modulo various hacks and scripted
guesswork.
(In fairness, GCC does have <a href="https://gcc.gnu.org/onlinedocs/gcc/Spec-Files.html">spec strings</a>
which go some way to
declaratively specifying how the driver generates subcommands.
I could possibly read these to eliminate some of the guesswork in my approach; for now it
doesn&#39;t seem worth it, especially as I&#39;m not aware of a Clang equivalent.)</p>

<p>For now, packaging this change-prone nastiness all up in one somewhat change-prone
script is about the best we can do.
My own script is in my <a href="https://github.com/stephenrkell/toolsub">toolsub repository</a>&#39;s <a href="https://github.com/stephenrkell/toolsub/tree/master/wrapper"><tt>wrapper</tt> subdirectory</a>
and there exists a minimal-ish demo of how to use it in the <tt>cilpp</tt> tool in the same repository.
<tt>cilpp</tt> is a replacement preprocessor that accepts CIL source-to-source passes as plugins.
Since it has its own command-line options for identifying these plugins,
it demos the “identified arguments” facility,
and has its own “outermost wrapper” that supplies the necessary <tt>CC_IDENTIFY_ARGS</tt>
configuration. As we saw above, this allows the wrapper script to do the scanning;
the <tt>cilpp</tt> tool now does not need to embed a full command-line scanner,
an improvement of earlier versions which did
attempt this and were found to be somewhat <a href="https://github.com/stephenrkell/toolsub/issues/7">bug-prone</a>.</p>

<p>I thought it important that my wrapper be a script, not a
compiled program—given its imperfections, I expected users to have to hack it from time to time,
so I want to make that as immediate as possible.
The downside is that my script is surprisingly slow:
on my (old) laptop it adds something like one second (!) per complete
preprocess-compile-assemble-link invocation.
That is pretty bad, although initial versions were much worse.
I gained some speed by delegating the command-scanning to
generated <tt>awk</tt> code (rather than generated shell code),
more by being careful to avoid quadratic logic that re-scanned the entire argument list
to process one argument (<tt>bash</tt> is such that even on a two-dozen-element list
you can really notice these),
and some more by allowing some <tt>bash</tt>isms while being careful about the
patterns I used (associative arrays are handy; piping into <tt>while read</tt> loops is mostly to be avoided).
That has got me to the point where more time is spent in <tt>awk</tt> than in <tt>bash</tt>, at least.
Still, I would like to get time to investigate why <tt>bash</tt> is so amazingly slow at certain things.</p>

<p>A final thought: a compiled <tt>wrapper</tt> program might not be any less hackable
if it is automatically re-made when new changes are applied.
Currently our <tt>awk</tt> code, generated from compiler help text,
is effectively cached in the wrapper script file itself,
but can be easily be remade—manually at present,
but perhaps automatically triggerable somehow.
If we can trigger this sensibly, then hackability
doesn&#39;t depend on being an interpreted script after all.
Why can&#39;t the wrapper be a <em>self-remaking</em> C program?
One could view this as a poor person&#39;s <a href="https://dl.acm.org/doi/10.1145/3276494">output-caching JIT</a>—and
possibly, on reflection, with less of the “poor”.
Is there a principled way to have self-remaking programs that is not too disruptive
to the established toolchain model?
There are some related tricks going on in Justine Tunney&#39;s <a href="https://justine.lol/ape.html">“actually portable executables”</a> (sorry, not doing the
Greek),
in Fabrice Bellard&#39;s <a href="https://bellard.org/tcc/"><tt>tcc -run</tt> hash-bang lines</a>,
and in my own <a href="https://www.humprog.org/%7Estephen/blog/2024/01/02#make-shell">shell–Make hybrids</a>.
Could we imagine a distribution, perhaps vaguely Nix-like, where all software is self-remaking by default?
How can we hide the latency of rebuilding a large program?
Can we tweak how ABIs work to enable builds to be made more incremental?
This and other kinds of “live programming by self-remaking” are
a topic I&#39;d like to explore further.</p>

<!-- Leaving these notes-to-self (or to scarily dedicated readers) here for now:
"lazily self-remaking" is a bit like JITting, especially
if we add "input-adaptively" to that.
The program becomes its own build system, which is a big change.
Or is it?
If re-making involves re-running the build system, then no.
If it involves, say, lazily JITting the new binary as we execute,
    to hide latency, and blatting it out on exit, maybe using libdlbind,
    then that is a big departure from most existing build systems -- maybe not Zig's funky linker.
I still need to write down my 
"principled basis" for thinking about software build and deployment, of course.... 

The obvious problem with self-remaking is the "multi-user deployment model"
of Unix et al, i.e. a user does not have write access to the programs they run.
With the exception of setuid programs,
this is trivial to work around because a user can always take a copy.
We could imagine an "overlay invariant":
we allow multi-user deployment as a cache
but every directory in the user's requested PATH (say /usr/bin:/bin)
has a mutable shadow in the effective PATH, i.e. the effective path is really /shadow/usr/bin:/usr/bin:/shadow/bin:/bin
and we hook the necessary execlp, execvp etc. in order to achieve this.
Or perhaps: just require PATH to be set up this way?
i.e. that any dir in PATH may contain only user-writable executables.
Then we can provide an automated way to generate such a PATH,
starting from a vanilla path, via shadowing / overlayfs.
If we "just do this", it doesn't guarantee persistence of changes, in the sense that
the shadow directories may not stick around on the user's PATH.
So the invariant seems the way to go -- then if we have to deploy the workaround
we can be complainy about it.

Probably the invariant is something like:

- for any dir in PATH that contains any not-writable-by-this-user-but-executable-by-them file,
   it must be preceded immediately on the PATH by a "shadow dir"
      which is one whose filenames form a subset of the shadowed dir
         but where all these files *are* executable-and-writable-by-this-user
   ... and then we can use an overlay file system to always be able to construct such a path
    (but we do explicitly construct it)
   ... how do we make the "all files" check efficient?
         if a path denotes a directory in the overlay fs, we can assume the property by construction
         (i.e. the overlay itself enforces the invariant)

         so an efficient path would be pairs
            /overlay/upper/xyz:/overlay/lower/xyz
         where ^- this is the shadow  ^- and this just maps to /usr/bin or whatever

         but we could also have
            /usr/bin
         and it would get rewritten to
            /overlay/shadow/usr/bin:/usr/bin
         following an expensive check over /usr/bin
         which we can skip if we see something under /overlay
            i.e. just knowing that /overlay is a mount of our trusted invariant-enforcing filesystem.

How can we have shared libraries in such a system?

-->
<p>
<i>[<a href="https://www.humprog.org/%7Estephen/blog/devel">/devel</a>] 
<a href="https://www.humprog.org/%7Estephen/blog/2024/08/27#how-to-wrap-cc-really">permanent link</a>
<a href="https://www.humprog.org/~stephen/#contact">contact</a>
</i>
</p>
<hr/>
<p><a href="http://blosxom.sourceforge.net/"><img src="https://www.humprog.org/~stephen/blog/pb_blosxom.gif" alt="Powered by blosxom"/></a></p><p>
<span>
<a href="http://validator.w3.org/check?uri=referer">validate this page</a>
</span>
</p>
</div></div>
  </body>
</html>
