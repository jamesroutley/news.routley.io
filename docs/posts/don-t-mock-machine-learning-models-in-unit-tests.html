<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eugeneyan.com/writing/unit-testing-ml/">Original</a>
    <h1>Don&#39;t Mock Machine Learning Models In Unit Tests</h1>
    
    <div id="readability-page-1" class="page"><div>



<p>I’ve been applying typical unit testing practices to machine learning code and it hasn’t been straightforward. In software, units are small, isolated pieces of logic that we can test independently and quickly. In machine learning, models are blobs of logic learned from data, and machine learning code is the logic to learn and use these derived blobs of logic. This difference makes it necessary to rethink how we unit test machine learning code.</p>
<h2 id="how-ml-code-differs-from-regular-software">How ML code differs from regular software</h2>
<p><strong>In software, we write code that <em>contains</em> logic; in ML, we write code that <em>learns</em> logic and then uses that learned logic.</strong> Software code transforms input data + handcrafted logic into expected output. We can then test these outputs against asserts. In contrast, machine learning code transforms input data + expected output into learned logic (i.e., a model).</p><p>
\[\text{Software}: \text{Input Data} + Handcrafted \text{ Logic} = \text{Expected Output}\]
\[\text{Machine Learning}: \text{Input Data} + \text{Expected Output} = Learned \text{ Logic}\]
</p><p>Thus, in machine learning, instead of writing code that contains logic, we write code to learn logic, such as via <a href="https://github.com/eugeneyan/testing-ml/blob/master/src/tree/decision_tree.py#L149" target="_blank">building a decision tree</a> or <a href="https://github.com/eugeneyan/visualizing-finetunes/blob/main/3_ft_usb_then_fib.ipynb" target="_blank">finetuning a hallucination classifier</a>. Because the logic that acts on the input data is embedded within the model, if we want to test the learned logic, we’ll need to load the model, perform inference on some sample output, and then assert if the output matches the expected input.</p>
<p><strong>In software, we typically mock dependencies like APIs; in ML, we want to test the actual model (sometimes).</strong> When unit testing software, it’s good practice to mock database calls, filesystem access, sending emails/push notifications, etc. In ML, there are scenarios where we’ll want to test against the actual model.</p>
<p>For example, we want to test that loss decreases with each batch and the model can overfit (before wasting compute on an hopeless run.) If the model is a classifier, we want to check that the inference logic is correct. For instance, two models may have different output classes: <a href="https://huggingface.co/google/t5_11b_trueteacher_and_anli" target="_blank">Google’s T5 NLI model</a> classifies factual consistency with class = 1 while <a href="https://huggingface.co/facebook/bart-large-mnli" target="_blank">Meta’s BART NLI model</a> classifies it with class = 2!</p>
<p><strong>Machine learning / language models can be large and unwieldy.</strong> Some neural networks can be in the billions of parameters, exceeding what a laptop or standard dev environment can load. And even if we have the memory for smaller models, they are slow to load and perform inference on, testing our patience as we unit test while coding.</p>
<h2 id="some-guidelines-for-unit-testing-ml-code--models">Some guidelines for unit testing ML code &amp; models</h2>
<p>(These are a work in progress and my thinking’s still evolving—all feedback welcome!)</p>
<p><strong>Use small, simple data samples.</strong> Avoid loading CSVs or Parquet files as sample data. (It’s fine for evals but not unit tests.) Define sample data directly in unit test code to test key functionality such as:</p>
<ul>
<li>Splitting into train/test tests when you have custom logic</li>
<li>Custom implementations, such as Cosine or Euclidean distance in Java</li>
<li>Preprocessing such as data augmentation or encoding</li>
<li>Postprocessing such as diversification or filtering recommendations</li>
<li>Error handling for empty or malformed input</li>
</ul>
<p><strong>When viable, test against random or empty weights.</strong> For example, we can initialize a model configuration with random weights to test output shape and device movement (from CPU to GPU and back). Here’s an example of how to initialize a model without having to download the weights and then assert the output shape:</p>
<div><div><pre><code><span>from</span> <span>transformers</span> <span>import</span> <span>AutoConfig</span><span>,</span> <span>AutoModelForSequenceClassification</span>

<span>model_name</span> <span>=</span> <span>&#34;valhalla/distilbart-mnli-12-1&#34;</span>
<span>config</span> <span>=</span> <span>AutoConfig</span><span>.</span><span>from_pretrained</span><span>(</span><span>&#34;valhalla/distilbart-mnli-12-1&#34;</span><span>)</span>
<span>model</span> <span>=</span> <span>AutoModelForSequenceClassification</span><span>.</span><span>from_config</span><span>(</span><span>config</span><span>)</span>
<span>assert</span> <span>model</span><span>.</span><span>classification_head</span><span>.</span><span>out_proj</span><span>.</span><span>out_features</span> <span>==</span> <span>3</span>
</code></pre></div></div>
<p>The accelerate library also has <a href="https://github.com/huggingface/accelerate/blob/main/tests/test_big_modeling.py#L955" target="_blank">an example of initializing a model with empty weights</a>:</p>
<div><div><pre><code><span>def</span> <span>test_dispatch_model_bnb</span><span>(</span><span>self</span><span>):</span>
    <span>&#34;&#34;&#34;Tests that `dispatch_model` quantizes int8 layers&#34;&#34;&#34;</span>
    <span>from</span> <span>huggingface_hub</span> <span>import</span> <span>hf_hub_download</span>
    <span>from</span> <span>transformers</span> <span>import</span> <span>AutoConfig</span><span>,</span> <span>AutoModel</span><span>,</span> <span>BitsAndBytesConfig</span>
    <span>from</span> <span>transformers.utils.bitsandbytes</span> <span>import</span> <span>replace_with_bnb_linear</span>

    <span>with</span> <span>init_empty_weights</span><span>():</span>
        <span>model</span> <span>=</span> <span>AutoModel</span><span>.</span><span>from_config</span><span>(</span><span>AutoConfig</span><span>.</span><span>from_pretrained</span><span>(</span><span>&#34;bigscience/bloom-560m&#34;</span><span>))</span>

    <span>quantization_config</span> <span>=</span> <span>BitsAndBytesConfig</span><span>(</span><span>load_in_8bit</span><span>=</span><span>True</span><span>)</span>
    <span>model</span> <span>=</span> <span>replace_with_bnb_linear</span><span>(</span>
        <span>model</span><span>,</span> <span>modules_to_not_convert</span><span>=</span><span>[</span><span>&#34;lm_head&#34;</span><span>],</span> <span>quantization_config</span><span>=</span><span>quantization_config</span>
    <span>)</span>

    <span>model_path</span> <span>=</span> <span>hf_hub_download</span><span>(</span><span>&#34;bigscience/bloom-560m&#34;</span><span>,</span> <span>&#34;pytorch_model.bin&#34;</span><span>)</span>

    <span>model</span> <span>=</span> <span>load_checkpoint_and_dispatch</span><span>(</span>
        <span>model</span><span>,</span>
        <span>checkpoint</span><span>=</span><span>model_path</span><span>,</span>
        <span>device_map</span><span>=</span><span>&#34;balanced&#34;</span><span>,</span>
    <span>)</span>

    <span>assert</span> <span>model</span><span>.</span><span>h</span><span>[</span><span>0</span><span>].</span><span>self_attention</span><span>.</span><span>query_key_value</span><span>.</span><span>weight</span><span>.</span><span>dtype</span> <span>==</span> <span>torch</span><span>.</span><span>int8</span>
    <span>assert</span> <span>model</span><span>.</span><span>h</span><span>[</span><span>0</span><span>].</span><span>self_attention</span><span>.</span><span>query_key_value</span><span>.</span><span>weight</span><span>.</span><span>device</span><span>.</span><span>index</span> <span>==</span> <span>0</span>

    <span>assert</span> <span>model</span><span>.</span><span>h</span><span>[(</span><span>-</span><span>1</span><span>)].</span><span>self_attention</span><span>.</span><span>query_key_value</span><span>.</span><span>weight</span><span>.</span><span>dtype</span> <span>==</span> <span>torch</span><span>.</span><span>int8</span>
    <span>assert</span> <span>model</span><span>.</span><span>h</span><span>[(</span><span>-</span><span>1</span><span>)].</span><span>self_attention</span><span>.</span><span>query_key_value</span><span>.</span><span>weight</span><span>.</span><span>device</span><span>.</span><span>index</span> <span>==</span> <span>1</span>
</code></pre></div></div>
<p><strong>Write critical tests against the actual model.</strong> If they take a while to run, <a href="https://docs.pytest.org/en/latest/how-to/mark.html#registering-marks" target="_blank">mark them as slow</a> and run only when needed (e.g., pre-commit and pre-merge). Some essentials include:</p>
<ul>
<li>Verify training is done correctly, such as loss going down, model overfitting, and training till convergence on a small sample of data</li>
<li>Verify model outputs match expectation, such as 0.99 = unsafe instead of safe</li>
<li>Verify model server can start, take batch input, and return the expected output</li>
</ul>
<p><strong>Don’t test external libraries.</strong> We can assume that external libraries work. Thus, no need to test data loaders, tokenizers, optimizers, etc.</p>
<p>• • •</p>
<p>What are your best practices for unit testing machine learning code and models? I would love to hear from you. <a href="https://twitter.com/eugeneyan" target="_blank">Please reach out!</a></p>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="https://eugeneyan.com/writing/testing-ml/" target="_blank">How to Test Machine Learning Code and Systems</a></li>
<li><a href="https://eugeneyan.com/writing/testing-pipelines/" target="_blank">Writing Robust Tests for Data &amp; Machine Learning Pipelines</a></li>
<li><a href="https://www.jeremyjordan.me/testing-ml/" target="_blank">Effective Testing for Machine Learning Systems</a></li>
<li><a href="https://krokotsch.eu/posts/deep-learning-unit-tests/" target="_blank">How to Trust Your Deep Learning Code</a></li>
<li><a href="https://karpathy.github.io/2019/04/25/recipe/" target="_blank">A Recipe for Training Neural Networks</a></li>
<li><a href="https://theaisummer.com/unit-test-deep-learning/" target="_blank">How to Unit Test Deep Learning</a></li>
<li><a href="https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-testing/" target="_blank">Testing Data Science and MLOps Code</a></li>
</ul>
</div></div>
  </body>
</html>
