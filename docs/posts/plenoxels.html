<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alexyu.net/plenoxels/?s=09">Original</a>
    <h1>Plenoxels</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p id="paper-title">
            
            <h3>
                Radiance Fields without Neural Networks
            </h3>
            <!-- <h3 class="col-md-12 text-center">
                <small>arXiv</small>
            </h3> -->
        </p>

        
        
        <div>
            <div>
                <p>
                    <iframe src="https://www.youtube.com/embed/ElnuwpQEqTA" frameborder="0" allow="accelerometer; autoplay muted; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                </p>
                <p>
                    We propose a view-dependent sparse voxel model, Plenoxel <em>(plenoptic volume element)</em>, that can optimize to the same fidelity as <a href="http://tancik.com/nerf">Neural Radiance Fields (NeRFs)</a>
                    without any neural networks. Our typical optimization time is 11 minutes on a single GPU, a speedup of two orders of magnitude compared to NeRF.
                </p>
            </div>
        </div>
        <div>
            <div>
                <p><img src="https://alexyu.net/plenoxels/img/pipeline.png" alt="Pipeline (figure 2)"/></p><p>
                    Given a set of calibrated images of an object or scene, we reconstruct a (a) sparse voxel (“Plenoxel”) grid with density and spherical harmonic coefficients at each voxel. To render a ray, we (b) compute the color and opacity of each sample point via trilinear interpolation of the neighboring voxel coefficients. We integrate the color and opacity of these samples using (c) differentiable volume rendering, following the recent success of NeRF. The voxel coefficients can then be (d) optimized using the standard MSE reconstruction loss relative to the training images, along with a total variation regularizer.
                </p>

            </div>
        </div>
        
        <div>
            <div>
                <h4>Superfast Convergence</h4>
                <p>
                    Our method converges rapidly. We reach comparable metrics (PSNR) 100x faster than NeRF, and achieve reasonable results within a few seconds of optimization.
                </p>
                <video controls="" muted="">
                    <source src="https://angjookanazawa.com/plenoxel_data/fastopt.mp4" type="video/mp4"/>
                </video>
            </div>
        </div>
        <div>
            <div>
                <h4>Results on Forward-facing Scenes</h4>
                <p>
                    Using NeRF&#39;s NDC parameterization, we are able to approximately match NeRF&#39;s results on forward-facing scenes as well.
                </p>
                <video controls="" autoplay="" muted="" loop="">
                    <source src="https://angjookanazawa.com/plenoxel_data/eight.mp4" type="video/mp4"/>
                </video>
            </div>
        </div>
        <div>
            <div>
                <h4>Results on 360° Scenes</h4>
                <p>
                    We extend the method to 360° real scenes using a background model based on multi-sphere images (MSI), similar to the approach used in <a href="https://arxiv.org/abs/2010.07492">NeRF++</a>.
                </p>
                
            </div>
        </div>
        <div>
            <div>
                <h4>Background / Foreground</h4>
                <p>
                    The following is a capture of a real Lego bulldozer. We show the background and foreground models separately.
                </p>
                
            </div>
        </div>
        <div>
            <div>
                <h4>Concurrent Works</h4>
                <p>
                    Please also check out <a href="https://arxiv.org/abs/2111.11215">DirectVoxGo</a>, a similar work which recently appeared on arXiv. They use a neural net to fit the color, but do not make use of TV regularization, SH, or sparse voxels. Additionally, their implementation does not require custom CUDA kernels.
                </p>
            </div>
        </div>
        <div>
            <div>
                <h4>Acknowledgements</h4>
                <p>
                    We note that Utkarsh Singhal and Sara Fridovich-Keil tried a related idea with point clouds some time prior to this project. Additionally, we would like to thank Ren Ng for helpful suggestions and Hang Gao for reviewing a draft of the paper.
                </p>
                <p>
                    The project is generously supported in part by the CONIX Research Center,
                    one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA; a Google research award to Angjoo Kanazawa; Benjamin Recht’s ONR awards N00014-20-1-2497 and N00014-18-1-2833, NSF CPS award 1931853, and the DARPA Assured Autonomy program (FA8750-18-C-0101).
                    Sara Fridovich-Keil and Matthew Tancik are supported by the NSF GRFP.
                </p>
                <p>
                    This website is in part based on a template of <a href="http://mgharbi.com/">Michaël Gharbi</a>, also used in <a href="https://alexyu.net/pixelnerf">PixelNeRF</a> and <a href="https://alexyu.net/plenoctrees">PlenOctrees</a>.
                </p>
            </div>
        </div>
    </div></div>
  </body>
</html>
