<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boringsql.com/posts/vacuum-is-lie/">Original</a>
    <h1>Vacuum Is a Lie: About Your Indexes</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>There is common misconception that troubles most developers using PostgreSQL:
tune VACUUM or run VACUUM, and your database will stay healthy. Dead tuples will
get cleaned up. Transaction IDs recycled. Space reclaimed. Your database will
live happily ever after.</p>
<p>But there are couple of dirty &#34;secrets&#34; people are not aware of. First of them
being <strong>VACUUM is lying to you about your indexes</strong>.</p>
<h2 id="the-anatomy-of-storage">The anatomy of storage<a href="#the-anatomy-of-storage" aria-label="Anchor link for: the-anatomy-of-storage"></a>
</h2>
<p>When you delete a row in PostgreSQL, it is just marked as a &#39;dead tuple&#39;.
Invisible for new transactions but still physically present. Only when all
transactions referencing the row are finished, VACUUM can come along and actually
remove them - reclamining the space in the heap (table) space.</p>
<p>To understand why this matters differently for tables versus indexes, you need
to picture how PostgreSQL actually stores your data.</p>
<p>Your table data lives in the heap - a collection of 8 KB pages where rows are
stored wherever they fit. There&#39;s no inherent order. When you INSERT a row,
PostgreSQL finds a page with enough free space and slots the row in. Delete a
row, and there&#39;s a gap. Insert another, and it might fill that gap - or not - they
might fit somewhere else entirely.</p>
<p>This is why <code>SELECT * FROM users</code> without an ORDER BY can return rows in order
initially, and after some updates in seemingly random order, and that order can
change over time. The heap is like Tetris. Rows drop into whatever space is
available, leaving gaps when deleted.</p>
<p><img src="https://brianagude.substack.com/images/posts/heap_page.png" alt="Heap Page"/></p>
<p>When VACUUM runs, it removes those dead tuples and compacts the remaining
rows within each page. If an entire page becomes empty, PostgreSQL can reclaim
it entirely.</p>
<p>And while indexes are on surface the same collection of 8KB pages, they are
different. A B-tree index must maintain sorted order - that&#39;s the
whole point of their existence and the reason why <code>WHERE id = 12345</code> is so
fast. PostgreSQL can binary-search down the tree instead of scanning every
possible row. You can learn more about the <a href="https://use-the-index-luke.com/sql/anatomy/the-tree">fundamentals of B-Tree Indexes and
what makes them fast</a>.</p>
<p>But if the design of the indexes is what makes them fast, it&#39;s also their
biggest responsibility. While PostgreSQL can fit rows into whatever space is
available, it can&#39;t move the entries in index pages to fit as much as possible.</p>
<p><img src="https://brianagude.substack.com/images/posts/leaf_page.png" alt="Leaf Page"/></p>
<p>VACUUM can remove dead index entries. But it doesn&#39;t restructure the B-tree.
When VACUUM processes the heap, it can compact rows within a page and reclaim
empty pages. The heap has no ordering constraint - rows can be anywhere. But
B-tree pages? They&#39;re locked into a structure. VACUUM can remove dead index
entries, yes.</p>
<p>Many developers assume VACUUM treats all pages same. No matter whether they are
heap or index pages. VACUUM is supposed to remove the dead entries, right?</p>
<p>Yes. But here&#39;s what it doesn&#39;t do - <strong>it doesn&#39;t restructure the B-tree</strong>.</p>
<p><strong>What VACUUM actually does</strong></p>
<ul>
<li>Removes dead tuple pointers from index pages</li>
<li>Marks completely empty pages as reusable</li>
<li>Updates the free space map</li>
</ul>
<p><strong>What VACUUM cannot do</strong>:</p>
<ul>
<li>Merge sparse pages together (can do it for empty pages)</li>
<li>Reduce tree depth</li>
<li>Deallocate empty-but-still-linked pages</li>
<li>Change the physical structure of the B-tree</li>
</ul>
<p>Your heap is Tetris, gaps can get filled. Your B-tree is a sorted bookshelf.
VACUUM can pull books out, but can&#39;t slide the remaining ones together. You&#39;re
left walking past empty slots every time you scan.</p>
<h2 id="the-experiment">The experiment<a href="#the-experiment" aria-label="Anchor link for: the-experiment"></a>
</h2>
<p>Let&#39;s get hands-on and create a table, fill it, delete most of it and watch what happens.</p>
<pre data-lang="sql"><code data-lang="sql"><span>CREATE EXTENSION IF NOT EXISTS pgstattuple;
</span><span>CREATE TABLE </span><span>demo</span><span> (id </span><span>integer PRIMARY KEY</span><span>, data </span><span>text</span><span>);
</span><span>
</span><span>-- insert 100,000 rows
</span><span>INSERT INTO</span><span> demo (id, data)
</span><span>SELECT</span><span> g, &#39;</span><span>Row number </span><span>&#39; || g || &#39;</span><span> with some extra data</span><span>&#39;
</span><span>FROM</span><span> generate_series(</span><span>1</span><span>, </span><span>100000</span><span>) g;
</span><span>
</span><span>ANALYZE demo;
</span></code></pre>
<p>At this point, our index is healthy. Let&#39;s capture the baseline:</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT
</span><span>    relname,
</span><span>    pg_size_pretty(pg_relation_size(</span><span>oid</span><span>)) as file_size,
</span><span>    pg_size_pretty((pgstattuple(</span><span>oid</span><span>)).tuple_len) as actual_data
</span><span>FROM</span><span> pg_class
</span><span>WHERE</span><span> relname IN (&#39;</span><span>demo</span><span>&#39;, &#39;</span><span>demo_pkey</span><span>&#39;);
</span></code></pre>
<pre><code><span>relname  | file_size | actual_data
</span><span>-----------+-----------+-------------
</span><span>demo      | 7472 kB   | 6434 kB
</span><span>demo_pkey | 2208 kB   | 1563 kB
</span></code></pre>
<p>Now remove some data, 80% to be precise - somewhere in the middle:</p>
<pre data-lang="sql"><code data-lang="sql"><span>DELETE FROM</span><span> demo </span><span>WHERE</span><span> id BETWEEN </span><span>10001 </span><span>AND </span><span>90000</span><span>;
</span></code></pre>
<p>The goal is to simulate a common real-world pattern: data retention policies,
bulk cleanup operations, or the aftermath of a data migration gone wrong.</p>
<pre><code><span>VACUUM demo;
</span><span>
</span><span>SELECT
</span><span>    relname,
</span><span>    pg_size_pretty(pg_relation_size(oid)) as file_size,
</span><span>    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
</span><span>FROM pg_class
</span><span>WHERE relname IN (&#39;demo&#39;, &#39;demo_pkey&#39;);
</span></code></pre>
<pre><code><span>relname  | file_size | actual_data
</span><span>-----------+-----------+-------------
</span><span>demo      | 7472 kB   | 1278 kB
</span><span>demo_pkey | 2208 kB   | 1563 kB
</span></code></pre>
<p>The table shrunk significantly, while index remained unchanged. You now have
20,000 rows indexed by a structure build to handle 100,000. Please, also notice
<code>file_size</code> remain unchanged. VACUUM doesn&#39;t return space to the OS, it only
marks pages as reusable within PostgreSQL.</p>
<p>This experiment is really an extreme case, but demonstrates the problem.</p>
<h2 id="understanding-page-states">Understanding page states<a href="#understanding-page-states" aria-label="Anchor link for: understanding-page-states"></a>
</h2>
<p>Leaf pages have several states:</p>
<p><strong>Full page (&gt;80% density)</strong>, when the page contains many index entries,
efficiently utilizing space. Each 8KB page read returns substantial useful data.
This is optimal state.</p>
<p><strong>Partial page (40-80% density)</strong> with some wasted space, but still reasonably
efficient. Common at tree edges or after light churn. Nothing to be worried about.</p>
<p><strong>Sparse page (&lt;40% density)</strong> is mostly empty. You&#39;re reading an 8KB page to
find a handful of entries. The I/O cost is the same as a full page, but you get
far less value.</p>
<p><strong>Empty page (0% density)</strong> with zero live entries, but the page still exists in
the tree structure. Pure overhead. You might read this page during a range scan
and find absolutely nothing useful.</p>
<h3 id="a-note-on-fillfactor">A note on fillfactor<a href="#a-note-on-fillfactor" aria-label="Anchor link for: a-note-on-fillfactor"></a>
</h3>
<p>You might be wondering how can fillfactor help with this? It&#39;s the setting you
can apply both for heap and leaf pages, and controls how full PostgreSQL packs the
pages during the data storage. The <strong>default value for B-tree indexes is 90%</strong>. This
leaves 10% of free space on each leaf page for future insertions.</p>
<pre data-lang="sql"><code data-lang="sql"><span>CREATE INDEX </span><span>demo_index ON </span><span>demo</span><span>(id) </span><span>WITH</span><span> (fillfactor = </span><span>70</span><span>);
</span></code></pre>
<p>A lower fillfactor (like 70%) leaves more room, which can reduce page splits
when you&#39;re inserting into the middle of an index - useful for tables random index
column inserts or those with heavily updated index columns.</p>
<p>But if you followed carefully the anatomy of storage section, it doesn&#39;t help
with the bloat problem. Quite the oppossite. If you set lower fillfactor and then
delete majority of your rows, you actually start with more pages, and bigger
chance to end up with more sparse pages than partial pages.</p>
<p>Leaf page fillfactor is about optimizing for updates and inserts. It&#39;s not a
solution for deletion or index-column update bloat.</p>
<h2 id="why-the-planner-gets-fooled">Why the planner gets fooled<a href="#why-the-planner-gets-fooled" aria-label="Anchor link for: why-the-planner-gets-fooled"></a>
</h2>
<p>PostgreSQL&#39;s query planner estimates costs based on physical
statistics, including the number of pages in an index.</p>
<pre data-lang="sql"><code data-lang="sql"><span>EXPLAIN ANALYZE </span><span>SELECT </span><span>* </span><span>FROM</span><span> demo </span><span>WHERE</span><span> id BETWEEN </span><span>10001 </span><span>AND </span><span>90000</span><span>;
</span></code></pre>
<pre><code><span>QUERY PLAN
</span><span>--------------------------------------------------------------------------------------------------------------------
</span><span>  Index Scan using demo_pkey on demo  (cost=0.29..29.29 rows=200 width=41) (actual time=0.111..0.112 rows=0 loops=1)
</span><span>    Index Cond: ((id &gt;= 10001) AND (id &lt;= 90000))
</span><span>  Planning Time: 1.701 ms
</span><span>  Execution Time: 0.240 ms
</span><span>(4 rows)
</span></code></pre>
<p>While the execution is almost instant, you need to look behind the scenes. The
planner estimated 200 rows and got zero. It traversed the B-tree structure
expecting data that doesn&#39;t exist. On a single query with warm cache, this is
trivial. Under production load with thousands of queries and cold pages,
you&#39;re paying I/O cost for nothing. Again and again.</p>
<p>If you dig further you discover much bigger problem.</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT</span><span> relname, reltuples::</span><span>bigint </span><span>as row_estimate, relpages as page_estimate
</span><span>FROM</span><span> pg_class 
</span><span>WHERE</span><span> relname IN (&#39;</span><span>demo</span><span>&#39;, &#39;</span><span>demo_pkey</span><span>&#39;);
</span></code></pre>
<pre><code><span>relname  | row_estimate | page_estimate
</span><span>-----------+--------------+---------------
</span><span>demo      |        20000 |           934
</span><span>demo_pkey |        20000 |           276
</span></code></pre>
<p>The <code>relpages</code> value comes from the physical file size divided by the 8 KB page
size. PostgreSQL updates it during VACUUM and ANALYZE, but it reflects the
actual file on disk - not how much useful data is inside. Our index file is still
2.2 MB (276 pages × 8 KB), even though most pages are empty.</p>
<p>The planner sees 276 pages for 20,000 rows and calculates a very low
rows-per-page ratio. This is when planner can come to conclusion - <em>this index
is very sparse - let&#39;s do a sequential scan instead</em>. Oops.</p>
<p>&#34;But wait,&#34; you say, &#34;doesn&#39;t ANALYZE fix statistics?&#34;</p>
<p>Yes and no. <code>ANALYZE</code> updates the row count estimate. It will no longer think you
have 100,000 rows but 20,000. But it does not shrink relpages, because that
reflects the physical file size on disk. <code>ANALYZE</code> can&#39;t change that.</p>
<p>The planner now has accurate row estimates but wildly inaccurate page estimates.
The useful data is packed into just ~57 pages worth of entries, but the planner
doesn&#39;t know that.</p>
<pre><code><span>cost = random_page_cost × pages + cpu_index_tuple_cost × tuples
</span></code></pre>
<p>With a bloated index:</p>
<ul>
<li>pages is oversize (276 instead of ~57)</li>
<li>The per-page cost gets multiplied by empty pages</li>
<li>Total estimated cost is artificially high</li>
</ul>
<h2 id="the-hollow-index">The hollow index<a href="#the-hollow-index" aria-label="Anchor link for: the-hollow-index"></a>
</h2>
<p>We can dig even more into the index problem when we look at internal stats:</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT </span><span>* </span><span>FROM</span><span> pgstatindex(&#39;</span><span>demo_pkey</span><span>&#39;);
</span></code></pre>
<pre><code><span>-[ RECORD 1 ]------+--------
</span><span>version            | 4
</span><span>tree_level         | 1
</span><span>index_size         | 2260992
</span><span>root_block_no      | 3
</span><span>internal_pages     | 1
</span><span>leaf_pages         | 57
</span><span>empty_pages        | 0
</span><span>deleted_pages      | 217
</span><span>avg_leaf_density   | 86.37
</span><span>leaf_fragmentation | 0
</span></code></pre>
<p>Wait, what? The avg_leaf_density is 86% and it looks perfectly healthy. That&#39;s a
trap. Due to the hollow index (we removed 80% right in the middle) we have 57
well-packed leaf pages, but the index still contains 217 deleted pages.</p>
<p>This is why <code>avg_leaf_density</code> alone is misleading. The density of used pages
looks great, but 79% of your index file is dead weight.</p>
<p>The simplest way to spot index bloat is comparing actual size to expected size.</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT
</span><span>    </span><span>c</span><span>.</span><span>relname </span><span>as index_name,
</span><span>    pg_size_pretty(pg_relation_size(</span><span>c</span><span>.</span><span>oid</span><span>)) as actual_size,
</span><span>    pg_size_pretty((</span><span>c</span><span>.</span><span>reltuples </span><span>* </span><span>40</span><span>)::</span><span>bigint</span><span>) as expected_size,
</span><span>    round((pg_relation_size(</span><span>c</span><span>.</span><span>oid</span><span>) / nullif(</span><span>c</span><span>.</span><span>reltuples </span><span>* </span><span>40</span><span>, </span><span>0</span><span>))::</span><span>numeric</span><span>, </span><span>1</span><span>) as bloat_ratio
</span><span>FROM</span><span> pg_class c
</span><span>JOIN</span><span> pg_index i ON </span><span>c</span><span>.</span><span>oid </span><span>= </span><span>i</span><span>.</span><span>indexrelid
</span><span>WHERE </span><span>c</span><span>.</span><span>relkind </span><span>= &#39;</span><span>i</span><span>&#39; 
</span><span>  AND </span><span>c</span><span>.</span><span>reltuples </span><span>&gt; </span><span>0
</span><span>  AND </span><span>c</span><span>.</span><span>relname</span><span> NOT LIKE &#39;</span><span>pg_%</span><span>&#39;
</span><span>  AND pg_relation_size(</span><span>c</span><span>.</span><span>oid</span><span>) &gt; </span><span>1024 </span><span>* </span><span>1024  </span><span>-- only indexes &gt; 1 MB
</span><span>ORDER BY</span><span> bloat_ratio </span><span>DESC</span><span> NULLS LAST;
</span></code></pre>
<pre><code><span>index_name | actual_size | expected_size | bloat_ratio
</span><span>------------+-------------+---------------+-------------
</span><span>demo_pkey  | 2208 kB     | 781 kB        |         2.8
</span></code></pre>
<p>A <code>bloat_ratio</code> of 2.8 means the index is nearly 3x larger than expected. Anything
above 1.8 - 2.0 deserves investigation.</p>
<p>We filter to indexes over 1 MB - bloat on tiny indexes doesn&#39;t matter that much.
Please, adjust the threshold based on your environment; for large databases, you
might only care about indexes over 100 MB.</p>
<p>But here comes <strong>BIG WARNING</strong>: pgstatindex() we used earlier physically reads
the entire index. On a 10 GB index, that&#39;s 10 GB of I/O. Don&#39;t run it against
all indexes on a production server - unless you know what you are doing!</p>
<h2 id="reindex">REINDEX<a href="#reindex" aria-label="Anchor link for: reindex"></a>
</h2>
<p>How to actually fix index bloat problem? <code>REINDEX</code> is s straightforward solution as
it rebuilds the index from scratch.</p>
<pre data-lang="sql"><code data-lang="sql"><span>REINDEX INDEX CONCURRENTLY demo_pkey ;
</span></code></pre>
<p>After which we can check the index health:</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT </span><span>* </span><span>FROM</span><span> pgstatindex(&#39;</span><span>demo_pkey</span><span>&#39;);
</span></code></pre>
<pre><code><span>-[ RECORD 1 ]------+-------
</span><span>version            | 4
</span><span>tree_level         | 1
</span><span>index_size         | 466944
</span><span>root_block_no      | 3
</span><span>internal_pages     | 1
</span><span>leaf_pages         | 55
</span><span>empty_pages        | 0
</span><span>deleted_pages      | 0
</span><span>avg_leaf_density   | 89.5
</span><span>leaf_fragmentation | 0
</span></code></pre>
<p>And</p>
<pre data-lang="sql"><code data-lang="sql"><span>SELECT
</span><span>    relname,
</span><span>    pg_size_pretty(pg_relation_size(</span><span>oid</span><span>)) as file_size,
</span><span>    pg_size_pretty((pgstattuple(</span><span>oid</span><span>)).tuple_len) as actual_data
</span><span>FROM</span><span> pg_class
</span><span>WHERE</span><span> relname IN (&#39;</span><span>demo</span><span>&#39;, &#39;</span><span>demo_pkey</span><span>&#39;);
</span></code></pre>
<pre><code><span>relname  | file_size | actual_data
</span><span>-----------+-----------+-------------
</span><span>demo      | 7472 kB   | 1278 kB
</span><span>demo_pkey | 456 kB    | 313 kB
</span></code></pre>
<p>Our index shrunk from 2.2 MB to 456 KB - 79% reduction (not a big surprise
though).</p>
<p>As you might have noticed we have used <code>CONCURRENTLY</code> to avoid using ACCESS
EXCLUSIVE lock. This is available since PostgreSQL 12+, and while there&#39;s an
option to omit it - the pretty much only reason to do so is during planned
maintenance to speed up the index rebuild time.</p>
<h2 id="pg-squeeze">pg_squeeze<a href="#pg-squeeze" aria-label="Anchor link for: pg-squeeze"></a>
</h2>
<p>If you look above at the file_size of our relations, we have managed to reclaim
the disk space for the affected index (it was <code>REINDEX</code> after all), but the table
space was not returned back to the operating system.</p>
<p>That&#39;s where <a href="https://github.com/cybertec-postgresql/pg_squeeze">pg_squeeze</a>
shines. Unlike trigger-based alternatives, pg_squeeze uses logical decoding,
resulting in lower impact on your running system. It rebuilds both the table and
all its indexes online, with minimal locking:</p>
<pre data-lang="sql"><code data-lang="sql"><span>CREATE EXTENSION pg_squeeze;
</span><span>
</span><span>SELECT </span><span>squeeze</span><span>.</span><span>squeeze_table</span><span>(&#39;</span><span>public</span><span>&#39;, &#39;</span><span>demo</span><span>&#39;);
</span></code></pre>
<p>The exclusive lock is only needed during the final swap phase, and its duration
can be configured. Even better, pg_squeeze is designed for regular automated
processing - you can register tables and let it handle maintenance whenever bloat
thresholds are met.</p>
<p>pg_squeeze makes sense when both table and indexes are bloated, or when you want
automated management. REINDEX CONCURRENTLY is simpler when only indexes need
work.</p>
<p>There&#39;s also older tool pg_repack - for a deeper comparison of bloat-busting
tools, see article <a href="https://brianagude.substack.com/posts/the-bloat-busters-pg-repack-pg-squeeze/">The Bloat Busters: pg_repack vs
pg_squeeze</a>.</p>
<h2 id="vacuum-full-the-nuclear-option">VACUUM FULL (The nuclear option)<a href="#vacuum-full-the-nuclear-option" aria-label="Anchor link for: vacuum-full-the-nuclear-option"></a>
</h2>
<p><code>VACUUM FULL</code> rewrites the entire table and all indexes. While it fixes
everything it comes with a big but - it requires an ACCESS EXCLUSIVE
lock - completely blocking all reads and writes for the entire duration. For a
large table, this could mean hours of downtime.</p>
<p><strong>Generally avoid this in production</strong>. Use pg_squeeze instead for the same
result without the downtime.</p>
<h2 id="when-to-act-and-when-to-chill">When to act, and when to chill<a href="#when-to-act-and-when-to-chill" aria-label="Anchor link for: when-to-act-and-when-to-chill"></a>
</h2>
<p>Before you now go and <code>REINDEX</code> everything in sight, let&#39;s talk about when index
bloat actually matters.</p>
<p><strong>B-trees expand and contract with your data</strong>. With random insertions affecting
index columns - UUIDs, hash keys, etc. the page splits happen constantly. Index
efficiency might get hit at occassion and also settle around 70 - 80% over
different natural cycles of your system usage. That&#39;s not bloat. That&#39;s the tree
finding its natural shape for your data.</p>
<p>The bloat we demonstrated - 57 useful pages drowning in 217 deleted ones - is
extreme. It came from deleting 80% of contiguous data. You won&#39;t see this
from normal day to day operations.</p>
<p>When do you need to act immediately:</p>
<ul>
<li>after a massive DELETE (retention policy, GDPR purge, failed migration cleanup)</li>
<li><code>bloat_ratio</code> exceeds 2.0 and keeps climbing</li>
<li>query plans suddenly prefer sequential scans on indexed columns</li>
<li>index size is wildly disproportionate to row count</li>
</ul>
<p>But in most cases you don&#39;t have to panic. Monitor weekly and when indexes bloat
ratio continously grow above warning levels, schedule a <code>REINDEX CONCURRENTLY</code>
during low traffic period.</p>
<p>Index bloat isn&#39;t an emergency until it is. Know the signs, have the tools
ready, and don&#39;t let VACUUM&#39;s silence fool you into thinking everything&#39;s fine.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="Anchor link for: conclusion"></a>
</h2>
<p>VACUUM is essential for PostgreSQL. Run it. Let autovacuum do its job. But
understand its limitations: it cleans up dead tuples, not index structure.</p>
<p>The truth about PostgreSQL maintenance is that VACUUM handles heap bloat
reasonably well, but index bloat requires explicit intervention. Know when your
indexes are actually sick versus just breathing normally - and when to reach for
REINDEX.</p>
<p>VACUUM handles heap bloat. Index bloat is your problem. Know the difference.</p>

  </div></div>
  </body>
</html>
