<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://interlaye.red/kubernetes_002degress_002dsquid.html">Original</a>
    <h1>Kubernetes egress control with squid proxy</h1>
    
    <div id="readability-page-1" class="page"><div id="kubernetes_002degress_002dsquid">

<hr/>
<h3 id="Kubernetes-Egress-Control-with-Squid-proxy"><span>Kubernetes Egress Control with Squid proxy<a href="#Kubernetes-Egress-Control-with-Squid-proxy"> ¶</a></span></h3>
<p><i id="_article_date">2025-12-28</i>
</p>
<blockquote>
<p>This Way to the Egress!
</p></blockquote>
<p>— <em>Sign at P.T. Barnum’s Americam Museum</em>
</p>
<p><i>Kubernetes ingress gets a lot of attention – Gateway API, Ingress controllers, service meshes – compared with the
Egress, mostly ignored until someone asks “what exactly is our cluster talking to?”, or, in even simple deployments,
“Can we see what we are talking to?”. This is a (very) simple approach to that, using the venerable Squid proxy and a
NetworkPolicy, without reaching for heavier machinery (but beginning to understand why we would).</i>
</p>
<p>This is the overview of the thing I’m about to describe:
</p>
<div>
<p><img src="https://blog.veitheller.de/images/squid-egress-mermaid.png" alt="Squid as egress proxy in k3s"/></p><p>Squid as egress proxy in k3s</p></div>
<div id="Why-do-I-care">
<h4><span>Why do I care<a href="#Why-do-I-care"> ¶</a></span></h4>

<p>Most Kubernetes tutorials focus on getting traffic <i>into</i> your cluster, which is fair since that’s where it usually
starts... but traffic flows both ways, and once your workloads start making outbound calls to APIs, databases, and
services beyond your cluster boundary, there’s a discussion on visibility and security to be had.
</p>
<p>I ran into this while working with OpenShift’s egress policies years ago, in so-called “regulated industries”: while
not the most flexible at the time, they were the most straightforward answer to security requirements that defined that
outbound traffic should go through a proxy.
</p>
<p>I’m using Kubernetes through <a href="https://k3s.io/">k3s</a> (mostly) and <a href="https://kind.sigs.k8s.io/">kind</a> (often, for develpment) for my own personal stuff (see <a href="https://blog.veitheller.de/Projects.html">Projects</a>),
so I went back to basics on this: what if we just used <a href="https://www.squid-cache.org/">Squid</a> – a proxy that’s been
solving this problem since 1996! – and enforced its usage with a NetworkPolicy? Nothing fancy, nothing “next-gen
cloud-native” just a proxy with logs, and see where that got me?
</p>
</div>
<div id="Squid-and-k3s_003a-the-solution">
<h4><span>Squid and k3s: the solution<a href="#Squid-and-k3s_003a-the-solution"> ¶</a></span></h4>

<p>The architecture is deliberately simple:
</p>
<div>
<pre>┌────────────────────────────────────────────────────────────┐
│ Cluster                                                    │
│                                                            │
│  ┌─────────────────────┐      ┌────────────────────────┐   │
│  │ workload namespace  │      │ egress-proxy namespace │   │
│  │                     │      │                        │   │
│  │  ┌─────┐            │ :3128│  ┌───────┐             │   │
│  │  │ pod │ HTTP_PROXY ├──────┼─▶│ squid │─────────────┼───┼──▶ internet
│  │  └─────┘            │      │  └───────┘             │   │
│  │     │               │      │                        │   │
│  │     x blocked       │      └────────────────────────┘   │
│  │   (direct egress)   │                                   │
│  └─────────────────────┘                                   │
└────────────────────────────────────────────────────────────┘
</pre></div>

<p>Workloads configure <code>HTTP_PROXY</code>/<code>HTTPS_PROXY</code> environment
variables pointing to Squid, and a  NetworkPolicy on the workload namespace
blocks direct egress, allowing traffic <i>only to the proxy</i>. Squid logs
everything that passes through. That’s it, and this gives us:
</p>
<ul>
<li><b>Visibility</b>: every outbound connection logged with timestamp, destination, bytes transferred
</li><li><b>Enforcement</b>: NetworkPolicy makes the proxy mandatory, not optional
</li><li><b>Simplicity</b>: no CNI plugins, no service mesh, no CRDs
</li></ul>

<p><b>Why Squid?</b>
</p>
<p>I used Squid <i>deliberately</i> because it predates Kubernetes and most “cloud-native” tooling, but still does exactly
what this problem requires: explicit HTTP/HTTPS egress control, logging, and policy enforcement. The point here isn’t to
be original, but to show that older, well-understood components still fit naturally inside Kubernetes when used
intentionally. Squid has a very good feature set around access control and visibility, and is much less
“ingress-first” than common alternatives.
</p>

</div>
<div id="The-demo-application">
<h4><span>The demo application<a href="#The-demo-application"> ¶</a></span></h4>

<p>To test this out, I’m using a small application I built: <a href="https://horizons.interlaye.red/"><i>Horizons</i></a>, a Common
Lisp application using <a href="https://data-star.dev/">Datastar</a> that displays the solar system and fetches data from
NASA’s JPL Horizons API when you click on a planet. It’s a good test case because it makes real HTTPS calls to an
external API – exactly the kind of traffic we want to observe. It’s a scaled-down version of
<a href="https://dataspice.interlaye.red/">DataSPICE</a>, an app I made to test my Common Lisp SDK for Datastar and that uses
NASA SPICE data for a 2D similation of the Cassini-Huygens probe.
</p>
<div>
<p><img src="https://blog.veitheller.de/images/horizons-datastar.png" alt="JPL Horizons Explorer"/></p><p>JPL Horizons Explorer</p></div>
<p>It uses a multi-stage build process that ends up with a reasonably small binary, <code>horizons-server</code> at 16MB, which
isn’t bad for an image-based language like Common Lisp (it can go into ~13MB with some more compression optimisations),
inside a <code>trixie-slim</code> Debian image for a total of ~100MB (this can also be optimised, aggressively so).
</p>

</div>
<div id="Setting-up-Squid">
<h4><span>Setting up Squid<a href="#Setting-up-Squid"> ¶</a></span></h4>

<p>All files are in the <a href="https://codeberg.org/fsm/horizons">Horizons repository</a>, under <code>k8s/</code> specifically.
I’ll go through the main aspects here.
</p>
<p>First, the egress-proxy namespace and Squid configuration:
</p>
<div>
<pre>apiVersion: v1
kind: Namespace
metadata:
  name: egress-proxy
  labels:
    purpose: egress-control

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: squid-config
  namespace: egress-proxy
data:
  squid.conf: |
    http_port 3128

    # File-based logging for persistence and analysis
    access_log /var/log/squid/access.log combined
    cache_log /var/log/squid/cache.log

    # No caching, that&#39;s not the focus (now, at least)
    cache deny all

    # Allow requests from private IP ranges (pod CIDRs)
    acl localnet src 10.0.0.0/8
    acl localnet src 172.16.0.0/12
    acl localnet src 192.168.0.0/16

    acl SSL_ports port 443
    acl Safe_ports port 80
    acl Safe_ports port 443
    acl CONNECT method CONNECT

    http_access deny !Safe_ports
    http_access deny CONNECT !SSL_ports
    http_access allow localnet
    http_access deny all
    
    # Optional: restrict to specific domains (uncomment to enforce)
    # acl allowed_domains .ssd.jpl.nasa.gov
    # http_access deny !allowed_domains
    
</pre></div>

<p>The <code>localnet</code> ACLs cover all <a href="https://datatracker.ietf.org/doc/html/rfc1918">RFC 1918 private IP space</a> – your
k3s pod CIDR will fall within one of these, adjust for other situations of course. In production, you might tighten this
to your specific pod CIDR for defence in depth etc.
</p>
</div>
<div id="The-Squid-deployment">
<h4><span>The Squid deployment<a href="#The-Squid-deployment"> ¶</a></span></h4>

<p>The deployment needs a few things: an init container to fix permissions on the log directory (<code>hostPath</code> volumes are
created as root, but Squid runs as the <code>proxy</code> user), and a sidecar to stream logs to stdout for <code>kubectl
logs</code>:
</p>
<div>
<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: squid
  namespace: egress-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: squid
  template:
    metadata:
      labels:
        app: squid
    spec:
      initContainers:
        - name: fix-permissions
          image: busybox:latest
          command: [&#34;sh&#34;, &#34;-c&#34;, &#34;chown -R 13:13 /var/log/squid&#34;]
          volumeMounts:
            - name: logs
              mountPath: /var/log/squid
      containers:
        - name: squid
          image: ubuntu/squid:latest
          ports:
            - containerPort: 3128
          volumeMounts:
            - name: config
              mountPath: /etc/squid/squid.conf
              subPath: squid.conf
            - name: logs
              mountPath: /var/log/squid

        - name: log-streamer
          image: busybox:latest
          command: [&#34;sh&#34;, &#34;-c&#34;, &#34;touch /var/log/squid/access.log &amp;&amp; tail -F /var/log/squid/access.log&#34;]
          volumeMounts:
            - name: logs
              mountPath: /var/log/squid

      volumes:
        - name: config
          configMap:
            name: squid-config
        - name: logs
          hostPath:
            path: /var/log/squid-egress
            type: DirectoryOrCreate
</pre></div>

<p>The <code>hostPath</code> volume means logs persist on the node at <code>/var/log/squid-egress/</code>, which I use for offline
analysis or feeding into external log aggregation, namely goaccess. This could be done inside the cluster as well, but
for simple deployments I often do it at the host level (for both k8s and non-k8s workloads).
</p>
</div>
<div id="Enforcing-proxy-usage-with-NetworkPolicy">
<h4><span>Enforcing proxy usage with NetworkPolicy<a href="#Enforcing-proxy-usage-with-NetworkPolicy"> ¶</a></span></h4>

<p>This is where it stops being optional: the NetworkPolicy blocks direct egress from the workload namespace, allowing only
DNS resolution and traffic to the proxy:
</p>
<div>
<pre>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: enforce-egress-proxy
  namespace: horizons
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    # DNS resolution
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53

    # Squid proxy only
    - to:
        - namespaceSelector:
            matchLabels:
              purpose: egress-control
      ports:
        - protocol: TCP
          port: 3128
</pre></div>

<p>The <code>purpose: egress-control</code> label on the egress-proxy namespace is what the selector matches, cleaner than
hardcoding namespace names. Without the proxy environment variables configured, workloads in this namespace cannot reach
the outside world. 
</p>
</div>
<div id="Configuring-the-workload">
<h4><span>Configuring the workload<a href="#Configuring-the-workload"> ¶</a></span></h4>

<p>How do they use the proxy will be application-dependent: in this example, I’m using the
<a href="https://github.com/fukamachi/dexador">dexador</a> Common Lisp HTTP client, and setting the proxy based on the
environment variables (which are set in the deployment manifest):
</p>
<div>
<pre>    spec:
      containers:
        - name: horizons
          image: localhost:5000/horizons:latest
          env:
            - name: HTTP_PROXY
              value: &#34;http://squid.egress-proxy.svc.cluster.local:3128&#34;
            - name: HTTPS_PROXY
              value: &#34;http://squid.egress-proxy.svc.cluster.local:3128&#34;
            - name: NO_PROXY
              value: &#34;localhost,127.0.0.1,.svc,.svc.cluster.local,10.0.0.0/8&#34;
</pre></div>

<p>The <code>NO_PROXY</code> setting is important, since without it, internal service-to-service calls would try to route through
Squid and fail.
</p>
<p>One caveat: not all HTTP clients respect these environment variables automatically. Most do (curl, wget, Python
requests, Go’s net/http), but some require explicit configuration. This is the case of
<a href="https://edicl.github.io/drakma/">Drakma</a>, but even in Dexador (which does use them) I had to set the default proxy explicitly:
</p>
<div>
<pre>  <span>(</span><span>let</span><span>* ((</span><span>planet-plist</span> <span>(</span><span>find</span> planet-name <span>*planets*</span>
                             <span>:key</span> <span>(</span><span>lambda</span> <span>(</span><span>p</span><span>) (</span><span>getf</span> p <span>:name</span><span>))</span>
                             <span>:test</span> <span>#&#39;</span>string-equal<span>))</span>
         <span>(</span><span>horizons-id</span> <span>(</span><span>when</span> planet-plist
                        <span>(</span><span>getf</span> planet-plist <span>:horizons</span>-id<span>)))</span>
         <span>(</span><span>dex</span><span>:*</span>default-proxy<span>* (</span><span>uiop</span><span>:getenv</span> <span>&#34;HTTPS_PROXY&#34;</span><span>)))</span>
</pre></div>

<p>The reason here is that I’m using SBCL’s <code>save-lisp-and-die</code> approach to build a binary, and this captures the env
variables at compile time: I need to refresh them at runtime.
</p>

</div>
<div id="Seeing-it-work">
<h4><span>Seeing it work<a href="#Seeing-it-work"> ¶</a></span></h4>

<p>With everything deployed, watching the logs while clicking around the
Horizons UI:
</p>
<div>
<pre>$ kubectl logs -f deploy/squid -n egress-proxy -c log-streamer
10.42.0.238 - - [27/Dec/2025:21:43:34 +0000] &#34;CONNECT ssd.jpl.nasa.gov:443 HTTP/1.1&#34; 200 5537 &#34;-&#34; &#34;-&#34; TCP_TUNNEL:HIER_DIRECT
10.42.0.238 - - [27/Dec/2025:21:43:45 +0000] &#34;CONNECT ssd.jpl.nasa.gov:443 HTTP/1.1&#34; 200 5537 &#34;-&#34; &#34;-&#34; TCP_TUNNEL:HIER_DIRECT
</pre></div>


<div>
<p><img src="https://blog.veitheller.de/images/k9s-squid.png" alt="k9s showing the log-streamer container"/></p><p>k9s showing the log-streamer container</p></div>
<p>Every call to the JPL Horizons API is logged. The <code>TCP_TUNNEL:HIER_DIRECT</code> indicates Squid is tunnelling the HTTPS
connection directly: no SSL interception, just a pass-through that logs the destination.
</p>
</div>
<div id="What-you-see-_0028and-don_0027t-see_0029">
<h4><span>What you see (and don’t see)<a href="#What-you-see-_0028and-don_0027t-see_0029"> ¶</a></span></h4>

<p>For HTTPS traffic, Squid logs the <code>CONNECT</code> tunnel: the destination host and port, timestamp, and bytes
transferred. You <i>don’t</i> see the full URL path, since that would require SSL interception (ssl-bump), which breaks
end-to-end encryption and requires deploying CA certificates to all clients. That’s a different architecture with
different trade-offs.
</p>
<p>What you do get is still valuable though: &#34;pod X talked to api.example.com:443 at 14:32, transferred 5KB.&#34; For
compliance, debugging, and security auditing, that’s often enough, and it certainly is enough for my own purposes. It
also brings you some lock-down features “for free”. For HTTP traffic you get the full URL.
</p>
<p>Also worth noting: connection pooling affects log frequency. If your HTTP client keeps connections alive, you’ll see one
<code>CONNECT</code> entry covering multiple requests. I added <code>:keep-alive nil</code> to Dexador’s <code>dex:get</code> to get a hit
everytime I clicked a planet, but this will be dependent on the application code.
</p>
<p>There’s also some latency introduced by adding a new hop. This shouldn’t be noticeable or an issue, but it will depend
on the specifics of your application.
</p>
</div>
<div id="Adding-GoAccess-for-real_002dtime-visualisation">
<h4><span>Adding GoAccess for real-time visualisation<a href="#Adding-GoAccess-for-real_002dtime-visualisation"> ¶</a></span></h4>

<p>Tailing logs is fine for debugging, but for ongoing visibility, I use <a href="https://goaccess.io/">GoAccess</a> provides a
real-time web dashboard. Adding it as another sidecar:
</p>
<div>
<pre>        - name: goaccess
          image: allinurl/goaccess:latest
          command:
            - sh
            - -c
            - |
              while [ ! -f /var/log/squid/access.log ]; do sleep 1; done
              goaccess /var/log/squid/access.log \
                --log-format=SQUID \
                --real-time-html \
                --output=/var/www/goaccess/index.html \
                --port=7890
          ports:
            - containerPort: 7890
          volumeMounts:
            - name: logs
              mountPath: /var/log/squid
</pre></div>

<p>and expose it with a NodePort service or equivalent, and you have a live dashboard showing which external hosts your
cluster is talking to, request rates, and traffic patterns <a id="DOCF1" href="#FOOT1"><sup>1</sup></a>
</p>
<p>I mostly use goaccess in TUI mode, which is easily done since I’m storing the Squid logs in the host.
</p>
<p><img src="https://blog.veitheller.de/images/goaccess-squid.png" alt="images/goaccess-squid"/>

</p></div>
<div id="Limitations-and-where-this-leads">
<h4><span>Limitations and where this leads<a href="#Limitations-and-where-this-leads"> ¶</a></span></h4>

<p>This approach is intentionally minimal, and it has real limitations:
</p>
<ul>
<li><b>Application changes required</b>: workloads must set proxy environment variables.
</li><li><b>HTTP/HTTPS only</b>: raw TCP, gRPC-over-HTTP/2, and other protocols need different handling.
</li><li><b>Single point of configuration</b>: one squid.conf for all namespaces.
</li><li><b>No per-namespace self-service</b>: teams can’t manage their own egress rules.
</li></ul>

<p>Linked with several of the above (mainly the centralised configuration) is that when using ACL rules to limit
communication to external domains, these are cumulative: all namespaces will be able to communicate with all whitelisted
domains, even if they only need to communicate with some of them.
</p>
<p>These limitations point toward why more sophisticated solutions exist, after all; a follow-up article will explore using
Squid’s <code>include</code> directive to enable per-namespace configuration, and in doing so, show why you’d eventually want
a controller or operator to manage the complexity.
</p>
<p>There are more questions that can be answered though: transparent interception for applications that can’t be
configured, sidecar proxies for per-workload control, and eventually the full service mesh model... each step solves a
real problem that the previous approach couldn’t handle, and the allure of adding “simple” things on top of each other
starts to fade...
</p>
<p>...but for many cases, especially when you just need to answer “what is my cluster talking to?” or enforce a fixed list
of egress destinations, a proxy and a NetworkPolicy is enough.
</p>
<p>It is for me, at least.
</p>
</div>

</div></div>
  </body>
</html>
