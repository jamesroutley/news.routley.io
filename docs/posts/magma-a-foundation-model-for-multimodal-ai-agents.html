<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://microsoft.github.io/Magma/">Original</a>
    <h1>Magma: A Foundation Model for Multimodal AI Agents</h1>
    
    <div id="readability-page-1" class="page">
    <section>
      <div>
        <div>
          <div>
            <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/logo.png" alt="Magma Logo"/></p>
              </div>
              
              
              </div>
    
              </div>
            </div>
          </div>
        
      
    </section>
  



  <section>
    <div>
      <div>
        <p><img src="https://microsoft.github.io/Magma/static/images/magma_teaser.png" alt="Image description" width="100%"/>
        </p>  
        <!-- <h2 class="subtitle has-text-centered"> -->
        
        
      </div>
  
    </div>
  </section>

  <section>
    <div>
      <div>
        
    
      <div>
        <div>
          <p>
            We present <strong>Magma</strong>, a foundation model serving multimodal AI agentic tasks in both the digital and physical worlds.  <strong>Magma</strong> is a significant extension of vision-language (VL) models in that the former not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial intelligence) and to complete agentic tasks ranging from UI navigation to robot manipulation.
            <strong>Magma</strong> is pretrained on large amounts of heterogeneous VL datasets including images, videos and robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) and the object movements (e.g., the trace of a robotic arm) in videos are labeled by Trace-of-Mark (ToM). 
            Extensive experiments show that SoM and ToM facilitate the acquisition of spatial intelligence from large-scale training data.
            <strong>Magma</strong> creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are tailored specifically to these tasks. On VL tasks, Magma also compares favorably to popular VL models that are trained on much larger datasets.
          </p>
        </div>
        </div>
      </div>
      </div>
    
  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Magma Pretraining</h2>
          <div>
            <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_pretrain.png"/>
                </p>
              </div>
              <p><strong>Magma pretraining pipeline. </strong>For all training data, texts are tokenized into tokens, while images and videos from different domains
                are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in
                verbal, spatial and action types.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Magma Technical Details</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div>
          <div>
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/som_fig.jpg" alt="Image description" width="100%" ;=""/>
                  </p>
                </div>
                <div>
                  <p><strong>Set-of-Mark (SoM) for Action Grounding. </strong>Set-of-Mark prompting enables effective action grounding in images for both UI screenshot (left), robot manipulation (middle) and human video (right) by having the model predict numeric marks for clickable buttons or robot arms in image space.</p>
              </div>
              
            

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/tom_fig.png" alt="Image description" width="100%" ;=""/>
                  </p>
                </div>
                <div>
                  <p><strong>Trace-of-Mark (ToM) for Action Planning. </strong>Trace-of-Mark supervisions for robot manipulation (left) and human action (right). It compels the model to comprehend temporal video dynamics and anticipate future states before acting, while using fewer tokens than next-frame prediction to capture longer temporal horizons and action-related dynamics without ambient distractions.</p>
              </div>
              
            

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_data_sunburst.png"/>
                    <img src="https://microsoft.github.io/Magma/static/images/magma_data_sample.png"/>
                </p>
                </div>
                <div>
                  <p>
                      <strong>Overview of Pretraining Data Sources</strong>: 
                      <span>instructional videos</span>, 
                      <span>robotics manipulation</span>, 
                      <span>UI navigation</span>, and 
                      <span>multimodal understanding.</span>
                      We apply SoM and ToM for different data types, with SoM enabling unified action grounding across all modalities while ToM is specifically applied to video and robotics data.
                  </p>
              </div>
              
            
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Agentic Intelligence</h2>
          <div>
          <div>
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_agentic.png"/>
                </p>
                </div>
                <div>
                  <p><strong>Zero-shot evaluation on agentic intelligence.</strong> We report the results for pretrained Magma without any domain-specific finetuning.
                        Magma is the <strong>only model</strong> that can conduct the <strong>full task spectrum</strong>.</p>
              </div>
              
            
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <!--div class="container is-max-fullhd" style="margin-top: -5em;"-->
    <!-- <hr width="100%" size="2"> -->
    <div>
    <div>
    <div>
      <h2>UI Navigation</h2>
      <div>
        <div>
          <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="./static/videos/ui_weather_and_flight_mode.mp4" type="video/mp4"/>
              </video>
              <p>What&#39;s weather in Seattle? &amp; turn on flight mode</p>
            </div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="./static/videos/ui_wordle.mp4" type="video/mp4"/>
              </video>
              <p>Share and message this to Bob Steve, click send button to complete</p>
            </div>
          </div>
        </div>
      </div>
      <div>
        <div>
          <div>
            <p><img src="https://microsoft.github.io/Magma/static/images/ui_efficient_mind2web.png"/>
            </p>
          </div>
          <p><strong>Efficient finetuning on Mind2Web for web UI navigation.</strong></p>
        </div>
      </div>
      <div>
        <div>
          <div>
            <p><img src="https://microsoft.github.io/Magma/static/images/ui_efficient_aitw.png"/>
            </p>
          </div>
          <p><strong>Efficient finetuning on AITW for mobile UI navigation.</strong></p>
        </div>
      </div>

    </div>
    </div>
    </div>
    
  </section>
  
  <section id="real">
    <!--div class="container is-max-fullhd" style="margin-top: -5em;"-->
    <!-- <hr width="100%" size="2"> -->
    <div>
    <div>
    <div>
      <h2>Robot Manipulation</h2>
      <div>
        <div>
          <p>
              WidowX videos show real robot rollouts with <strong>Magma</strong> model. (Videos are sped up by 2.5x.)<br/>
            </p>
        </div>
      </div>

      

      

      <div>
        <div>
          <div>
            <p>Pick Place Hotdog Sausage</p>
            <p>Put Mushroom Place Pot</p>
            <p>Push Cloth Left to Right (Out-of-Dist.)</p>
          </div>
        </div>
      </div>

      <div>
        <div>
          <p><strong>Few-shot finetuning on WidowX robot. </strong><strong>Magma</strong> consistently outperforms OpenVLA (finetuning) across soft object manipulation and pick-and-place operations, demonstrating reliable performance in both in-distribution and out-of-distribution generalization tasks on real robots.</p>
          <div>
            <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_real.png"/></p><p>Magma on Real Robot</p>
              </div>
            </div>
            <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_libero.png"/></p><p>Magma on LIBERO</p>
              </div>
            </div>
          </div>
          <p><strong>Few-shot finetuning on LIBERO. </strong> <strong>Magma</strong> achieves a significantly higher average success rate
            in all task suites. Additionally, removing SoM and ToM during pretraining has a negative impact on model performance.
          </p>
          <p><img src="https://microsoft.github.io/Magma/static/images/magma_simpler.png" alt="Image description" width="100%"/>
          </p>
          <p><strong>Zero-shot SimplerEnv evaluation on Google Robots and Bridge. </strong> <strong>Magma</strong> shows strong zero-shot cross-domain robustness and demonstrates impressive results in cross-embodiment manipulation simulation tasks.
          </p>
        </div>
      </div>

    </div>
    </div>
    </div>
    
</section>

  <!--section class="section body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3" style="text-align: center; padding-bottom: 10px; margin-top: -40px;">Simulation Benchmark</h2>
          <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">
          
            <tr>
              <td>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 10px;"><strong>Few-shot finetuning on LIBERO. </strong> <strong>Magma</strong> achieves a significantly higher average success rate
                        in all task suites. Additionally, removing SoM and ToM during pretraining has a negative impact on model performance.</p>
                  </div>
                </div>
                <div class="row">
                  <div class="col">
                    <img src="./static/images/magma_simpler.png" alt="Image description" width="100%">
                  </div>
                </div>
                <div class="columns is-centered ">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 10px;"><strong>Zero-shot SimplerEnv evaluation on Google Robots and Bridge. </strong> <strong>Magma</strong> shows strong zero-shot cross-domain robustness and demonstrates impressive results in cross-embodiment manipulation simulation tasks.</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section-->

  <section>
    <div>
      <div>
        <div>
          <h2>Spatial Reasoning</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <!--div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths ">
          <div class="content has-text-justified">

            <tr>
              <td>
                <div class="row">
                  <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; width: 100%;">
                    <img src="./static/images/magma_spatial_table.png" style="max-width: 750px; margin-bottom: 20px;">
                    <img src="./static/images/magma_spatial_visualizations.png" style="max-width: 750px;">
                  </div>
                </div>
                <div class="columns is-centered">
                  <div class="column">
                      <p style="text-align: left;font-size: 18px; margin-top: 20px;"><strong>Spatial reasoning evaluations.</strong> Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data</p>
                </div>
              </div>
              </td>
            </tr>
          </div>
          </div>
          </div-->
          <div>
            <div>
              <div>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_spatial_table.png"/>
                </p>
                <p><img src="https://microsoft.github.io/Magma/static/images/magma_spatial_visualizations.png"/>
                </p>
              </div>
              <p><strong>Spatial reasoning evaluations.</strong> Spatial reasoning questions are challenging even for GPT-4o but Magma can answer relatively well despite relying on much fewer pretraining data.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Multimodal Understanding</h2>
          <!-- <h2 class="subtitle is-4" style="text-align: center;">Defining Visual Affordances</h2> -->
          <div>
          <div>
          <div>

            
              
                <div>
                  <p><img src="https://microsoft.github.io/Magma/static/images/magma_video_qa.png"/>
                </p>
                </div>
                <div>
                  <p><strong>Zero-shot Video QA benchmarks.</strong> <strong>Magma</strong> performs competitively and even outperforms some state-of-the-art approaches such as Video-Llama2 and ShareGPT4Video on most benchmarks, despite using much fewer video instruction tuning data.</p>
              </div>
              
            
              
            </div>

          </div>
          </div>
          </div>
        </div>
      </div>
    
  </section>

  <section id="real">
    <!--div class="container is-max-fullhd" style="margin-top: -5em;"-->
    <div>
    <div>
    <div>
      <!-- <hr width="100%" size="2"> -->
      <h2>Video conversation examples</h2>
      

      <div>
        <div>
          <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="./static/videos/magma_chess.mp4" type="video/mp4"/>
              </video>
              <p><strong><u>User</u></strong>: I really want to win this game. Can you suggest the next move that I should make to maximize my chances of winning?</p>
              <p><strong><u>Magma</u></strong>: I suggest moving the black pawn to d4 to maximize your chances of winning.</p>
            </div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="./static/videos/magma_relaxation.mp4" type="video/mp4"/>
              </video>
              <p><strong><u>User</u></strong>: I want to relax for a few hours in here. Can you suggest some things for me to do?</p>
              <p><strong><u>Magma</u></strong>: Watch TV, read a book, and enjoy the view of the autumn forest.</p>
            </div>
          </div>
        </div>
      </div>

      <div>
        <div>
          <div>
            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" height="80%">
                <source src="./static/videos/magma_counter_checkout.mp4" type="video/mp4"/>
              </video>
              <p><strong><u>User</u></strong>: Please provide a detailed description of the video, focusing on the main subject and the actions taking place as well as predict what is going to happen next.</p>
              <p><strong><u>Magma</u></strong>: The video takes place in a convenience store, focusing on the checkout counter. It begins with a view of the counter displaying a sign that reads &#39;PICK 5 FOR $8.00&#39; along with images of various snacks. The counter is filled with various items, including snacks and a payment terminal. An employee, wearing a black shirt with white sleeves, is seen handling the payment terminal. A customer, identifiable by their white sleeve and colorful bracelet, places a red shopping basket filled with snacks and a drink on the counter. The employee continues to operate the payment terminal while the customer adjusts the items in the basket. The interaction between the customer and the employee is highlighted, with the employee assisting the customer with the items.                 The scene continues at the same checkout counter, with the &#39;PICK 5 FOR $8.00&#39; sign still prominently displayed. The customer&#39;s red shopping basket, now filled with snacks including a drink cup labeled &#39;Fruit Swoosh&#39; and a blue package of cookies, is placed on the counter. The employee is seen handling the payment terminal. The customer&#39;s hand, adorned with a colorful bracelet, interacts with the items in the basket. The employee continues to assist the customer, adjusting the items on the counter. The interaction between the customer and the employee remains the focal point, with the employee assisting the customer with the items. The person behind the counter is likely to process the payment and possibly hand over the items.</p>
            </div>

            <div>
              <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
                <source src="./static/videos/magma_potted_plant.mp4" type="video/mp4"/>
              </video>
              <p><strong><u>User</u></strong>: Please describe in detail what is happening in the video by focusing on the main actions.</p>
              <p><strong><u>Magma</u></strong>: The video begins with a close-up of a hand reaching for a red potted plant placed on a wooden shelf. The plant has green and yellow-striped leaves and is accompanied by a book titled &#39;YOUNG&#39; and a wooden bowl stacked on top of each other. The hand carefully adjusts the position of the plant on the shelf, ensuring it is properly placed. The background includes a beige watering can hanging on a stand and a radiator with white pipes. The hand then adjusts the plant once more, making sure it is centered on the shelf. The scene develops as the hand, now wearing a yellow sweater and a ring on the left hand, reaches out to adjust the plant&#39;s position once more. The hand then moves away from the plant, and the camera shifts to show more of the surrounding area, including a red high chair and a wooden countertop with various items, such as a smartphone and a small red cup. The video wraps up with a wider view of the room, revealing a checkered floor and several other plants in the background, adding a homely touch to the setting.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>


    <!--div class="container is-max-fullhd" style="margin-top: -5em;">
      <hr width="100%" size="2">
      <h2 class="title is-3 has-text-centered">Video conversation examples</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          </div>
        </div>
      </div>

      <div style="background-color: #f0f0f0; padding: 1.5rem; border-radius: 10px; margin-bottom: 2rem; max-width: 800px; margin-left: auto; margin-right: auto;">
      <h3 class="title is-4 has-text-left" style="margin-top: -1rem; margin-bottom: 1.5rem;"><strong><u>User</u></strong>: I really want to win this game. Can you suggest the next move that I should make to maximize my chances of winning?</h3>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <div class="column content has-text-centered">
              <video autoplay muted loop controls playsinline height="80%" style="max-width: 450px; width: 100%; border-radius: 10px; overflow: hidden;">
                <source src="./static/videos/magma_chess.mp4" type="video/mp4">
              </video>
              
              <h4 class="title is-5 has-text-left" style="margin-top: 1rem; margin-bottom: -1rem;"><strong><u>Magma</u></strong>: I suggest moving the black pawn to d4 to maximize your chances of winning.</h4>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div style="background-color: #f0f0f0; padding: 1.5rem; border-radius: 10px; margin-bottom: 2rem; max-width: 800px; margin-left: auto; margin-right: auto;">
      <h3 class="title is-4 has-text-left" style="margin-top: -1rem; margin-bottom: 2rem;"><strong><u>User</u></strong>: Please provide a detailed description of the video, focusing on the main subject and the actions taking place as well as predict what is going to happen next.
      </h3>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <div class="column content has-text-centered">
              <video autoplay muted loop controls playsinline height="80%" style="max-width: 450px; width: 100%; border-radius: 10px; overflow: hidden;">
                <source src="./static/videos/magma_counter_checkout.mp4" type="video/mp4">
              </video>
              
              <h4 class="title is-5 has-text-left" style="margin-top: 1rem; margin-bottom: -1rem;"><strong><u>Magma</u></strong>: The video takes place in a convenience store, focusing on the checkout counter. It begins with a view of the counter displaying a sign that reads 'PICK 5 FOR $8.00' along with images of various snacks. The counter is filled with various items, including snacks and a payment terminal. An employee, wearing a black shirt with white sleeves, is seen handling the payment terminal. A customer, identifiable by their white sleeve and colorful bracelet, places a red shopping basket filled with snacks and a drink on the counter. The employee continues to operate the payment terminal while the customer adjusts the items in the basket. The interaction between the customer and the employee is highlighted, with the employee assisting the customer with the items.                 The scene continues at the same checkout counter, with the 'PICK 5 FOR $8.00' sign still prominently displayed. The customer's red shopping basket, now filled with snacks including a drink cup labeled 'Fruit Swoosh' and a blue package of cookies, is placed on the counter. The employee is seen handling the payment terminal. The customer's hand, adorned with a colorful bracelet, interacts with the items in the basket. The employee continues to assist the customer, adjusting the items on the counter. The interaction between the customer and the employee remains the focal point, with the employee assisting the customer with the items. The person behind the counter is likely to process the payment and possibly hand over the items.</h4>
            </div>
          </div>
        </div>
      </div>
    </div>

      <div style="background-color: #f0f0f0; padding: 1.5rem; border-radius: 10px; margin-bottom: 2rem; max-width: 800px ; margin-left: auto; margin-right: auto;">
      <h3 class="title is-4 has-text-left" style="margin-top: -1rem; margin-bottom: 2rem;"><strong><u>User</u></strong>: I want to relax for a few hours in here. Can you suggest some things for me to do? </h3>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <div class="column content has-text-centered">
              <video autoplay muted loop controls playsinline height="80%" style="max-width: 450px; width: 100%; border-radius: 10px; overflow: hidden;">
                <source src="./static/videos/magma_relaxation.mp4" type="video/mp4">
              </video>
              
              <h4 class="title is-5 has-text-left" style="margin-top: 1rem; margin-bottom: -1rem;"><strong><u>Magma</u></strong>: Watch TV, read a book, and enjoy the view of the autumn forest.
              </h4>
            </div>
          </div>
        </div>
      </div>
      </div>

      <div style="background-color: #f0f0f0; padding: 1.5rem; border-radius: 10px; margin-bottom: 2rem; max-width: 800px; margin-left: auto; margin-right: auto;">
      <h3 class="title is-4 has-text-left" style="margin-top: -1rem; margin-bottom: 2rem;"><strong><u>User</u></strong>: Please describe in detail what is happening in the video by focusing on the main actions.</h3>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="columns is-centered">
            <div class="column content has-text-centered">
              <video autoplay muted loop controls playsinline height="80%" style="max-width: 450px; width: 100%; border-radius: 10px; overflow: hidden;">
                <source src="./static/videos/magma_potted_plant.mp4" type="video/mp4">
              </video>
              
              <h4 class="title is-5 has-text-left" style="margin-top: 1rem; margin-bottom: -1rem;"><strong><u>Magma</u></strong>: The video begins with a close-up of a hand reaching for a red potted plant placed on a wooden shelf. The plant has green and yellow-striped leaves and is accompanied by a book titled 'YOUNG' and a wooden bowl stacked on top of each other. The hand carefully adjusts the position of the plant on the shelf, ensuring it is properly placed. The background includes a beige watering can hanging on a stand and a radiator with white pipes. The hand then adjusts the plant once more, making sure it is centered on the shelf. The scene develops as the hand, now wearing a yellow sweater and a ring on the left hand, reaches out to adjust the plant's position once more. The hand then moves away from the plant, and the camera shifts to show more of the surrounding area, including a red high chair and a wooden countertop with various items, such as a smartphone and a small red cup. The video wraps up with a wider view of the room, revealing a checkered floor and several other plants in the background, adding a homely touch to the setting.
              </h4>
            </div>
          </div>
        </div>
      </div>
    </div>

    </div-->
    
</section>

<section id="real">
  <div>
    

  <div>
    <div>
      <div>
        <div>
          <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
            <source src="./static/videos/rayban-video-1.mp4" type="video/mp4"/>
          </video>
          <p><strong><u>User</u></strong>: Please describe in detail what is happening in the video by focusing on the main actions.</p>
          <p><strong><u>Magma</u></strong>: The video begins with a close-up view of a kitchen sink area. The sink has a shiny silver faucet with water running, and a person is holding a black mug under the running water. On the left side of the sink, there is a bottle of dish soap and a blue toothbrush. The person then picks up a blue sponge and starts scrubbing the inside of the mug. The scene continues with the person methodically scrubbing the mug, ensuring it is clean. The video wraps up with the person placing the scrubbed mug in the sink and the blue sponge beside it, indicating that the cleaning process is complete.</p>
        </div>
        <div>
          <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
            <source src="./static/videos/rayban-video-2.MOV" type="video/mp4"/>
          </video>
          <p><strong><u>User</u></strong>: What drink did the person choose in the video?</p>
          <p><strong><u>Magma</u></strong>: The person chose a soy beverage labeled as &#39;SAVE SOY&#39;.</p> 
        </div>
      </div>
    </div>
  </div>
</div>

  
</section>

<section id="real">
  <div>
    

<div>
  <div>
    <div>
      <div>
        <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
          <source src="./static/videos/rayban-video-3.MOV" type="video/mp4"/>
        </video>
        <p><strong><u>User</u></strong>: Can you predict the next actions that the person is going to carry out beyond the video?</p>
        <p><strong><u>Magma</u></strong>: The person is likely about to pour hot water into the cup and proceed with the tea preparation process.</p>
      </div>
      <div>
        <video autoplay="" muted="" loop="" controls="" playsinline="" width="100%" height="80%">
          <source src="./static/videos/rayban-video-5.MOV" type="video/mp4"/>
        </video>
        <p><strong><u>User</u></strong>: Can you describe what the person is doing in the video?</p>
        <p><strong><u>Magma</u></strong>: The person is interacting with the coffee machine by pressing buttons, likely to make a coffee choice.</p>
      </div>
    </div>
  </div>
</div>
</div>
  
</section>

  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@misc{yang2025magmafoundationmodelmultimodal,
        title={Magma: A Foundation Model for Multimodal AI Agents}, 
        author={Jianwei Yang and Reuben Tan and Qianhui Wu and Ruijie Zheng and Baolin Peng and Yongyuan Liang and Yu Gu and Mu Cai and Seonghyeon Ye and Joel Jang and Yuquan Deng and Lars Liden and Jianfeng Gao},
        year={2025},
        eprint={2502.13130},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2502.13130},
      }</code></pre>
    </div>
  </section>
     

  </div>
  </body>
</html>
