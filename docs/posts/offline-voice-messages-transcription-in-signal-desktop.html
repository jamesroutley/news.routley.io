<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.a2p.it/tech-stuff/coquistt-signal-love-death-to-voice-messages/">Original</a>
    <h1>Show HN: Offline voice messages transcription in Signal Desktop</h1>
    
    <div id="readability-page-1" class="page"><div><p>Let’s face it: if you’re reading this, chances are that you are receiving the dreaded voice messages more often than you would want. I like the romantic feeling behind voice messages on Instant Messaging platforms: you can feel the nuances of the sender’s voice, quickly deciphering their mood. However, as you start receiving voice messages more often, that romantic feeling will collapse under the weight of a stack of these:</p>
<figure>
    <img loading="lazy" src="https://www.a2p.it/images/2022/voice_messages_stack.png#center" alt="A stack of voice messages in Signal"/> <figcaption>
            <p>A stack of voice messages in Signal</p>
        </figcaption>
</figure>

<p>I get why people send voice messages: it saves <em>their</em> time, allowing them to record their voice while doing other things. Some go even as far as sending a voice message just to say “yes” or “no”. Sadly, while it can be convenient to listen to a message while doing something else, there are, in my opinion, some major drawbacks:</p>
<ul>
<li><em>searching</em> them is <strong>awful</strong>; you need to go through the voice messages and listen to them in order to find what is needed;</li>
<li>voice messages can’t be skimmed, like text messages; while it’s true that most IM allow reproducing voice messages faster, it’s not the same.</li>
</ul>

<p>And then it finally occurred to me: everybody is shouting Machine Learning all over the place. Let’s try to put it to good use, shall we? What if we had a way to use Speech To Text ML algorithms to transcribe the received audio messages and show their text, instead?</p>
<p>By using the fantastic <a href="https://github.com/coqui-ai/STT">STT APIs</a> from <a href="https://coqui.ai/">Coqui</a> and the Italian model trained by Mozilla Italia from the <a href="https://commonvoice.mozilla.org/">Mozilla Common Voice</a> data I was able to stitch together a prototype Signal Desktop doing just that. I picked Signal both because I like it quite a lot and because, being the desktop version basically built with web tech, I felt very familiar with it. The Coqui STT community and its developers were quite helpful and friendly throughout the whole process, what’s not to love?</p>
<p>The full source code of this experiment is available on my <a href="https://github.com/dexterp37/Signal-Desktop/tree/stt_audio">GitHub fork (<code>stt_audio</code> branch)</a>. Here’s out it behaves:</p>
<figure>
    <img loading="lazy" src="https://www.a2p.it/images/2022/signal_desktop_stt.gif#center" alt="Signal Desktop using Coqui STT to transcribe audio messages"/> <figcaption>
            <p>Signal Desktop using Coqui STT to transcribe audio messages</p>
        </figcaption>
</figure>


<p>The first thing I did was to fork Signal Desktop and, before making any change to its source code, I made sure I was able to build it and run it locally. Their <a href="https://github.com/signalapp/Signal-Desktop/blob/development/CONTRIBUTING.md">Contributor Guidelines</a> have a nice step by step guide that got me through the process, with minimal hiccups due to node-pre-gyp. I then made sure to set up a <a href="https://github.com/signalapp/Signal-Desktop/blob/development/CONTRIBUTING.md#the-staging-environment">staging environment</a> using data from my real Signal install: I had just received a couple of audio messages, so I made sure to use them as testing material!</p>
<p>With a development version of Signal Desktop, along with sample audio, up and running, I started tinkering with STT.</p>
<h2 id="step-1---adding-the-coquistt-dependency-and-downloading-a-model">Step 1 - Adding the CoquiSTT dependency and downloading a model</h2>
<p>Adding the CoquiSTT was as easy as doing <code>yarn add stt@1.3.0</code> (truth is, I worked with version 1.2.0 and had to hack around to add Electron 16 support to CoquiSTT; but that resulted in my first contribution to CoquiSTT which <a href="https://github.com/coqui-ai/STT/releases/tag/v1.3.0">got live in 1.3.0</a>!). I then downloaded the Mozilla Italian model from the <a href="https://coqui.ai/models">Coqui Models page</a> and unpacked it in <code>Signal-Desktop/models/it</code>.</p>
<h2 id="step-2---build-an-abstraction-around-the-stt-low-level-apis">Step 2 - Build an abstraction around the STT low level APIs</h2>
<p>The core of the integration is in the <a href="https://github.com/Dexterp37/Signal-Desktop/blob/b27cf082c21331c7a2863b0e39cdc496facc15d6/ts/stt/SpeechToText.ts"><code>SpeechToText.ts</code> file</a>, which exports two main functions <code>start()</code> and <code>getText(url)</code>.</p>
<p>The <code>start</code> function gets called at startup, when Signal attempts to load other things as well (e.g. stickers, emojis). It’s responsible for loading the model downloaded in the previous step using the <a href="https://stt.readthedocs.io/en/latest/NodeJS-API.html#Model">Model class</a> and for setting up the conversion mechanisms so that the sample rate of the voice messages will match the one used in the downloaded model.
The beauty of the “Web as a platform” enables us to do such conversion automagically by creating an <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext">AudioContext</a> with a <code>sampleRate</code> that matches the one from the model. Whenever the context will be used, the sample rate conversion will automatically take place.</p>
<div><pre tabindex="0"><code data-lang="typescript"><span>export</span> <span>async</span> <span>function</span> <span>start</span>()<span>:</span> <span>Promise</span>&lt;<span>void</span>&gt; {
  <span>log</span>.<span>info</span>(<span>`SpeechToText.start: loading model at </span><span>${</span><span>BASE_MODEL_PATH</span><span>}</span><span> with Coqui v </span><span>${</span><span>coquiSTT</span>.<span>Version</span>()<span>}</span><span>`</span>);
  <span>try</span> {
    <span>// Yes, file names are hardcoded for the sake of building a working PoC fast :-)
</span><span></span>    <span>activeModel</span> <span>=</span> <span>new</span> <span>coquiSTT</span>.<span>Model</span>(<span>path</span>.<span>join</span>(<span>BASE_MODEL_PATH</span>, <span>&#34;model.tflite&#34;</span>));
    <span>activeModel</span>.<span>enableExternalScorer</span>(<span>path</span>.<span>join</span>(<span>BASE_MODEL_PATH</span>, <span>&#34;it-mzit-1-prune-kenlm.scorer&#34;</span>));
    <span>log</span>.<span>info</span>(<span>`SpeechToText.start: model at </span><span>${</span><span>BASE_MODEL_PATH</span><span>}</span><span> successfully loaded`</span>);
  } <span>catch</span> (<span>e</span>) {
    <span>log</span>.<span>error</span>(<span>&#34;SpeechToText.start: failed to load model&#34;</span>, <span>e</span>);
    <span>activeModel</span> <span>=</span> <span>undefined</span>;
    <span>return</span>;
  }

  <span>// Create an audio context for future processing.
</span><span></span>  <span>audioContext</span> <span>=</span> <span>new</span> <span>AudioContext</span>({
    <span>// Use the model&#39;s sample rate so that the decoder will resample
</span><span></span>    <span>// for us when calling `getText`.
</span><span></span>    <span>sampleRate</span>: <span>activeModel.sampleRate</span>()
  });
  <span>audioContext</span>.<span>suspend</span>();
}
</code></pre></div><p>The <code>getText(url)</code> takes an URI representing the location of the audio sample (the actual voice message), <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">fetches</a> it and then uses the previously initialized audio context to an raw audio buffer with a sample rate that matches the one of the model. That’s as simple as calling <a href="https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/decodeAudioData">decodeAudioData</a> (again, the beauty of the Web as a platform). The Coqui STT APIs expect the audio values to be an array of 16-bit, mono raw audio samples. Unfortunately the audio context returns a <code>Float32Array</code> so some conversion is needed before the audio files can be processed.</p>
<p>The converted, raw buffer can then be fed to the loaded model by <a href="https://stt.readthedocs.io/en/latest/NodeJS-API.html#Model.createStream">creating a stream</a>, <a href="https://stt.readthedocs.io/en/latest/NodeJS-API.html#StreamImpl.feedAudioContent">feeding it</a> with the buffer, and then <a href="https://stt.readthedocs.io/en/latest/NodeJS-API.html#StreamImpl.finishStream">waiting for the transcribed text to be computed</a>.</p>
<div><pre tabindex="0"><code data-lang="typescript"><span>export</span> <span>async</span> <span>function</span> <span>getText</span>(<span>url</span>: <span>string</span>)<span>:</span> <span>Promise</span>&lt;<span>string</span>&gt; {
  <span>if</span> (<span>!</span><span>activeModel</span> <span>||</span> <span>!</span><span>audioContext</span>) {
    <span>throw</span> <span>new</span> Error(
      <span>&#39;SpeechToText.start() must be successfully called before transcribing messages&#39;</span>
    );
  }

  <span>log</span>.<span>info</span>(<span>`SpeechToText.getText: transcribing </span><span>${</span><span>url</span><span>}</span><span> with model </span><span>${</span><span>BASE_MODEL_PATH</span><span>}</span><span>`</span>);
  <span>const</span> <span>response</span> <span>=</span> <span>await</span> <span>fetch</span>(<span>url</span>);
  <span>const</span> <span>raw</span> <span>=</span> <span>await</span> <span>response</span>.<span>arrayBuffer</span>();
  <span>const</span> <span>audioBuffer</span> <span>=</span> <span>await</span> <span>audioContext</span>.<span>decodeAudioData</span>(<span>raw</span>);

  <span>if</span> (<span>audioBuffer</span>.<span>sampleRate</span> <span>!=</span> <span>activeModel</span>.<span>sampleRate</span>()) {
    <span>// In practice, this should never happen. The audio context should do its magic
</span><span></span>    <span>// for us to prevent it.
</span><span></span>    <span>throw</span> <span>new</span> Error(
      <span>`SpeechToText.getText: message rate </span><span>${</span><span>audioBuffer</span>.<span>sampleRate</span><span>}</span><span>,  model </span><span>${</span><span>activeModel</span>.<span>sampleRate</span>()<span>}</span><span>`</span>
    );
  }

  <span>let</span> <span>processedData</span> <span>=</span> <span>converFloat32ToInt16</span>(<span>audioBuffer</span>.<span>getChannelData</span>(<span>0</span>));
  <span>let</span> <span>modelStream</span> <span>=</span> <span>activeModel</span>.<span>createStream</span>();
  <span>modelStream</span>.<span>feedAudioContent</span>(<span>Buffer</span>.<span>from</span>(<span>processedData</span>.<span>buffer</span>));
  <span>return</span> <span>modelStream</span>.<span>finishStream</span>();
}
</code></pre></div><h2 id="step-3---wiring-things-together">Step 3 - Wiring things together</h2>
<p>The easiest way that I found to stitch everything together was to add a specialized reactjs effect to perform the speech to text transcribing for messages that had “audio attachments”, in MessageAudio.tsx. This was as easy as calling the previously discussed <code>getText</code> function and providing it with the URL of the audio attachment. I slightly extended the HTML of the component showing the message in Signal in order for it to support showing the transcribed audio once it’s available. See <a href="https://github.com/Dexterp37/Signal-Desktop/commit/4600c55d3f8dd45687507e6c9a23d3dadd18ec4e">this commit</a>.</p>

<p>This was a fun experiment, but nowhere near production ready. I really wish this was a polished feature of the products I use the most! While the current implementation relies on models to be downloaded, the major upside is that the raw unencrypted audio streams never have to leave the local machine. The speech to text computation process is completely offline. Aside from the transcription quality, there are a few companion features that I believe would make such a capability amazing:</p>
<ul>
<li>once a transcription is computed, we should be able to save it locally and not automatically retrigger STT next time (unless requested so by the user, in case a new model is available); this would allow us to save processing time and make loading huge conversations much faster;</li>
<li>with transcriptions being safely stored to the disk, we could even think about making audio messages searchable, by indexing the transcribed text. That would make my days so much better;</li>
<li>allow users to tweak the transcribed text, to manually fix wrong transcriptions and make searches more effective; who knows, maybe in the future this could feed back into the model via <a href="https://arxiv.org/abs/1511.03575">federated learning</a> or <a href="https://arxiv.org/abs/1911.00731">one-shot distributed learning</a>.</li>
</ul>


  </div></div>
  </body>
</html>
