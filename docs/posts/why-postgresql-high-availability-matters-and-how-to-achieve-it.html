<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.yugabyte.com/postgresql/postgresql-high-availability/">Original</a>
    <h1>Why PostgreSQL High Availability Matters and How to Achieve It</h1>
    
    <div id="readability-page-1" class="page"><div><p>Ensuring your application can handle failures and outages is crucial, and the availability of your application is only as good as the availability of your PostgreSQL instance. With that in mind, you may be wondering which PostgreSQL high availability (HA) deployment option is best for your application.</p><p>Let’s review several popular solutions that increase the high availability of PostgreSQL deployments and, as a result, the availability overall of your application. Why several and not one? Well, there’s no silver bullet or one-size-fits-all solution when it comes to high availability and PostgreSQL. So, walk through the options for a highly available deployment of PostgreSQL and then you can make a choice that fits your use case.</p><p><iframe title="PostgreSQL High Availability Options" width="580" height="326" frameborder="0" allowfullscreen="allowfullscreen" data-lazy-type="iframe" data-src="https://www.youtube.com/embed/wEYGAxDuV8Q?feature=oembed"></iframe><br/> </p><h2 id="difference-between-high-availability-and-disaster-recovery"><a href="#difference-between-high-availability-and-disaster-recovery">Difference Between High Availability and Disaster Recovery</a></h2><p>High availability should not be confused with disaster recovery (DR). Both relate to and influence each other during an application’s design and development, but HA differs significantly from DR.</p><p><img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/high-availability-vs-disaster-recovery-600x263.png" alt="" width="600" height="263"/></p><p><b>High availability </b>refers to a system’s ability to operate continuously by removing the possibility of a single point of failure.</p><p><b>Disaster recovery</b> is the process of getting the system back to normal operations after a failure or outage.</p><p>This article focuses on how you can make a PostgreSQL deployment highly available by eliminating a single point of failure. However, disaster recovery techniques are still relevant when discussing details of the failover process.</p><h2 id="how-to-measure-high-availability"><a href="#how-to-measure-high-availability">How to Measure High Availability</a></h2><p>Sooner or later, every system experiences a failure or an outage. Even if a company claims that “our service is here for you 24×7”, in reality, that service runs on software and hardware that can’t provide the same guarantees. For instance, disks can be corrupted, or an application instance can crash due to a memory overflow caused by a sudden load spike.</p><p>How should high availability be measured, then? Well, it can be measured by defining and committing to a certain <b>uptime</b> that’s part of your availability SLA (Service Level Agreement).</p><p><img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/yugabytedb-postgresql-ha-seo-guide-01-availability-sla.svg" alt="Availability SLA" width="600" height="600"/></p><p>If your 24×7 service guarantees three 9s (99.9%) of uptime, then it can still be unavailable up to 8.76 hours a year. If you commit to five 9s (99.999%) of uptime, your service can only be down 5.25 mins per year. This is a significant difference.</p><p>So how should you calculate those 9s for your use case? Should you provide three, four, or even six 9s of uptime?</p><p>The most accurate (and straightforward) way to calculate target uptime is to figure out your recovery point objective (RPO) and recovery time objective (RTO). Those two metrics will influence and eventually help you determine needed uptime.</p><p><i>Note: Even though RPO and RTO are characteristics and metrics for disaster recovery, you should still use them to determine your availability SLAs and uptime.</i></p><p><b>Recovery time objective</b> (RTO) is the amount of time within which the service must be restored and returned to normal operations.</p><p><img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/yugabytedb-postgresql-ha-seo-guide-01-availability-sla-rto.svg" alt="Availability SLA Recovery time objective" width="600" height="600"/></p><p>For some services, the RTO must be as low as a few seconds; while for others, it can be as much as several hours.</p><p><b>Recovery point objective</b> (RPO) is the maximum acceptable amount of data loss after an incident/failure/outage. Usually, data loss is measured in bytes/megabytes/etc., by the number of lost transactions, or by the last seconds/minutes/etc. of changes.</p><p><img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/yugabytedb-postgresql-ha-seo-guide-01-rpo-availability-sla.svg" alt="Recovery point objective" width="600" height="600"/></p><p>For some services, there is no amount data loss that is acceptable (RPO=0), while for others, the loss of the last two minutes of data changes won’t disrupt the business or impact users.</p><p>Once you know your RTO and RPO, you can define the availability SLA for your application. And once you know your SLA, you can pick the most suitable PostgreSQL deployment option and maximize its high availability.</p><h2 id="postgresql-deployment-options-for-high-availability"><a href="#postgresql-deployment-options-for-high-availability">PostgreSQL Deployment Options for High Availability</a></h2><p>Let’s review how to approach and increase high availability for the following PostgreSQL deployment options:</p><ol><li aria-level="1"><b>Single database instance</b>—This standard PostgreSQL deployment option has a single instance that handles all reads and writes.</li><li aria-level="1"><b>Single primary instance with read replicas</b>— This deployment option has a single primary PostgreSQL instance and one or more read replicas. The primary serves both writes and reads, while the replicas get only the read traffic.</li><li aria-level="1"><b>Multi-master (sharded PostgreSQL with a coordinator)</b>—Data is sharded across multiple standalone PostgreSQL instances (aka. nodes or workers). A coordinator node routes requests among those standalone instances.</li><li aria-level="1"><b>Multi-master (sharded PostgreSQL without the coordinator)—</b>Data is distributed across multiple database nodes that can communicate with each other. In other words, it is a shared-nothing architecture without the coordinator node.</li></ol><p>Let’s now look at how high availability is achieved or maximized for these PostgreSQL deployment options.</p><ol><li aria-level="1"><h3 id="high-availability-with-single-postgresql-database-instance"><a href="#high-availability-with-single-postgresql-database-instance">High Availability With Single PostgreSQL Database Instance</a></h3><p>A single database instance is used when the compute (CPU) and capacity (storage, memory) resources are sufficient to handle the application read and write workloads.</p><p>To maximize high availability for this deployment option, the primary PostgreSQL instance needs to be complemented with standby servers. Changes are replicated from the primary to standbys, and a failover/failback solution simplifies the switch between the servers during an outage.<img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/YugabyteDB_PostgreSQL-HA-SEO_Guide_HA-Single-Database-Instance_01.svg" alt="High Availability with Single Database Instance" width="600" height="600" data-wp-editing="1"/></p><p>The primary and standby instances are your ordinary PostgreSQL servers. So, there’s nothing more to elaborate on here. However, the replication and failover/failback solutions depend on your RTO, RPO, availability SLA, and preferences. So let’s take a look at those.</p><h4 id="postgresql-replication-solutions"><a href="#postgresql-replication-solutions">PostgreSQL Replication Solutions</a></h4><p>PostgreSQL offers many <a href="https://www.postgresql.org/docs/current/different-replication-solutions.html" target="_blank" rel="noopener">replication solutions</a>. Two commonly used ones are streaming and logical replication.</p><p><b>Logical replication</b> also works with the WAL records of the primary instance. But, instead of sending the changes as-is in the binary format, the WAL records are decoded into logical changes that are then replicated to the standbys. You can pick which tables to replicate, and the primary and standbys configuration doesn’t need to be the same.</p><p>Both streaming and logical replication support several synchronization modes that you can pick based on your RPO, RTO and performance requirements.</p><div><h4 id="a-chat-about-postgresql-replication-modes"><a href="#a-chat-about-postgresql-replication-modes">A Chat About PostgreSQL Replication Modes<img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/yugabytedb-postgresql-ha-seo-guide-01-asynchronous-synchronous-replication.svg" alt="YugabyteDB postgresql ha seo asynchronous synchronous replication" width="600" height="600"/></a></h4><p>The first three modes from the top are used if changes from the primary to standbys need to be replicated <b>synchronously</b>. For instance, with the <i>remote_write</i> mode, the primary waits while standbys acknowledge that a change is written to their file systems, but has not necessarily reached durable storage.</p><p>With the <i>synchronous_commit=on</i> mode, the primary waits while the change is flushed to the durable storage of the standbys. The <i>remote_apply</i> mode provides the guarantees of the previous two, plus ensures that the change will be visible immediately to all of the queries executed on the standbys.</p><p>If you’re RPO=0, meaning no data loss is acceptable, then changes must be replicated in the <i>synchronous_commit=on </i>or<i> synchronous_commit=remote_apply.</i></p><p>If the RPO &gt; 0, you can improve performance by using either the synchronous <i>remote_write</i> mode or two remaining <b>asynchronous</b> modes – <i>local</i> and <i>off</i>. The <i>local</i> mode waits while the change is flushed to durable storage on the primary instance, but doesn’t wait for an acknowledgment from the standbys. The <i>off</i> mode doesn’t even wait while the change is flushed to disk on the primary.</p></div><h4 id="failover-and-failback-solutions-for-postgresql"><a href="#failover-and-failback-solutions-for-postgresql">Failover and Failback Solutions for PostgreSQL</a></h4><p>Having standbys that receive updates from the primary improves high availability by removing the single point of failure represented by the primary. The standbys can take over if the primary becomes unavailable.</p><p>The process for promoting a standby to the new primary (in case the current primary has failed) is known as the <b>failover</b>. After restoring the failed primary, you can instantiate a reverse process called <b>failback</b> to reinstate it as the main instance.</p><p>Failover and failback do not happen instantly. The duration of the failover and failback procedures affects your RTO and uptime. Therefore, automating these procedures as much as possible is crucial for improving their efficiency.</p><p><a href="https://pgpool.net/mediawiki/index.php/Main_Page">Pgpool </a>and <a href="https://github.com/zalando/patroni">Patroni</a> are widely used to automate failover and failback processes. We’ll discuss <a href="#high-availability-with-multi-master-deployments-with-coordinator">Patroni later</a> in relation to the multi-master deployments with a coordinator. As for Pgpool, let’s see how it can be used by the deployments with a single primary and one or more standbys.</p><h4 id="a-sample-configuration-for-high-availability-with-single-postgresql-database-instance"><a href="#a-sample-configuration-for-high-availability-with-single-postgresql-database-instance">A Sample Configuration for High Availability With Single PostgreSQL Database Instance</a></h4><p>Let’s combine all the pieces and look at a sample, highly available (HA) configuration for deployments with a single primary instance. Assume that your solution is deployed in one of the regions of a public cloud environment:<img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/YugabyteDB_PostgreSQL-HA-SEO_Guide_Configuration-Single-Standby_02-1.svg" alt="Sample PostgreSQL High Availability Configuration with Single Standby" width="600" height="600"/></p><p>The <b>client</b> application connects to a <b>virtual IP</b> address whose configuration supports the sending of traffic across zone A and B. During normal operations, the virtual IP forwards requests to the <b>active instance of Pgpool</b> in availability zone A. The <b>primary</b> PostgreSQL instance runs in the same zone and receives requests from the abovementioned Pgpool instance.</p><p>The <b>standby</b> instance is located in availability zone B. Because the current solution requires RPO=0, the replication from the primary to standby is synchronous (NOTE: For the synchronous replication, you should have 2+ standby, otherwise, the primary will get stuck if a single standby instance becomes unavailable).</p><p>A highly available architecture assumes that any single points of failure will be eliminated, which is why the primary Pgpool instance from zone A is complemented with a standby one in zone B. The <b>watchdog</b> instances of Pgpool work in both zones and monitor the healthiness of the Pgpool processes.</p><p>Now, imagine that the primary PostgreSQL instance goes down. If that happens, the active Pgpool instance in zone A will failover to the standby PostgreSQL instance in zone B. If the entire zone A goes down, the virtual IP can be reassigned to the standby Pgpool instance in zone B, and that instance will forward all requests to the standby PostgreSQL instance within the same zone.</p></li><li aria-level="1"><h3 id="high-availability-with-single-primary-instance-and-read-replicas"><a href="#high-availability-with-single-primary-instance-and-read-replicas">High Availability With Single Primary Instance and Read Replicas</a></h3><p>The previous deployment option uses the standby PostgreSQL instances to improve availability. These instances can either remain isolated from all application workloads and wait to be promoted as a primary during an outage, or they can be utilized as read replica instances to scale reads.</p><p>An HA solution for deployments with read replicas is not much different from deployments utilizing standbys:</p><p>As with the failover/failback solution, there are many options for the load balancing solution. Consider <a href="http://www.haproxy.org/" target="_blank" rel="noopener">HAProxy</a>, Pgpool, or a cloud native load balancer if you want to deploy PostgreSQL in the cloud.</p><h4 id="a-sample-configuration-for-high-availability-with-single-primary-instance-and-read-replicas"><a href="#a-sample-configuration-for-high-availability-with-single-primary-instance-and-read-replicas">A Sample Configuration for High Availability With Single Primary Instance and Read Replicas</a></h4><p>As long as Pgpool can be used for failover, load balancing, and connection pooling, let’s continue using that tool for the sample HA setup of the deployment with a read replica.</p><p>Depending on the <b>replication</b> mode between the primary and replica, reads from the replica can return the latest data or have a data lag. You select between synchronous and asynchronous replication modes based on your RPO, RTO, and other requirements.</p><p>The <b>failover/failback</b> setup is similar to the one used for the deployment with standbys discussed earlier.</p></li><li aria-level="1"><h3 id="high-availability-with-multi-master-deployments-with-coordinator"><a href="#high-availability-with-multi-master-deployments-with-coordinator">High Availability With Multi-Master Deployments With Coordinator</a></h3><p>You use <a href="https://www.yugabyte.com/tech/database-sharding/">sharding</a> when a single PostgreSQL primary instance is no longer enough for the application’s needs. Some apps need to scale out to improve (or get) unbound storage capacity. Some need to scale out to get more CPUs and load balance writes (in addition to reads). Some may even need both.</p><p>Sharding is a simple yet powerful method for distributing a data set across multiple database instances.</p><h4 id="sharding-with-postgresql"><a href="#sharding-with-postgresql">Sharding With PostgreSQL</a></h4><h4 id=""><a href="#"></a></h4><p>You can manually implement sharding or use an existing 3rd party solution to automate the process. One such solution is <a href="https://www.citusdata.com/">CitusData</a>, a PostgreSQL extension that simplifies the sharding of PostgreSQL deployments.<img decoding="async" loading="lazy" src="https://www.yugabyte.com/wp-content/plugins/a3-lazy-load/assets/images/lazy_placeholder.gif" data-lazy-type="image" data-src="https://www.yugabyte.com/wp-content/uploads/2023/06/yugabytedb-postgresql-ha-seo-guide-01-worker-nodes.svg" alt="YugabyteDB postgresql ha seo worker nodes" width="600" height="600"/></p><h4 id="complete-sample-configuration-for-high-availability-with-multi-master-deployments-with-coordinator"><a href="#complete-sample-configuration-for-high-availability-with-multi-master-deployments-with-coordinator">Complete Sample Configuration for High Availability with Multi-Master Deployments with Coordinator</a></h4><p>A sample of a highly available configuration for the CitusData deployment can look like this:</p><ul><li aria-level="1">The <b>client</b> connects to <b>HAProxy</b>, which routes all requests to the <b>primary Citus coordinator</b>. In the case of a coordinator outage, this proxy will forward requests to the <b>coordinator’s standby</b>.</li><li aria-level="1">The Citus primary coordinator is connected to <b>PgBouncer connection poolers</b> that are deployed alongside each <b>worker node</b> (PostgreSQL instance). The Citus coordinator forwards application requests to the worker nodes using connections from of those pools.</li><li aria-level="1">Each <b>primary worker instance</b> has its own <b>standby instances</b> (or replica instances). The changes are replicated from a primary to an associated standby. So for a RPO=0 scenario using synchronous replication, it is crucial to have at least two standbys for each primary. This ensures that the primary can successfully commit changes even if one standby becomes unavailable.</li><li aria-level="1"><a href="https://github.com/zalando/patroni">Patroni</a> agents are deployed alongside primary and standby workers, as well as primary and standby coordinator instances. Patroni monitors the state and healthiness of the cluster and takes care of the failover/failback procedures if there’s an outage.</li><li aria-level="1">Patroni agents use <a href="https://etcd.io">etcd</a> cluster to reach consensus for various operations, including failover and failback.</li></ul></li><li aria-level="1"><h3 id="high-availability-for-multi-master-deployments-without-coordinator"><a href="#high-availability-for-multi-master-deployments-without-coordinator">High Availability for Multi-Master Deployments Without Coordinator</a></h3><p>Generally, the fewer components a system has, the more reliable it is. If you need to shard PostgreSQL, but want a simpler solution in terms of high availability, then a multi-master deployment without the coordinator is a suitable option.</p><p>In reality, removing the coordinator from existing sharding solutions for PostgreSQL is challenging. The main obstacle is the lack of direct communication among PostgreSQL workers (instances). Each worker is a standalone database server that has no knowledge of any other worker in the deployment.</p><p>So what options exist if you want to continue using PostgreSQL but run on a simple scalable and fault-tolerant architecture? A great option is <a href="https://www.yugabyte.com/">YugabyteDB</a>, a cloud native distributed SQL database built on PostgreSQL source code.</p><p>YugabyteDB’s query layer is the PostgreSQL query layer (source code) with some enhancements to make it possible for the PostgreSQL’s parser/analyzer/planner/executor to execute requests over YugabyteDB’s distributed storage layer—DocDB.</p><p>YugabyteDB nodes can (and do) talk to each other, which explains why there is no single coordinator node in the database architecture.</p><p>Changes are replicated using the Raft protocol.</p><h4 id="a-sample-configuration-for-high-availability-with-multi-master-deployment-without-coordinator"><a href="#a-sample-configuration-for-high-availability-with-multi-master-deployment-without-coordinator">A Sample Configuration for High Availability with Multi-Master Deployment without Coordinator</a></h4><p>YugabyteDB is built on a shared-nothing architecture, with nodes that communicate with each other. Therefore, it doesn’t require any external components for highly available deployments. YugabyteDB is built with high availability in mind. So, a highly available and scalable architecture with YugabyteDB can be as simple as:</p><p>The nodes deal with potential outages themselves. For instance, if one node goes down, the others will detect the outage. Since the remaining nodes have a redundant and consistent copy of data, they will start immediately processing application requests that were previously sent to the failed node. YugabyteDB provides RPO=0 (no data loss) and <a href="https://dev.to/yugabyte/yugabytedb-recovery-time-objective-rto-with-pgbench-continuous-availability-with-max-15-seconds-latency-on-failure-2po4" target="_blank" rel="noopener">RTO within the range of 3-15 seconds</a> (depending on the database and TCP/IP configuration defaults).</p></li></ol><h2 id="summary"><a href="#summary">Summary</a></h2><p>Whether you use a single PostgreSQL database server, or shard data across multiple database instances, you need to ensure that your deployment can withstand various outages and failures.</p><p>Know your RPO, RTO, and availability SLAs, and then choose a highly available configuration that will keep your PostgreSQL deployment running—even in the face of the most adverse of incidents.</p><h2 id="postgresql-high-availability-faq"><a href="#postgresql-high-availability-faq">PostgreSQL High Availability FAQ</a></h2><ul><li aria-level="1"><b>Does PostgreSQL support high availability?</b></li><li aria-level="1"><b>What are the main differences between high availability and disaster recovery?</b></li><li aria-level="1"><b>How to make PostgreSQL highly available?</b></li><li aria-level="1"><b>What are the methods to measure high availability?</b></li></ul></div></div>
  </body>
</html>
