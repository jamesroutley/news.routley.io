<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/">Original</a>
    <h1>Wasmi v0.32: WebAssembly interpreter is now faster than ever</h1>
    
    <div id="readability-page-1" class="page"><div><article><div><p>After many months of research, development and QA, Wasmi’s most significant update ever is finally ready for production use.</p><p><a href="https://github.com/wasmi-labs/wasmi">Wasmi</a> is an efficient and versatile <a href="https://webassembly.org/">WebAssembly</a> (Wasm) interpreter with a focus on embedded environments. It is an excellent choice for plugin systems, cloud hosts and as smart contract execution engine.</p><p>Wasmi intentionally mirrors the Wasmtime API on a best-effort basis, making it an ideal drop-in replacement or prototyping runtime.</p><blockquote><p>Install <a href="https://crates.io/crates/wasmi_cli">Wasmi’s CLI tool</a> via <code>cargo install wasmi_cli</code> or use it as library via the <a href="https://crates.io/crates/wasmi"><code>wasmi</code> crate</a>.</p></blockquote><p>Wasmi v0.32 comes with a new execution engine that utilizes register-based bytecode enhancing its execution performance by a factor of up to 5.
Additionally, its startup performance has been improved by several orders of magnitudes thanks to lazy compilation and other new techniques.</p><p>The <a href="https://github.com/wasmi-labs/wasmi/releases/tag/v0.32.0">changelog for v0.32</a> is huge and the following sections will present the most significant changes.</p><p>Wasmi is a <em>rewriting interpreter</em>, meaning that it rewrites the incoming WebAssembly bytecode into Wasmi’s own internal bytecode that is geared towards efficient execution performance.</p><p>This re-writing is what we call <em>compilation</em> or <em>translation</em> in an interpreter. This is not to be confused with compiling the Wasmi interpreter itself.</p><h3 id="why-is-translation-speed-important-for-wasmi">Why is translation speed important for Wasmi?</h3><p>Fast translation enables a fast startup time which is the time spent until the first instruction is executed.</p><p>As an interpreter, Wasmi is naturally optimized for fast startup times, making it well-suited for translation-intensive workloads where the time required to translate a Wasm binary exceeds the time needed to execute it.</p><h3 id="lazy-translation">Lazy Translation</h3><p>Translation can be costly, especially with the new register-based bytecode. To address this, lazy translation has been implemented, translating only the parts of the Wasm binary necessary for execution.</p><p>Wasmi supports 3 different modes of translation:</p><ul><li><code>Eager</code>: Code is eagerly validated and eagerly translated ahead of time.<ul><li><strong>Note:</strong> This is the default mode for Wasmi v0.32.</li></ul></li><li><code>Lazy</code>: Code is lazily translated and lazily validated.<ul><li><strong>Note:</strong> One downside is that this allows for partially validated Wasm modules which are controversial within the wider Wasm community. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li></ul></li><li><code>LazyTranslation</code>: Code is lazily translated but eagerly validated.<ul><li><strong>Note:</strong> While slower than <code>Lazy</code> this fixes the problem with partially validated Wasm modules.</li></ul></li></ul><h4 id="usage-as-library">Usage as Library</h4><div><pre tabindex="0"><code data-lang="rust"><span><span><span>let</span> <span>mut</span> c <span>=</span> wasmi::Config::default();
</span></span><span><span>c.compilation_mode(wasmi::CompilationMode::Lazy);
</span></span></code></pre></div><h4 id="usage-in-wasmis-cli">Usage in Wasmi’s CLI</h4><p>Wasmi CLI now supports the command-line option <code>--compilation-mode=&lt;mode&gt;</code> where <code>&lt;mode&gt;</code> is one of <code>eager</code>, <code>lazy</code>, or <code>lazy-translation</code>.</p><h3 id="unchecked-translation">Unchecked Translation</h3><p>Wasmi validates the Wasm binary which accounts for roughly 20-40% of the total time spent during the startup phase.
However, some users might want to skip Wasm validation altogether since they know ahead of time that used Wasm binaries are pre-validated. This is now possible via the <code>unsafe fn Module::new_unchecked</code> API.</p><h3 id="non-streaming-translation">Non-streaming Translation</h3><p>Wasmi v0.31 and earlier always used streaming translation to process their Wasm input. However, in practice most users never even made use of this, so in v0.32 Wasmi uses non-streaming translation by default which gives it yet another nice performance win.</p><blockquote><p>Users who actually want to use streaming translation simply can use the new <code>Module::new_streaming</code> API for their needs.</p></blockquote><h3 id="linker-caching">Linker Caching</h3><p>The Wasmi <code>Linker</code> is used to define the set of host functions that a Wasm binary can use to communicate with the host. Oftentimes, dozens of host functions are defined, which can quickly become costly.</p><p>To address this, Wasmi now offers a <code>LinkerBuilder</code>, which allows to efficiently instantiate new <code>Linker</code>s after the initial setup. <sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p><p>Benchmarks with 50 defined host functions have demonstrated a 120x speedup using this approach.</p><h2 id="benchmarks">Benchmarks</h2><p>By combining all of the techniques above it is possible to speed up the startup time of Wasmi by several orders of magnitudes compared to the previous Wasmi v0.31.</p><p>The newest versions of all Wasm runtimes have been used at the time of writing this article. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p><blockquote><p>Currently, Winch only supports <code>x86_64</code> platforms and therefore was only tested on those systems.</p></blockquote><h3 id="erc-20---7kb">ERC-20 - 7KB</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/erc20.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/erc20.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/erc20.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/erc20.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/erc20.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/erc20.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/erc20.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/erc20.svg" alt=""/></a></td></tr></tbody></table><h3 id="argon2---61kb">Argon2 - 61KB</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/argon2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/argon2.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/argon2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/argon2.svg" alt=""/></a></td></tr></tbody></table><h3 id="bz---147kb">BZ - 147KB</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/bz2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/bz2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/bz2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/bz2.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/bz2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/bz2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/bz2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/bz2.svg" alt=""/></a></td></tr></tbody></table><h3 id="pulldown-cmark---16mb">Pulldown-Cmark - 1.6MB</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/pulldown-cmark.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/pulldown-cmark.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/pulldown-cmark.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/pulldown-cmark.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/pulldown-cmark.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/pulldown-cmark.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/pulldown-cmark.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/pulldown-cmark.svg" alt=""/></a></td></tr></tbody></table><h3 id="spidermonkey---42mb">Spidermonkey - 4.2MB</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/spidermonkey.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/spidermonkey.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/spidermonkey.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/spidermonkey.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/spidermonkey.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/spidermonkey.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/spidermonkey.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/spidermonkey.svg" alt=""/></a></td></tr></tbody></table><h3 id="ffmpeg---193mb">FFMPEG - 19.3MB</h3><blockquote><p><strong>Note:</strong> Wasmtime (Cranelift) timed out and
Stitch failed to compile <code>ffmpeg.wasm</code>.</p></blockquote><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/ffmpeg.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/compile/ffmpeg.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/ffmpeg.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/compile/ffmpeg.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/ffmpeg.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/compile/ffmpeg.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/ffmpeg.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/compile/ffmpeg.svg" alt=""/></a></td></tr></tbody></table><h3 id="translation-benchmarks-conclusion">Translation Benchmarks: Conclusion</h3><p>Wasmi and Wasm3 perform best by far due to their lazy compilation capabilities. As expected, optimizing JIT-based Wasm runtimes like Wasmtime and Wasmer perform worse in this context. Single-pass JITs, which are designed for fast startup, such as <a href="https://crates.io/crates/wasmtime-winch">Winch</a> and <a href="https://crates.io/crates/wasmer-compiler-singlepass">Wasmer Singlepass</a>, are also significantly slower. Despite also using lazy translation, <a href="https://github.com/makepad/stitch">Stitch</a>’s translation performance is not ideal. However, it is important to note that both Winch and Stitch are still in an experimental phase of their development and improvements are to be expected.</p><h2 id="execution-speed">Execution Speed</h2><p>For an execution engine, the speed of computation is naturally of paramount importance. Unfortunately, the old Wasmi v0.31 left much to be desired in this regard.</p><h2 id="register-based-bytecode">Register-Based Bytecode</h2><p>The old Wasmi v0.31 internally uses a stack-based intermediate representation (IR) to drive execution. This IR is similar to WebAssembly bytecode and thus allows for fast translation times.</p><p>Stack-based IRs generally use more instructions to represent the same problem as register-based IRs. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> However, the performance of interpreters is mostly dictated by the dispatch of instructions. Hence, it is usually a good tradeoff to execute fewer instructions even if every executed instruction is more complex. <sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></p><p>This is why starting with version 0.32 Wasmi now uses a register-based IR to drive its execution.</p><h2 id="memory-consumption">Memory Consumption</h2><p>The new register-based IR was carefully designed to enhance execution performance and to minimize memory usage. As the vast majority of a Wasm binary is comprised of encoded instructions, this substantially decreases memory usage and enhances cache efficiency when executing Wasm through Wasmi. <sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p><h2 id="benchmarks-1">Benchmarks</h2><p>The newest versions of all Wasm runtimes have been used at the time of writing this article. <sup id="fnref1:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p><h3 id="fibonacci-iterative---compute-intense">Fibonacci (Iterative) - Compute Intense</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/fib.iterative.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/fib.iterative.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/fib.iterative.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/fib.iterative.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/fib.iterative.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/fib.iterative.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/fib.iterative.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/fib.iterative.svg" alt=""/></a></td></tr></tbody></table><h3 id="fibonacci-recursive---call-intense">Fibonacci (Recursive) - Call Intense</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/fib.recursive.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/fib.recursive.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/fib.recursive.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/fib.recursive.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/fib.recursive.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/fib.recursive.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/fib.recursive.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/fib.recursive.svg" alt=""/></a></td></tr></tbody></table><p>Wasmi v0.32 has not significantly improved over v0.31 in this test case. This is partly because Wasmi v0.31 was already comparatively fast and that the new register-based bytecode favors compute-intense workloads over call-intense ones. This usually is a good tradeoff since most Wasm producers (such as LLVM) produce compute intense workloads due to aggressive inlining.</p><h3 id="primes---balanced">Primes - Balanced</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/primes.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/primes.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/primes.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/primes.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/primes.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/primes.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/primes.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/primes.svg" alt=""/></a></td></tr></tbody></table><h3 id="matrix-multiplication---memory-intense">Matrix Multiplication - Memory Intense</h3><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/matmul.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/matmul.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/matmul.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/matmul.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/matmul.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/matmul.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/matmul.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/matmul.svg" alt=""/></a></td></tr></tbody></table><p>Interestingly, Wasmer (Singlepass) seems to have some trouble on Apple silicon being even slower than some of the interpreters.</p><h3 id="argon---compute-hash">Argon - Compute Hash</h3><blockquote><p><strong>Note:</strong> Stitch and Winch could not execute the <code>argon2.wasm</code> test case.</p></blockquote><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-epyc-7763/execute/argon2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/linux-amd-threadripper-3990x/execute/argon2.svg" alt=""/></a></td></tr><tr><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/macos-m2/execute/argon2.svg" alt=""/></a></td><td><a href="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/argon2.svg"><img loading="lazy" src="https://wasmi-labs.github.io/blog/posts/wasmi-v0.32/benches/windows-intel-i7-14700k/execute/argon2.svg" alt=""/></a></td></tr></tbody></table><p>The following table shows Coremark scores for the Wasm interpreters by CPU. <sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup></p><table><thead><tr><th></th><th>AMD Epyc 7763</th><th>AMD Threadripper 3990x</th><th>Apple M2 Pro</th><th>Intel i7 14700K</th></tr></thead><tbody><tr><td>Wasmi v0.31</td><td>657</td><td>944</td><td>884</td><td>1759</td></tr><tr><td>Wasmi v0.32</td><td><strong>1457</strong></td><td>1779</td><td>1577</td><td>2979</td></tr><tr><td>Tinywasm</td><td>235</td><td>339</td><td>592</td><td>772</td></tr><tr><td>Wasm3</td><td>1309</td><td>1999</td><td>2931</td><td>3831</td></tr><tr><td>Stitch</td><td>1390</td><td><strong>2187</strong></td><td><strong>3056</strong></td><td><strong>4892</strong></td></tr></tbody></table><h3 id="execution-benchmarks-conclusion">Execution Benchmarks: Conclusion</h3><p>Wasmi is especially strong on AMD server chips and lacks behind on Apple silicon. An explanation for this could be the difference in the technique for instruction dispatch being used. <sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup></p><p>The Stitch interpreter performs really well. The reason likely is that Stitch encourages the LLVM optimizer to produce tail calls for its instruction dispatch, despite Rust not supporting them. Due to various downsides this design decision was discussed and dismissed during the development of Wasmi v0.32. <sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> <sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup> Given Stitch’s impressive execution performance especially on Apple silicon and Windows platforms those decisions should be reevaluated again.</p><p>Confusingly the great results for Wasmi on the test cases for the Intel i7 14700K are not reflected by its Coremark score. This probably is because every test case, including Coremark, is biased towards some kinds of workloads to some degree.</p><h2 id="benchmark-suite">Benchmark Suite</h2><p>The benchmarks and plots above have been gathered and generated using the <a href="https://github.com/wasmi-labs/wasmi-benchmarks"><code>wasmi-benchmarks</code></a> repository.
The reader is encouraged to run the benchmarks and plot the results on their own computer to confirm or disprove the claims. Usage instructions can be found in the <a href="https://github.com/wasmi-labs/wasmi-benchmarks?tab=readme-ov-file#plotting"><code>wasmi-benchmarks</code>’s <code>README.md</code></a>.</p><p>Contributions adding more Wasm runtimes, improving the plots or to add new test cases are welcome!</p><p>This article displayed the highlights of the new Wasmi version 0.32 and demonstrated the significant improvements in both startup and execution performance through various test cases.</p><p>With this new major update Wasmi now has a solid foundation for future development.
Many <a href="https://github.com/WebAssembly/proposals">WebAssembly proposals</a> such as the <code>multi-memory</code>, <code>simd</code> and <code>gc</code> proposals that have been put on hold for the development of Wasmi v0.32 are awaiting their implementation.</p><p>The promising results especially on the AMD server chips are a decent indicator that
Wasmi has great potential.
The performance of Wasmi on Apple silicon will be improved in future releases.</p><p>Plans are underway to implement the <a href="https://github.com/WebAssembly/wasm-c-api">Wasm C-API</a>, enabling various ecosystems that can interface with C to use Wasmi as a library.</p><p>Wasmi will continue to solidify its position as an efficient and versatile Wasm interpreter
with a fantastic startup performance and low memory consumption especially suited for embedded environments.</p><ul><li>First and foremost, I want to thank <a href="https://www.parity.io/">Parity Technologies</a> for financing and supporting the development of Wasmi for such a long time and for allowing Wasmi to become an independent project.</li></ul><ul><li>I want to commend the members of the <a href="https://bytecodealliance.org/">Bytecode Alliance</a> for their outstanding efforts in shaping the WebAssembly specification and ecosystem. Their contributions, among others, include runtimes such as <a href="https://github.com/bytecodealliance/wasmtime">Wasmtime</a> and <a href="https://github.com/bytecodealliance/wasm-micro-runtime">WAMR</a> as well as <a href="https://github.com/bytecodealliance/wasm-tools">advanced WebAssembly tooling</a>.</li></ul><ul><li>Additionally, I want to extend my gratitude to <a href="https://github.com/OLUWAMUYIWA">OLUWAMUYIWA</a>, who dedicated their time and effort to implement WASI preview1 support for Wasmi — which was absolutely amazing!</li></ul><ul><li>Furthermore, I want to thank <a href="https://github.com/yamt">yamt</a>, who inspired me with their Wasm runtime benchmarking platform. I highly recommend checking out their <a href="https://github.com/yamt/toywasm">toywasm</a> Wasm interpreter!</li></ul><ul><li>Finally, I would like to acknowledge <a href="https://github.com/Neopallium">Neopallium</a> for the thought-provoking discussions and experiments we shared about efficient interpreter dispatching techniques in Rust. I highly recommend checking out one of his Wasm experiments, <a href="https://github.com/Neopallium/s1vm">s1vm</a>.</li></ul></div></article></div></div>
  </body>
</html>
