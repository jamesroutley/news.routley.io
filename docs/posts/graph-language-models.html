<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aclanthology.org/2024.acl-long.245/">Original</a>
    <h1>Graph Language Models</h1>
    
    <div id="readability-page-1" class="page"><div id="main-container"><section id="main"><hr/><div><div><div><div><h5>Abstract</h5><p><span>While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.</span></p></div></div><dl><dt>Anthology ID:</dt><dd>2024.acl-long.245</dd><dt>Volume:</dt><dd><a href="https://aclanthology.org/volumes/2024.acl-long/">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2024</dd><dt>Address:</dt><dd>Bangkok, Thailand</dd><dt>Editors:</dt><dd><a href="https://aclanthology.org/people/l/lun-wei-ku/">Lun-Wei Ku</a>,
<a href="https://aclanthology.org/people/a/andre-f-t-martins/">Andre Martins</a>,
<a href="https://aclanthology.org/people/v/vivek-srikumar/">Vivek Srikumar</a></dd><dt>Venue:</dt><dd><a href="https://aclanthology.org/venues/acl/">ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>4477–4494</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href="https://aclanthology.org/2024.acl-long.245">https://aclanthology.org/2024.acl-long.245</a></dd><dt>DOI:</dt><dd></dd><dt>Bibkey:</dt><dd></dd><dt>Cite (ACL):</dt><dd><span id="citeACL">Moritz Plenz and Anette Frank. 2024. <a href="https://aclanthology.org/2024.acl-long.245">Graph Language Models</a>. In <i>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pages 4477–4494, Bangkok, Thailand. Association for Computational Linguistics.</span></dd><dt>Cite (Informal):</dt><dd><span id="citeRichText"><a href="https://aclanthology.org/2024.acl-long.245">Graph Language Models</a> (Plenz &amp; Frank, ACL 2024)</span></dd><dt>Copy Citation:</dt><dd>



</dd><dt>PDF:</dt><dd><a href="https://aclanthology.org/2024.acl-long.245.pdf">https://aclanthology.org/2024.acl-long.245.pdf</a></dd></dl></div></div><hr/><div id="citeModal" tabindex="-1" role="dialog" aria-labelledby="citeModalLabel" aria-hidden="true"><div role="document"><div><div><ul id="citeFormats" role="tablist"><li><a data-toggle="list" href="#citeBibtex" role="tab" aria-controls="citeBibtex" aria-selected="true">BibTeX</a></li><li><a data-toggle="list" href="#citeMods" role="tab" aria-controls="citeMods" aria-selected="false">MODS XML</a></li><li><a data-toggle="list" href="#citeEndnote" role="tab" aria-controls="citeEndnote" aria-selected="false">Endnote</a></li><li><a data-toggle="list" href="#citeMarkdown" role="tab" aria-controls="citeMarkdown" aria-selected="false">Preformatted</a></li></ul><div id="citeFormatsContent"><div id="citeBibtex" role="tabpanel"><pre id="citeBibtexContent">@inproceedings{plenz-frank-2024-graph,
    title = &#34;Graph Language Models&#34;,
    author = &#34;Plenz, Moritz  and
      Frank, Anette&#34;,
    editor = &#34;Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek&#34;,
    booktitle = &#34;Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&#34;,
    month = aug,
    year = &#34;2024&#34;,
    address = &#34;Bangkok, Thailand&#34;,
    publisher = &#34;Association for Computational Linguistics&#34;,
    url = &#34;https://aclanthology.org/2024.acl-long.245&#34;,
    pages = &#34;4477--4494&#34;,
    abstract = &#34;While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs {--} which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure {--} but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM{&#39;}s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.&#34;,
}
</pre></div><div id="citeMods" role="tabpanel"><pre id="citeModsContent">﻿&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;plenz-frank-2024-graph&#34;&gt;
    &lt;titleInfo&gt;
        &lt;title&gt;Graph Language Models&lt;/title&gt;
    &lt;/titleInfo&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Moritz&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Plenz&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;name type=&#34;personal&#34;&gt;
        &lt;namePart type=&#34;given&#34;&gt;Anette&lt;/namePart&gt;
        &lt;namePart type=&#34;family&#34;&gt;Frank&lt;/namePart&gt;
        &lt;role&gt;
            &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
        &lt;/role&gt;
    &lt;/name&gt;
    &lt;originInfo&gt;
        &lt;dateIssued&gt;2024-08&lt;/dateIssued&gt;
    &lt;/originInfo&gt;
    &lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
    &lt;relatedItem type=&#34;host&#34;&gt;
        &lt;titleInfo&gt;
            &lt;title&gt;Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/title&gt;
        &lt;/titleInfo&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Lun-Wei&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Ku&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Andre&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Martins&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;name type=&#34;personal&#34;&gt;
            &lt;namePart type=&#34;given&#34;&gt;Vivek&lt;/namePart&gt;
            &lt;namePart type=&#34;family&#34;&gt;Srikumar&lt;/namePart&gt;
            &lt;role&gt;
                &lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
            &lt;/role&gt;
        &lt;/name&gt;
        &lt;originInfo&gt;
            &lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt;
            &lt;place&gt;
                &lt;placeTerm type=&#34;text&#34;&gt;Bangkok, Thailand&lt;/placeTerm&gt;
            &lt;/place&gt;
        &lt;/originInfo&gt;
        &lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt;
    &lt;/relatedItem&gt;
    &lt;abstract&gt;While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.&lt;/abstract&gt;
    &lt;identifier type=&#34;citekey&#34;&gt;plenz-frank-2024-graph&lt;/identifier&gt;
    &lt;location&gt;
        &lt;url&gt;https://aclanthology.org/2024.acl-long.245&lt;/url&gt;
    &lt;/location&gt;
    &lt;part&gt;
        &lt;date&gt;2024-08&lt;/date&gt;
        &lt;extent unit=&#34;page&#34;&gt;
            &lt;start&gt;4477&lt;/start&gt;
            &lt;end&gt;4494&lt;/end&gt;
        &lt;/extent&gt;
    &lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
</pre></div><div id="citeEndnote" role="tabpanel"><pre id="citeEndnoteContent">﻿%0 Conference Proceedings
%T Graph Language Models
%A Plenz, Moritz
%A Frank, Anette
%Y Ku, Lun-Wei
%Y Martins, Andre
%Y Srikumar, Vivek
%S Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
%D 2024
%8 August
%I Association for Computational Linguistics
%C Bangkok, Thailand
%F plenz-frank-2024-graph
%X While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.
%U https://aclanthology.org/2024.acl-long.245
%P 4477-4494

</pre></div><div id="citeMarkdown" role="tabpanel"><h5>Markdown (Informal)</h5><p id="citeMarkdownContent">[Graph Language Models](https://aclanthology.org/2024.acl-long.245) (Plenz &amp; Frank, ACL 2024)</p><ul><li><a href="https://aclanthology.org/2024.acl-long.245">Graph Language Models</a> (Plenz &amp; Frank, ACL 2024)</li></ul><h5>ACL</h5><ul><li id="citeACLstyleContent">Moritz Plenz and Anette Frank. 2024. <a href="https://aclanthology.org/2024.acl-long.245">Graph Language Models</a>. In <i>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pages 4477–4494, Bangkok, Thailand. Association for Computational Linguistics.</li></ul></div></div></div></div></div></div></section></div></div>
  </body>
</html>
