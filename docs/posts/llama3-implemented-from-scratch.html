<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/naklecha/llama3-from-scratch">Original</a>
    <h1>Llama3 implemented from scratch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">in this file, i implemented llama3 from scratch, one tensor and matrix multiplication at a time.
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/archi.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/archi.png"/></a>
</p>

<p dir="auto">im not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation)
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/karpathyminbpe.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/karpathyminbpe.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path
import tiktoken
from tiktoken.load import load_tiktoken_bpe
import torch
import json
import matplotlib.pyplot as plt

tokenizer_path = &#34;Meta-Llama-3-8B/tokenizer.model&#34;
special_tokens = [
            &#34;&lt;|begin_of_text|&gt;&#34;,
            &#34;&lt;|end_of_text|&gt;&#34;,
            &#34;&lt;|reserved_special_token_0|&gt;&#34;,
            &#34;&lt;|reserved_special_token_1|&gt;&#34;,
            &#34;&lt;|reserved_special_token_2|&gt;&#34;,
            &#34;&lt;|reserved_special_token_3|&gt;&#34;,
            &#34;&lt;|start_header_id|&gt;&#34;,
            &#34;&lt;|end_header_id|&gt;&#34;,
            &#34;&lt;|reserved_special_token_4|&gt;&#34;,
            &#34;&lt;|eot_id|&gt;&#34;,  # end of turn
        ] + [f&#34;&lt;|reserved_special_token_{i}|&gt;&#34; for i in range(5, 256 - 5)]
mergeable_ranks = load_tiktoken_bpe(tokenizer_path)
tokenizer = tiktoken.Encoding(
    name=Path(tokenizer_path).name,
    pat_str=r&#34;(?i:&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&#34;,
    mergeable_ranks=mergeable_ranks,
    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},
)

tokenizer.decode(tokenizer.encode(&#34;hello world!&#34;))"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>
<span>import</span> <span>tiktoken</span>
<span>from</span> <span>tiktoken</span>.<span>load</span> <span>import</span> <span>load_tiktoken_bpe</span>
<span>import</span> <span>torch</span>
<span>import</span> <span>json</span>
<span>import</span> <span>matplotlib</span>.<span>pyplot</span> <span>as</span> <span>plt</span>

<span>tokenizer_path</span> <span>=</span> <span>&#34;Meta-Llama-3-8B/tokenizer.model&#34;</span>
<span>special_tokens</span> <span>=</span> [
            <span>&#34;&lt;|begin_of_text|&gt;&#34;</span>,
            <span>&#34;&lt;|end_of_text|&gt;&#34;</span>,
            <span>&#34;&lt;|reserved_special_token_0|&gt;&#34;</span>,
            <span>&#34;&lt;|reserved_special_token_1|&gt;&#34;</span>,
            <span>&#34;&lt;|reserved_special_token_2|&gt;&#34;</span>,
            <span>&#34;&lt;|reserved_special_token_3|&gt;&#34;</span>,
            <span>&#34;&lt;|start_header_id|&gt;&#34;</span>,
            <span>&#34;&lt;|end_header_id|&gt;&#34;</span>,
            <span>&#34;&lt;|reserved_special_token_4|&gt;&#34;</span>,
            <span>&#34;&lt;|eot_id|&gt;&#34;</span>,  <span># end of turn</span>
        ] <span>+</span> [<span>f&#34;&lt;|reserved_special_token_<span><span>{</span><span>i</span><span>}</span></span>|&gt;&#34;</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>5</span>, <span>256</span> <span>-</span> <span>5</span>)]
<span>mergeable_ranks</span> <span>=</span> <span>load_tiktoken_bpe</span>(<span>tokenizer_path</span>)
<span>tokenizer</span> <span>=</span> <span>tiktoken</span>.<span>Encoding</span>(
    <span>name</span><span>=</span><span>Path</span>(<span>tokenizer_path</span>).<span>name</span>,
    <span>pat_str</span><span>=</span><span>r&#34;(?i:&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&#34;</span>,
    <span>mergeable_ranks</span><span>=</span><span>mergeable_ranks</span>,
    <span>special_tokens</span><span>=</span>{<span>token</span>: <span>len</span>(<span>mergeable_ranks</span>) <span>+</span> <span>i</span> <span>for</span> <span>i</span>, <span>token</span> <span>in</span> <span>enumerate</span>(<span>special_tokens</span>)},
)

<span>tokenizer</span>.<span>decode</span>(<span>tokenizer</span>.<span>encode</span>(<span>&#34;hello world!&#34;</span>))</pre></div>


<p dir="auto">normally, reading this depends on how the model classes are written and the variable names inside them.
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/model.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/model.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = torch.load(&#34;Meta-Llama-3-8B/consolidated.00.pth&#34;)
print(json.dumps(list(model.keys())[:20], indent=4))"><pre><span>model</span> <span>=</span> <span>torch</span>.<span>load</span>(<span>&#34;Meta-Llama-3-8B/consolidated.00.pth&#34;</span>)
<span>print</span>(<span>json</span>.<span>dumps</span>(<span>list</span>(<span>model</span>.<span>keys</span>())[:<span>20</span>], <span>indent</span><span>=</span><span>4</span>))</pre></div>
<div data-snippet-clipboard-copy-content="[
    &#34;tok_embeddings.weight&#34;,
    &#34;layers.0.attention.wq.weight&#34;,
    &#34;layers.0.attention.wk.weight&#34;,
    &#34;layers.0.attention.wv.weight&#34;,
    &#34;layers.0.attention.wo.weight&#34;,
    &#34;layers.0.feed_forward.w1.weight&#34;,
    &#34;layers.0.feed_forward.w3.weight&#34;,
    &#34;layers.0.feed_forward.w2.weight&#34;,
    &#34;layers.0.attention_norm.weight&#34;,
    &#34;layers.0.ffn_norm.weight&#34;,
    &#34;layers.1.attention.wq.weight&#34;,
    &#34;layers.1.attention.wk.weight&#34;,
    &#34;layers.1.attention.wv.weight&#34;,
    &#34;layers.1.attention.wo.weight&#34;,
    &#34;layers.1.feed_forward.w1.weight&#34;,
    &#34;layers.1.feed_forward.w3.weight&#34;,
    &#34;layers.1.feed_forward.w2.weight&#34;,
    &#34;layers.1.attention_norm.weight&#34;,
    &#34;layers.1.ffn_norm.weight&#34;,
    &#34;layers.2.attention.wq.weight&#34;
]"><pre><code>[
    &#34;tok_embeddings.weight&#34;,
    &#34;layers.0.attention.wq.weight&#34;,
    &#34;layers.0.attention.wk.weight&#34;,
    &#34;layers.0.attention.wv.weight&#34;,
    &#34;layers.0.attention.wo.weight&#34;,
    &#34;layers.0.feed_forward.w1.weight&#34;,
    &#34;layers.0.feed_forward.w3.weight&#34;,
    &#34;layers.0.feed_forward.w2.weight&#34;,
    &#34;layers.0.attention_norm.weight&#34;,
    &#34;layers.0.ffn_norm.weight&#34;,
    &#34;layers.1.attention.wq.weight&#34;,
    &#34;layers.1.attention.wk.weight&#34;,
    &#34;layers.1.attention.wv.weight&#34;,
    &#34;layers.1.attention.wo.weight&#34;,
    &#34;layers.1.feed_forward.w1.weight&#34;,
    &#34;layers.1.feed_forward.w3.weight&#34;,
    &#34;layers.1.feed_forward.w2.weight&#34;,
    &#34;layers.1.attention_norm.weight&#34;,
    &#34;layers.1.ffn_norm.weight&#34;,
    &#34;layers.2.attention.wq.weight&#34;
]
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="with open(&#34;Meta-Llama-3-8B/params.json&#34;, &#34;r&#34;) as f:
    config = json.load(f)
config"><pre><span>with</span> <span>open</span>(<span>&#34;Meta-Llama-3-8B/params.json&#34;</span>, <span>&#34;r&#34;</span>) <span>as</span> <span>f</span>:
    <span>config</span> <span>=</span> <span>json</span>.<span>load</span>(<span>f</span>)
<span>config</span></pre></div>
<div data-snippet-clipboard-copy-content="{&#39;dim&#39;: 4096,
 &#39;n_layers&#39;: 32,
 &#39;n_heads&#39;: 32,
 &#39;n_kv_heads&#39;: 8,
 &#39;vocab_size&#39;: 128256,
 &#39;multiple_of&#39;: 1024,
 &#39;ffn_dim_multiplier&#39;: 1.3,
 &#39;norm_eps&#39;: 1e-05,
 &#39;rope_theta&#39;: 500000.0}"><pre><code>{&#39;dim&#39;: 4096,
 &#39;n_layers&#39;: 32,
 &#39;n_heads&#39;: 32,
 &#39;n_kv_heads&#39;: 8,
 &#39;vocab_size&#39;: 128256,
 &#39;multiple_of&#39;: 1024,
 &#39;ffn_dim_multiplier&#39;: 1.3,
 &#39;norm_eps&#39;: 1e-05,
 &#39;rope_theta&#39;: 500000.0}
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">we use this config to infer details about the model like</h2><a id="user-content-we-use-this-config-to-infer-details-about-the-model-like" aria-label="Permalink: we use this config to infer details about the model like" href="#we-use-this-config-to-infer-details-about-the-model-like"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>the model has 32 transformer layers</li>
<li>each multi-head attention block has 32 heads</li>
<li>the vocab size and so on</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="dim = config[&#34;dim&#34;]
n_layers = config[&#34;n_layers&#34;]
n_heads = config[&#34;n_heads&#34;]
n_kv_heads = config[&#34;n_kv_heads&#34;]
vocab_size = config[&#34;vocab_size&#34;]
multiple_of = config[&#34;multiple_of&#34;]
ffn_dim_multiplier = config[&#34;ffn_dim_multiplier&#34;]
norm_eps = config[&#34;norm_eps&#34;]
rope_theta = torch.tensor(config[&#34;rope_theta&#34;])"><pre><span>dim</span> <span>=</span> <span>config</span>[<span>&#34;dim&#34;</span>]
<span>n_layers</span> <span>=</span> <span>config</span>[<span>&#34;n_layers&#34;</span>]
<span>n_heads</span> <span>=</span> <span>config</span>[<span>&#34;n_heads&#34;</span>]
<span>n_kv_heads</span> <span>=</span> <span>config</span>[<span>&#34;n_kv_heads&#34;</span>]
<span>vocab_size</span> <span>=</span> <span>config</span>[<span>&#34;vocab_size&#34;</span>]
<span>multiple_of</span> <span>=</span> <span>config</span>[<span>&#34;multiple_of&#34;</span>]
<span>ffn_dim_multiplier</span> <span>=</span> <span>config</span>[<span>&#34;ffn_dim_multiplier&#34;</span>]
<span>norm_eps</span> <span>=</span> <span>config</span>[<span>&#34;norm_eps&#34;</span>]
<span>rope_theta</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>config</span>[<span>&#34;rope_theta&#34;</span>])</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">converting text to tokens</h2><a id="user-content-converting-text-to-tokens" aria-label="Permalink: converting text to tokens" href="#converting-text-to-tokens"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">here we use tiktoken (i think an openai library) as the tokenizer</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/tokens.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/tokens.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="prompt = &#34;the answer to the ultimate question of life, the universe, and everything is &#34;
tokens = [128000] + tokenizer.encode(prompt)
print(tokens)
tokens = torch.tensor(tokens)
prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]
print(prompt_split_as_tokens)"><pre><span>prompt</span> <span>=</span> <span>&#34;the answer to the ultimate question of life, the universe, and everything is &#34;</span>
<span>tokens</span> <span>=</span> [<span>128000</span>] <span>+</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>)
<span>print</span>(<span>tokens</span>)
<span>tokens</span> <span>=</span> <span>torch</span>.<span>tensor</span>(<span>tokens</span>)
<span>prompt_split_as_tokens</span> <span>=</span> [<span>tokenizer</span>.<span>decode</span>([<span>token</span>.<span>item</span>()]) <span>for</span> <span>token</span> <span>in</span> <span>tokens</span>]
<span>print</span>(<span>prompt_split_as_tokens</span>)</pre></div>
<div data-snippet-clipboard-copy-content="[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
[&#39;&lt;|begin_of_text|&gt;&#39;, &#39;the&#39;, &#39; answer&#39;, &#39; to&#39;, &#39; the&#39;, &#39; ultimate&#39;, &#39; question&#39;, &#39; of&#39;, &#39; life&#39;, &#39;,&#39;, &#39; the&#39;, &#39; universe&#39;, &#39;,&#39;, &#39; and&#39;, &#39; everything&#39;, &#39; is&#39;, &#39; &#39;]"><pre><code>[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
[&#39;&lt;|begin_of_text|&gt;&#39;, &#39;the&#39;, &#39; answer&#39;, &#39; to&#39;, &#39; the&#39;, &#39; ultimate&#39;, &#39; question&#39;, &#39; of&#39;, &#39; life&#39;, &#39;,&#39;, &#39; the&#39;, &#39; universe&#39;, &#39;,&#39;, &#39; and&#39;, &#39; everything&#39;, &#39; is&#39;, &#39; &#39;]
</code></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">converting tokens to their embedding</h2><a id="user-content-converting-tokens-to-their-embedding" aria-label="Permalink: converting tokens to their embedding" href="#converting-tokens-to-their-embedding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">IM SORRY but this is the only part of the codebase where i use an inbuilt neural network module
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/embeddings.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/embeddings.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="embedding_layer = torch.nn.Embedding(vocab_size, dim)
embedding_layer.weight.data.copy_(model[&#34;tok_embeddings.weight&#34;])
token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)
token_embeddings_unnormalized.shape"><pre><span>embedding_layer</span> <span>=</span> <span>torch</span>.<span>nn</span>.<span>Embedding</span>(<span>vocab_size</span>, <span>dim</span>)
<span>embedding_layer</span>.<span>weight</span>.<span>data</span>.<span>copy_</span>(<span>model</span>[<span>&#34;tok_embeddings.weight&#34;</span>])
<span>token_embeddings_unnormalized</span> <span>=</span> <span>embedding_layer</span>(<span>tokens</span>).<span>to</span>(<span>torch</span>.<span>bfloat16</span>)
<span>token_embeddings_unnormalized</span>.<span>shape</span></pre></div>

<div dir="auto"><h2 tabindex="-1" dir="auto">we then normalize the embedding using rms normalization</h2><a id="user-content-we-then-normalize-the-embedding-using-rms-normalization" aria-label="Permalink: we then normalize the embedding using rms normalization" href="#we-then-normalize-the-embedding-using-rms-normalization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">please, note after this step the shapes dont change, the values are just normalized
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/rms.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/rms.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="# def rms_norm(tensor, norm_weights):
#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5
#     return tensor * (norm_weights / rms)
def rms_norm(tensor, norm_weights):
    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"><pre><span># def rms_norm(tensor, norm_weights):</span>
<span>#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5</span>
<span>#     return tensor * (norm_weights / rms)</span>
<span>def</span> <span>rms_norm</span>(<span>tensor</span>, <span>norm_weights</span>):
    <span>return</span> (<span>tensor</span> <span>*</span> <span>torch</span>.<span>rsqrt</span>(<span>tensor</span>.<span>pow</span>(<span>2</span>).<span>mean</span>(<span>-</span><span>1</span>, <span>keepdim</span><span>=</span><span>True</span>) <span>+</span> <span>norm_eps</span>)) <span>*</span> <span>norm_weights</span></pre></div>


<p dir="auto">you will see me accessing layer.0 from the model dict (this is the first layer)
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/norm.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/norm.png" width="600"/></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="token_embeddings = rms_norm(token_embeddings_unnormalized, model[&#34;layers.0.attention_norm.weight&#34;])
token_embeddings.shape"><pre><span>token_embeddings</span> <span>=</span> <span>rms_norm</span>(<span>token_embeddings_unnormalized</span>, <span>model</span>[<span>&#34;layers.0.attention_norm.weight&#34;</span>])
<span>token_embeddings</span>.<span>shape</span></pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto">attention implemented from scratch</h3><a id="user-content-attention-implemented-from-scratch" aria-label="Permalink: attention implemented from scratch" href="#attention-implemented-from-scratch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">let&#39;s load the attention heads of the first layer of the transformer</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/naklecha/llama3-from-scratch/blob/main/images/qkv.png"><img src="https://github.com/naklecha/llama3-from-scratch/raw/main/images/qkv.png" width="600"/></a>
</p>
</article></div></div>
  </body>
</html>
