<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://voxel51.com/blog/how-computer-vision-is-changing-manufacturing-in-2023/">Original</a>
    <h1>How Computer Vision Is Changing Manufacturing in 2023</h1>
    
    <div id="readability-page-1" class="page"><div data-id="9cdd4a9" data-element_type="widget" data-widget_type="raven-post-content.default">
				<div>
			


<p>Welcome to the second installment of <a href="https://voxel51.com/">Voxel51</a>’s computer vision industry spotlight blog series. Each month, we highlight how different industries – from construction to climate tech, from retail to robotics, and more – are using computer vision, machine learning, and artificial intelligence to drive innovation. We’ll dive deep into the main computer vision tasks being put to use, current and future challenges, and companies at the forefront.</p>



<p>In this edition, we’ll focus on <em>manufacturing</em>! Read on to learn about computer vision in manufacturing and industrial automation.</p>



<h2>Industry overview</h2>



<p>Key facts and figures:</p>



<ul>
<li><a href="https://www.nist.gov/el/applied-economics-office/manufacturing/total-us-manufacturing#:~:text=In%202021%2C%20Manufacturing%20contributed%20%242.3,an%20estimated%2024%20%25%20of%20GDP.">14.7 million Americans are employed in manufacturing</a></li>



<li>The industry contributed <a href="https://www.nist.gov/el/applied-economics-office/manufacturing/total-us-manufacturing#:~:text=In%202021%2C%20Manufacturing%20contributed%20%242.3,an%20estimated%2024%20%25%20of%20GDP.">$2.3 trillion to US GDP in 2021, accounting for 24% </a>of the national total</li>



<li><a href="https://www.fortunebusinessinsights.com/industry-reports/industrial-automation-market-101589">Industrial automation is expected to reach $359 billion in the United States by 2029</a></li>



<li><a href="https://www.powermotiontech.com/news/article/21242721/global-manufacturing-production-to-reach-value-of-445-trillion-in-2022">Global manufacturing production is estimated to be north of $40 trillion</a></li>



<li>Manufacturing accounts for <a href="https://www.mckinsey.com/capabilities/operations/our-insights/the-future-of-manufacturing">16 percent of global GDP and 14 percent of employment</a></li>
</ul>



<p>Manufacturing is integral in the production of most of the physical goods that make up our modern world, from food and beverages to laptops, toys, and tools. The sector is also a critical component for both developing and developed economies, driving technological advancements and offering employment to millions. </p>



<p>The manufacturing industry is currently undergoing a massive transformation referred to as the <a href="https://en.wikipedia.org/wiki/Fourth_Industrial_Revolution">fourth industrial revolution</a> (4IR), headlined by the adoption of computer vision, artificial intelligence, robotics, and the <a href="https://www.ptc.com/en/technologies/iiot">industrial internet of things</a> (IIoT). 4IR technologies present an estimated <a href="https://www.mckinsey.com/capabilities/operations/our-insights/industrys-fast-mover-advantage-enterprise-value-from-digital-factories">multi-trillion dollar opportunity</a> and will enable factories to operate more accurately, efficiently, and safely. Computer vision is already playing a central role in this transformation, with companies using machine vision techniques to automate strenuous tasks, identify issues in both product and machinery, and improve safety conditions for workers.</p>



<p>Before we dive into several popular applications of computer vision-based AI technologies in manufacturing, here are some of the industry’s key challenges.</p>



<h2>Key industry challenges in manufacturing</h2>



<ul>
<li>Labor shortages: <a href="https://www2.deloitte.com/us/en/insights/industry/manufacturing/manufacturing-industry-diversity.html">According to projections by Deloitte</a>, there will be more than two million unfilled manufacturing jobs in the United States by 2030. Globally, the industry is expected to reach a deficit of up to 7.9 million jobs over the same period, <a href="https://www.kornferry.com/content/dam/kornferry/docs/pdfs/KF-Future-of-Work-Talent-Crunch-Report.pdf">according to a study by Korn-Ferry</a>.</li>



<li>Inflation: <a href="https://www.macny.org/manufacturers-feeling-the-impacts-of-inflation-and-supply-chain-challenges/">Rising costs for materials, energy, and transportation</a> put pressure on manufacturers to streamline their operations.</li>



<li>Scope and diversity: By its very nature, manufacturing touches almost every product we encounter, from screws, to shoes, to cars. Each product has its own manufacturing requirements and challenges, and each factory uses its own combination of cameras and sensors, so there are no one-size-fits all solutions.</li>
</ul>



<p>Continue reading for some ways computer vision is enabling industrial automation and improving safety in manufacturing.</p>



<h2>Applications of computer vision in manufacturing</h2>



<h3>Bin picking</h3>


<div>
<figure><img decoding="async" loading="lazy" width="950" height="534" src="https://voxel51.com/wp-content/uploads/2023/03/image2.png" alt="" srcset="https://voxel51.com/wp-content/uploads/2023/03/image2.png 950w, https://voxel51.com/wp-content/uploads/2023/03/image2-300x169.png 300w, https://voxel51.com/wp-content/uploads/2023/03/image2-768x432.png 768w" sizes="(max-width: 950px) 100vw, 950px"/><figcaption><em>Bin picking relies on the combination of robotics and computer vision. Image: </em><a href="https://www.atriainnovation.com/en/bin-picking-in-the-industry/"><em>ATRIA Innovation</em></a></figcaption></figure></div>


<p>A common industrial robotics application, <a href="https://en.wikipedia.org/wiki/Bin_picking"><em>bin picking</em></a><em> </em>is the action of selecting an object from a bin, picking it up, and placing it in another location. </p>



<p>For a robot to be successful with this task, it needs to precisely navigate and interact with objects of different shapes, sizes, and materials in a potentially cluttered, occluded, and poorly lit environment. Machine vision systems make this possible by mapping the environment and guiding the robotic arm’s motion.</p>



<p>In the simplest cases, camera images are passed into object detection routines. In many cases, however, grabbing an object from a bin requires mapping depth information and performing 3D object detection to situate the object in three dimensional space. Some computer vision systems use point clouds from LiDAR sensors to generate these 3D representations. One additional complication is that some objects can only be easily grabbed and held in certain orientations. To overcome this, some machine vision systems <a href="https://paperswithcode.com/task/pose-estimation">estimate the “pose” of objects</a> and use this information to orient the robotic arm for picking.</p>



<p>Here are a few papers on computer vision in bin picking:</p>



<ul>
<li><a href="https://www.mdpi.com/1424-8220/19/16/3602">Bin-Picking for Planar Objects Based on a Deep Learning Network: A Case Study of USB Packs</a></li>



<li><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=3436fffa61474e5fbfc5db547367d303c7cc02ee">Occlusion, Clutter, and Illumination Invariant Object Recognition</a></li>



<li><a href="https://www.merl.com/publications/docs/TR2012-007.pdf">Fast Object Localization and Pose Estimation in Heavy Clutter for Robotic Bin Picking</a></li>



<li><a href="https://ieeexplore.ieee.org/document/8967594">Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking</a></li>
</ul>



<h3>Palletizing and depalletizing</h3>


<div>
<figure><img decoding="async" loading="lazy" src="https://lh4.googleusercontent.com/uUdtRYULDp_9PGB57AM-7WGJlbkd1kdI_9RmwJv16-3gI55DZO8-zqg3nbI9J02_C6KWYsnhhT7Fugr1tb6uAGjMP1RGivKNAJEL8Xnv1qwdgShWnqoLreP5eLpU_lU2kRMZGSCygzLQqPN3iY0OqdM" alt="" width="-477" height="-636"/><figcaption><em>Pallets stacked in a warehouse. Image courtesy of </em><a href="https://unsplash.com/@arnosenoner"><em>Arno Senoner</em></a><em>.</em></figcaption></figure></div>


<p>Pallets have been called the unsung heroes of our modern age. These flat, typically wooden, platforms are essential to the scale and economy of global logistics and transportation. Before pallets make their way to shipping containers, they are loaded up with goods. These stacks of goods can reach <a href="https://igps.net/blog/2018/09/18/how-to-stack-empty-pallets-safely/">up to 15 feet tall</a> and <a href="https://www.gigacalculator.com/calculators/pallet-calculator.php">weigh more than two tons</a>. The process of loading and stacking products onto a pallet is known as <em>palletizing</em>. Similarly, once products have reached their destination, they must be unloaded from the pallet. This unloading process is referred to as <em>depalletizing</em>. </p>



<p>To reduce injury and error, manufacturers have been automating palletizing and depalletizing tasks through the use of robotic arms equipped with computer vision systems. Computer vision is a great fit for this problem, as the items being loaded and unloaded are typically nearly similar, so object detection models can be trained with very high accuracy. </p>



<p>Another crucial enabling element is calibration. As the robot arm loads or unloads items, it takes note of the disparity between where it estimated the object to be located and where it was actually located as feedback to refine its future predictions.</p>



<p>A few resources to get you started:</p>



<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-319-01378-7_5">3D-Computer Vision for Automation of Logistic Processes</a></li>



<li><a href="https://link.springer.com/article/10.1007/s12159-012-0095-8">Automated detection of euro pallet loads by interpreting PMD camera depth images</a></li>



<li><a href="https://www.mdpi.com/2076-3417/11/13/5959">Toward Future Automatic Warehouses: An Autonomous Depalletizing System Based on Mobile Manipulation and 3D Perception</a></li>



<li><a href="https://ieeexplore.ieee.org/abstract/document/5354054">Robotic de-palletizing using uncalibrated vision and 3D laser-assisted image analysis</a></li>



<li><a href="https://www.scitepress.org/papers/2016/56747/56747.pdf">Robust Pallet Detection for Automated Logistics Operations</a></li>
</ul>



<h3>Machine tending</h3>


<div>
<figure><img decoding="async" loading="lazy" src="https://lh6.googleusercontent.com/taJBFShFqG-LYK75pl-LCYY7qW2sad3pRNmybtkQ0rhsGttJbjnnp-qWrYb_sGVhPyUqwUJa7a5DcF46F850yZG6-yrN6qi1suXO5nwmggjojvtLcoaEm3VqS2WG54r7le_4j_jb1qyDLPKNcgeojRw" alt="" width="-407" height="-270"/><figcaption><em>A robotic arm loading boxes onto a conveyor belt. Image courtesy of Mech-Mind Robotics.</em></figcaption></figure></div>


<p><em>Machine tending</em> is the process of automatically loading raw material or components for input into a machine. This includes placing parts on a conveyor belt, as well as preparing materials for welding, grinding, milling, or injection molds. </p>



<p>Automated machine tending has multiple advantages, from reducing injury risk to improving consistency. Computer vision-enabled automation in machine tending also allows for higher precision in these applications. With real-time monitoring, <a href="https://paperswithcode.com/task/object-localization">object localization</a> can be used to precisely situate the input materials relative to the machine being tended, and a robotic arm can adjust positioning accordingly. </p>



<p>As with many computer vision applications in manufacturing, data availability and quality in machine tending is a significant challenge. Machine vision systems for machine tending often need to be built with limited labeled training data. This means that data cleaning and curation are essential, and techniques like data augmentation and transfer learning can be incredibly important.</p>



<p>Here are a few preliminary resources:</p>



<ul>
<li><a href="https://www.mdpi.com/2076-3417/12/11/5697">An Intelligent Manufacturing Approach Based on a Novel Deep Learning Method for Automatic Machine and Working Status Recognition</a></li>



<li><a href="https://pdf.sciencedirectassets.com/282173/1-s2.0-S2212827121X0011X/1-s2.0-S2212827121011574/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEMaCXVzLWVhc3QtMSJHMEUCIGXhlAGFj%2FyW9h%2Ft4mMovZwJvicNyjOMDIjx1ZALpK9UAiEA%2BsnTuSCVGtgt1J6WHgns4HOEfPW7rE0y%2BynOsA%2FmFjQq1QQI3P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDObBXkVbYODaiabeqSqpBOdK9Z%2BqgDYHD%2FfN2Qdxtwonr2mJQqGTu%2FASwcvYyKnUodRM%2FhpH1PmLOkY4PDLy6SVXWiZdCavelQvDR%2FOQu1cLjzGdo7x1NGaJzjFUfrTTDf2%2FVezda%2Fio9BS%2F2Nm9OdqTu0HhJmjBDbTE0vXguJO7y6iiuMeAbBfGfNRLljSKKgQNISTB%2FtVvHMsRkcfcmY4izzTCCkPtpI302IWEU3SP87CBy5hEBcT%2FoYABhCkA0mSDcS0JvrZWu3jHgDWTaRg9uopQRrO%2BLlZMFbggIjT87f%2BDgBRryfTMBeh0HY5VcCd4YoTCthibtvLZ7FHd4dOUaOfSqdpRxaa1ljI1OdY3t0hAgEweWk%2B5%2BWwRBkvYqt62dy33yTMS6aJOt8n8CjUQDEli%2FsykLg8gRkjIhcliispYAh6Z6ctuwz6HCBxUPBxRr%2F%2FrTMdq5b99JuS2KlanaZxiTMuW7x2qDK38yEnByKwSBy7VDBD91GCi%2FVX5Gu%2Fzbt5IUt2VU9fac4I5PBQvLFmg%2FepVFow7xZQ3di%2BY6X74LAU%2FxqWcJWK5kWNwyYHaMSnvBAL0Zkxd1BEwzxCxEV7L56lIPS0o0p%2BB3THd9f3OohTBeiTr8n59vsOS8sgQCqq3dZP%2BraZdQ3ikzSQEEXDWmzRW2yBD4i7LulIwZo3%2F5Gbw1JkltEXaFdYMfsLNwIxcmkexYUYYc%2BmJFR0i8lPVJjo4odrrGqSwQAO2nha4Uzk7sX8wxr7ZnwY6qQG2r%2BObK%2FXl6kBscM3s3DTClW%2BygFSpAJ8KuI1M8SQNQTGTjJ1aq64Q52W8zjWpFGB7E1qkhflSPbSGKQhfybzrIOzikCRReHgAdFwwav4ny9cZ0Ifr%2FDtf6xq9qU4JQ9jCGox9utCUT5VbWM4Wjy3%2BF9LinfS6nstL%2BgQhQhO6EhTj%2BftG3lXrewOnFJ60nBwUS%2FwArl5AnIFGGqlco481vZyv2CSJ2cQf&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20230222T191740Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTYTCU2DNOK%2F20230222%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=ac9f1097e9e14d4fa12ac25e547d1fb8af096f45156b06bd6a21774715c860e4&amp;hash=29f90f7f80bc78d0793432626e73f52cdf5eaab757fa9d69d370c12303b41dc0&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S2212827121011574&amp;tid=spdf-0dce9604-1711-4baa-af20-c380786f2573&amp;sid=a24d03dd5bf8b3414d6ae3646965868a94degxrqa&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=0f1650580a065250530402&amp;rr=79da102cfb09ce98&amp;cc=us">Vision-Based Associative Robotic Recognition of Working Status in Autonomous Manufacturing Environment</a></li>
</ul>



<h3>Defect detection</h3>





<p>Computer vision has already become indispensable in ensuring quality control in industrial processes. Instance segmentation, for instance, is used in conjunction with high-resolution sensor data, to check if a manufactured part has the desired spatial dimensions, within an allowed tolerance. </p>



<p>One area of quality control where computer vision features prominently is defect detection. Manufacturers want to identify defective parts and products as early in their journey as possible. In some applications, where the possible varieties of defects are known, object detection and classification are used to identify problems.</p>



<p>In other cases, the full variety of possible defects is either unknown, or too complicated to easily categorize. In manufacturing, defects can range from minutiae like small scratches to entirely missing components, like a missing screw. This can also be exacerbated by class imbalance, where there are many more examples of “normal” products than “defective” products.</p>



<p><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a> provides an alternative, <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised approach</a> that takes normal and defective examples as input and predicts a new product as normal, or “nominal”, if it is likely to have come from the same <em>distribution</em> as the previously seen normal samples. To make this determination, anomaly detection models learn an approximate representation of this nominal distribution, which may involve using density-based models like <a href="https://en.wikipedia.org/wiki/DBSCAN">DBSCAN</a>, <a href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">support vector machines</a>, or deep learning models like <a href="https://en.wikipedia.org/wiki/Autoencoder#:~:text=An%20autoencoder%20is%20a%20type,data%20from%20the%20encoded%20representation.">autoencoders</a> and generative adversarial networks (<a href="https://blog.paperspace.com/complete-guide-to-gans/">GANs</a>). Libraries like Intel’s <a href="https://github.com/openvinotoolkit/anomalib">Anomalib</a> provide tools for implementing and benchmarking anomaly detection algorithms.</p>



<p>Here are a few papers on defect detection and anomaly detection in manufacturing:</p>



<ul>
<li><a href="https://www.mdpi.com/1996-1944/13/24/5755">Using Deep Learning to Detect Defects in Manufacturing: A Comprehensive Survey and Current Challenges</a></li>



<li><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0203192">Automatic Detection and Classification of Manufacturing Defects in Metal Boxes using Deep Neural Networks</a></li>



<li><a href="https://reader.elsevier.com/reader/sd/pii/S2212827119302409?token=0AE42B55604D8922A34AE64006FF4185B10B8F72E4441EBE7B7E96F25163A51070ABEB86ABD1271204ADFE2B2DC391CB&amp;originRegion=us-east-1&amp;originCreation=20230214052745">Anomaly detection with convolutional neural networks for industrial surface inspection</a></li>



<li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.pdf">Towards Total Recall in Industrial Anomaly Detection</a></li>



<li><a href="https://ieeexplore.ieee.org/document/8954181">The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</a></li>
</ul>



<h3>Predictive maintenance</h3>





<p>Tools and machinery in manufacturing plants gradually accumulate wear and tear which, if left untreated, could lead to complete breakdown, as well as potential injuries and lost productivity. To avoid such failure, manufacturers have historically performed <em>routine maintenance</em>, cleaning, refurbishing, and changing out old parts on a regular basis. </p>



<p>But routine maintenance has two predominant downsides: first, it introduces downtime and costly upkeep when maintenance may not be necessary; second, it is not able to pick up on issues that arise and escalate between maintenance intervals.</p>



<p>Preventive maintenance (PdM) seeks to address these issues with more active monitoring. Computer vision and predictive analytics help manufacturers to save on maintenance costs and avert catastrophe. PdM is already applied to a wide variety of machines and mechanical parts, from blades and bearings to gears and gaskets.</p>



<p>In some cases, as with the saw blade pictured above, computer vision techniques like segmentation, object detection, and classification suffice to identify and predict all likely failure modes. Common signs include cracks, corrosion, and leaks. As with defect detection, when the spectrum of failure modes is complex or unknown, anomaly detection is applied to the state of the machines. </p>



<ul>
<li><a href="https://link.springer.com/article/10.1007/s00170-020-05449-w">Review of tool condition monitoring in machining and opportunities for deep learning</a></li>



<li><a href="https://www.sciencedirect.com/science/article/pii/S2212827121010842">A computer vision system for saw blade condition monitoring</a></li>



<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0141635917302817">A machine vision system for micro-milling tool condition monitoring</a></li>



<li><a href="https://arxiv.org/abs/2010.03207">Deep learning models for predictive maintenance: a survey, comparison, challenges and prospect</a></li>
</ul>



<h2>Companies at the cutting edge of computer vision in manufacturing</h2>



<h3>Mech-Mind Robotics</h3>


<div>
<figure><img decoding="async" loading="lazy" src="https://lh3.googleusercontent.com/-8CK-3Ae3TVgb5xBwEaowHz63SZHC1QNqF5pnC1C-Ix_DQolWL5P-lOy_x0Is1FHIkTlcn7qW1WIjxIlcod27bfpOUW1_UbIg66XQoqE14WRQsDlyt-PNJ9tlrTwa9imLV-iwBwuM_YadcF0vZsviXI" alt="" width="920" height="259"/><figcaption><em>Left: randomly piled track links in a bin. Right: 3D reconstruction of the scene with point clouds generated by Mech-Mind’s Mech-Eye industrial 3D cameras. Image courtesy of Mech-Mind Robotics. </em></figcaption></figure></div>


<p>With more than 700 employees, 1000+ customers, and more than $200M in funding, Mech-Mind Robotics is the largest 3D vision company in China, and one of the world’s largest providers of 3D vision cameras and machine vision software for robotic automation.</p>



<p>Mech-Mind’s integrated hardware and software solutions are used in a wide range of manufacturing applications, including bin picking, machine tending, palletizing and depalletizing, assembly, and gluing. The <a href="https://www.mech-mind.com/product/mech-eye-industrial-3d-camera.html">Mech-Eye</a> industrial 3D camera uses <a href="https://en.wikipedia.org/wiki/Structured-light_3D_scanner">structured light technology</a> to generate high resolution, high accuracy point clouds.</p>



<p>Mech-Mind’s <a href="https://www.mech-mind.com/product/mech-vision-machine-vision-software.html">Mech-Vision machine vision software</a> provides a platform for customers to build industrial computer vision applications with Mech-Eye cameras and customers’ robots. Mech-Vision has built-in support for common computer vision tasks like pose adjustment, and 2D and 3D matching, wherein the customer generates a point cloud model of the object to be recognized, either from a CAD file or directly from a camera image, and this model is recognized in the scene. Built-in support for common industrial robots means that the <a href="https://en.wikipedia.org/wiki/Robot_calibration">robot calibration</a> process, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0736584506000743">while traditionally time consuming</a> (on the scale of hours), can be completed in less than 20 minutes.</p>



<p>To round things out, <a href="https://www.mech-mind.com/product/mech-dlk-deep-learning-software.html">Mech-Mind’s deep learning software</a> allows customers to fine-tune computer vision models for their specific use cases. Customers load in their own data, which are automatically pre-labeled, and can then be rapidly edited and revised. Typically, Mech-Mind’s deep learning software only needs 20-50 images of an object to train a model to recognize it in a scene. </p>



<h3>Instrumental</h3>


<div>
<figure><img decoding="async" loading="lazy" src="https://lh4.googleusercontent.com/fE5tPN5hDRyJ294CmhW0e0DSREQ2kxbiasoF1cCmKYtDJ4sIblBWnHQ16LVWKDFh4j05OHOrKPNnZu9NcWky5r1uL_mO_ZXCbC_1QjRKver2jaBPbDbhNmleFFIjo1dAIy-2TactHAEkOI6Tk58EQds" alt="" width="868" height="612"/><figcaption><em>Instrumental dashboard displaying normal and anomalous products. Image courtesy of Instrumental.</em></figcaption></figure></div>


<p>Founded by two Stanford and MIT grads and former Apple employees in 2015 and based in Palo Alto, CA, <a href="https://instrumental.com/">Instrumental</a> is leading the way in ensuring product quality in electronics manufacturing. They use computer vision in conjunction with predictive analytics to provide real-time monitoring and alerts, as well as root cause analysis for prior failures.</p>



<p>Instrumental’s AI-based computer vision suite supports both new product introduction (NPI) manufacturing, which is characterized by low volume, and mass production (MP) manufacturing, which is high volume.</p>



<p>Even within electronics manufacturing, the wide variety of products and substantial variation from product to product means that general purpose computer vision models have seen very little success. Nevertheless, manufacturers want to detect defects and issues on their particular use case as quickly as possible.</p>



<p>Instrumental’s suite of computer vision tools is designed to achieve high performance application-specific defect detection given as few samples as possible. To do this, they use techniques like data augmentation, transfer learning, and active learning to build a robust dataset that they use to train an anomaly detection model. Their models are built easily with no coding. Once deployed, these models run real-time inference on the edge in the factory and create a record that can be shared, inspected, and evaluated.</p>



<h3>Protex AI</h3>



<p>Founded in 2020 and backed by YCombinator, Notion Capital, and Playfair Capital, Irish startup <a href="https://www.protex.ai/">Protex AI</a> is helping enterprise safety teams to revolutionize how they make proactive safety decisions that contribute to a safer work environment.</p>



<p>Their AI-powered technology is enabling businesses to gain greater visibility of unsafe behaviors in their facilities. The privacy-preserving platform plugs into existing CCTV infrastructure and uses its computer vision technologies to capture unsafe events autonomously in settings such as warehouses, manufacturing facilities, and ports.</p>



<p>Protex AI provides a simple interface so that each user can create their own “rules”, including setting exclusion zones, speed limits for forklifts, or even minimum distances workers must maintain between themselves and machines. Protex then uses computer vision techniques, including object detection, object tracking, and pose estimation, in order to check these rules. For rules involving speeds or distances, the vision system employs calibration. Typically, calibration is performed using inputs from multiple cameras, but Protex uses special routines to estimate calibration from a sole CCTV camera.</p>



<p>Due to privacy concerns surrounding customer image and video data, Protex AI runs all of their models on the edge on Nvidia powered devices. For power and compute efficiency, they <a href="https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7">quantize</a> their model weights.</p>



<p>As use cases can differ greatly, Protex AI deploys custom models for each customer. Their base model is trained on hundreds of thousands of images, and then a unique version is fine-tuned on a given customer’s data. In their line of work, data quantity is not an issue. The most important factor in model performance is having a clean, high quality dataset.  </p>



<h3>Cognex</h3>





<p>More than forty years old but still on the cutting edge, Nasdaq-listed (CGNX) <a href="https://www.cognex.com/">Cognex</a> is a world leader in machine vision for industrial automation. Their 2000+ employee team has a hand in almost every step of industrial automation processes, from sensors and barcode scanners to industrial cameras and fully integrated vision systems.</p>



<p>Cognex has machine vision tools for rule-based applications, such as monitoring object location and detecting edges, as well as deep learning tools for cloud connected and edge devices. Their <a href="https://www.cognex.com/products/machine-vision/vision-software/visionpro-deep-learning">VisionPro Deep Learning</a> software supports standard tasks like defect detection and segmentation, and assembly verification, as well as burgeoning tasks like <a href="https://link.springer.com/article/10.1007/s10845-019-01508-6">material classification</a>.</p>



<p>Beyond specific tasks, Cognex’s VisionPro software expedites time to deployment with <a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML</a> capabilities. The <em>label checker</em> automatically verifies the vast majority of labels and flags the remaining images for manual review, minimizing the number of samples a user needs to assess. </p>



<p>During training, <em>parameter autotune </em>will use input example images to determine the optimal set of hyperparameters. In <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">optical character recognition</a> (OCR), for instance, it can be difficult to recognize text due to the wide spectrum of fonts and potential distortions. Traditional OCR systems require that users specify segmentation hyperparameters to achieve high precision and recall. Cognex <a href="https://www.cognex.com/products/machine-vision/vision-tools/ai-tools/deep-learning-tools/blue-read">Blue Read</a> eliminates this requirement by comparing an input image to the library of hundreds of fonts on which it was trained, and automatically selecting the best hyperparameters.</p>



<h3>RIOS Intelligent Machines</h3>





<p><a href="https://www.rios.ai/">RIOS Intelligent Machines</a> is on a mission to transform labor-intensive factories into smart factories of the future powered by robotics and AI. The company helps its global customers automate their factories, warehouses, and supply chain operations by deploying AI-powered end-to-end robotic workcells that integrate within existing workflows. The Menlo Park, CA-based company was founded by former Xerox PARC engineers who saw a massive failure of traditional robots and predicted that factories over reliance on labor would soon reach a breaking point. </p>



<p>RIOS has developed some of the most advanced hardware and AI/software platforms in robotics, including human-like tactile sensors for robots, haptics intelligence platform, and highest performance end-of-arm tooling and food-grade grippers. Their Factory Automation-as-a-Service delivers fixed, programmable, flexible and integrated automation. They also offer palletizing robots to load and unload products on or off of pallets, plus robotic packaging systems. </p>



<h3>A few more </h3>



<p>It’s impossible to highlight every company doing amazing work at the intersection of computer vision and manufacturing and industrial automation. Here are a few more companies that are pushing the boundaries:</p>



<ul>
<li><a href="https://www.preml.io/">PreML GmBH</a>: German startup founded in 2020 focused on automated visual quality inspection.</li>



<li><a href="https://www.prophesee.ai/about-prophesee/">Prophesee</a>: French series C startup pioneering <a href="https://www.prophesee.ai/2019/07/28/event-based-vision-2/">event-based vision</a>.</li>



<li><a href="https://www.datalogic.com/eng/index.html">Datalogic</a>: Italy-based leader in automated data capture, barcode readers, sensors, and vision systems.</li>



<li><a href="https://www.stemmer-imaging.com/">Stemmer Imaging</a>: Based in Germany, S9I is Europe’s largest imaging technology provider, with a hand in everything from photography to factory floor vision systems.</li>



<li><a href="https://www.pickit3d.com/en/">Pickit 3D</a>: 2016 spinout of NASA robotics software provider <a href="https://www.intermodalics.eu/">Intermodalics</a>, focused on 3D vision systems for robotic guidance. </li>



<li><a href="https://www.matroid.com/">Matroid</a>: End-to-end no-code computer vision solutions for quality assurance, assembly verification, and safety and compliance founded by Stanford adjunct professor <a href="https://www.linkedin.com/in/rezab/">Reza Zadeh</a>.</li>
</ul>



<h2>Datasets</h2>



<p>Due to the highly proprietary nature of manufacturing and industrial automation processes, public computer vision datasets are few and far between. Hopefully these datasets will help you get started: </p>



<ul>
<li><a href="https://www.mvtec.com/company/research/datasets">MVTech Datasets for anomaly detection and industrial object detection</a></li>



<li><a href="https://github.com/Rishit-dagli/CPPE-Dataset/">CPPE-5: Medical Personal Protective Equipment Dataset</a></li>



<li><a href="https://universe.roboflow.com/phantom/forklift-1">Roboflow Forklift and Pallet Detection Dataset</a></li>



<li><a href="https://github.com/maximiliangilles/MetaGraspNet">MetaGraspNet: A Large-Scale Benchmark Dataset for Scene-Aware Ambidextrous Bin Picking via Physics-based Metaverse Synthesis</a></li>



<li><a href="https://www.vicos.si/resources/kolektorsdd/">Kolektor Surface Defect Dataset (KSSD)</a></li>



<li><a href="https://paperswithcode.com/dataset/btad">beanTech Anomaly Detection Dataset (BTAD)</a></li>
</ul>



<p>If you would like to see any of these, or other computer vision manufacturing datasets added to the <a href="https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/index.html">FiftyOne Dataset Zoo</a>, get in touch and we can work together to make this happen!</p>



<h2>Join the FiftyOne community!</h2>



<p>Developers of manufacturing and industrial automation applications can benefit from FiftyOne’s ability to easily filter through the huge amounts of visual data collected daily from farms and other sources. Using FiftyOne, this data can be curated into datasets for model training, or to share with experts for annotation or analysis of CV models. Join the thousands of engineers and data scientists already using FiftyOne to solve some of the most challenging problems in computer vision today!</p>



<ul>
<li>1,400+ <a href="https://join.slack.com/t/fiftyone-users/shared_invite/zt-s6936w7b-2R5eVPJoUw008wP7miJmPQ">FiftyOne Slack</a> members</li>



<li>2,600+ stars on <a href="https://github.com/voxel51/fiftyone">GitHub</a></li>



<li>3,300+ <a href="https://www.meetup.com/pro/computer-vision-meetups/">Meetup members</a></li>



<li><a href="https://github.com/voxel51/fiftyone/network/dependents?package_id=UGFja2FnZS0xNzAxODM0MjUx">Used by</a> 254+ repositories</li>



<li>56+ <a href="https://github.com/voxel51/fiftyone/graphs/contributors">contributors</a></li>
</ul>



<h2>What’s next?</h2>



<ul>
<li>See how computer vision is impacting agriculture in our <a href="https://voxel51.com/blog/how-computer-vision-is-changing-agriculture-in-2023/">Agriculture Industry Spotlight</a>.</li>



<li>If you like what you see on GitHub, <a href="https://github.com/voxel51/fiftyone">give the project a star</a>.</li>



<li><a href="https://voxel51.com/docs/fiftyone/index.html">Get started!</a> We’ve made it easy to get up and running in a few minutes.</li>



<li>Join the FiftyOne <a href="https://join.slack.com/t/fiftyone-users/shared_invite/zt-s6936w7b-2R5eVPJoUw008wP7miJmPQ">Slack community</a>, we’re always happy to help.</li>
</ul>
		</div>
				</div></div>
  </body>
</html>
