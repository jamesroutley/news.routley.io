<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pubby.games/codegen.html">Original</a>
    <h1>Explaining my fast 6502 code generator (2023)</h1>
    
    <div id="readability-page-1" class="page">
    
    
        <p>
        To learn how optimizing compilers are made, <a href="https://pubby.games/nesfab.html">I built one</a> targeting the <a href="https://en.wikipedia.org/wiki/MOS_Technology_6502">6502 architecture</a>.
        In a bizarre twist, my compiler generates faster code than GCC, LLVM, and every other compiler I compared it to.
        </p>

        <img src="https://pubby.games/nesfab/combined.png"/>

        <p>I reckon my compiler isn&#39;t doing more when it comes to high-level optimizations,
        so the gains must be from the <a href="https://en.wikipedia.org/wiki/Code_generation_(compiler)">code generation</a> side.
        This makes sense, as most compilers are multi-target, with backends
        designed for modern RISC-like systems, not the ancient 6502.
        It doesn&#39;t matter how good GCC or LLVM&#39;s high-level optimizations are
        if they falter at the last leg of the race.
        </p>

        <p>Still, my compiler also beats those designed for retro and embedded systems, like VBCC, SDCC, and KickC.
        For this reason, it seemed like a good idea to write about my technique.</p>



        <h2>Cheating Disclaimer</h2>

        <p>Before I get into it, I want to cover three areas my compiler has an advantage, which aren&#39;t tied to algorithmic design.
        I call these my &#34;cheats&#34;, as other compilers <i>could</i> do these things, but there are trade-offs involved.</p>

        <p>
        First, my compiler generates what are know as
        <a href="https://en.wikipedia.org/wiki/Illegal_opcode">&#34;illegal&#34; instructions</a>.
        An illegal instruction is one which isn&#39;t officially documented by the manufacturer,
        but still exists in the hardware. 
        On most 6502 chips, a few illegal instructions exist which combine the behavior of two &#34;legal&#34; instructions.
        Their use can save a few cycles, but not every backend generates them.
        </p>

        <p>Second, some of my compiler&#39;s gains can be explained by its computational effort.
        My compiler spends the majority of its CPU budget (a few milliseconds) doing instruction selection,
        but not all compilers do. 
        Typically, the more time spent searching for a solution, the better the results.
        </p>

        <p>
        Lastly, there is loop unrolling and other optimizations that trade space efficiency for speed,
        which is hard to compare fairly when benchmarking different compilers.
        I tried to pick tests which weren&#39;t affected by this, 
        but obviously I can&#39;t be perfect. 
        </p>

        <p>With my conscience clear, onto the algorithm!</p>

        <h2>A Quick, Vague Overview</h2>

        <p>
        I&#39;ve been calling my own attempt &#34;<a href="https://en.wikipedia.org/wiki/Outsider_art">outsider art</a>&#34;,
        as I didn&#39;t know much about code generation when writing it.
        In fact, the technique didn&#39;t even originate in my compiler;
        I based it on an old animation system I wrote years prior.
        My algorithm is interesting because it combines instruction selection
        with register allocation, but also because it&#39;s written using <a href="https://en.wikipedia.org/wiki/Continuation">continuations</a>.
        In all my years of programming, it&#39;s the only practical use I&#39;ve found for such black magic.
        </p>

        <p>
        To briefly explain how it works, I&#39;ll say that each basic block in my 
        <a href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a> is represented as a
        <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> in 
        <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form</a>.
        The first step of code generation is to break both of these properties,
        converting basic blocks to <a href="https://en.wikipedia.org/wiki/Total_order">totally-ordered</a> lists that do not contain phi nodes.
        </p>

        <p>
        The ordered basic blocks are then processed from beginning to end,
        generating multiple assembly code combinations per operation.
        The list of combinations will grow at an exponential rate, 
        but most can be pruned using ideas from <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a> 
        and <a href="https://en.wikipedia.org/wiki/Branch_and_bound">branch-and-bound</a>.
        </p>

        <p>
        To improve the generated code, output states of each basic block are fed into their successor blocks as inputs.
        This process repeats until a fixed point is reached.
        A cooling schedule a-la <a href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a> 
        is applied to find results faster.
        </p>

        <p>
        After this, each basic block will have multiple code sequences to choose from.
        The most expensive ones can be heuristically pruned, 
        then a good selection can be made by solving a <a href="https://beza1e1.tuxen.de/articles/pbqp.html">partitioned boolean quadratic problem</a> (PBQP).
        </p>

        <p>
        Finally, additional instructions are inserted as transitions between the basic blocks,
        and a few optimization passes are run on the resulting assembly code for good measure.
        </p>

        <hr/>

        <h2>A More Detailed Explanation</h2>

        <h3>Basic Block IR</h3>

        <p>As stated, the 
        <a href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a> 
        of each basic block is a 
        <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> in 
        in <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form</a>,
        which sounds complicated until you see a picture of it.
        </p>
        <p>Given the code below:</p>

        <pre>foo = fn() ^ 5
return foo - (foo &amp; 3)</pre>
        <p>The IR DAG would look like this:</p>
        <img src="https://pubby.games/codegen/dag.png"/>
        <p>Each node in the graph represents a value, with edges denoting the flow of data.
        To compute a value, the nodes pointing to it must be computed first â€”
        e.g. to compute the node labeled <code>(&amp;)</code>, the nodes <code>(^)</code> and <code>(3)</code> must be computed first.
        It&#39;s a <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graph</a>.
        </p>


        <h3>Ordering</h3>

        <p>The first step of code generation is to take the IR and create a 
        <a href="https://en.wikipedia.org/wiki/Total_order">total order</a>
        out of its nodes, positioning each node to occur after its dependencies.
        This is akin to putting each node on a number line such that every edge points in the same direction (downwards in the diagram).</p>

        <img src="https://pubby.games/codegen/scheduled.png"/>

        <p>The generated assembly code will follow this order.</p>

        <h4>How does one find this order?</h4>

        <p>It&#39;s easy to do using a <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sort</a>,
        but there&#39;s a problem: DAGs can have multiple valid orderings, and some will generate better code than others.</p>

        <p>To find a good ordering (one which generates efficient code), 
        the topological sorting algorithm is still used, but it can be modified using a few heuristics.
        First, &#34;fake&#34; edges can be added to the graph to constrain nodes to occur after other nodes.
        Second, a greedy algorithm can be used to influence the traversal order of the topological sort.
        In my compiler, I prioritize nodes which form the 
        <a href="https://en.wikipedia.org/wiki/Longest_path_problem">longest path</a> through the graph,
        which produces an ordering that has small
        <a href="https://en.wikipedia.org/wiki/Live-variable_analysis">live ranges</a>,
        resulting in less register pressure and stores.
        </p>

        <img src="https://pubby.games/codegen/dag2.png"/>
        </div>
  </body>
</html>
