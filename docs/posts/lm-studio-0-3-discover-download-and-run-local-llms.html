<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lmstudio.ai/blog/lmstudio-v0.3.0">Original</a>
    <h1>LM Studio 0.3 â€“ Discover, download, and run local LLMs</h1>
    
    <div id="readability-page-1" class="page"><div id="__blog-post-container"><p>We&#39;re incredibly excited to finally share LM Studio 0.3.0 ðŸ¥³.</p>
<p><img src="https://lmstudio.ai/blog/assets/images/0.3.0-screen-e68e056465f8f2a74bc334956e597202.png"/></p><p>Since its inception, LM Studio packaged together a few elements for making the most out of local LLMs when you run them on your computer:</p>
<ol>
<li>A desktop application that runs entirely offline and has no telemetry</li>
<li>A familiar chat interface</li>
<li>Search &amp; download functionality (via Hugging Face ðŸ¤—)</li>
<li>A local server that can listen on OpenAI-like endpoints</li>
<li>Systems for managing local models and configurations</li>
</ol>
<p>With this update, we&#39;ve improved upon, deepened, and simplified many of these aspects through what we&#39;ve learned from over a year of running local LLMs.</p>
<p>Download LM Studio for Mac, Windows (x86 / ARM), or Linux (x86) from <a href="https://lmstudio.ai" target="_blank" rel="noopener noreferrer">https://lmstudio.ai</a>.</p>
<h2 id="whats-new-in-lm-studio-030">What&#39;s new in LM Studio 0.3.0<a href="#whats-new-in-lm-studio-030" aria-label="Direct link to What&#39;s new in LM Studio 0.3.0" title="Direct link to What&#39;s new in LM Studio 0.3.0">â€‹</a></h2>
<h3 id="chat-with-your-documents">Chat with your documents<a href="#chat-with-your-documents" aria-label="Direct link to Chat with your documents" title="Direct link to Chat with your documents">â€‹</a></h3>
<p>LM Studio 0.3.0 comes with built-in functionality to provide a set of document to an LLM and ask questions about them.
If the document is short enough (i.e., if it fits in the model&#39;s &#34;context&#34;), LM Studio will add the file contents to the conversation in full. This is particularly useful for models that support long context such as Meta&#39;s Llama 3.1 and Mistral Nemo.</p>
<p>If the document is very long, LM Studio will opt into using &#34;Retrieval Augmented Generation&#34;, frequently referred to as &#34;RAG&#34;. RAG means attempting to fish out relevant bits of a very long document (or several documents) and providing them to the model for reference. This technique sometimes works really well, but sometimes it requires some tuning and experimentation.</p>
<p><strong>Tip for successful RAG</strong>: provide as much context in your query as possible. Mention terms, ideas, and words you expect to be in the relevant source material. This will often increase the chance the system will provide useful context to the LLM. As always, experimentation is the best way to find what works best.</p>
<h3 id="openai-like-structured-output-api">OpenAI-like Structured Output API<a href="#openai-like-structured-output-api" aria-label="Direct link to OpenAI-like Structured Output API" title="Direct link to OpenAI-like Structured Output API">â€‹</a></h3>
<p>OpenAI recently announced a JSON-schema based API that can result in reliable JSON outputs. LM Studio 0.3.0 supports this with any local model that can run in LM Studio!  We&#39;ve included a code snippet for doing this right inside the app. Look for it in the Developer page, on the right-hand pane.</p>
<h3 id="ui-themes">UI themes<a href="#ui-themes" aria-label="Direct link to UI themes" title="Direct link to UI themes">â€‹</a></h3>
<p>LM Studio first shipped in May 2024 in dark retro theme, complete with Comic Sans sprinkled for good measure. The OG dark theme held strong, and LM Studio 0.3.0 introduces 3 additional themes: Dark, Light, Sepia.  Choose &#34;System&#34; to automatically switch between Dark and Light, depending on your system&#39;s dark mode settings.</p>
<h3 id="automatic-load-parameters-but-also-full-customizability">Automatic load parameters, but also full customizability<a href="#automatic-load-parameters-but-also-full-customizability" aria-label="Direct link to Automatic load parameters, but also full customizability" title="Direct link to Automatic load parameters, but also full customizability">â€‹</a></h3>
<p>Some of us are well versed in the nitty gritty of LLM load and inference parameters. But many of us, understandably, can&#39;t be bothered. <strong>LM Studio 0.3.0 auto-configures everything</strong> based on the hardware you are running it on. If you want to pop open the hood and configure things yourself, LM Studio 0.3.0 has even more customizable options.</p>
<p>Pro tip: head to the My Models page and look for the gear icon next to each model. You can set per-model defaults that will be used anywhere in the app.</p>
<h3 id="serve-on-the-network">Serve on the network<a href="#serve-on-the-network" aria-label="Direct link to Serve on the network" title="Direct link to Serve on the network">â€‹</a></h3>
<p>If you head to the server page you&#39;ll see a new toggle that says &#34;Serve on Network&#34;. Turning this on will open up the server to requests outside of &#39;localhost&#39;. This means you could use LM Studio server from other devices on the network. Combined with the ability to load and serve multiple LLMs simultaneously, this opens up a lot of new use cases.</p>
<h3 id="folders-to-organize-chats">Folders to organize chats<a href="#folders-to-organize-chats" aria-label="Direct link to Folders to organize chats" title="Direct link to Folders to organize chats">â€‹</a></h3>
<p>Useful if you&#39;re working on multiple projects at once. You can even nest folders inside folders!</p>
<h3 id="multiple-generations-for-each-chat">Multiple generations for each chat<a href="#multiple-generations-for-each-chat" aria-label="Direct link to Multiple generations for each chat" title="Direct link to Multiple generations for each chat">â€‹</a></h3>
<p>LM Studio had a &#34;regenerate&#34; feature for a while. Now clicking &#34;regenerate&#34; keeps previous message generations and you can easily page between them using a familiar arrow right / arrow left interface.</p>
<h3 id="how-to-migrate-your-chats-from-lm-studio-0231-to-030">How to migrate your chats from LM Studio 0.2.31 to 0.3.0<a href="#how-to-migrate-your-chats-from-lm-studio-0231-to-030" aria-label="Direct link to How to migrate your chats from LM Studio 0.2.31 to 0.3.0" title="Direct link to How to migrate your chats from LM Studio 0.2.31 to 0.3.0">â€‹</a></h3>
<p>To support features like multi-version regenerations we introduced a new data structure under the hood.
You can migrate your pre-0.3.0 chats by going to Settings and clicking on &#34;Migrate Chats&#34;. This will make a copy, and will not delete any old files.</p>
<h2 id="full-list-of-updates">Full list of updates<a href="#full-list-of-updates" aria-label="Direct link to Full list of updates" title="Direct link to Full list of updates">â€‹</a></h2>
<h4 id="completely-refreshed-ui">Completely Refreshed UI:<a href="#completely-refreshed-ui" aria-label="Direct link to Completely Refreshed UI:" title="Direct link to Completely Refreshed UI:">â€‹</a></h4>
<ul>
<li>Includes themes, spellcheck, and corrections.</li>
<li>Built on top of lmstudio.js (TypeScript SDK).</li>
<li>New chat settings sidebar design.</li>
</ul>
<h4 id="basic-rag-retrieve--generate">Basic RAG (Retrieve &amp; Generate):<a href="#basic-rag-retrieve--generate" aria-label="Direct link to Basic RAG (Retrieve &amp; Generate):" title="Direct link to Basic RAG (Retrieve &amp; Generate):">â€‹</a></h4>
<ul>
<li>Drag and drop a PDF, .txt file, or other files directly into the chat window.</li>
<li>Max file input size for RAG (PDF / .docx) increased to 30MB.</li>
<li>RAG accepts any file type, but non-.pdf/.docx files are read as plain text.</li>
</ul>
<h4 id="automatic-gpu-detection--offload">Automatic GPU Detection + Offload:<a href="#automatic-gpu-detection--offload" aria-label="Direct link to Automatic GPU Detection + Offload:" title="Direct link to Automatic GPU Detection + Offload:">â€‹</a></h4>
<ul>
<li>Distributes tasks between GPU and CPU based on your machineâ€™s capabilities.</li>
<li>Can still be overridden manually.</li>
</ul>
<h4 id="browse--download-lm-runtimes">Browse &amp; Download &#34;LM Runtimes&#34;:<a href="#browse--download-lm-runtimes" aria-label="Direct link to Browse &amp; Download &#34;LM Runtimes&#34;:" title="Direct link to Browse &amp; Download &#34;LM Runtimes&#34;:">â€‹</a></h4>
<ul>
<li>Download the latest LLM engines (e.g., llama.cpp) without updating the whole app.</li>
<li>Available options: ROCm, AVX-only, with more to come.</li>
</ul>
<h4 id="automatic-prompt-template">Automatic Prompt Template:<a href="#automatic-prompt-template" aria-label="Direct link to Automatic Prompt Template:" title="Direct link to Automatic Prompt Template:">â€‹</a></h4>
<ul>
<li>LM Studio reads the metadata from the model file and applies prompt formatting automatically.</li>
</ul>
<h4 id="new-developer-mode">New Developer Mode:<a href="#new-developer-mode" aria-label="Direct link to New Developer Mode:" title="Direct link to New Developer Mode:">â€‹</a></h4>
<ul>
<li>View model load logs, configure multiple LLMs for serving, and share an LLM over the network (not just localhost).</li>
<li>Supports OpenAI-like Structured Outputs with <code>json_schema</code>.</li>
</ul>
<h4 id="folder-organization-for-chats">Folder Organization for Chats:<a href="#folder-organization-for-chats" aria-label="Direct link to Folder Organization for Chats:" title="Direct link to Folder Organization for Chats:">â€‹</a></h4>
<ul>
<li>Create folders to organize chats.</li>
</ul>
<h4 id="prompt-processing-progress-indicator">Prompt Processing Progress Indicator:<a href="#prompt-processing-progress-indicator" aria-label="Direct link to Prompt Processing Progress Indicator:" title="Direct link to Prompt Processing Progress Indicator:">â€‹</a></h4>
<ul>
<li>Displays progress % for prompt processing.</li>
</ul>
<h4 id="enhanced-model-loader">Enhanced Model Loader:<a href="#enhanced-model-loader" aria-label="Direct link to Enhanced Model Loader:" title="Direct link to Enhanced Model Loader:">â€‹</a></h4>
<ul>
<li>Easily configure load parameters (context, GPU offload) before model load.</li>
<li>Ability to set defaults for every configurable parameter for a given model file.</li>
<li>Improved model loader UI with a checkbox to control parameters.</li>
</ul>
<h4 id="support-for-embedding-models">Support for Embedding Models:<a href="#support-for-embedding-models" aria-label="Direct link to Support for Embedding Models:" title="Direct link to Support for Embedding Models:">â€‹</a></h4>
<ul>
<li>Load and serve embedding models.</li>
<li>Parallelization support for multiple models.</li>
</ul>
<h4 id="vision-enabled-models">Vision-Enabled Models:<a href="#vision-enabled-models" aria-label="Direct link to Vision-Enabled Models:" title="Direct link to Vision-Enabled Models:">â€‹</a></h4>
<ul>
<li>Image attachments in chats and API</li>
</ul>
<h4 id="show-conversation-token-count">Show Conversation Token Count:<a href="#show-conversation-token-count" aria-label="Direct link to Show Conversation Token Count:" title="Direct link to Show Conversation Token Count:">â€‹</a></h4>
<ul>
<li>Displays the current tokens and total context.</li>
</ul>
<h4 id="prompt-template-customization">Prompt Template Customization:<a href="#prompt-template-customization" aria-label="Direct link to Prompt Template Customization:" title="Direct link to Prompt Template Customization:">â€‹</a></h4>
<ul>
<li>Ability to override prompt templates.</li>
<li>Edit the &#34;Jinja&#34; template or manually provide prefixes/suffixes.</li>
<li>Prebuilt chat templates (ChatML, Alpaca, blank, etc.).</li>
</ul>
<h4 id="conversation-management">Conversation Management:<a href="#conversation-management" aria-label="Direct link to Conversation Management:" title="Direct link to Conversation Management:">â€‹</a></h4>
<ul>
<li>Add conversation notes.</li>
<li>Clone and branch a chat on a specific message.</li>
</ul>
<h4 id="customizable-chat-settings">Customizable Chat Settings:<a href="#customizable-chat-settings" aria-label="Direct link to Customizable Chat Settings:" title="Direct link to Customizable Chat Settings:">â€‹</a></h4>
<ul>
<li>Choose chat style and font size.</li>
<li>Remember settings for each model on load.</li>
</ul>
<h4 id="initial-translations">Initial Translations:<a href="#initial-translations" aria-label="Direct link to Initial Translations:" title="Direct link to Initial Translations:">â€‹</a></h4>
<ul>
<li>Support for Spanish, German, Russian, Turkish, Norwegian.</li>
<li>Community contributions are welcomed for missing strings and new languages!</li>
<li><a href="https://github.com/lmstudio-ai/localization" target="_blank" rel="noopener noreferrer">https://github.com/lmstudio-ai/localization</a></li>
</ul>
<h4 id="subtitles-for-config-parameters">Subtitles for Config Parameters:<a href="#subtitles-for-config-parameters" aria-label="Direct link to Subtitles for Config Parameters:" title="Direct link to Subtitles for Config Parameters:">â€‹</a></h4>
<ul>
<li>Descriptive subtitles for every configuration parameter.</li>
</ul>
<hr/>
<p>For more, join our Discord community: <a href="https://discord.gg/aPQfnNkxGC" target="_blank" rel="noopener noreferrer">https://discord.gg/aPQfnNkxGC</a></p>
<p>If you want to use LM Studio at your organization, get in touch! <a href="https://lmstudio.ai/cdn-cgi/l/email-protection#becadbdfd3fed2d3cdcacbdad7d190dfd7" target="_blank" rel="noopener noreferrer"><span data-cfemail="d2a6b7b3bf92bebfa1a6a7b6bbbdfcb3bb">[emailÂ protected]</span></a></p></div></div>
  </body>
</html>
