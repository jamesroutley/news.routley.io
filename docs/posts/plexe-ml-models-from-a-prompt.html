<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/plexe-ai/plexe">Original</a>
    <h1>Show HN: Plexe – ML Models from a Prompt</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/16990733/433660339-05ac238b-464c-457c-a63a-819bfe9a4fed.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY2MjE4ODQsIm5iZiI6MTc0NjYyMTU4NCwicGF0aCI6Ii8xNjk5MDczMy80MzM2NjAzMzktMDVhYzIzOGItNDY0Yy00NTdjLWE2M2EtODE5YmZlOWE0ZmVkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTA3VDEyMzk0NFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFjNDY4NDhlYmNiOTFkZTI1YThmY2FlN2YyMjQxM2MyNGU2MjcwMDg3ZDczZWYxMjQ4NWRlMWUzYTA3ZmNlMzYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LOkgSWVE5UYDqV4BQqP6pTqP-at823qzpEf31pxnE3c" data-canonical-src="https://private-user-images.githubusercontent.com/16990733/433660339-05ac238b-464c-457c-a63a-819bfe9a4fed.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDY2MjE4ODQsIm5iZiI6MTc0NjYyMTU4NCwicGF0aCI6Ii8xNjk5MDczMy80MzM2NjAzMzktMDVhYzIzOGItNDY0Yy00NTdjLWE2M2EtODE5YmZlOWE0ZmVkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTA3VDEyMzk0NFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFjNDY4NDhlYmNiOTFkZTI1YThmY2FlN2YyMjQxM2MyNGU2MjcwMDg3ZDczZWYxMjQ4NWRlMWUzYTA3ZmNlMzYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LOkgSWVE5UYDqV4BQqP6pTqP-at823qzpEf31pxnE3c" controls="controls" muted="muted">

  </video>
</details>





<p dir="auto">You can use plexe as a Python library to build and train machine learning models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import plexe

# Define the model
model = plexe.Model(
    intent=&#34;Predict sentiment from news articles&#34;,
    input_schema={&#34;headline&#34;: str, &#34;content&#34;: str},
    output_schema={&#34;sentiment&#34;: str}
)

# Build and train the model
model.build(
    datasets=[your_dataset],
    provider=&#34;openai/gpt-4o-mini&#34;,
    max_iterations=10
)

# Use the model
prediction = model.predict({
    &#34;headline&#34;: &#34;New breakthrough in renewable energy&#34;,
    &#34;content&#34;: &#34;Scientists announced a major advancement...&#34;
})

# Save for later use
plexe.save_model(model, &#34;sentiment-model&#34;)
loaded_model = plexe.load_model(&#34;sentiment-model.tar.gz&#34;)"><pre><span>import</span> <span>plexe</span>

<span># Define the model</span>
<span>model</span> <span>=</span> <span>plexe</span>.<span>Model</span>(
    <span>intent</span><span>=</span><span>&#34;Predict sentiment from news articles&#34;</span>,
    <span>input_schema</span><span>=</span>{<span>&#34;headline&#34;</span>: <span>str</span>, <span>&#34;content&#34;</span>: <span>str</span>},
    <span>output_schema</span><span>=</span>{<span>&#34;sentiment&#34;</span>: <span>str</span>}
)

<span># Build and train the model</span>
<span>model</span>.<span>build</span>(
    <span>datasets</span><span>=</span>[<span>your_dataset</span>],
    <span>provider</span><span>=</span><span>&#34;openai/gpt-4o-mini&#34;</span>,
    <span>max_iterations</span><span>=</span><span>10</span>
)

<span># Use the model</span>
<span>prediction</span> <span>=</span> <span>model</span>.<span>predict</span>({
    <span>&#34;headline&#34;</span>: <span>&#34;New breakthrough in renewable energy&#34;</span>,
    <span>&#34;content&#34;</span>: <span>&#34;Scientists announced a major advancement...&#34;</span>
})

<span># Save for later use</span>
<span>plexe</span>.<span>save_model</span>(<span>model</span>, <span>&#34;sentiment-model&#34;</span>)
<span>loaded_model</span> <span>=</span> <span>plexe</span>.<span>load_model</span>(<span>&#34;sentiment-model.tar.gz&#34;</span>)</pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto">2.1. 💬 Natural Language Model Definition</h3><a id="user-content-21--natural-language-model-definition" aria-label="Permalink: 2.1. 💬 Natural Language Model Definition" href="#21--natural-language-model-definition"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Define models using plain English descriptions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = plexe.Model(
    intent=&#34;Predict housing prices based on features like size, location, etc.&#34;,
    input_schema={&#34;square_feet&#34;: int, &#34;bedrooms&#34;: int, &#34;location&#34;: str},
    output_schema={&#34;price&#34;: float}
)"><pre><span>model</span> <span>=</span> <span>plexe</span>.<span>Model</span>(
    <span>intent</span><span>=</span><span>&#34;Predict housing prices based on features like size, location, etc.&#34;</span>,
    <span>input_schema</span><span>=</span>{<span>&#34;square_feet&#34;</span>: <span>int</span>, <span>&#34;bedrooms&#34;</span>: <span>int</span>, <span>&#34;location&#34;</span>: <span>str</span>},
    <span>output_schema</span><span>=</span>{<span>&#34;price&#34;</span>: <span>float</span>}
)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">2.2. 🤖 Multi-Agent Architecture</h3><a id="user-content-22--multi-agent-architecture" aria-label="Permalink: 2.2. 🤖 Multi-Agent Architecture" href="#22--multi-agent-architecture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The system uses a team of specialized AI agents to:</p>
<ul dir="auto">
<li>Analyze your requirements and data</li>
<li>Plan the optimal model solution</li>
<li>Generate and improve model code</li>
<li>Test and evaluate performance</li>
<li>Package the model for deployment</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">2.3. 🎯 Automated Model Building</h3><a id="user-content-23--automated-model-building" aria-label="Permalink: 2.3. 🎯 Automated Model Building" href="#23--automated-model-building"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Build complete models with a single method call:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model.build(
    datasets=[dataset_a, dataset_b],
    provider=&#34;openai/gpt-4o-mini&#34;,  # LLM provider
    max_iterations=10,              # Max solutions to explore
    timeout=1800                    # Optional time limit in seconds
)"><pre><span>model</span>.<span>build</span>(
    <span>datasets</span><span>=</span>[<span>dataset_a</span>, <span>dataset_b</span>],
    <span>provider</span><span>=</span><span>&#34;openai/gpt-4o-mini&#34;</span>,  <span># LLM provider</span>
    <span>max_iterations</span><span>=</span><span>10</span>,              <span># Max solutions to explore</span>
    <span>timeout</span><span>=</span><span>1800</span>                    <span># Optional time limit in seconds</span>
)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">2.4. 🚀 Distributed Training with Ray</h3><a id="user-content-24--distributed-training-with-ray" aria-label="Permalink: 2.4. 🚀 Distributed Training with Ray" href="#24--distributed-training-with-ray"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Plexe supports distributed model training and evaluation with Ray for faster parallel processing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from plexe import Model

# Optional: Configure Ray cluster address if using remote Ray
# from plexe import config
# config.ray.address = &#34;ray://10.1.2.3:10001&#34;

model = Model(
    intent=&#34;Predict house prices based on various features&#34;,
    distributed=True  # Enable distributed execution
)

model.build(
    datasets=[df],
    provider=&#34;openai/gpt-4o-mini&#34;
)"><pre><span>from</span> <span>plexe</span> <span>import</span> <span>Model</span>

<span># Optional: Configure Ray cluster address if using remote Ray</span>
<span># from plexe import config</span>
<span># config.ray.address = &#34;ray://10.1.2.3:10001&#34;</span>

<span>model</span> <span>=</span> <span>Model</span>(
    <span>intent</span><span>=</span><span>&#34;Predict house prices based on various features&#34;</span>,
    <span>distributed</span><span>=</span><span>True</span>  <span># Enable distributed execution</span>
)

<span>model</span>.<span>build</span>(
    <span>datasets</span><span>=</span>[<span>df</span>],
    <span>provider</span><span>=</span><span>&#34;openai/gpt-4o-mini&#34;</span>
)</pre></div>
<p dir="auto">Ray distributes your workload across available CPU cores, significantly speeding up model generation and evaluation when exploring multiple model variants.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">2.5. 🎲 Data Generation &amp; Schema Inference</h3><a id="user-content-25--data-generation--schema-inference" aria-label="Permalink: 2.5. 🎲 Data Generation &amp; Schema Inference" href="#25--data-generation--schema-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Generate synthetic data or infer schemas automatically:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Generate synthetic data
dataset = plexe.DatasetGenerator(
    schema={&#34;features&#34;: str, &#34;target&#34;: int}
)
dataset.generate(500)  # Generate 500 samples

# Infer schema from intent
model = plexe.Model(intent=&#34;Predict customer churn based on usage patterns&#34;)
model.build(provider=&#34;openai/gpt-4o-mini&#34;)  # Schema inferred automatically"><pre><span># Generate synthetic data</span>
<span>dataset</span> <span>=</span> <span>plexe</span>.<span>DatasetGenerator</span>(
    <span>schema</span><span>=</span>{<span>&#34;features&#34;</span>: <span>str</span>, <span>&#34;target&#34;</span>: <span>int</span>}
)
<span>dataset</span>.<span>generate</span>(<span>500</span>)  <span># Generate 500 samples</span>

<span># Infer schema from intent</span>
<span>model</span> <span>=</span> <span>plexe</span>.<span>Model</span>(<span>intent</span><span>=</span><span>&#34;Predict customer churn based on usage patterns&#34;</span>)
<span>model</span>.<span>build</span>(<span>provider</span><span>=</span><span>&#34;openai/gpt-4o-mini&#34;</span>)  <span># Schema inferred automatically</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">2.6. 🌐 Multi-Provider Support</h3><a id="user-content-26--multi-provider-support" aria-label="Permalink: 2.6. 🌐 Multi-Provider Support" href="#26--multi-provider-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Use your preferred LLM provider, for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model.build(provider=&#34;openai/gpt-4o-mini&#34;)          # OpenAI
model.build(provider=&#34;anthropic/claude-3-opus&#34;)     # Anthropic
model.build(provider=&#34;ollama/llama2&#34;)               # Ollama
model.build(provider=&#34;huggingface/meta-llama/...&#34;)  # Hugging Face    "><pre><span>model</span>.<span>build</span>(<span>provider</span><span>=</span><span>&#34;openai/gpt-4o-mini&#34;</span>)          <span># OpenAI</span>
<span>model</span>.<span>build</span>(<span>provider</span><span>=</span><span>&#34;anthropic/claude-3-opus&#34;</span>)     <span># Anthropic</span>
<span>model</span>.<span>build</span>(<span>provider</span><span>=</span><span>&#34;ollama/llama2&#34;</span>)               <span># Ollama</span>
<span>model</span>.<span>build</span>(<span>provider</span><span>=</span><span>&#34;huggingface/meta-llama/...&#34;</span>)  <span># Hugging Face    </span></pre></div>
<p dir="auto">See <a href="https://docs.litellm.ai/docs/providers" rel="nofollow">LiteLLM providers</a> for instructions and available providers.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">Plexe <em>should</em> work with most LiteLLM providers, but we actively test only with <code>openai/*</code> and <code>anthropic/*</code>
models. If you encounter issues with other providers, please let us know.</p>
</div>

<div dir="auto"><h3 tabindex="-1" dir="auto">3.1. Installation Options</h3><a id="user-content-31-installation-options" aria-label="Permalink: 3.1. Installation Options" href="#31-installation-options"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="pip install plexe                  # Standard installation
pip install plexe[lightweight]     # Minimal dependencies
pip install plexe[all]             # With deep learning support"><pre>pip install plexe                  <span><span>#</span> Standard installation</span>
pip install plexe[lightweight]     <span><span>#</span> Minimal dependencies</span>
pip install plexe[all]             <span><span>#</span> With deep learning support</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Set your preferred provider&#39;s API key
export OPENAI_API_KEY=&lt;your-key&gt;
export ANTHROPIC_API_KEY=&lt;your-key&gt;
export GEMINI_API_KEY=&lt;your-key&gt;"><pre><span><span>#</span> Set your preferred provider&#39;s API key</span>
<span>export</span> OPENAI_API_KEY=<span>&lt;</span>your-key<span>&gt;</span>
<span>export</span> ANTHROPIC_API_KEY=<span>&lt;</span>your-key<span>&gt;</span>
<span>export</span> GEMINI_API_KEY=<span>&lt;</span>your-key<span>&gt;</span></pre></div>
<p dir="auto">See <a href="https://docs.litellm.ai/docs/providers" rel="nofollow">LiteLLM providers</a> for environment variable names.</p>

<p dir="auto">For full documentation, visit <a href="https://docs.plexe.ai" rel="nofollow">docs.plexe.ai</a>.</p>

<p dir="auto">See <a href="https://github.com/plexe-ai/plexe/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for guidelines. Join our <a href="https://discord.gg/SefZDepGMv" rel="nofollow">Discord</a> to connect with the team.</p>

<p dir="auto"><a href="https://github.com/plexe-ai/plexe/blob/main/LICENSE">Apache-2.0 License</a></p>

<ul>
<li> Fine-tuning and transfer learning for small pre-trained models</li>
<li> Use Pydantic for schemas and split data generation into a separate module</li>
<li> Plexe self-hosted platform ⭐ (More details coming soon!)</li>
<li> Lightweight installation option without heavy deep learning dependencies</li>
<li> Distributed training with Ray on AWS</li>
<li> Support for non-tabular data types in model generation</li>
</ul>

<p dir="auto">If you use Plexe in your research, please cite it as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{plexe2025,
  author = {De Bernardi, Marcello AND Dubey, Vaibhav},
  title = {Plexe: Build machine learning models using natural language.},
  year = {2025},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/plexe-ai/plexe}},
}"><pre><span>@software</span>{<span>plexe2025</span>,
  <span>author</span> = <span><span>{</span>De Bernardi, Marcello AND Dubey, Vaibhav<span>}</span></span>,
  <span>title</span> = <span><span>{</span>Plexe: Build machine learning models using natural language.<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2025<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>howpublished</span> = <span><span>{</span>\url{https://github.com/plexe-ai/plexe}<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
