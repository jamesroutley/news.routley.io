<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huonw.github.io/blog/2024/08/async-hazard-mmap/">Original</a>
    <h1>Async hazard: MMAP is blocking IO</h1>
    
    <div id="readability-page-1" class="page"><article>
      
        
        
      
      
         
      
      <p>Memory mapping a file for reading sounds nice: turn inconvenient read calls and manual buffering into just simple indexing of a memory… but it does blocking IO under the hood, turn a <code>&amp;[u8]</code> byte arrays into an async hazard and making “concurrent” async code actually run sequentially!</p>

<p>Code affected likely runs slower, underutilises machine resources, and has undesirable latency spikes.</p>

<p>I’ve done some experiments in Rust that show exactly what this means, but I think this applies to any system that doesn’t have special handling of memory-mapped IO (including Python, and manual non-blocking IO in C).</p>

<h2 id="i-want-numbers">I want numbers</h2>

<p>I set up <a href="https://github.com/huonw/async-mmap-experiments">some benchmarks</a> that scan through 8 files, each 256 MiB, summing the value of every 512th byte. This is an uninteresting task, but it is designed to maximise IO usage and minimise everything else. This is thus a worst case, the impact on real code is unlikely to be quite this severe!</p>

<figure>
  <div>
    
  </div>
  <figcaption><p>Results of 100 iterations of the benchmarks on macOS on a M1 Macbook Pro, one plotted point per iteration, with a cold file system cache.</p>
</figcaption>
</figure>

<p>There’s 6 configurations, using all combinations of how files are read and what concurrency is used.<sup id="fnref:benchmark-details" role="doc-noteref"><a href="#fn:benchmark-details" rel="footnote">0</a></sup></p>

<p>For reading files, there’s two dimensions:</p>

<ol>
  <li>conventional IO: using explicit <code>read</code> calls.</li>
  <li>memory-mapped IO: using <a href="https://crates.io/crates/memmap2">the <code>memmap2</code> library</a>.</li>
</ol>

<p>For concurrency, there’s three:</p>

<ol>
  <li><code>async</code>/<code>await</code>: using <a href="https://tokio.rs/">the Tokio library</a><sup id="fnref:tokio-details" role="doc-noteref"><a href="#fn:tokio-details" rel="footnote">1</a></sup> in single-threaded mode<sup id="fnref:tokio-single-threaded" role="doc-noteref"><a href="#fn:tokio-single-threaded" rel="footnote">2</a></sup>.</li>
  <li>synchronously, with 8 threads: <a href="https://doc.rust-lang.org/stable/std/thread/fn.spawn.html">spawning a thread</a> per file.</li>
  <li>synchronously, with 1 thread: a baseline using a single thread to read sequentially.</li>
</ol>

<p>The results<sup id="fnref:table" role="doc-noteref"><a href="#fn:table" rel="footnote">3</a></sup> are clear: using memory mapped IO with <code>async</code>/<code>await</code> seems to be <strong>using no concurrency</strong>: the clusters of dots are near-identical for the first two rows! An <strong>explicitly-sequential single thread</strong> takes 2.5 to 3 seconds to scan all 2GiB with memory-mapped IO, and the “concurrent” <code>async</code>/<code>await</code> version looks identical. By comparison, using memory-mapped IO on 8 full threads is far faster, around 0.75 to 0.8 seconds.</p>

<p>Meanwhile, using conventional IO behaves more like we’d hope: using either form of concurrency is <strong>noticeably faster</strong> than running sequentially. Using either <code>async</code>/<code>await</code> or operating system threads results in reading all files in less than 0.65 seconds<sup id="fnref:maxed-out" role="doc-noteref"><a href="#fn:maxed-out" rel="footnote">4</a></sup>, while the sequential single-threaded version takes only a bit longer (0.7s) but with little overlap in the distributions.</p>

<h2 id="whats-going-on">What’s going on?</h2>

<p>Operating systems generally distinguish between bytes in RAM and files stored on disk. Reading a file is a dedicated operation that slurps bytes from disk into an array in memory. But there’s more shades of gray: <a href="https://en.wikipedia.org/wiki/Mmap">the <code>mmap</code> Unix system call</a> (or equivalent on other platforms) blurs this disk vs. memory distinction by setting up “memory mapped IO”.</p>

<p>The <code>mmap</code> operation allocates a range of (virtual) memory to a chosen file, making that memory “contain” the file: reading<sup id="fnref:writing" role="doc-noteref"><a href="#fn:writing" rel="footnote">5</a></sup> the first byte in memory gives the value of the first byte of the file, as does the second, and so on. The file essentially acts as a normal <code>&amp;[u8]</code> array that can be indexed and sliced.</p>

<figure><pre><code data-lang="rust"><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
</pre></td><td><pre><span>let</span> <span>file</span> <span>=</span> <span>std</span><span>::</span><span>fs</span><span>::</span><span>File</span><span>::</span><span>open</span><span>(</span><span>&#34;data.bin&#34;</span><span>)</span><span>?</span><span>;</span>
<span>let</span> <span>mmapped</span> <span>=</span> <span>unsafe</span> <span>{</span> <span>memmap2</span><span>::</span><span>Mmap</span><span>::</span><span>map</span><span>(</span><span>file</span><span>)</span><span>?</span> <span>};</span>

<span>let</span> <span>first_byte</span><span>:</span> <span>u8</span> <span>=</span> <span>mmapped</span><span>[</span><span>0</span><span>];</span>
<span>println!</span><span>(</span><span>&#34;File starts {first_bytes}&#34;</span><span>);</span>

<span>let</span> <span>central_bytes</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>]</span> <span>=</span> <span>&amp;</span><span>mmapped</span><span>[</span><span>1200</span><span>..</span><span>1300</span><span>];</span>
<span>println!</span><span>(</span><span>&#34;File contains {central_bytes:?}&#34;</span><span>);</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>This sounds a lot like what one would get by manually allocating a memory array as a buffer, seeking within the file, reading the file to fill that buffer, and then accessing the buffer… just without all the book-keeping. What’s not to like?</p>

<h2 id="under-the-hood">Under the hood</h2>

<p>To make this work, the operating system has to do magic.</p>

<p>One possible implementation might be to literally have the operating system allocate a chunk of physical memory and load the file into it, byte by byte, right when <code>mmap</code> is called… but this is slow, and defeats half the magic of memory mapped IO: manipulating files without having to pull them into memory. Maybe the machine has 4GiB of RAM installed, but we want to manipulate a 16GiB file.</p>

<p>Instead, operating systems will use the power of <strong>virtual memory</strong>: this allows some parts of the file to be in physical memory, while other parts are happily left on disk, and different parts can be paged in and out of memory as required. Sections of file are only loaded into memory <strong>as they are accessed</strong>.</p>

<figure><pre><code data-lang="rust"><table><tbody><tr><td><pre>1
2
3
4
5
</pre></td><td><pre><span>let</span> <span>mmapped</span><span>:</span> <span>Mmap</span> <span>=</span> <span>/* map some file into memory */</span><span>;</span>

<span>let</span> <span>index</span> <span>=</span> <span>12345</span><span>;</span>
<span>let</span> <span>byte_of_interest</span> <span>=</span> <span>mmapped</span><span>[</span><span>index</span><span>];</span>
<span>println!</span><span>(</span><span>&#34;byte #{index} has value {byte_of_interest}&#34;</span><span>);</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>That indexing accesses the 12 345th byte of the file. On the first access, there’ll be a <strong>page fault</strong>, and the operating system takes over: it loads the relevant data from the file on disk into actual physical memory, meaning the data is now available directly in normal RAM. This includes loading a small surrounding region—a page—which means future accesses to nearby addresses are fast.</p>

<p>While loading, the thread and its code will be <strong>blocked</strong>: the thread will be descheduled and won’t make further progress, because the next lines need the byte value to do their work.</p>

<p>Once loaded, the page of the file will be cached in memory for a while, and during that time any further accesses to addresses in the page will be fast, straight from memory. Depending on system memory usage and other factors, the operating system might then decide to evict the page from memory, freeing space for something else. Accessing it after eviction will require reloading from disk, with the same blocking behaviour and the cycle repeats.</p>

<p>In our application code, all this magic is packaged up into the simple indexing: <code>byte_of_interest = mmapped[index]</code>. The separate “already in memory” (fast) and “load from disk” (slow) cases are invisible<sup id="fnref:timing" role="doc-noteref"><a href="#fn:timing" rel="footnote">6</a></sup> in the code. In either case, we start with our memory address and end up with byte value.</p>

<h2 id="losing-the-thread">Losing the thread</h2>

<p>This magic is why our concurrency doesn’t work. The <code>async</code>/<code>await</code> construct is co-operative scheduling, with a “executor” or “event loop” that manages many tasks, swapping between them as required… but this swapping is <strong>only possible at explicit <code>await</code> points</strong>. If a task runs for a long time or is “blocked” without an <code>await</code>, the executor won’t be able to preempt it to run another task. Others have <a href="https://ryhl.io/blog/async-what-is-blocking/">written more about this blocking pitfall, far more knowledgeably than I</a>.</p>

<p>Remember that the code just looks like <code>let byte_of_interest = mmapped[index];</code>? There’s <strong>no <code>await</code></strong>, so the <code>async</code>/<code>await</code> executor <strong>cannot swap to another task</strong>. Instead, the operation blocks and deschedules the whole thread… But the executor is just normal code using that thread too, so the very thing that coordinates the concurrency is descheduled and can’t do any other work.</p>

<p>This is different to proper async IO, the individual task will still need to wait for the data to be read, but <code>await</code>s mean the executor can swap to another task while that happens: concurrency!</p>

<figure><pre><code data-lang="rust"><table><tbody><tr><td><pre>1
2
3
</pre></td><td><pre><span>let</span> <span>mut</span> <span>file</span><span>:</span> <span>tokio</span><span>::</span><span>fs</span><span>::</span><span>File</span> <span>=</span> <span>/* some file */</span><span>;</span>
<span>let</span> <span>mut</span> <span>buffer</span><span>:</span> <span>[</span><span>u8</span><span>;</span> <span>N</span><span>]</span> <span>=</span> <span>[</span><span>0</span><span>;</span> <span>N</span><span>];</span>
<span>file</span><span>.read</span><span>(</span><span>&amp;</span><span>mut</span> <span>buffer</span><span>)</span><span>.await</span><span>?</span><span>;</span> <span>// crucial await</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="at-a-distance">At a distance</h2>

<p>The <strong>most subtle</strong> part of this hazard to me is that the memory-mapped file <strong>looks like a normal buffer in memory</strong>. Code is likely to be implicitly assuming that indexing a <code>&amp;[u8]</code> is fast, and does not secretly read a file.</p>

<p>For instance, <code>memmap2::Mmap</code> derefs <a href="https://docs.rs/memmap2/0.6.2/memmap2/struct.Mmap.html#impl-Deref-for-Mmap">to <code>[u8]</code></a>, meaning we can take <code>fn f(b: &amp;[u8]) { ... }</code>, and call it like <code>f(&amp;mmapped)</code>. That function just sees <code>b: &amp;[u8]</code>, and won’t have <em>any</em> idea that indexing <code>b</code> might do blocking IO. It won’t know that it needs to take care with any co-operative concurrency it is performing.</p>

<p>Rust is just the demonstration here, <strong>this applies universally</strong>: <a href="https://docs.python.org/3/library/mmap.html">Python’s <code>mmap</code></a> explicitly says “behave like … <code>bytearray</code>”, while the C/POSIX <code>mmap</code> function returns <code>void *</code> (same as <code>malloc</code>). The whole point of memory-mapping is <strong>pretending a file is a byte array</strong>, and that’s what bites us here!</p>

<h2 id="caching-cha-ching">Caching: Cha-ching!</h2>

<p>We’ve seen above that the problem is when data has to be loaded from disk into memory. What happens if the data is <strong>in memory already</strong>?  We might expect memory-mapped IO to run much faster in that case, since it’s theoretically just accessing normal memory, straight from RAM.</p>

<p>This version of the benchmark times how long it takes to run the same task, immediately after doing the cold version we saw above, without purging caches.</p>

<figure>
  <div>
    
  </div>
  <figcaption><p>100 iterations of the same benchmark as above, but with a warm file system cache.</p>
</figcaption>
</figure>

<p>Our theory was right, it is <strong>much faster</strong>: with a cold cache, we were seeing memory-mapped IO on a single thread take 2.5s to read 2GiB. Now it takes 0.12s, 21× quicker, with <code>async</code>/<code>await</code> or without<sup id="fnref:table:1" role="doc-noteref"><a href="#fn:table" rel="footnote">3</a></sup>.</p>

<p>There’s some other notable observations about running with a warm cache:</p>

<ul>
  <li>Memory-mapped IO is faster than conventional IO. I imagine because there’s less overhead: for conventional IO, there’s an extra <code>memcpy</code> of data from the operating system page cache into the buffer passed into the <code>read</code> call. Using Tokio and <code>async</code>/<code>await</code> has even more overhead, likely because there’s synchronisation overhead with the thread-pool that has to be used for asynchronous file IO.</li>
  <li>Asynchronous and single-threaded sequential memory-mapped IO still take the time same time as each other, same as with a cold cache… just both are now fast.</li>
  <li>If you want speed, using multiple threads is handy: memory-mapped IO is achieving “file” IO speeds of  ~41GiB/s, by reading 2GiB in 49ms.</li>
  <li>This is likely system-dependent: I was running on arm64 macOS, and other systems may see different results.</li>
</ul>

<h2 id="more-questions">More questions</h2>

<p>I’ve done some experiments here, putting the “science” in “computer science”, and of course there’s more questions: I don’t know the answers!</p>

<ol>
  <li>How does this manifest across <strong>other platforms</strong>, beyond arm64 macOS? (Hypothesis: similar on all platforms.)</li>
  <li>Does the <strong>type of disk</strong> backing the files influence behaviour? (Hypothesis: yes, I’d expect running this on a high-latency disk like a spinning-rust HDD to show even worse blocking behaviour than the SSD used here, as each thread will be blocked for longer.)</li>
  <li>Does <strong><a href="https://en.wikipedia.org/wiki/Cache_control_instruction#Prefetch">prefetching</a> via CPU instructions</strong> help mitigate the problems? (Hypothesis: not sure. I have no idea if running a prefetch instruction on a memory-mapped address will cause the page to be loaded in the background. One may have to prefetch many, many loop iterations in advance, since this is reading all the way from disk, not just from RAM to CPU cache, for which prefetching is often used.)</li>
  <li>How do <strong>other <code>mmap</code>/<code>madvise</code> options</strong> influence this (for instance, <code>MADV_SEQUENTIAL</code>, <code>MADV_WILLNEED</code>, <code>MADV_POPULATE</code>, <code>MADV_POPULATE_READ</code>, <code>mlock</code>)? (Hypothesis: these options will make it more likely that data is pre-cached and thus fall into fast path more often, but without a guarantee.)</li>
  <li>What about <strong><a href="https://en.wikipedia.org/wiki/Readahead"><code>readahead</code></a></strong>? (Hypothesis: making the fast path more likely but not guaranteed, similar to the previous point.)</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Accessing files via memory mapped IO makes for a very convenient API: just read a byte with normal indexing. However, it’s <strong>error-prone</strong> when used with co-operatively scheduled concurrency, like <code>async</code>/<code>await</code> in Rust or Python, or even “manual” non-blocking IO in C:</p>

<ul>
  <li>If the data isn’t available in memory yet, the operating system will have to <strong>block the thread</strong> while reading from disk, and there’s no <code>await</code>s to allow the <code>async</code> executor to run another task.</li>
  <li>Thus, heavy use of <code>mmap</code> can potentially be <strong>as slow as sequential code</strong>!</li>
  <li>It’s a <strong>hazard at a distance</strong>: a memory-mapped file can be coerced to a look like a normal byte array (<code>&amp;[u8]</code>, <code>bytearray</code>/<code>list[int]</code>, or <code>void *</code>/<code>char *</code>) and passed deep into the async code before being indexed.</li>
  <li>If data is <strong>cached</strong> by the operating system, <code>mmap</code> has less overhead than conventional IO and can be a little faster (on my system).</li>
</ul>

<section id="external-links">
  
  
  
  
</section>





    </article></div>
  </body>
</html>
