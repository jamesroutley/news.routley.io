<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://izzys.casa/2024/08/463-python-interpreters/">Original</a>
    <h1>500 Python Interpreters</h1>
    
    <div id="readability-page-1" class="page"><article><p>As we approach the final release date for Python 3.13, I’ve seen an uptick in
discussion regarding 3.13’s introduction of an optional <abbr title="Global
Interpreter Lock">GIL</abbr>. While <em>removing</em> the GIL has been a long time
coming for the average user (I’ve dreamt of this for nearly 20 years), there
have actually been two concurrent efforts to improve Python’s performance for
multithreading. The first is the optional GIL, specified in <a href="https://peps.python.org/pep-0703/">PEP 703</a>, and
second is the introduction of a per-interpreter GIL, specified in <a href="https://peps.python.org/pep-0684/">PEP 684</a> and
introduced in <a href="https://docs.python.org/3/whatsnew/3.12.html#pep-684-a-per-interpreter-gil">Python 3.12</a>.</p><p>In this post, I’ll explain some of the history of the GIL, how it’s affected
the C API design of Python, and what steps were taken at the C API level to get
us to where we are today. I’ll also briefly discuss how <a href="https://peps.python.org/pep-0684/">PEP 684</a> and <a href="https://peps.python.org/pep-0703/">PEP 703</a>
will be able to work in tandem. And to explain all of this, we’re going to
start with my personal history on the matter and my first run-in (and
faceplant) with Python’s GIL. Learning to make</p><p>✨<em><strong><span><span>video games</span>
<span aria-hidden="true"><span>v</span><span>i</span><span>d</span><span>e</span><span>o</span> <span>g</span><span>a</span><span>m</span><span>e</span><span>s</span></span></span></strong></em>✨</p><h2 id="a-dark-seed-is-planted">A Dark Seed Is Planted</h2><p>In the summer of 2005, I attended a several week long game development summer
camp that was located at Stanford University. This was my first real
introduction to programming. I don’t count the C++ class I took in freshman
year of high school because we just simply copied text from JPEGs of a scan of
a C++ book that were scaled down so that two pages could fit on an 8.5x11&#34;
paper. At the time, this was (for me) no different than reading the <a href="https://en.wikipedia.org/wiki/Voynich_manuscript">Voynich
Manuscript</a> or <a href="https://en.wikipedia.org/wiki/Codex_Seraphinianus">Codex
Seraphinianus</a>.</p><p>I was being taught Python 2.4, a whole year before the 2.5 release. Now at the
time, <a href="https://www.pygame.org/">PyGame</a> was at around version 1.7.1 and both
then and now requires some knowledge of Python to be used effectively. I would
argue that even today teaching someone Python via PyGame is not a good
introduction to Python <em>the language</em>. It is an intermediate step after you’ve
gotten the basics of objects and for loops and the like down. However we were
using a (slightly modified fork) of a library named <em>livewires</em>, which was a
wrapper that greatly simplified a <em>lot</em> of the logic behind PyGame so you could
focus on teaching.</p><p>This fork had been written by my camp instructor, a programmer, TV show writer,
and teacher by the name of Michael Dawson. For those of you who read the
section title, and are also familiar with ancient
<span><span>eldritch</span><span arid-hidden="true"><span>
<span><span>e</span>
</span></span><span><span><span>l</span>
</span></span><span><span><span>d</span>
</span></span><span><span><span>r</span>
</span></span><span><span><span>i</span>
</span></span><span><span><span>t</span>
</span></span><span><span><span>c</span>
</span></span><span><span><span>h</span>
</span></span></span></span>video game lore, you might recognize his name
as the lead programmer, writer, and star of the 1992 DOS game <a href="https://en.wikipedia.org/wiki/Dark_Seed_(video_game)"><em>Dark
Seed</em></a><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. This was, for
me, a huge deal, as it lended an air of authority to his teaching: he’d made an
<em>actual game</em> and mostly by himself. Surely, I could do the same thing one day.
Right? 🙂</p><p>During this instructional period, I was using a large number of sprites. I was
<em>too inexperienced</em> to know what a sprite atlas was, let alone to implement it
with the API we were using under livewires. My load times were atrocious. I had
maybe 100+ sprites, each with about 20 animations, that I’d pulled from various
geocities websites. I was quite honestly at a loss. Outside of these
instructional classes, we had additional activities where we could socialize
with other people attending the camp. One of these people was very on the up
and up in terms of open source, Linux, and the like (they owned a <a href="https://en.wikipedia.org/wiki/GP2X">GP2X</a> and
managed to get their game to work on the device) compared to myself. I
explained to them the issue I was having, and they showed me how to profile my
code, and we saw that because I was loading in a sprite one at a time, it would
take forever to open and read each and every individual sprite. They then
suggested I use the <code>threading</code> module to at least speed things up. After
spending some time tinkering we got it working and… my load times had
increased 😭.</p><p>The next day, I asked Michael what the issue could be and with a heavy sigh he
had to explain the issue with threads and the Python interpreter, as I had
stumbled into the oldest issue Python has had to date for performance<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>: that
<em>damned</em> GIL.</p><p>I’m also not the only person who ran into this issue within the context of
games. <em>Sid Meier’s Civilization IV</em> used Python for its game logic and
scripting systems. Games would run slower the more units were on the map and
the more turns that had executed. It was a known issue that Civ IV had (and one
might even argue still has) performance issues. With the release of <em>Sid
Meier’s Civilization V</em> the team had switched to Lua and this was touted as a
big deal because it meant a huge performance improvement. And it was! Fewer
lockups and games didn’t take forever to process an AI turn (usually).</p><p>Alas! The beef I’ve had with the GIL was planted as a dark seed about 20 years
ago. For many years I’ve been filled with jealousy towards Lua even going so
far as to <a href="https://peterlyons.com/2015/03/why-i-dont-use-lua/">refuse to use it unless forced to for the last 9 years</a>, though this was mostly because I had to operate
directly on a stack and worry about things like <em>upvalues</em>. What’re upvalues? I
don’t know dawg, what’s up with you?</p><h2 id="erm-what-the-sigma-gil">Erm, what the <del>sigma</del> GIL?</h2><p>OK, so there’s a general misunderstanding of <em>what</em> the GIL <em>is</em>, in addition
to <em>why</em> it even exists in the first place. To better comprehend the situation,
we’re going to talk specifically about the evolution of CPython and it’s
interpreter.</p><h3 id="a-short-history-of-threading">A Short History of Threading</h3><p>Threading was first introduced in Python on <time datetime="1998-02-17">February 17th, 1998</time> with the release of Python 1.5. This release brought
along with it the <code>threading</code> module and is also where the
<a href="https://docs.python.org/3/c-api/init.html#c.PyInterpreterState"><code>PyInterpreterState</code></a> and <a href="https://docs.python.org/3/c-api/init.html#c.PyThreadState"><code>PyThreadState</code></a> objects were first introduced,
bringing the GIL along with them. Prior to this, Python had a simple evaluation
execution step in a single function named <code>code_eval</code> that would in some cases
recurse into itself. The introduction of <code>PyInterpreterState</code> and
<code>PyThreadState</code> were a big deal as this allowed Python to finally begin moving
towards an embeddable state where users could not only <em>extend</em> Python, but
also <em>embed</em> it. This might not seem like such a big deal, but it was 1998. The
number of scripting languages in use at the time were many, but each had their
own limitations with regards for C to interact with it in custom programs.
Hell, go and look at Perl’s C API <em>today</em> and you can see a very 90s API
design that might be viewed as anti-embedding, but really was just a pragmatic
approach to assume that if you were trying to embed Perl as a C library, you
were the Perl interpreter.</p><p>This newly introduced <code>PyInterpreterState</code> had a single instance, and even with
the upcoming release of Python 3.13, there is still a <code>static PyInterpreterState*</code> object that represents the main <code>PyInterpreterState</code>. Even
if this <code>PyInterpreterState</code> is not used, it still exists.</p><p>This is why the <code>multiprocessing</code> module has historically been faster for
Python. We’re creating a whole new interpreter instance as a subprogram, and
thus it does not need to worry about a GIL. However, this comes at the cost of
an entire subprocess and the various work involved with communication between
the two. For Windows especially, this is a very heavy approach for a speedup,
yet manages to still be faster in the high contention cases.</p><p>Thus, with the release of 3.13, <code>multiprocessing</code> is <em>technically</em> only needed
in cases where the GIL is still needed, and even then it still can carry a cost
compared to threading in <em>some</em> specific use cases.</p><h3 id="big-gil-so-what">Big GIL, So What?</h3><p>OK, so if you’re coming from other scripting languages this approach might not
make much sense. For those of you coming from <code>lua_State*</code>s, <code>JSContextRef</code>s,
<code>v8::Isolate</code>s, and more, we need to take a look at how Python was evolving.
There was no real line in the sand to explain the direction Python would be
taking. Call me one to speculate, but it seemed like features were being added
because someone wanted them, rather than because of some goal in mind. This 1.5
release predates both the Python Software Foundation and <a href="https://peps.python.org/pep-0001">PEP 1</a>, which was
introduced only with the release of Python 2.0 (alongside the PEP process).
Thus, there was no real community direction that Python could go in, beyond
whatever work someone was willing to contribute, and therefore these changes
were made because… well what else was there at the time? We had Perl, Tcl, a
bunch of implementations of scheme that are completely forgettable, and while
Lua 3.0 was out, it wouldn’t be until about 5 months later in July of 1998 that
Lua 3.1 was released along with its now well known <code>lua_State*</code> (and even then
there were semantic issues that weren’t resolved until the 4.0 release).</p><p>In fact if we look at what was available for existing API designs to pull from,
the only real stable designs for execution were Tcl (which people have gone
great lengths to avoid copying, because when you do try to copy its design you
get CMake<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>), and OpenGL. As OpenGL has a “context” object that you must swap
to/make current on the given thread it should be no surprise that Python
implemented a similar interface while eschewing the isolated single-thread
interpreter design.</p><p>It’s extremely important to keep in mind options for these decisions were
limited at the time. People were more or less stumbling around in the dark for
the C API side of dynamically typed scripting languages. We didn’t have
decades of mistakes and real world use to learn from. Even today when we know
there are “better” options sometimes the worse one wins out. For example, Lua
has constantly been heralded in various spaces for it’s register based VM. Yet
WASM, which is shaping up to be the cross platform <em>du jour</em> VM of the 21st
century, went with a stack based VM. This decision was driven entirely because
of ✨ reasons ✨ that are outside the scope of this post and that I don’t
actually think hold water and if <em>you</em> think they do you can <em>shut up nerd</em>.</p><h3 id="how-can-she-interpret">How Can She Interpret‽‽</h3><p>We’re near the end of this explanation so let’s take a brief look at how the
GIL works in practice at the C API level prior to making the GIL optional. For
starters, users create something called a <em>sub-interpreter</em>, which is what a
<code>PyThreadState</code> technically <em>is</em>. This is done via the poorly named
<a href="https://docs.python.org/3/c-api/init.html#c.Py_NewInterpreter"><code>Py_NewInterpreter</code></a>. When this is called, it uses whatever is set as the
<em>global interpreter</em> and adds it to the list of <code>PyThreadState</code>s that
<code>PyInterpreterState</code> keeps track of. Users are then expected to acquire the GIL
at some point upon the <code>PyThreadState</code> and then evaluate some Python code, then
<em>release</em> the GIL from the <code>PyThreadState</code> and at this point they are free to
either <em>destroy</em> the <code>PyThreadState</code> <em><strong>or</strong></em> they are able to just keep it
around and keep executing as needed. No matter <em>what</em> happens there is going to
be <em>a</em> lock occurring. No two <code>PyThreadState</code>s can execute <em>Python bytecode</em> at
the same time. However, they <em>can</em> execute multiple C calls at the same time
which is why for long running pure C operations extension and embedding
developers are encouraged to release the GIL temporarily. Technically speaking
this is where something like
<a href="https://clang.llvm.org/docs/ThreadSanitizer.html">ThreadSanitizer</a> can come in
handy.</p><p>This approach to extensions is also sort of why we have the GIL in the first
place. Up until Python 3.5, which introduced so-called multi-phase extension
module initialization via <a href="https://peps.python.org/pep-0489/">PEP 489</a> we didn’t actually have a way to even
really isolate modules once they were imported. All objects, exceptions,
methods, etc. were all added at the same time and are inserted into the central
main interpreter. The real purpose is to effectively separate native code
(written in whatever language you want, though most folks are using C++, C,
Zig, or Rust these days) with the actual binding. Thus “when in Python, there
is a lock” and extension developers are given a level of trust that the lock
can be disengaged and re-engaged as long as Python objects are not being
touched<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p><p>As far as I know, Rust’s lifetimes can’t <em>entirely</em> be enforced across these
dynamic library extensions unless it’s encoded at the C ABI level in some
manner and to my knowledge <a href="https://py03.rs">PyO3</a> and other wrappers do <em>not</em>
expose this at the type system level for the compiler and to warn users for raw
locking/unlocking operations, preferring to instead provide an
<a href="https://docs.rs/pyo3/latest/pyo3/marker/struct.Python.html#method.allow_threads"><code>allow_threads</code></a>
method that will handle the locking/unlocking within the execution of a user
provided closure that performs the actual work (an operation that any “RAII”
capable language might provide for sanity and safety’s sake)</p><p>With the release of Python <em>3.12</em> however we actually saw the first vestiges of
the Python C API that would allow us to create a <em>sub interpreter</em> that has no
connection to the <em>global</em> interpreter. This API exists in the form of
<a href="https://docs.python.org/3/c-api/init.html#c.Py_NewInterpreterFromConfig"><code>Py_NewInterpreterFromConfig</code></a>, where a <a href="https://docs.python.org/3/c-api/init.html#c.PyInterpreterConfig"><code>PyInterpreterConfig</code></a> structure
is passed in. This structure allows users to set a few “hard” settings. For
example, you can disable the <code>threading</code> module, <code>os.execv()</code>, and process
forking (though <code>subprocess</code> does not create forked processes, launching
entirely brand new ones instead). The most important setting of these however
is the <a href="https://docs.python.org/3/c-api/init.html#c.PyInterpreterConfig.gil"><code>PyInterpreterConfig.gil</code></a> member, which allows users to either
<em>share</em> the sub-interpreter’s GIL with the <em>main</em> interpreter’s GIL <em>or</em> they
can make the sub-interpreter have it’s <em>own</em> GIL, thus keeping it isolated from
the rest of the system. There are a few caveats here. Although the <code>sys.stdin</code>
object is a <em>unique</em> <code>PyObject*</code>, it does refer to the same underlying handle
as other <code>sys.stdin</code> objects. This can be semi-resolved by just changing it.</p><h2 id="500-interpreter-states">500 Interpreter States</h2><p>OK, so with everything we’ve mentioned up until now you’re more or less
prepared to understand <em>why</em> we’ve got these wacky shenanigans. So let’s make
an extreme example. Let’s make:</p><figure><img src="https://peterlyons.com/2024/08/463-python-interpreters/what-python-does-to-a-motherfucker.webp"/><figcaption><h4>500 Interpreter States</h4></figcaption></figure><p>This code example is going to be very “high level”. We won’t be doing proper
due diligence for API design because this is meant to be an example, not
production ready code. With that in mind, we have a few things we need to do
before we even create our python interpreters. The first of these is to create
an isolated configuration for the default interpreter. This is extremely
important if you’re doing a proper embed of C Python, as you might <em>not</em> want
to care about things like user directories and the like to modify how python
might execute.</p><div><pre tabindex="0"><code data-lang="cxx"><span><span>PyConfig config {};
</span></span><span><span>PyConfig_InitIsolatedConfig(<span>&amp;</span>config);
</span></span><span><span><span>if</span> (<span>auto</span> status <span>=</span> Py_InitializeFromConfig(<span>&amp;</span>config); PyStatus_IsError(status)) {
</span></span><span><span>    <span>/* handle error here */</span>
</span></span><span><span>}
</span></span><span><span>PyConfig_Clear(<span>&amp;</span>config);
</span></span></code></pre></div><p>There is additional work one might want to do with the <a href="https://docs.python.org/3/c-api/init_config.html#c.PyPreConfig"><code>PyPreConfig</code></a> set of
APIs, but I… don’t care about handling that. Read the documentation. You can
figure it out 😉</p><p>This configuration creation does the very basic steps for initializing Python
into an <em>isolated</em> state, which is intended for situations where users might
not be shipping the Python stdlib alongside their executable.</p><p>Next, we’re going to just spin up all our threads. When creating these new
<code>PyThreadState</code> instances they are set as the <em>current</em> thread state after
creation. This means we more or less need to initialize them <em>inside</em> of a
thread. We can get creative with the situation if we’re using C++, where
<code>thread_local</code> exists as a keyword + storage qualifier. This also lets us know
if a given thread has been initialized for a thread state. We won’t be doing
any fancy checking here, since this is “babby’s first example”, but this sort
of design paradigm might come in handy. We’ll also just define the maximum
number of states as a constant to save us some time.</p><div><pre tabindex="0"><code data-lang="cxx"><span><span><span>// Mark this as inline if you&#39;re placing it in a header. Save your sanity. 🙂
</span></span></span><span><span><span></span><span>static</span> <span>thread_local</span> <span>inline</span> PyThreadState<span>*</span> state <span>=</span> <span>nullptr</span>;
</span></span><span><span><span>// 500 interpreter states 😌
</span></span></span><span><span><span></span><span>static</span> <span>constexpr</span> <span>auto</span> MAXIMUM_STATES <span>=</span> <span>463</span>;
</span></span></code></pre></div><p>Next we’ll create our default <code>PyInterpreterConfig</code>. Because we’re relying on
capturing this configuration object we don’t need to make it global in the
function we’re executing.</p><div><pre tabindex="0"><code data-lang="cxx"><span><span>PyInterpreterConfig config <span>=</span> {
</span></span><span><span>  .use_main_obmalloc <span>=</span> <span>0</span>,
</span></span><span><span>  .allow_fork <span>=</span> <span>0</span>,
</span></span><span><span>  .allow_exec <span>=</span> <span>0</span>,
</span></span><span><span>  .allow_threads <span>=</span> <span>0</span>,
</span></span><span><span>  .allow_daemon_threads <span>=</span> <span>0</span>,
</span></span><span><span>  .check_multi_interp_extensions <span>=</span> <span>1</span>,
</span></span><span><span>  .gil <span>=</span> PyInterpreterConfig_OWN_GIL,
</span></span><span><span>};
</span></span></code></pre></div><p>There’s a few details here that are more accurately explained in the Python
documentation, however the biggest two things that matter are the
<code>check_multi_interp_extensions</code> value and <code>gil</code>. The
<code>check_multi_interp_extensions</code> value effectively forces all extensions to be
what is known as a <em>multi phase</em> extension. This is where you initialize some
of the module data, but don’t hook up objects, exceptions, and actual python
bindings until a separate function is called. This was a huge stumbling block
prior to either the per-GIL subinterpreter and the optional GIL. By forcing all
extensions to be multi-phase, you can create per-extension instances of all
your objects. This can get hairy if you <em>actually</em> have some hidden globals
within your native code that you’re binding, but surely you wouldn’t lie about
that ever, right? 🙂</p><p>Moving on, we’ll generate all our threads to create an interpreter state,
execute some python code, print to <code>sys.stdout</code> and then shut down our
interpreter. Easy Peesy, Lemon Squeezy.</p><div><pre tabindex="0"><code data-lang="cxx"><span><span><span>// We want the threads to join on destruction, so std::jthread it is
</span></span></span><span><span><span></span>std<span>::</span>vector<span>&lt;</span>std<span>::</span>jthread<span>&gt;</span> tasks { };
</span></span><span><span>tasks.reserve(MAXIMUM_STATES);
</span></span><span><span><span>for</span> (<span>auto</span> count <span>=</span> <span>0</span>zu; count <span>&lt;</span> tasks.capacity(); count<span>++</span>) {
</span></span><span><span>  tasks.emplace_back([config, count] {
</span></span><span><span>    <span>if</span> (<span>auto</span> status <span>=</span> Py_NewInterpreterFromConfig(<span>&amp;</span>state, <span>&amp;</span>config); PyStatus_IsError(status)) {
</span></span><span><span>        std<span>::</span>println(<span>&#34;Failed to initialize thread state {}. Received error {}&#34;</span>, count, status.err_msg);
</span></span><span><span>        <span>return</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>auto</span> text <span>=</span> std<span>::</span>format(R<span>&#34;(print(&#34;</span>Hello, world<span>!</span> From Thread {}<span>&#34;))&#34;</span>, count);
</span></span><span><span>    <span>auto</span> globals <span>=</span> PyDict_New();
</span></span><span><span>    <span>auto</span> code <span>=</span> Py_CompileString(text.data(), __FILE__, Py_eval_input);
</span></span><span><span>    <span>auto</span> result <span>=</span> PyEval_EvalCode(code, globals, globals);
</span></span><span><span>    Py_DecRef(result);
</span></span><span><span>    Py_DecRef(code);
</span></span><span><span>    Py_DecRef(globals);
</span></span><span><span>    Py_EndInterpreter(state);
</span></span><span><span>    state <span>=</span> <span>nullptr</span>;
</span></span><span><span>  });
</span></span><span><span>}
</span></span></code></pre></div><p>Now, we can just let the threads go out of scope and call <code>Py_Finalize()</code>. What
could possibly go wrong?</p><h3 id="fuck-you-asshole">FUCK YOU, ASSHOLE!</h3><p>IF YOU’RE A DUMB ENOUGH TO TRY TO RUN <strong><del>500</del> 463 PYTHON INTERPRETER STATES IN A
DEBUG BUILD</strong>, YOU’RE A BIG ENOUGH SHMUCK TO COME ON DOWN TO BIG BILL’S HELL
INTERPRETERS.</p><ul><li>MEMORY CORRUPTIONS!</li><li>SYNCHRONIZATION ISSUES</li><li>THE MESSAGE <code>OUCH</code> IN YOUR TERMINAL FROM PYTHON’S MEMORY TRACKING</li><li>THIEVES</li></ul><p>IF YOU THOUGHT THIS C AND C++ CODE WAS GONNA RUN JUST FINE WITHOUT ANY ISSUE</p><p><strong>YOU CAN KISS MY ASS</strong></p><p>YOU HEARD ME RIGHT</p><p><strong>YOU CAN KISS MY ASS</strong></p><h3 id="ok-hold-up-whats-happening">OK, hold up, what’s happening?</h3><p>So unfortunately during the writing of this post, and the reason it’s been
delayed by so much (I wanted to post it last weekend 😭), is because I was
running into memory corruption issues. I thought “Hey, you know it’s been so
long since I’ve use C++’s <code>std</code> threading API that I probably fucked up.”</p><p>Unfortunately, I <em>did not fuck up</em> and my C++ code was correct. It seems there
are some latent memory corruption issues if you are creating and then
destroying 500 python interpreter states in debug mode. I unfortunately wasn’t
able to nail it down to any one specific issue beyond “at some point after
about 10 <code>PyThreadState</code>s being created and destroyed constantly”, you get
memory corruption, and there’s no way to recover.</p><p>On closer inspection, this is a case of memory reuse, where destroyed
<code>PyThreadState</code>s are not fully zeroed out, and this causes Python’s memory
debug assertions to fire off. Effectively, because we’re creating and
destroying these <code>PyThreadState</code>s as we execute, we’re breaking some
assumptions within Python’s C code. I do not know if this a security issue or
not, but it most certainly IS something that should be resolved, even if the
reason I found it was by writing a shitpost 😅. I collected what little
information I could, and have submitted an issue on the Python GitHub
repository, which you can find
<a href="https://github.com/python/cpython/issues/123134">here</a>.</p><h2 id="the-dream-of-the-child">The Dream Of The Child</h2><p>So here we are in 2024, and what I’d hoped and dreamed would eventually happen
in my childhood (the GIL going away) is finally on the horizon. I know the
current state of things is not great in the Python space, but for a language
that’s been running as long as it has, it should be no surprise that there will
be significant challenges along the way.</p><p>The only thing that pains me is that these days there <em>are</em> better solutions
than using Python for an embedded scripting language (that aren’t named Lua).
From JS implementations like V8, to .NET Core being self hostable, to so, so
many WASM implementations, to CPU emulators like
<a href="https://github.com/libriscv/libriscv/">libriscv</a>. Sadly, even alternative
Python VM implementations like <a href="https://github.com/pocketpy/pocketpy">pocketpy</a>
all seem like better options.</p><section><a href="https://peterlyons.com/tags/#python">python</a></section></article></div>
  </body>
</html>
