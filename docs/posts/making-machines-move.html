<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fly.io/blog/machine-migrations/">Original</a>
    <h1>Making Machines Move</h1>
    
    <div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Thomas Ptacek" src="https://elijer.github.io/static/images/thomas.webp"/>
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Thomas Ptacek
                 </dd>
                  <dt>@tqbf</dt>
                  <dd>
                    <a href="https://twitter.com/tqbf" target="_blank">
                      @tqbf
                    </a>
                  </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://elijer.github.io/blog/machine-migrations/assets/migrations-cover.webp" alt="Two rectangles on a dark background. A cartoon bird stands within one rectangle, its gaze fixed on the empty space within the other. A dashed arrow arcs from the bird to its targeted landing spot."/>
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p>We’re Fly.io, a global public cloud with simple, developer-friendly ergonomics. If you’ve got a working Docker image, we’ll transmogrify it into a Fly Machine: a VM running on our hardware anywhere in the world. <a href="https://fly.io/speedrun" title="">Try it out; you’ll be deployed in just minutes</a>.</p>
<p>At the heart of our platform is a systems design tradeoff about durable storage for applications.  When we added storage three years ago, to support stateful apps, we built it on attached NVMe drives. A benefit: a Fly App accessing a file on a Fly Volume is never more than a bus hop away from the data. A cost: a Fly App with an attached Volume is anchored to a particular worker physical.</p>
<p><code>bird</code>: a BGP4 route server.</p>
<p>Before offering attached storage, our on-call runbook was almost as simple as “de-bird that edge server”, “tell <a href="https://fly.io/blog/carving-the-scheduler-out-of-our-orchestrator/" title="">Nomad</a> to drain that worker”, and “go back to sleep”. NVMe cost us that drain operation, which terribly complicated the lives of our infra team. We’ve spent the last year getting “drain” back. It’s one of the biggest engineering lifts we’ve made, and if you didn’t notice, we lifted it cleanly.</p>
<h2 id="the-goalposts"><a href="#the-goalposts" aria-label="Anchor"></a><span>The Goalposts</span></h2>
<p>With stateless apps, draining a worker is easy. For each app instance running on the victim server, start a new instance elsewhere. Confirm it’s healthy, then kill the old one. Rinse, repeat. At our 2020 scale, we could drain a fully loaded worker in just a handful of minutes.</p>

<p>You can see why this process won’t work for apps with attached volumes. Sure, create a new volume elsewhere on the fleet, and boot up a new Fly Machine attached to it. But the new volume is empty. The data’s still stuck on the original worker. We asked, and customers were not OK with this kind of migration.</p>

<p>Of course, we back Volumes snapshots up (at an interval) to off-network storage. But for “drain”, restoring backups isn’t nearly good enough. No matter the backup interval, a “restore from backup migration&#34; will lose data, and a “backup and restore” migration  incurs untenable downtime.</p>

<p>The next thought you have is, “OK, copy the volume over”. And, yes, of course you have to do that. But you can’t just <code>copy</code>, <code>boot</code>, and then <code>kill</code> the old Fly Machine. Because the original Fly Machine is still alive and writing, you have to <code>kill</code> first, then <code>copy</code>, then <code>boot</code>.</p>

<p>Fly Volumes can get pretty big. Even to a rack buddy physical server, you’ll hit a point where draining incurs minutes of interruption, especially if you’re moving lots of volumes simultaneously. <code>Kill</code>, <code>copy</code>, <code>boot</code> is too slow.</p>
<p>There’s a world where even 15 minutes of interruption is tolerable. It’s the world where you run more than one instance of your application to begin with, so prolonged interruption of a single Fly Machine isn’t visible to your users. Do this! But we have to live in the same world as our customers, many of whom don’t run in high-availability configurations.</p><h2 id="behold-the-clone-o-mat"><a href="#behold-the-clone-o-mat" aria-label="Anchor"></a><span>Behold The Clone-O-Mat</span></h2>
<p><code>Copy</code>, <code>boot</code>, <code>kill</code> loses data. <code>Kill</code>, <code>copy</code>, <code>boot</code> takes too long. What we needed is a new operation: <code>clone</code>.</p>

<p><code>Clone</code> is a lazier, asynchronous <code>copy</code>. It creates a new volume elsewhere on our fleet, just like <code>copy</code> would. But instead of blocking, waiting to transfer every byte from the original volume, <code>clone</code> returns immediately, with a transfer running in the background.</p>

<p>A new Fly Machine can be booted with that cloned volume attached. Its blocks are mostly empty. But that’s OK: when the new Fly Machine tries to read from it, the block storage system works out whether the block has been transferred. If it hasn’t, it’s fetched over the network from the original volume; this is called “hydration”. Writes are even easier, and don’t hit the network at all.</p>

<p><code>Kill</code>, <code>copy</code>, <code>boot</code> is slow. But <code>kill</code>, <code>clone</code>, <code>boot</code> is fast; it can be made asymptotically as fast as stateless migration.</p>

<p>There are three big moving pieces to this design.</p>

<ol>
<li>First, we have to rig up our OS storage system to make this <code>clone</code> operation work.
</li><li>Then, to read blocks over the network, we need a network protocol. (Spoiler: iSCSI, though we tried other stuff first.)
</li><li>Finally, the gnarliest of those pieces is our orchestration logic: what’s running where, what state is it in, and whether it’s plugged in correctly.
</li></ol>
<h2 id="block-level-clone"><a href="#block-level-clone" aria-label="Anchor"></a><span>Block-Level Clone</span></h2>
<p>The Linux feature we need to make this work already exists; <a href="https://docs.kernel.org/admin-guide/device-mapper/dm-clone.html" title="">it’s called <code>dm-clone</code></a>. Given an existing, readable storage device, <code>dm-clone</code> gives us a new device, of identical size, where reads of uninitialized blocks will pull from the original. It sounds terribly complicated, but it’s actually one of the simpler kernel lego bricks. Let’s demystify it.</p>

<p>As far as Unix is concerned, random-access storage devices, be they spinning rust or NVMe drives, are all instances of the common class “block device”. A block device is addressed in fixed-size (say, 4KiB) chunks, and <a href="https://elixir.bootlin.com/linux/v5.11.11/source/include/linux/blk_types.h#L356" title="">handles (roughly) these operations</a>:</p>
<div>
  <div>
    <pre><code id="code-u40fuubp"><span>enum</span> <span>req_opf</span> <span>{</span>
    <span>/* read sectors from the device */</span>
    <span>REQ_OP_READ</span>     <span>=</span> <span>0</span><span>,</span>
    <span>/* write sectors to the device */</span>
    <span>REQ_OP_WRITE</span>        <span>=</span> <span>1</span><span>,</span>
    <span>/* flush the volatile write cache */</span>
    <span>REQ_OP_FLUSH</span>        <span>=</span> <span>2</span><span>,</span>
    <span>/* discard sectors */</span>
    <span>REQ_OP_DISCARD</span>      <span>=</span> <span>3</span><span>,</span>
    <span>/* securely erase sectors */</span>
    <span>REQ_OP_SECURE_ERASE</span> <span>=</span> <span>5</span><span>,</span>
    <span>/* write the same sector many times */</span>
    <span>REQ_OP_WRITE_SAME</span>   <span>=</span> <span>7</span><span>,</span>
    <span>/* write the zero filled sector many times */</span>
    <span>REQ_OP_WRITE_ZEROES</span> <span>=</span> <span>9</span><span>,</span>
    <span>/* ... */</span>
<span>};</span>
</code></pre>
  </div>
</div>
<p>You can imagine designing a simple network protocol that supported all these options. It might have messages that looked something like:</p>

<p><img alt="A packet diagram, just skip down to &#34;struct bio&#34; below" src="https://elijer.github.io/blog/machine-migrations/assets/packet.png?2/3&amp;center"/>
Good news! The Linux block system is organized as if your computer was a network running a protocol that basically looks just like that. Here’s the message structure:</p>
<div><p>I’ve <a href="https://elixir.bootlin.com/linux/v5.11.11/source/include/linux/blk_types.h#L223" title="">stripped a bunch of stuff out of here</a> but you don’t need any of it to understand what’s coming next.</p>
</div><div>
  <div>
    <pre><code id="code-ui2rfcbw"><span>/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */</span>
<span>struct</span> <span>bio</span> <span>{</span>
    <span>struct</span> <span>gendisk</span>      <span>*</span><span>bi_disk</span><span>;</span>
    <span>unsigned</span> <span>int</span>        <span>bi_opf</span>
    <span>unsigned</span> <span>short</span>      <span>bi_flags</span><span>;</span>   
    <span>unsigned</span> <span>short</span>      <span>bi_ioprio</span><span>;</span>
    <span>blk_status_t</span>        <span>bi_status</span><span>;</span>
    <span>unsigned</span> <span>short</span>      <span>bi_vcnt</span><span>;</span>         <span>/* how many bio_vec&#39;s */</span>
    <span>struct</span> <span>bio_vec</span>      <span>bi_inline_vecs</span><span>[];</span> <span>/* (page, len, offset) tuples */</span>
<span>};</span>
</code></pre>
  </div>
</div>
<p>No nerd has ever looked at a fixed-format message like this without thinking about writing a proxy for it, and <code>struct bio</code> is no exception. The proxy system in the Linux kernel for <code>struct bio</code> is called <code>device mapper</code>, or DM.</p>

<p>DM target devices can plug into other DM devices. For that matter, they can do whatever the hell else they want, as long as they honor the interface. It boils down to a <code>map(bio)</code> function, which can dispatch a <code>struct bio</code>, or drop it, or muck with it and ask the kernel to resubmit it.</p>

<p>You can do a whole lot of stuff with this interface: carve a big device into a bunch of smaller ones (<a href="https://docs.kernel.org/admin-guide/device-mapper/linear.html" title=""><code>dm-linear</code></a>), make one big striped device out of a bunch of smaller ones (<a href="https://docs.kernel.org/admin-guide/device-mapper/striped.html" title=""><code>dm-stripe</code></a>), do software RAID mirroring (<code>dm-raid1</code>), create snapshots of arbitrary existing devices (<a href="https://docs.kernel.org/admin-guide/device-mapper/snapshot.html" title=""><code>dm-snap</code></a>), cryptographically verify boot devices (<a href="https://docs.kernel.org/admin-guide/device-mapper/verity.html" title=""><code>dm-verity</code></a>), and a bunch more. Device Mapper is the kernel backend for the <a href="https://sourceware.org/lvm2/" title="">userland LVM2 system</a>, which is how we do <a href="https://fly.io/blog/persistent-storage-and-fast-remote-builds/" title="">thin pools and snapshot backups</a>.</p>

<p>Which brings us to <code>dm-clone</code> : it’s a map function that boils down to:</p>
<div>
  <div>
    <pre><code id="code-ah0yd18h">    <span>/* ... */</span> 
    <span>region_nr</span> <span>=</span> <span>bio_to_region</span><span>(</span><span>clone</span><span>,</span> <span>bio</span><span>);</span>

    <span>// we have the data</span>
    <span>if</span> <span>(</span><span>dm_clone_is_region_hydrated</span><span>(</span><span>clone</span><span>-&gt;</span><span>cmd</span><span>,</span> <span>region_nr</span><span>))</span> <span>{</span>
        <span>remap_and_issue</span><span>(</span><span>clone</span><span>,</span> <span>bio</span><span>);</span>
        <span>return</span> <span>0</span><span>;</span>

    <span>// we don&#39;t and it&#39;s a read</span>
    <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>bio_data_dir</span><span>(</span><span>bio</span><span>)</span> <span>==</span> <span>READ</span><span>)</span> <span>{</span>
        <span>remap_to_source</span><span>(</span><span>clone</span><span>,</span> <span>bio</span><span>);</span>
        <span>return</span> <span>1</span><span>;</span>
    <span>}</span>

    <span>// we don&#39;t and it&#39;s a write</span>
    <span>remap_to_dest</span><span>(</span><span>clone</span><span>,</span> <span>bio</span><span>);</span>
    <span>hydrate_bio_region</span><span>(</span><span>clone</span><span>,</span> <span>bio</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
    <span>/* ... */</span> 
</code></pre>
  </div>
</div><p>a <a href="https://docs.kernel.org/admin-guide/device-mapper/kcopyd.html" title=""><code>kcopyd</code></a> thread runs in the background, rehydrating the device in addition to (and independent of) read accesses.</p>
<p><code>dm-clone</code> takes, in addition to the source device to clone from, a “metadata” device on which is stored a bitmap of the status of all the blocks: either “rehydrated” from the source, or not. That’s how it knows whether to fetch a block from the original device or the clone.</p>
<h2 id="network-clone"><a href="#network-clone" aria-label="Anchor"></a><span>Network Clone</span></h2><p><strong><code>flyd</code> in a nutshell:</strong> worker physicals run a service, <code>flyd</code>, which manages a couple of databases that are the source of truth for all the Fly Machines running there. Conceptually, <code>flyd</code> is a server for on-demand instances of durable finite state machines, each representing some operation on a Fly Machine (creation, start, stop, &amp;c), with the transition steps recorded carefully in a BoltDB database. An FSM step might be something like “assign a local IPv6 address to this interface”, or “check out a block device with the contents of this container”, and it’s straightforward to add and manage new ones.</p>
<p>Say we’ve got <code>flyd</code> managing a Fly Machine with a volume on <code>worker-xx-cdg1-1</code>. We want it running on <code>worker-xx-cdg1-2</code>. Our whole fleet is meshed with WireGuard; everything can talk directly to everything else. So, conceptually:</p>

<ol>
<li><code>flyd</code> on <code>cdg1-1</code> stops the Fly Machine, and
</li><li>sends a message to <code>flyd</code> on <code>cdg1-2</code> telling it to clone the source volume.
</li><li><code>flyd</code> on <code>cdg1-2</code> starts a <code>dm-clone</code> instance, which creates a clone volume on <code>cdg1-2</code>, populating it, over some kind of network block protocol, from <code>cdg1-1</code>, and
</li><li>boots a new Fly Machine, attached to the clone volume.
</li><li><code>flyd</code> on <code>cdg1-2</code> monitors the clone operation, and, when the clone completes, converts the clone device to a simple linear device and cleans up.
</li></ol>

<p>For step (3) to work, the “original volume” on <code>cdg1-1</code> has to be visible on <code>cdg1-2</code>, which means we need to mount it over the network.</p>
<p><code>nbd</code> is so simple that it’s used as a sort of <code>dm-user</code> userland block device; to prototype a new block device, <a href="https://lwn.net/ml/linux-kernel/20201203215859.2719888-1-palmer@dabbelt.com/" title="">don’t bother writing a kernel module</a>, just write an <code>nbd</code> server.</p>
<p>Take your pick of  protocols. iSCSI is the obvious one, but it’s relatively complicated, and Linux has native support for a much simpler one: <code>nbd</code>, the “network block device”. You could implement an <code>nbd</code> server in an afternoon, on top of a file or a SQLite database or S3, and the Linux kernel could mount it as a drive.</p>

<p>We started out using <code>nbd</code>. But we kept getting stuck <code>nbd</code> kernel threads when there was any kind of network disruption. We’re a global public cloud; network disruption  happens. Honestly, we could have debugged our way through this. But it was simpler just to spike out an iSCSI implementation, observe that didn’t get jammed up when the network hiccuped, and move on.</p>
<h2 id="putting-the-pieces-together"><a href="#putting-the-pieces-together" aria-label="Anchor"></a><span>Putting The Pieces Together</span></h2>
<p>To drain a worker with minimal downtime and no lost data, we turn workers into temporary SANs, serving the volumes we need to drain to fresh-booted replica Fly Machines on a bunch of “target” physicals. Those SANs — combinations of <code>dm-clone</code>, iSCSI, and <a href="https://fly.io/blog/carving-the-scheduler-out-of-our-orchestrator/" title="">our <code>flyd</code> orchestrator</a> — track the blocks copied from the origin, copying each one exactly once and cleaning up when the original volume has been fully copied.</p>

<p>Problem solved!</p>
<h2 id="no-there-were-more-problems"><a href="#no-there-were-more-problems" aria-label="Anchor"></a><span>No, There Were More Problems</span></h2>
<p>When your problem domain is hard, anything you build whose design you can’t fit completely in your head is going to be a fiasco. Shorter form: “if you see Raft consensus in a design, we’ve done something wrong”.</p>

<p>A virtue of this migration system is that, for as many moving pieces as it has, it fits in your head. What complexity it has is mostly shouldered by strategic bets we’ve already  built teams around, most notably the <code>flyd</code> orchestrator. So we’ve been running this system for the better part of a year without much drama. Not no drama, though. Some drama.</p>

<p>Example: we encrypt volumes. Our key management is fussy. We do per-volume encryption keys that provision alongside the volumes themselves, so no one worker has a volume skeleton key.</p>

<p>If you think “migrating those volume keys from worker to worker” is the problem I’m building up to, well, that too, but the bigger problem is <code>trim</code>.</p>

<p>Most people use just a small fraction of the volumes they allocate. A 100GiB volume with just 5MiB used wouldn’t be at all weird. You don’t want to spend minutes copying a volume that could have been fully hydrated in seconds.</p>

<p>And indeed, <code>dm-clone</code> doesn’t want to do that either. Given a source block device (for us, an iSCSI mount) and the clone device, a <code>DISCARD</code> issued on the clone device will get picked up by <code>dm-clone</code>, which will simply <a href="https://elixir.bootlin.com/linux/v5.11.11/source/drivers/md/dm-clone-target.c#L1357" title="">short-circuit the read</a> of the relevant blocks by marking them as hydrated in the metadata volume. Simple enough.</p>

<p>To make that work, we need the target worker to see the plaintext of the source volume (so that it can do an <code>fstrim</code> — don’t get us started on how annoying it is to sandbox this — to read the filesystem, identify the unused block, and issue the <code>DISCARDs</code> where <code>dm-clone</code> can see them) Easy enough.</p>
<p>these curses have a lot to do with how hard it was to drain workers!</p>
<p>Except: two different workers, for cursed reasons, might be running different versions of <a href="https://gitlab.com/cryptsetup/cryptsetup" title="">cryptsetup</a>, the userland bridge between LUKS2 and the <a href="https://docs.kernel.org/admin-guide/device-mapper/dm-crypt.html" title="">kernel dm-crypt driver</a>. There are (or were) two different versions of cryptsetup on our network, and they default to different <a href="https://fossies.org/linux/cryptsetup/docs/on-disk-format-luks2.pdf" title="">LUKS2 header sizes</a> — 4MiB and 16MiB. Implying two different plaintext volume sizes. </p>

<p>So now part of the migration FSM is an RPC call that carries metadata about the designed LUKS2 configuration for the target VM. Not something we expected to have to build, but, whatever.</p>
<p>Corrosion deserves its own post.</p>
<p>Gnarlier example: workers are the source of truth for information about the Fly Machines running on them. Migration knocks the legs out from under that constraint, which we were relying on in Corrosion, the SWIM-gossip SQLite database we use to connect Fly Machines to our request routing. Race conditions. Debugging. Design changes. Next!</p>

<p>Gnarliest example: our private networks. Recall: we automatically place every Fly Machine into <a href="https://fly.io/blog/incoming-6pn-private-networks/" title="">a private network</a>; by default, it’s the one all the other apps in your organization run in. This is super handy for setting up background services, databases, and clustered applications. 20 lines of eBPF code in our worker kernels keeps anybody from “crossing the streams”, sending packets from one private network to another.</p>
<p>we’re members of an elite cadre of idiots who have managed to find designs that made us wish IPv6 addresses were even bigger.</p>
<p>We call this scheme 6PN (for “IPv6 Private Network”). It functions by <a href="https://fly.io/blog/ipv6-wireguard-peering#ipv6-private-networking-at-fly" title="">embedding routing information directly into IPv6 addresses</a>. This is, perhaps, gross. But it allows us to route diverse private networks with constantly changing membership across a global fleet of servers without running a distributed routing protocol. As the beardy wizards who kept the Internet backbone up and running on Cisco AGS+’s once said: the best routing protocol is “static”.</p>

<p>Problem: the embedded routing information in a 6PN address refers in part to specific worker servers.</p>

<p>That’s fine, right? They’re IPv6 addresses. Nobody uses literal IPv6 addresses. Nobody uses IP addresses at all; they use the DNS. When you migrate a host, just give it a new 6PN address, and update the DNS.</p>

<p>Friends, somebody did use literal IPv6 addresses. It was us. In the configurations for Fly Postgres clusters.</p>
<p>It’s also not operationally easy for us to shell into random Fly Machines, for good reason.</p>
<p>The obvious fix for this is not complicated; given <code>flyctl</code> ssh access to a Fly Postgres cluster, it’s like a 30 second ninja edit. But we run a <em>lot</em> of Fly Postgres clusters, and the change has to be coordinated carefully to avoid getting the cluster into a confused state. We went as far as adding feature to our <code>init</code> to do network address mappings to keep old 6PN addresses reachable before biting the bullet and burning several weeks doing the direct configuration fix fleet-wide.</p>
<figure>
  <figcaption>
    
    <p>This engineering work belongs to Shaun Davis, Simon Horne, Dov Alperin, Ben Ang, Dusty Hall, Saleem Rashid, and like 9 other people I’m going to have to apologize to later.</p>
      <a href="https://fly.io/speedrun">
        They want you to try it out!  <span>→</span>
      </a>
  </figcaption>
  <p><img src="https://elijer.github.io/static/images/cta-dog.webp" srcset="/static/images/cta-dog@2x.webp 2x" alt=""/>
  </p>
</figure>

<h2 id="the-learning-it-burns"><a href="#the-learning-it-burns" aria-label="Anchor"></a><span>The Learning, It Burns!</span></h2>
<p>We get asked a lot why we don’t do storage the “obvious” way, with an <a href="https://aws.amazon.com/ebs/" title="">EBS-type</a> SAN fabric, abstracting it away from our compute. Locally-attached NVMe storage is an idiosyncratic choice, one we’ve had to write disclaimers for (single-node clusters can lose data!) since we first launched it.</p>

<p>One answer is: we’re a startup. Building SAN infrastructure in every region we operate in would be tremendously expensive. Look at any feature in AWS that normal people know the name of, like EC2, EBS, RDS, or S3 — there’s a whole company in there. We launched storage when we were just 10 people, and even at our current size we probably have nothing resembling the resources EBS gets. AWS is pretty great!</p>

<p>But another thing to keep in mind is: we’re learning as we go. And so even if we had the means to do an EBS-style SAN, we might not build it today.</p>

<p>Instead, we’re a lot more interested in log-structured virtual disks (LSVD). LSVD uses NVMe as a local cache, but durably persists writes in object storage. You get most of the performance benefit of bus-hop disk writes, along with unbounded storage and S3-grade reliability.</p>

<p><a href="https://community.fly.io/t/bottomless-s3-backed-volumes/15648" title="">We launched LSVD experimentally last year</a>; in the intervening year, something happened to make LSVD even more interesting to us: <a href="https://www.tigrisdata.com/" title="">Tigris Data</a> launched S3-compatible object storage in every one our regions, so instead of backhauling updates to Northern Virginia, <a href="https://community.fly.io/t/tigris-backed-volumes/20792" title="">we can keep them local</a>. We have more to say about LSVD, and a lot more to say about Tigris.</p>

<p>Our first several months of migrations were done gingerly. By summer of 2024, we got to where our infra team can pull “drain this host” out of their toolbelt without much ceremony.</p>

<p>We’re still not to the point where we’re migrating casually. Your Fly Machines are probably not getting migrated! There’d need to be a reason! But the dream is fully-automated luxury space migration, in which you might get migrated semiregularly, as our systems work not just to drain problematic hosts but to rebalance workloads regularly. No time soon. But we’ll get there.</p>

<p>This is the biggest thing our team has done since we replaced Nomad with flyd. Only the new billing system comes close. We did this thing not because it was easy, but because we thought it would be easy. It was not. But: worth it!</p>

          
        </section>
        <dl>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://elijer.github.io/blog/oidc-cloud-roles/">
                AWS without Access Keys
              </a>
            </dd>
        </dl>
      </article></div>
  </body>
</html>
