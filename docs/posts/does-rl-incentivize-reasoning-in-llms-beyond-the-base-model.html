<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://limit-of-rlvr.github.io/">Original</a>
    <h1>Does RL Incentivize Reasoning in LLMs Beyond the Base Model?</h1>
    
    <div id="readability-page-1" class="page">

<nav role="navigation" aria-label="main navigation">
  
</nav>


<section>
  <div>
    <div>
      <div>
        <div>
          

          <div>
              <!-- 作者列表 -->
            
          
            <!-- 机构信息 -->
            <div>
              <p><span><sup>1</sup> Tsinghua University, LeapLab</span>
              <span>  </span></p><!-- 间隔符 -->
              <p><span><sup>2</sup> Shanghai Jiao Tong University</span>
            </p></div>
          
            <!-- 贡献说明 -->
            <p><span><sup>*</sup> Equal Contribution</span>
              <span>  </span>
              <span><sup>†</sup> Project Lead</span>
              <span>  </span>
              <span><sup>‡</sup> Corresponding Author</span>
            </p>
          
            
          </div>

          
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Ad -->
    <div>
      <div>
        <p>
            <i><a href="https://yueyang130.github.io/"><b>Yang Yue</b></a> is currently focused on developing new paradigms for incentivizing LLM/MLLM reasoning, generalized world models, and exploring the generalization of VLA. 
            He is seeking active collaboration opportunities with companies that offer the freedom to explore these frontier and fundamental questions, alongside abundant resources and a strong technical atmosphere. 
            Additionally, he is seeking a Ph.D. visit. Please feel free to reach out if there is potential for collaboration.
            </i>
          </p>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Video -->
    <div>
      <div>
        <h2>
          <video autoplay="" loop="" muted="" playsinline="">
            <source src="./static/video/introvideo.mp4" type="video/mp4"/>
            <source src="./static/video/introvideo.webm" type="video/webm"/>
            Your browser does not support the video tag.
          </video>
        </h2>
        <p>
          Video: pass@<i>k</i> curves of base models and their zero-RL-trained counterparts across multiple mathematical benchmarks. </p>
      </div>
    </div>
  </div>
</section>    

<section>
  <div>
    <!-- Abstract -->
    <div>
      <div>
        <h2>Introducing Our Work</h2>
        <div>
          <p>  
            Recent breakthroughs in reasoning-focused large language models (LLMs) like OpenAI-o1, DeepSeek-R1, and Kimi-1.5 have largely relied on <i>Reinforcement Learning with Verifiable Rewards</i> (RLVR), which replaces human annotations with automated rewards (e.g., verified math solutions or passing code tests) to scale self-improvement. While RLVR enhances reasoning behaviors such as self-reflection and iterative refinement, we challenge a core assumption:  
          </p>  
          <p>  
          <i><b>Does RLVR actually expand LLMs&#39; reasoning capabilities, or does it merely optimize existing ones?</b></i>  
          </p>  
          <p>  
          By evaluating models via <i>pass@k</i>, where success requires just one correct solution among <i>k</i> attempts, we uncover that RL-trained models excel at low <i>k</i> (e.g., pass@1) but are consistently <i>outperformed by base models</i> at high <i>k</i> (e.g., pass@256). This demonstrates that RLVR <i>narrows the model&#39;s exploration</i>, favoring known high-reward paths instead of discovering new reasoning strategies. Crucially, all correct solutions from RL-trained models already exist in the base model&#39;s distribution, proving RLVR enhances <i>sampling efficiency</i>, not reasoning capacity, while inadvertently shrinking the solution space.  
          </p>
        </div>
      </div>
    </div>
    <div>
      <div>
        <video autoplay="" loop="" muted="" playsinline="">
          <source src="./static/video/overviewvideo.mp4" type="video/mp4"/>
          <source src="./static/video/overviewvideo.webm" type="video/webm"/>
          Your browser does not support the video tag.
        </video>
        <!-- <h2 class="subtitle has-text-centered">
          <img src="./static/images/overview.png"/>
        </h2> -->
        <p>
          Video: The effect of RLVR on LLM&#39;s reasoning ability. Search trees are generated by repeated sampling from the base and </p>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    <!-- Conclusion -->
    <div>
      <div>
        <h2>Conclusion</h2>
        <div>
          <ol>
            <li>
              <span><b>RL-trained models perform worse than base models in pass@<i>k</i> at large k values.</b></span> </li>
            <li>
              <span><b>RL boosts sampling efficiency but reduces the reasoning capacity boundary.</b></span> </li>
            <li>
              <span><b>RLVR algorithms perform similarly and remain far from optimal.</b></span> </li>
            <li>
              <span><b>RLVR and distillation are fundamentally different.</b></span> </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>


<section>
  <div>
    <!-- Fully Open-Source -->
    <div>
      <div>
        <h2>Experiments</h2>
        <div>
          <p>
            <span>We conducted experiments across <b>three representative domains</b> to evaluate the effect of RLVR on the reasoning ability boundaries of base and RLVR models.</span> <br/>
          </p>
          <h3>Math</h3>
          
          <p>
            In the <b>math</b> experiments, we evaluate multiple LLM families (Qwen-2.5 and LLaMA-3.1) and their RL-trained variants on benchmarks like GSM8K, MATH500, and AIME24. 
            We analyze pass@<i>k</i> curves to compare base and RL-trained models, observing that RL improves low-<i>k</i> performance but reduces problem coverage at high <i>k</i>. 
            We manually inspect CoT validity to ensure correct answers stem from valid reasoning, not lucky guesses. 
            Additionally, we examine Oat-Zero-trained models and filter guessable problems to focus on challenging cases. 
            The results show base models maintain broader reasoning coverage despite RL&#39;s initial accuracy gains. 
          </p>
          <h3>Coding</h3>
          
          <p>
            In the <b>coding</b> experiments, we evaluate the RLVR-trained model CodeR1-Zero-Qwen2.5-7B, derived from Qwen2.5-7B-Instruct-1M, on benchmarks like LiveCodeBench, HumanEval+, and MBPP+. 
            We assess performance using pass@<i>k</i> metrics, measuring correctness based on predefined test cases. 
            The results show RLVR improves single-sample pass@1 scores but reduces coverage at higher sampling counts (<i>k</i> = 128). 
            The original model exhibits continued potential for improvement with larger <i>k</i>, while RLVR&#39;s performance plateaus. 
            This indicates RLVR enhances deterministic accuracy but limits exploration diversity. 
          </p>
          <h3>Visual Reasoning</h3>
          
          <p>
            In the experiments on <b>visual reasoning</b>, we evaluate Qwen-2.5-VL-7B on filtered visual reasoning benchmarks (MathVista and MathVision), removing multiple-choice questions to focus on robust problem-solving. 
            The improvements from RLVR in visual reasoning align with those seen in math and coding benchmarks, indicating that the original model already covers a broad range of solvable problems, even in multimodal tasks.
            The consistency across domains suggests that RLVR enhances reasoning capabilities without fundamentally altering the model&#39;s problem-solving approach.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Fully Open-Source -->
    <div>
      <div>
        <h2>Case Study</h2>
        <div>
          <p>
            We present <span><i>ONE</i></span> of the sampled correct CoTs from the <span><b>base</b></span> model, 
            manually selected from 2048 samplings for the hardest questions in AIME24. 
            The responses from the base model tend to be long CoTs and exhibit reflective behavior, 
            highlighting the strong reasoning ability inherent in the base model.
          </p>
          <h3>Example</h3>
          <h2>
            <img src="https://limit-of-rlvr.github.io/static/images/AIME24_16_Base_Answer.png"/>
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div>
    <!-- Fully Open-Source -->
    <div>
      <div>
        <h2>BibTeX</h2>
        <div>
          <pre><code>@article{yue2025limit-of-rlvr,
  title={Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?},
  author={Yue, Yang and Chen, Zhiqi and Lu, Rui and Zhao, Andrew and Wang, Zhaokai and Yue, Yang and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2504.13837},
  year={2025}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>




</div>
  </body>
</html>
