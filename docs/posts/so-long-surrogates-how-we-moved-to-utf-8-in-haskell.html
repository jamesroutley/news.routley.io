<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.channable.com/tech/so-long-surrogatesa">Original</a>
    <h1>So Long Surrogates: How We Moved to UTF-8 in Haskell</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>We released a <a href="https://www.channable.com/tech/how-we-made-haskell-search-strings-as-fast-as-rust">blazingly fast Aho-Corasick implementation</a>, written in Haskell, in 2019. This implementation was based on UTF-16 strings, since Haskell&#39;s text library uses that for its internal string representation. However, the most recent major update of text changed its internal string representation from UTF-16 to UTF-8. This is good news for us, since most of our customer‚Äôs data is ASCII, this update will cut our memory consumption in half. The big problem though is that our highly optimized string search library <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> assumes that its input is encoded as UTF-16 and uses that assumption to cut a few corners to improve performance. In this post we will illustrate the challenges we encountered implementing UTF-8 support in alfred-margaret and also give some insights into how we optimized our Haskell code for maximal performance.</p>
<h2 id="unicode-terms">Unicode Terms</h2>
<p>Before we get started, we will have to specify some of the terms used in this post. As programmers, we usually use the term <em>character</em> differently than our users. Unicode provides us with terms that allow us to be more specific: <em>User-perceived characters</em> may be represented as multiple <em>abstract characters</em>. A visual example for this is the user-perceived character ‚úåüèø, consisting of the following two abstract characters:</p>
<ul>
<li><code>U+270C VICTORY HAND</code> ‚úå</li>
<li><code>U+1F3FF EMOJI MODIFIER FITZPATRICK TYPE-6</code> üèø</li>
</ul>
<p>Similarly, the user-perceived character ‚ÄúxÃ£‚Äù consists of these abstract characters:</p>
<ul>
<li><code>U+0078 LATIN SMALL LETTER X</code> x</li>
<li><code>U+0323 COMBINING DOT BELOW</code> ‚óåÃ£</li>
</ul>
<p>When writing programs, we usually work with abstract characters and leave the user-perceived characters to the user‚Äôs GUI/font rendering system (of course there are counterexamples, e.g. the <a target="_blank" rel="noop" href="https://developer.twitter.com/en/docs/counting-characters">character count of a tweet</a>). Abstract characters include simple letters, but also combining marks such as <code>U+0323 COMBINING DOT BELOW</code> above and control characters such as <code>U+000A LINE FEED (LF)*</code>.  A <em>code point</em> is a value in the Unicode code space which ranges from 0 to 0x10FFFF. Unicode 14.0.0 (from September 2021) maps 144,697 of those 1,114,112 code points to abstract characters. We will use the notation <code>U+XXXX</code> for code points, where <code>XXXX</code> is a hexadecimal number. In order to avoid confusion, we will use the term code point in this post to refer to abstract characters. For further information, you can check out the (surprisingly readable) <a target="_blank" rel="noop" href="https://www.unicode.org/versions/latest">Unicode standard</a> itself or the website/manifest <a target="_blank" rel="noop" href="https://utf8everywhere.org/">UTF-8 Everywhere</a>, which has a section discussing the different concepts one might associate with the term &#34;character&#34;.</p>
<p>Other terms defined in Unicode that we will use are:</p>
<ul>
<li>Rules for <em>case conversion</em>, mapping code points to their lowercase or uppercase version.</li>
<li>Rules for <em>case folding</em>, mapping code points to a specific case for case-insensitive comparison. Mostly but not fully congruent to lowercasing, e.g.:<ul>
<li><code>U+00DF LATIN SMALL LETTER SHARP S</code>: <code>toCasefold(√ü) = ss ‚â† √ü = toLowercase(√ü)</code></li>
<li><code>U+AB7B CHEROKEE LETTER GU</code>: <code>toCaseFold(·é´) = ·é´ ‚â† Í≠ª = toLowercase(·é´)</code></li></ul></li>
<li><em>Encoding forms</em>, the general term for UTF-8, UTF-16 and UTF-32, which map code point sequences to byte sequences.</li>
</ul>
<h2 id="moving-on-from-utf-16">Moving on from UTF-16</h2>
<p>In Channable‚Äôs Haskell codebase, we use the <a target="_blank" rel="noop" href="https://hackage.haskell.org/package/text">text</a> package for string operations. This package provides a <code>Text</code> type, which uses the UTF-16 encoding form to store Unicode strings in memory. For our use case, <code>Text</code> is much more efficient than Haskell‚Äôs String, which is a linked list of boxed 32-bit integers.</p>
<p>In its <a target="_blank" rel="noop" href="https://hackage.haskell.org/package/text-2.0/changelog">recent update to version 2.0</a>, the <code>Text</code> type‚Äôs encoding was changed to UTF-8. This change has multiple advantages for us:</p>
<ul>
<li>When we load serialized customer data (e.g. from a Postgres database or JSON/CBOR files), we have to decode its UTF-8 strings and encode them again into UTF-16 <code>Text</code> values. This is a lot of overhead for what should essentially be a no-op.</li>
<li>Most of our customer data (more than 99% of our benchmark data set used for the <a href="https://www.channable.com/tech/how-we-made-haskell-search-strings-as-fast-as-rust">first alfred-margaret blog post</a>) uses characters from the <a target="_blank" rel="noop" href="https://codepoints.net/basic_latin">Unicode block Basic Latin</a>. This block corresponds to the 128 ASCII characters and can be encoded using just a single byte per character.</li>
</ul>
<p>The second point is more important: It means that compared to UTF-16, where every code point takes at least two bytes to encode, we save a lot of memory! Consider this example:</p>
<p><img src="https://media.graphassets.com/pMAmo6tQIONCr0tNgGcF" alt="The string beeblebr√∂x, encoded using UTF-16 LE and UTF-8"/></p>
<p>The diagrams show the string &#34;Beeblebr√∂x&#34; encoded using two different encoding forms: UTF-16 and UTF-8. The lowest row of both diagrams shows the <em>code units</em> used by the encoding in form. In UTF-16, a code unit is 16 bits; in UTF-8, it&#39;s 8 bits. For UTF-16, we have to keep in mind that endianness is important since code units span two bytes. <code>Text</code> uses UTF-16 Little-Endian, meaning that the least significant byte goes first. The second row in the UTF-16 diagram shows the actual bytes of the encoded string.</p>
<p>This small example already shows an important point: Code points in the block Basic Latin (from <code>U+0000</code> to <code>U+007F</code>) only take up a single byte of memory in UTF-8 and two bytes in UTF-16. In total, &#34;Beeblebr√∂x&#34; uses 20 bytes in UTF-16 and only 11 bytes in UTF-8.</p>
<p><code>U+00F6 LATIN SMALL LETTER O WITH DIAERESIS</code> (the letter √∂) is part of the second most used block <a target="_blank" rel="noop" href="https://codepoints.net/latin-1_supplement">Latin-1 Supplement</a> (from <code>U+0080</code> to <code>U+00FF</code>). UTF-8 encodes this block as two bytes meaning we don‚Äôt gain or lose anything here compared to UTF-16. Less than 0.1% of the benchmark data set needs more bytes using UTF-8 than using UTF-16. Compared to the memory saved for Basic Latin code points, their contribution is tiny. In total, our benchmark data set weighs 3.08 GiB using UTF-16 and 1.55 GiB using UTF-8 (this excludes the 16 bytes used to store offset and length of the byte slice in each case). This means we save almost exactly 50% of memory for these strings by simply using another encoding.</p>
<h2 id="variable-width-encoding">Variable-width Encoding</h2>
<p>When working with <code>Text</code> values, we usually just import the <code>Data.Text</code> module. This module acts as an interface which hides implementation details such as the encoding form and provides a view onto the string as a sequence of <code>Char</code>, which is Haskell&#39;s built-in Unicode code point type. The upside of this abstraction is that it makes it easy for us to update to <code>text-2.0</code> even though the underlying encoding changes from UTF-16 to UTF-8.</p>
<p>However, using this interface exclusively also has a downside: If the encoding used for these <code>Char</code>s is variable-width, meaning that not all <code>Char</code>s encode to the same number of bytes, random access into a <code>Text</code> has a linear complexity. This example shows that both UTF-16 and UTF-8 are variable-width encodings:</p>
<p><img src="https://media.graphassets.com/eB0WsU7fQ4G4xy7m8bdq" alt="The string hello√ó‚úåüèø, encoded using UTF-16 LE and UTF-8"/></p>
<p>In UTF-16, all valid code points in the range <code>U+0000</code> to <code>U+FFFF</code> are encoded as a single code unit consisting of two bytes. Code points outside that range such as the last one, <code>U+1F3FF EMOJI MODIFIER FITZPATRICK TYPE-6</code>, need two code units and therefore take up four bytes. In UTF-8, a code point can be encoded as one, two, three or four single-byte code units, e.g. <code>U+006F LATIN SMALL LETTER O</code>, <code>U+00D7 MULTIPLICATION SIGN</code>, <code>U+270C VICTORY HAND</code> and <code>U+1F3FF EMOJI MODIFIER FITZPATRICK TYPE-6</code>, respectively.</p>
<p>Regardless of whether we use UTF-8 or UTF-16, in order to find the nth code point in a string, we need to do a linear search from its start. For example, in order to find out that the 8th code point in the UTF-8 string begins at byte 10, we need to iterate through &#34;hello√ó&#34; first and count the number of bytes for each code point.</p>
<h2 id="interlude-aho-corasick">Interlude: Aho-Corasick</h2>
<p>In a <a href="https://www.channable.com/tech/how-we-made-haskell-search-strings-as-fast-as-rust">previous article</a>, we discuss our fast Haskell implementation of the Aho-Corasick string search algorithm. If you‚Äôve already read that article, you can skip this section.</p>
<p>In order to conceptualize string search, we use the &#34;Needle in a Haystack&#34; idiom. We are searching for all occurrences of one or more strings (the <em>needles</em>) in another string (the <em>haystack</em>). The use case where Aho-Corasick shines is searching different haystacks for the same set of multiple needles of which the matches possibly overlap. In order to do this, we construct a finite state automaton where each state represents the progress we‚Äôve made to a full match of one of the needles. Feeding this automaton the input string code point by code point is faster than simply searching for each needle separately because we only need to scrutinize each code point of the haystack once.</p>
<p>Consider this example matching the needles &#34;shirts&#34;, &#34;shorts&#34; and &#34;tshirt&#34;:</p>
<p><img src="https://media.graphassets.com/ACjZ6EwXR12VouEErPbg" alt="An Aho-Corasick automaton matching the needles shirts, shorts and tshirt"/></p>
<p>Suppose our haystack is &#34;tshorts&#34;. Reading the code points &#34;t&#34;, &#34;s&#34; and &#34;h&#34;, we reach state 3. Because the next code point, &#34;o&#34;, doesn‚Äôt match any transitions from state 3, we fall back to state 8, where there is a transition for &#34;o&#34;, leading us to state 13. Reading the remaining code points &#34;r&#34;, &#34;t&#34; and &#34;s&#34; we end up in state 16, having reached a match for the needle &#34;shorts&#34;. This small example demonstrates that Aho-Corasick enables us to switch between needles without starting the search over. For the following sections, we assume that the code points used for transitions are in lowercase.</p>
<h2 id="utf-16-in-alfred-margaret">UTF-16 in alfred-margaret</h2>
<p>For our implementation of the Aho-Corasick algorithm in <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> we went past the external <code>Char</code>-based <code>Text</code> interface in order to be able to iterate through the input efficiently. Instead, we accessed the underlying UTF-16 encoded byte array directly, viewing it as a 16-bit code unit sequence.</p>
<p><img src="https://media.graphassets.com/7qjzbeXS0eXGpDZnPO1E" alt=""/></p>
<p>For an exact string search, comparing strings on the code unit level is sufficient. However, in order to power all necessary features in Channable, <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> provides functionality to not only match strings exactly, but also case-insensitively. A common approach for this type of problem is to simply run the case-sensitive search algorithm on a lowercase or case folded copy of the input. As we don‚Äôt want to keep what is essentially two copies of the same string in memory, our implementation lowercases each code point individually while looping through the input.</p>
<p>In order to do this, we first need to decode code points from our code unit sequence, because Unicode case conversions and case foldings are defined on code points, not code units. In UTF-16, the first 2<sup>16</sup> code points are encoded as a single 16-bit code unit containing the code point itself, shown on the left side of this diagram:</p>
<p><img src="https://media.graphassets.com/TLYHo94RQWeqULFXWTdL" alt="Visualisation of the UTF-16 encoding form"/></p>
<p>Other code points are encoded as two code units called a <em>surrogate pair</em>, shown on the right side. In order to tell apart a code unit that encodes a code point and the beginning of a surrogate pair (the <em>high surrogate</em>), Unicode reserves a range of code points that are to be used for surrogates only, namely <code>U+D800</code> to <code>U+DFFF</code> (in other words, the code units with the prefix <code>0b11011</code>). Surrogates may never appear outside a surrogate pair: A high surrogate must always be followed by a <em>low surrogate</em>. As an interesting aside, this means that UTF-16 can not actually encode all Unicode code points, while UTF-8 and UTF-32 can. However, such Unicode strings are considered <em>ill-formed</em> (see for example <a target="_blank" rel="noop" href="https://www.unicode.org/versions/latest">Unicode</a> Section 3.9 D92).</p>
<p>A very useful property of UTF-16 is the fact that the code points which can be encoded as a single code unit (the first 2<sup>16</sup> code points) coincide with the <a target="_blank" rel="noop" href="https://codepoints.net/basic_multilingual_plane">Basic Multilingual Plane</a> (BMP, from <code>U+0000</code> to <code>U+FFFF</code>) of Unicode. According to the introduction of the Unicode standard, the BMP encodes the &#34;majority of the common characters used in the major languages of the world&#34;. Since the bulk of our customers&#39; data falls into the BMP, we can get away with ignoring surrogates and lowercasing BMP code points only. This saves us the overhead of decoding multiple code units at a time. While it may seem like a questionable hack, the Unicode standard actually acknowledges this approach in Section 5.4 &#34;Handling Surrogate Pairs in UTF-16&#34;:</p>
<blockquote>
  <p>UTF-16 enjoys a beneficial frequency distribution in that, for the majority of all text data, surrogate pairs will be very rare; non-surrogate code points, by contrast, will be very common. Not only does this help to limit the performance penalty incurred when handling a variable-width encoding, but it also allows many processes either to take no specific action for surrogates or to handle surrogate pairs with existing mechanisms that are already needed to handle character sequences.</p>
</blockquote>
<p>Reflecting these facts we chose to treat the input as a sequence of fixed-width 16-bit code <em>points</em>, skipping surrogates. Consider how we would step the Aho-Corasick automation using as input the code unit sequence of the string &#34;FO√ñBARüôà&#34;:</p>
<p><img src="https://media.graphassets.com/ZJfDLaOQxCyp5GFWLmHA" alt="A showcase of the way alfred-margaret lowercases UTF-16 strings"/></p>
<p>For each code unit, we determine whether it is a surrogate by checking if it is in the range <code>U+D800</code> to <code>U+DFFF</code>. If it isn‚Äôt, we interpret it as a code point and use its lowercase version to step the automaton (black arrows). For example, <code>U+0046 LATIN CAPITAL LETTER F</code> lowercases to <code>U+0066 LATIN SMALL LETTER F</code>. If the code unit is a surrogate, as in the case of the two code units <code>U+D83D</code> and <code>U+DB48</code> constituting <code>U+1F648 SEE-NO-EVIL MONKEY</code>, we simply use it to step the automaton directly (gray arrows).</p>
<p>While the point of this post is not to explain our Aho-Corasick implementation, this should give you some context for the considerations we made when we updated <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> to work with UTF-8 strings. As opposed to most of our codebase, where we view a <code>Text</code> value as an opaque sequence of <code>Char</code>s, alfred-margaret depends on the encoding used for the internal byte slice.</p>
<h2 id="on-the-fly-lowercasing-in-utf-8">On-the-fly Lowercasing in UTF-8</h2>
<p>So what‚Äôs the problem with switching to UTF-8? In a nutshell, using UTF-8 means that we can‚Äôt treat the input as a sequence of fixed-width BMP code points any more. Consider this example, where only code points consisting of a single code unit are converted to lowercase in the UTF-8-encoded string &#34;FO√ñBARüôà&#34;:</p>
<p><img src="https://media.graphassets.com/nfjtPISwuoxEGLASZfgg" alt="A showcase of the way alfred-margaret does not lowercase UTF-8 strings"/></p>
<p>This figure illustrates what happens if we just reuse the UTF-16 approach. In the case of UTF-8, only Basic Latin code points are encoded as a single code unit. While we can convert these, for any other code points such as <code>U+00D6 LATIN CAPITAL LETTER O WITH DIAERESIS</code> above, we can‚Äôt, since it is encoded using two code units. At least on the BMP, the UTF-8 version should have the same lowercasing behavior as the UTF-16 version in order not to break compatibility. With UTF-16, BMP code points would always be encoded as two bytes; with UTF-8, BMP code points may be encoded as one, two or three bytes. This means that in order to stay compatible, we need to actually decode the UTF-8 sequence in order to lowercase code points on the fly.</p>
<h2 id="decoding-the-bmp-in-utf-8">Decoding the BMP in UTF-8</h2>
<p>If we want to stay compatible with the UTF-16 version, we need to lowercase BMP code points and leave any other code points as they are. Let&#39;s take a look at how the BMP is encoded in UTF-8.</p>
<p><img src="https://media.graphassets.com/3VqgQMS0TtKqQp8W7kW0" alt="Table 3-6 of the Unicode standard, visualisation of the UTF-8 encoding form"/></p>
<p>The first column shows the binary representation of a 21-bit code point in a specific range. We can read the table as:</p>
<ul>
<li>Code points in the range [0, 2<sup>7</sup>) are encoded as a single code unit.</li>
<li>Code points in the range [2<sup>7</sup>, 2<sup>11</sup>) are encoded as two code units.</li>
<li>Code points in the range [2<sup>11</sup>, 2<sup>16</sup>) are encoded as three code units.</li>
<li>Code points in the range [2<sup>16</sup>, 2<sup>21</sup>) are encoded as four code units.</li>
</ul>
<p>How exactly the code units are composed is not relevant to our use case. What‚Äôs important is that, in order to know how many code units we need to read in order to read a full code point, we only need to scrutinize the first code unit of a code unit sequence:</p>
<p><img src="https://media.graphassets.com/36wVKCwLSnWvmTagT4R6" alt="UTF-8 encoding example"/></p>
<p>Consider the UTF-8 encoding of the string &#34;F√ñ‚úìüôà&#34;:</p>
<ul>
<li>The MSB of the first code unit <code>0x46</code> is <code>0</code>. This means that the code point at position 0, <code>U+0046 LATIN CAPITAL LETTER F</code>, is one code unit long.</li>
<li>The MSBs of the code unit <code>C3</code> at position 1 are <code>110</code> ‚áù 2 code units.</li>
<li>The MSBs of the code unit <code>E2</code> at position 3 are <code>1110</code> ‚áù 3 code units.</li>
<li>The MSBs of the code unit <code>F0</code> at position 6 are <code>11110</code> ‚áù 4 code units.</li>
</ul>
<p>Code units that are not at the beginning of a code point always have the MSBs <code>10</code>, which means that it is trivial to skip to the previous or next code point from any position in a UTF-8 encoded string. Ultimately we need to decode any code points consisting of up to three code units in order to decode all BMP code points.</p>
<h2 id="alternative-change-automaton-construction">Alternative: Change Automaton Construction</h2>
<p>Knowing all this, we assumed that decoding every input code point would add a lot of processing overhead. In order to avoid this, we tried to change the way the Aho-Corasick automaton is built instead. Rather than lowercasing code points on the fly, we would add two transitions for each needle code point to the automaton: One for its lowercase version and another for its uppercase version. The advantages of this idea are:</p>
<ul>
<li>Most of the changes would be in the automaton construction code. Compared to the heavily optimized automaton stepping code, there is a smaller risk of breaking things in this part of the code.</li>
<li>By building an automaton specifically for case-insensitive matching, we could even simplify the automaton stepping code quite a bit because it wouldn‚Äôt need to lowercase the input characters any more.</li>
</ul>
<p>The following example illustrates this using the needle &#34;Gr√∂√üe&#34; (the German word for size, appearing quite often in the context of E-Commerce). Encoded using UTF-8, it becomes the code unit sequence <code>0x47 0x72 0xC3 0xB6 0xC3 0x9F 0x65</code>. This Aho-Corasick automaton matches that sequence:</p>
<p><img src="https://media.graphassets.com/dFAdosrKQi28Rv7mwUMj" alt=""/></p>
<p>Whenever we read a character without a designated transition at any state, we fall back into state 0. To show how we can extend the automaton, consider transitioning from state 2 to state 4 in this automaton. This transition means &#34;we have read the code point √∂ (code units <code>0xC3 0xB6</code>)&#34;. We can extend this automaton by adding extra transitions and intermediate states where necessary to make it mean &#34;we have read either the code point √∂ (<code>0xC3 0xB6</code>) or the code point √ñ (<code>0xC3 0x96</code>)&#34;:</p>
<p><img src="https://media.graphassets.com/ogTQvzZnSPS4kICuGA0G" alt=""/></p>
<p>We could now use the same stepping code we used for the previous automaton to match our needles case-insensitively, removing the on-the-fly lowercasing code entirely. This can‚Äôt be implemented in a way that stays compatible with the old BMP-based solution though. The reason is that for any given <code>c :: Char</code>, the property</p>
<pre><code>   (<span>isUpper</span> c || toUpper (<span>toLower</span> c) == c)
&amp;&amp; (<span>isLower</span> c || toLower (<span>toUpper</span> c) == c)</code></pre>
<p>doesn‚Äôt necessarily hold. Here are a few examples:</p>
<ul>
<li>From our &#34;Gr√∂√üe&#34; example: While the property holds for the Basic Latin code points as well as &#34;√ñ&#34; and &#34;√∂&#34;, it doesn‚Äôt for &#34;·∫û&#34; (<code>U+1E9E LATIN CAPITAL LETTER SHARP S</code>, the uppercase &#34;Eszett&#34;): <code>Char.toLower &#39;·∫û&#39; == &#39;√ü&#39;</code> (<code>U+00DF LATIN SMALL LETTER SHARP S</code>) but <code>Char.toUpper &#39;√ü&#39; == &#39;√ü&#39;</code> again. This means that the UTF-16 version would match &#34;GR√ñ·∫ûE&#34;, but the new approach would not<sup id="link-sharp-s"><a href="#footnote-sharp-s">3</a></sup>.</li>
<li>Another infamous example is &#34;ƒ∞&#34; (<code>U+0130 LATIN CAPITAL LETTER I WITH DOT ABOVE</code>). In Haskell, <code>Char.toLower &#39;ƒ∞&#39; == &#39;i&#39;</code> (<code>U+0069 LATIN SMALL LETTER I</code>) and <code>Char.toUpper &#39;i&#39; == &#39;I&#39;</code> (<code>U+0049 LATIN CAPITAL LETTER I</code>). The letter is also one of the few case conversions that are locale-sensitive; in a turkish locale, <code>Char.toUpper &#39;i&#39; == &#39;ƒ∞&#39;</code> holds, breaking the property for &#34;I&#34;.</li>
<li>Some others, e.g. &#34;‚Ñ™&#34; (<code>U+212A KELVIN SIGN</code>) to &#34;k&#34; (<code>U+006B LATIN SMALL LETTER K</code>) to &#34;K&#34; (<code>U+004B LATIN CAPITAL LETTER K</code>). In total, there are around 30 such code points in Unicode.</li>
</ul>
<p>An avid reader and Unicode buff will be pulling their hair out by now. That&#39;s because the official way to do case-insensitive string comparisons is to apply Unicode <em>case folding</em> to both the needles and the haystack instead of applying a <em>case conversion</em> to a specific case. There are a few reasons <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> does not do this:</p>
<ul>
<li>Case folding is not a <code>Char -&gt; Char</code> mapping but <code>Char -&gt; [Char]</code> mapping. This makes it a bit harder to construct the automaton since it adds a few edge cases.</li>
<li>Moving from lowercasing to case folding now would break the API in subtle ways.</li>
</ul>
<p>Unicode defines a simple case folding algorithm that <em>is</em> a <code>Char -&gt; Char</code> mapping solving at least the first problem. It is also mostly compatible with lowercasing. However, Haskell&#39;s <code>base</code> package does not implement it. In an internal Hackathon, we hacked together an implementation but didn&#39;t have enough time to polish it to a point where it&#39;s usable.</p>
<p>Finally we discarded this approach. While nice in theory, the subtle bugs it certainly would‚Äôve caused down the road weren‚Äôt worth it in practice. However, we will probably examine this again at some point in the future.</p>
<h2 id="stepping-the-automaton-using-code-points">Stepping the Automaton Using Code Points</h2>
<p>At this point we caved and decided to just decode the UTF-8 input into code points for lowercasing. In order for this to work we‚Äôd have to change the automaton construction: So far, a single transition in our Aho-Corasick state machine has the following representation in memory using a 64-bit unsigned integer:</p>
<p><img src="https://media.graphassets.com/tB6otKpRSOBI63C4yDbY" alt=""/></p>
<p>Each transition has a code unit that it activates on as well as the state it leads to. When the wildcard bit is set, the transition activates on every code unit. Each state is represented by an array of these transitions. The last transition in this array always has the wildcard bit set and leads to a fallback state. To illustrate this, let&#39;s loop back to the &#34;shirts&#34;, &#34;shorts&#34; and &#34;tshirts&#34; example:</p>
<p><img src="https://media.graphassets.com/ACjZ6EwXR12VouEErPbg" alt=""/></p>
<p>Using the layout above, states 3 and 8 would be represented like this:</p>
<p><img src="https://media.graphassets.com/cpEiL2kBRG6PZtcofSvN" alt=""/></p>
<p>State 3 has two transitions: Transition 0 leads to state <code>0b100</code> (= 4) with code unit <code>0b1101001</code> (= &#34;i&#34;). Transition 1 is a wildcard transition that falls back into state <code>0b1000</code> (= 8). State 8 has three transitions: Transitions 2 and 3 leads to states 9 and 13 on the code units corresponding to &#34;i&#34; and &#34;o&#34; respectively, and transition 4 is a wildcard transition that back into state 0.</p>
<p>Because the UTF-8 code uses code points rather than code units for transitions, we had to change this layout just a little bit, increasing the number of bits available for the automaton input:</p>
<p><img src="https://media.graphassets.com/dM8gqTwSQGOB2R9ZYbU3" alt=""/></p>
<p>Other than that, the layout stays the same. In Haskell, the code that handles transitions depends on a few type aliases:</p>
<pre><code><span><span>type</span> <span>Transition</span> = <span>Word64</span></span>
<span><span>type</span> <span>State</span>      = <span>Word32</span></span>
<span><span>type</span> <span>CodeUnit</span>   = <span>Word16</span></span>
<span><span>type</span> <span>CodePoint</span>  = <span>Char</span></span></code></pre>
<p>Those make the following function definitions a bit nicer to read. To work with transitions, we exclusively use these functions:</p>
<pre><code><span>transitionCodeUnit</span>    :: <span>Transition</span> -&gt; <span>CodeUnit</span>
<span>transitionState</span>       :: <span>Transition</span> -&gt; <span>State</span>
<span>transitionIsWildcard</span>  :: <span>Transition</span> -&gt; <span>Bool</span>
<span>newTransition</span>         :: <span>CodeUnit</span> -&gt; <span>State</span> -&gt; <span>Transition</span>
<span>newWildcardTransition</span> :: <span>State</span> -&gt; <span>Transition</span></code></pre>
<p>This means that changing the layout of the transitions is purely a matter of updating their implementation and changing <code>CodeUnit</code> to <code>CodePoint</code>. GHC helps us find any other places where we need to change the code by pointing out type errors where <code>CodePoint</code> is to be expected according to the updated interface but <code>CodeUnit</code> is given. Working backwards from there we updated the remaining code. Finally, we decided to handle code points encoded as 4 code units correctly instead of staying 100% true to the UTF-16 implementation, which ignored those since they were encoded using surrogates. In practice this doesn‚Äôt make much of a difference since most of our input data is in the BMP anyways.</p>
<h2 id="haskell-performance-tuning">Haskell Performance Tuning</h2>
<p>By now it should be clear that we care a lot about the performance of <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a>. Optimizing Haskell code can be a little tricky, so we added this section to show some of the things that you should watch out for.</p>
<h3 id="enforce-strictness-to-improve-workerwrapper-results">Enforce Strictness to Improve Worker/Wrapper Results</h3>
<p>Laziness is a neat tool for writing high-level code while also saving a few CPU cycles here and there by avoiding unnecessary evaluation. However, it can also get in your way if you‚Äôre not careful, as it can keep the compiler from applying specific optimizations. Let‚Äôs take a look at <code>consumeInput</code>, the entry point of the automaton stepping code<sup id="link-codeunitindex"><a href="#footnote-codeunitindex">4</a></sup>:</p>
<pre><code><span>consumeInput</span> :: <span>CodeUnitIndex</span> -&gt; <span>CodeUnitIndex</span> -&gt; a -&gt; <span>State</span> -&gt; a
<span>consumeInput</span> !_offset <span>0</span> !acc !_state = acc
<span>consumeInput</span> !offset !remaining !acc !state =
  <span>let</span>
    ...
  <span>in</span>
    followCodePoint (offset + codeUnits) (remaining - codeUnits) acc state casedCodePoint</code></pre>
<p>Each parameter of this function is prefixed by an exclamation point. This syntax, enabled by the extension <em>bang patterns</em>, effectively allows us to give GHC a helping hand during <a target="_blank" rel="noop" href="https://ghc.gitlab.haskell.org/ghc/doc/users_guide/using-optimisation.html#ghc-flag--fstrictness">demand analysis</a>. Demand analysis (a GHC-specific term that includes strictness analysis) determines how a binding is used after its definition. Consider the following example:</p>
<pre><code><span>f</span> :: <span>Int</span> -&gt; <span>Int</span> -&gt; <span>Int</span>
<span>f</span> x y = <span>case</span> x <span>of</span>
          <span>0</span> -&gt; y
          _ -&gt; x</code></pre>
<p>In <code>f</code>, the function parameter <code>x</code> is always evaluated as it needs to be pattern-matched on. We call <code>x</code> a strict parameter. Meanwhile <code>y</code> is not used in every code path, making it a lazy parameter. Adding a bang pattern to <code>y</code> forces GHC to evaluate <code>y</code> whenever <code>f</code> is entered, which means it will be treated as strict in any case. While GHC&#39;s demand analysis handles most cases quite well it is sometimes necessary to add these annotations manually when working with performance-sensitive code. You can find more information of demand analysis in the <a target="_blank" rel="noop" href="https://gitlab.haskell.org/ghc/ghc/-/wikis/commentary/compiler/demand?version_id=b119a5760c88ef788475216865f7cce6e48ddc5d">GHC commentary</a> and this <a target="_blank" rel="noop" href="https://fixpt.de/blog/2017-12-04-strictness-analysis-part-1.html">example-driven blog article</a>.</p>
<p>Demand analysis is interesting to us because its results drive some optimizations in GHC, most notably the so-called <em>worker/wrapper transformation</em>. This transformation splits a function into a worker function that has an improved type and a wrapper function that observes the original type. In particular, the wrapper typically unboxes boxed arguments in order to reduce allocations and memory accesses.</p>
<p>Why is this so important if we are working with 32-bit <code>Char</code>s anyways? The reason is that in GHC&#39;s implementation of Haskell, even simple values such as <code>Char</code>s, <code>Int</code>s or <code>Bool</code>s are per default &#34;boxed&#34;, i.e. they are pointers to some data on the heap. For example, in this <code>square</code> function, x is actually a pointer to a heap-residing <code>Int</code> object:</p>
<pre><code><span>square</span> :: <span>Int</span> -&gt; <span>Int</span>
<span>square</span> x = x * x</code></pre>
<p>This means that this function has to (at least) dereference <code>x</code>, perform the multiplication and allocate a new heap object for the result. The worker-wrapper transformation saves us here by splitting this function into something akin to:</p>
<pre><code>
<span>square</span> :: <span>Int</span> -&gt; <span>Int</span>
<span>square</span> (<span>I</span># x) = <span>I</span># ($wsquare x)


$wsquare# :: <span>Int</span># -&gt; <span>Int</span>#
$wsquare# x = x *# x</code></pre>
<p>The worker function <code>$wsquare</code> now uses unboxed types and operations only (those marked with <code>#</code>) and performs no allocations. In the wrapper, you can read the pattern match <code>(I# x)</code> as a dereference to an <code>Int#</code> and the call to <code>I#</code> as an allocation on the heap. Wherever the unboxed arguments to <code>square</code> are available in the remaining codebase, GHC can now simply substitute for <code>$wsquare</code> to minimize intermediate allocations.</p>
<p>Let‚Äôs backtrack to <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a> now and check out the real-world implications. What happens to <code>consumeInput</code> when we leave GHC‚Äôs demand analysis to its own devices and remove some of those bang patterns?</p>
<pre><code><span>consumeInput</span> :: <span>CodeUnitIndex</span> -&gt; <span>CodeUnitIndex</span> -&gt; a -&gt; <span>State</span> -&gt; a
<span>consumeInput</span> _offset <span>0</span> acc _state = acc
<span>consumeInput</span> !offset !remaining !acc !state =
  <span>let</span>
    ...
  <span>in</span>
    followCodePoint (offset + codeUnits) (remaining - codeUnits) acc state casedCodePoint</code></pre>
<p>In this piece of code, the bang patterns in the first branch of <code>consumeInput</code> have been removed. We can make GHC dump the results of demand analysis by passing the <code>-ddump-stranal</code> option. GHC then prints out an intermediate representation of our program in the <a target="_blank" rel="noop" href="https://www.youtube.com/watch?v=uR_VzYxvbxg">Core language</a>, which is a glorified lambda calculus that it uses for optimizations specific to functional languages. In the dumped Core, the declaration of <code>consumeInput</code>&#39;s worker looks like this:</p>
<pre><code><span>consumeInput</span> [...]
  :: <span>Int</span> -&gt; <span>Int</span> -&gt; a -&gt; <span>Int</span> -&gt; a
[...
 <span>Str</span>=&lt;<span>L</span>,_&gt;&lt;<span>S</span>,_&gt;&lt;<span>S</span>,_&gt;&lt;<span>L</span>,_&gt;,
 ...]</code></pre>
<p><code>CodeUnitIndex</code> and <code>State</code> have been resolved to <code>Int</code>. The <code>Str</code> field in <code>consumeInput</code>&#39;s metadata gives us the results of demand analysis: In order, it lists strictness and usage information (which is not important for our case and therefore elided) for each of the function&#39;s parameters. While <code>remaining</code> and <code>acc</code> are used in both branches of <code>consumeInput</code>, <code>offset</code> and <code>state</code> are not and are therefore treated as lazy parameters. To see the results of the worker/wrapper transformation, we use <code>-ddump-worker-wrapper</code>. The worker for consumeInput looks like this:</p>
<pre><code>$wconsumeInput [...]
  :: <span>Int</span> -&gt; <span>Int</span># -&gt; a -&gt; <span>Int</span> -&gt; a</code></pre>
<p>Because <code>offset</code> and <code>state</code> are lazily used, they must always be provided as boxed <code>Int</code>s even to the worker function. <code>consumeInput</code> is called with a different <code>offset</code> for <em>each code point in the input</em>, so this implies <strong>a lot</strong> of small allocations, which is terrible for garbage collection! For this reason, we added a bang pattern to all patterns in the first branch as well:</p>
<pre><code><span>consumeInput</span> :: <span>CodeUnitIndex</span> -&gt; <span>CodeUnitIndex</span> -&gt; a -&gt; <span>State</span> -&gt; a
<span>consumeInput</span> !_offset <span>0</span> !acc !_state = acc
<span>consumeInput</span> !offset !remaining !acc !state =
  ...</code></pre>
<p>The demand information and the worker declaration now look like this:</p>
<pre><code>
<span>consumeInput</span> [...]
  :: <span>Int</span> -&gt; <span>Int</span> -&gt; a -&gt; <span>Int</span> -&gt; a
[...
 <span>Str</span>=&lt;<span>S</span>,_&gt;&lt;<span>S</span>,_&gt;&lt;<span>S</span>,_&gt;&lt;<span>S</span>,_&gt;,
 ...]


$wconsumeInput [...]
  :: <span>Int</span># -&gt; <span>Int</span># -&gt; a -&gt; <span>Int</span># -&gt; a</code></pre>
<p>Because they are dubbed to be strict parameters, <code>offset</code> and <code>state</code> are unboxed <code>Int#</code>s now as well! Overall, this three-character change cut our benchmark times in half and saved loads of allocations.</p>
<h3 id="use-tail-recursion-where-possible">Use Tail Recursion where Possible</h3>
<p>Basic functional programming courses often place a lot of emphasis on <a target="_blank" rel="noop" href="https://en.wikipedia.org/wiki/Tail_call">tail recursion</a> ‚Äì and for a good reason: Tail recursive function calls can be compiled into jumps, thereby avoiding the overhead that usually comes with a call. This is already described in the <a href="https://www.channable.com/tech/how-we-made-haskell-search-strings-as-fast-as-rust">original alfred-margaret blog article</a> in the section &#34;From State Machines to Mutual Recursion&#34;.</p>
<h3 id="mind-your-data-structures">Mind your Data Structures</h3>
<p>Finally, we did another low-level optimization that had a surprisingly big impact. We represent a Aho-Corasick state machine using the following data structure:</p>
<pre><code><span><span>data</span> <span>AcMachine</span> v = <span>AcMachine</span></span>
  { machineValues               :: !(<span>Vector</span> [v])
  
  , machineTransitions          :: !(<span>UnboxedVector</span> <span>Transition</span>)
  , machineOffsets              :: !(<span>UnboxedVector</span> <span>Int</span>)
  , machineRootAsciiTransitions :: !(<span>UnboxedVector</span> <span>Transition</span>)
  } <span>deriving</span> (<span>Generic</span>)</code></pre>
<p>An <code>UnboxedVector</code> (canonical name: <code>Data.Vector.Unboxed.Vector</code>) is an array-based data structure that doesn&#39;t store pointers to its elements but the elements itself. In C, we&#39;d declare <code>uint64_t machine_transitions[]</code>. Using unboxed vectors provides a way to store those transitions from before neatly packed adjacently in memory. However, on a hunch, we replaced them by a thin wrapper around <code>ByteArray</code> from the <code>primitive</code> package:</p>
<pre><code><span><span>newtype</span> <span>TypedByteArray</span> a = <span>TypedByteArray</span> <span>ByteArray</span></span></code></pre>
<p>As it turns out, this simple change led to a speedup of around 15% for array reads:</p>
<p><img src="https://media.graphassets.com/eXqiCDfFTcmNivP3wQRi" alt="Results of the UnboxedVector vs TypedByteArray microbenchmark"/></p>
<p>We can see why by checking out the definition of <code>UnboxedVector</code>:</p>
<pre><code><span><span>data</span> <span>Vector</span> a = <span>Vector</span> <span>Int</span> <span>Int</span> <span>ByteArray</span></span></code></pre>
<p>An <code>UnboxedVector</code> is actually just a slice into a <code>ByteArray</code>! This means that every time we read an element, we save an addition by simply using a <code>ByteArray</code> instead. Since we know that our fields are not slices but occupy a whole <code>ByteArray</code> which we don‚Äôt need to resize, this is a sensible choice. The <code>TypedByteArray</code> wrapper made it easy to replace the type where necessary:</p>
<pre><code><span><span>data</span> <span>AcMachine</span> v = <span>AcMachine</span></span>
  { machineValues               :: !(<span>Vector</span> [v])
  , machineTransitions          :: !(<span>TypedByteArray</span> <span>Transition</span>)
  , machineOffsets              :: !(<span>TypedByteArray</span> <span>Int</span>)
  , machineRootAsciiTransitions :: !(<span>TypedByteArray</span> <span>Transition</span>)
  } <span>deriving</span> (<span>Generic</span>)</code></pre>
<p>In exchange for just a small amount of additional code, this gave us a noticeable performance boost not only in the microbenchmark above but also across our whole benchmark data set.</p>
<h2 id="results">Results</h2>
<p>By using UTF-8 in our updated version of <a target="_blank" rel="noop" href="https://github.com/channable/alfred-margaret">alfred-margaret</a>, we now save close to 50% of memory when working with <code>Text</code> values. However, it was also important to preserve the library&#39;s performance. To measure this we ran three versions of alfred-margaret on our benchmark data set of roughly 1.6 billion code points:</p>
<ul>
<li>UTF-16 uvec: Original implementation which uses UTF-16 <code>Text</code> and <code>UnboxedVector</code>s.</li>
<li>UTF-8 tba: UTF-8 implementation which decodes all code points and uses the <code>TypedByteArray</code> improvement.</li>
<li>UTF-16 tba: Updated UTF-16 implementation that incorporates the <code>ByteArray</code> improvement.</li>
</ul>
<p>The task is the same for each version: Across 242 test cases, count the number of occurrences for a set of needles in a haystack. Of course we made sure that all versions deliver the same results. The distribution of code points across the whole benchmark looks like this:</p>
<table>
<thead>
<tr>
<th id="unicode_block">Unicode Block</th>

<th id="number_of_code_points">Number of Code Points</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic Latin</td>
<td>99.83%</td>
<td>1,652,853,274</td>
</tr>
<tr>
<td>Latin-1 Supplement</td>
<td>0.14%</td>
<td>2,314,640</td>
</tr>
<tr>
<td>Latin Extended-A</td>
<td>0.01%</td>
<td>242,392</td>
</tr>
<tr>
<td>Latin Extended-B</td>
<td>0.00%</td>
<td>746</td>
</tr>
<tr>
<td>Other Block, 2 UTF-8 Code Units</td>
<td>0.00%</td>
<td>4,114</td>
</tr>
<tr>
<td>Other Block, 3 UTF-8 Code Units</td>
<td>0.02%</td>
<td>371,955</td>
</tr>
<tr>
<td>Other Block, 4 UTF-8 Code Units</td>
<td>0.00%</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Here‚Äôs how our candidates did; red bars indicate time needed for case-insensitive search and blue bars indicate time needed for case-sensitive search:</p>
<p><img src="https://media.graphassets.com/64UtyySdRba5mO0jVbTN" alt="Benchmark results"/></p>
<p>Overall, running time actually improved compared to the original UTF-16 implementation. Unsurprisingly, the UTF-8 implementation takes a bit longer than the improved UTF-16 implementation with the same <code>TypedByteArray</code> improvement because it needs to decode each code point, even if it‚Äôs just a single code unit. One curious observation is that in spite of that, the UTF-8 implementation is actually faster than the improved UTF-16 implementation doing a case-insensitive search. At this point we haven‚Äôt found out why that is and are still investigating.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We showcased some internals of alfred-margaret and some approaches of moving it to UTF-8. Having done so enables us to move our codebase to <code>text-2.0</code>, cutting our memory consumption for strings in half! Even though the UTF-8 version has additional decoding overhead, we managed to speed it up compared to the old UTF-16 version. All in all our journey makes it clear that it is absolutely possible to write low-level code in Haskell to tackle performance-sensitive problems. This is especially nice since Haskell offers us a vast amount of tools to hide this complexity and integrate it cleanly into high-level code. </p>
<p>The section Haskell Performance Tuning shows that it is crucial to have at least some understanding of what happens inside GHC though. It helps a lot to have colleagues well-versed in GHC internals and the tooling that is available in the Haskell ecosystem. I&#39;d like to thank mine for their guidance and the opportunity to work on this project.</p>
<p>Moving all of <a target="_blank" rel="noop" href="https://hackage.haskell.org/">Hackage</a> to <code>text-2.0</code> is likely going to take a while, since it contains a lot of open-source contributions. However, since the module <code>Data.Text</code> is encoding-agnostic, most updates shouldn&#39;t need as much deliberation as this one.</p>
<hr/>
<p><a id="footnote-dotted-circle">1</a>: The user-perceived character ‚óåÃ£ actually consists of multiple abstract characters as well, <code>U+25CC DOTTED CIRCLE</code> and <code>U+0323 COMBINING DOT BELOW</code>. It just looks nicer that way. <a href="#link-footnote-dotted-circle">‚Ü©</a></p>
<p><a id="footnote-text-iteration">2</a>: Of course <code>Text</code>s can be traversed using the <code>Data.Text</code> module only, for example using any one of these <a target="_blank" rel="noop" href="https://hackage.haskell.org/package/text-2.0/docs/Data-Text.html#g:11">Folds</a> or <a target="_blank" rel="noop" href="https://hackage.haskell.org/package/text-2.0/docs/Data-Text-Unsafe.html#v:iter"><code>iter</code></a>. However, both of these methods construct intermediate data structures on the heap and we don&#39;t want to rely on GHC to optimize those away. <a href="#link-footnote-text-iteration">‚Ü©</a></p>
<p><a id="footnote-sharp-s">3</a>: Unicode actually defines <code>toUppercase(√ü) = SS</code>. This is because <code>U+1E9E LATIN CAPITAL LETTER SHARP S</code> is a very recent addition to the German writing system, officially established in 2017. In Germany, you will find a lot of street signs where &#34;Stra√üe&#34; (street) is written &#34;STRASSE&#34;, see for example <a target="_blank" rel="noop" href="https://www.stadtlandsand.de/?p=2099">this article</a>. However, Haskell&#39;s implementations of <code>toUpper</code> and <code>toLower</code> have the type <code>Char -&gt; Char</code>, only ever mapping to single code points. They use the simple case conversion algorithms which Unicode provides exactly for this use case. <a href="#link-sharp-s">‚Ü©</a></p>
<p><a id="footnote-codeunitindex">4</a>: Working with UTF-encoded strings, we have (at least) two ways to index a string: By code points and by code units. Using <code>CodeUnitIndex</code> here makes it clear what kind of index we are dealing with: Although we ultimately want a CodePoint, we want to use the fixed-width code units to access the string in constant time. <code>CodeUnitIndex</code> is a <code>newtype</code> which means that GHC throws errors if we use an <code>Int</code> instead. <a href="#link-codeunitindex">‚Ü©</a></p></div></div></div>
  </body>
</html>
