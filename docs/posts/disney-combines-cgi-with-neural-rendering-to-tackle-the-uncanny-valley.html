<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.unite.ai/disney-combines-cgi-with-neural-rendering-to-tackle-the-uncanny-valley/">Original</a>
    <h1>Disney Combines CGI with Neural Rendering to Tackle the ‘Uncanny Valley’</h1>
    
    <div id="readability-page-1" class="page"><div id="mvp-content-main"><p>Disney’s AI research division has developed a hybrid method for movie-quality facial simulation, combining the strengths of facial neural rendering with the consistency of a CGI-based approach.</p><p>The pending paper is titled <em><i>Rendering with Style: Combining Traditional and Neural Approaches for High Quality Face Rendering</i></em>, and is previewed in a <a href="https://www.youtube.com/watch?v=k-RKSGbWLng">new 10-minute video</a> at the Disney Research YouTube channel (embedded at end of this article).</p><div id="attachment_178954"><p><img aria-describedby="caption-attachment-178954" src="https://www.unite.ai/wp-content/uploads/2021/11/examples-disney-cgi-neural-rendering-faces2.gif" alt="Meshes combined with neural facial renders. Source: https://www.youtube.com/watch?v=k-RKSGbWLng" width="690" height="388"/></p><p id="caption-attachment-178954"><em>Meshes combined with neural facial renders. See video embed at end of article for better detail and quality.</em> Source: https://www.youtube.com/watch?v=k-RKSGbWLng</p></div><p>As the video notes, <a href="https://www.unite.ai/sofgan-a-gan-face-generator-that-offers-greater-control/">neural rendering of faces</a> (including <a href="https://www.unite.ai/what-are-deepfakes/">deepfakes</a>) can produce far more realistic eyes and mouth interiors than CGI is capable of, while CGI-driven facial textures are more consistent and suitable for cinema-level VFX output.</p><p>Therefore Disney is experimenting with letting NVIDIA’s <a href="https://github.com/NVlabs/stylegan2">StyleGan2</a> neural generator handle the surrounding features of a face and the ‘life-critical’ elements such as eyes, while superimposing consistent CGI facial skin and related elements into the output.</p><div id="attachment_178955"><p><img decoding="async" data-opt-src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-n44wSPff/w:1200/h:519/q:auto/https://www.unite.ai/wp-content/uploads/2021/11/disney-combined-neural-cgi-approach.jpg" aria-describedby="caption-attachment-178955" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-n44wSPff/w:1200/h:519/q:eco/https://www.unite.ai/wp-content/uploads/2021/11/disney-combined-neural-cgi-approach.jpg" alt="From the video (see end of article), the architectural concept behind Disney&#39;s hybrid approach, where an old-school CGI mesh, of the type used to recreate &#39;young&#39; Carrie Fisher and the late Peter Cushing for Rogue One (2016), is integrated into neurally-rendered face environments." width="1200" height="519"/></p><p id="caption-attachment-178955"><em>From the video (see end of article), the architectural concept behind Disney’s hybrid approach, where an old-school CGI mesh, of the type used to recreate ‘young’ Carrie Fisher and the late Peter Cushing for Rogue One (2016), is integrated into neurally-rendered face environments.</em></p></div><p>The video makes a tacit reference to <a href="https://nerdreactor.com/2016/12/20/the-cgi-debate-in-rogue-one-a-star-wars-story/">frequent criticism</a> of the inauthenticity and ‘uncanny valley’ effect of the CGI recreation of late British <em><i>Star Wars</i></em> actor Peter Cushing in <em><i>Rogue One</i></em> (2016), conceding:</p><p><em><i>‘[There’s] still a huge gap between what people can easily capture and render versus final photorealistic digital doubles, complete with hair, eyes and inner mouth. To close this gap, it usually takes a lot of manual work from skilled artists.’</i></em></p><p>In truth, even the most modern facial capture systems do not even attempt to recreate eyes, mouth interiors or hair, which either have issues of authenticity in such techniques (eyes) or else of temporal consistency (hair).</p><div id="attachment_178956"><p><img decoding="async" data-opt-src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-nREils7I/w:1000/h:517/q:auto/https://www.unite.ai/wp-content/uploads/2021/11/cgi-skin.jpg" aria-describedby="caption-attachment-178956" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-nREils7I/w:1000/h:517/q:eco/https://www.unite.ai/wp-content/uploads/2021/11/cgi-skin.jpg" alt="The video illustrates what VFX artists will get after a typical modern facial capture session. Eyes, hair, facial hair, and mouth interiors will all have to be handled by separate teams in the production pipeline." width="1000" height="517"/></p><p id="caption-attachment-178956"><em>The video illustrates what VFX artists will get after a typical modern facial capture session. Eyes, hair, facial hair, and mouth interiors will all have to be handled by separate teams in the production pipeline, in addition to texturing and lighting.</em></p></div><h3><strong>Illumination Control</strong></h3><p>The hybrid approach is also a benefit with relighting – a notable challenge for neural rendering of faces, since CGI skin superimpositions can be more easily relit.</p><div id="attachment_178957"><p><img aria-describedby="caption-attachment-178957" src="https://www.unite.ai/wp-content/uploads/2021/11/relighting-animated-version-disney-cgi-neural-face.gif" alt="An animated version of the CGI/Neural approach." width="690" height="388"/></p><p id="caption-attachment-178957"><em>An animated version of the CGI/Neural approach.</em></p></div><p>In more challenging environments, such as exterior shoots, the researchers have developed a method of inpainting around a kind of demilitarized zone surrounding the person being ‘created’.</p><div id="attachment_178958"><p><img decoding="async" data-opt-src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-9yC4-yiX/w:841/h:628/q:auto/https://www.unite.ai/wp-content/uploads/2021/11/dmz-disney.jpg" aria-describedby="caption-attachment-178958" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-9yC4-yiX/w:841/h:628/q:eco/https://www.unite.ai/wp-content/uploads/2021/11/dmz-disney.jpg" alt="A black margin is generated to allow a &#39;canvas&#39; for inpainting the outer parts of the identity and integrating the CGI skin into the combined CGI/neural output." width="841" height="628"/></p><p id="caption-attachment-178958"><em>A black margin is generated to allow a ‘canvas’ for inpainting the outer parts of the identity and integrating the CGI skin into the combined CGI/neural output.</em></p></div><p>The video notes:</p><p><em><i>‘[The] neural render does not match the background constraint perfectly. – it’s only meant as a guide, since optimizing for realistic human components like the hair, eyes and teeth is the main goal. More challenging is to try and maintain a consistent identity, while changing the environment lighting.’</i></em></p><h3><strong>Creating CGI Meshes From Neural Renders</strong></h3><p>The research team have also developed a variational autoencoder trained on a (unspecified) large database of 3D face images, and claims that it can produce ‘random but plausible’ 3D face meshes from ground truth data.</p><p>There are limitations for this research to overcome, including the difficulty in getting hair to stay temporally consistent in the neural renderings, and the video (see below) shows several examples of rapidly mutating hair in an otherwise consistent pan around a CGI/neural face.</p><p>Temporal consistency in neural video rendering is a far wider problem than just Disney’s, and it seems likely that later iterations of this system may resort to adding hair ‘in post’, or various other possible approaches to hair generation than hoping a novel neural approach will eventually solve it.</p><h3><strong>Uses for Dataset Generation</strong></h3><p>The method is proposed also as a potential method of generating <a href="https://www.unite.ai/what-is-synthetic-data/">synthetic data</a>, and enriching the facial image set landscape, which has in recent years become <a href="https://www.unite.ai/why-adversarial-image-attacks-are-no-joke/">dangerously monotonous</a>.</p><div id="attachment_178959"><p><img decoding="async" data-opt-src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-UTZ0BOSX/w:1000/h:566/q:auto/https://www.unite.ai/wp-content/uploads/2021/11/dataset-generation.jpg" aria-describedby="caption-attachment-178959" src="https://ml8ygptwlcsq.i.optimole.com/fMKjlhs-UTZ0BOSX/w:1000/h:566/q:eco/https://www.unite.ai/wp-content/uploads/2021/11/dataset-generation.jpg" alt="Disney envisages the new technique populating facial image datasets." width="1000" height="566"/></p><p id="caption-attachment-178959"><em>Disney envisages the new technique populating facial image datasets.</em></p></div><p><em><i>‘[Every] photorealistic result we generate has an underlying corresponding geometry, and appearance maps, rendered from unknown camera viewpoints with known illumination. This ‘ground truth’ information can be vital for training downstream applications, such as monocular, 3D face reconstruction, facial recognition, or scene understanding. And so every results render could be considered a data sample, and we can generate many variations of many different individuals. </i></em></p><p><em><i>‘Furthermore, even for a single person rendered in a single expression with a single viewpoint and illumination, we can generate random variations of the photo-real render by varying the randomization seed during optimization.’</i></em></p><p>The researchers note that this diversity of configurable output could be useful in training facial recognition applications, concluding:</p><p><em><i>‘[Our] method is able to leverage current technology for facial skin capture, modeling and rendering, and automatically create complete photorealistic face renders that match the desired identity, expression and scene configuration. This approach has applications and facial rendering for film and entertainment, saving manual artists labor and also for data generation in different fields of <a href="https://www.unite.ai/what-is-deep-learning/">deep learning</a>.’</i></em></p><p>For a deeper look at the new approach, check out the 10-minute video released today:</p><div title="Rendering with Style Combining Traditional and Neural Approaches for High Quality Face Rendering"><div id="WYL_k-RKSGbWLng" itemprop="video" itemscope="" itemtype="https://schema.org/VideoObject"><div id="lyte_k-RKSGbWLng" data-src="https://www.unite.ai/wp-content/plugins/wp-youtube-lyte/lyteCache.php?origThumbUrl=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fk-RKSGbWLng%2Fhqdefault.jpg"><div><p>Rendering with Style Combining Traditional and Neural Approaches for High Quality Face Rendering</p></div></div><meta itemprop="description" content="In this work we propose to combine incomplete, high-quality renderings showing only facial skin with recent methods for neural rendering of faces, in order to automatically and seamlessly create photo-realistic full-head portrait renders from captured data without the need for artist intervention. Our method begins with traditional face rendering, where the skin is rendered with the desired appearance, expression, viewpoint, and illumination. These skin renders are then projected into the latent space of a pre-trained neural network that can generate arbitrary photo-real face images (StyleGAN2). The result is a sequence of realistic face images that match the identity and appearance of the 3D character at the skin level, but is completed naturally with synthesized hair, eyes, inner mouth and surroundings. Link to publication file:"/></div></div></div></div>
  </body>
</html>
