<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/imelnyk/ArxivPapers">Original</a>
    <h1>ArXiv Papers as Audiobooks</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Official implementation of the algorithm behind:</p>
<div data-snippet-clipboard-copy-content="YouTube: https://www.youtube.com/@ArxivPapers
TikTok: https://www.tiktok.com/@arxiv_papers
Apple Podcasts: https://podcasts.apple.com/us/podcast/arxiv-papers/id1692476016
Spotify: https://podcasters.spotify.com/pod/show/arxiv-papers"><pre><code>YouTube: https://www.youtube.com/@ArxivPapers
TikTok: https://www.tiktok.com/@arxiv_papers
Apple Podcasts: https://podcasts.apple.com/us/podcast/arxiv-papers/id1692476016
Spotify: https://podcasters.spotify.com/pod/show/arxiv-papers
</code></pre></div>
<p dir="auto">The main idea of this work is to simplify and streamline ArXiv paper reading.
If you&#39;re a visual learner, this code will covert a paper to an engaging video format.
If you are on the run and like listening, this code will also generate audio for listening.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/imelnyk/ArxivPapers/blob/main/imgs/overview.png"><img src="https://github.com/imelnyk/ArxivPapers/raw/main/imgs/overview.png" alt="Teaser image"/></a></p>
<p dir="auto">Here are the main steps of the algorithm:</p>
<ol dir="auto">
<li>
<p dir="auto">Download paper source code, given its ArXiv ID</p>
</li>
<li>
<p dir="auto">Use <code>latex2html</code> or <code>latexmlc</code> to convert latex code to HTML page</p>
</li>
<li>
<p dir="auto">Parse HTML page to extract text and equations, ignoring tables, figures, etc</p>
</li>
<li>
<p dir="auto">If creating video, also create a map that matches pdf page to text
and also text chunks to page blocks.</p>
</li>
<li>
<p dir="auto">Split the text into sections and pass them through OpenAI GPT api to paraphrase, simplify and explain.</p>
</li>
<li>
<p dir="auto">Split GPT-generated text into chunks and convert them to audio using text-to-speach Google api</p>
</li>
<li>
<p dir="auto">Pack all the necessary pieces and create a zip file for further video processing</p>
</li>
<li>
<p dir="auto">Using earlier computed text-block map, create video using <code>ffmpeg</code></p>
</li>
</ol>
<p dir="auto"><strong>Note 1</strong> The code can create both long, more detailed, as well as short, summarized versions of the paper.</p>
<p dir="auto"><strong>Note 2</strong> The long video version will also contain summary blocks after each section</p>
<p dir="auto"><strong>Note 3</strong> The short video version will contain automatically generated slides summarizing the paper</p>
<p dir="auto"><strong>Note 4</strong> The code can also upload the generated audio files to your Google Drive, if provided with proper credentials</p>

<ul dir="auto">
<li>LaTeXML (<a href="https://github.com/brucemiller/LaTeXML">https://github.com/brucemiller/LaTeXML</a>)</li>
<li>latex2html (<a href="https://github.com/latex2html/latex2html">https://github.com/latex2html/latex2html</a>)</li>
<li>OpenAI key</li>
<li>ffmpeg (to make videos)</li>
<li>gcloud: (<a href="https://cloud.google.com/sdk/docs/install" rel="nofollow">https://cloud.google.com/sdk/docs/install</a>)</li>
<li>setup ADC: (<a href="https://cloud.google.com/docs/authentication/provide-credentials-adc#how-to" rel="nofollow">https://cloud.google.com/docs/authentication/provide-credentials-adc#how-to</a>)</li>
<li>Google Text-to-Speech (<a href="https://cloud.google.com/text-to-speech" rel="nofollow">https://cloud.google.com/text-to-speech</a>)</li>
<li>Google Drive setup (optional, follow <a href="https://github.com/iterative/PyDrive2">https://github.com/iterative/PyDrive2</a>)</li>
</ul>

<p dir="auto">openai, PyPDF2, spacy, tiktoken, pyperclip, google-cloud-texttospeech, pydrive2, pdflatex</p>

<div dir="auto" data-snippet-clipboard-copy-content="# to create audio, both short and long, and prepare for video creation

python main.py --verbose --include_summary --create_short --create_video --openai_key &lt;your_key&gt; --paperid &lt;arxiv_paper_id&gt; --l2h"><pre><span><span>#</span> to create audio, both short and long, and prepare for video creation</span>

python main.py --verbose --include_summary --create_short --create_video --openai_key <span>&lt;</span>your_key<span>&gt;</span> --paperid <span>&lt;</span>arxiv_paper_id<span>&gt;</span> --l2h</pre></div>
<p dir="auto">The default latex conversion tool <code>latex2html</code> sometimes fails, in this case remove <code>--l2h</code>
to use <code>latexmlc</code>. Also, by default the code will process the whole paper up to references, if you want to stop earlier, pass <code>--stop_word &#34;experiments&#34;</code> (e.g., to stop before Experiments Section).</p>

<div data-snippet-clipboard-copy-content="&lt;arxiv_paper_id&gt;_files/
├── final_audio.mp3
├── final_audio_short.mp3
├── abstract.txt
├── zipfile-&lt;time_stamp&gt;.zip
├── ...
├── extracted_orig_text_clean.txt
├── original_text_split_pages.txt
├── original_text_split_sections.txt
├── ...
├── gpt_text.txt
├── gpt_text_short.txt
├── gpt_verb_steps.txt
├── ...
├── slides
    ├── slide1.pdf
    ├── ..."><pre><code>&lt;arxiv_paper_id&gt;_files/
├── final_audio.mp3
├── final_audio_short.mp3
├── abstract.txt
├── zipfile-&lt;time_stamp&gt;.zip
├── ...
├── extracted_orig_text_clean.txt
├── original_text_split_pages.txt
├── original_text_split_sections.txt
├── ...
├── gpt_text.txt
├── gpt_text_short.txt
├── gpt_verb_steps.txt
├── ...
├── slides
    ├── slide1.pdf
    ├── ...
</code></pre></div>
<p dir="auto">The output directory, among other things, contains generated audio files, slides, extracted original text
and GPT generated output, split across pages or sections. The output also contains <code>zipfile-&lt;time_stamp&gt;.zip</code>
which includes data for video generation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# to extract only the original text from ArXiv paper, without any GPT/audio/video processing

python main.py --verbose --extract_text_only --paperid &lt;arxiv_paper_id&gt;"><pre><span><span>#</span> to extract only the original text from ArXiv paper, without any GPT/audio/video processing</span>

python main.py --verbose --extract_text_only --paperid <span>&lt;</span>arxiv_paper_id<span>&gt;</span></pre></div>
<p dir="auto">Now, we are ready to generate the video:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# to generate video based on the results from above, point to the 

python makevideo.py --paperid &lt;arxiv_paper_id&gt;"><pre><span><span>#</span> to generate video based on the results from above, point to the </span>

python makevideo.py --paperid <span>&lt;</span>arxiv_paper_id<span>&gt;</span></pre></div>

<div data-snippet-clipboard-copy-content="output_&lt;time_stamp&gt;/
├── output.mp4
├── output_short.mp4
├── ..."><pre><code>output_&lt;time_stamp&gt;/
├── output.mp4
├── output_short.mp4
├── ...
</code></pre></div>
<p dir="auto">The output directory now contains two video files, one for the long and another for the short video.</p>
</article></div></div>
  </body>
</html>
