<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://isakfalk.com/notes/diffusion-models.html">Original</a>
    <h1>Diffusion and score-based generative modeling</h1>
    
    <div id="readability-page-1" class="page"><div id="text-learning-the-score-function">
          <p>The reason we didn&#39;t have to learn the score function in <a href="#tractable-mixture-models">Tractable mixture models</a> was because
          we restricted ourselves to a distribution with a tractable score
          function. In reality this is seldom the case, and even if we could do
          it in theory, it may be too computationally expensive to do it
          directly. Additionally, If we have a set of points which we interpret
          as an empirical distribution then the score function is not even
          well-defined as there is no density. We have to resort to learning it
          in some way.</p>
          <p>First, we will use the MNIST dataset where we view an image as a
          discrete distribution by normalizing the pixel intensities over the
          total intensity of all the pixels in the image. Since each pixel is a
          value between 0 and 1, we can view this as a distribution over pixel
          coordinates. To make this point clear, we assume that the underlying
          space is a 2d cartesian square, \(\mathcal{X} = [0,
          1]^{2}\)<sup><a id="fnr.3" href="#fn.3" role="doc-backlink">3</a></sup>, with each pixel coordinate being
          normalized to be between 0 and 1. So, an image is a collection of
          coordinate pairs and pixel intensity values, in the case of MNIST
          which is \(28 \times 28\) we have pixel coordinates \((i, j)\) where
          \(i, j \in \{(l + 1/2) / 28\}_{l=0}^{27}\) and the corresponding
          pixel intensities \(I(i, j) \in [0, 1]\). With this we have an
          empirical distribution where \(\hat{p}(i, j) = I(i, j) / \sum_{i&#39;,
          j&#39;}I(i&#39;, j&#39;) \propto I(i, j)\).</p>
          <p>Behold, the first image to the MNIST training dataset!</p>
          <div>
            <pre>

<span>import</span> torchvision

<span>mnist</span> <span>=</span> torchvision.datasets.MNIST(<span>&#34;~/data&#34;</span>, download<span>=</span><span>True</span>)
<span>mnist_images</span> <span>=</span> mnist.data.numpy()
<span>image</span> <span>=</span> mnist_images[0]
<span>image</span> <span>=</span> image.astype(<span>float</span>) <span>/</span> 255.0

<span>def</span> <span>create_sample_fn</span>(image):
    <span>&#34;&#34;&#34;Generate a function that samples from the image distribution&#34;&#34;&#34;</span>
    <span>def</span> <span>sample</span>(num_samples, key):
        <span>h</span>, <span>w</span> <span>=</span> image.shape
        
        <span>return</span> jnp.array(
            [<span>divmod</span>(x.item(), w) <span>for</span> x <span>in</span> random.categorical(logits<span>=</span>jnp.log(image.ravel()), key<span>=</span>key, shape<span>=</span>(num_samples,))]
        )
    <span>return</span> sample

<span>fig</span>, <span>ax</span> <span>=</span> plt.subplots()
<span>im</span> <span>=</span> ax.imshow(image, cmap<span>=</span><span>&#34;gray&#34;</span>)
plt.colorbar(im)
ax.axis(<span>&#34;off&#34;</span>)
</pre>
          </div>
          <figure id="orgb89370e">
            <img src="https://blog.schmizz.net/assets/images/diffusion_models/5-mnist-imshow.webp" alt="5-mnist-imshow.webp"/>
          </figure>
          <p>Let&#39;s check the histogram when we sample many times according to
          the distribution defined by the image, we should get something
          similar as the sample size becomes large. The histogram function will
          rotate the image though.</p>
          <div>
            <pre><span>key</span> <span>=</span> jax.random.PRNGKey(<span>sum</span>(<span>ord</span>(c) <span>for</span> c <span>in</span> <span>&#34;five&#34;</span>))
<span>sample</span> <span>=</span> create_sample_fn(image)
<span>x</span> <span>=</span> sample(10000, key)
<span>fig</span>, <span>ax</span> <span>=</span> plt.subplots(figsize<span>=</span>(4, 4))
<span>h</span> <span>=</span> ax.hist2d(x[:, 0], x[:, 1], cmap<span>=</span><span>&#34;gray&#34;</span>)
ax.axis(<span>&#34;off&#34;</span>)
</pre>
          </div>
          <figure id="org0c78764">
            <img src="https://blog.schmizz.net/assets/images/diffusion_models/5-mnist-hist2d.webp" alt="5-mnist-hist2d.webp"/>
          </figure>
          <p>Now we set up the training. The architecture here is a combination
          of things</p>
          <ol>
            <li>We use the insights from (<a href="#citeproc_bib_item_2">Tancik
            et al. 2020</a>) which roughly says that using a pre-processing
            fourier feature map before the MLP is helpful for learning
            high-frequency mappings for coordinate based inputs. We add a
            residual connection here, so that the input to the MLP is
            <kbd>jnp.concatenate(f_layer(x), x)</kbd>.
            </li>
            <li>The <kbd>RFLayer</kbd> has noise-level specific parameters
            <kbd>alpha, beta</kbd> which linearly transform the random features
            and we learn one such transformation for each noise level (the rest
            of the architecture is shared, like the MLP and the original random
            feature mappings).</li>
            <li>We freeze the random feature parameters <kbd>B_cos, B_sin</kbd>
            by following the <a href="https://docs.kidger.site/equinox/examples/frozen_layer/">guide on
            how to freeze layer in the equinox documents</a>.
            </li>
            <li>The rest of the training is done using the objective
            \(\hat{\mathcal{E}}(\theta; (\sigma_{l})_{l=1}^{L})\).</li>
          </ol>
          <div>
            <pre><span>class</span> <span>RFLayer</span>(eqx.Module):
    <span>&#34;&#34;&#34;Random Feature layer with learnable linear output transformations alpha, beta&#34;&#34;&#34;</span>
    B_cos: jax.Array
    B_sin: jax.Array
    alpha: jax.Array
    beta: jax.Array
    num_noise_levels: <span>int</span>
    sigma: <span>float</span>

    <span>def</span> <span>__init__</span>(<span>self</span>, in_size: <span>int</span>, num_rf: <span>int</span>, num_noise_levels: <span>int</span>, key, sigma: <span>float</span> <span>=</span> 1.0):
        <span>cos_key</span>, <span>sin_key</span> <span>=</span> random.split(key, 2)
        <span>self</span>.<span>B_cos</span> <span>=</span> random.normal(cos_key, (num_rf, in_size)) <span>*</span> sigma
        <span>self</span>.<span>B_sin</span> <span>=</span> random.normal(sin_key, (num_rf, in_size)) <span>*</span> sigma
        <span>self</span>.<span>sigma</span> <span>=</span> sigma
        <span>self</span>.<span>num_noise_levels</span> <span>=</span> num_noise_levels
        <span>self</span>.<span>alpha</span> <span>=</span> jnp.ones(num_noise_levels)
        <span>self</span>.<span>beta</span> <span>=</span> jnp.zeros(num_noise_levels)

    <span>def</span> <span>__call__</span>(<span>self</span>, x: jax.Array, noise_level_idx: <span>int</span>) <span>-&gt;</span> jax.Array:
        <span>rf_features</span> <span>=</span> jnp.concatenate(
            [jnp.cos(2 <span>*</span> math.pi <span>*</span> <span>self</span>.B_cos @ x), jnp.sin(2 <span>*</span> math.pi <span>*</span> <span>self</span>.B_sin @ x)], axis<span>=-</span>1
        )
        <span>return</span> <span>self</span>.alpha[noise_level_idx] <span>*</span> rf_features <span>+</span> <span>self</span>.beta[noise_level_idx]

<span>class</span> <span>Model</span>(eqx.Module):
    rf_layer: RFLayer
    mlp: eqx.nn.MLP

    <span>def</span> <span>__init__</span>(<span>self</span>, in_size: <span>int</span>, num_rf: <span>int</span>, width_size: <span>int</span>, depth: <span>int</span>, out_size: <span>int</span>, num_noise_levels: <span>int</span>, key):
        <span>self</span>.<span>rf_layer</span> <span>=</span> RFLayer(in_size, num_rf, num_noise_levels, key)
        <span>self</span>.<span>mlp</span> <span>=</span> eqx.nn.MLP(in_size<span>=</span>num_rf <span>*</span> 2 <span>+</span> 2,
                              width_size<span>=</span>width_size,
                              depth<span>=</span>depth,
                              out_size<span>=</span>out_size,
                              activation<span>=</span>jax.nn.softplus,
                              key<span>=</span>key)

    <span>def</span> <span>__call__</span>(<span>self</span>, x: jax.Array, noise_level_idx: <span>int</span>) <span>-&gt;</span> jax.Array:
        <span>x</span> <span>-=</span> 0.5
        <span>x</span> <span>=</span> jnp.concatenate((<span>self</span>.rf_layer(x, noise_level_idx), x)) 
        <span>return</span> <span>self</span>.mlp(x)


<span>def</span> <span>one_sample_loss</span>(model, x, sigmas, key):
    
    <span>perturbations</span> <span>=</span> random.normal(key, (sigmas.shape[0], x.shape[0])) <span>*</span> jnp.expand_dims(sigmas, 1)
    <span>x_bars</span> <span>=</span> x <span>+</span> perturbations
    
    <span>scores_pred</span> <span>=</span> vmap(model)(x_bars, jnp.arange(sigmas.shape[0]))
    <span>scores</span> <span>=</span> <span>-</span>(x_bars <span>-</span> x) <span>/</span> jnp.expand_dims(sigmas <span>**</span> 2, 1)
    
    
    <span>result</span> <span>=</span> jnp.mean(jnp.square(scores_pred <span>-</span> scores).mean(<span>-</span>1) <span>*</span> sigmas <span>**</span> 2)
    <span>return</span> result

<span>def</span> <span>loss</span>(diff_model, static_model, xs, sigmas, keys):
    <span>&#34;&#34;&#34;Objective function, we separeate the parameters into active and frozen parameters&#34;&#34;&#34;</span>
    <span>model</span> <span>=</span> eqx.combine(static_model, diff_model)
    <span>batch_loss</span> <span>=</span> vmap(one_sample_loss, (<span>None</span>, 0, <span>None</span>, 0))
    <span>return</span> jnp.mean(batch_loss(model, xs, sigmas, keys))

<span>def</span> <span>train</span>(
        model: eqx.Module,
        filter_spec: PyTree,
        sample,
        optim: optax.GradientTransformation,
        steps: <span>int</span>,
        batch_size: <span>int</span>,
        print_every: <span>int</span>,
        sigmas: Float[Array, <span>&#34;...&#34;</span>],
        key
) <span>-&gt;</span> eqx.Module:
    <span>@eqx.filter_jit</span>
    <span>def</span> <span>make_step</span>(
            model: eqx.Module,
            xs: Float[Array, <span>&#34;batch_size 2&#34;</span>],
            opt_state: PyTree,
            keys: Float[Array, <span>&#34;batch_size&#34;</span>],
    ):
        <span>diff_model</span>, <span>static_model</span> <span>=</span> eqx.partition(model, filter_spec)
        <span>loss_value</span>, <span>grads</span> <span>=</span> eqx.filter_value_and_grad(loss)(diff_model, static_model, xs, sigmas, keys)
        <span>updates</span>, <span>opt_state</span> <span>=</span> optim.update(grads, opt_state)
        <span>model</span> <span>=</span> eqx.apply_updates(model, updates)
        <span>return</span> model, opt_state, loss_value

    <span>original_model</span> <span>=</span> model
    <span>opt_state</span> <span>=</span> optim.init(eqx.<span>filter</span>(model, eqx.is_inexact_array))
    <span>for</span> step <span>in</span> <span>range</span>(steps):
        <span>*</span><span>loss_keys</span>, <span>sample_key</span>, <span>key</span> <span>=</span> random.split(key, batch_size <span>+</span> 2)
        <span>loss_keys</span> <span>=</span> jnp.stack(loss_keys)
        <span>xs</span> <span>=</span> sample(batch_size, sample_key) <span>/</span> 27
        <span>model</span>, <span>opt_state</span>, <span>loss_value</span> <span>=</span> make_step(model, xs, opt_state, loss_keys)
        <span>if</span> step <span>%</span> print_every <span>==</span> 0:
            <span>print</span>(f<span>&#34;Step </span>{step}<span>, Loss </span>{loss_value}<span>&#34;</span>)

    <span>return</span> model
</pre>
          </div>
          <p>Now let&#39;s train it. We squint and choose some good hyperparameters
          and pray to the ML-gods for an auspicious training run (actually I
          did some hand-tuning).</p>
          <div>
            <pre><span>sigmas</span> <span>=</span> jnp.geomspace(0.0001, 1, 30, endpoint<span>=</span><span>True</span>)
<span>DEPTH</span> <span>=</span> 3
<span>WIDTH_SIZE</span> <span>=</span> 128
<span>NUM_RF</span> <span>=</span> 256
<span>BATCH_SIZE</span> <span>=</span> 128
<span>STEPS</span> <span>=</span> 5 <span>*</span> 10 <span>**</span> 4
<span>PRINT_EVERY</span> <span>=</span> 5000

<span>model</span> <span>=</span> Model(in_size<span>=</span>2,
              num_rf<span>=</span>NUM_RF,
              width_size<span>=</span>WIDTH_SIZE,
              depth<span>=</span>DEPTH,
              out_size<span>=</span>2,
              num_noise_levels<span>=</span><span>len</span>(sigmas),
              key<span>=</span>random.PRNGKey(0))

<span>LEARNING_RATE</span> <span>=</span> 1e<span>-</span>3
<span>optim</span> <span>=</span> optax.adam(LEARNING_RATE)




<span>filter_spec</span> <span>=</span> jtu.tree_map(<span>lambda</span> x: <span>True</span> <span>if</span> <span>isinstance</span>(x, jax.Array) <span>else</span> <span>False</span>, model)
<span>filter_spec</span> <span>=</span> eqx.tree_at(
    <span>lambda</span> tree: (tree.rf_layer.B_cos, tree.rf_layer.B_sin),
    filter_spec,
    replace<span>=</span>(<span>False</span>, <span>False</span>),
)
<span>model</span> <span>=</span> train(model, filter_spec, sample, optim, STEPS, BATCH_SIZE, PRINT_EVERY, sigmas, key)
</pre>
          </div>
          <pre id="org1c3034d">Step 0, Loss 1.0177688598632812
Step 5000, Loss 0.7470076084136963
Step 10000, Loss 0.6700457334518433
Step 15000, Loss 0.6010410785675049
Step 20000, Loss 0.5470178127288818
Step 25000, Loss 0.5063308477401733
Step 30000, Loss 0.47549256682395935
Step 35000, Loss 0.4591177701950073
Step 40000, Loss 0.4523712992668152
Step 45000, Loss 0.43943890929222107
</pre>
          <p>Let&#39;s visualize the vector field for this new model by repurposing
          the <a href="#org2c2bd13"><kbd>plot_logdistribution</kbd>
          function</a> to just plot the vector field. Since we don&#39;t have an
          actual density we will not plot the level</p>
          <div>
            <pre><span>def</span> <span>plot_vector_field</span>(fig, ax, score_fun, xlim<span>=</span>(0.0, 1.0), ylim<span>=</span>(0.0, 1.0), n_quiver<span>=</span>10):
    
    <span>x</span> <span>=</span> np.linspace(<span>*</span>xlim, n_quiver)
    <span>y</span> <span>=</span> np.linspace(<span>*</span>ylim, n_quiver)
    <span>X</span>, <span>Y</span> <span>=</span> np.meshgrid(x, y)
    <span>XY</span> <span>=</span> np.stack([X.ravel(), Y.ravel()], axis<span>=-</span>1)
    <span>grads</span> <span>=</span> vmap(score_fun)(XY)
    <span>grad_X</span> <span>=</span> grads[:, 0].reshape(n_quiver, n_quiver)
    <span>grad_Y</span> <span>=</span> grads[:, 1].reshape(n_quiver, n_quiver)
    ax.quiver(X, Y, grad_X, grad_Y)
    <span>return</span> fig, ax

<span>fig</span>, <span>ax</span> <span>=</span> plt.subplots(3, 3, figsize<span>=</span>(3 <span>*</span> 2, 3 <span>*</span> 2))
<span>for</span> axis, i <span>in</span> <span>zip</span>(ax.ravel(), <span>range</span>(0, 30, 3)):
    axis.axis(<span>&#39;off&#39;</span>)
    axis.set_aspect(<span>&#39;equal&#39;</span>)
    axis.set_title(f<span>&#34;noise level </span>{i}<span>: </span>{sigmas[i]:.2f}<span>&#34;</span>)
    plot_vector_field(fig, axis, functools.partial(model, noise_level_idx<span>=</span>i), n_quiver<span>=</span>15)

plt.tight_layout()
</pre>
          </div>
          <figure id="orgfa6ae93">
            <img src="https://blog.schmizz.net/assets/images/diffusion_models/5-mnist-vector-fields.webp" alt="5-mnist-vector-fields.webp"/>
            <figcaption>
              <span>Figure 3:</span> The score function
              for all noise levels used to train the model. It seems like the
              sweetspot is around 18 (I choose 17 after inspecting all noise
              levels manually).
            </figcaption>
          </figure>
          <p>Let&#39;s see what we have learned. We define the update step (return
          tuple due to using <kbd>lax.scan</kbd> later)</p>
          <div>
            <pre><span>@eqx.filter_jit</span>
<span>def</span> <span>update_x</span>(x, z, model, step_size):
    <span>g</span> <span>=</span> model(x)
    <span>xp1</span> <span>=</span> x <span>+</span> (step_size <span>/</span> 2) <span>*</span> g <span>+</span> jnp.sqrt(step_size) <span>*</span> z
    <span>return</span> xp1, xp1
</pre>
          </div>
          <p>and evolve a particle over many steps, <kbd>lax.scan</kbd> simply
          makes this efficient</p>
          <div>
            <pre><span>step_size</span> <span>=</span> 0.001
<span>num_steps</span> <span>=</span> 400_000
<span>key</span> <span>=</span> random.PRNGKey(0)
<span>z_key</span>, <span>x0_key</span>, <span>key</span> <span>=</span> random.split(key, 3)
<span>z</span> <span>=</span> random.normal(z_key, shape<span>=</span>(num_steps, 2))
<span>x0</span> <span>=</span> jnp.ones(2,) <span>*</span> 0.5
<span>score_model</span> <span>=</span> functools.partial(model, noise_level_idx<span>=</span>17)
<span>update_fun</span> <span>=</span> functools.partial(update_x, model<span>=</span>score_model, step_size<span>=</span>step_size)
<span>final</span>, <span>result</span> <span>=</span> lax.scan(update_fun, x0, z)
</pre>
          </div>
          <p>Let&#39;s look at this. Since we sample so many particles, let&#39;s just
          plot a 2d histogram of this (choosing <kbd>noise_level_idx</kbd>
          being 17 but other indices in the vicinity should work too). Note
          that this has a finer resolution than the original mnist images which
          are \(27 \times 27\)</p>
          <div>
            <pre><span>fig</span>, <span>ax</span> <span>=</span> plt.subplots(figsize<span>=</span>(6, 6))
<span>h</span> <span>=</span> ax.hist2d(result[:, 0], result[:, 1], cmap<span>=</span><span>&#34;gray&#34;</span>, bins<span>=</span>(50, 50))
ax.axis(<span>&#34;off&#34;</span>)
</pre>
          </div>
          <figure id="orgd04a0c7">
            <img src="https://blog.schmizz.net/assets/images/diffusion_models/5-mnist-from-samples.webp" alt="5-mnist-from-samples.webp"/>
          </figure>
        </div></div>
  </body>
</html>
