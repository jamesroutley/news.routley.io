<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://digitalsociety.coop/posts/migrating-to-hetzner-cloud/">Original</a>
    <h1>3x performance for 1/4 of the price by migrating from AWS to Hetzner</h1>
    
    <div id="readability-page-1" class="page"><div><section><div><div><div><p><img alt="" src="https://digitalsociety.coop/images/hetzner-logo.png"/></p><div><p>02/10/2025</p><p><strong>We saved 76% on our cloud bills while tripling our capacity by migrating to Hetzner from AWS and DigitalOcean.</strong></p></div></div><h2>Background</h2><p>All the software we build at <span>Digital</span><span>Society</span> runs in the cloud. Prior to the migration we ran workloads on two platforms:</p><ul><li><a href="https://aws.amazon.com/"><img alt="AWS" src="https://digitalsociety.coop/images/aws-logo.svg"/></a><div><p>We use AWS for some of our core hosting needs (DNS via <a href="https://aws.amazon.com/route53" target="_blank">Route53</a> and sending emails via <a href="https://aws.amazon.com/ses/" target="_blank">SES</a>).</p><p>We also chose AWS to host <a href="https://tapintodata.com/" target="_blank">tap</a>, our first SaaS product, using a variety of AWS services (<a href="https://aws.amazon.com/ecs/" target="_blank">ECS</a> for container orchestration, <a href="https://aws.amazon.com/rds/" target="_blank">RDS</a> for relational databases, <a href="https://aws.amazon.com/elasticloadbalancing/application-load-balancer/" target="_blank">ALB</a> for ingress, and a long tail of peripheral services, as is the AWS way).</p><p>We chose AWS for familiarity since we have worked with it for nearly 15 years. We also prize their reliability, particularly when it comes to API stability. We automate as much of our infrastructure management as possible, and don&#39;t want to spend time chasing API breaking changes.</p></div></li><li><a href="https://www.digitalocean.com/"><img alt="DigitalOcean" src="https://digitalsociety.coop/images/digitalocean-logo.svg"/></a><div><p>We used <a href="https://www.digitalocean.com/products/kubernetes" target="_blank">DigitalOcean Kubernetes</a> to host several lightweight services, such as <a href="https://epcdata.scot/" target="_blank">epcdata.scot</a>, and monitoring services (<a href="https://umami.is/" target="_blank">Umami</a> for web analytics, <a href="https://openobserve.ai/" target="_blank">OpenObserve</a> for telemetry, and <a href="https://uptime.kuma.pet/" target="_blank">Uptime Kuma</a> for availability monitoring).</p><p>We chose DigitalOcean for its relatively simple and cost-effective managed Kubernetes offering, where you pay for the cluster&#39;s resources (nodes, block storage, load balancers) but the controlplane is free.</p><p>We chose Kubernetes for familiarity since we have worked with it for nearly 10 years. Although it requires a lot of <a href="https://github.com/digital-society-coop/runtime" target="_blank">boilerplate configuration</a>, once it has been set up Kubernetes enables a frictionless developer experience for deploying applications quickly.</p></div></li></ul><p>Why two cloud providers? Initially we used only DigitalOcean, but a data intensive SaaS like tap needs a lot of cloud resources and AWS have a generous <a href="https://aws.amazon.com/startups/credits#packages" target="_blank">$1,000 credit</a> package for self-funded startups. Building tap with AWS credits let us experiment with our infrastructure needs without worrying about the cost.</p><h2>Credits don&#39;t last forever</h2><p>In the spirit of minimising our operational costs we opted to use AWS&#39; serverless container runtime, <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html" target="_blank">Fargate</a>. This lets us pay per-second for the CPU and memory used by our application. Fargate&#39;s monthly pricing scales down fairly well, with a minimal workload (0.25 CPU, 0.5 GiB RAM) costing around $10/month.</p><p>However, tap is a data-intensive SaaS that needs to be able to execute complex queries over gigabytes of data in seconds. Even though we use the blazingly fast <a href="https://www.rust-lang.org/" target="_blank">Rust</a> programming language and modern, efficient data technologies like <a href="https://arrow.apache.org/" target="_blank">Apache Arrow</a> and <a href="https://datafusion.apache.org/" target="_blank">DataFusion</a>, we have found the minimum resource requirements for good performance to be around 2x CPUs and 4 GiB RAM â€“ ideally even more to get a good experience for demanding queries.</p><p>How much does a 2x CPU, 4 GiB RAM container cost on AWS Fargate? Just over $70/month. We run two worker instances, which need these higher resources, along with smaller web instances and all the other infrastructure needed to host an application on AWS (load balancer, relational DB, NAT gateway, ...). All together, our total costs for two environments of tap grew to <strong>$449.50/month</strong>.</p><p>In the end we used up our free credits in less than 6 months, and as a bootstrapped startup absorbing that kind of running cost is painful.</p><h2>Investigating alternatives</h2><p>Faced with these high running costs we started to investigate alternative cloud providers. Around the same time, tariff wars and the growth of AI-powered technofeudalism made us look specifically for <a href="https://european-alternatives.eu/" target="_blank">UK or EU based</a> cloud providers.</p><p>We quickly came across <a href="https://www.hetzner.com/cloud/" target="_blank">Hetzner</a>, and while their offering is geared towards self-managed VPS, meaning additional maintenance compared to managed solutions, we were sold on their pricing (more detail later). So much so that we decided to migrate our DigitalOcean infrastructure as well.</p><p>Since most of our services were already running in Kubernetes, and tap was already container-based, we decided we would run Kubernetes. Having operated Kubernetes clusters before this wasn&#39;t a decision taken lightly, but we discovered <a href="https://www.talos.dev/" target="_blank">Talos Linux</a> which promised to simplify the cluster setup and maintenance.</p><p>Our existing Kubernetes clusters in DigitalOcean used a <a href="https://github.com/digital-society-coop/runtime" target="_blank">runtime</a> that we created to cover basic infrastructure needs for web applications. Combined with Kubernetes&#39; native container orchestration features, these covered all the functionality we were using in AWS and DigitalOcean except for managed PostgreSQL databases. Given that these are critical pieces of infrastructure, we wanted a robust solution that included detailed monitoring, automated failover, seamless upgrades, and scheduled backups. We found <a href="https://cloudnative-pg.io/" target="_blank">CloudNativePG</a> which ticks all our boxes.</p><h2>The new stack</h2><p>Altogether, this is the stack we landed on:</p><ul><li><a href="https://www.hetzner.com/cloud/" target="_blank">Hetzner</a> as the core infrastructure provider. We use their ARM shared vCPU cloud servers, block storage volumes, load balancers, networks, firewalls, and S3-compatible object storage.</li><li><a href="https://www.talos.dev/" target="_blank">Talos Linux</a> as the operating system for cloud servers. Talos lets you manage Kubernetes nodes in a similar way to Kubernetes resources, by applying declarative configuration from which the OS figures out the actual changes (if any) to make on the node.</li><li><a href="https://cloudnative-pg.io/" target="_blank">CloudNativePG</a> fills the role of a managed database service (e.g. RDS) for the cluster. PostgreSQL clusters can be declared in Kubernetes manifests alongside the workload(s) using them, and can be configured with scheduled backups, failover replicas, configuration overrides, etc.</li><li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">Ingress NGINX Controller</a> fills the role of a managed load balancer or API gateway for the cluster, consolidating and making available the ingress routes declared by workloads.</li><li><a href="https://github.com/kubernetes-sigs/external-dns" target="_blank">ExternalDNS</a> allows DNS names to be associated with ingress resources. Roughly, Ingress NGINX Controller manages HTTP routing <em>in</em> the cluster while ExternalDNS handles routing <em>to</em> the cluster.</li><li><a href="https://cert-manager.io/" target="_blank">cert-manager</a> creates TLS certificates to secure workload routes with HTTPS.</li></ul><p>All infrastructure is codified using <a href="https://developer.hashicorp.com/terraform" target="_blank">Terraform</a> and <a href="https://helm.sh/" target="_blank">Helm</a> with deployments automated through <a href="https://docs.github.com/en/actions" target="_blank">GitHub Actions</a>.</p><h2>What a savings</h2><p>It&#39;s not easy to do a strictly apples-to-apples comparison between cloud providers since they tend to differ in features (technical or contractual, e.g. SLAs), but an easy point of comparison is our monthly bill:</p><div><div><p>AWS and DigitalOcean*</p><p><span>$559.36</span></p></div></div><p>* Based on peak invoice amount. Technically DigitalOcean peaked in July ($109.86) before we started our migration. AWS peaked in August ($449.50) since we migrated tap later.</p><p>We get a lot more capacity for this price as well:</p><div><div><p>AWS and DigitalOcean</p><p><span>12</span> vCPUs</p><p><span>24 GiB</span> RAM</p></div><div><p>Hetzner*</p><p><span>44</span> vCPUs <span>+367%</span></p><p><span>88 GiB</span> RAM <span>+367%</span></p></div></div><p>* This is just the capacity available for workloads, with controlplanes excluded (an additional 6 vCPUs and 12 GiB RAM).</p><h2>Challenges</h2><p>So much for the upsides, but the migration wasn&#39;t always straightforward. Our cloud estate is small in the grand scheme of things but inevitably there were challenges.</p><p><strong>Hetzner&#39;s network zones are not equivalent to AWS&#39; availability zones.</strong></p><p>AWS&#39;s topology is based on <a href="https://aws.amazon.com/about-aws/global-infrastructure/regions_az/" target="_blank">regions and availability zones</a>. Typically your infrastructure will live in a single region, but be split across availability zones for fault tolerance. Notably, private networking is region-wide (i.e. servers in different availability zones can easily communicate over private networks).</p><p>Hetzner&#39;s topology is based on <a href="https://docs.hetzner.com/cloud/general/locations/" target="_blank">locations and network zones</a>. There is only one EU network zone, <code>eu-central</code>, which has 3 locations. Since servers in different locations in the same network zone can communicate over private networks, we equivalated them to AWS&#39; availability zones.</p><p>In reality, there is significant latency between Hetzner locations that make running multi-location workloads challenging, and potentially harmful to performance as we discovered through our post-deployment monitoring.</p><p>Instead, we opted to use a single location (Nuremberg) and use <a href="https://docs.hetzner.com/cloud/placement-groups/overview" target="_blank">placement groups</a> to improve resilience. Placement groups ensure that virtual servers in the same group run on different physical servers, significantly reducing the likelihood that they will fail together.</p><p><strong>A service being docker-based doesn&#39;t mean it will be trivial to migrate.</strong></p><p>On AWS we deployed our SaaS product, tap, to the Elastic Container Service (ECS) container runtime. This meant we were already building and push containers as part of the automated build and we had expected that migrating the rest of the configuration from ECS CloudFormation to Kubernetes manifests wouldn&#39;t be too laborious.</p><p>Unfortunately hadn&#39;t considered the deployment automation around the configuration. In particular, we had scripts to gather the right configuration from GitHub and pass it along to CloudFormation. The difficulty wasn&#39;t in adapting the scripts to Kubernetes, but rather that we hadn&#39;t anticipated the work and so that part of the migration took longer than we expected.</p><p>In the end we used <a href="https://kustomize.io/" target="_blank">Kustomize</a> as the glue between sensitive configuration in GitHub and our Kubernetes manifests. We moved our non-sensitive configuration out of GitHub settings and into config files in the repo itself since this works more easily with Kustomize. It also makes tracking and reviewing changes to these settings easier, so we are happy with the result.</p><h2>Conclusions</h2><p>Hetzner is an incredibly cost-effective cloud provider. While their offerings are less expansive and hands-off than AWS or DigitalOcean&#39;s, it&#39;s possible to mitigate this with your stack if you don&#39;t mind getting your hands dirty.</p><p>We&#39;re particularly happy that this will allow us to keep <a href="https://tapintodata.com/" target="_blank">tap</a> running cheaply and performantly while we develop and launch it.</p></div></div></section></div><div><div><p><a href="https://www.uk.coop/directory/digital-society" aria-label="To uk.coop"><img alt="Cooperatives UK logo" src="https://digitalsociety.coop/images/coop.svg"/></a></p></div><p>Digital Society Ltd is a private company limited by guarantee without share capital registered in Scotland (SC768012).</p></div></div>
  </body>
</html>
