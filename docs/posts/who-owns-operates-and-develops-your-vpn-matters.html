<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.opentech.fund/news/who-owns-operates-and-develops-your-vpn-matters-an-analysis-of-transparency-vs-anonymity-in-the-vpn-ecosystem-and-implications-for-users/">Original</a>
    <h1>Who Owns, Operates, and Develops Your VPN Matters</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>I forgot to write a devlog for a while :)</p>
<p>I finished consulting, but then I got covid and was sick for a while. Now I&#39;m slowly getting back to working on zest, starting with figuring out namespaces/imports (<a href="https://github.com/jamii/zest/blob/main/docs/decisions/0001-namespaces-and-imports.md">design doc</a>).</p>
<p>Assorted writing:</p>
<ul>
<li><a href="https://www.opentech.fund/writing/store-tags-after-payloads/">Store tags after payloads</a>. Tiny musings on improving the space usage of sum types.</li>
<li><a href="https://www.opentech.fund/writing/go-allocation-probe/">Go allocation probe</a>. Using bpf uprobes to count allocations in a go program by type.</li>
<li><a href="https://www.opentech.fund/writing/everyones-got-one/">Everyones got one</a>. Inevitable opinioning on llms.</li>
</ul>
<h2 id="pprof-labelguns">pprof labelguns</h2>
<p>The go api for <a href="https://rakyll.org/profiler-labels/">pprof labels</a> is mostly footgun.</p>
<p>I wanted to label samples by customer id, so that when we&#39;re looking at a big spike in the profile in datadog we can tell which customer caused it.</p>
<p>The api lets you set labels on a goroutine, which is where the profiler reads them from, and also on a context. But you can only read labels from a context. So if you want to label a particular function call, you have to:</p>
<ul>
<li>read the labels from a context</li>
<li>add your label to label list</li>
<li>set the new label list on the goroutine</li>
<li>call the function</li>
<li>reset the old label list on the goroutine</li>
</ul>
<p>But what if the labels on the context don&#39;t match the labels that are currently on the goroutine? Or what if you don&#39;t have the matching context? Sucks to be you.</p>
<p>I made a note when writing that code that we&#39;ll have to be careful reviewing other uses of labels, because it&#39;s really easy to accidentally overwrite the label set. What&#39;s that on the mantelpiece? Oh, that&#39;s just my old footgun. Don&#39;t worry about it.</p>
<p>The labels work fine locally, but when we ship it and look at the profiles on datadog they&#39;re all messed up. Turns out that datadog themselvs, <a href="https://github.com/DataDog/dd-trace-go/blob/bec204b102b79e2b51329b4e89730566d2392640/ddtrace/tracer/tracer.go#L616-L622">in their own client library</a>, are erasing our labels whenever we report a span with no context.</p>
<pre data-lang="go"><code data-lang="go"><span>		</span><span>// For root span&#39;s without context, there is no pprofContext, but we need
</span><span>		</span><span>// one to avoid a panic() in pprof.WithLabels(). Using context.Background()
</span><span>		</span><span>// is not ideal here, as it will cause us to remove all labels from the
</span><span>		</span><span>// goroutine when the span finishes. However, the alternatives of not
</span><span>		</span><span>// applying labels for such spans or to leave the endpoint/hotspot labels
</span><span>		</span><span>// on the goroutine after it finishes are even less appealing. We&#39;ll have
</span><span>		</span><span>// to properly document this for users.
</span></code></pre>
<p>Spoiler alert - they did not document this for users.</p>
<p>But it&#39;s not really their fault. If go just provided a way to read labels from the goroutine, datadog would have used it to restore my labels.</p>
<h2 id="go-value-types">go value types</h2>
<p>C# and julia split types into reference types (always heap-allocated, passed by reference) and value types (may be allocated inline or on the stack, passed by value). Coming from rust, it&#39;s kind of annoying to have to decide in advance how each type is used, rather than having the flexibility to do different things in different situations.</p>
<p>So I expected to like having this flexibility in go. But after a few months of shooting myself in the foot, I think go is actually in a pretty bad point in the tradeoff space.</p>
<p>The first problem is that there is no equivalent to rust&#39;s <code>Copy</code> trait, so it&#39;s really easy to accidentally copy a big struct. This is bad enough when it&#39;s just a performance hit, but it also causes bugs when you accidentally mutate a copy of a thing instead of the original. This is exacerbated by the fact that most of the builtin apis default to pass-by-value eg:</p>
<pre data-lang="go"><code data-lang="go"><span>for </span><span>_</span><span>, </span><span>thing </span><span>:= </span><span>range </span><span>things {
</span><span>  </span><span>// If things has type []*Thing, this does what you expect.
</span><span>  </span><span>// If things has type []Thing, this is a no-op.
</span><span>  thing</span><span>.</span><span>mutate</span><span>()
</span><span>}
</span><span>
</span><span>// If things has type map[Key]*Thing, this does what you expect.
</span><span>// If things has type map[Key]Thing, this is a no-op.
</span><span>things[key]</span><span>.</span><span>mutate</span><span>()
</span></code></pre>
<p>The second problem is that you can&#39;t reliably stack-allocate a value and pass around references (because the reference might escape).</p>
<pre data-lang="go"><code data-lang="go"><span>thing </span><span>:= </span><span>Thing{}
</span><span>// This will probably force thing to be heap-allocated, depending on inlining and escape analysis.
</span><span>thing</span><span>.</span><span>mutate</span><span>()
</span></code></pre>
<p>The way people seem to deal with these problems is to adopt a coding style where, for each struct, either:</p>
<ul>
<li>The struct has a constructor that immediately heap-allocates it, and the struct is always passed by reference.</li>
<li>The struct is small, is usually not mutated, and is always passed by value.</li>
</ul>
<p>So we&#39;re back to the c#/julia world, except without the compiler support. There are linting tools that will catch some of these mistakes, but not all of them, and they produce false positives too.</p>
<p>In the codebase I was working on I found plenty of examples messing this up. Mistakes that wouldn&#39;t happen in either rust or c#/julia.</p>
<h2 id="go-perf-probe">go perf probe</h2>
<p>You can also use uprobe to create new perf events.</p>
<pre data-lang="bash"><code data-lang="bash"><span># Create an event that triggers when GetEntityForID is called.
</span><span>perf</span><span> probe</span><span> -x</span><span> ./run_snapshot_test</span><span> --add </span><span>&#39;GetEntityForID=github.com/runway/runway/api-server/app/calculator.(*RunwayCalculator).GetEntityForID&#39;
</span><span>
</span><span># Count the number of calls to GetEntityForID.
</span><span>perf</span><span> stat</span><span> -e</span><span> probe_run_snapshot_test:GetEntityForID
</span><span>
</span><span># Record a stacktrace every time GetEntityForID is called.
</span><span>perf</span><span> record</span><span> --call-graph</span><span> fp</span><span> -e</span><span> probe_run_snapshot_test:GetEntityForID
</span></code></pre>
<p>Using the <code>stat</code> is faster than adding a counter and recompiling, and the <code>record</code> is nice for functions which aren&#39;t called often enough to get accurate results from random sampling. Sadly <a href="https://github.com/KDAB/hotspot">hotspot</a> can&#39;t open the results of <code>record</code> though, even though it can open the results of <code>record</code> for builtin events.</p>
<p>I also tried to get <a href="https://lldb.llvm.org/use/intel_pt.html">processor trace</a> working, but intels viewer is painful to install on nixos and neither <a href="https://magic-trace.org/">magic-trace</a> nor <a href="https://github.com/michoecho/perf2perfetto">perf2perfetto</a> could succesfully translate a 1gb trace.</p>
<h2 id="tpde-a-fast-adaptable-compiler-back-end-framework"><a href="https://arxiv.org/pdf/2505.22610">TPDE: A Fast Adaptable Compiler Back-End Framework</a></h2>
<p>For compiling quickly, the compelling options are 1-2 pass compilers like baseline compilers in most js jits or template jits like <a href="https://en.wikipedia.org/wiki/Copy-and-patch">copy-and-patch</a>. Copy-and-patch doesn&#39;t allow doing any inter-operator optimizations or register allocation, so it tends to generate worse code (cf the cpython jit), but it has the advantage of requiring very little target-specific effort.</p>
<p>TPDE combines some of the advantages of both. It allows implementing most of your opcodes in C for portability, but it also does baseline-style optimizations and register allocation against the resulting llvm mir. As a bonus, it can also operate directly on your own internal ir rather than requiring a conversion pass.</p>
<p>It seems like a strict improvement on llvm o0 and is competitive with <a href="https://link.springer.com/content/pdf/10.1007/s00778-020-00643-4.pdf">DirectEmit</a> while easily supporting more platforms (I hear the arm backend written for <a href="https://db.in.tum.de/people/sites/gruber/p791-gruber.pdf?lang=en">this paper</a> ended up being too hard to merge). But their wasm backend seems barely competitive with the winch interpreter, which is hard to reconcile. Maybe a lot of the possible optimizations where already done when compiling to wasm, compared to the opportunities available in unoptimized llvm ir?</p>
<h2 id="anyblox-a-framework-for-self-decoding-datasets"><a href="https://gienieczko.com/anyblox-paper">AnyBlox: A Framework for Self-Decoding Datasets</a></h2>
<p>I love the idea for this. There have been a ton of improved data encodings coming out of both academic and industrial research. But everyone still uses parquet because everything supports parquet. The proposed solution is to standardize on a new data format where the decoding logic is embedded in the data itself as a small wasm binary, so that new encodings can be adopted without requiring everyone to add support for them.</p>
<p>Wasm support for simd isn&#39;t nearly as good as native, so the wasm version of REE is something like 4x slower to decode than the native version using avx512. They point at <a href="https://github.com/WebAssembly/flexible-vectors">flexible-vectors</a> as a potential solution but that proposal doesn&#39;t seem to be moving at the moment.</p>
<p>By far the biggest weakness though is that they don&#39;t support filter pushdown. That would require standardizing an expression language across multiple query engines, which seems unlikely. (They can&#39;t just write the expressions in wasm because different encodings will shortcut the filter expression in different ways - we need an abstract representation.)</p>
<h2 id="books">books</h2>
<p><a href="https://www.goodreads.com/book/show/197716282-the-unaccountability-machine">The unaccountability machine</a>. Part history of cybernetics, part cheerleading. The history was interesting, but I found the actual cybernetics ideas to be vague to the point of being unfalsifiable. Something something feedback loops.</p>
<p><a href="https://www.goodreads.com/book/show/242472.The_Black_Swan">The black swan</a>. I liked the basic idea of noticing that in some areas results are mostly dictated by out-of-distribution events and so you can&#39;t really protect yourself there by forecasting everything as a normal distribution. Adam Mastroianni&#39;s idea of <a href="https://www.experimental-history.com/p/science-is-a-strong-link-problem">strong-link problems</a> seems closely related, and also didn&#39;t get padded into a whole book so it&#39;s much easier to read.</p>
<p><a href="https://www.goodreads.com/book/show/145624879-change-your-diet-change-your-mind">Change your diet, change your mind</a>. The basic premise of nutritional pyschiatry - that some mental illnesses might be caused by, or at least exarcarbated by, metabolic illnesses - is really exciting. There is a plausible mechanism - insulin resistance prevents insulin from crossing the blood-brain barrier, preventing your brain cells from burning glucose - and it would explain why mental illnesses have soared over the last few decades and also provide an avenue for actual treatment rather than just ameliorating the symptoms. Unfortunately this book reads less like a new area of research and more like a cross between a fad diet book and one of those psychiatry books where every single patient is enlightened in a single conversation. Not the most trustworthy overview.</p>
<p><a href="https://www.goodreads.com/book/show/27259680-the-multivitamin-lie">The multivitamin lie</a>. This probably could have been just a blog post, but it&#39;s at least a convincing argument. Big diet surveys in the US show that almost everyone has enough of every micronutrient in their diet. Big blood tests show that most people have no deficiencies, and the exceptions are in a few specific nutrients that you would be better off testing and treating individually (eg iron/calcium in women, vitamin b12 in vegans, vitamin d in general). In high-powered epidemiological experiments we don&#39;t see multivitamin intake cause any reduction in illness or all-cause mortality. All micronutrient stores take weeks to months to deplete and your body upregulates absorbtion for anything you are low on, so your diet only needs to be balanced on the scale of a week or so rather than each day. Finally, the author puts together several week-long diet plans using typical western foods that hit all the RDAs in only 1350 calories per day, leaving a lot of slack for junk food.</p>
<p><a href="https://www.goodreads.com/book/show/230403458-antimemetics">Antimemetics</a>. Incredibly vague - antimemes seem to include ideas that are boring, ideas that are exciting but not immediately actionable, ideas that are dangerous, ideas that are unpolite to talk about in public, ideas that are actively suppressed by authorities, and anything that is discussed in a private chat because town-square social media is incapable of nuance. I didn&#39;t find any explanatory power here. Also the author says that her previous book <a href="https://www.goodreads.com/book/show/54140556-working-in-public">working in public</a>, which I thought was pretty insightful about the realities of maintaining a small open-source project, was actually a covert critique of democracy. Which would make sense if the country was run by three people in their spare time and strangers kept barging into their office and throwing half-completed paperwork at them. But if we look at more complex projects like linux, which is surely still much less complex than running an entire country, we see a big assortment of long-term expert maintainers divided into different (occasionally squabbling) departments, and a foundation that dictates funding and provides feedback/direction from stakeholders. Looks awfully like a civil service and an elected executive branch. And any big company has a similar split into employees and board, who are elected by shareholders. It seems like the structure we always land on for large institutions.</p>
<p>Kind of a disappointing summer as far as non-fiction goes.</p>

</article></div>
  </body>
</html>
