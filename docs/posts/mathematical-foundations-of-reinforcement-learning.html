<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning">Original</a>
    <h1>Mathematical Foundations of Reinforcement Learning</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This textbook has received 5,000+ stars! Glad that it is helpful to many readers.</p>

<p dir="auto"><a href="https://youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;si=B6mRR7vxBAjRAm_F" rel="nofollow"><img src="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/raw/main/Figure_EnglishLectureVideo.png" alt=""/></a></p>
<p dir="auto"><strong>My English open course is online now.</strong> You can click the above figure or the <a href="https://youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;si=D1T4pcyHsMxj6CzB" rel="nofollow">link here</a> to jump to our YouTube channel. You can also click the following links to be directed to specific lecture videos.</p>
<ul dir="auto">
<li><a href="https://www.youtube.com/watch?v=ZHMWHr9811U&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=1" rel="nofollow">Overview of Reinforcement Learning in 30 Minutes</a></li>
<li><a href="https://www.youtube.com/watch?v=zJHtM5dN69g&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=2" rel="nofollow">L1: Basic Concepts (P1-State, action, policy, ...)</a></li>
<li><a href="https://www.youtube.com/watch?v=repVl3_GYCI&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=3" rel="nofollow">L1: Basic Concepts (P2-Reward,return, Markov decision process)</a></li>
<li><a href="https://www.youtube.com/watch?v=XCzWrlgZCwc&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=4" rel="nofollow">L2: Bellman Equation (P1-Motivating examples)</a></li>
<li><a href="https://www.youtube.com/watch?v=DSvi3xEN13I&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=5" rel="nofollow">L2: Bellman Equation (P2-State value)</a></li>
<li><a href="https://www.youtube.com/watch?v=eNtId8yPWkA&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=6" rel="nofollow">L2: Bellman Equation (P3-Bellman equation-Derivation)</a></li>
<li><a href="https://www.youtube.com/watch?v=EtCfBG_eP2w&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=7" rel="nofollow">L2: Bellman Equation (P4-Matrix-vector form and solution)</a></li>
<li><a href="https://www.youtube.com/watch?v=zJo2sLDzfcU&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=8" rel="nofollow">L2: Bellman Equation (P5-Action value)</a></li>
<li><a href="https://www.youtube.com/watch?v=lXKY_Hyg4SQ&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=9" rel="nofollow">L3: Bellman Optimality Equation (P1-Motivating example)</a></li>
<li><a href="https://www.youtube.com/watch?v=BxyjdHhK8a8&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=10" rel="nofollow">L3: Bellman Optimality Equation (P2-Optimal policy)</a></li>
<li><a href="https://www.youtube.com/watch?v=FXftTCKotC8&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=11" rel="nofollow">L3: Bellman Optimality Equation (P3-More on BOE)</a></li>
<li><a href="https://www.youtube.com/watch?v=a--bck2ow9s&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=12" rel="nofollow">L3: Bellman Optimality Equation (P4-Interesting properties)</a></li>
<li><a href="https://www.youtube.com/watch?v=wMAVmLDIvQU&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=13" rel="nofollow">L4: Value Iteration and Policy Iteration (P1-Value iteration)</a></li>
<li><a href="https://www.youtube.com/watch?v=Pka6Om0nYQ8&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=14" rel="nofollow">L4: Value Iteration and Policy Iteration (P2-Policy iteration)</a></li>
<li><a href="https://www.youtube.com/watch?v=tUjPFPD3Vc8&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=15" rel="nofollow">L4: Value Iteration and Policy Iteration (P3-Truncated policy iteration)</a></li>
<li><a href="https://www.youtube.com/watch?v=DO1yXinAV_Q&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=16" rel="nofollow">L5: Monte Carlo Learning (P1-Motivating examples)</a></li>
<li><a href="https://www.youtube.com/watch?v=6ShisunU0zs&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=17" rel="nofollow">L5: Monte Carlo Learning (P2-MC Basic-introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=axA0yns9FxU&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=18" rel="nofollow">L5: Monte Carlo Learning (P3-MC Basic-examples)</a></li>
<li><a href="https://www.youtube.com/watch?v=Qt8OMHPkLqg&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=19" rel="nofollow">L5: Monte Carlo Learning (P4-MC Exploring Starts)</a></li>
<li><a href="https://www.youtube.com/watch?v=dM3fYE630pY&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=20" rel="nofollow">L5: Monte Carlo Learning (P5-MC Epsilon-Greedy-introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=x6X_5ePT9gQ&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=21" rel="nofollow">L5: Monte Carlo Learning (P6-MC Epsilon-Greedy-examples)</a></li>
<li><a href="https://www.youtube.com/watch?v=1bMgejvWoAo&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=22" rel="nofollow">L6: Stochastic Approximation and SGD (P1-Motivating example)</a></li>
<li><a href="https://www.youtube.com/watch?v=1FTGcNUUnCE&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=23" rel="nofollow">L6: Stochastic Approximation and SGD (P2-RM algorithm: introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=juNDoAFEre4&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=24" rel="nofollow">L6: Stochastic Approximation and SGD (P3-RM algorithm: convergence)</a></li>
<li><a href="https://www.youtube.com/watch?v=EZO7Iadp5m4&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=25" rel="nofollow">L6: Stochastic Approximation and SGD (P4-SGD algorithm: introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=BsxU_4qvvNA&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=26" rel="nofollow">L6: Stochastic Approximation and SGD (P5-SGD algorithm: examples)</a></li>
<li><a href="https://www.youtube.com/watch?v=fWxX9YuEHjE&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=27" rel="nofollow">L6: Stochastic Approximation and SGD (P6-SGD algorithm: properties)</a></li>
<li><a href="https://www.youtube.com/watch?v=yNEV2cLKuzU&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=28" rel="nofollow">L6: Stochastic Approximation and SGD (P7-SGD algorithm: comparison)</a></li>
<li><a href="https://www.youtube.com/watch?v=u1X-7XX3dtI&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=29" rel="nofollow">L7: Temporal-Difference Learning (P1-Motivating example)</a></li>
<li><a href="https://www.youtube.com/watch?v=XiCUsc7CCE0&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=30" rel="nofollow">L7: Temporal-Difference Learning (P2-TD algorithm: introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=faWg8M91-Oo&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=31" rel="nofollow">L7: Temporal-Difference Learning (P3-TD algorithm: convergence)</a></li>
<li><a href="https://www.youtube.com/watch?v=jYwQufkBUPo&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=32" rel="nofollow">L7: Temporal-Difference Learning (P4-Sarsa)</a></li>
<li><a href="https://www.youtube.com/watch?v=0kKzQbWZOlk&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=33" rel="nofollow">L7: Temporal-Difference Learning (P5-Expected Sarsa &amp; n-step Sarsa)</a></li>
<li><a href="https://www.youtube.com/watch?v=4BvYR2hm730&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=34" rel="nofollow">L7: Temporal-Difference Learning (P6-Q-learning: introduction)</a></li>
<li><a href="https://www.youtube.com/watch?v=I0YhlOIFF4s&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=35" rel="nofollow">L7: Temporal-Difference Learning (P7-Q-learning: pseudo code)</a></li>
<li><a href="https://www.youtube.com/watch?v=3t74lvk1GBM&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=36" rel="nofollow">L7: Temporal-Difference Learning (P8-Unified viewpoint and summary)</a></li>
<li><a href="https://www.youtube.com/watch?v=uJXcI8fcdWc&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=37" rel="nofollow">L8: Value Function Approximation (P1-Motivating example–curve fitting)</a></li>
<li><a href="https://www.youtube.com/watch?v=Z3HI1TfpJP0&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=38" rel="nofollow">L8: Value Function Approximation (P2-Objective function)</a></li>
<li><a href="https://www.youtube.com/watch?v=piBDwrKt0uU&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=39" rel="nofollow">L8: Value Function Approximation (P3-Optimization algorithm)</a></li>
<li><a href="https://www.youtube.com/watch?v=VFyBNEZxMMs&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=40" rel="nofollow">L8: Value Function Approximation (P4-illustrative examples and analysis)</a></li>
<li><a href="https://www.youtube.com/watch?v=C-HtY4-W_zw&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=41" rel="nofollow">L8: Value Function Approximation (P5-Sarsa and Q-learning)</a></li>
<li><a href="https://www.youtube.com/watch?v=lZCcbZbqVSQ&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=42" rel="nofollow">L8: Value Function Approximation (P6-DQN–basic idea)</a></li>
<li><a href="https://www.youtube.com/watch?v=rynEdAdebi0&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=43" rel="nofollow">L8: Value Function Approximation (P7-DQN–experience replay)</a></li>
<li><a href="https://www.youtube.com/watch?v=vQHuCHjd6hA&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=44" rel="nofollow">L8: Value Function Approximation (P8-DQN–implementation and example)</a></li>
<li><a href="https://www.youtube.com/watch?v=mtFHOj83QSo&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=45" rel="nofollow">L9: Policy Gradient Methods (P1-Basic idea)</a></li>
<li><a href="https://www.youtube.com/watch?v=la8jQc3hX1M&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=46" rel="nofollow">L9: Policy Gradient Methods (P2-Metric 1–Average value)</a></li>
<li><a href="https://www.youtube.com/watch?v=8RZ_rQFe69E&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=47" rel="nofollow">L9: Policy Gradient Methods (P3-Metric 2–Average reward)</a></li>
<li><a href="https://www.youtube.com/watch?v=MvmtPXur3Ls&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=48" rel="nofollow">L9: Policy Gradient Methods (P4-Gradients of the metrics)</a></li>
<li><a href="https://www.youtube.com/watch?v=1DQnnUC8ng8&amp;list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8&amp;index=49" rel="nofollow">L9: Policy Gradient Methods (P5-Gradient-based algorithms &amp; REINFORCE)</a></li>
<li>The lecture videos of the last chapter (Chapter 10) will be uploaded shortly. Please stay tuned!</li>
</ul>
<p dir="auto">You are warmly welcome to check out the English videos to see if they are helpful!</p>
<hr/>
<hr/>

<p dir="auto">This book aims to provide a <strong>mathematical but friendly</strong> introduction to the fundamental concepts, basic problems, and classic algorithms in reinforcement learning. Some essential features of this book are highlighted as follows.</p>
<ul dir="auto">
<li>
<p dir="auto">The book introduces reinforcement learning from a mathematical point of view. Hopefully, readers will not only know the procedure of an algorithm but also understand why it was designed in the first place and why it works effectively.</p>
</li>
<li>
<p dir="auto">The depth of the mathematics is carefully controlled to an adequate level. The mathematics is also presented in a carefully designed manner to ensure that the book is friendly to read. Readers can selectively read the materials presented in gray boxes according to their interests.</p>
</li>
<li>
<p dir="auto">Many illustrative examples are given to help readers better understand the topics. All the examples in this book are based on a grid world task, which is easy to understand and helpful for illustrating concepts and algorithms.</p>
</li>
<li>
<p dir="auto">When introducing an algorithm, the book aims to separate its core idea from complications that may be distracting. In this way, readers can better grasp the core idea of an algorithm.</p>
</li>
<li>
<p dir="auto">The contents of the book are coherently organized. Each chapter is built based on the preceding chapter and lays a necessary foundation for the subsequent one.</p>
</li>
</ul>
<p dir="auto"><a href="https://link.springer.com/book/9789819739431" rel="nofollow"><img src="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/raw/main/springerBookCover.png" alt="Book cover"/></a></p>

<p dir="auto">The topics addressed in the book are shown in the figure below. This book contains ten chapters, which can be classified into two parts: the first part is about basic tools, and the second part is about algorithms. The ten chapters are highly correlated. In general, it is necessary to study the earlier chapters first before the later ones.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/blob/main/Figure_chapterMap.png"><img src="https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/raw/main/Figure_chapterMap.png" alt="The map of this book"/></a></p>

<p dir="auto">This book is designed for senior undergraduate students, graduate students, researchers, and practitioners interested in reinforcement learning.</p>
<p dir="auto">It does not require readers to have any background in reinforcement learning because it starts by introducing the most basic concepts. If the reader already has some background in reinforcement learning, I believe the book can help them understand some topics more deeply or provide different perspectives.</p>
<p dir="auto">This book, however, requires the reader to have some knowledge of probability theory and linear algebra. Some basics of the required mathematics are also included in the appendix of this book.</p>

<p dir="auto">By combining the book with my lecture videos, I believe you can study better.</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Chinese lecture videos:</strong> You can check the <a href="https://space.bilibili.com/2044042934" rel="nofollow">Bilibili channel</a> or the <a href="https://www.youtube.com/channel/UCztGtS5YYiNv8x3pj9hLVgg/playlists" rel="nofollow">Youtube channel</a>.
The lecture videos have received <strong>1,300,000+ views</strong> up to Feb 2025 over the Internet and received very good feedback!</p>
</li>
<li>
<p dir="auto"><strong>English lecture videos:</strong> The English lecture videos have been uploaded to YouTube. Please see the links and details in another part of this document.</p>
</li>
</ul>

<p dir="auto">You can find my info on my homepage <a href="https://www.shiyuzhao.net/" rel="nofollow">https://www.shiyuzhao.net/</a> (GoogleSite) and my research group website <a href="https://shiyuzhao.westlake.edu.cn" rel="nofollow">https://shiyuzhao.westlake.edu.cn</a></p>
<p dir="auto">I have been teaching a graduate-level course on reinforcement learning since 2019. Along with teaching, I have been preparing this book as the lecture notes for my students.</p>
<p dir="auto">I sincerely hope this book can help readers smoothly enter the exciting field of reinforcement learning.</p>

<div data-snippet-clipboard-copy-content="@book{zhao2025RLBook,
  title={Mathematical Foundations of Reinforcement Learning},
  author={S. Zhao},
  year={2025},
  publisher={Springer Nature Press and Tsinghua University Press}
}"><pre><code>@book{zhao2025RLBook,
  title={Mathematical Foundations of Reinforcement Learning},
  author={S. Zhao},
  year={2025},
  publisher={Springer Nature Press and Tsinghua University Press}
}
</code></pre></div>

<p dir="auto">Many enthusiastic readers sent me the source code or notes that they developed when they studied this book. If you create any materials based on course, you are welcome to write an email. I am happy to share the links here and hope they may be helpful to other readers. I must emphasize that I have not verified the code. If you have any questions, you can directly contact the developers.</p>
<p dir="auto"><strong>Code</strong></p>
<p dir="auto"><em>Python:</em></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/10-OASIS-01/minrl">https://github.com/10-OASIS-01/minrl</a> (Feb 2025)</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning">https://github.com/SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning</a>  (by Zehong Cao, Aug 2024)</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/ziwenhahaha/Code-of-RL-Beginning">https://github.com/ziwenhahaha/Code-of-RL-Beginning</a> by RLGamer (Mar 2024)</p>
<ul dir="auto">
<li>Videos for code explanation: <a href="https://www.bilibili.com/video/BV1fW421w7NH" rel="nofollow">https://www.bilibili.com/video/BV1fW421w7NH</a></li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://github.com/jwk1rose/RL_Learning">https://github.com/jwk1rose/RL_Learning</a> by Wenkang Ji (Feb 2024)</p>
</li>
</ul>
<p dir="auto"><em>R:</em></p>
<ul dir="auto">
<li><a href="https://github.com/NewbieToEverything/Code-Mathmatical-Foundation-of-Reinforcement-Learning">https://github.com/NewbieToEverything/Code-Mathmatical-Foundation-of-Reinforcement-Learning</a></li>
</ul>
<p dir="auto"><em>C++:</em></p>
<ul dir="auto">
<li><a href="https://github.com/purundong/test_rl">https://github.com/purundong/test_rl</a></li>
</ul>
<p dir="auto"><strong>Study notes</strong></p>
<p dir="auto"><em>English:</em></p>
<ul dir="auto">
<li><a href="https://lyk-love.cn/tags/reinforcement-learning/" rel="nofollow">https://lyk-love.cn/tags/reinforcement-learning/</a>
by a graduate student from UC Davis</li>
</ul>
<p dir="auto"><em>Chinese:</em></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://zhuanlan.zhihu.com/p/692207843" rel="nofollow">https://zhuanlan.zhihu.com/p/692207843</a></p>
</li>
<li>
<p dir="auto"><a href="https://blog.csdn.net/qq_64671439/category_12540921.html" rel="nofollow">https://blog.csdn.net/qq_64671439/category_12540921.html</a></p>
</li>
<li>
<p dir="auto"><a href="http://t.csdnimg.cn/EH4rj" rel="nofollow">http://t.csdnimg.cn/EH4rj</a></p>
</li>
<li>
<p dir="auto"><a href="https://blog.csdn.net/LvGreat/article/details/135454738" rel="nofollow">https://blog.csdn.net/LvGreat/article/details/135454738</a></p>
</li>
<li>
<p dir="auto"><a href="https://xinzhe.blog.csdn.net/article/details/129452000" rel="nofollow">https://xinzhe.blog.csdn.net/article/details/129452000</a></p>
</li>
<li>
<p dir="auto"><a href="https://blog.csdn.net/v20000727/article/details/136870879?spm=1001.2014.3001.5502" rel="nofollow">https://blog.csdn.net/v20000727/article/details/136870879?spm=1001.2014.3001.5502</a></p>
</li>
<li>
<p dir="auto"><a href="https://blog.csdn.net/m0_64952374/category_12883361.html" rel="nofollow">https://blog.csdn.net/m0_64952374/category_12883361.html</a></p>
</li>
</ul>
<p dir="auto">There are also many others notes made by many other readers on the Internet. I am not able to put them all here. You are welcome to recommend to me if you find a good one.</p>
<p dir="auto"><strong>Bilibili videos made based on my course</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://www.bilibili.com/video/BV1fW421w7NH" rel="nofollow">https://www.bilibili.com/video/BV1fW421w7NH</a></p>
</li>
<li>
<p dir="auto"><a href="https://www.bilibili.com/video/BV1Ne411m7GX" rel="nofollow">https://www.bilibili.com/video/BV1Ne411m7GX</a></p>
</li>
<li>
<p dir="auto"><a href="https://www.bilibili.com/video/BV1HX4y1H7uR" rel="nofollow">https://www.bilibili.com/video/BV1HX4y1H7uR</a></p>
</li>
<li>
<p dir="auto"><a href="https://www.bilibili.com/video/BV1TgzsYDEnP" rel="nofollow">https://www.bilibili.com/video/BV1TgzsYDEnP</a></p>
</li>
<li>
<p dir="auto"><a href="https://www.bilibili.com/video/BV1CQ4y1J7zu" rel="nofollow">https://www.bilibili.com/video/BV1CQ4y1J7zu</a></p>
</li>
</ul>

<p dir="auto"><strong>(Dec 2024) 4,000+ stars</strong>
This textbook has received 4,000+ stars! Glad that it is helpful to many readers.</p>
<p dir="auto"><strong>(Oct 2024) Book cover</strong></p>
<p dir="auto">The design of the book cover is finished. The book will be officially published by Springer early next year. It has been published by Tsinghua University Press.</p>
<p dir="auto"><strong>(Sep 2024) Minor update before printing by Springer</strong></p>
<p dir="auto">I revised some very minor places that readers may hardly notice. It is supposed to be the final version before printing by Springer.</p>
<p dir="auto"><strong>(Aug 2024) 3000 Stars and more code</strong></p>
<p dir="auto">The book has received 3000+ stars, which is a great achievement to me. Thanks to everyone. Hope it really helped you.</p>
<p dir="auto">I also received more code implementation from enthusiastic readers. For example, this <a href="https://github.com/SupermanCaozh/The_Coding_Foundation_in_Reinforcement_Learning">GitHub page</a> provided Python implementation of almost all examples in my book. On the one hand, I am very glad to see that. On the other hand, I am a little worried that my students in my offline class may use the code to do their homework:-). Overall, I am happy because it indicates that the book and open course are really helpful to the readers; Otherwise, they would not bother to develop the code by themselves:-)</p>
<p dir="auto"><strong>(Jun 2024) Minor update before printing</strong></p>
<p dir="auto">This is the fourth version of the book draft. It is supposed to be the final one before the book is officially published. Specifically, when proofreading the book manuscript, I detected some very minor issues. Together with some reported by enthusiastic readers, they have been revised in this version.</p>
<p dir="auto"><strong>(Apr 2024) Code for the Grid-World Environment</strong></p>
<p dir="auto">We added the code for the grid-world environment in my book. Interested readers can develop and test their own algorithms in this environment. Both Python and MATLAB versions are provided.</p>
<p dir="auto">Please note that we do not provide the code of all the algorithms involved in the book. That is because they are the homework for the students in offline teaching: the students need to develop their own algorithms using the provided environment. Nevertheless, there are third-party implementations of some algorithms. Interested readers can check the links on the home page of the book.</p>
<p dir="auto">I need to thank my PhD students, Yize Mi and Jianan Li, who are also the Teaching Assistants of my offline teaching. They contributed greatly to the code.</p>
<p dir="auto">You are welcome to provide any feedback about the code such as bugs if detected.</p>
<p dir="auto"><strong>(Mar 2024) 2K stars</strong></p>
<p dir="auto">The book has received 2K stars. I also received many positive evaluations of the book from many readers. Very glad that it can be helpful.</p>
<p dir="auto"><strong>(Mar 2024) Minor update</strong></p>
<p dir="auto">The third version of the draft of the book is online now.</p>
<p dir="auto">Compared to the second version, the third version is improved in the sense that some minor typos have been corrected. Here, I would like to thank the readers who sent me their feedback.</p>
<p dir="auto"><strong>(Sep 2023) 1000+ stars</strong></p>
<p dir="auto">The book received 1000+ stars! Thank everybody!</p>
<p dir="auto"><strong>(Aug 2023) Major update - second version</strong></p>
<p dir="auto"><em>The second version of the draft of the book is online now!!</em></p>
<p dir="auto">Compared to the first version, which was online one year ago, the second version has been improved in various ways. For example, we replotted most of the figures, reorganized some contents to make them clearer, corrected some typos, and added Chapter 10, which was not included in the first version.</p>
<p dir="auto">I put the first draft of this book online in August 2022. Up to now, I have received valuable feedback from many readers worldwide. I want to express my gratitude to these readers.</p>
<p dir="auto"><strong>(Nov 2022) Will be jointly published</strong></p>
<p dir="auto">This book will be published <em>jointly by Springer Nature and Tsinghua University Press</em>. It will probably be printed in the second half of 2023.</p>
<p dir="auto">I have received some comments and suggestions about this book from some readers. Thanks a lot, and I appreciate it. I am still collecting feedback and will probably revise the draft in several months. Your feedback can make this book more helpful for other readers!</p>
<p dir="auto"><strong>(Oct 2022) Lecture notes and vidoes</strong></p>
<p dir="auto">The <em>lecture slides</em> have been uploaded in the folder &#34;Lecture slides.&#34;</p>
<p dir="auto">The <em>lecture videos</em> (in Chinese) are online. Please check our Bilibili channel <a href="https://space.bilibili.com/2044042934" rel="nofollow">https://space.bilibili.com/2044042934</a> or the Youtube channel <a href="https://www.youtube.com/channel/UCztGtS5YYiNv8x3pj9hLVgg/playlists" rel="nofollow">https://www.youtube.com/channel/UCztGtS5YYiNv8x3pj9hLVgg/playlists</a></p>
<p dir="auto"><strong>(Aug 2022) First draft</strong></p>
<p dir="auto">The first draft of the book is online.</p>
</article></div></div>
  </body>
</html>
