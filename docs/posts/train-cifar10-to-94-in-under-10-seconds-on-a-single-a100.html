<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/tysam-code/hlb-CIFAR10">Original</a>
    <h1>Train CIFAR10 to 94% in under 10 seconds on a single A100</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 dir="auto"><a id="user-content-cifar10-hyperlightspeedbench" aria-hidden="true" href="#cifar10-hyperlightspeedbench"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>CIFAR10 hyperlightspeedbench</h2>
<p dir="auto">Welcome to the hyperlightspeedbench CIFAR-10 (HLB-CIFAR10) repo.</p>
<h3 dir="auto"><a id="user-content-how-to-run" aria-hidden="true" href="#how-to-run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>How to Run</h3>
<p dir="auto"><code>git clone https://github.com/tysam-code/hlb-CIFAR10 &amp;&amp; cd hlb-CIFAR10 &amp;&amp; python -m pip install -r requirements.txt &amp;&amp; python main.py</code></p>
<p dir="auto">If you&#39;re curious, this code is generally Colab friendly (in fact -- most of this was developed in Colab!). Just be sure to uncomment the reset block at the top of the code.</p>
<h3 dir="auto"><a id="user-content-main" aria-hidden="true" href="#main"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Main</h3>
<p dir="auto">Goals:</p>
<ul dir="auto">
<li>minimalistic</li>
<li>beginner-friendly</li>
<li>torch- and python-idiomatic</li>
<li>hackable</li>
<li>few external dependencies (currently only torch and torchvision)</li>
<li>~world-record single-GPU training time (this repo holds the current world record at ~&lt;10 seconds on an A100, down from ~18.1 seconds originally).</li>
<li>&lt;2 seconds training time in &lt;2 years (yep!)</li>
</ul>
<p dir="auto">This is a neural network implementation of a very speedily-training network that originally started as a painstaking reproduction of <a href="https://myrtle.ai/learn/how-to-train-your-resnet/" rel="nofollow">David Page&#39;s original ultra-fast CIFAR-10 implementation on a single GPU</a>, but written nearly from the ground-up to be extremely rapid-experimentation-friendly. Part of the benefit of this is that we now hold the world record for single GPU training speeds on CIFAR10 (under 10 seconds on an A100!!!)</p>
<p dir="auto">What we&#39;ve added:</p>
<ul dir="auto">
<li>squeeze and excite layers</li>
<li>way too much hyperparameter tuning</li>
<li>miscellaneous architecture trimmings (see the patch notes)</li>
<li>memory format changes (and more!) to better use tensor cores/etc</li>
<li>and more!</li>
</ul>
<p dir="auto">This code, in comparison to David&#39;s original code, is in a single file and extremely flat, but is not as durable for long-term production-level bug maintenance. You&#39;re meant to check out a fresh repo whenever you have a new idea. It is excellent for rapid idea exploring -- almost everywhere in the pipeline is exposed and built to be user-friendly. I truly enjoy personally using this code, and hope you do as well! :D Please let me know if you have any feedback. I hope to continue publishing updates to this in the future, so your support is encouraged. Share this repo with someone you know that might like it!</p>
<p dir="auto">Feel free to check out my<a href="https://www.patreon.com/user/posts?u=83632131" rel="nofollow">Patreon</a> if you like what I&#39;m doing here and want more!. Additionally, if you want me to work up to a part-time amount of hours with you, feel free to reach out to me at <a href="mailto:hire.tysam@gmail.com">hire.tysam@gmail.com</a>. I&#39;d love to hear from you.</p>
<h3 dir="auto"><a id="user-content-known-bugs" aria-hidden="true" href="#known-bugs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Known Bugs</h3>
<p dir="auto">The Colab-specific code is commented out at the top, and the timing/performance table reprints the entire table instead of appropriately updating in-place each epoch.</p>
<h3 dir="auto"><a id="user-content-why-a-convnet-still-why-cifar10-arent-transformers-the-new-thing-now" aria-hidden="true" href="#why-a-convnet-still-why-cifar10-arent-transformers-the-new-thing-now"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Why a ConvNet Still? Why CIFAR10? Aren&#39;t Transformers the New Thing Now?</h3>
<p dir="auto">Transformers are indeed the new thing, but I personally believe that the way information condenses from a training set into a neural network will still practically always follow the same underlying set of mathematical principles. The goal for this codebase is to get training in under two (2) seconds within a year or two (2), and under one (1) seconds within 4-5 years. This should allow for some very interesting scaled experiments for different techniques on a different kind of level. I have a rough path planned down to about 2-3 seconds of training or so, all things working out as they should. It will likely get very, very difficult beyond that point.</p>
<p dir="auto">Basically -- the information gained from experimenting with a technique here should translate in some kind of a way. No need to scale up size arbitrarily when looking to suss out the basics of certain applied mathematical concepts for a problem.</p>
<h3 dir="auto"><a id="user-content-submissions" aria-hidden="true" href="#submissions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Submissions</h3>
<p dir="auto">Currently, submissions to this codebase as a benchmark are closed as we figure out the level of interest, how to track disparate entries, etc. Feel free to open an issue if you have thoughts on this!</p>
<h4 dir="auto"><a id="user-content-bugs--etc" aria-hidden="true" href="#bugs--etc"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bugs &amp; Etc.</h4>
<p dir="auto">If you find a bug, open an issue! L:D If you have a success story, let me know! It helps me understand what works and doesn&#39;t more than you might expect -- if I know how this is specifically helping people, that can help me further improve as a developer, as I can keep that in mind when developing other software for people in the future. :D :)</p>
</article>
          </div></div>
  </body>
</html>
