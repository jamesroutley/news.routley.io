<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://avi.im/blag/2024/s3-log/">Original</a>
    <h1>Building a distributed log using S3 (under 150 lines of Go)</h1>
    
    <div id="readability-page-1" class="page"><div><article><p>I will show how we can implement a durable, distributed, and highly available log using S3. This post is the third part in the series:</p><ol><li><a href="https://avi.im/blag/2024/disaggregated-storage/">Disaggregated Storage - a brief introduction</a></li><li><a href="https://avi.im/blag/2024/zero-disk-architecture/">Zero Disk Architecture</a></li><li>Building a distributed log using S3</li></ol><p>tl;dr The code is open source, comes with tests and open issues to contribute: <a href="https://github.com/avinassh/s3-log">s3-log</a></p><h2 id="log">Log</h2><img src="https://avi.im/blag/images/2024/log.svg" alt="log"/><p>I love logs. The log is the heart of data and event streaming systems. A database is a log. Kafka is a log. Simply put, it’s an ordered collection of records. The log is append-only, and once records are written, they are immutable. Each inserted record gets a unique, sequentially increasing identifier.</p><p>Log is a powerful storage abstraction. Using a log, you can build a database, message queue, or an event streaming system. If you would like to learn more, read this excellent blog post by Jay Kreps, the creator of Kafka: <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Log: What every software engineer should know about real-time data’s unifying abstraction</a>.</p><h2 id="why-s3">Why S3?</h2><p>In my previous post, I explained the benefits of <a href="https://avi.im/blag/2024/zero-disk-architecture/">Zero Disk Architecture</a>. A log on S3 is attractive for several reasons:</p><ol><li>No disks, so it is elastic and scalable.</li><li>We don’t have to roll our own distributed storage server. We get durability, availability, and replication for free just by using S3.</li><li>No operational overhead.</li><li>Cost. Systems like <a href="https://www.warpstream.com/bring-your-own-cloud-kafka-data-streaming">WarpStream</a> and <a href="https://buf.build/blog/bufstream-kafka-lower-cost">BufStream</a> claim to be 10x cheaper than Kafka.</li><li>Customers and enterprises love BYOC! You make $$$</li></ol><h2 id="the-log-interface">The Log Interface</h2><div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>Record</span> <span>struct</span> {
</span></span><span><span>	<span>Offset</span> <span>uint64</span>
</span></span><span><span>	<span>Data</span>   []<span>byte</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>type</span> <span>WAL</span> <span>interface</span> {
</span></span><span><span>	<span>Append</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>data</span> []<span>byte</span>) (<span>uint64</span>, <span>error</span>)
</span></span><span><span>	<span>Read</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>offset</span> <span>uint64</span>) (<span>Record</span>, <span>error</span>)
</span></span><span><span>}
</span></span></code></pre></div><p>We will write each payload as an object in S3 and ensure it gets a unique offset in the log. We need to make sure that record numbers are unique and sequentially increasing.</p><img src="https://avi.im/blag/images/2024/s3-log.svg" alt="s3 log"/><h3 id="append">Append</h3><p>The only ‘write’ operation we can do on a log is <code>Append</code>. Append takes a bunch of bytes and writes them to the end of the log. It returns the offset, which is the position of this record in the log.</p><p>Let’s define a struct that maintains a counter <code>length</code>. Every time we insert, we will increment this counter by one.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>S3WAL</span> <span>struct</span> {
</span></span><span><span>	<span>client</span>     <span>*</span><span>s3</span>.<span>Client</span>
</span></span><span><span>	<span>bucketName</span> <span>string</span>
</span></span><span><span>	<span>length</span>     <span>uint64</span>
</span></span><span><span>}
</span></span></code></pre></div><p>The very first record will have offset <code>0000000001</code>. For every new object we insert in the S3 bucket, we will increment it by one. Once a record is inserted, we will return its offset to the caller.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>w</span> <span>*</span><span>S3WAL</span>) <span>Append</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>data</span> []<span>byte</span>) (<span>uint64</span>, <span>error</span>) {
</span></span><span><span>	<span>nextOffset</span> <span>:=</span> <span>w</span>.<span>length</span> <span>+</span> <span>1</span>
</span></span><span><span>
</span></span><span><span>	<span>input</span> <span>:=</span> <span>&amp;</span><span>s3</span>.<span>PutObjectInput</span>{
</span></span><span><span>		<span>Bucket</span>:      <span>aws</span>.<span>String</span>(<span>w</span>.<span>bucketName</span>),
</span></span><span><span>		<span>Key</span>:         <span>aws</span>.<span>String</span>(<span>fmt</span>.<span>Sprintf</span>(<span>&#34;%020d&#34;</span>, <span>nextOffset</span>)),
</span></span><span><span>		<span>Body</span>:        <span>bytes</span>.<span>NewReader</span>(<span>data</span>),
</span></span><span><span>		<span>IfNoneMatch</span>: <span>aws</span>.<span>String</span>(<span>&#34;*&#34;</span>),
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>if</span> <span>_</span>, <span>err</span> <span>:=</span> <span>w</span>.<span>client</span>.<span>PutObject</span>(<span>ctx</span>, <span>input</span>); <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>return</span> <span>0</span>, <span>fmt</span>.<span>Errorf</span>(<span>&#34;failed to put object to S3: %w&#34;</span>, <span>err</span>)
</span></span><span><span>	}
</span></span><span><span>	<span>w</span>.<span>length</span> = <span>nextOffset</span>
</span></span><span><span>	<span>return</span> <span>nextOffset</span>, <span>nil</span>
</span></span><span><span>}
</span></span></code></pre></div><p>How do we prevent two writers appending records with same offset? This is one of the crucial property of a log. Using S3 Conditional Write it is very simple. That’s why we have added <code>IfNoneMatch: aws.String(&#34;*&#34;)</code> in the request. If an object already exists with the same record offset, the request will be rejected. Let’s write a basic test to confirm this:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>TestSameOffset</span>(<span>t</span> <span>*</span><span>testing</span>.<span>T</span>) {
</span></span><span><span>	<span>wal</span>, <span>cleanup</span> <span>:=</span> <span>getWAL</span>(<span>t</span>)
</span></span><span><span>	<span>defer</span> <span>cleanup</span>()
</span></span><span><span>	<span>ctx</span> <span>:=</span> <span>context</span>.<span>Background</span>()
</span></span><span><span>	<span>data</span> <span>:=</span> []byte(<span>&#34;threads are evil&#34;</span>)
</span></span><span><span>	<span>_</span>, <span>err</span> <span>:=</span> <span>wal</span>.<span>Append</span>(<span>ctx</span>, <span>data</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>		<span>t</span>.<span>Fatalf</span>(<span>&#34;failed to append first record: %v&#34;</span>, <span>err</span>)
</span></span><span><span>	}
</span></span><span><span>
</span></span><span><span>	<span>// reset the WAL counter so that it uses the same offset
</span></span></span><span><span><span></span>	<span>wal</span>.<span>length</span> = <span>0</span>
</span></span><span><span>	<span>_</span>, <span>err</span> = <span>wal</span>.<span>Append</span>(<span>ctx</span>, <span>data</span>)
</span></span><span><span>	<span>if</span> <span>err</span> <span>==</span> <span>nil</span> {
</span></span><span><span>		<span>t</span>.<span>Error</span>(<span>&#34;expected error when appending at same offset, got nil&#34;</span>)
</span></span><span><span>	}
</span></span><span><span>}
</span></span></code></pre></div><p>You might be thinking, “Why not use S3’s latest append feature and write to the same object?” We can certainly do that, but it’s tricky to get right since a zombie writer might come back and append to an old object while a new leader is writing to a new file. Unlike typical Raft-based storage systems, S3 does not have a concept of fencing tokens. I’ve left this optimization to tackle later.</p><p>I’ve also kept the sequencing simpler by considering no gaps. If we allow gaps, it might be possible for a zombie writer to write to an old sequence number. There are ways to prevent this, but that’s a problem for another day! (Note to self: I should probably write another blog post about these problems.)</p><h3 id="checksums">Checksums</h3><p>S3 provides 99.99999999% durability. But like any sane man, I would never trust an external system for data integrity. <a href="https://avi.im/blag/2024/databases-checksum/">Most databases don’t do checksums</a>, but we can do better. For now, let’s use SHA-256 for checksums (Go std lib has it). We’ll store the offset, the data, and the checksum.</p><img src="https://avi.im/blag/images/2024/record-format.svg" alt="record format"/><p>By storing offset we make the record self contained. For e.g. if we do compaction tomorrow and change file names, the record’s offset remains same.</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> <span>calculateChecksum</span>(<span>buf</span> <span>*</span><span>bytes</span>.<span>Buffer</span>) [<span>32</span>]<span>byte</span> {
</span></span><span><span>	<span>return</span> <span>sha256</span>.<span>Sum256</span>(<span>buf</span>.<span>Bytes</span>())
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> <span>prepareBody</span>(<span>offset</span> <span>uint64</span>, <span>data</span> []<span>byte</span>) ([]<span>byte</span>, <span>error</span>) {
</span></span><span><span>	<span>// 8 bytes for offset, len(data) bytes for data, 32 bytes for checksum
</span></span></span><span><span><span></span>	<span>bufferLen</span> <span>:=</span> <span>8</span> <span>+</span> len(<span>data</span>) <span>+</span> <span>32</span>
</span></span><span><span>	<span>buf</span> <span>:=</span> <span>bytes</span>.<span>NewBuffer</span>(make([]<span>byte</span>, <span>0</span>, <span>bufferLen</span>))
</span></span><span><span>	<span>binary</span>.<span>Write</span>(<span>buf</span>, <span>binary</span>.<span>BigEndian</span>, <span>offset</span>)
</span></span><span><span>	<span>buf</span>.<span>Write</span>(<span>data</span>)
</span></span><span><span>	<span>checksum</span> <span>:=</span> <span>calculateChecksum</span>(<span>buf</span>)
</span></span><span><span>	<span>_</span>, <span>err</span> <span>:=</span> <span>buf</span>.<span>Write</span>(<span>checksum</span>[:])
</span></span><span><span>	<span>return</span> <span>buf</span>.<span>Bytes</span>(), <span>err</span>
</span></span><span><span>}
</span></span></code></pre></div><h3 id="read">Read</h3><p>Our log is coming along nicely! Let’s implement the read. It’s straightforward. Given an offset, we will construct the appropriate S3 object name and fetch it:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>w</span> <span>*</span><span>S3WAL</span>) <span>Read</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>offset</span> <span>uint64</span>) (<span>Record</span>, <span>error</span>) {
</span></span><span><span>	<span>key</span> <span>:=</span> <span>w</span>.<span>getObjectKey</span>(<span>offset</span>)
</span></span><span><span>	<span>input</span> <span>:=</span> <span>&amp;</span><span>s3</span>.<span>GetObjectInput</span>{
</span></span><span><span>		<span>Bucket</span>: <span>aws</span>.<span>String</span>(<span>w</span>.<span>bucketName</span>),
</span></span><span><span>		<span>Key</span>:    <span>aws</span>.<span>String</span>(<span>key</span>),
</span></span><span><span>	}
</span></span><span><span>	<span>result</span>, <span>_</span> <span>:=</span> <span>w</span>.<span>client</span>.<span>GetObject</span>(<span>ctx</span>, <span>input</span>)
</span></span><span><span>	<span>defer</span> <span>result</span>.<span>Body</span>.<span>Close</span>()
</span></span><span><span>
</span></span><span><span>	<span>data</span>, <span>_</span> <span>:=</span> <span>io</span>.<span>ReadAll</span>(<span>result</span>.<span>Body</span>)
</span></span><span><span>	<span>if</span> len(<span>data</span>) &lt; <span>40</span> {
</span></span><span><span>		<span>return</span> <span>Record</span>{}, <span>fmt</span>.<span>Errorf</span>(<span>&#34;invalid record: data too short&#34;</span>)
</span></span><span><span>	}
</span></span><span><span>	<span>if</span> !<span>validateOffset</span>(<span>data</span>, <span>offset</span>) {
</span></span><span><span>		<span>return</span> <span>Record</span>{}, <span>fmt</span>.<span>Errorf</span>(<span>&#34;offset mismatch&#34;</span>)
</span></span><span><span>	}
</span></span><span><span>	<span>if</span> !<span>validateChecksum</span>(<span>data</span>) {
</span></span><span><span>		<span>return</span> <span>Record</span>{}, <span>fmt</span>.<span>Errorf</span>(<span>&#34;checksum mismatch&#34;</span>)
</span></span><span><span>	}
</span></span><span><span>	<span>return</span> <span>Record</span>{
</span></span><span><span>		<span>Offset</span>: <span>offset</span>,
</span></span><span><span>		<span>Data</span>:   <span>data</span>[<span>8</span> : len(<span>data</span>)<span>-</span><span>32</span>],
</span></span><span><span>	}, <span>nil</span>
</span></span><span><span>}
</span></span></code></pre></div><p>We will do a couple of validations:</p><ol><li>The record has to be minimum 40 bytes</li><li>The offset in the request should match the one with request</li><li>The checksums should match</li></ol><h3 id="failover--crash-recovery">Failover / Crash Recovery</h3><p>Now that we have our basic operations working, let’s handle failure scenarios. What if our node crashes? How do we recover it? We always initialize our WAL with length 0. Subsequently, new writes will try to write at <code>0000000001</code> offset. This is not a catastrophic bug! S3 conditional writes protect us and reject the writes. However, we will not be able to proceed with new writes. Let’s fix this. Let’s add a method which goes through the list of keys, finds the last inserted object. There are a couple of <a href="https://github.com/avinassh/s3-log/issues/1">ways to optimize this</a>, but let’s iterate through all the keys:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>WAL</span> <span>interface</span> {
</span></span><span><span>	<span>LastRecord</span>(<span>ctx</span> <span>context</span>.<span>Context</span>) (<span>Record</span>, <span>error</span>)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>func</span> (<span>w</span> <span>*</span><span>S3WAL</span>) <span>LastRecord</span>(<span>ctx</span> <span>context</span>.<span>Context</span>) (<span>Record</span>, <span>error</span>) {
</span></span><span><span>	<span>input</span> <span>:=</span> <span>&amp;</span><span>s3</span>.<span>ListObjectsV2Input</span>{
</span></span><span><span>		<span>Bucket</span>: <span>aws</span>.<span>String</span>(<span>w</span>.<span>bucketName</span>),
</span></span><span><span>	}
</span></span><span><span>	<span>paginator</span> <span>:=</span> <span>s3</span>.<span>NewListObjectsV2Paginator</span>(<span>w</span>.<span>client</span>, <span>input</span>)
</span></span><span><span>
</span></span><span><span>	<span>var</span> <span>maxOffset</span> <span>uint64</span> = <span>0</span>
</span></span><span><span>	<span>for</span> <span>paginator</span>.<span>HasMorePages</span>() {
</span></span><span><span>		<span>output</span>, <span>_</span> <span>:=</span> <span>paginator</span>.<span>NextPage</span>(<span>ctx</span>)
</span></span><span><span>		<span>for</span> <span>_</span>, <span>obj</span> <span>:=</span> <span>range</span> <span>output</span>.<span>Contents</span> {
</span></span><span><span>			<span>key</span> <span>:=</span> <span>*</span><span>obj</span>.<span>Key</span>
</span></span><span><span>			<span>offset</span>, <span>_</span> <span>:=</span> <span>w</span>.<span>getOffsetFromKey</span>(<span>key</span>)
</span></span><span><span>			<span>if</span> <span>offset</span> &gt; <span>maxOffset</span> {
</span></span><span><span>				<span>maxOffset</span> = <span>offset</span>
</span></span><span><span>			}
</span></span><span><span>		}
</span></span><span><span>	}
</span></span><span><span>	<span>if</span> <span>maxOffset</span> <span>==</span> <span>0</span> {
</span></span><span><span>		<span>return</span> <span>Record</span>{}, <span>fmt</span>.<span>Errorf</span>(<span>&#34;WAL is empty&#34;</span>)
</span></span><span><span>	}
</span></span><span><span>	<span>w</span>.<span>length</span> = <span>maxOffset</span>
</span></span><span><span>	<span>return</span> <span>w</span>.<span>Read</span>(<span>ctx</span>, <span>maxOffset</span>)
</span></span><span><span>}
</span></span></code></pre></div><p>That’s it! The project is open source: <a href="https://github.com/avinassh/s3-log">s3-log</a>. You can check the code and some <a href="https://github.com/avinassh/s3-log/blob/master/s3_wal_test.go">tests here</a>. There are a couple of open issues if you’d like to contribute!</p><p><small>open issues: <a href="https://github.com/avinassh/s3-log/issues/1">improving LastRecord</a>, <a href="https://github.com/avinassh/s3-log/issues/2">cache</a>, <a href="https://github.com/avinassh/s3-log/issues/3">batch write</a>, <a href="https://github.com/avinassh/s3-log/issues/4">buffered write</a></small>.</p><hr/><p><small>1. Any object store would work. But I like S3.</small></p></article></div></div>
  </body>
</html>
