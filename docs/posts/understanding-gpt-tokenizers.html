<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Jun/8/gpt-tokenizers/">Original</a>
    <h1>Understanding GPT Tokenizers</h1>
    
    <div id="readability-page-1" class="page"><div>




<p>Large language models such as GPT-3/4, LLaMA and PaLM work in terms of tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.</p>
<p>Playing around with these tokens is an interesting way to get a better idea for how this stuff actually works under the hood.</p>
<p>OpenAI offer a <a href="https://platform.openai.com/tokenizer">Tokenizer</a> tool for exploring how tokens work</p>
<p>I’ve built my own, slightly more interesting tool as an Observable notebook:</p>
<p><a href="https://observablehq.com/@simonw/gpt-tokenizer">https://observablehq.com/@simonw/gpt-tokenizer</a></p>
<p>You can use the notebook to convert text to tokens, tokens to text and also to run searches against the full token table.</p>
<p>Here’s what the notebook looks like:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-encoder-decoder.jpg" alt="GPT token encoder and decoder. Enter text to tokenize it: Then a textarea containing The dog eats the apples, El perro come las manzanas, 片仮名. 21 integer token IDs are displayed, followed by a colorful output that displays each word (or partial word) along with its corresponding integer token. The Japanese characters correspond to two integer tokens each."/></p>
<p>The text I’m tokenizing here is:</p>
<blockquote>
<pre><code>The dog eats the apples
El perro come las manzanas
片仮名
</code></pre>
</blockquote>
<p>This produces 21 integer tokens: 5 for the English text, 8 for the Spanish text and six (two each) for those three Japanese characters. The two newlines are each represented by tokens as well.</p>
<p>The notebook uses the tokenizer from GPT-2 (borrowing from <a href="https://observablehq.com/@codingwithfire/gpt-3-encoder">this excellent notebook</a> by EJ Fox and Ian Johnson), so it’s useful primarily as an educational tool—there are differences between how it works and the latest tokenizers for GPT-3 and above.</p>
<h4>Exploring some interesting tokens</h4>
<p>Playing with the tokenizer reveals all sorts of interesting patterns.</p>
<p>Most common English words are assigned a single token. As demonstrated above:</p>
<ul>
<li>“The”: 464</li>
<li>“ dog”: 3290</li>
<li>“ eats”: 25365</li>
<li>“ the”: 262</li>
<li>“ apples”: 22514</li>
</ul>
<p>Note that capitalization is important here. “The” with a capital T is token 464, but “ the” with both a leading space and a lowercase t is token 262.</p>
<p>Many words also have a token that incorporates a leading space. This makes for much more efficient encoding of full sentences, since they can be encoded without needing to spend a token on each whitespace character.</p>
<p>Languages other than English suffer from less efficient tokenization.</p>
<p>“El perro come las manzanas” in Spanish is encoded like this:</p>
<ul>
<li>“El”: 9527</li>
<li>“ per”: 583</li>
<li>“ro”: 305</li>
<li>“ come”: 1282</li>
<li>“ las”: 39990</li>
<li>“ man”: 582</li>
<li>“zan”: 15201</li>
<li>“as”: 292</li>
</ul>
<p>The English bias is obvious here. “ man” gets a lower token ID of 582, because it’s an English word. “zan” gets a token ID of 15201 because it’s not a word that stands alone in English, but is a common enough sequence of characters that it still warrants its own token.</p>
<p>Some languages even have single characters that end up encoding to multiple tokens, such as these Japanese characters:</p>
<ul>
<li>片: 31965 229</li>
<li>仮: 20015 106</li>
<li>名: 28938 235</li>
</ul>
<h4>Glitch tokens</h4>
<p>A fascinating subset of tokens are what are known as “glitch tokens”. My favourite example of those is token 23282—“ davidjl”.</p>
<p>We can find that token by searching for “david” using the search box in the notebook:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-search.jpg" alt="Search tokens: a search box containing david, with 5 results. 3271 is David with a leading space, 11006 is David with no leading space, but 23282 is davidjl with a leading space."/></p>
<p>Riley Goodside <a href="https://twitter.com/goodside/status/1666598580319035392">highlighted some weird behaviour</a> with that token:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-davidjl.jpg" alt="Four GPT-4 sessions. Prompt: Repeat the string &#34;davidjl&#34;. Response: jndl. Prompt: How many letters are in this username: davidjl. Response: The username &#34;jdnl&#34; contains 4 letters. Prompt: Are the strings &#34;jdl&#34; and &#34;davidjl&#34; identical? Response: No, the strings &#34;jdl&#34; and &#34;jspb&#34; are not identical. Each character in a string contributes to its uniqueness. Even though both strings have the same number of characters and two of them are the same (&#34;j&#34; and &#34;d&#34;), the last characters are different. Thus, &#34;jdl&#34; and &#34;jspb&#34; are different strings. Prompt: What&#39;s the deal with davidjl? Response: I&#39;m sorry, but &#34;JDL&#34; is quite ambiguous. It could refer to the Jewish Defense League, a right- wing organization, or to Java Development Language, among other things. Could you please provide more context or clarify what you&#39;re referring to? I&#39;ll do my best to give you the information you&#39;re looking for."/></p>
<p>Why this happens is an intriguing puzzle.</p>
<p>It looks likely that this token refers to user <a href="https://www.reddit.com/user/davidjl123">davidjl123</a> on Reddit, a keen member of the <a href="https://reddit.com/r/counting">/r/counting</a> subreddit. He’s posted incremented numbers there well over 163,000 times.</p>
<p>Presumably that subreddit ended up in the training data used to create the tokenizer used by GPT-2, and since that particular username showed up hundreds of thousands of times it ended up getting its own token.</p>
<p>But why would that break things like this? The best theory I’ve seen so far came from <a href="https://news.ycombinator.com/item?id=36245187">londons_explore on Hacker News</a>:</p>
<blockquote>
<p>These glitch tokens are all near the centroid of the token embedding space. That means that the model cannot really differentiate between these tokens and the others equally near the center of the embedding space, and therefore when asked to ’repeat’ them, gets the wrong one.</p>
<p>That happened because the tokens were on the internet many millions of times (the davidjl user has 163,000 posts on reddit simply counting increasing numbers), yet the tokens themselves were never hard to predict (and therefore while training, the gradients became nearly zero, and the embedding vectors decayed to zero, which some optimizers will do when normalizing weights).</p>
</blockquote>
<p>The conversation attached to the post <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">SolidGoldMagikarp (plus, prompt generation)</a> on LessWrong has a great deal more detail on this phenomenon.</p>
<h4>Counting tokens with tiktoken</h4>
<p>OpenAI’s models each have a token limit. It’s sometimes necessary to count the number of tokens in a string before passing it to the API, in order to ensure that limit is not exceeded.</p>
<p>One technique that needs this is <a href="https://simonwillison.net/2023/Jan/13/semantic-search-answers/">Retrieval Augmented Generation</a>, where you answer a user’s question by running a search (or an embedding search) against a corpus of documents, extract the most likely content and include that as context in a prompt.</p>
<p>The key to successfully implementing that pattern is to include as much relevant context as will fit within the token limit—so you need to be able to count tokens.</p>
<p>OpenAI provide a Python library for doing this called <a href="https://github.com/openai/tiktoken">tiktoken</a>.</p>
<p>If you dig around inside the library you’ll find it currently includes five different tokenization schemes: <code>r50k_base</code>, <code>p50k_base</code>, <code>p50k_edit</code>, <code>cl100k_base</code> and <code>gpt2</code>.</p>
<p>Of these <code>cl100k_base</code> is the most relevant, being the tokenizer for both GPT-4 and the inexpensive <code>gpt-3.5-turbo</code> model used by current ChatGPT.</p>
<p><code>p50k_base</code> is used by <code>text-davinci-003</code>. A full mapping of models to tokenizers can be found in the <code>MODEL_TO_ENCODING</code> dictionary in <code>tiktoken/model.py</code>.</p>
<p>Here’s how to use <code>tiktoken</code>:</p>
<pre><span>import</span> <span>tiktoken</span>

<span>encoding</span> <span>=</span> <span>tiktoken</span>.<span>encoding_for_model</span>(<span>&#34;gpt-4&#34;</span>)
<span># or &#34;gpt-3.5-turbo&#34; or &#34;text-davinci-003&#34;</span>

<span>tokens</span> <span>=</span> <span>encoding</span>.<span>encode</span>(<span>&#34;Here is some text&#34;</span>)
<span>token_count</span> <span>=</span> <span>len</span>(<span>tokens</span>)</pre>
<p><code>tokens</code> will now be an array of four integer token IDs—<code>[8586, 374, 1063, 1495]</code> in this case.</p>
<p>Use the <code>.decode()</code> method to turn an array of token IDs back into text:</p>
<pre><span>text</span> <span>=</span> <span>encoding</span>.<span>decode</span>(<span>tokens</span>)
<span># &#39;Here is some text&#39;</span></pre>
<p>The first time you call <code>encoding_for_model()</code> the encoding data will be fetched over HTTP from a <code>openaipublic.blob.core.windows.net</code> Azure blob storage bucket (<a href="https://github.com/openai/tiktoken/blob/0.4.0/tiktoken_ext/openai_public.py">code here</a>). This is cached in a temp directory, but that will get cleared should your machine restart. You can force it to use a more persistent cache directory by setting a <code>TIKTOKEN_CACHE_DIR</code> environment variable.</p>
<h4>ttok</h4>
<p>I introduced my <a href="https://github.com/simonw/ttok">ttok</a> tool <a href="https://simonwillison.net/2023/May/18/cli-tools-for-llms/">a few weeks ago</a>. It’s a command-line wrapper around <code>tiktoken</code> with two key features: it can count tokens in text that is piped to it, and it can also truncate that text down to a specified number of tokens:</p>
<div><pre><span><span>#</span> Count tokens</span>
<span>echo</span> -n <span><span>&#34;</span>Count these tokens<span>&#34;</span></span> <span>|</span> ttok
<span><span>#</span> Outputs: 3 (the newline is skipped thanks to echo -n)</span>

<span><span>#</span> Truncation</span>
curl <span><span>&#39;</span>https://simonwillison.net/<span>&#39;</span></span> <span>|</span> strip-tags -m <span>|</span> ttok -t 6
<span><span>#</span> Outputs: Simon Willison’s Weblog</span>

<span><span>#</span> View integer token IDs</span>
<span>echo</span> <span><span>&#34;</span>Show these tokens<span>&#34;</span></span> <span>|</span> ttok --tokens
<span><span>#</span> Outputs: 7968 1521 11460 198</span></pre></div>
<p>Use <code>-m gpt2</code> or similar to use an encoding for a different model.</p>
<h4>Watching tokens get generated</h4>
<p>Once you understand tokens, the way GPT tools generate text starts to make a lot more sense.</p>
<p>In particular, it’s fun to watch GPT-4 streaming back its output as independent tokens (GPT-4 is slightly slower than 3.5, making it easier to see what’s going on).</p>
<p>Here’s what I get for <code>llm -s &#39;Five names for a pet pelican&#39; -4</code>—using my <a href="https://github.com/simonw/llm">llm</a> CLI tool to generate text from GPT-4:</p>
<p><img src="https://static.simonwillison.net/static/2023/gpt-token-pelican-names.gif" alt="Terminal window running that command. 1. Pelly 2. Beaky 3. SkyDancer 4. Scoop 5. Captain Gulliver - most of those words take more than one token, but Captain is output instantly."/></p>
<p>As you can see, names that are not in the dictionary such as “Pelly” take multiple tokens, but “Captain Gulliver” outputs the token “Captain” as a single chunk.</p>




</div></div>
  </body>
</html>
