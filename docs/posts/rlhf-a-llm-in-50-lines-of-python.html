<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://datadreamer.dev/docs/latest/pages/get_started/quick_tour/aligning.html">Original</a>
    <h1>RLHF a LLM in &lt;50 lines of Python</h1>
    
    <div id="readability-page-1" class="page"><div>
        <a href="#">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div>
          
          <p><label for="__toc">
            <p>Toggle table of contents sidebar</p>
            <i><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </p></div>
        <article role="main">
          <section id="aligning-a-llm-with-human-preferences">

<p>In order to better align the responses <a href="https://datadreamer.dev/docs/latest/pages/get_started/quick_tour/instruction_tuning.html"><span>instruction-tuned LLMs</span></a> generate to what humans would prefer, we can train LLMs against a reward model or a dataset of human preferences in a process known as <a href="https://arxiv.org/abs/2203.02155">RLHF (Reinforcement Learning with Human Feedback)</a>.</p>
<p>DataDreamer makes this process extremely simple and straightforward to accomplish. We demonstrate it below using LoRA to only train
a fraction of the weights with <a href="https://arxiv.org/abs/2305.18290">DPO</a>.</p>
<div><div><pre><span></span><span>from</span> <span>datadreamer</span> <span>import</span> <span>DataDreamer</span>
<span>from</span> <span>datadreamer.steps</span> <span>import</span> <span>HFHubDataSource</span>
<span>from</span> <span>datadreamer.trainers</span> <span>import</span> <span>TrainHFDPO</span>
<span>from</span> <span>peft</span> <span>import</span> <span>LoraConfig</span>

<span>with</span> <span>DataDreamer</span><span>(</span><span>&#34;./output&#34;</span><span>):</span>
    <span># Get the DPO dataset</span>
    <span>dpo_dataset</span> <span>=</span> <span>HFHubDataSource</span><span>(</span>
        <span>&#34;Get DPO Dataset&#34;</span><span>,</span> <span>&#34;Intel/orca_dpo_pairs&#34;</span><span>,</span> <span>split</span><span>=</span><span>&#34;train&#34;</span>
    <span>)</span>

    <span># Keep only 1000 examples as a quick demo</span>
    <span>dpo_dataset</span> <span>=</span> <span>dpo_dataset</span><span>.</span><span>take</span><span>(</span><span>1000</span><span>)</span>

    <span># Create training data splits</span>
    <span>splits</span> <span>=</span> <span>dpo_dataset</span><span>.</span><span>splits</span><span>(</span><span>train_size</span><span>=</span><span>0.90</span><span>,</span> <span>validation_size</span><span>=</span><span>0.10</span><span>)</span>

    <span># Align the TinyLlama chat model with human preferences</span>
    <span>trainer</span> <span>=</span> <span>TrainHFDPO</span><span>(</span>
        <span>&#34;Align TinyLlama-Chat&#34;</span><span>,</span>
        <span>model_name</span><span>=</span><span>&#34;TinyLlama/TinyLlama-1.1B-Chat-v1.0&#34;</span><span>,</span>
        <span>peft_config</span><span>=</span><span>LoraConfig</span><span>(),</span>
        <span>device</span><span>=</span><span>[</span><span>&#34;cuda:0&#34;</span><span>,</span> <span>&#34;cuda:1&#34;</span><span>],</span>
        <span>dtype</span><span>=</span><span>&#34;bfloat16&#34;</span><span>,</span>
    <span>)</span>
    <span>trainer</span><span>.</span><span>train</span><span>(</span>
        <span>train_prompts</span><span>=</span><span>splits</span><span>[</span><span>&#34;train&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;question&#34;</span><span>],</span>
        <span>train_chosen</span><span>=</span><span>splits</span><span>[</span><span>&#34;train&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;chosen&#34;</span><span>],</span>
        <span>train_rejected</span><span>=</span><span>splits</span><span>[</span><span>&#34;train&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;rejected&#34;</span><span>],</span>
        <span>validation_prompts</span><span>=</span><span>splits</span><span>[</span><span>&#34;validation&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;question&#34;</span><span>],</span>
        <span>validation_chosen</span><span>=</span><span>splits</span><span>[</span><span>&#34;validation&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;chosen&#34;</span><span>],</span>
        <span>validation_rejected</span><span>=</span><span>splits</span><span>[</span><span>&#34;validation&#34;</span><span>]</span><span>.</span><span>output</span><span>[</span><span>&#34;rejected&#34;</span><span>],</span>
        <span>epochs</span><span>=</span><span>3</span><span>,</span>
        <span>batch_size</span><span>=</span><span>1</span><span>,</span>
        <span>gradient_accumulation_steps</span><span>=</span><span>32</span><span>,</span>
    <span>)</span>
</pre></div>
</div>
</section>

        </article>
      </div></div>
  </body>
</html>
