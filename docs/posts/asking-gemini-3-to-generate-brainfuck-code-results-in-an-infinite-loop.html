<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://teodordyakov.github.io/brainfuck-agi/">Original</a>
    <h1>Asking Gemini 3 to generate Brainfuck code results in an infinite loop</h1>
    
    <div id="readability-page-1" class="page"><article>
        <header>
            
        </header>

        <section>
            <p>Asking Gemini 3 to generate Brainf*ck code results in an infinite loop, akin amost to a DDoS attack:</p>
            <img src="https://teodordyakov.github.io/brainfuck-agi/bf.gif"/>
            <p>That is fascinating. So it made me wonder.
            Is Brainf*ck the ultimate test for AGI?
            I think so, and for 3 good reasons.
            </p>
            <h2>1. The Data Scarcity Problem</h2>
            <p>Large Language Models (LLMs) thrive on sheer volume. To master JavaScript, an LLM has been trained on
                virtually every available line of open-source code—hundreds of millions of lines of code (LOC). By
                comparison, the amount of functional <strong>Brainf*ck</strong> code on the web is a statistical
                rounding error.</p>
            <p>We are talking about a <span>million times less training data</span>. Without the
                luxury of infinite patterns to copy, the model can&#39;t rely on mimicry; it has to understand the
                underlying logic.</p>
        </section>

        <section>
            <h2>2. Anti-Literate Programming</h2>
            <p>Brainf*ck is the antithesis of modern software engineering. There are no comments, no meaningful variable
                names, and no structure. In many ways, looking at existing Brainf*ck code is actually
                <em>detrimental</em> to a novice. Consider this typical snippet:</p>

            <code>&gt;++++++++[&lt;+++++++++&gt;-]&lt;.&gt;++++[&lt;+++++++&gt;-]&lt;+.+++++++..+++.&gt;&gt;++++++[&lt;+++++++&gt;-]&lt;+
+.------------.&gt;++++++[&lt;+++++++++&gt;-]&lt;+.&lt;.+++.------.--------.&gt;&gt;&gt;++++[&lt;++++++++&gt;-
]&lt;+.</code>

            <p>Writing in this environment is akin to <span>zero-shot learning</span>. Success
                requires reasoning at a high level of abstraction based on the fundamental rules of the language and a
                precise mental model of semantics, rather than memorized syntax.</p>
        </section>

        <section>
            <h2>3. The Repetition Problem</h2>
              <div>
            <p>As we saw earlier, asking a modern model for complex Brainf*ck code often results in the model falling into an infinite loop—spewing the same characters over and over. The minimalistic nature of the language results in highly repetitive structures in the code. This poses a unique challenge to the way LLMs work.</p>
            <p>An LLM is more likely to output what it has already seen based on previous tokens, and that pertains to its own output too. When some structure is repeated more than a couple of times, there is a likelihood that the model may learn that token <strong>X</strong> is the most likely output following <strong>itself</strong>. With every subsequent iteration, this increases the likelihood of outputting <strong>X</strong> in a self-fulfilling prophecy, resulting in the infinite loop.</p>
        </div>
        </section>

        <section>
            <p> <strong>So, is Brainf*ck the ultimate test for LLMs? You be the judge.</strong></p>
        </section>

        
    </article></div>
  </body>
</html>
