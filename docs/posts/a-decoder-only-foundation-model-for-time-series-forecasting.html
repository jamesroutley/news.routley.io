<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html">Original</a>
    <h1>A decoder-only foundation model for time-series forecasting</h1>
    
    <div id="readability-page-1" class="page"><div>
<div id="post-body-6956767920612914706">
<p><span>Posted by Rajat Sen and Yichen Zhou, Google Research</span>


</p><p>
<a href="https://en.wikipedia.org/wiki/Time_series">Time-series</a> forecasting is ubiquitous in various domains, such as retail, finance, manufacturing, healthcare and natural sciences. In retail use cases, for example, it has been observed that <a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning">improving demand forecasting accuracy</a> can meaningfully reduce inventory costs and increase revenue. Deep learning (DL) models have emerged as a popular approach for forecasting rich, multivariate, time-series data because they have proven to perform well in a variety of settings (e.g., DL models dominated the <a href="https://www.sciencedirect.com/science/article/pii/S0169207021001874">M5 competition leaderboard</a>).
</p>
<p>
At the same time, there has been rapid progress in large foundation language models used for natural language processing (NLP) tasks, such as <a href="https://en.wikipedia.org/wiki/Machine_translation">translation</a>, <a href="https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/">retrieval-augmented generation</a>, and <a href="https://en.wikipedia.org/wiki/Intelligent_code_completion">code completion</a>. These models are trained on massive amounts of <em>textual </em>data derived from a variety of sources like <a href="https://commoncrawl.org/">common crawl</a> and open-source code that allows them to identify patterns in languages. This makes them very powerful <a href="https://en.wikipedia.org/wiki/Zero-shot_learning">zero-shot</a> tools; for instance, <a href="https://blog.google/products/bard/google-bard-try-gemini-ai/">when paired with retrieval</a>, they can answer questions about and summarize current events.
</p>

<p>
Despite DL-based forecasters largely <a href="https://arxiv.org/abs/1704.04110">outperforming</a> traditional methods and progress being made in <a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting">reducing training and inference costs</a>, they face challenges: most DL architectures require <a href="https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-forecasting">long and involved training and validation cycles</a> before a customer can test the model on a new time-series. A foundation model for time-series forecasting, in contrast, can provide decent out-of-the-box forecasts on unseen time-series data with no additional training, enabling users to focus on refining forecasts for the actual downstream task like <a href="https://en.wikipedia.org/wiki/Customer_demand_planning">retail demand planning</a>.
</p>

<p>
To that end, in “<a href="https://arxiv.org/pdf/2310.10688.pdf">A decoder-only foundation model for time-series forecasting</a>”, we introduce TimesFM, a single forecasting model pre-trained on a large time-series corpus of 100 billion real world time-points. Compared to the latest large language models (LLMs), TimesFM is much smaller (200M parameters), yet we show that even at such scales, its zero-shot performance on a variety of unseen datasets of different domains and temporal granularities come close to the state-of-the-art supervised approaches trained explicitly on these datasets. Later this year we plan to make this model available for external customers in <a href="https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/train-model#aiplatform_create_training_pipeline_tabular_forecasting_sample-python">Google Cloud Vertex AI</a>.
</p>





<p>
LLMs are usually trained in a <a href="https://arxiv.org/pdf/1801.10198.pdf">decoder-only</a> fashion that involves three steps. First, text is broken down into subwords called tokens. Then, the tokens are fed into stacked causal <a href="https://arxiv.org/abs/1706.03762">transformer</a> layers that produce an output corresponding to each input token (it cannot attend to future tokens). Finally, the output corresponding to the <em>i</em>-th token summarizes all the information from previous tokens and predicts the (<em>i</em>+1)-th token. During inference, the LLM generates the output one token at a time. For example, when prompted with “What is the capital of France?”, it might generate the token “The”, then condition on “What is the capital of France? The” to generate the next token “capital” and so on until it generates the complete answer: “The capital of France is Paris”.
</p>

<p>
A foundation model for time-series forecasting should adapt to variable context (what we observe) and horizon (what we query the model to forecast) lengths, while having enough capacity to encode all patterns from a large pretraining dataset. Similar to LLMs, we use stacked transformer layers (self-attention and <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward</a> layers) as the main building blocks for the TimesFM model. In the context of time-series forecasting, we treat a patch (a group of contiguous time-points) as a token that was popularized by a recent <a href="https://arxiv.org/abs/2211.14730">long-horizon forecasting work</a>. The task then is to forecast the (<em>i</em>+1)-th patch of time-points given the <em>i</em>-th output at the end of the stacked transformer layers. 
</p>

<p>
However, there are several key differences from language models. Firstly, we need a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a> block with residual connections to convert a patch of time-series into a token that can be input to the transformer layers along with <a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">positional encodings</a> (PE). For that, we use a residual block similar to our prior work in <a href="https://arxiv.org/abs/2304.08424">long-horizon forecasting</a>. Secondly, at the other end, an output token from the stacked transformer can be used to predict a longer length of subsequent time-points than the input patch length, i.e., the output patch length can be larger than the input patch length.  
</p>

<p>
Consider a time-series of length 512 time-points being used to train a TimesFM model with input patch length 32 and output patch length 128. During training, the model is simultaneously trained to use the first 32 time-points to forecast the next 128 time-points, the first 64 time-points to forecast time-points 65 to 192, the first 96 time-points to forecast time-points 97 to 224 and so on. During inference, suppose the model is given a new time-series of length 256 and tasked with forecasting the next 256 time-points into the future. The model will first generate the future predictions for time-points 257 to 384, then condition on the initial 256 length input plus the generated output to generate time-points 385 to 512. On the other hand, if in our model the output patch length was equal to the input patch length of 32 then for the same task we would have to go through eight generation steps instead of just the two above. This increases the chances of more errors accumulating and therefore, in practice, we see that a longer output patch length yields better performance for long-horizon forecasting
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s1084/image3.jpg"><img data-original-height="674" data-original-width="1084" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4G0lBOLUqlPIXJ3R68kjS984MBIKBPDBrCWtgmjVVTyQRqY6-rn3aHJjgxCbG-8csyBLsp0POILdeJ2VcsRy8lrip0k5DWsUpuL9LU1qOPXLW99mraNdd6HVU791NYqJeTyY7LjuMnOIo6RGmkxBQqqaPrSsC0dELrwy21QUs1Jgwxr8flmdNkDV2tZsT/s16000/image3.jpg"/></a></td></tr><tr><td>TimesFM architecture.</td></tr></tbody></table>

</div>
</div></div>
  </body>
</html>
