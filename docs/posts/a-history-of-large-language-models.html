<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gregorygundersen.com/blog/2025/10/01/large-language-models/">Original</a>
    <h1>A History of Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div>
        
<p>Large language models (LLMs) still feel a bit like magic to me. Of course, I understand
the general machinery enough to know that they aren’t, but the gap between my outdated
knowledge of the field and the state-of-the-art feels especially large right
now. Things are moving fast. So six months ago, I decided to close that gap just a little by digging into what I believed was one of the core primitives underpinning LLMs: the attention mechanism in neural networks.</p>

<p>I started by reading one of the landmark papers in the literature, which was
published by Google Brain in 2017 under the catchy title <em>Attention is all
you need</em> <a href="#vaswani2017attention">(Vaswani et al., 2017)</a>. As the title suggests, the authors
did not invent the attention mechanism. Rather, they introduced a neural network architecture which in was some sense “all attention”. This architecture is the now-famous <em>transformer</em>. Clearly the transformer stands in contrast
to whatever came before it, but what was that and what did the transformer do differently?</p>

<p>To answer these questions, I read a lot of papers, and the context that felt natural to provide here grew the more that I read. I went down the rabbit hole, and when I came out, I realized that what had started as a study of attention had grown into a bigger story. Attention is still the throughline, but there are other important themes, such as how neural networks generalize and the bitter lesson that simple methods that scale seem to triumph over clever methods which do not. This post is the product of that deep dive, and it is a stylized history of LLMs.</p>

<p>As a caveat, real life is endlessly detailed, and any summary or synthesis inevitably flattens this detail. So I will accidentally or intentionally skip over many important and related papers and ideas in the service of a synthesis. I will also skip over practicalities such as data preprocessing and advances in hardware and computing. My focus will be on what I view as the main methodological landmarks, and this history is simply one of many ways to tell this story.</p>

<h2 id="distributed-representations">Distributed representations</h2>

<p>I’ll start with an old idea, one so ubiquitous today that it might seem silly to belabor here. The idea is that neural networks automatically generalize using <em>distributed representations</em>. This idea has its roots in computational neuroscience, particularly Connectionism <a href="#mcculloch1943logical">(McCulloch &amp; Pitts, 1943)</a> and was discussed explicitly in the 1980s in papers like <em>Learning representations by back-propagating errors</em> <a href="#rumelhart1986learning">(Rumelhart et al., 1986)</a> and <em>Learning distributed representations of concepts</em> <a href="#hinton1986learning">(Hinton, 1986)</a>. Understanding it is key to understanding why LLMs work at all and thus understanding the long line of academic research driving towards them.</p>

<p>But first, a problem. The goal of natural language processing (NLP) is to model human language using computers. Until the 1980s, NLP systems were mostly based on handwritten rules and handcrafted features. However, by the early 1990s, researchers were exploring the use of statistical methods from machine learning. For an early and seminal example, see <em>A statistical approach to machine translation</em> <a href="#brown1990statistical">(Brown et al., 1990)</a>.</p>

<p>The core idea of statistical NLP is to model human language using a <em>statistical language model</em>, which is a probability distribution over all possible sequences in a language. This distribution is typically factorized such that each word depends on all words that precede it:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_{1:T}) = \prod_{t=1}^T p\left(w_t \mid w_{1:t-1} \right). \tag{1}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∏</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>p</span><span></span><span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span></span><span>)</span></span></span></span></span></span></p>

<p>Throughout this post, I will use the notation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>:</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i:j}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>i</span><span>:</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to denote elements in a sequence from positions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>j</span></span></span></span> inclusive (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>≤</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i \leq j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span><span></span><span>≤</span><span></span></span><span><span></span><span>j</span></span></span></span>):</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>:</mo><mi>j</mi></mrow></msub><mo>:</mo><mo>=</mo><mo stretchy="false">{</mo><msub><mi>w</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>w</mi><mi>j</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
w_{i:j} := \{w_i, w_{i+1}, \dots, w_{j-1}, w_j\}. \tag{2}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>i</span><span>:</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>:</span></span><span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>w</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>i</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>j</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span></span><span>)</span></span></span></span></span></span></p>

<p>Given a good statistical model <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_{1:T})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>, we can do many things. For example, we can rank the likelihood of different sequences of words and use that ranking to decide on things like a conversational agent’s output. Or we can translate a source sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> into a target sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> if we have the conditional probabilities between the two:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi>s</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo><mo>∝</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_{1:T} \mid s_{1:T}) \propto p(s_{1:T} \mid w_{1:T}) p(w_{1:T}). \tag{3}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>∝</span><span></span></span><span><span></span><span>p</span><span>(</span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span></span><span>)</span></span></span></span></span></span></p>

<p>Here, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_{1:T})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> would be our <em>language model</em> of the target language, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(s_{1:T} \mid w_{1:T})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> would be our <em>translation model</em>.</p>

<p>Today, this view is so pervasive that it might feel obvious, but with a little imagination, I think it’s easy to see how wrong this might have felt to a linguist forty-odd years ago. Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> captures no language structure or parts of speech such as nouns or verbs or adjectives—see e.g. <a href="#chomsky1956three">(Chomsky, 1956)</a> on <em>formal grammars</em>. Instead, it reduces the complexity of human language to next-word prediction. If we didn’t know already that this worked, we might doubt that it would.</p>

<p>More importantly for us, estimating the model in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> is hard! The main challenge is the <em>curse of dimensionality</em>. There are many, many words in a vocabulary. For example, <a href="https://www.merriam-webster.com/help/faq-how-many-english-words" target="_blank">linguists estimate</a> that English has roughly a million words, give or take a few hundred thousand depending on how you count them. Furthermore, this problem explodes in some tasks such as translation, where there are many possible conditional probabilities <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(s_{1:T} \mid w_{1:T})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>s</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. So when estimating the conditional probabilities of our language model, we cannot possibly encounter all possible combinations. We have a data sparsity problem, and estimating the true probabilities becomes impossible.</p>

<p>Perhaps the oldest idea to tackle this problem was proposed in Andrey Markov’s pioneering mathematical analysis of Pushkin’s <em>Eugene Onegin</em> <a href="#markov1913exmaple">(Markov, 1913)</a>. He made the assumption that each conditional probability in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> only depends on the previous <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> terms:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mo>≈</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>p</mi><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(4)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_{1:T}) = \prod_{t=1}^T p \left( w_t \mid w_{1:t-1} \right) \approx \prod_{t=1}^T p \left(w_t \mid w_{t-N:t-1} \right). \tag{4}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∏</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>p</span><span></span><span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>≈</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∏</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>p</span><span></span><span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span></span><span>)</span></span></span></span></span></span></p>

<p>Today, we would call this a “Markov assumption”, and Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span></span></span></span> is the famous <em><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram model</em>. Particularly for small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>, say <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N=1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N=2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span></span></span></span>, we might be able to get reasonable estimates of data. But here is the problem, and this problem is a central theme driving towards the attention mechanism: the Markov assumption destroys <em>context</em>. Without more context, a language model can never replicate the complexity and nuance of natural language.</p>

<p>As I understand it, this was conceptually the state of the field circa 2000. But then in 2003, a seminal paper was published: <em>A neural probabilistic language model</em> <a href="#bengio2003neural">(Bengio et al., 2003)</a>. In that paper, the authors proposed a novel idea: to avoid this data sparsity problem, this curse of dimensionality, we can use neural networks to learn a language model using what they call “distributed representations” of words. (Today, we might call these “word embeddings”.) They proposed three core ideas. First, they represented each word as a real-valued vector or embedding; then, they expressed Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> in terms of these embeddings; and finally, they trained a neural network to simultaneously learn the embeddings and the parameters of the probability function (neural network) in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> using back-propagation <a href="#rumelhart1986learning">(Rumelhart et al., 1986)</a>.</p>

<p>That’s a lot, so let’s break it down a bit. Our goal here is to learn a good model <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi mathvariant="normal">Θ</mi></msub></mrow><annotation encoding="application/x-tex">f_{\Theta}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span>Θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> of natural language such that</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>f</mi><mi mathvariant="bold-italic">Θ</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(5)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_t \mid w_{1:t-1}) \approx f_{\boldsymbol{\Theta}}(w_{t-1}, \dots, w_{t-N}). \tag{5}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>5</span></span><span>)</span></span></span></span></span></span></p>

<p>So the left-hand side is the true conditional distribution, capturing next-word prediction. It’s the goal of language modeling. But in practice, modeling the full context is hard. So we settle for the right hand side, which is a parametric approximation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi mathvariant="bold-italic">Θ</mi></msub></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\Theta}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> of this true distribution with <em>context window</em> of size <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>.</p>

<p>In Bengio, they model <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi mathvariant="bold-italic">Θ</mi></msub></mrow><annotation encoding="application/x-tex">f_{\boldsymbol{\Theta}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> using two components. First, they represent words as vectors. Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">V</mi></mrow><annotation encoding="application/x-tex">\mathcal{V}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span></span></span></span></span> denote our vocabulary, which is simply a set of integers <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">V</mi><mo>=</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{V} = \{1, 2,\dots, V\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span>1</span><span>,</span><span></span><span>2</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>V</span><span>}</span></span></span></span> indexing all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span> words in a language. We will represent each word as a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-vector, and so we can represent the entire language as a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{C} \in \mathbb{R}^{V \times D}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>V</span><span>×</span><span>D</span></span></span></span></span></span></span></span></span></span></span></span> (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span>). Now for the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>-th word in a sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, we have an associated index in the vocabulary, which we will denote as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mi mathvariant="script">V</mi></mrow><annotation encoding="application/x-tex">I(w_t) \in \mathcal{V}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>∈</span><span></span></span><span><span></span><span><span>V</span></span></span></span></span>. This notation might be a bit odd, but I’m careful here because <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is not a well-defined mathematical object, and it cannot index <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>. But <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(w_t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> is an integer and can index <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>, and so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_{I(w_t)}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-dimensional vector (a row vector of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>) representing the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(w_t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>-th word in the vocabulary, associated with the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>-th word in the sequence. This vector is what we are calling an “embedding” or “distributed representation”.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig1-embeddings.png" alt=""/></p><p><span>Figure 1.</span> The matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{C} \in \mathbb{R}^{V \times D}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>V</span><span>×</span><span>D</span></span></span></span></span></span></span></span></span></span></span></span> represents every word in a vocabulary <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">V</mi><mo>=</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{V} = \{1,2,\dots,V\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span>1</span><span>,</span><span></span><span>2</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>V</span><span>}</span></span></span></span>. The <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span>-th row is a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-vector representing the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span>-th word, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mi mathvariant="script">V</mi></mrow><annotation encoding="application/x-tex">i \in \mathcal{V}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span><span></span><span>∈</span><span></span></span><span><span></span><span><span>V</span></span></span></span></span>. This row vector is a distributed representation of that word, called a &#34;word embedding&#34;.
    </p>
</div>

<p>Second, Bengio et al represent the probability function over words (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span>) as as a feed-forward neural network <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span> with parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">Ω</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\Omega}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>Ω</span></span></span></span></span></span> and arguments <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>f</mi><mi mathvariant="bold-italic">Θ</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>g</mi><mi mathvariant="bold-italic">Ω</mi></msub><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(6)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
f_{\boldsymbol{\Theta}}(w_{t-1}, \dots, w_{t-N}) = g_{\boldsymbol{\Omega}}\left(\mathbf{c}_{I(w_{t-1})}, \dots, \mathbf{c}_{I(w_{t-N})}\right). \tag{6}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>g</span><span><span><span><span><span><span></span><span><span><span><span><span>Ω</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>6</span></span><span>)</span></span></span></span></span></span></p>

<p>They then use back-propagation to jointly estimate the parameters</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="bold-italic">Θ</mi><mo>:</mo><mo>=</mo><mo stretchy="false">{</mo><mi mathvariant="bold">C</mi><mo separator="true">,</mo><mi mathvariant="bold-italic">Ω</mi><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(7)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\boldsymbol{\Theta} := \{\mathbf{C}, \boldsymbol{\Omega}\}. \tag{7}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>Θ</span></span></span><span></span><span>:</span></span><span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>C</span></span><span>,</span><span></span><span><span><span>Ω</span></span></span><span>}</span><span>.</span></span><span><span></span><span><span>(</span><span><span>7</span></span><span>)</span></span></span></span></span></span></p>

<p>In other words, they learn the neural network parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold-italic">Ω</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\Omega}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>Ω</span></span></span></span></span></span> at the same time as learning the word embeddings <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>. Note that “distributed representation” can refer to either the continuously-valued vector, e.g. word embedding, or the concept distributed across neurons. This duality is exemplified in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span> which is both a set of learnable parameters <em>and</em> the embeddings themselves!</p>

<p>Why might this work? The authors explain the idea so well that it’s worth just quoting the original paper:</p>

<blockquote>
  <p>In the proposed model, it will so generalize because “similar” words are expected to have a similar feature vector, and because the probability function is a <em>smooth</em> function of these feature values, a small change in the features will induce a small change in the probability. Therefore, the presence of only one of the above sentences in the training data will increase the probability, not only of that sentence, but also of its combinatorial number of “neighbors” in sentence space.</p>
</blockquote>

<p>This is a beautiful idea. If we have word embeddings that are “well-organized” in the sense that words that play similar roles in sentences (semantically and syntactically) have similar embeddings <em>and</em> if we have a smooth function from word embeddings to probabilities, then small changes in words lead to small changes in embeddings which lead to small changes in probabilities (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span></span></span></span>).</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig2-generalization.png" alt=""/></p><p><span>Figure 2.</span> <a href="#bengio2003neural">(Bengio et al., 2003)</a> hypothesized that neural networks would generalize well on word embeddings because a small change in the input embedding could produce a small change in the output probability. Thus, if the model only saw one sentence with a given word, it might still generalize to similar sentences with similar words.
    </p>
</div>

<p>Pause for a moment to really think about this. Words are discrete objects, and a “small change in a word”, while intuitive to humans, is ill-defined. But this approach concretizes what that means. To quote the paper <em>Linguistic regularities in continuous space word representations</em> <a href="#mikolov2013linguistic">(Mikolov et al., 2013)</a>, which we’ll discuss later:</p>

<blockquote>
  <p>Whereas an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.</p>
</blockquote>

<p>For example, if the words “dog” and “cat” are nearby in word-embedding space, then maybe “The cat is walking on the sidewalk” and “The dog is walking on the sidewalk” should have similar probabilities. And only one of these two sentences would need to exist in the training data for the model to generalize well to both sentences!</p>

<p>As I mentioned, this idea was not entirely new in 2003. Since the 1980s, researchers had known that neural networks can generalize because they distribute their representation across many neurons <a href="#hinton1986learning">(Hinton, 1986)</a>. Each new example modifies the weights, incorporating new knowledge into the old. However <a href="#bengio2003neural">(Bengio et al., 2003)</a> is a landmark paper in NLP because it was the first application of this idea to language modeling. The Bengio paper took seriously the idea that we could build a statistical model of language using the distributed representations of words. It was the first hint that we could use neural networks to overcome the curse of dimensionality that plagued statistical NLP.</p>

<h2 id="autoregressive-framework">Autoregressive framework</h2>

<p>This is a promising idea, but we glossed over an important detail: how do we actually train this model? What is the loss function or objective that the neural network should use? And given a fit model, how do we generate a new sequence? These are important questions to answer per se, but they are also important questions because, at a conceptual level, there is really no difference between Bengio’s model and the frontier large language models today. So understanding this is critical to understanding LLMs. Both are <em>autoregressive</em> models and trained using <em>next-word prediction</em>.</p>

<p>As an example, imagine we have the following input sentence, which is a quote from Virginia Woolf’s <em>A Room of One’s Own</em>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtext>“Intellectual freedom depends upon material things.”</mtext></mtd><mtd width="50%"></mtd><mtd><mtext>(8)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\text{``Intellectual freedom depends upon material things.&#39;&#39;} \tag{8}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>“Intellectual freedom depends upon material things.”</span></span></span><span><span></span><span><span>(</span><span><span>8</span></span><span>)</span></span></span></span></span></span></p>

<p>Now imagine that our model’s context window has size <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N=2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span></span></span></span> and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_p</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>p</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> denote a padding <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-vector of all zeros. In Bengio’s model, we would start by representing just the first word, “intellectual”, as a word embedding. So the first non-zero input to our model would be:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">x</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mi>p</mi></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mi>p</mi></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><mtext>“intellectual”</mtext><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(9)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{x}_2 = \left[ \begin{array}{l}
\mathbf{c}_p
\\
\mathbf{c}_{I(w_1)}
\end{array} \right] = \left[ \begin{array}{l}
\mathbf{c}_p
\\
\mathbf{c}_{I(\text{``intellectual&#39;&#39;})}
\end{array} \right]. \tag{9}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>p</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span>]</span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>p</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>“intellectual”</span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span>]</span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>9</span></span><span>)</span></span></span></span></span></span></p>

<p>The output of the neural network would be a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span>-dimensional vector representing the probability distribution over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_2 \mid w_1)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. Illustratively:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">y</mi><mn>2</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>=</mo><mtext>“about”</mtext><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>=</mo><mtext>“above”</mtext><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="2em"></mspace><mtext>  </mtext><mtext>  </mtext><mi><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>=</mo><mtext>“freedom”</mtext><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="2em"></mspace><mtext>  </mtext><mtext>  </mtext><mi><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(10)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{y}_2 = \left[ \begin{array}{l}
p(w_2 = \text{``about&#39;&#39;})
\\
p(w_2 = \text{``above&#39;&#39;})
\\
\qquad\;\;\vdots
\\
p(w_2 = \text{``freedom&#39;&#39;})
\\
\qquad\;\;\vdots
\\
\end{array} \right]. \tag{10}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span><span></span><span><span>⎣</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎡</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span></span><span><span><span><span><span><span></span><span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>“about”</span></span><span>)</span></span></span><span><span></span><span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>“above”</span></span><span>)</span></span></span><span><span></span><span><span></span><span></span><span></span><span><span>⋮</span><span></span></span></span></span><span><span></span><span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span>“freedom”</span></span><span>)</span></span></span><span><span></span><span><span></span><span></span><span></span><span><span>⋮</span><span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span><span><span><span><span><span></span><span><span>⎦</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎤</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>0</span></span><span>)</span></span></span></span></span></span></p>

<p>We would then compute the cross-entropy loss between this output vector and the true distribution, which is really just a one-hot vector with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span> for the word “freedom” and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>0</span></span></span></span> everywhere else. We would then repeat this process on the next word. So the next input sequence would be</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">x</mi><mn>3</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><mtext>“intellectual”</mtext><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><mtext>“freedom”</mtext><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(11)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{x}_3 = \left[ \begin{array}{l}
\mathbf{c}_{I(\text{``intellectual&#39;&#39;})}
\\
\mathbf{c}_{I(\text{``freedom&#39;&#39;})}
\end{array} \right], \tag{11}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>3</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>“intellectual”</span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>“freedom”</span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span>]</span></span></span><span></span><span>,</span></span><span><span></span><span><span>(</span><span><span>1</span><span>1</span></span><span>)</span></span></span></span></span></span></p>

<p>and the output would represent the probability distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_3 \mid w_{1:2})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. And again, we would minimize the cross-entropy loss between its associated output vector and a one-hot vector encoding the word “depends”. We would repeat this process until the end of the sentence.</p>

<p>Of course, longer sequences are more expensive to train in this way, and this is precisely the point of the context window in Bengio’s paper. We only consider the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> previous words when predicting the next word. This idea of a limited context window is critical, as it is a constraint that persists into the present day. In this example, since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N=2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span></span></span></span>, the third input would be</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">x</mi><mn>4</mn></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><mtext>“freedom”</mtext><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><mtext>“depends”</mtext><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(12)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{x}_4 = \left[ \begin{array}{l}
\mathbf{c}_{I(\text{``freedom&#39;&#39;})}
\\
\mathbf{c}_{I(\text{``depends&#39;&#39;})}
\end{array} \right]. \tag{12}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>4</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>[</span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>“freedom”</span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>“depends”</span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span>]</span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>2</span></span><span>)</span></span></span></span></span></span></p>

<p>So the model completely loses the word “intellectual”. It is now outside the context.</p>

<p>Since minimizing the cross-entropy loss is equivalent to maximizing the log likelihood—see <a href="https://stats.stackexchange.com/questions/364216/" target="_blank">here</a> for an example if this idea is new to you—we can generalize the logic above by saying that we want to maximize the log likelihood of our training data, again using a neural network as a parametric function approximation of the true distribution:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msup><mi mathvariant="bold-italic">Θ</mi><mo lspace="0em" rspace="0em">⋆</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><mtext> ⁣</mtext><munder><mo><mi>max</mi><mo>⁡</mo></mo><mi mathvariant="bold-italic">Θ</mi></munder><mrow><mo fence="true">{</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>g</mi><mi mathvariant="bold-italic">Ω</mi></msub><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo fence="true">)</mo></mrow><mo fence="true">}</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(13)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\boldsymbol{\Theta}^{\star} = \arg\!\max_{\boldsymbol{\Theta}} \left\{ \sum_{t=1}^T \log g_{\boldsymbol{\Omega}} \left(\mathbf{c}_{I(w_{t-N})}, \dots, \mathbf{c}_{I(w_{t-1})} \right) \right\}. \tag{13}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>Θ</span></span></span><span><span><span><span><span><span></span><span><span><span>⋆</span></span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>ar<span>g</span></span><span></span><span></span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span><span><span></span><span><span>max</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span>{</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span><span>g</span><span><span><span><span><span><span></span><span><span><span><span><span>Ω</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span><span>(</span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span><span><span>}</span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>3</span></span><span>)</span></span></span></span></span></span></p>

<p>Of course, we can estimate <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">Θ</mi><mo lspace="0em" rspace="0em">⋆</mo></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Theta}^{\star}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>Θ</span></span></span><span><span><span><span><span><span></span><span><span><span>⋆</span></span></span></span></span></span></span></span></span></span></span></span> by minimizing the negative log likelihood using gradient descent via back-propagation. That’s it. At the conceptual level, this framework is no different from how frontier large language models are trained today. As we will see later though, there is a lot of additional machinery that is needed to make these models work in practice.</p>

<p>Finally, imagine we fit our model, meaning we find good parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold-italic">Θ</mi><mo lspace="0em" rspace="0em">⋆</mo></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{\Theta}^{\star}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>Θ</span></span></span><span><span><span><span><span><span></span><span><span><span>⋆</span></span></span></span></span></span></span></span></span></span></span></span> that maximize our log likelihood. How can we use these parameters to generate a random sequence or sentence? We could draw the first word at random from the vocabulary. And then we could draw the next word conditional on the first word from our parametric approximation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>∣</mo><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_2 \mid w_1)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. And then we could draw the third word conditional on the second and first words from our parametric approximation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>∣</mo><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_3 \mid w_{1:2})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>3</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. And so on. This is why LLMs can both understand natural language and generate new sentences. They are not just descriptive models; they are <a href="https://404wolf.com/blog/2018/06/13/probabilistic-ml/">generative models</a>.</p>

<p>There are some subtleties I am glossing over, such as special embeddings to denote the start and end of a sequence, preprocessing steps like lowercasing words, tokenization, and handling out-of-vocabulary words. But I don’t think these details matter much here.</p>

<p>As an aside, we can call any model trained in this way <em>autoregressive</em>. In statistics, an autoregressive model is any model where a variable is predicted using its own previous values. A classic example of this are <a href="https://404wolf.com/blog/2022/01/06/autoregressive-model/">AR models</a> such as AR(1).</p>

<h2 id="the-breakthrough">The breakthrough</h2>

<p>While <a href="#bengio2003neural">(Bengio et al., 2003)</a> was a landmark paper, its full impact was delayed by roughly a decade. This is because training neural networks was hard at the time. It’s worth checking out that paper and seeing just how primitive the engineering feels today. For example, they trained on CPUs and without modern tooling like automatic differentiation libraries.</p>

<p>In the intervening decade, there was some early work that built on Bengio’s model. For example, in <em>A unified architecture for natural language processing: Deep neural networks with multitask learning</em> <a href="#collobert2008unified">(Collobert &amp; Weston, 2008)</a>, the authors demonstrate that Bengio’s neural language model could be trained and used on a variety of downstream tasks. And in <em>Word representations: A simple and general method for semi-supervised learning</em> <a href="#turian2010word">(Turian et al., 2010)</a>, the authors demonstrate that word embeddings improve state-of-the-art NLP systems when included as additional features. But none of these contributions were convincing demonstrations of Bengio’s main idea.</p>

<p>So seven years after Bengio et al, it was <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-grams, not neural networks, which were still the state-of-the-art, at least in practice and outside specialized benchmarks. Honestly, I found this surprising, but I kept reading this claim in various papers. For example, in the introduction to <em>Recurrent neural network based language model</em> <a href="#mikolov2010recurrent">(Mikolov et al., 2010)</a>, the authors wrote:</p>

<blockquote>
  <p>It is questionable if there has been any significant progress in language modeling over simple <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram models… In fact, most of the proposed advanced language modeling techniques provide only tiny improvements over simple baselines, and are rarely used in practice.</p>
</blockquote>

<p>Or two years after that, in <em>A fast and simple algorithm for training neural probabilistic language models</em> <a href="#mnih2012fast">(Mnih &amp; Teh, 2012)</a>, the authors wrote:</p>

<blockquote>
  <p>In spite of their superior performance, neural probabilistic language models remain far less widely used than <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets.</p>
</blockquote>

<p>Of course, advanced techniques existed and were well known, but they were often impractical. So roughly a hundred years after Andrey Markov’s pioneering work, researchers were still struggling to represent human language in a form amenable for mathematics and computation, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-grams were still considered a reasonable choice in NLP. Today, neural networks are definitively state-of-the-art. What changed? The answer is that we learned to train variants of Bengio’s model at scale.</p>

<p>Around 2012, researchers were finally able to train neural networks on large datasets. My understanding is that it was the so-called “AlexNet” paper, <em>ImageNet classification with deep convolutional neural networks</em> <a href="#krizhevsky2012imagenet">(Krizhevsky et al., 2012)</a>, that convinced many in the research community to pay attention. Convolutional neural networks were already well known and had been trained on small datasets since the 1980s <a href="#lecun1989backpropagation">(LeCun et al., 1989)</a>. But AlexNet was the first time a deep convolutional neural network was trained end-to-end on a very large (at the time) dataset, ImageNet <a href="#deng2009imagenet">(Deng et al., 2009)</a> and using GPUs. The results were a tour de force. To quote the paper:</p>

<blockquote>
  <p>We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn></mrow><annotation encoding="application/x-tex">5</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>5</span></span></span></span> test error rate of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>15.3</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">15.3\%</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>5</span><span>.</span><span>3</span><span>%</span></span></span></span>, compared to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>26.2</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">26.2\%</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>6</span><span>.</span><span>2</span><span>%</span></span></span></span> achieved by the second-best entry.</p>
</blockquote>

<p>In other words, AlexNet demolished the state-of-the-art in computer vision. It achieved a roughly <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">40\%</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span><span>0</span><span>%</span></span></span></span> reduction in relative error rate. Nothing else came close. As a comparison, the current fastest time for a men’s marathon is 2 hours and 35 seconds. The previous record was 2 hours and 69 seconds, so 34 seconds slower. Now imagine if someone came along and beat the record by half an hour. It would revolutionize the running world.</p>

<p>At the time, computer vision was still dominated by handcrafted feature pipelines, and so the AlexNet results were extremely surprising. For example, in <em>Introduction to the bag of features paradigm for image classification and retrieval</em> <a href="#ohara2011introduction">(O’Hara &amp; Draper, 2011)</a>, the authors wrote:</p>

<blockquote>
  <p>The past decade has seen the growing popularity of Bag of Features (BoF) approaches to many computer vision tasks, including image classification, video search, robot localization, and texture recognition… BoF-based systems have set new performance standards on popular image classification benchmarks and have achieved scalability breakthroughs in image retrieval.</p>
</blockquote>

<p>This introduction to bag of feature models was put on arXiv in January 2011, whereas AlexNet was published at NeurIPS in December 2012, meaning that the claim above was contemporaneous with the training of AlexNet! My point here is to underscore just how surprising the rise of neural networks was. To be clear, I am sure many in the research community believed neural networks would work—Hinton has been a believer since probably the 1970s—, but this was hardly the consensus view that it is today. So the year 2012 was a changepoint. In 2003, Bengio et al set the stage conceptually. In 2012, Krizhevsky et al set the stage technologically.</p>

<h2 id="shallow-log-linear-models">Shallow log-linear models</h2>

<p>With hindsight, the obvious implication of AlexNet was that NLP researchers circa 2012 should try to train neural networks at scale. Of course, many researchers tried, but let’s ground ourselves in one particular model. This will help focus the narrative.</p>

<p>To my knowledge, two of the earliest and most successful papers to try this idea were <em>Efficient estimation of word representations in vector space</em> <a href="#mikolov2013efficient">(Mikolov et al., 2013)</a> and <em>Distributed representations of words and phrases and their compositionality</em> <a href="#mikolov2013distributed">(Mikolov et al., 2013)</a>. These papers are tightly related by both authorship and time, and together, they helped unlock the core ideas in Bengio’s paper, as well as introduce the famous <a href="https://code.google.com/archive/p/word2vec/" target="_blank">word2vec</a> model. So I think it’s fair to treat them as both a unit and as a landmark in our story.</p>

<p>To understand these two papers, we need to understand the computational problems Bengio faced, which means we need to understand the model in more technical detail. Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> be the input to the model, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> be the output. Bengio’s model did not support variable-length inputs, and thus the input sequence could be only a fixed number of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> words, each represented as an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-dimensional embedding. Let’s represent this input as the concatenation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> different <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-vectors from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span> mentioned above, so:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>:</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mspace width="1em"></mspace><mi><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(14)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{x}_t := \left[ \begin{array}{l}
\mathbf{c}_{I(w_{t-1})} \\ \quad\quad\vdots \\ \mathbf{c}_{I(w_{t-N+1})}
\end{array} \right]. \tag{14}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>:</span></span><span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span><span></span><span><span>⎣</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎢</span></span></span><span><span></span><span><span>⎡</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span></span><span></span><span><span>⋮</span><span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span><span>+</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span><span><span><span><span><span></span><span><span>⎦</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎥</span></span></span><span><span></span><span><span>⎤</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>4</span></span><span>)</span></span></span></span></span></span></p>

<p>One way we can imagine constructing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is if we represent every word in our context window as a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span>-dimensional one-hot vector. Call this a matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">Q</mi><mi>t</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>V</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Q}_t \in \mathbb{R}^{N \times V}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>Q</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>×</span><span>V</span></span></span></span></span></span></span></span></span></span></span></span>. Then <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>=</mo><msub><mi mathvariant="bold">Q</mi><mi>t</mi></msub><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}_t = \mathbf{Q}_t \mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>Q</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>C</span></span></span></span></span> gives us the associated embeddings. In practice, though, we would never do a dense matrix multiplication with complexity <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>V</mi><mi>N</mi><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(VND)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span></span><span>(</span><span>V</span><span>N</span><span>D</span><span>)</span></span></span></span>. Instead, we would simply index into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span>. So this operation has computational complexity <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(ND)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span></span><span>(</span><span>N</span><span>D</span><span>)</span></span></span></span>. I only belabor this point because I found it confusing when first reading Bengio’s paper. (This point is made more clearly in <a href="#collobert2008unified">(Collobert &amp; Weston, 2008)</a>)</p>

<p>After construction, this input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is then fed into an extremely simple (relative to today’s models) architecture, a feed-forward neural network with a linear projection layer and a nonlinear hidden layer:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>g</mi><mi mathvariant="bold-italic">Ω</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><mi mathvariant="bold">b</mi><mo>+</mo><msub><mrow><mi mathvariant="bold">W</mi><mi mathvariant="bold">x</mi></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold">U</mi><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">z</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">z</mi><mi>t</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><mi mathvariant="bold">d</mi><mo>+</mo><msub><mrow><mi mathvariant="bold">H</mi><mi mathvariant="bold">x</mi></mrow><mi>t</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(15)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
 g_{\boldsymbol{\Omega}}(\mathbf{x}_t) = \mathbf{y}_t &amp;:= \mathbf{b} + \mathbf{Wx}_t + \mathbf{U} \tanh(\mathbf{z}_t),
\\
\mathbf{z}_t &amp;:= \mathbf{d} + \mathbf{Hx}_t.
\end{aligned} \tag{15}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>g</span><span><span><span><span><span><span></span><span><span><span><span><span>Ω</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span>b</span></span><span></span><span>+</span><span></span><span><span><span>W</span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span>U</span></span><span></span><span>tanh</span><span>(</span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span>d</span></span><span></span><span>+</span><span></span><span><span><span>H</span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>1</span><span>5</span></span><span>)</span></span></span></span></span></span></p>

<p>The output <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>V</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{y}_t \in \mathbb{R}^{V}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>V</span></span></span></span></span></span></span></span></span></span></span></span> represents the un-normalized probability of each word in the vocabulary. If normalized, this vector would represent the probability distribution we discussed in the autoregressive framework.</p>

<p>Here, we see that that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>×</mo><mi>N</mi><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{V \times ND}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>V</span><span>×</span><span>N</span><span>D</span></span></span></span></span></span></span></span></span></span></span></span> is a linear projection of the input embeddings <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">H</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>N</mi><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{H} \in \mathbb{R}^{H \times ND}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>H</span><span>×</span><span>N</span><span>D</span></span></span></span></span></span></span></span></span></span></span></span> is a linear projection into a hidden state vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">z</mi><mi>t</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{z}_t \in \mathbb{R}^H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span>H</span></span></span></span></span></span></span></span></span></span></span>, and that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">U</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>×</mo><mi>H</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{U} \in \mathbb{R}^{V \times H}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>U</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>V</span><span>×</span><span>H</span></span></span></span></span></span></span></span></span></span></span></span> is a linear projection of the nonlinear hidden state vector. So clearly the parameters mentioned in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn></mrow><annotation encoding="application/x-tex">7</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>7</span></span></span></span> can be concretized as</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mo stretchy="false">{</mo><mi mathvariant="bold">C</mi><mo separator="true">,</mo><mi mathvariant="bold-italic">Ω</mi><mo stretchy="false">}</mo><mo>:</mo><mo>=</mo><mo stretchy="false">{</mo><mrow><mi mathvariant="bold">C</mi><mo separator="true">,</mo><mi mathvariant="bold">b</mi><mo separator="true">,</mo><mi mathvariant="bold">W</mi><mo separator="true">,</mo><mi mathvariant="bold">U</mi><mo separator="true">,</mo><mi mathvariant="bold">d</mi><mo separator="true">,</mo><mi mathvariant="bold">H</mi></mrow><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(16)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\{\mathbf{C}, \boldsymbol{\Omega}\} := \{\mathbf{C, b, W, U, d, H}\}. \tag{16}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>C</span></span><span>,</span><span></span><span><span><span>Ω</span></span></span><span>}</span><span></span><span>:</span></span><span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span>C</span><span>,</span><span></span><span>b</span><span>,</span><span></span><span>W</span><span>,</span><span></span><span>U</span><span>,</span><span></span><span>d</span><span>,</span><span></span><span>H</span></span><span>}</span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>6</span></span><span>)</span></span></span></span></span></span></p>

<p>So why was this expensive to train? We can see that the computational complexity to compute <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is proportional to:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><munder><munder><mrow><mi>N</mi><mi>D</mi></mrow><mo stretchy="true">⏟</mo></munder><mrow><mi mathvariant="bold">Q</mi><mi mathvariant="bold">C</mi></mrow></munder><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><mo>+</mo><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><munder><munder><mrow><mi>V</mi><mi>N</mi><mi>D</mi></mrow><mo stretchy="true">⏟</mo></munder><msub><mrow><mi mathvariant="bold">W</mi><mi mathvariant="bold">x</mi></mrow><mi>t</mi></msub></munder><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><mo>+</mo><mtext>  </mtext><munder><munder><mrow><mi>V</mi><mi>H</mi></mrow><mo stretchy="true">⏟</mo></munder><mrow><mi mathvariant="bold">U</mi><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></munder><mo>+</mo><mtext>  </mtext><mtext>  </mtext><munder><munder><mrow><mi>H</mi><mi>N</mi><mi>D</mi></mrow><mo stretchy="true">⏟</mo></munder><msub><mrow><mi mathvariant="bold">H</mi><mi mathvariant="bold">x</mi></mrow><mi>t</mi></msub></munder><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(17)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\underbrace{ND}_{\mathbf{QC}}
\;\;\;+\;\;\;
\underbrace{VND}_{\mathbf{Wx}_t}
\;\;\;\;+\;
\underbrace{VH}_{\mathbf{U} \tanh(\mathbf{z}_t)}
+\;\;
\underbrace{HND}_{\mathbf{Hx}_t}. \tag{17}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span><span>Q</span><span>C</span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13  35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688  0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7 -331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214 c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14  53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3  11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0 -5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3  28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237 -174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span>N</span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span></span><span>+</span><span></span><span></span><span></span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span>W</span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13  35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688  0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7 -331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214 c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14  53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3  11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0 -5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3  28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237 -174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span>V</span><span>N</span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span></span><span></span><span></span><span></span><span>+</span><span></span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>U</span></span><span></span><span><span>t</span><span>a</span><span>n</span><span>h</span></span><span>(</span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13  35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688  0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7 -331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214 c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14  53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3  11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0 -5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3  28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237 -174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span>V</span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>+</span><span></span><span></span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span>H</span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMinYMin slice"><path d="M0 6l6-6h17c12.688 0 19.313.3 20 1 4 4 7.313 8.3 10 13  35.313 51.3 80.813 93.8 136.5 127.5 55.688 33.7 117.188 55.8 184.5 66.5.688  0 2 .3 4 1 18.688 2.7 76 4.3 172 5h399450v120H429l-6-1c-124.688-8-235-61.7 -331-161C60.687 138.7 32.312 99.3 7 54L0 41V6z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMidYMin slice"><path d="M199572 214 c100.7 8.3 195.3 44 280 108 55.3 42 101.7 93 139 153l9 14c2.7-4 5.7-8.7 9-14  53.3-86.7 123.7-153 211-199 66.7-36 137.3-56.3 212-62h199568v120H200432c-178.3  11.7-311.7 78.3-403 201-6 8-9.7 12-11 12-.7.7-6.7 1-18 1s-17.3-.3-18-1c-1.3 0 -5-4-11-12-44.7-59.3-101.3-106.3-170-141s-145.3-54.3-229-60H0V214z"></path></svg></span><span><svg width="400em" height="0.548em" viewBox="0 0 400000 548" preserveAspectRatio="xMaxYMin slice"><path d="M399994 0l6 6v35l-6 11c-56 104-135.3 181.3-238 232-57.3  28.7-117 45-179 50H-300V214h399897c43.3-7 81-15 113-26 100.7-33 179.7-91 237 -174 2.7-5 6-9 10-13 .7-1 7.3-1 20-1h17z"></path></svg></span></span></span><span><span></span><span><span>H</span><span>N</span><span>D</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>7</span></span><span>)</span></span></span></span></span></span></p>

<p>Note that this complexity is for every single word in the corpus, and we must also account for the number of training epochs. In <a href="#mikolov2013efficient">(Mikolov et al., 2013)</a>, the authors write that a “common choice” is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">N=10</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span><span>0</span></span></span></span> and that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">ND</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span>D</span></span></span></span> is typically around <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500</mn></mrow><annotation encoding="application/x-tex">500</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>5</span><span>0</span><span>0</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2000</mn></mrow><annotation encoding="application/x-tex">2000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span><span>0</span><span>0</span></span></span></span>. However, the hidden layer has dimension <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> (commonly around <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2000</mn></mrow><annotation encoding="application/x-tex">2000</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span><span>0</span><span>0</span></span></span></span> or so) and this is multiplied by the size of the vocabulary! What does this mean? The dominating term in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>17</mn></mrow><annotation encoding="application/x-tex">17</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>7</span></span></span></span> is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>H</mi></mrow><annotation encoding="application/x-tex">VH</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span><span>H</span></span></span></span>. Furthermore, this complexity is just for computing the un-normalized probabilities <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. To normalize these, we must compute the softmax function over the size of the vocabulary <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">y</mi><mi>i</mi></msub><mo fence="true">)</mo></mrow></mrow></mfrac><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(18)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_t \mid w_{t-N:t-1}) = \frac{\exp\left(\mathbf{y}_t\right)}{\sum_{i=1}^V \exp\left( \mathbf{y}_i \right)}. \tag{18}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>V</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>exp</span><span></span><span><span>(</span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>exp</span><span></span><span><span>(</span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>8</span></span><span>)</span></span></span></span></span></span></p>

<p>As I understand it, these were the computational problems Bengio faced. The two Mikolov papers did not present a single trick to solve them. Rather, the papers made a number of modeling choices, mostly already established in the literature, that in combination finally made learning distributed representations of words scalable.</p>

<p>First, in the first paper, they avoided computing the full softmax function using hierarchical softmax, introduced by Morin and Bengio in <em>Hierarchical probabilistic neural network language model</em> <a href="#morin2005hierarchical">(Morin &amp; Bengio, 2005)</a>. I don’t think the details of this matter much here. See <a href="https://talbaumel.github.io/blog/softmax/" target="_blank">this blog post</a> for a nice explanation with code. Suffice to say that it’s an efficient way to compute the normalized probabilities in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>18</mn></mrow><annotation encoding="application/x-tex">18</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>8</span></span></span></span>. The computational complexity is reduced from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(V)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span></span><span>(</span><span>V</span><span>)</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mo><mi>log</mi><mo>⁡</mo></mo><mn>2</mn></msub><mi>V</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(\log_2 V )</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>O</span></span><span>(</span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>V</span><span>)</span></span></span></span>. In the second paper, they further sped up the softmax computation by introducing a technique called <em>negative sampling</em>. The theory here is rich and deserving of its own post, but the main idea is to draw <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> samples from a noise distribution and train the model to disambiguate observations from noise. The important point here is that one can prove this converges to the correct probabilities without explicitly computing the normalizing constant. See <a href="#gutmann2010noise">(Gutmann &amp; Hyvärinen, 2010)</a> for details. We don’t need to fully grok these techniques; just know that these two approaches are both ways of getting around the expensive normalization in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>18</mn></mrow><annotation encoding="application/x-tex">18</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>8</span></span></span></span>. For example, if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">V = 1\times 10^6</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span>6</span></span></span></span></span></span></span></span></span></span></span>, then <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo><mi>log</mi><mo>⁡</mo></mo><mn>2</mn></msub><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo><mo>≈</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">\log_2(V) \approx 20</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>V</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span>2</span><span>0</span></span></span></span>. And in the second paper, they chose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> to be <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span></span></span></span> depending on the dataset.</p>

<p>Second, they stripped out the non-linear part of Bengio’s model (so removing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">U</mi><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{U} \tanh(\mathbf{z}_t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>U</span></span><span></span><span>tanh</span><span>(</span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>), reducing the model to a simple linear operation: a dot product. The result is model that is log-linear on the features, which I’ll explain in a moment.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig3-word2vec.png" alt=""/></p><p><span>Figure 3.</span> Diagrams of the continuous bag-of-words model or CBOW (left) and the skip-gram model (right). CBOW is trained such that neighboring words predict a target word. Skip-gram is trained such that a center word predicts its neighboring words. Both are simple log-linear models.
    </p>
</div>

<p>Now the models. In the first paper, they presented two models, a continuous bag-of-words model (CBOW) and a continuous skip-gram model (skip-gram). These are the foundations of the <a href="https://code.google.com/archive/p/word2vec/" target="_blank">word2vec</a> NLP toolkit. In the CBOW model, a set of neighboring words are averaged to predict a target word; and in the skip-gram model, a target word is used to predict its neighboring words (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span></span></span></span>). Both worked empirically in practice, but the authors only built on the skip-gram model in the second paper. And since I don’t think it’s that important here to understand both, I’ll just focus on the skip-gram model.</p>

<p>Let’s build a little intuition by going into detail. The objective of the skip-gram model is to minimize the cross-entropy loss between a single target word and its neighboring words. So the input to the model is only a single <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span>-vector representing a single word (so no context window). The output, however, are the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> words surrounding the input. Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn><mi>C</mi></mrow><annotation encoding="application/x-tex">N = 2C</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>2</span><span>C</span></span></span></span>. Then the objective function is:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><munder><mo>∑</mo><mrow><mo>−</mo><mi>C</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>C</mi><mo separator="true">,</mo><mtext>  </mtext><mi>j</mi><mo mathvariant="normal">≠</mo><mn>0</mn></mrow></munder><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(19)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\frac{1}{T} \sum_{t=1}^T \sum_{-C \leq j \leq C,\;j \neq 0} \log p(w_{t+j} \mid w_t). \tag{19}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>T</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>−</span><span>C</span><span>≤</span><span>j</span><span>≤</span><span>C</span><span>,</span><span></span><span>j</span><span><span><span><span><span><span><span><span></span><span><span></span><span><span></span></span><span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>=</span></span><span>0</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span><span>9</span></span><span>)</span></span></span></span></span></span></p>

<p>I will continue to use the notation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> for this context window, but clearly it is different in precise meaning from the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> in an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram or the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> in Bengio’s paper. We model the conditional probability in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>19</mn></mrow><annotation encoding="application/x-tex">19</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>9</span></span></span></span> via a simple log-linear function:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">u</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo>∣</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi mathvariant="bold">u</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">⟩</mo><mo fence="true">)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi mathvariant="script">V</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi mathvariant="bold">u</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">⟩</mo></mrow></msub><mo fence="true">)</mo></mrow></mrow></mfrac></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(20)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
p(w_{t+j} \mid w_t)
= p(\mathbf{u}_{I(w_{t+j})} \mid \mathbf{c}_{I(w_{t})})
= \frac{\exp\left( \langle \mathbf{u}_{I(w_{t+j})}, \mathbf{c}_{I(w_{t})} \rangle \right)}{\sum_{i \in \mathcal{V}} \exp\left( \langle \mathbf{u}_i, \mathbf{c}_{I(w_{t}) \rangle} \right)} \tag{20}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>p</span><span>(</span><span><span><span>u</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>i</span><span>∈</span><span><span>V</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>exp</span><span></span><span><span><span>(</span></span><span>⟨</span><span><span><span>u</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>⟩</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>exp</span><span></span><span><span><span>(</span></span><span>⟨</span><span><span><span>u</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span><span>)</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span></span><span><span></span><span><span>(</span><span><span>2</span><span>0</span></span><span>)</span></span></span></span></span></span></p>

<p>Here, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are word embeddings of the inputs. These are analogous to the row-vectors of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span> in Bengio’s model and again are constructed via a lookup. The output embeddings <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">u</mi></mrow><annotation encoding="application/x-tex">\mathbf{u}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>u</span></span></span></span></span> are a little trickier to interpret. If we were using the full softmax function, we would have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span> such output embeddings, and these would represent the weights of the softmax function. But when using hierarchical softmax or negative sampling, the interpretation changes a bit. Again, I don’t think the details really matter here. The key point is that we take a sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, select the appropriate embeddings <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and compute Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span></span></span></span> directly, learning both the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>C</span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">U</mi></mrow><annotation encoding="application/x-tex">\mathbf{U}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>U</span></span></span></span></span>.</p>

<p>This is called a “log-linear model” because the log of the conditional probability is linear with respect to its arguments:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo>∣</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi mathvariant="bold">u</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mrow><mi>I</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">⟩</mo><mo>−</mo><mi>Z</mi><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(21)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\log p(w_{t+j} \mid w_t) = \langle \mathbf{u}_{I(w_{t+j})}, \mathbf{c}_{I(w_{t})} \rangle  - Z, \tag{21}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>lo<span>g</span></span><span></span><span>p</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span><span>u</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>+</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span><span>I</span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span></span><span>−</span><span></span></span><span><span></span><span>Z</span><span>,</span></span><span><span></span><span><span>(</span><span><span>2</span><span>1</span></span><span>)</span></span></span></span></span></span></p>

<p>Here, I just write <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Z</span></span></span></span> to denote the normalizing constant, the denominator in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span></span></span></span>, because it is not particularly interesting, and we do not even need to compute it when using negative sampling. The key relationship that the model is learning is a simple linear weighting of the input embeddings that allow it to predict nearby words.</p>

<p>Hopefully, it is clear why this model is so fast to train. We have no hidden layers or nonlinearities. We simply compute a dot product and ignore the normalizing constant. For example, when using the full softmax, the computational complexity is:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>N</mi><mo stretchy="false">(</mo><mi>D</mi><mo>+</mo><mi>D</mi><mi>V</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(22)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
N (D + D V). \tag{22}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span>(</span><span>D</span><span></span><span>+</span><span></span></span><span><span></span><span>D</span><span>V</span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>2</span></span><span>)</span></span></span></span></span></span></p>

<p>Here, we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>+</mo><mi>D</mi><mi>V</mi></mrow><annotation encoding="application/x-tex">D + D V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span><span></span><span>+</span><span></span></span><span><span></span><span>D</span><span>V</span></span></span></span> dot products, and we need to do it over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> words in our context window. However, in practice, we can eliminate <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span></span></span></span> entirely, replacing it with something around <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo><mi>log</mi><mo>⁡</mo></mo><mn>2</mn></msub><mo stretchy="false">(</mo><mi>V</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log_2(V)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>lo<span>g</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>V</span><span>)</span></span></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span>. This is significantly smaller than Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>17</mn></mrow><annotation encoding="application/x-tex">17</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>7</span></span></span></span>. For example, if we assume that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mi>D</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">H=D=500</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span><span></span><span>=</span><span></span></span><span><span></span><span>D</span><span></span><span>=</span><span></span></span><span><span></span><span>5</span><span>0</span><span>0</span></span></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">N=10</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span><span>0</span></span></span></span>, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">V=1 \times 10^{6}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>V</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>6</span></span></span></span></span></span></span></span></span></span></span></span>, then hierarchical softmax is five orders of magnitude smaller in terms of complexity.</p>

<p>So in these two seminal Mikolov papers, the authors stripped down Bengio’s core idea to a simple log-linear model, and thus were able to train that model at scale. That said, I want to stress a subtlety that took me time to grok. Neither the CBOW nor the continuous skip-gram models presented here are full language models. Notice that their objective functions (nearby-word prediction) are not in the autoregressive framework and thus cannot easily plug into Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span>. That’s because the goal of these papers was not to learn a full language model but rather to learn good word embeddings. They say this explicitly in the first paper (emphasis mine):</p>

<blockquote>
  <p>Representation of words as continuous vectors has a long history. A very popular model
architecture for estimating neural network language model (NNLM) was proposed in <a href="#bengio2003neural">(Bengio et al., 2003)</a>, where a feed-forward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.</p>

  <p>Another interesting architecture of NNLM was presented in <a href="#mikolov2007language">(Mikolov, 2007; Mikolov et al., 2009)</a>, where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. <em>In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.</em></p>
</blockquote>

<p>So the word2vec models were simple and shallow (single layer) neural networks designed for fast training and to learn good embeddings. They were not full language models. This is a major distinction from similar prior art, such as <em>A scalable hierarchical distributed language model</em> <a href="#mnih2008scalable">(Mnih &amp; Hinton, 2008)</a>. In this paper, the authors demonstrate more scalable inference of Bengio’s model by representing the vocabulary compactly through binary trees and by using a log-bilinear model. But they go end-to-end to a language model, as the paper title suggests. Mikolov et al’s two models were relentlessly simple and efficient.</p>

<p>As I understand it, both CBOW and skip-gram worked well in practice. It did not matter if neighboring words predict a target word or if that target word predicts its neighboring words. The real differentiator was that both models could be efficiently trained at scale. And with scale, something remarkable happened: the authors discovered that distributed representations of words, trained in this fashion, captured semantic and syntactic information.</p>

<h2 id="emergent-linguistic-regularities">Emergent linguistic regularities</h2>

<p>Today, linguistic regularities in word embeddings is so well-established that it might seem boring to read here. But understood in context, these regularities should be surprising! How can a simple linear model, trained on essentially next- or nearby-word prediction via maximum likelihood estimation, learn distributed representations of words with remarkable syntactic and semantic properties and relationships? In my mind, this was the first big result that suggested neural networks would not just work but <em>really work</em> in language modeling.</p>

<p>The word2vec papers were not the first to observe these properties. My understanding is that that credit goes to yet another Mikolov paper from 2013, <em>Linguistic regularities in continuous space word representations</em> <a href="#mikolov2013linguistic">(Mikolov et al., 2013)</a>. Here, the authors showed that many semantic and syntactic relationships correspond to approximately constant vector offsets in the embedding’s vector space.</p>

<p>To be clear, researchers had long observed that one could uncover structure in vector representations of words. For example, in the 1989 paper <em>Self-organizing semantic maps</em> <a href="#ritter1989self">(Ritter &amp; Kohonen, 1989)</a>, the authors trained self-organizing maps <a href="#kohonen1982self">(Kohonen, 1982)</a> on pre-computed two-dimensional vectors representing words and demonstrated that these maps contain semantic structure. However, these models were not trained end-to-end (the representations themselves were not learned) and did not have linear structure. It would be a stretch to call these vectors “word embeddings”. But log-linear models like word2vec were remarkable precisely because they enabled analogical reasoning through simple vector offset, i.e. linear operations (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span></span></span></span>)!</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig4-algebraic-manipulation.png" alt=""/></p><p><span>Figure 4.</span> Word embeddings can capture semantic and syntactic regularities in natural language. Here, the vectors for &#34;paris&#34;, &#34;rome&#34;, &#34;france&#34;, and &#34;italy&#34; are illustrated. The vector algebra of &#34;paris - france + italy&#34; gives us a vector quite close to &#34;rome&#34;. This suggests that word embeddings are embedded in a vector space that captures some of the regularities of natural language.
    </p>
</div>

<p>Perhaps the most famous example of analogical reasoning with word embeddings is the relationship “king is to queen as man is to woman”:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“king”</mtext><mo fence="true">)</mo></mrow><mo>−</mo><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“man”</mtext><mo fence="true">)</mo></mrow><mo>+</mo><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“woman”</mtext><mo fence="true">)</mo></mrow><mo>≈</mo><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“queen”</mtext><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(23)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\text{vec}\left(\text{``king&#39;&#39;}\right) - \text{vec}\left(\text{``man&#39;&#39;}\right) + \text{vec}\left(\text{``woman&#39;&#39;}\right) \approx \text{vec}\left(\text{``queen&#39;&#39;}\right). \tag{23}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“king”</span></span><span>)</span></span><span></span><span>−</span><span></span></span><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“man”</span></span><span>)</span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“woman”</span></span><span>)</span></span><span></span><span>≈</span><span></span></span><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“queen”</span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>3</span></span><span>)</span></span></span></span></span></span></p>

<p>Or in <a href="#mikolov2013distributed">(Mikolov et al., 2013)</a>, the authors give the example that “Russia” plus “river” is the Volga:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“Russia”</mtext><mo fence="true">)</mo></mrow><mo>+</mo><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“river”</mtext><mo fence="true">)</mo></mrow><mo>≈</mo><mtext>vec</mtext><mrow><mo fence="true">(</mo><mtext>“Volga River”</mtext><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(24)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\text{vec}\left(\text{``Russia&#39;&#39;}\right) + \text{vec}\left(\text{``river&#39;&#39;}\right) \approx \text{vec}\left(\text{``Volga River&#39;&#39;}\right). \tag{24}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“Russia”</span></span><span>)</span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“river”</span></span><span>)</span></span><span></span><span>≈</span><span></span></span><span><span></span><span><span>vec</span></span><span></span><span><span>(</span><span><span>“Volga River”</span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>4</span></span><span>)</span></span></span></span></span></span></p>

<p>In my mind, these are pretty fascinating and non-obvious results. It suggests that the methods are not mixing vector dimensions in undesirable ways and staying approximately linear. Again, viewed with fresh eyes, it is really quite remarkable! If you were a researcher in 2003 reading Bengio’s paper, would you have predicted this result with high confidence?</p>

<p>While these two Mikolov papers are landmark papers on learning word embeddings at scale, they are by no means the only ones. Many other researchers worked in this area. Perhaps the most famous paper on word embeddings that we do not have time to discuss is <em>GloVe: Global vectors for word representation</em> <a href="#pennington2014glove">(Pennington et al., 2014)</a>. In this paper, the authors present a unifying view between two common methods for learning word embeddings, global matrix factorization methods and local context window methods. But there were many others as well, such as <em>Skip-thought vectors</em> <a href="#kiros2015skip">(Kiros et al., 2015)</a>, <em>Word embeddings through Hellinger PCA</em> <a href="#lebret2013word">(Lebret &amp; Collobert, 2013)</a>, and <em>Eigenwords: spectral word embeddings</em> <a href="#dhillon2015eigenwords">(Dhillon et al., 2015)</a> to cite just a few illustrative examples.</p>

<p>For ease of presentation, I have focused on word-level embeddings. But the idea was naturally and quickly extended to larger contexts. This was motivated by the fact that a word’s meaning is obviously context-dependent (polysemy). For example, the word “bank” might refer to a financial institution or the side of a river. A word embedding for “bank” that is not context dependent must somehow flatten this distinction. So lack of context is obviously a limitation.</p>

<p>Researchers tackled this through a variety of approaches. One approach was to use the hidden states of a bidirectional long short-term memory network (LSTM) as context-specific embeddings as in <em>context2vec: Learning generic context embedding with bidirectional LSTM</em> <a href="#melamud2016context2vec">(Melamud et al., 2016)</a> or <em>Learned in translation: contextualized word vectors</em> <a href="#mccann2017learned">(McCann et al., 2017)</a>. But perhaps the most noteworthy example of this idea—and one I mention here because it will come up later—was <em>Deep contextualized word representations</em> <a href="#peters2018dissecting">(Peters et al., 2018)</a> or ELMO. Here, the authors both used a bidirectional LSTM to extract more context-dependent word embeddings and then trained on an objective function that was dependent on the downstream task. This hints at combining pre-trained embeddings with supervised fine-tuning, which we’ll see later.</p>

<h2 id="long-range-dependencies">Long-range dependencies</h2>

<p>By 2013, word- and phrase-level embeddings demonstrably worked. The key to unlocking them was simple methods that scaled on modern hardware. However, the problem with these embeddings is that they were still with respect to a fixed window. It was not immediately obvious how this idea could be extended to longer phrases or sentences or to larger texts. Of course, researchers had tried. For example, <a href="#collobert2008unified">(Collobert &amp; Weston, 2008)</a> used the idea of time-delay neural networks <a href="#waibel1989phoneme">(Waibel et al., 1989)</a> to model sentences of variable lengths, but the authors used convolutions that still had a fixed-width window size. The embedding itself, then, was not constructed while accounting for long-range dependencies. So word embeddings, while a beautiful idea, only set the stage for the next big idea in our history: tackling the problem of modeling long-range dependencies without an explicit context window.</p>

<p>The key innovation here was <em>sequence-to-sequence</em> models. In a sequence-to-sequence model, a neural network encodes a variable-length input sequence into a fixed-length vector, while a second neural network decodes this fixed-length vector back into a variable-length output sequence. In both Bengio and Mikolov’s papers, the input was an embedding (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> in Equations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>14</mn></mrow><annotation encoding="application/x-tex">14</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>4</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span><span>0</span></span></span></span>). In a sequence-to-sequence model, this intermediate fixed-length vector is now the word embedding. The precise architectures used for the encoder and decoder can vary, but clearly they should be architectures that support variable-length sequences, such as recurrent neural networks (RNNs) or LTSMs.</p>

<p>To me, the most intuitive example of a sequence-to-sequence model is a translation model. The input sequence is a sentence in a <em>source language</em> like English, and the output sequence is a sentence in a <em>target language</em> like Chinese (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn></mrow><annotation encoding="application/x-tex">5</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>5</span></span></span></span>). And since some of the most important early work in sequence-to-sequence modeling was in <em>neural machine translation</em> (NMT), I’ll often use translation as a default example. However, the more general case is any mapping from one sequence to another.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig5-encoder-decoder.png" alt=""/></p><p><span>Figure 5.</span> An illustrative example of a sequence-to-sequence model, a neural machine translator. The input sequence is a sentence in English and the output sequence is the same sentence, translated into Chinese. The information in the variable-length input is compressed into hidden variables, which are then used to generate the variable-length output. NB: I translated this sentence with an LLM; apologies for any mistranslation.
    </p>
</div>

<p>This idea is fairly straightforward; it is analogous to an auto-encoder but for variable-length sequences, and auto-encoders <a href="#bourlard1988auto">(Bourlard &amp; Kamp, 1988)</a> are nearly as old as back-propagation. However, as we have already seen, even seemingly simple ideas are hard-won. The original work in RNNs and LSTMs goes back to at least the early 1990s, with seminal papers like <em>Finding structure in time</em> <a href="#elman1990finding">(Elman, 1990)</a>, <em>Serial order: A parallel distributed processing approach</em> <a href="#jordan1997serial">(Jordan, 1997)</a> and <em>Long short-term memory</em> <a href="#hochreiter1997long">(Hochreiter &amp; Schmidhuber, 1997)</a>. By the 2010s, these sequential models were well-known and already used in NLP. See <a href="#mikolov2010recurrent">(Mikolov et al., 2010; Sutskever et al., 2011; Graves, 2013)</a> for example. These models were an important bridge, proving that we could train RNNs at scale and overcome the vanishing gradient problem discussed in <em>Learning long-term dependencies with gradient descent is difficult</em> <a href="#bengio1994learning">(Bengio et al., 1994)</a>. But they were not yet sequence-to-sequence models.</p>

<p>To my knowledge, the first paper to propose a full encoder–decoder architecture for NLP was <em>Recurrent continuous translation models</em> <a href="#kalchbrenner2013recurrent1">(Kalchbrenner &amp; Blunsom, 2013)</a>. Here, the authors proposed training two neural networks end-to-end. The decoder was an RNN, inspired by the model in <a href="#mikolov2010recurrent">(Mikolov et al., 2010)</a>. But somewhat surprisingly, the encoder was not also an RNN. With hindsight, two RNNs feels like the obvious choice, but instead the authors used a <em>convolutional sentence model</em> (CSM). The details don’t really matter here, but this is essentially an NLP model which uses convolutional layers. Why this choice? Well, CSMs were actually developed by the same authors in the same year, in <em>Recurrent convolutional neural networks for discourse compositionality</em> <a href="#kalchbrenner2013recurrent0">(Kalchbrenner &amp; Blunsom, 2013)</a>, and my hypothesis is that this choice just felt obvious to them at the time.</p>

<p>So <a href="#kalchbrenner2013recurrent1">(Kalchbrenner &amp; Blunsom, 2013)</a> was a landmark paper in the sense that it was the first attempt at a sequence-to-sequence model, but with hindsight we can immediately see how to improve it with a better sequential model for the encoder. And that is precisely what happens in two follow up papers. First, in <em>Learning phrase representations using RNN encoder–decoder for statistical machine translation</em> <a href="#cho2014learning">(Cho et al., 2014)</a>, the authors propose the first encoder–decoder architecture in which both neural networks were RNNs. And then in <em>Sequence to sequence learning with neural networks</em> <a href="#sutskever2014sequence">(Sutskever et al., 2014)</a>, the authors proposed a similar model but using LSTMs, since LSTMs often work better at handling the aforementioned vanishing gradient problem. In this paper, Sutskever makes the connection to Kalchbrenner explicitly:</p>

<blockquote>
  <p>Our work is closely related to Kalchbrenner and Blunsom, who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words.</p>
</blockquote>

<p>As a nitpick, convolutional neural networks do model local patterns and order, but they lose global order without very large <a href="https://404wolf.com/blog/2017/02/24/cnns/">receptive fields</a>. But Sutskever’s point is directionally correct. So even at the time, the academic history we are tracing here was clear.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig6-rnn-encoder-decoder.png" alt=""/></p><p><span>Figure 6.</span> Diagram of the relationship between the encoder hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span></span></span></span></span>, the decoder hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">S</mi></mrow><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span></span></span></span></span>, and the context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> in the RNN encoder–decoder proposed by <a href="#cho2014learning">(Cho et al., 2014)</a>. Any sequential structure modeled by the encoder is ultimately flattened into the fixed-width context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span>.
    </p>
</div>

<p>To understand these models in a bit more detail, let’s go through the RNN encoder–decoder in <a href="#cho2014learning">(Cho et al., 2014)</a>, using Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>6</span></span></span></span> as a reference. Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span> be a variable-length input sequence with length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">T_x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Y</span></span></span></span></span> be a variable-length output sequence with length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">T_y</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi mathvariant="script">X</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo stretchy="false">{</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mi mathvariant="script">Y</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo stretchy="false">{</mo><msub><mi mathvariant="bold">y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><msub><mi>T</mi><mi>y</mi></msub></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(25)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathcal{X} &amp;= \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_{T_x} \},
\\
\mathcal{Y} &amp;= \{ \mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_{T_y} \}.
\end{aligned} \tag{25}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>X</span></span></span></span><span><span></span><span><span><span>Y</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>=</span><span></span><span>{</span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span>{</span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>2</span><span>5</span></span><span>)</span></span></span></span></span></span></p>

<p>Note that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="script">X</mi><mo separator="true">,</mo><mi mathvariant="script">Y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathcal{X}, \mathcal{Y})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>X</span></span><span>,</span><span></span><span><span>Y</span></span><span>)</span></span></span></span> is a single observation pair, but I am suppressing the sample index for ease of notation. Also, I bold each vector in both sequences because they are embedded words.</p>

<p>In an RNN, we iteratively compute hidden state variables over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">T_x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> steps, where for the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>-th step we define a recurrence relation between hidden states as:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">h</mi><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mtext mathvariant="sans-serif">enc</mtext></msub><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(26)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{h}_t = f_{\textsf{enc}} \left( \mathbf{h}_{t-1}, \mathbf{x}_t \right). \tag{26}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>enc</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>6</span></span><span>)</span></span></span></span></span></span></p>

<p>This might be a little abstract. So concretely, a simple RNN network might instantiate <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext mathvariant="sans-serif">enc</mtext></msub></mrow><annotation encoding="application/x-tex">f_{\textsf{enc}}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>enc</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> as the following nonlinear function of the current word embedding and the previous hidden state:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">h</mi><mi>t</mi></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi mathvariant="bold">h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(27)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{h}_t = \tanh \left(\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{xh} \mathbf{x}_t \right). \tag{27}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>tanh</span><span></span><span><span>(</span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span><span>h</span><span>h</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span><span>x</span><span>h</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>7</span></span><span>)</span></span></span></span></span></span></p>

<p>The matrices hopefully have obvious dimensions, and we can initialize the first hidden state vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> however we like, such as a vector of all zeros. This is simply one choice, though. We can imagine many types of choices, such as a vanilla RNN unit or an LSTM unit. The key point is that the hidden state vectors</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="script">H</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi mathvariant="bold">h</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">h</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">}</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(28)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathcal{H} = \{\mathbf{h}_1, \mathbf{h}_2,\dots, \mathbf{h}_{T_x}\} \tag{28}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span><span><span></span><span><span>(</span><span><span>2</span><span>8</span></span><span>)</span></span></span></span></span></span></p>

<p>carry forward information from previous words in the sequence via these recurrent connections, much like a <a href="https://404wolf.com/blog/2020/11/28/hmms/" target="_blank">hidden Markov model</a> <a href="#baum1966statistical">(Baum &amp; Petrie, 1966)</a>. A powerful consequence of this model is that RNNs do not limit the size of the input context window. Different input sequences <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span> can be different sizes, unlike in the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span>-gram model or in Bengio’s model (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>14</mn></mrow><annotation encoding="application/x-tex">14</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>4</span></span></span></span>). See Andrej Karpathy’s excellent blog post, <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank"><em>The unreasonable effectiveness of recurrent neural networks</em></a>, for a more detailed presentation of RNNs.</p>

<p>Finally, we define the <em>context vector</em> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> as some function of the hidden states:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="bold">c</mi><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mi mathvariant="script">H</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(29)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{c} = q(\mathcal{H}). \tag{29}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span><span></span><span>=</span><span></span></span><span><span></span><span>q</span><span>(</span><span><span>H</span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span><span>9</span></span><span>)</span></span></span></span></span></span></p>

<p>Notice that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> does not have a time index, because it compresses all the temporal information in the input sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span> into a fixed-width vector. The easiest definition of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> is simply as the last hidden state vector or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi><mo>=</mo><msub><mi mathvariant="bold">h</mi><msub><mi>T</mi><mi>x</mi></msub></msub></mrow><annotation encoding="application/x-tex">\mathbf{c} = \mathbf{h}_{T_x}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>

<p>This context vector becomes an input to the decoder, another RNN with recurrence relation</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">s</mi><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mtext mathvariant="sans-serif">dec</mtext></msub><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="bold">c</mi><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(30)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{s}_t = f_{\textsf{dec}} \left( \mathbf{s}_{t-1}, \mathbf{y}_{t-1}, \mathbf{c} \right), \tag{30}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>dec</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>c</span></span><span>)</span></span><span></span><span>,</span></span><span><span></span><span><span>(</span><span><span>3</span><span>0</span></span><span>)</span></span></span></span></span></span></p>

<p>and hidden states</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="script">S</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi mathvariant="bold">s</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">s</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">s</mi><msub><mi>T</mi><mi>y</mi></msub></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(31)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathcal{S} = \{\mathbf{s}_1, \mathbf{s}_2,\dots, \mathbf{s}_{T_y}\}. \tag{31}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span></span><span></span><span>=</span><span></span></span><span><span></span><span>{</span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>2</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span><span>1</span></span><span>)</span></span></span></span></span></span></p>

<p>The decoder then outputs the sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Y</span></span></span></span></span>, one word at a time. The typical objective of a sequence-to-sequence model is again the autoregressive objective of next-word prediction: maximize a log likelihood, in which each conditional probability is modeled via the decoder RNN:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="script">Y</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>y</mi></msub></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mi>t</mi></msub><mo>∣</mo><msub><mi mathvariant="bold">y</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>y</mi></msub></munderover><mi>log</mi><mo>⁡</mo><msub><mi>f</mi><mtext mathvariant="sans-serif">dec</mtext></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="bold">c</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(32)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\log p(\mathcal{Y}) = \sum_{t=1}^{T_y} \log p(\mathbf{y}_t \mid \mathbf{y}_{1:t-1}) = \sum_{t=1}^{T_y} \log f_{\textsf{dec}}(\mathbf{s}_{t-1}, \mathbf{y}_{t-1} , \mathbf{c}). \tag{32}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>lo<span>g</span></span><span></span><span>p</span><span>(</span><span><span>Y</span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span>p</span><span>(</span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>dec</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>c</span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span><span>2</span></span><span>)</span></span></span></span></span></span></p>

<p>Again, this might be a bit abstract. So for example, one possible instantiation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>g</span></span></span></span> is as a linear transformation of the input variables:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>f</mi><mtext mathvariant="sans-serif">dec</mtext></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi mathvariant="bold">c</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>z</mi><mi>s</mi></mrow></msub><msub><mi mathvariant="bold">s</mi><mi>t</mi></msub><mo>+</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>z</mi><mi>y</mi></mrow></msub><msub><mi mathvariant="bold">y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>z</mi><mi>c</mi></mrow></msub><mi mathvariant="bold">c</mi><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(33)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
f_{\textsf{dec}}(\mathbf{s}_{t-1}, \mathbf{y}_{t-1}, \mathbf{c}) = \mathbf{W}_{zs} \mathbf{s}_t + \mathbf{W}_{zy} \mathbf{y}_{t-1} + \mathbf{W}_{zc} \mathbf{c}. \tag{33}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>dec</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>c</span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span><span>z</span><span>s</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span><span>z</span><span>y</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span><span>z</span><span>c</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>c</span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span><span>3</span></span><span>)</span></span></span></span></span></span></p>

<p>Of course, this is just one choice.</p>

<p>Then all the model weights are learned end-to-end by optimizing this log likelihood (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn></mrow><annotation encoding="application/x-tex">32</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>2</span></span></span></span>). In this way, we can convert a variable-length input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">X</mi></mrow><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span></span></span></span></span> into a variable-length output <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Y</span></span></span></span></span>.</p>

<p>This RNN encoder–decoder framework is powerful, since many problems in NLP can be framed in this way. For example, text summarization, machine translation, and agentic conversation can all be framed as a sequence-to-sequence modeling challenge. To be clear, other researchers around this time had attempted other approaches to handling variable-length sequences, such as the recursive neural tensor network in <em>Recursive deep models for semantic compositionality over a sentiment treebank</em> <a href="#socher2013recursive">(Socher et al., 2013)</a>. But the RNN encoder–decoder would become the de facto framework of choice for a large range of NLP tasks.</p>

<p>As an aside, sometimes these models are call <em>sequence transduction models</em> or <em>transduction models</em> or even just <em>transducers</em>. My understanding is that “transduction” here just means converting one sequence into another by learning a conditional distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi mathvariant="bold">x</mi><mrow><mn>1</mn><mo>:</mo><mi>S</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\theta}(\mathbf{y}_{1:T} \mid \mathbf{x}_{1:S})</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span>θ</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span></span><span><span></span><span><span><span>x</span></span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>S</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>. In this context, “transduction” does not have the sense that Vladimir Vapnik gave it. In Vapnik’s definition, transduction loosely means classification of a specific example rather than a general rule for classifying future examples <a href="#gammerman2013learning">(Gammerman et al., 2013)</a>. But this is not the sense which people mean when they refer to models like the transformer as a “transducer”.</p>

<h2 id="adaptive-context">Adaptive context</h2>

<p>In my mind, Kalchbrenner, Cho, and Sutskever’s three papers <a href="#kalchbrenner2013recurrent1">(Kalchbrenner &amp; Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014)</a> were the foundations of sequence-to-sequence modeling, and many other papers have built around and off this core idea. But the key point for us here is that these three papers make the same logic choice: they lift the idea of a fixed-length embedding for words or phrases into the context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span></span></span></span></span> of a sequential model, such that the models can now support variable-length inputs and outputs and long-range dependencies in each.</p>

<p>However, a problem with this approach was that long-range dependencies got “lost” in this context vector. For example, imagine we had a very long English language text that we wanted to translate into Chinese. Even if our encoder LSTM was good at capturing long-range dependencies in the English sentence, it would be forced to compress that information into a much shorter, fixed-width vector with no temporal structure that would then be fed into the decoder.</p>

<p>This effect was observed by Cho et al in <em>On the properties of neural machine translation: encoder–decoder approaches</em> <a href="#cho2014properties">(Cho et al., 2014)</a>. In this paper, the authors write:</p>

<blockquote>
  <p>Our analysis shows that the performance of the neural machine translation model degrades quickly as the length of a source sentence increases.</p>
</blockquote>

<p>And later:</p>

<blockquote>
  <p>The most obvious explanatory hypothesis is that the fixed-length vector representation does not have enough capacity to encode a long sentence with complicated structure and meaning.</p>
</blockquote>

<p>The authors test this hypothesis through a variety of experiments. For example, in one experiment, they report the <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank">BLEU score</a> for an RNN encoder–decoder as a function of sequence length, and they show that the model’s performance degrades as the sentences become longer. So the RNN decoder–encoder was promising, but the fixed-width context vector was a bottleneck on modeling long-range dependencies.</p>

<p>Then in 2014, a seminal paper was published that addressed this problem, <em>Neural machine translation by jointly learning to align and translate</em> <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a>. The main invention of this paper was to use the well-known <em>attention mechanism</em> to attend to this context vector. However, the authors barely use the word “attention” in the paper. Instead, they seem to conceptualize it more as a search problem:</p>

<blockquote>
  <p>In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.</p>
</blockquote>

<p>I say this paper is “seminal” because, at least to my knowledge, it was really the first paper to use a differentiable attention layer in the rapidly-growing field of NMT. To be clear, the attention mechanism was already known and used outside of NLP. For example, see <em>Learning to combine foveal glimpses with a third-order Boltzmann machine</em> <a href="#larochelle2010learning">(Larochelle &amp; Hinton, 2010)</a>, <em>Learning where to attend with deep architectures for image tracking</em> <a href="#denil2012learning">(Denil et al., 2012)</a>, or <em>Recurrent models of visual attention</em> <a href="#mnih2014recurrent">(Mnih et al., 2014)</a>. These were all papers that were published between 2010 and 2014 and that applied an attention mechanism to a neural network computer vision system. However, to my knowledge, Bahdanau was the first paper to successfully use attention in NLP. To quote <em>Effective approaches to attention-based neural machine translation</em> <a href="#luong2015effective">(Luong et al., 2015)</a>:</p>

<blockquote>
  <p>In the context of NMT, Bahdanau et al… has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.</p>
</blockquote>

<p>All that said, “jointly align and translate” is pretty vague, so let’s get technical. Bahdanau’s solution to this bottleneck was to allow each hidden state vector in the decoder to pay attention to possibly all the hidden state vectors in the encoder. What do I mean by “pay attention to”? Here, each decoder hidden state variable <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> depends not only on the previous hidden state and previous word but also on its own context vector, which is a weighted combination of the encoder’s hidden states!</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>f</mi><mtext mathvariant="sans-serif">dec</mtext></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(34)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathbf{s}_i &amp; = f_{\textsf{dec}}(\mathbf{s}_{i-1}, \mathbf{y}_{i-1}, \mathbf{c}_i),
\\
\mathbf{c}_i &amp;= \sum_{j=1}^{T_x} \alpha_{ij} \mathbf{h}_j.
\end{aligned} \tag{34}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span><span><span>dec</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>y</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>3</span><span>4</span></span><span>)</span></span></span></span></span></span></p>

<p>This is the main idea of the paper. Each decoder hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> has access to <em>all</em> the hidden states in the encoder via this context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn></mrow><annotation encoding="application/x-tex">7</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>7</span></span></span></span>).</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig7-rnn-encoder-decoder-attention.png" alt=""/></p><p><span>Figure 7.</span> Diagram of the relationship between the encoder hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span></span></span></span></span>, the decoder hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">S</mi></mrow><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>S</span></span></span></span></span>, and context vectors in the RNN encoder–decoder with attention proposed by <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a>. Each context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> can attend to any hidden state vector in the encoder to align it with the previous hidden state in the decoder, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_{i-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. This context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is then fed into the recurrence relation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.
    </p>
</div>

<p>We can finally define the attention mechanism! Here, it is the weighted sum of hidden state vectors, as this allows each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to attend to different parts of the input sequence through its hidden state. Each weight <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a linear function of the previous decoder hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_{i-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and the current decoder hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>a</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi mathvariant="bold">z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">W</mi><mi>a</mi></msub><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi mathvariant="bold">U</mi><mi>a</mi></msub><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(35)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\alpha_{ij} &amp;:= \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})},
\\
e_{ij} &amp;:= \mathbf{v}_a^{\top } \mathbf{z}_{ij},
\\
\mathbf{z}_{ij} &amp;:= \tanh\left( \mathbf{W}_a \mathbf{s}_{i-1} + \mathbf{U}_a \mathbf{h}_j \right).
\end{aligned} \tag{35}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>k</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>exp</span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span><span><span></span><span></span></span><span><span></span><span><span>exp</span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span>tanh</span><span></span><span><span>(</span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span><span>U</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>3</span><span>5</span></span><span>)</span></span></span></span></span></span></p>

<p>Let’s call <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\alpha}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>α</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> the an <em>alignment vector</em>, which we infer one per step at a time during the decoding process.</p>

<p>So <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{z}_{ij}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>z</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> can be viewed as a shared hidden state, capturing nonlinear information about both the input and output sequence. Importantly, there is one such vector for each input-output pair. And for a given decoder hidden state, the model can up or downweight the relationship to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> via the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. The neural network learns all these model parameters end-to-end via back-propagation, maximizing the log likelihood in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn></mrow><annotation encoding="application/x-tex">32</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>2</span></span></span></span>.</p>

<p>So that’s it. As I understand it, <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a> was really the first paper to use attention in neural machine translation and probably the most successful use of attention in NLP at the time. The method worked surprisingly well. To quote the paper’s conclusion:</p>

<blockquote>
  <p>Perhaps more importantly, the proposed approach achieved a translation performance comparable to the existing phrase-based statistical machine translation. It is a striking result, considering that the proposed architecture, or the whole family of neural machine translation, has only been proposed as recently as this year.</p>
</blockquote>

<p>As an aside, they actually use a bidirectional RNN for the encoder and then concatenated the forward and backward hidden states. But I don’t think that adds much to our story or to intuition, and it would muddy Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn></mrow><annotation encoding="application/x-tex">7</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>7</span></span></span></span>. The key point is that it was the attention mechanism that allowed for the long-range dependencies encoded by the RNN to be captured through an adaptive context vector. Hopefully, we can now see why the paper uses the words “align and translate”. Here, <em>alignment</em> really means allowing the model to uncover which parts of the input sequence matter to each part of the output sequence—and it does this via the attention mechanism.</p>

<p>Finally, while writing this blog post, I came across this incredible comment by <a href="https://scholar.google.com/citations?user=ezllEwMAAAAJ&amp;hl=en" target="_blank">Edward Grefenstette</a>, published on 3 May 2014:</p>

<blockquote>
  <p>By and large, the case for deep learning in language hasn’t been fully made. It works well for vision and speech, but that doesn’t entail that it would carry to semantics. Some excellent shallow models without non-linearities, like the Mnih and Hinton log-bilinear models, are excellent and can be trained very quickly. It’s a problem with much “deep learning” work in NLP these days that shallow baselines are never considered or compared to. Deep learning is fascinating and will certainly have an impact in NLP, but don’t rush to believe that it’s the best solution for your NLP problems.</p>
</blockquote>

<p>I love this comment because it is a time-capsule, perfectly capturing how experts in the field felt about neural networks at the time. (Note that Grefenstette has published papers with other researchers in this story, such as Kalchbrenner and Graves.) So even around the time that Bahdanau et al were publishing groundbreaking work on RNN encoder–decoders with attention, deep learning had still not fully proven itself to the community.</p>

<h2 id="types-of-attention">Types of attention</h2>

<p>The attentive reader might be wondering: wasn’t the argument around log-linear models that they were simple and therefore scalable? But Bahdanau’s RNN encoder–decoder with attention seems anything but simple. So on some level, yes, Bahdanau’s model was a step backwards in terms of complexity. But on another level, it was a proof-of-concept that the attention mechanism worked. (Also, Moore’s law.) So researchers quickly built on Bahdanau by studying simpler models and simpler types of attention.</p>

<p>Perhaps the most important paper to directly build on Bahdanau’s model was <a href="#luong2015effective">(Luong et al., 2015)</a>. In this paper, the authors simplified the model used by Bahdanau, proposed several alternative forms of attention, and showed that an ensemble of attention-based methods produced state-of-the-art results on neural machine translation problems. To be clear, Bahdanau had shown that attention worked and that it seemed to address problems in translating longer sentences, but it did not demonstrably beat the state-of-the-art. Luong’s results more directly suggested that attention might be the way forward. So before we get to the transformer, let’s understand the attention mechanism better through the lens of this paper.</p>

<p>The first dimension along which we can define attention is local versus global attention. For example, in the attention mechanism in an RNN encoder–decoder, the conceptual lynchpin is that at each time step <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>T</mi><mi>y</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">i \in \{1, \dots, T_y\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span><span></span><span>∈</span><span></span></span><span><span></span><span>{</span><span>1</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>y</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span> in the decoding phase, we construct a context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> which summarizes information from the source sentence via the encoder’s hidden states:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>a</mi></mrow><mi>b</mi></munderover><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(36)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\mathbf{c}_i = \sum_{j=a}^{b} \alpha_{ij} \mathbf{h}_j. \tag{36}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>j</span><span>=</span><span>a</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span>b</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span><span>6</span></span><span>)</span></span></span></span></span></span></p>

<p>But now I don’t precisely define the limits of the sum, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span>. If <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">a=1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><msub><mi>T</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">b=T_x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span><span></span><span>=</span><span></span></span><span><span></span><span><span>T</span><span><span><span><span><span><span></span><span><span>x</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, then the context vector is constructed by considering all the hidden states of the source sentence. This is what Luong calls <em>global attention</em> (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>8</span></span></span></span>, left), since each word in the target sentence has access to information about all the words in the source sentence. But we could also define <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>b</span></span></span></span> such that they form a window around the decoder’s hidden state or model the left-to-right structure of many natural languages. This is what Luong calls <em>local attention</em> (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>8</span></span></span></span>, middle). So these are two ways in which we can construct the context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig8-local-global-attention.png" alt=""/></p><p><span>Figure 8.</span> (Left) Global attention is when the attention mechanism considers all possible variables of interest. (Middle) Local attention is when the attention mechanism only considers a subset of possible variables. (Right) Masked attention is a kind of local attention and typically refers to not attending to forward-looking information such as hidden states later in the sequence.
    </p>
</div>

<p>The second dimension along which we can define attention is how we define the alignment weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\alpha}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>α</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. For example, the simplest choice is simply that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold-italic">α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\boldsymbol{\alpha}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span>α</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a one-hot vector, such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> selects a single encoder hidden state vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">h</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to use in the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span>-th decoding step. This would be hard- rather than soft-search. But more generally, we can write these alignment weights as the unnormalized output of a <em>score function</em>. Using the notation from Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>35</mn></mrow><annotation encoding="application/x-tex">35</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>5</span></span></span></span> above, we can write this as:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>:</mo><mo>=</mo><mtext>score</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(37)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
e_{ij} := \text{score}(\mathbf{h}_j, \mathbf{s}_{i-1}). \tag{37}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>:</span></span><span><span></span><span>=</span><span></span></span><span><span></span><span><span>score</span></span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>3</span><span>7</span></span><span>)</span></span></span></span></span></span></p>

<p>And in Luong, the authors explore three main scoring functions. These are <em>dot-product attention</em>, <em>general attention</em>, and <em>additive attention</em>, defined as:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mtext>score</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi mathvariant="bold">h</mi><mi>j</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>dot,</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi mathvariant="bold">h</mi><mi>j</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi mathvariant="bold">W</mi><mi>a</mi></msub><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>general,</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi mathvariant="bold">v</mi><mi>a</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">W</mi><mi>a</mi></msub><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo>+</mo><msub><mi mathvariant="bold">U</mi><mi>a</mi></msub><msub><mi mathvariant="bold">s</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>additive (Bahdanau).</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(38)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
e_{ij} = \text{score}(\mathbf{h}_j, \mathbf{s}_{i-1}) = \begin{cases}
\mathbf{h}_j^{\top} \mathbf{s}_{i-1} &amp; \text{dot,}
\\
\mathbf{h}_j^{\top} \mathbf{W}_a \mathbf{s}_{i-1} &amp; \text{general,}
\\
\mathbf{v}_a^{\top } \tanh \left( \mathbf{W}_a \mathbf{h}_j + \mathbf{U}_a \mathbf{s}_{i-1} \right) &amp; \text{additive (Bahdanau).}
\end{cases} \tag{38}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>score</span></span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span><span><span></span><span><span>⎩</span></span></span><span><span></span><span><span>⎪</span></span></span><span><span></span><span><span>⎪</span></span></span><span><span></span><span><span>⎨</span></span></span><span><span></span><span><span>⎪</span></span></span><span><span></span><span><span>⎪</span></span></span><span><span></span><span><span>⎧</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span><span><span><span><span><span></span><span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>tanh</span><span></span><span><span>(</span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span><span><span><span>U</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span><span><span><span><span></span><span><span><span>dot,</span></span></span></span><span><span></span><span><span><span>general,</span></span></span></span><span><span></span><span><span><span>additive (Bahdanau).</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span></span></span><span><span></span><span><span>(</span><span><span>3</span><span>8</span></span><span>)</span></span></span></span></span></span></p>

<p>Of course, you can imagine many other score functions. My own view is that it’s too difficult here to reason about which form of attention is better in some theoretical sense. Which form works best is an empirical result. In <a href="#luong2015effective">(Luong et al., 2015)</a>, the empirical results were mixed in the sense that all three score functions worked well. In fact, the results weren’t even strong enough for the authors to claim that attention-based methods were demonstrably better. This was their conclusion:</p>

<blockquote>
  <p>Our analysis shows that attention-based NMT models are superior to non-attentional ones in many cases, for example in translating names and handling long sentences.</p>
</blockquote>

<p>So by late 2015, just two years before the transformer, attention was just becoming popular in NMT but was not yet the de facto modeling choice.</p>

<p>That said, obviously this will change, and when it does, there will be a clear winner amongst the choices above, and that winner is dot-product attention. Dot-product attention is the variant used by the transformer, and thankfully, in my mind it is the most intuitive since the dot product is a standard way to measure the <a href="https://404wolf.com/blog/2018/06/26/dot-product/">similarity between two vectors</a>. So we can interpret the dot-product score function as measuring the similarity between the encoder and decoder hidden states.</p>

<p>The third and final dimension along which we can define attention is through the variables of interest. In order to understand what I mean, we can no longer refer to attention in terms of hidden states of RNNs. We need more general terminology. In the literature, attention is often viewed through the lens of information retrieval. In this literature, a <em>query</em> is what you are asking for; a <em>key</em> is what you can search through; and a <em>value</em> is what you can return.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig9-queries-keys-values.png" alt=""/></p><p><span>Figure 9.</span> An illustration of queries, keys, and values using information retrieval from a data source about restaurants. The queries are user search terms; the keys are metadata against which we weight the results; and the values are the returned results.
    </p>
</div>

<p>Let me give an example (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn></mrow><annotation encoding="application/x-tex">9</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>9</span></span></span></span>). Imagine I type some text into a search bar: “indian food near me”. This text is the query. Now imagine the search engine runs that query against a bunch of metadata associated with different restaurants. For example, restaurant descriptions, keywords, reviews, ratings, and distances from my location. These metadata are the <em>keys</em>. So the query is “run against” the keys. Finally, the thing returned are candidate restaurants. These are <em>values</em>. In the language of information retrieval, we can describe the attention mechanism as a kind of soft-search, since it can return a linear combination of the values. As you may recall, this is precisely how Bahdanau described their model in the quote above.</p>

<p>So in Bahdanau’s RNN encoder–decoder, the decoder’s hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are the queries, since for each hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> we want to search through the source sentence. The encoder’s hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{h}_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are the keys, since these are the metadata associated with the source sentence that we can search through. Finally, the encoder’s hidden states are also the values, since the context vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a weighted combination of these encoder hidden states. This language is useful because it disambiguates the attention mechanism from a specific choice of model and even from which variables in that model are being used for what.</p>

<p>Now that we understand this terminology, we can express ourselves more cleanly and abstractly. And with this terminology, it becomes clear that the keys, queries, and values need not be different objects in our model at all! In fact, queries, keys, and values can all be taken from the same set. For example, imagine we have a model with a hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">h</mi></mrow><annotation encoding="application/x-tex">\mathbf{h}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>h</span></span></span></span></span>. This is not necessarily the hidden state of an RNN or even a sequential model. We could define a kind of attention such that the queries (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">q</mi></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>q</span></span></span></span></span>), keys (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">k</mi></mrow><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span></span></span></span></span>), and values (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">v</mi></mrow><annotation encoding="application/x-tex">\mathbf{v}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>v</span></span></span></span></span>) are all functions of this hidden state:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><msub><mi>f</mi><mi>q</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">h</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>:</mo><mo>=</mo><msub><mi>f</mi><mi>v</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">h</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext>score</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="2em"></mspace><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi mathvariant="bold">c</mi><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(39)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\mathbf{q}_i &amp;:= f_q(\mathbf{h}_i),
\\
\mathbf{k}_j &amp;:= f_k(\mathbf{h}_j),
\\
\mathbf{v}_j &amp;:= f_v(\mathbf{h}_j),
\\
\alpha_{ij} &amp;= \text{softmax}(\text{score}(\mathbf{q}_i, \mathbf{k}_j)), \qquad \sum_{j} \alpha_{ij} = 1,
\\
\mathbf{c}_i &amp;= \sum_j \alpha_{ij} \mathbf{v}_j.
\end{aligned} \tag{39}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span><span>q</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>k</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span><span>c</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>q</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>:</span><span>=</span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>v</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>h</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span>softmax</span></span><span>(</span><span><span>score</span></span><span>(</span><span><span><span>q</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>k</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>)</span><span>,</span><span></span><span></span><span><span><span><span><span><span></span><span><span><span>j</span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span>1</span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>3</span><span>9</span></span><span>)</span></span></span></span></span></span></p>

<p>This is obviously different from the attention mechanism in Bahdanau. In Bahdanau, the authors use <em>cross-attention</em>, which is attention where the queries come from one set and the keys and values come from a different set. As you can imagine, typically the keys and values come from the same set, although they might have their own maps or projections such that they are correlated but not identical. For example, we might run a query against restaurants (keys) and also return restaurants (values). However, <em>self-attention</em> is when the queries, keys, and values all come from the same set of variables! To continue abusing our running example, we essentially compute the similarity between restaurants of interest and restaurants we have data about, and then use those weights to return a weighted combination of restaurants!</p>

<p>To my knowledge, the first paper to use self-attention in NLP was <em>Long short-term memory-networks for machine reading</em> <a href="#cheng2016long">(Cheng et al., 2016)</a>. This model is a bit complicated, and I don’t think it’s that important to understand here. The key point is only to grok that attention does not have to be cross-attention as in Bahdanau. Instead, we can have a sequence attend to itself to decide what parts of the sequence matter—or self-attention! This is how this idea was described in the paper:</p>

<blockquote>
  <p>A remaining practical bottleneck for RNNs is memory compression <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a>: since the inputs are recursively combined into a single memory representation which is typically too small in terms of parameters, it becomes difficult to accurately memorize sequences <a href="#zaremba2014learning">(Zaremba &amp; Sutskever, 2014)</a>. In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism which learns soft alignments between the decoding states and the encoded memories <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a>. In our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens.</p>
</blockquote>

<p>The important phrase here is “within a sequence encoder”. Here, the attention is not applied across the encoder and decoder but rather is applied as intra- or self-attention within the encoder.</p>

<p>So circa 2017, attention was being studied in its many forms: local versus global, additive versus multiplicative, and cross versus self. And it was being more widely used in NLP, with papers like <em>A structured self-attentive sentence embedding</em> <a href="#lin2017structured">(Lin et al., 2017)</a> and <em>Bidirectional attention flow for machine comprehension</em> <a href="#seo2016bidirectional">(Seo et al., 2016)</a>. That said, I do not think any specific form was clearly the dominant one. Rather, each showed promise in its own way. For example, in March 2017, Google Brain published <em>Massive exploration of neural machine translation architectures</em> <a href="#britz2017massive">(Britz et al., 2017)</a>. This was published just months before the transformer would be published, and even here, attention is only a minor player. In that paper’s conclusions, the authors list six main results, and the only one about attention is a single sentence:</p>

<blockquote>
  <p>Parameterized additive attention yielded the overall best results.</p>
</blockquote>

<p>Notice that additive attention is not even the form of attention used by the transformer!</p>

<p>So at least as best as I understand it, attention was well-understood and widely-studied in 2017, but it was by no means considered the main ingredient or the next logical step. Many researchers were still pushing the limits of training RNNs at scale, rather than trying other approaches. See <em>Exploring the limits of language modeling</em> <a href="#jozefowicz2016exploring">(Jozefowicz et al., 2016)</a> for example. However, in June 2017, all that was about to change. The transformer’s time had come.</p>

<h2 id="the-transformer">The transformer</h2>

<p>In 2017, researchers at Google Brain published <em>Attention is all you need</em> <a href="#vaswani2017attention">(Vaswani et al., 2017)</a>, which is the original paper introducing the transformer architecture. This was their proposal, which I hope now makes sense given the context so far:</p>

<blockquote>
  <p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p>
</blockquote>

<p>The authors acknowledge that the sequence-to-sequence framework with neural networks was state-of-the-art, and they specifically call out the RNN encoder–decoder architecture with attention from Bahdanau, Luong, and others. Their proposal is simple: keep the encoder–decoder framework but replace everything else with attention.</p>

<p>How might someone have come to this idea at the time? Why would it be a good idea to try? Their observation is that the sequential nature of RNNs inhibits training these models at scale:</p>

<blockquote>
  <p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>h</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, as a function of the previous hidden state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mtext>−</mtext><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t−1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>h</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and the input for position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span>. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.</p>
</blockquote>

<p>Their proposal is to use attention rather than RNNs to uncover dependencies within the input and output sequences. This is a good idea to try not because attention is obviously better than recurrence per se. It’s that attention is parallelizable! They write:</p>

<blockquote>
  <p>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p>
</blockquote>

<p>We have seen this before. Recall how the unlock for word embeddings in <a href="#mikolov2013efficient">(Mikolov et al., 2013; Mikolov et al., 2013)</a> was simplifying the models and focusing on scale. But then the RNN encoder–decoder architecture in <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a> with attention took us backwards in terms of model complexity. So the transformer is a similar story: take the best modeling ideas, strip them down, and train the simplified model at scale.</p>

<p>That’s it. Properly understood in context, the transformer is a modest conceptual leap from the existing literature. My point is not that the transformer is “obvious” in the sense that it is not an impressive invention. My point is to demystify the research product by underscoring the process. In context, the transformer should make sense as something someone might have tried in 2017.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig10-transformer.png" alt=""/></p><p><span>Figure 10.</span> The transformer model architecture. Both the encoder and decoder use positional encoding and multi-head self-attention. The decoder uses masked attention to maintain the left-to-right structure of most natural languages. The encoder–decoder uses cross-attention a la <a href="#bahdanau2014neural">(Bahdanau et al., 2014)</a> to align the hidden states before outputting probabilities.
    </p>
</div>

<p>The model architecture might look intimidating, but it is pretty straightforward when viewed in the right context (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>0</span></span></span></span>). At a high level, the transformer is an encoder–decoder, with two big kinds of attention. First, we have cross-attention between the outputs of the encoder and the inputs to the decoder. This is completely analogous to the cross-attention in Bahdanau and others. But then we also have self-attention within the decoder and encoder. This completely replaces the recurrence relations of RNNs. Finally, the model uses something called <em>positional encoding</em>, which I’ll define shortly, to handle the fact that attention is not naturally sequential a la an RNN. Everything else is details. For example, the transformer also uses layer normalization <a href="#ba2016layer">(Ba et al., 2016)</a> and residual connections <a href="#he2016deep">(He et al., 2016)</a>, but these are not unique or novel contributions. Even multi-head attention is not conceptually hard. So understood in context, the transformer is pretty straightforward. Let’s go through the main bits in detail.</p>

<p>First, positional encoding. A key challenge for the attention mechanism is that it does not inherently capture sequential structure. Thus, the relative positions of words in a sequence can be easily lost. In Vaswani, the authors propose attaching vectors of numbers to the inputs to capture this position-dependent information. The precise functional form of these numbers doesn’t really matter to us. The point is that we’re encoding the position of each word so that we can still model the sequential structure of natural language.</p>

<p>After adding position-dependent information, the transformer encodes the input sequence. But rather than passing the data through an RNN, it passes the data through multi-head attention layers. We’ll discuss “multi-head” in a moment, but the basic attention mechanism is what the authors call <em>scaled-dot product attention</em>. Let’s define it. Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>M</mi><mo>×</mo><msub><mi>D</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Q} \in \mathbb{R}^{M \times D_k}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Q</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>M</span><span>×</span><span><span>D</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> be a matrix of queries, let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><msub><mi>D</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{K} \in \mathbb{R}^{N \times D_k}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>×</span><span><span>D</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> be a matrix of keys, and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{V} \in \mathbb{R}^{N \times D_v}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>V</span></span><span></span><span>∈</span><span></span></span><span><span></span><span><span><span>R</span></span><span><span><span><span><span><span></span><span><span><span>N</span><span>×</span><span><span>D</span><span><span><span><span><span><span></span><span><span>v</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> be a matrix of values. Then scaled dot-product attention is:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>attention</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">Q</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>D</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi mathvariant="bold">V</mi><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(40)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\text{attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{D_k}} \right) \mathbf{V}. \tag{40}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>attention</span></span><span>(</span><span><span>Q</span></span><span>,</span><span></span><span><span>K</span></span><span>,</span><span></span><span><span>V</span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>softmax</span></span><span></span><span><span><span>(</span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>D</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>Q</span></span><span><span><span>K</span></span><span><span><span><span><span><span></span><span><span><span>⊤</span></span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span>)</span></span></span><span></span><span><span>V</span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>0</span></span><span>)</span></span></span></span></span></span></p>

<p>When I first read Vaswani, I had not yet read Bahdanau or Luong, and thus I was completely confused by Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span><span>0</span></span></span></span>. It was not at all obvious what any of these values represented or why any of this machinery worked. And the paper itself gave a pretty opaque explanation:</p>

<blockquote>
  <p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
</blockquote>

<p>Without context, this explanation is not very helpful. However, armed with a better understanding of attention, we can make sense of this. In the cross-attention between the encoder and decoder, the queries are analogous to the hidden states of the RNN decoder, while the keys and values are analogous to the hidden states of the RNN encoder. And if we remove the sample dimension (so let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N=1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span>), we can rewrite Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span><span>0</span></span></span></span> in a way that looks like the types of attention in Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>38</mn></mrow><annotation encoding="application/x-tex">38</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>8</span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>score</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msubsup><mi mathvariant="bold">q</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub></mrow><msqrt><msub><mi>D</mi><mi>k</mi></msub></msqrt></mfrac><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>D</mi><mi>v</mi></msub></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>attention</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">α</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>D</mi><mi>v</mi></msub></munderover><msub><mi>α</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable></mtd><mtd width="50%"></mtd><mtd><mtext>(41)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\text{score}(\mathbf{q}_i, \mathbf{k}_j) &amp;= e_{ij} = \frac{\mathbf{q}_i^{\top} \mathbf{k}_j}{\sqrt{D_k}},
\\
\alpha_{ij} &amp;= \frac{\exp(e_{ij})}{\sum_{k=1}^{D_v} \exp(e_{ik})},
\\
\text{attention}(\boldsymbol{\alpha}_i, \mathbf{v}_i) &amp;= \sum_{k=1}^{D_v} \alpha_{ik} \mathbf{v}_i.
\end{aligned} \tag{41}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>score</span></span><span>(</span><span><span><span>q</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>k</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span><span><span></span><span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span><span>attention</span></span><span>(</span><span><span><span><span>α</span></span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span><span><span><span></span><span><span><span>D</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span><span>q</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span>⊤</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>k</span></span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>∑</span><span><span><span><span><span><span></span><span><span><span>k</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span><span><span>D</span><span><span><span><span><span><span></span><span><span>v</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>exp</span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span><span><span></span><span></span></span><span><span></span><span><span>exp</span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>i</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>,</span></span></span><span><span></span><span><span></span><span></span><span>=</span><span></span><span><span><span><span><span><span></span><span><span><span>k</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span><span><span>D</span><span><span><span><span><span><span></span><span><span>v</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>α</span><span><span><span><span><span><span></span><span><span><span>i</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span>(</span><span><span>4</span><span>1</span></span><span>)</span></span></span></span></span></span></p>

<p>So this is identical to the multiplicative or dot-product attention proposed in Luong (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>38</mn></mrow><annotation encoding="application/x-tex">38</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>8</span></span></span></span>), modulo a scaling factor <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>D</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{D_k}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span><span><span>D</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span>. In Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>40</mn></mrow><annotation encoding="application/x-tex">40</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span><span>0</span></span></span></span>, we are just packaging it into a matrix form so that we can compute this attention over many samples at once. In other words, this is a highly parallelizable version of the dot-product attention.</p>

<p>I think one of the reasons the transformer can be confusing is the use of two types of attention and the generic language of queries, keys, and values, whose definitions change depending on the type of attention. In the encoder, the transformer uses self-attention. So the query represents the current vector in the input sequence, while the keys and values are all the other vectors in the input sequence. And in the decoder, the query represents the current vector in the output sequence, while the keys and values are all the other vectors in the output sequence—modulo masking, which I’ll mention in a moment. Finally, the attention between the encoder and decoder (in the paper, Vaswani calls this “encoder–decoder attention”), the query is the current vector in the decoder output (analogous to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>s</span></span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> in the RNN encoder–decoder), while the keys and values are the encoder’s hidden outputs (analogous to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span></span></span></span></span> in the RNN encoder–decoder).</p>

<p>Note that “masked” in “masked multi-head self-attention” just refers to a masking out of words in the decoder’s self-attention mechanism. This is because attention has no inherent sequential structure a la RNNs. So we have to enforce this by masking regions of the output sequence (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>8</span></span></span></span>, right). This allows the transformer to be trained in the standard autoregressive framework we have discussed since <a href="#bengio2003neural">(Bengio et al., 2003)</a>.</p>

<p>Finally, the transformer learns multiple sets of parameters associated with the attention mechanism at once. This is what the paper calls <em>multi-head attention</em>. Instead of having a single attention function, we can run multiple attention functions in parallel, say <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span></span></span></span> times. By way of analogy, recall that in the RNN encoder–decoder, we had the following attention parameters (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>35</mn></mrow><annotation encoding="application/x-tex">35</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>5</span></span></span></span>):</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mo stretchy="false">{</mo><msub><mi mathvariant="bold">W</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">U</mi><mi>a</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>a</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(42)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
\{\mathbf{W}_a, \mathbf{U}_a, \mathbf{v}_a \}. \tag{42}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span><span>W</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>U</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span><span>v</span></span><span><span><span><span><span><span></span><span><span>a</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>2</span></span><span>)</span></span></span></span></span></span></p>

<p>In Bahdanau (Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>35</mn></mrow><annotation encoding="application/x-tex">35</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>5</span></span></span></span>) the subscript <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> just denotes that these are attention-related weights. It is not actually indexing into multiple such weights (that is, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">A=1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>A</span><span></span><span>=</span><span></span></span><span><span></span><span>1</span></span></span></span>). But we <em>could</em> do that. We could say that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span></span></span></span> is indexing into different parameters, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>A</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">a \in \{1, 2,\dots, A\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>a</span><span></span><span>∈</span><span></span></span><span><span></span><span>{</span><span>1</span><span>,</span><span></span><span>2</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>A</span><span>}</span></span></span></span>. This would have made Bahdanau’s model slower to train, but it would have allowed for multiple cross-attention mechanisms to be learned at once. In Bahdanau, they don’t actually do this, likely because it’s too expensive! The precise details are different in Vaswani, but this is all multi-head attention is in theory. It is multiple parallel attention mechanisms.</p>

<p>So that’s it. That’s the transformer. The results were impressive. To be clear, it was not an AlexNet moment, but the results were clearly better than benchmarks and more importantly, the model was way more efficient. For example, one of the benchmarks in Vaswani is the ConvS2S Ensemble from <em>Convolutional sequence to sequence learning</em> <a href="#gehring2017convolutional">(Gehring et al., 2017)</a>. The idea of this paper is similar to the transformer: train a bigger sequence-to-sequence model by eschewing recurrent connections in favor of parallelizable convolutional layers. In both English-to-German and English-to-French translation, the transformer beats this model in BLEU score. But more importantly, it is more efficient. For example, according to Vaswani, the ConvS2S Ensemble required <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>21</mn></msup></mrow><annotation encoding="application/x-tex">1.2 \times 10^{21}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>.</span><span>2</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>2</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span> flops to train their English-to-French model, whereas the transformer required <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3.3</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>18</mn></msup></mrow><annotation encoding="application/x-tex">3.3 \times 10^{18}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>3</span><span>.</span><span>3</span><span></span><span>×</span><span></span></span><span><span></span><span>1</span><span><span>0</span><span><span><span><span><span><span></span><span><span><span>1</span><span>8</span></span></span></span></span></span></span></span></span></span></span></span> flops. So the transformer had comparable results with a 360x reduction in flops! In my mind, this the real insight. It is not that attention is absolutely the best way to model the problem. Rather, the transformer is on the Pareto frontier between modeling the problem well enough and being scalable enough.</p>

<p>To see the transformer in code, see Sasha Rush’s excellent <a href="https://nlp.seas.harvard.edu/annotated-transformer/" target="_blank"><em>The annotated transformer</em></a>.</p>

<h2 id="generative-pre-training">Generative pre-training</h2>

<p>The transformer was a revolutionary architecture, and explicitly designed to scale. However, in reality the original model was tiny by today’s standards. The biggest variant only had 2.13 million parameters, and the largest dataset it was trained on, the WMT 2014 English–French datasets, only had 36 million sentences. But the paper proved that the transformer worked well as a generic transduction model.</p>

<p>However, despite the paper’s name, the transformer architecture was not enough. Researchers also needed advancements in how these models were trained in order to make the commodity LLMs most people interact with today. To simplify the discussion, I’ll focus on training for OpenAI’s GPT series. My understanding is that OpenAI made a lot of the big contributions here, and so their papers are good landmarks to follow. Loosely, the three training stages they discuss in their GPT papers are generative pre-training, discriminative fine-tuning, and reinforcement learning with human feedback. Let’s work through the first two in detail here and the last one in detail in the next section.</p>

<p>In 2018, roughly a year after the transformer was published, OpenAI published <em>Improving language understanding by generative pre-training</em> <a href="#radford2018improving">(Radford et al., 2018)</a>. The main idea of the paper is to pre-train a transformer with as much unlabeled data as possible before fine-tuning it with task-specific supervised training. In the paper, the authors call the first step <em>generative pre-training</em> and the second step <em>discriminitive fine-tuning</em>. (The words “generative” and “discriminitive” have a long history in machine learning; see <a href="#ng2001discriminative">(Ng &amp; Jordan, 2001)</a> for a discussion.) As the OpenAI paper title suggests, the key focus was on generative pre-training. Supervised learning obviously matters, but the idea was that one could use unsupervised training at scale to build a base model and then use supervised learning to train more task-specific downstream models.</p>

<p>Let’s look at generative-pretraining in a bit more detail. Since we do not have labels, we need some way to formalize the problem. In generative pre-training, the objective is <em>next-word prediction</em> as in the autoregressive framework. In other words, the objective is maximum likelihood estimation on Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span></span></span></span>:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>L</mi><mtext mathvariant="sans-serif">GPT</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Θ</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi mathvariant="bold-italic">Θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mi>N</mi><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(43)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
L_{\textsf{GPT}}(\boldsymbol{\Theta}) = \sum_{t=1}^T \log p_{\boldsymbol{\Theta}}\left(w_t \mid w_{t-N:t-1}\right). \tag{43}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>GPT</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>Θ</span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>t</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>T</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>t</span><span>−</span><span>N</span><span>:</span><span>t</span><span>−</span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>3</span></span><span>)</span></span></span></span></span></span></p>

<p>As we saw around Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>2</span></span></span></span>, maximum likelihood estimation here is equivalent to minimizing the cross-entropy loss between our model’s prediction of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">w_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and the ground truth. So this whole process is unsupervised, and we can train our model on lots and lots and lots of data.</p>

<p>It’s worth observing that Equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>43</mn></mrow><annotation encoding="application/x-tex">43</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>4</span><span>3</span></span></span></span> is only one generative pre-training objective function, and it has limitations. In particular, note that the autoregressive framework means that the model is pre-trained “left to right” and thus limits the set of suitable downstream tasks. To address this limitation, in 2019, Google AI published <em>BERT: Pre-training of deep bidirectional transformers for language understanding</em> <a href="#devlin2019bert">(Devlin et al., 2019)</a>. Here, the authors propose a pre-training objective that learns bidirectional representations. Rather than pre-training using the autoregressive framework, they pre-train using a “masked language model”, which randomly masks some of the tokens to predict, without assuming a left-to-right relationship. Quoting that paper:</p>

<blockquote>
  <p>Unlike left-to-right language model pre-training, the [masked language model] objective enables the representation to fuse the left and the right context, which allows us pre-train a deep bidirectional Transformer.</p>
</blockquote>

<p>More formally, let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">M</mi><mo>⊆</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{M} \subseteq \{1,2,\dots,T\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>M</span></span><span></span><span>⊆</span><span></span></span><span><span></span><span>{</span><span>1</span><span>,</span><span></span><span>2</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>T</span><span>}</span></span></span></span> be a mask denoting positions in the input sequence <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1:T}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">¬</mi><mi mathvariant="script">M</mi></mrow><annotation encoding="application/x-tex">\neg \mathcal{M}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>¬</span><span><span>M</span></span></span></span></span> denote all indices that are <em>not</em> in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">M</mi></mrow><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>M</span></span></span></span></span>. The denoising objective is to maximize</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>L</mi><mtext mathvariant="sans-serif">MLM</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Θ</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi mathvariant="script">M</mi></mrow></munder><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi mathvariant="bold-italic">Θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>w</mi><mrow><mi mathvariant="normal">¬</mi><mi mathvariant="script">M</mi></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(44)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
L_{\textsf{MLM}}(\boldsymbol{\Theta}) = \sum_{i \in \mathcal{M}} \log p_{\boldsymbol{\Theta}}\left(w_i \mid w_{\neg \mathcal{M}} \right). \tag{44}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>MLM</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>Θ</span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>∈</span><span><span>M</span></span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span><span><span>Θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span><span>w</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>∣</span><span></span><span><span>w</span><span><span><span><span><span><span></span><span><span><span>¬</span><span><span>M</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>4</span></span><span>)</span></span></span></span></span></span></p>

<p>This idea was inspired by the <a href="https://en.wikipedia.org/wiki/Cloze_test" target="_blank">Cloze test</a> <a href="#taylor1953cloze">(Taylor, 1953)</a>, and the idea was that this bidirectional transformer can then be fine-tuned on a much wider range of downstream tasks. That said, my understanding is that generative pre-training is fairly standard. The left-to-right assumption is simple and matches natural language, coding, and so forth. But I am not confident about what is used in absolutely state-of-the-art foundation models right now.</p>

<p>Either way, neither objective function is enough. For example, consider a conversational agent built on top of a large language model. Now imagine the user prompts an LLM with the following question: “I am having trouble getting a date. Any advice?” If the LLM is only trained on next-word prediction, a plausible response might be: “You’ll never find true love!” From the perspective of the distribution of English words on the internet, this is not an unreasonable response. But it is not helpful and hopefully not true. In other words, next-word prediction is obviously not enough for most meaningful tasks that leverage LLMs. So the second step in training is <em>discriminative fine-tuning</em>.</p>

<p>“Discriminative fine-tuning” is just a fancy way of saying supervised learning on specific tasks:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>L</mi><mtext mathvariant="sans-serif">DFT</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>y</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub></mrow></munder><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi mathvariant="bold-italic">θ</mi></msub><mrow><mo fence="true">(</mo><mi>y</mi><mo>∣</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(45)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
L_{\textsf{DFT}}(\boldsymbol{\theta}) = \sum_{y, x_{1:T}} \log p_{\boldsymbol{\theta}}\left(y \mid x_{1:T} \right). \tag{45}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>DFT</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span>θ</span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>y</span><span>,</span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>lo<span>g</span></span><span></span><span><span>p</span><span><span><span><span><span><span></span><span><span><span><span><span>θ</span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span>y</span><span></span><span>∣</span><span></span><span><span>x</span><span><span><span><span><span><span></span><span><span><span>1</span><span>:</span><span>T</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>5</span></span><span>)</span></span></span></span></span></span></p>

<p>Here, I am using standard notation for supervised learning <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x, y)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span></span></span></span>, rather than the notation in this post. There are some possible subtleties here. For example, in the GPT-1 paper, they optimize a weighted objective function to balance between generative pre-training and discriminative fine-tuning:</p>

<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>L</mi><mtext mathvariant="sans-serif">final</mtext></msub><mo>=</mo><msub><mi>L</mi><mtext mathvariant="sans-serif">DFT</mtext></msub><mo>+</mo><mi>λ</mi><mtext> </mtext><msub><mi>L</mi><mtext mathvariant="sans-serif">GPT</mtext></msub><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(46)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">
L_{\textsf{final}} = L_{\textsf{DFT}} + \lambda \, L_{\textsf{GPT}}. \tag{46}
</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>final</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>DFT</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>λ</span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span><span><span>GPT</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span><span><span></span><span><span>(</span><span><span>4</span><span>6</span></span><span>)</span></span></span></span></span></span></p>

<p>This ensures that during fine-tuning, the model does not unlearn parameters that are good for next-word prediction.</p>

<p>In the process of trying to fine-tune LLMs, researchers have built ever more task-specific datasets to tackle problems like question-and-answering <a href="#reddy2019coqa">(Reddy et al., 2019)</a>, text summarization <a href="#nallapati2016abstractive">(Nallapati et al., 2016)</a>, commonsense inference <a href="#zellers2019hellaswag">(Zellers et al., 2019)</a>, code generation <a href="#chen2021evaluating">(Chen et al., 2021)</a>, broader discourse context <a href="#paperno2016lambada">(Paperno et al., 2016)</a>, and grade school math <a href="#cobbe2021training">(Cobbe et al., 2021)</a>. A pre-trained LLM can be fine-tuned in a dizzying number of ways.</p>

<p>I have two caveats to the above presentation. First, I want to emphasize that this two-step training procedure was not a conceptual leap for researchers. At the time, researchers were already training models with pre-trained word embeddings, and even before this, this two-step training procedure was both understood and used in practice. For examples, see <a href="#collobert2008unified">(Collobert &amp; Weston, 2008; Ramachandran et al., 2016; Hinton et al., 2012)</a>. Furthermore, researchers knew to use both pre-trained word embeddings and to even have task-specific objectives when training their word embeddings. Remember ELMO? The earliest reference I have found to this idea of pre-training—I am sure there are earlier ones—is from the 2006 paper <em>Greedy layer-wise training of deep networks</em> <a href="#bengio2006greedy">(Bengio et al., 2006)</a>. Here, the authors write:</p>

<blockquote>
  <p>We hypothesize that three aspects of this strategy are particularly important: first, pre-training one layer at a time in a greedy way; second, using unsupervised learning at each layer in order to preserve information from the input; and finally, fine-tuning the whole network with respect to the ultimate criterion of interest.</p>
</blockquote>

<p>In these examples above, it’s clear the authors recognize that one can pre-train a model with unsupervised learning and then fine-tune it with supervised learning. So even in the GPT paper, the novel contribution is not generative pre-training per se, but only applying it to language modeling at an unprecedented scale.</p>

<p>My second caveat is that while discriminative fine-tuning is used in commodity LLMs that many people interact with, the early GPT models were remarkable in part because they did not need fine-tuning! For example, as their titles suggest, the GPT-2 paper <em>Language models are unsupervised multitask learners</em> <a href="#radford2019language">(Radford et al., 2019)</a> and the GPT-3 paper <em>Language models are few-shot learners</em> <a href="#brown2020language">(Brown et al., 2020)</a> both focus on massively pre-trained transformers that excel in the zero-shot <a href="#palatucci2009zero">(Palatucci et al., 2009)</a> and few-shot settings, on a variety of tasks like reading comprehension, summarization, and translation. For example, in the GPT-3 paper, the authors are explicit:</p>

<blockquote>
  <p>For all tasks, GPT-3 is applied without any gradient updates or fine-tuning.</p>
</blockquote>

<p>That said, many related research projects did fine-tune these models, and the GPT-4 technical report <a href="#achiam2023gpt">(Achiam et al., 2023)</a> does discuss post-training alignment, which we’ll discuss next. So while each LLM may be trained in slightly different ways, I am fairly confident most foundation models today are trained with some combination of massive pre-training and then optionally task-specific fine-tuning and alignment. I’m sure the precise details vary depending on the final product. For example, OpenAI’s <a href="https://cdn.openai.com/pdf/97cc5669-7a25-4e63-b15f-5fd5bdc4d149/gpt-5-codex-system-card.pdf" target="_blank">Codex</a> is a version of GPT-5 but optimized for agentic coding.</p>

<h2 id="alignment">Alignment</h2>

<p>Making LLMs bigger does not necessarily make them better at following a user’s intent or make them more aligned with human values. For example, we might not want conversational agents to lie, to make racist jokes, or to sexually harass the user. But nothing in the autoregressive framework accounts for this. We need to somehow encode these human values into the model. For some of these properties, we might be able to use a form of fine-tuning. There are datasets for this, such as the ETHICS dataset <a href="#hendrycks2020aligning">(Hendrycks et al., 2020)</a> or the RealToxicityPrompts dataset <a href="#gehman2020realtoxicityprompts">(Gehman et al., 2020)</a>. But the limitations here are fairly obvious. And for many human values, it would be difficult to encode because the property itself is hard to define.</p>

<p>To encode these properties, state-of-the-art LLMs are often trained using something called <em>reinforcement learning with human feedback</em> (RLHF). RLHF was developed around the same time as the transformer, in <em>Deep reinforcement learning from human preferences</em> <a href="#christiano2017deep">(Christiano et al., 2017)</a>. The original motivation was how to expand the reinforcement learning (RL) framework beyond problems with well-specified reward functions. For example, RL has been used to great effect to play Go <a href="#silver2016mastering">(Silver et al., 2016)</a>, Atari <a href="#mnih2013playing">(Mnih et al., 2013)</a>, and Dota 2 <a href="#berner2019dota">(Berner et al., 2019)</a>, but what these tasks have in common is that their reward functions are relatively simple and their environments are relatively easy to simulate. But to borrow two examples from Christiano et al, how would you teach a machine-learning model to clean a table or to scramble an egg? It’s hard to come up with an objective function or simulation environment for these kinds of tasks.</p>

<div>
    <p><img src="https://404wolf.com/image/llm/fig11-rlhf.png" alt=""/></p><p><span>Figure 11.</span> Illustration of the three steps to reinforcement learning with human feedback, inspired by Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>2</span></span></span></span> in <a href="#stiennon2020learning">(Stiennon et al., 2020)</a>. (Left) Collect data by generating multiple candidate responses from one or multiple policies; have human labelers rank these responses. (Middle) Train a reward model to prefer human responses. (Right) Train a reinforcement learning policy, using the rewards from the reward model.
    </p>
</div>

<p>What we need, then, is a reward function that can be defined by human feedback and thus by human preferences. Broadly, RLHF is a three-step training procedure (Figure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>11</mn></mrow><annotation encoding="application/x-tex">11</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>1</span><span>1</span></span></span></span>). First, humans are used to label a dataset which captures human preferences. For example, if the task is text summarization, the dataset might be different candidate summarizations, with the best summarization being defined by human scorers. Second, researchers train a reward function on these data, which predicts which output the humans would prefer. Finally, given this reward function, researchers can apply standard RL algorithms such as proximal policy optimization or PPO <a href="#schulman2017proximal">(Schulman et al., 2017)</a> to fine-tune the model.</p>

<p>Fine-tuning LLMs with RLHF is now fairly standard practice. For example, GPT-2 was fine-tuned this way in <em>Fine-tuning language models from human preferences</em> <a href="#ziegler2019fine">(Ziegler et al., 2019)</a>, while GPT-3 was fine-tuned this way in <em>Training language models to follow instructions with human feedback</em> <a href="#ouyang2022training">(Ouyang et al., 2022)</a> and in <em>Learning to summarize with human feedback</em> <a href="#stiennon2020learning">(Stiennon et al., 2020)</a>. And the GPT-4 whitepaper <a href="#achiam2023gpt">(Achiam et al., 2023)</a> states that the model was trained with RLHF.</p>

<p>That said, as the content of this post approaches present day, it is increasingly likely I am writing things that lack nuance. For example, in the GPT-4 whitepaper, the authors write:</p>

<blockquote>
  <p>The model’s capabilities on exams appear to stem primarily from the pre-training process and are not significantly affected by RLHF.</p>
</blockquote>

<p>So while I am confident that generative pre-training is not enough and that certainly large foundation models trained today do more than just pre-training, the precise details of what else goes into which models are both opaque and rapidly changing.</p>

<p>Finally, it’s worth mentioning other work on LLM alignment beyond RLHF. In particular, Anthropic has a number of papers on model alignment. For example, the paper <em>A general language assistant as a laboratory for alignment</em> <a href="#askell2021general">(Askell et al., 2021)</a> focuses on encoding alignment into LLMs, where they define an aligned model as a model that is “helpful, honest, and harmless”. They explore a variety of techniques, such as imitation learning, binary discrimination, and ranked preference modeling. However, the best way to tackle alignment is still an open-ended problem.</p>

<h2 id="zero-to-one">Zero to one</h2>

<p>Large language models are the result of at least forty years of research, dating back to work by Hinton, Rumelhart, and others on distributed representations in the 1980s. In the early 2000s, Bengio et al introduced the first probabilistic language model using neural networks. However, it wasn’t until after AlexNet, nearly a decade later, that researchers were finally able to train neural network language models at scale. They quickly discovered that these distributed representations captured semantic and syntactic structure, even when using simple log-linear models. This idea of word and phrase-level embeddings was then extended to variable-length sequences with long-range dependencies via transduction models, particularly models with an attention mechanism on the hidden states. Finally in 2017, Vaswani et al introduced the transformer, which simplified transduction models by using all attention. In the eight years since, the main advancements have been training these models on more and more data, using techniques such as generative pre-training and reinforcement learning with human feedback.</p>

<p>After learning about how LLMs work, I am reminded of one of my favorite Richard Feynman quotes: <a href="https://www.youtube.com/watch?v=lmTmGLzPVyM" target="_blank">“It is not complicated. It’s just a lot of it.”</a> Of course, this is dramatic, but I do think it emphasizes an important point: none of the ideas in this post are terribly complicated. No single idea is beyond the abilities of a smart teenager to understand. But what is beautiful and surprising and remarkable is that the phenomena we observe in LLMs is not magic but simply the emergence of a complex system from simple rules.</p>

<p>Today, LLMs are everywhere, and it’s easy to get lost in the models and benchmarks. OpenAI has the GPT series <a href="#radford2018improving">(Radford et al., 2018; Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023)</a>. Google has the Gemini family of models <a href="#team2023gemini">(Team et al., 2023)</a> as well as PaLM <a href="#chowdhery2023palm">(Chowdhery et al., 2023)</a>, LaMDA <a href="#thoppilan2022lamda">(Thoppilan et al., 2022)</a>, Gopher <a href="#rae2021scaling">(Rae et al., 2021)</a>, and BERT <a href="#devlin2019bert">(Devlin et al., 2019)</a>. Anthropic has the Claude family of models, named in ascending order of size and power: Haiku, Sonnet, and Opus. Finally, Meta has its LLaMA series <a href="#touvron2023llama">(Touvron et al., 2023; Touvron et al., 2023)</a>. And there are many, many more, such as open-weight models like DeepSeek-R1 <a href="#guo2025deepseek">(Guo et al., 2025)</a>, which made headlines earlier this year. It would be its own blog post to cover the differences between these. But in essence, every model is the same: a large transformer-style model, pre-trained at massive scale using next-word prediction.</p>

<p>The biggest differences have been the size of the training data and the size of the model. For example, GPT-1 is thought to have 117 million parameters (estimated from “Model specifications” in the original paper), while GPT-2 and GPT-3 had 1.5 billion and 1.75 billion parameters respectively—although in <a href="#stiennon2020learning">(Stiennon et al., 2020)</a>, the authors, OpenAI researchers, mention using “large pretrained GPT-3 models with as many as 6.7 billion parameters”. Regardless, there are roughly three orders of magnitude in the number of parameters in just two generations. OpenAI did not publish the model sizes for GPT-4 and GPT-5, and the latter does not even have a whitepaper but only a <a href="https://cdn.openai.com/gpt-5-system-card.pdf">“system card”</a>. I have not seen published numbers for Google’s large Gemini models, but the smallest model (the nano) has 1.8-3.25 billion parameters <a href="#team2023gemini">(Team et al., 2023)</a>. Google DeepMind’s Gopher had 280 billion parameters in 2021, while PaLM had 540 billion parameters in 2022! So industry secrets aside, it is safe to say that large foundation models today are likely pushing into the trillions of parameters. The era of truly large language models has begun.</p>

<p>In my mind, the main counterintuitive result of LLMs is that training ever larger models using primarily next-word prediction is enough to exhibit human-level performance on such a broad range of tasks. And scale truly does matter here. For example, in the GPT-4 technical report, the authors observe that on a simulated bar exam, GPT-4 scored in the 90th percentile, while GPT-3.5 scored in the 10th. Or consider chain-of-thought reasoning <a href="#ling2017program">(Ling et al., 2017)</a>, which is a new way of prompting LLMs in order to improve their reasoning by forcing them to explain each step. In <em>Chain-of-thought prompting elicits reasoning in large language models</em> <a href="#wei2022chain">(Wei et al., 2022)</a>, the authors write:</p>

<blockquote>
  <p>Chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ∼100B parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.</p>
</blockquote>

<p>So you don’t get anything useful from chain-of-thought reasoning until you have a model that is roughly 50,000 times the size of the original transformer.</p>

<p>Why does scaling work? I don’t think anyone knows. But it is an effect that has been observed repeatedly since AlexNet, in the above works and also in meta-analyses such as <em>Scaling laws for neural language models</em> <a href="#kaplan2020scaling">(Kaplan et al., 2020)</a>, <em>Scaling language models: methods analysis, and insights from training Gopher</em> <a href="#rae2021scaling">(Rae et al., 2021)</a>, and <em>Emergent abilities of large language models</em> <a href="#wei2022emergent">(Wei et al., 2022)</a>. And this phenomenon was both observed and predicted in the famous blog post <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank"><em>The Bitter Lesson</em></a>. In that post, one of the pioneers of RL, Richard Sutton, argues that the “bitter lesson” from AI history is that general, compute-efficient, scalable methods outperform human knowledge and domain-specific insights. This lesson is bitter because it means that expert labor, clever domain-specific theories, handcrafted features, elegant mathematics and beautiful algorithms—these all get dwarfed and outpaced by brute-force search and learned representations.</p>

<p>As a stark example of this, consider the observation that early LLMs were bad at mathematics <a href="#hendrycks2021measuring">(Hendrycks et al., 2021)</a>. But today, state-of-the-art models are now <a href="https://www.nytimes.com/2025/07/21/technology/google-ai-international-mathematics-olympiad.html" target="_blank">winning gold at the International Math Olympiad</a>, and Terry Tao has <a href="https://mathstodon.xyz/@tao/113132502735585408" target="_blank">compared o1 to a mediocre graduate student</a>. The rate of change is immense. By “early LLMs”, I am referring to models from five years ago, and the transistor is only a hundred years old. Did you know that a modern graphics card can perform <a href="https://www.youtube.com/watch?v=h9Z4oGN89MU" target="_blank">36 trillion calculations a second</a>? Moore’s law and all that.</p>

<p>If you feel that it’s a bit perverse that next-word prediction is a sufficient objective to solve elite math problems, if this feels like a stochastic parrot outsmarting you, then you might feel some of the discomfort early linguists felt at statistical language modeling. This is the visceral feeling of the bitter lesson. Our specialized knowledge feels expendable and our intuitions about understanding seem irrelevant in the face of raw computation and speed.</p>

<p>But my own view—since you’ve read this far—is that for the time being, machine learning systems are powerful tools that can still be combined with real expertise. Perhaps the best example of this is AlphaFold from Google DeepMind, published in <em>Highly accurate protein structure prediction with AlphaFold</em> <a href="#jumper2021highly">(Jumper et al., 2021)</a>. This model achieved near-experimental accuracy on the protein prediction problem. On the one hand, it did so with black-box deep learning. On the other hand, the work leaned heavily on prior biological art, for example using sequences from evolutionarily related proteins and 3D coordinates of homologous structures as inputs to the model. It clearly sidestepped Levinthal’s combinatorial search, even if we do not know how.</p>

<p>So what happens next? Even the world’s leading experts can disagree. But in my mind, if anyone deserves the last word here, it is Geoff Hinton, who has been a contrarian believer in neural networks since the 1980s and who, along with Yoshua Bengio and Yann LeCun, won the <a href="https://awards.acm.org/about/2018-turing" target="_blank">Turing Prize in 2018</a> in part for work mentioned in this post. In a 2024 <a href="https://www.youtube.com/watch?v=MGJpR591oaM" target="_blank">BBC interview</a>, Hinton argued that LLMs do in fact understand natural language and that they are our current best theory of how the brain understands language as well. In his view, it is only a matter of time before LLMs exceed human intelligence. Certainly, by some metrics and along some dimensions, they already have.</p>



<h3 id="resources">Resources</h3>

<p>Below are some additional resources, which I found useful or interesting while writing this post:</p>

<ul>
  <li>3Blue1Brown: <a href="https://www.youtube.com/watch?v=wjZofJX0v4M" target="_blank">Inside an LLM</a></li>
  <li>Stefania Cristina: <a href="https://machinelearningmastery.com/the-bahdanau-attention-mechanism/" target="_blank">The Bahdanau attention mechanism</a></li>
  <li>Stefania Cristina: <a href="https://machinelearningmastery.com/the-attention-mechanism-from-scratch/" target="_blank">The attention mechanism from scratch</a></li>
  <li>Dan Jurafsky and James H. Martin: <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank">Speech and language processing</a></li>
  <li>Andrej Karpathy: <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">The unreasonable effectiveness of recurrent neural networks</a></li>
  <li>Andrej Karpathy: <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank">Let’s build GPT: from scratch, in code, spelled out</a></li>
  <li>Chris Olah: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTM networks</a></li>
  <li>Dwarkesh Patel: <a href="https://www.youtube.com/watch?v=21EYKqUsPfg" target="_blank">Interview with Richard Sutton</a></li>
  <li>Sasha Rush: <a href="https://nlp.seas.harvard.edu/annotated-transformer/" target="_blank">The annotated transformer</a></li>
  <li>Ari Seff: <a href="https://www.youtube.com/watch?v=VPRSBzXzavo" target="_blank">How ChatGPT is trained</a></li>
  <li>Ari Seff: <a href="https://www.youtube.com/watch?v=XSSTuhyAmnI" target="_blank">What are transformer neural networks?</a></li>
  <li>StackOverflow: <a href="https://stats.stackexchange.com/questions/421935/" target="_blank">What exactly are keys, queries, and values in attention mechanisms?</a></li>
  <li>Mohammed Terry-Jack: <a href="https://medium.com/@b.terryjack/deep-learning-the-transformer-9ae5e9c5a190" target="_blank">Deep learning: The transformer</a></li>
</ul>


    </div></div>
  </body>
</html>
