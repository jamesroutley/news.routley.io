<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lightly.ai/post/segment-anything-model-and-friends">Original</a>
    <h1>Segment Anything Model and Friends</h1>
    
    <div id="readability-page-1" class="page"><div><p>Recent years have witnessed the emergence of language models pre-trained on enormous corpora of unlabeled data. These systems demonstrate the ability to perform new tasks with minimal or no specific training. Research shows that this behavior increases with model scale, dataset size, and training compute. These capabilities are often guided by engineered text prompts that provide the necessary guidance to direct the system’s output toward desired results.</p><p>Their vision counterparts, however, have been explored to a lesser extent (<a href="https://arxiv.org/abs/2304.02643" target="_blank">Segment Anything, 2023</a>). Models such as <a href="https://openai.com/index/clip/" target="_blank">CLIP (2021</a>) and <a href="https://arxiv.org/abs/2102.05918v2" target="_blank">ALIGN (2021)</a> provide strong baselines for aligning images and text with separate encoders pre-trained using a contrastive objective. While vision-language models have seen significant advancement, core computer vision tasks such as segmentation have yet to progress at the same pace. This discrepancy is likely due to the challenges in obtaining large-scale, high-quality annotated datasets for tasks like segmentation.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7b3_66b28ad4ca436e17fa2e8bc7_1_3ZtKuqQoXMgawyrgC168mw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Summary of recent SAM variants</figcaption></figure><p>‍</p><p>This article will dive into the Segment Anything (SAM) family of models, their architectures, applications, and performance.</p><h2>Segment Anything (SAM 1) Where it All Started<br/></h2><p>Kirillov et al. introduced the Segment Anything Model (SAM) and its corresponding dataset in <a href="https://arxiv.org/abs/2304.02643" target="_blank">Segment Anything, 2023</a> to build a foundational model for image segmentation with the aim:</p><blockquote>To develop a promptable model pre-trained on a broad dataset using a task that enables powerful generalization.</blockquote><p>Notably, the success of the SAM project can be attributed to its three core components:</p><ol role="list"><li>A <strong>task</strong> formulation that enables zero-shot generalization.</li><li>A <strong>model</strong> architecture that allows for such flexibility.</li><li>A comprehensive <strong>dataset</strong> to allow the model to perform well at the task.</li></ol><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b28afe9bf4b8bfada5ceb4_1*cq0oEWZmiBHIDxxdo1EB1w.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> The three fundament components of the Segment Anything Project: Promptable Segmentation Task, Segment Anything Model, and the Segment Anything Dataset. Source: Figure 1 from <a href="https://arxiv.org/abs/2304.02643" target="_blank">Segment Anything, 2023</a></figcaption></figure><p>‍</p><p>Let’s look into each component in detail.</p><h3>Segment Anything Task</h3><p>Inspired by the success of next-token prediction in NLP the authors translate the idea of a “prompt” to segmentation. A prompt in this case can be a set of foreground/background points, a rough box or mask, free-form text, or in general <strong>any source of information about what to segment in an image</strong>.</p><blockquote>The <strong>promptable segmentation task</strong> then is to return a valid segmentation mask given any “prompt”.</blockquote><p>This notion of a valid segmentation mask is of importance since it helps deal with ambiguity wherein a prompt could refer to any number of objects. In particular, this forces the model to output a reasonable mask for at least one of those objects. This is illustrated by the following image:</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7b8_66b28b46c018c94fe8e2962a_1_EDSEEtuvgemn8XSo-1tTIg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Each of these columns represents valid masks generated by SAM based on a single ambiguous point. Source: Figure 3 from <a href="https://arxiv.org/abs/2304.02643" target="_blank">Segment Anything, 2023</a></figcaption></figure><p>‍</p><p>Since the objective is to always predict a valid mask even if the prompt is ambiguous this ensures that the pre-trained model is effective in use cases that involve ambiguity, this comes in useful in the automatic annotation in the SAM data engine. Moreover, it leads to a general method for zero-shot transfer to downstream segmentation tasks via prompting. For example, one can feed the model with bounding boxes outputted from an object detector as the input prompt.</p><p>An important clarification to make here is that SAM is inherently different from prior works in multi-task segmentation systems, where a model is pre-trained and evaluated on a fixed set of tasks. A model trained for promptable segmentation is capable of performing new, different tasks at inference time by acting as a component in a bigger system (much like how DALL-E uses CLIP as the text-image alignment component).</p><h3>Segment Anything Model</h3><p>The promptable segmentation task requires a model that can support flexible prompting and can output segmentation masks in amortized real-time to allow for interactive use. The authors build on top of recent work in Transformer vision models while making careful adjustments to allow for speed and efficiency. </p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b778_66b28b7af1dd10e7c8d5fc33_1_oVuxDQhX0Cr2U64t4GuMBw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Segment Anything Model Architecture. Source: Figure 4from <a href="https://arxiv.org/abs/2304.02643" target="_blank">Segment Anything, 2023</a></figcaption></figure><p>‍</p><p>The Segment Anything Model has 3 key components:</p><ol role="list"><li><strong>Image Encoder:</strong> Owing to the success of Self-Supervised pre-training methods, the authors use a Masked Auto-Encoder (MAE) adapted to process high-resolution images. This encoder runs once per image and can be applied before prompting the model. <a href="https://www.lightly.ai/post/knowledge-distillation-trends" target="_blank">You can read more about MAEs on our blog</a>.</li><li><strong>Flexible Prompt Encoder:</strong> We can broadly classify possible prompts into <em>sparse</em> (points, boxes, and text) and <em>dense</em> (masks).</li><li><strong>Fast Mask Decoder:</strong> The decoder is tasked with efficiently mapping image embeddings, prompt embeddings, and an output mask token to a valid segmentation mask. The authors base their decoder on the Transformer decoder block with a modified dynamic mask prediction head that uses prompt self-attention and cross-attention to update all embeddings. An MLP is then used to map the output token to a dynamic linear classifier that is tasked with computing mask foreground probabilities at each image location.</li></ol><p>To deal with ambiguity, the authors modify the model to predict multiple output segmentation masks for a single prompt. The outputs are thus accompanied by the model confidence scores (IoU) for each mask. The model is trained with a linear combination of focal loss and dice loss by applying backprop to the minimum loss over masks.</p><h3>Segment Anything Data Engine and Dataset</h3><p>To achieve strong generalization it was essential to train the model on a large and diverse set of masks. Since such a dataset was not available publicly the authors developed a “data-engine”, which allowed them to co-develop the model by using model-in-the-loop dataset annotations. Thus, they could iterate between using the model to assist in data collection and using this newly collected data to improve the model. The data engine has three stages:</p><ol role="list"><li><strong>Assisted Manual:</strong> In this stage, SAM assists annotators in annotating masks similar to a classic interactive segmentation setting.</li><li><strong>Semi-automatic:</strong> In this stage, SAM can automatically generate masks for a subset of objects by prompting it with likely object locations while annotators can focus on annotating the remaining objects helping increase mask diversity.</li><li><strong>Fully automatic:</strong> In this stage, SAM is prompted with a regular grid of foreground points which it uses to output high-quality masks.</li></ol><p>Using this data engine the authors were able to release the <strong>Segment Anything 1B dataset</strong> containing more than 1B masks from 11M licensed and privacy-preserving images containing 400x more masks than any existing segmentation dataset.</p><h2>Segment Anything Model v1: Results</h2><p>The authors report zero-shot transfer performance on five tasks that evaluated SAM on datasets and tasks that were not seen during training, including novel image distributions.</p><h3><strong>Zero-Shot Single Point Valid Mask Evaluation</strong></h3><p><strong>‍</strong></p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b773_66b28be335378762b8529446_1_w6zDj5uuZMlG02cWggvGsQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Mask quality ratings for zero-shot single-point valid mask evaluation</figcaption></figure><p>‍</p><p>These results indicate that <em>SAM has learned to segment valid masks from a single point</em>.</p><h3><strong>Zero-Shot Edge Detection</strong></h3><p>The authors evaluated SAM on the classic low-level task of edge detection using a simplified version of the automatic mask generation pipeline of the data engine. Compared to the ground truth, SAM predicts more edges, and qualitatively even though SAM was not trained for edge detection it produces reasonable edge maps.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b76c_66b28c0dc7127e8483d8c834_1_gwQRKukZHss1o5Us3KhgmQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Zero-shot transfer to edge detection</figcaption></figure><p>‍</p><h3>Zero-Shot Object Proposals</h3><p>When evaluated on the mid-level task of object proposal generation, SAM does remarkably well on several metrics. It outperforms ViTDet-H on medium and large objects as well as rare and common objects. SAM only underperforms VitDet-H on small objects and frequent objects, likely since VitDet-H was trained on LVIS, unlike SAM.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7ed_66b28c314b93d03a18638551_1_N1T8aQTFRGM7slNCl8KZXg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Object Proposal Generation on LVIS</figcaption></figure><p>‍</p><h3>Zero-Shot Instance Segmentation</h3><p>SAM can also be used as the segmentation module of an instance segmenter. SAM performs reasonably close to VitDet-H (even outperforming in human studies) and qualitatively produces better and crisper masks.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b77e_66b28c5c54af162ebba51f3b_1__ifqeRX7LTHO01l3Kx70GQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Instance Segmentation results when performing zero-shot transfer</figcaption></figure><p>‍</p><h3>Zero-Shot Text-to-Mask</h3><p>When evaluated on segmenting objects from free-form text, SAM is successfully able to segment objects based on simple text prompts. When it does fail to pick the right object, an additional point often fixes the prediction.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7d6_66b28c846bb6fd9735a36ffe_1_cYKM9RGhkfgobDHLvNGsqQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> SAM’s ability to produce segmentation masks based on simple text prompts</figcaption></figure><p>‍</p><h2>🏃 Making SAM faster</h2><h3>FastSAM</h3><p>SAM was regarded as a milestone vision foundation model with its ability to segment any object within the image, guided by various possible user interaction prompts. However, its industrial impact was limited by its computational cost.</p><p>SAM uses a transformer backbone (<em>worst-case quadratic complexity in terms of tokens</em>), at higher input resolutions it has a heavy computational resource demand, which presents a hurdle to its practical deployment, especially in real-time applications. Zhao et al. in <a href="https://arxiv.org/abs/2306.12156" target="_blank">Fast Segment Anything, 2023</a> decoupled the segment anything task introduced by SAM into two sequential stages relying on a CNN-based detector. </p><p>By directly training a CNN detector on only 2% (1/50) of the SA-1B dataset, the authors achieved comparable performance to SAM, but with drastically reduced computational and resource demands, enabling real-time application.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7dd_66b28cb3e4014dfef6fe6983_1_sN2kdTKb6Zd7gYq_LHbRig.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> FastSAM framework. Source: Figure 2 from <a href="https://arxiv.org/abs/2306.12156" target="_blank">Fast Segment Anything, 2023</a></figcaption></figure><p>‍</p><p>The authors propose to break down the Segment Anything Task into All-instance segmentation and Prompt-guided selection. The first stage produces the segmentation masks of all instances in the image using a <a href="https://yolov8.com/" target="_blank">YOLOv8-</a>based CNN backbone, while the second stage outputs the region of interest corresponding to the prompt.</p><p>Having been trained on only 2% of the SA-1B dataset, FastSAM offers competitive performance to SAM with faster inference times.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7f6_66b28cdfa0f902c4981e82ff_1_wxEeqGkOz9IJkc1Pd3RZlg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of running speeds FastSAM vs SAM. Source: Table 1 from <a href="https://arxiv.org/abs/2306.12156" target="_blank">Fast Segment Anything, 2023</a></figcaption></figure><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7a9_66b28cf535890730711e0d44_1_Je0wLuACC5WWQbjS-bN9fg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> FastSAM performs comparable to SAM on zero-shot transfer to edge detection. Source: Table 2 from <a href="https://arxiv.org/abs/2306.12156" target="_blank">Fast Segment Anything, 2023</a></figcaption></figure><p>‍</p><h3>MobileSAM</h3><p>Improving on FastSAM, the authors of <a href="https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications" target="_blank">MobileSAM, 2023</a> propose to distill the knowledge from a heavy image encoder to a lightweight image encoder enabling 5x speedup than the concurrent FastSAM and 7x smaller size.</p><p>Performing Knowledge Distillation on SAM proves to be a challenge since optimization of the image encoder depends on the quality of the image decoder, and vice versa. When the two modules in the SAM are both in a bad state, it is more challenging to train them both to a good state. The authors therefore propose to distill the small image encoder directly from the encoder in the original SAM without resorting to the combined decoder. This brings the benefit of a readily used combined decoder for finetuning instead of training it from scratch.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7a5_66b28d32a62d2516564ea192_1_iT-LEDRoGO8K0uTt8Nz4DQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Proposed Decoupled Knowledge Distillation paradigm as introduced in <a href="https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications" target="_blank">MobileSAM, 2023</a>.</figcaption></figure><p>‍</p><p>Compared to SAM, MobileSAM offers comparable performance at a much smaller size and faster inference time.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b78b_66b28d57417b6eac7eb37141_1_BjxnqqFKy-p0xs6rRw190w.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> SAM vs MobileSAM in terms of model size and inference speed as measured on a single GPU. Source: Table 3 from <a href="https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications" target="_blank">MobileSAM, 2023</a></figcaption></figure><p>‍</p><p>The authors also provide qualitative results, when comparing SAM, MobileSAM, and FastSAM.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7fc_66b28d8ce1489ed4d68f454e_1_8ZUJuQTV6maPNCsGUzMdFQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison of generated masks. Source: Figure 6 from <a href="https://www.researchgate.net/publication/371851844_Faster_Segment_Anything_Towards_Lightweight_SAM_for_Mobile_Applications" target="_blank">MobileSAM, 2023</a>.</figcaption></figure><p>‍</p><h3>EfficientSAM </h3><p>Xiong et al. propose an interesting framework in <a href="https://arxiv.org/pdf/2312.00863" target="_blank">EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything, 2023</a> exploiting recent success in Masked Pre-training and Knowledge Distillation.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b793_66b28db4a6e6d6c8b7a63c28_1_DWBGUOK2jYEZ4vojgUkPcQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> EfficientSAM framework. Source: Figure 2 from <a href="https://arxiv.org/pdf/2312.00863" target="_blank">EfficientSAM, 2023</a>.</figcaption></figure><p>‍</p><p>They propose SAM-leveraged masked image pre-training (SAMI) that uses a SAM encoder (ViT-H) to generate feature embeddings to train a masked image model with lightweight encoders to reconstruct features from ViT-H of SAM instead of image patches. This leads to generalized ViT backbones, which can be used for downstream tasks such as image classification, object detection, and the segment anything task.</p><p>At each training iteration, SAMI consists of a feedforward feature extraction from the SAM image encoder, and a feedforward and a backpropagation procedure of MAE. The outputs from the SAM image encoder and MAE linear projection head are compared to compute the reconstruction loss.</p><blockquote>This can be seen as a form of Knowledge Distillation, to read more about Knowledge Distillation and its connections with masked pre-training, please <a href="https://www.lightly.ai/post/knowledge-distillation-trends" target="_blank">refer to our article</a>.</blockquote><p>This SAMI pre-trained light-weight encoder is then served as the image encoder of EfficientSAM for finetuning on SA-1B.</p><p>The authors evaluate performance on point-based and box-based prompt segmentation and report comparisons with SAM, FastSAM, and MobileSAM.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7a2_66b28dd9d81cd47664ab9ec4_1_1Q6pf6VbV7hmrperKJQk9Q.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison on Zero-shot instance segmentation. Source: Table 5 from <a href="https://arxiv.org/pdf/2312.00863" target="_blank">EfficientSAM, 2023</a>.</figcaption></figure><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7c2_66b28df84581a86f5b09cdde_1_DehRg5wISuHq2TBaweRcNg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Qualitative analysis of point prompts. Source: Figure 3  from <a href="https://arxiv.org/pdf/2312.00863" target="_blank">EfficientSAM, 2023</a>.</figcaption></figure><p>‍</p><h2>SAM 2</h2><p>Ravi et al. generalize the notion of the Segmentation Anything Task to videos as a Promptable Visual Segmentation (PVS) task in the latest <a href="https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/" target="_blank">SAM 2 2024 paper</a>. Following the recent push toward multi-modal models, the authors aim to create a unified model for video and image segmentation.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7e0_66b28e272cb28961ab87e5c5_1_sVyHCVK9u8BfQTpkTg5n-g.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> SAM 2 Framework. Source: Figure 1 from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a></figcaption></figure><p>‍</p><p>Similar to SAM, this version also includes a task, a model, and a data engine + dataset.</p><blockquote>You can play around with a CPU version of the model on this app</blockquote><h4>Promptable Visual Segmentation (PVS) Task</h4><p>This generalization of the Segment Anything task allows us to provide prompts to the model on any video frame. Prompts can be positive/negative clicks, bounding boxes, or masks, either to define an object to segment or to refine a model-predicted one.</p><p>Moreover, to work in an interactive setting the model must immediately respond with a valid segmentation mask of the object on this frame upon receiving an input.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7bf_66b28e5735890730711f2e52_1_wlgjpqGUiSUv3oFJqPSDSQ.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Propagation of input prompts in SAM 2. Source: Figure 2 from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a></figcaption></figure><p>‍</p><p>Upon receiving input, the model <strong>propagates</strong> these prompts to obtain the masklet (a masked video block) of the required object across the entire video. Moreover, additional prompts can be provided to the model on any frame to refine the segment throughout the video.</p><p>This allows them to segment objects across videos with a good interactive experience and build a strong model with a large and diverse dataset.</p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7c8_66b28e80e4014dfef6ff799d_1_kqPy2yioYC7u8MRx2FgLrw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Previously studied tasks such as Segment Anything (SA) and semi-supervised Video Object Segmentation (VOS) can be seen as special cases of the PVS task. Source: Figure 8 from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a></figcaption></figure><p>‍</p><h3>SAM 2 Model</h3><p>The authors extend the model architecture of SAM and adapt it to make it work with videos. SAM 2 takes as input a stream of video frames along with point, box, or mask prompts and outputs segmentation masks. </p><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7ce_66b28ea7d82f40ce75ab38a3_1_rspUSz7Fo55WQ6-N6B7ADg.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> SAM 2 model architecture. Source: Figure 3 from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a></figcaption></figure><p>‍</p><ul role="list"><li>A <a href="https://arxiv.org/abs/2306.00989" target="_blank">Hiera</a> pre-trained MAE encoder is used to generate image embeddings of each frame.</li><li>A transformer-based memory attention block is used to condition the current frame features on the past frames&#39; features + predictions as well as on any new prompts. This allows the model to learn spatio-temporal features.</li><li>SAM 2 uses the same prompt encoder as SAM</li><li>Unlike SAM, however, for the PVS task, there might be cases where no valid masks exist for a given frame due to occlusion. To account for this the authors add a head that predicts whether the object of interest is present in the current frame.</li><li>A memory encoder and bank are used to retain information about predictions for past frames.</li></ul><h3>SAM 2 Model Performance</h3><p>The authors report significant performance improvements for zero-shot video tasks and image tasks. </p><ul role="list"><li>Human annotators using SAM 2 are 8x faster in labeling a frame with SAM 2 than SAM 1.</li><li>SAM 2 shows significant improvement over the best existing methods in both accuracy and inference speed</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b7c5_66b28ee21526ef0a1c9c57e9_1_85fodcpPPJBIuy3SKYu1sw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Comparison to prior work on Video Object Segmentation. Source: Table 7  from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a>.</figcaption></figure><p>‍</p><ul role="list"><li>Compared to SAM, SAM 2 also performs better on the Segment Anything Task.</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/62cd5ce03261cb3e98188470/66b2902d919489513b79b786_66b28f09b45452c130802bab_1_ukiUDLv42kip3G9-uKvAiw.png" loading="lazy" alt=""/></p><figcaption><strong>Figure:</strong> Zero-shot accuracy on the Segment Anything (SA) task. Source: Table 6 from <a href="https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=iEdf_eLLDBIQ7kNvgHfhQx9&amp;_nc_ht=scontent-lhr8-1.xx&amp;oh=00_AYDP9MuKVFfAe3wLEPXFm-TZT_u1fs2tg7iDJ6j7OXqh4w&amp;oe=66B28679" target="_blank">SAM 2, 2024</a>.</figcaption></figure><p>‍</p><h2>Conclusion</h2><p>The Segment Anything Model (SAM) and its successors made a significant leap forward in computer vision, particularly in image and video segmentation. Along with SAM’s innovative approach to promptable segmentation, the literature rapidly evolved to address key challenges like computational efficiency and real-time performance. FastSAM, MobileSAM, and EfficientSAM each brought unique optimizations, dramatically reducing model size and inference time while maintaining competitive performance. </p><p>The latest iteration, SAM 2, extends these capabilities to video, introducing the Promptable Visual Segmentation task and demonstrating impressive results in both image and video domains. These advancements showcase the power of foundation models in computer vision, mirroring the success of large language models in NLP. </p></div></div>
  </body>
</html>
