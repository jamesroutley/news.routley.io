<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cloud.google.com/compute/docs/disks/about-regional-persistent-disk">Original</a>
    <h1>About synchronous disk replication</h1>
    
    <div id="readability-page-1" class="page"><div>

  
    
    

    
    
  <hr/>
    

    

    

    

    
    

    









  <p>



  


Regional Persistent Disk and Hyperdisk Balanced High Availability are storage options that let you implement
high availability (HA) services in Compute Engine.
Regional Persistent Disk and Hyperdisk Balanced High Availability synchronously replicate data between two
zones in the same region and ensure HA for disk data for up to one
zonal failure.</p>

<p>Regional Persistent Disk and Hyperdisk Balanced High Availability volumes are designed for workloads that
require a lower Recovery Point Objective (RPO) and Recovery Time Objective
(RTO). To learn more about RPO and RTO, see
<a href="https://cloud.google.com/architecture/dr-scenarios-planning-guide#basics_of_dr_planning">Basics of disaster recovery planning</a>.</p><p>




Regional Persistent Disk and Hyperdisk Balanced High Availability volumes are
designed to work with regional
<a href="https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances">managed instance groups</a>.</p>

<p>This document provides an overview of how to build HA services with

Regional Persistent Disk and Hyperdisk Balanced High Availability volumes.</p>

<p>When you decide to use 
Regional Persistent Disk or Hyperdisk Balanced High Availability, make sure that you
compare the different options for increasing service availability and the
<a href="https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk#compare_cost_performance_and_resiliency">cost, performance, and resiliency</a>
for different service architectures.</p>



    

    

<h2 id="zonal_replication" data-text="About synchronous disk replication" tabindex="-1">About synchronous disk replication</h2>

<p>A 
Regional Persistent Disk or Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>) volume, also referred to as a
replicated disk, has a primary and a secondary zone within its region where it
stores disk data:</p>

<ul>
<li><strong>Primary zone</strong> is the same zone where the compute instance that you attach
the disk to is located.</li>
<li><strong>Secondary zone</strong> is an alternate zone of your choice within the same
region.</li>
</ul>

<p>Compute Engine maintains replicas of your disk in both
these zones. When you write data to your disk, Compute Engine synchronously
replicates that data to the disk replicas in both zones to ensure HA. The data
of each zonal replica is spread across multiple physical machines within the
zone to ensure durability. Zonal replicas ensure that the data of the
disk remains available and provide protection against temporary
outages in one of the disk zones.</p>

<h3 id="zonal_replica_state" data-text="Replica state for zonal replicas" tabindex="-1">Replica state for zonal replicas</h3>

<p>Disk replica state for 
Regional Persistent Disk or Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>) shows you the state of
a zonal replica in comparison to the content of the disk. Zonal replicas for
your disks are in one of the following disk replica states at all times:</p>

<ul>
<li><strong>Synced</strong>: The replica is available, synchronously receives all the writes
performed to the disk, and is up to date with all the data on the disk.</li>
<li><strong>Catching up</strong>: The replica is available but is still catching up with
the data on the disk from the other replica.</li>
<li><strong>Out of sync</strong>: The replica is temporarily unavailable and out of sync
with the data on the disk.</li>
</ul>

<p>To learn how to check and track the replica states of your zonal replicas,
see <a href="https://cloud.google.com/compute/docs/disks/monitor-regional-persistent-disk-replica-state">Monitor the disk replica states</a>.</p>

<h3 id="repd_replication_state" data-text="Replication states for synchronously replicated disks" tabindex="-1">Replication states for synchronously replicated disks</h3>

<p>Depending on the state of the individual zonal replicas, your

Regional Persistent Disk or Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>) volume can be in one of
the following replication states:</p>

<ul>
<li><strong>Fully replicated:</strong> Replicas in both zones are available and are
synced with the latest disk data.</li>
<li><strong>Catching up:</strong> Your zonal replicas are available, but one of the zonal
replicas is catching up with the latest disk data.</li>
<li><strong>Degraded:</strong> One of the zonal replicas has a status of <code translate="no" dir="ltr">out of sync</code>
due to a failure or an outage.</li>
</ul>

<p>If the disk replication status is <code translate="no" dir="ltr">catching up</code> or <code translate="no" dir="ltr">degraded</code>, then one of
the zonal replicas is not updated with all the data. Any outage during this
time in the zone of the healthy replica results in an unavailability of the
disk until the healthy replica zone is restored.</p>

<p>When your 
Regional Persistent Disk or Hyperdisk Balanced High Availability volume is catching up,
Google Cloud starts healing the zonal replica that is catching up.
Google recommends that you wait for the affected zonal replica to catch up with
the data on the disk, at which point its status changes to <code translate="no" dir="ltr">Synced</code>. After the
zonal replica then moves to the synced state, the replicated disk status
changes back to the <code translate="no" dir="ltr">Fully replicated</code> state.</p>

<p>If the replicated disk has a status of <code translate="no" dir="ltr">catching up</code> or <code translate="no" dir="ltr">degraded</code> for a
prolonged period of time and does not meet your organization&#39;s RPO requirements,
we recommend that you take snapshots of the primary replica in either of
following ways:</p>

<ul>
<li>Enable scheduled snapshots.</li>
<li>Create a manual snapshot of your

Regional Persistent Disk or Hyperdisk Balanced High Availability disk.</li>
</ul>

<p>After you create a snapshot, you can create a new

Regional Persistent Disk or Hyperdisk Balanced High Availability disk by using that
snapshot as the source. This restores the snapshot to the new disk. Your new
disk also starts in a fully replicated state with healthy data replication.</p>

<p>To learn how to check the replication state of your

Regional Persistent Disk or Hyperdisk Balanced High Availability disk, see
<a href="https://cloud.google.com/compute/docs/disks/monitor-regional-persistent-disk-replica-state#determine_repd_replication_state">Determine the replication state of disks</a>.</p>

<h3 id="repd-replica-recovery-checkpoint" data-text="Replica recovery checkpoint" tabindex="-1">Replica recovery checkpoint</h3>

<p>A <em>replica recovery checkpoint</em> is a disk attribute that
represents the most recent
<a href="https://cloud.google.com/compute/docs/disks/snapshot-best-practices#prepare_for_consistency">crash-consistent</a>
point in time of a fully replicated disk. Compute Engine automatically creates
and maintains a single replica recovery checkpoint for each replicated disk.
When a disk is fully replicated, Compute Engine
keeps refreshing its checkpoint approximately every 10 minutes to ensure that
the checkpoint remains updated. When the disk replication status is
<code translate="no" dir="ltr">degraded</code>, Compute Engine lets you create a standard snapshot from the
replica recovery checkpoint of that disk. The resulting standard snapshot
captures the data from the most recent crash-consistent version of the fully
replicated disk.</p>











































































<p>
  In rare scenarios, when your disk is degraded, the zonal replica that is
  synced with the latest disk data can also fail before the out-of-sync
  replica catches up. You won&#39;t be able to force-attach your disk to
  compute instances in either zone. Your replicated disk becomes unavailable
  and you must migrate the data to a new disk.
  In such scenarios, if you don&#39;t have any existing standard snapshots available
  for your disk, you might still be able to recover your disk
  data from the incomplete replica by using a standard snapshot created
  from the replica recovery checkpoint.
</p>




<p>Compute Engine automatically creates replica recovery checkpoints for each
mounted 
Regional Persistent Disk or Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>) disk. You don&#39;t incur any
additional charges for the creation of these checkpoints. However, you do incur
any applicable storage charges for the creation of snapshots and compute
instances when you use these checkpoints to migrate your replicated disk to
functioning zones.</p>

<p>Learn more about how to
<a href="https://cloud.google.com/compute/docs/disks/repd-failover#recover-repd-using-checkpoint">recover your replicated disk data using a replica recovery checkpoint</a>.</p>

<h2 id="repd-failover" data-text="Replicated disk failover" tabindex="-1">Replicated disk failover</h2>

<p>In the event of an outage in a zone, the zone becomes inaccessible and the
compute instance in that zone can&#39;t perform read or write operations on its
disk. To allow the instance to keep performing read and write operations for
the replicated disk, Compute Engine allows migration of disk data to the other
zone where the disk has a replica. This process is called <em>failover</em>.</p>

<p>The failover process involves detaching the zonal replica from the instance in
the affected zone and then attaching the zonal replica to a new instance in
the secondary zone. Compute Engine synchronously replicates the data on your
disk to the secondary zone to ensure a quick failover in case of a single
replica failure.</p>

<h3 id="repd-failover-app-control-plane" data-text="Failover by application-specific regional control plane" tabindex="-1">Failover by application-specific regional control plane</h3>

<p>The application-specific regional control plane is not a Google Cloud service.
When you design HA service architectures, you must build your own
application-specific regional control plane. This application control plane
decides which instance must have the replicated disk attached and which
instance is the current primary instance.</p>

<p>When a failure is detected in the primary instance or database of
the replicated disk, the application-specific regional control plane
of your HA service architecture can automatically initiate failover to the
standby instance in the secondary zone. During the failover, the
application-specific regional control plane reattaches the replicated disk to
the standby instance in the secondary zone. Compute Engine then directs all
traffic to that instance based on health check signals.</p>

<p>The overall failover latency, excluding failure-detection time, is the sum of
the following latencies:</p>

<ul>
<li>Less than 1 minute to attach a replicated disk to a standby instance</li>
<li>Time required for application initialization and crash recovery</li>
</ul>

<p>For more information, see
<a href="https://cloud.google.com/compute/docs/disks/design-considerations-for-resilient-workloads-with-regional-persistent-disks#understanding_the_application-specific_regional_control_plane">Understanding the application-specific regional control plane</a>.</p>

<p>The <a href="https://cloud.google.com/architecture/dr-scenarios-building-blocks">Disaster Recovery Building Blocks</a>
page covers the building blocks available on Compute Engine.</p>

<h3 id="repd-failover-force-attach" data-text="Failover by force-attach" tabindex="-1">Failover by force-attach</h3>

<p>One of the benefits of 
Regional Persistent Disk and Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>) is that in the
unlikely event of a zonal outage, you can manually failover your workload
to another zone. When the original zone has an outage, you can&#39;t
complete the disk detach operation until that zonal replica is restored. In this
scenario, you might need to attach the secondary zonal replica to a new
compute instance without detaching the primary zonal replica from your primary
instance. This process is called <em>force-attach</em>.</p>

<p>When your compute instance in the primary zone becomes unavailable, you can
force attach your disk to an instance in the secondary zone.
To perform this task, you must do one of the following:</p>

<ul>
<li>Start another compute instance in the same zone as the replicated disk
that you are force attaching.</li>
<li>Maintain a hot standby compute instance in that zone. A <em>hot standby</em> is a
running instance that is identical to the one in the primary zone. The two
instances have the same data.</li>
</ul>

<p>Compute Engine executes the force-attach operation in less than one minute.
The total <a href="https://wikipedia.org/wiki/Recovery_time_objective">recovery time objective (RTO)</a>
depends not only on the storage failover (the force attachment of the replicated
disk), but also on other factors, including the following:</p>

<ul>
<li>Whether you must first create a secondary instance</li>
<li>The length of time that it takes the underlying file system to detect
a hot-attached disk</li>
<li>The recovery time of the corresponding applications</li>
</ul>

<p>For more information about how to failover your compute instance using
force-attach, see
<a href="https://cloud.google.com/compute/docs/disks/repd-failover#force-attach">Failover your replicated disk using <code translate="no" dir="ltr">force-attach</code></a>.</p><p>


Regional Persistent Disk and Hyperdisk Balanced High Availability favor workload
availability, which means there are tradeoffs for data protection in the
unlikely event that both disk replicas are unavailable at the same time.
For more information, see
<a href="https://cloud.google.com/compute/docs/disks/repd-failover">Manage failures for replicated disks</a>.</p>

<h2 id="limitations" data-text="Limitations" tabindex="-1">Limitations</h2>

<p>The following sections list the limitations that apply for

Regional Persistent Disk and Hyperdisk Balanced High Availability
(<a href="https://cloud.google.com/products#product-launch-stages">Preview</a>).</p>









<h3 id="general_limitations_for_replicated_disks" data-text="General limitations for replicated disks" tabindex="-1">General limitations for replicated disks</h3>

<ul>
  
  <li>You can attach regional Persistent Disk only to VMs that use
    <a href="https://cloud.google.com/compute/docs/general-purpose-machines#e2_machine_types">E2</a>,
    <a href="https://cloud.google.com/compute/docs/general-purpose-machines#n1_machines">N1</a>,
    <a href="https://cloud.google.com/compute/docs/general-purpose-machines#n2_machines">N2</a>, and
    <a href="https://cloud.google.com/compute/docs/general-purpose-machines#n2d_machines">N2D</a> machine types.</li> 
  <li>You can attach Hyperdisk Balanced High Availability only to <a href="https://cloud.google.com/compute/docs/disks/hyperdisks#machine-type-support">supported
      machine types</a>. 
  </li><li>You cannot create a regional Persistent Disk from an image, or from a disk that was created from
      an image.</li>
  <li>When using read-only mode, you can attach a regional balanced Persistent Disk to a maximum of 10
      VM instances.</li>
  <li>The minimum size of a regional standard Persistent Disk is 200Â GiB.</li> 
  <li>You can only increase the size of a
      regional Persistent Disk or
      Hyperdisk Balanced High Availability volume; you can&#39;t decrease its size.</li>
  <li>
      Regional Persistent Disk and Hyperdisk Balanced High Availability volumes have different performance
      characteristics than their corresponding zonal disks. For more information, see
    <a href="https://cloud.google.com/compute/docs/disks/performance">Block storage performance</a>.
  </li>
  <li>If you create a replicated disk by cloning a zonal disk, then the two zonal replicas
      aren&#39;t fully in sync at the time of creation. After creation, you can use the regional disk
      clone within 3 minutes, on average. However, you might need to wait for tens of minutes
      before the disk reaches a fully replicated state and the
    <a href="https://wikipedia.org/wiki/Recovery_point_objective">
      recovery point objective (RPO)</a> is close to zero. Learn how to
    <a href="https://cloud.google.com/compute/docs/disks/monitor-regional-persistent-disk-replica-state#determine_repd_replication_state">
      check if your replicated disk is fully replicated</a>.</li>
  </ul>



<h3 id="limitations_for_replica_recovery_checkpoints" data-text="Limitations for replica recovery checkpoints" tabindex="-1">Limitations for replica recovery checkpoints</h3>

<ul>
  <li>A replica recovery checkpoint is part of the device metadata and doesn&#39;t show
    you any disk data by itself. You can only use the checkpoint as a mechanism to
    create a snapshot of your degraded disk. After you create the snapshot by using
    the checkpoint, you can use the snapshot to restore your data.
  </li>
  <li>You can create snapshots from a replica recovery checkpoint only when your disk
    is degraded.
  </li>
  <li>Compute Engine refreshes the replica recovery checkpoint of your disk only when
    the disk is fully replicated.
  </li>
  <li>Compute Engine maintains only one replica recovery checkpoint for a disk and
    only maintains the latest version of that checkpoint.
  </li>
  <li>You can&#39;t view the exact creation and refresh timestamps of a replica recovery
    checkpoint.
  </li>
  <li>You can create a snapshot from your replica recovery checkpoint only by using the
    Compute Engine API.
  </li>
</ul>



<h2 id="whats_next" data-text="What&#39;s next" tabindex="-1">What&#39;s next</h2>

<ul>
<li>Learn about <a href="https://cloud.google.com/compute/disks-image-pricing">disk pricing</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/regional-persistent-disk">create and manage replicated disks</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/monitor-regional-persistent-disk-replica-state">monitor the replica states of disks</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/monitor-regional-persistent-disk-replica-state#determine_repd_replication_state">determine the replication state of a disk</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/repd-failover">manage failures for replicated disks</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/create-snapshots">create a snapshot of a replicated disk</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk">build high availability services using replicated disks</a>.</li>
<li>Learn how to <a href="https://cloud.google.com/solutions/scalable-and-resilient-apps">build scalable and resilient web applications on Google Cloud</a>.</li>
<li>Review the <a href="https://cloud.google.com/architecture/dr-scenarios-planning-guide">disaster recovery planning guide</a>.</li>
</ul>



    
  
  

  
    <devsite-hats-survey hats-id="4a78eE2Yw0j6coqRkdD0UcJAGSiv" listnr-id="81820"></devsite-hats-survey>
  
</div></div>
  </body>
</html>
