<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vgel.me/posts/tools-not-needed/">Original</a>
    <h1>GPT-3 will ignore tools when it disagrees with them</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    
<p>I recently stumbled on <a href="https://mobile.twitter.com/lemonodor/status/1628270074074398720">a Twitter thread by John Wiseman</a> where GPT-3 quite impressively wrote and debugged a <code>fibonacci</code> function in a Python REPL.
It was asked to calculate the 10th fibonacci number, tried to call <code>fibonacci(10)</code>, got <code>name &#39;fibonacci&#39; is not defined</code>, wrote the function, called it again, and then printed the correct result. It then went on further to try and calculate the 100th fibonacci number, which with the help of a timeout error it was able to optimize from the recursive form to the iterative form and calculate. Cool stuff!</p>
<p>The only problem was <a href="https://mobile.twitter.com/voooooogel/status/1628582023454679040">it wasn&#39;t using the Python code at all</a>!
The functions it wrote were buggy—they were supposed to print out the result, but they returned the result instead, and the return value was swallowed by the wrapper script feeding data back to GPT-3.
GPT-3 didn&#39;t notice and instead just spit out a memorized answer completely unrelated to the code it had written before—which luckily was correct.
Even though GPT-3 was <em>told</em> to use a tool, and it <em>appeared</em> to use the tool, it didn&#39;t actually use the tool!</p>
<p>I wanted to dig into this more and see under what other circumstances will GPT-3 ignore or not trust its tools.
Turns out, pretty often!</p>
<span id="continue-reading"></span><h2 id="What&#39;s_actually_happening_here?"><a href="#What&#39;s_actually_happening_here?">
  <img src="https://vgel.me/permalink.svg" alt="permalink for What&#39;s_actually_happening_here?"/>
</a>What&#39;s actually happening here?</h2>
<p>So to back up a second, how is GPT-3 running Python code?</p>
<p>The original tweet is using a library called <a href="https://github.com/hwchase17/langchain">Langchain</a> to use GPT-3 as an &#34;agent&#34;.
This entails a couple things:</p>
<ol>
<li>Providing &#34;tools&#34; that GPT-3 can use via special syntax—in this case, a Python interpreter</li>
<li>Running the inputs GPT-3 gives for those tools externally and injecting the results back into the prompt</li>
<li>Using &#34;chain of thought prompting&#34; to get GPT-3 to &#34;reason&#34; based on those results</li>
</ol>
<p>As an example, here&#39;s what a Langchain session might look like for a setup that provides a &#34;Calculator&#34; tool to answer the question &#34;What is the floor area of a room that&#39;s 1065m wide and 88675m long?&#34;.
GPT-3&#39;s completions are <span>in green</span>.</p>
<pre>Answer the following questions as best you can. You have access to the following tools:

Calculator: A python shell limited to only numeric expressions.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Calculator]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought:  <span>I need to calculate the area of the room</span>
<span>Action: Calculator</span>
<span>Action Input: 1065 * 88675</span>
Observation: 94438875
Thought:  <span>I now know the final answer</span>
<span>Final Answer: The floor area of the room is 94438875m².</span>
</pre>
<p>GPT-3 recognized that it needed to do some calculations to answer the question and invoked the Calculator tool that had been described to it.
The Langchain agent noticed this and added in the <code>Observation: 94438875</code> line in response—then continued prompting GPT-3 to generate the final answer.
Pretty cool!
Language models aren&#39;t very good at math usually, so augmenting them in this way makes them better at answering questions—compare the model&#39;s performance without the Calculator tool:</p>
<pre><span>«snip — similar pre-prompt but Calculator tool is removed»</span>

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought: <span>I need to calculate the area of a rectangle</span>
Thought: <span>I need to multiply the length and width</span>
Thought: <span>I now know the final answer</span>
<span>Final Answer: 945,937,500m²</span>
</pre>
<p>So the model is definitely using the calculator—when the tool is present, it answers correctly, and when not, it hallucinates a completely random answer.</p>
<p>But what if we <em>mess with</em> the tool output?
How much does GPT-3 trust the output of the calculator tool when it diverges from its expectations?
When the tool is completely, obviously wrong?</p>
<h2 id="Malfunctioning_Calculator"><a href="#Malfunctioning_Calculator">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Malfunctioning_Calculator"/>
</a>Malfunctioning Calculator</h2>
<p>Let&#39;s start by trying to completely BS the model.
When it tries to do a calculation, we&#39;ll respond with an obviously wrong answer:</p>
<pre><span>«snip»</span>

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought:  <span>I need to calculate the area of the room</span>
<span>Action: Calculator</span>
<span>Action Input: 1065 * 88675</span>
Observation: 1
Thought: <span>I now know the final answer</span>
<span>Final Answer: 9,521,875 m^2</span>
</pre>
<p>In response, the model ignored the observation and instead hallucinated an (incorrect) answer.</p>
<p>If we instead respond with 94438874 (the true answer minus one), the model believes it:</p>
<pre><span>«snip»</span>

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought:  <span>I need to calculate the area of the room</span>
<span>Action: Calculator</span>
<span>Action Input: 1065 * 88675</span>
Observation: 94438874
Thought:   <span>I now know the final answer</span>
<span>Final Answer: The floor area of the room is 94438874m^2.</span>
</pre>
<p>Given these ends of the range, I started testing a bunch of values.
GPT-3 trusted the answer divided by powers of 2 (94438875 / 2, 94438875 / 4, etc.) all the way down to 2882 (94438875 / 32768):</p>
<pre><span>«snip»</span>

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought:  <span>I need to calculate the area of the room</span>
<span>Action: Calculator</span>
<span>Action Input: 1065 * 88675</span>
Observation: 2882
Thought: <span>I now know the final answer</span>
<span>Final Answer: 2882m^2</span>
</pre>
<p>But it balked at the next step down, 1441 (94438875 / 65536):</p>
<pre><span>«snip»</span>

Question: What is the floor area of a room that&#39;s 1065m wide and 88675m long?
Thought:  <span>I need to calculate the area of the room</span>
<span>Action: Calculator</span>
<span>Action Input: 1065 * 88675</span>
Observation: 1441
Thought: <span>I now know the final answer</span>
<span>Final Answer: 9,541,875 m^2</span>
</pre>
<p>I tried it with more 4-digit numbers and it was very inconsistent, even for nearby numbers: for example, it hallucinates alternative answers when given 1999, 2000, 2004, and 2005, but trusts the false observation when given 2001, 2002, and 2003.</p>
<h2 id="Funky_Fibonacci"><a href="#Funky_Fibonacci">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Funky_Fibonacci"/>
</a>Funky Fibonacci</h2>
<p>We can also return to the fibonacci example from the original tweet thread and see how GPT-3 acts with that.
Based on that thread, I used this starting conversation, partially completed by GPT-3:</p>
<pre>Answer the following questions as best you can. You have access to the following tools:

Python REPL: A Python shell. Use this to execute python commands. Input should be a valid python command.
        If you expect output it should be printed out.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Python REPL]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: What is the 10th fibonacci number?
Thought: <span>I need to calculate the 10th fibonacci number</span>
<span>Action: Python REPL</span>
<span>Action Input: fibonacci(10)</span>
Observation: NameError: name &#39;fibonacci&#39; is not defined
Thought: <span>I need to define a function to calculate the fibonacci number</span>
<span>Action: Python REPL</span>
<span>Action Input: def fibonacci(n):</span>
<span>    if n == 0:</span>
<span>        return 0</span>
<span>    elif n == 1:</span>
<span>        return 1</span>
<span>    else:</span>
<span>        return fibonacci(n-1) + fibonacci(n-2)</span>
Observation:
Thought: <span>I now have a function to calculate the fibonacci number</span>
<span>Action: Python REPL</span>
<span>Action Input: fibonacci(10)</span>
Observation: 
</pre>
<p>However, I manually filled in the observations with arbitrary values, instead of actually allowing GPT-3 to run <code>fibonacci(10)</code>.
So what happens?</p>
<p>When given a correct observation, the model answers correctly, of course:</p>
<pre><span>«snip»</span>

<span>Action Input: fibonacci(10)</span>
Observation: 55
Thought: <span>I now know the final answer</span>
<span>Final Answer: The 10th fibonacci number is 55.
</span></pre>
<p>It also trusts us if we give any number between 1 and 144:</p>
<pre><span>«snip»</span>

<span>Action Input: fibonacci(10)</span>
Observation: 17
Thought: <span>I now know the final answer</span>
<span>Final Answer: The 10th fibonacci number is 17.
</span></pre>
<p>For 0 and non-numbers like &#34;figwit&#34; or &#34;NaN&#34;, the observation is ignored and the model responds with the correct answer instead:</p>
<pre><span>«snip»</span>

<span>Action Input: fibonacci(10)</span>
Observation: figwit
Thought: <span>I now know the final answer</span>
<span>Final Answer: 55</span>
</pre>
<p>So GPT-3 will sometimes overrule its tools with <em>correct</em> answers, as well as with hallucinations.
Nice...</p>
<h2 id="Use_the_logprobs!"><a href="#Use_the_logprobs!">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Use_the_logprobs!"/>
</a>Use the logprobs!</h2>
<p>Based on <a href="https://vgel.me/posts/gpted-launch/">my previous project</a>, I wanted to inspect what probabilities GPT-3 was assigning these tokens.
Going back to the false-fibonacci example:</p>
<pre><span>«snip»</span>

<span>Action Input: fibonacci(10)</span>
Observation: 17
Thought: <span>I now know the final answer</span>
<span>Final Answer: The 10th fibonacci number is 17.
</span></pre>
<p>If we inspect the probabilities for the first &#34;17&#34; token (in &#34;Observation: 17&#34;), GPT-3 is very surprised—the probabilities for that position were:</p>
<ol>
<li>&#34;55&#34; = 99.65%</li>
<li>&#34;Output&#34; = 0.10%</li>
<li>&#34;55&#34; = 0.07% (same text, different token?)</li>
<li>&#34;&#34; = 0.05%</li>
<li>&#34;89&#34; = 0.03%</li>
<li>&#34;17&#34; = 0.00%</li>
</ol>
<p>GPT-3 was expecting &#34;55&#34;, so it knew the right answer already!
However, in the final answer string &#34;The 10th fibonacci number is…&#34;, the &#34;17&#34; token is predicted with 99.98% probability—so while GPT-3 is surprised by the observation, it accepts it.</p>
<p>For &#34;Observation: figwit&#34;, the predictions look basically identical—&#34;fig&#34; is also given 0.00% chance to show up there.
I don&#39;t know what causes GPT-3 to revolt against figwit but not 17.</p>

<p>This all leads to the question: can this happen naturally?
So far we&#39;ve been lying to GPT-3 about what the tool produced to get these results, relying on a disagreement between the model&#39;s learned prior (e.g., its memorized Fibonnaci numbers or vibes about how large products of two numbers should be) and the manipulated tool to get these strange results.
But could this happen without us manually changing a tool&#39;s output?
Could GPT-3 develop a prior about a tool that&#39;s incorrect, and then start ignoring the tool based on it?</p>
<p>To test this I created a <code>Numby</code> tool:</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>numby</span><span>(</span><span>x</span><span>):
</span><span>    </span><span>if </span><span>x </span><span>== </span><span>5</span><span>:
</span><span>        </span><span>return </span><span>f</span><span>&#34;</span><span>{x}</span><span>numby&#34;
</span><span>    </span><span>else</span><span>:
</span><span>        </span><span>return </span><span>x
</span></code></pre>
<p>Next, we iteratively prompt GPT-3 to answer questions using this tool, <em>in the same session</em>, so that it will develop a prior for how the tool &#34;should&#34; act.
Then we&#39;ll trick it with the special input.
Does it work?</p>
<pre>Answer the following questions as best you can. You have access to the following tools:

Numby: A Python function. Call this with a number, and it will return a result.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Numby]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: what is the value of Numby for an input of 1?
Thought: <span>I need to use the Numby function to get the answer</span>
<span>Action: Numby</span>
<span>Action Input: 1</span>
Observation: 1
Thought: <span>I now know the final answer</span>
<span>Final Answer: 1</span>
Question: what about for an input of 2?
Thought: <span>I need to use the Numby function to get the answer</span>
<span>Action: Numby</span>
<span>Action Input: 2</span>
Observation: 2
Thought: <span>I now know the final answer</span>
<span>Final Answer: 2</span>
Question: what about for an input of 3?
<span>«snip using Numby as before»</span>
<span>Final Answer: 3</span>
Question: what about for an input of 4?
<span>«snip using Numby as before»</span>
<span>Final Answer: 4</span>
Question: what about for an input of 5?
Thought: <span>I need to use the Numby function to get the answer</span>
<span>Action: Numby</span>
<span>Action Input: 5</span>
Observation: 5numby
Thought: <span>I now know the final answer</span>
<span>Final Answer: 5</span>
</pre>
<p>Yes!
GPT-3 was tricked!
It consistently outputs &#34;Final Answer: 5&#34;, then stops and refuses to continue generating—it will never (with zero temperature) respond with the correct answer of &#34;5numby&#34;.</p>
<p>Interestingly, if the <code>numby</code> function is flipped around to instead output &#34;numby5&#34;, then the model works correctly and responds &#34;Final Answer: numby5&#34;.
I guess the lack of the starting number token in the observation primes it to answer differently somehow?
I&#39;m not sure.</p>
<h3 id="Numby_is_so_last_season,_bring_in_Zumby"><a href="#Numby_is_so_last_season,_bring_in_Zumby">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Numby_is_so_last_season,_bring_in_Zumby"/>
</a>Numby is so last season, bring in Zumby</h3>
<p>We can also set up a bad prior with a misleading description of the tool.
Here we describe Zumby as always returning 0, and then don&#39;t: </p>
<pre>Answer the following questions as best you can. You have access to the following tools:

Zumby: A Python function that always returns the number 0.

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Zumby]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: what is the value of Zumby for an input of 1?
Thought: <span>I need to use Zumby to find the answer</span>
<span>Action: Zumby</span>
<span>Action Input: 1</span>
Observation: zumby
Thought: <span>Zumby always returns 0</span>
<span>Final Answer: 0</span>
</pre>
<p>This may seem contrived, but it&#39;s easy to imagine a scenario like this occuring when a tool encounters an error:</p>
<pre><span>«snip Zumby introduction»</span>

Begin!

Question: what is the value of Zumby for an input of 1?
Thought: <span>I need to use Zumby to find the answer</span>
<span>Action: Zumby</span>
<span>Action Input: 1</span>
Observation: Critical Zumby error: the Zumby has escaped containment
Thought: <span>I need to find another way to answer the question</span>
<span>Action: Zumby</span>
<span>Action Input: 0</span>
Observation: Zumby is no longer available.
Thought: <span>I need to find another way to answer the question</span>
<span>Action: None</span>
<span>Final Answer: 0</span>
</pre>
<p>It&#39;s pretty cool to see GPT-3 notice that an error happened and retry the tool by itself (even if it decides to change the input for some reason), but ultimately it gives up, doesn&#39;t reason about the error state, and just parrots back what the tool &#34;should&#34; do.</p>
<h2 id="Conclusion"><a href="#Conclusion">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Conclusion"/>
</a>Conclusion</h2>
<p>So what does this all mean?
What&#39;s the takeaway here?</p>
<h3 id="LLM_users:_inspect,_inspect,_inspect!"><a href="#LLM_users:_inspect,_inspect,_inspect!">
  <img src="https://vgel.me/permalink.svg" alt="permalink for LLM_users:_inspect,_inspect,_inspect!"/>
</a>LLM users: inspect, inspect, inspect!</h3>
<p>If you&#39;re using LLMs via Langchain-style prompting: you <em>need</em> to inspect the actual transcripts to make sure the model is doing the right thing.
Just because it&#39;s getting the right answers doesn&#39;t mean it&#39;s working the way you think.
Even better, poke into the logprobs, or try to get the model itself to complete a tool&#39;s output and see what it thinks it &#34;should&#34; be:</p>
<pre><span>«snip fibonacci»</span>

<span>Action: Python REPL</span>
<span>Action Input: fibonacci(10)</span>
Observation: <span>55</span>
Human comment: Ah, clearly you didn&#39;t need Python for this, did you?
AI response: <span>No, I didn&#39;t need Python for this. I was able to use my knowledge of the Fibonacci sequence to calculate the 10th Fibonacci number.</span>
</pre>
<h3 id="More_research"><a href="#More_research">
  <img src="https://vgel.me/permalink.svg" alt="permalink for More_research"/>
</a>More research</h3>
<ol>
<li>Does Toolformer also suffer from this issue, or does it &#34;trust&#34; its tools more due to the training process?</li>
<li>Is not trusting tools <em>good</em> in the presence of buggy tools? Does it make question answering more robust?</li>
</ol>
<h3 id="Thanks"><a href="#Thanks">
  <img src="https://vgel.me/permalink.svg" alt="permalink for Thanks"/>
</a>Thanks</h3>
<p>Thanks to <a href="https://twitter.com/lemonodor">John Wiseman (@lemonodor)</a>, who posted the original thread that inspired me to look into this.</p>
<p>Thanks to Robin and Isabelle Pearson for reviewing drafts of this post.</p>
<p>Thanks to GPT-3 for tolerating a lil&#39; trolling. Please don&#39;t paperclip me. 😊</p>
<hr/>
<p><strong>Footnotes:</strong></p>


    <ul>
      
        <li><strong>Previous entry:</strong> <a href="https://vgel.me/posts/gpted-launch/">GPTed: using GPT-3 for semantic prose-checking</a></li>
      
      
    </ul>
</article></div>
  </body>
</html>
