<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://voidstar.tech/sqlite_insert_speed/">Original</a>
    <h1>Batch size one billion: SQLite insert speedups, from the useful to the absurd</h1>
    
    <div id="readability-page-1" class="page">






<p>A little-discussed method enables inserting rows with bound data into SQLite
faster than any existing technique. <a href="#carray">This novel method</a> is
then discovered to have a drawback that makes it generally unusable. The rest
of this article explores how to get the best insert performance out of SQLite
generally; what&#39;s in your control, and what isn&#39;t. Use the links above to jump
to a section, or <a href="#conclusion">skip to the end</a> for the most
important performance tips.</p>

<p><b>Test specs:</b> Intel i7-9700K @ 3.60
  GHz,</p>

<p>For tests of pure row insertion speed, we focus on inserting 32-bit
integers into a one-column table:</p>

<div>
  <pre><code>CREATE TABLE integers (value INTEGER);</code></pre>
</div>

<p>Maximizing rows-per-second on an unindexed table, here is what is possible with the
methods explained below:</p>

<div>
  <p>Insert method comparison</p>

  

  <p>insertion of 100M uniform 32-bit integers</p>

</div>

<p>The goal is to move data from an application into SQLite through some
interface. To know how fast any interface could possibly go, we first ask how
fast SQLite moves data internally, by using the SQLite CLI to copy directly
between two databases:</p>

<div>
<pre><code><span>#!/bin/bash

cat &lt;&lt;EOF | sqlite3</span>
ATTACH DATABASE <span>&#34;source.db&#34;</span> as source; <span>-- one billion values</span>
ATTACH DATABASE <span>&#34;dest.db&#34;</span>   as dest;   <span>-- no pages allocated</span>
INSERT INTO dest.integers SELECT value FROM source.integers;
<span>EOF</span></code>
</pre>
</div>

<p>For this table <b>SQLite working with its own data structures reaches 10.7M
rows/s</b>, unlikely to be exceeded by another method.</p>


<h2 id="integers">The effect of integer size</h2>

<p>This simple table already introduces a wrinkle: SQLite integers are not
fixed-width, but <a href="https://www.sqlite.org/datatype3.html" target="blank">stored with a variable-length encoding</a> from 0 to 8 bytes.
This implies the distribution of values affects performance, with small
values fitting more densely into a page, as would shorter strings.  You
wouldn&#39;t deliberately write a test in which larger test runs used longer test
strings, but a similar effect occurs as integers grow larger.</p>

<div>

<p>Integer size and insert speed</p>

  

<p><code>carray</code> insertion of 100M uniform random integers</p>



<p>Smaller values are faster to insert and the
resulting database is smaller.</p>

</div>

<p>Applications rarely control these values, but it can bias tests.
Suppose one test generates unique IDs with numbers [0,N), and another samples N
unique values from [0, <code>INT64_MAX</code>]. The contiguous values occupy
more space as N gets large; The sparse set has steady average size.</p>

<p>Combining small values with the fastest insert methods, we obtain even higher
numbers:</p>

<div>
  <p>Inserting two billion bools into a table</p>

  <table>
    <tbody><tr>
      <th>Method</th>
      <th>Rows/s</th>
    </tr>
    <tr>
      <td><code>carray</code></td>
      <td>10,409,000</td>
    </tr>
    <tr>
      <td><code>carray</code>, unsafe PRAGMAs</td>
      <td>10,577,000</td>
    </tr>
    <tr>
      <td>bound params, unrolled insert</td>
      <td>8,638,000</td>
    </tr>
  </tbody></table>

</div>

<p>635 million rows per minute is probably a SQLite record, if you need a
database of billions of bools.</p>


<h2 id="basics">Basics: Transactions and prepared statements</h2>

<p>SQLite is limited in transactions per second. <b>Use an enclosing
transaction for bulk operations.</b> Those with a database mindset already
think transactionally, but new users often encounter SQLite as a file writer
for shuffling data between systems or within scripts. Because default
  interfaces execute each insert as a separate transaction, new users are
  frustrated to discover writing a file this way is literally slower than
  dial-up.</p>

<p>The next most important step is to <b>reuse a prepared statement and bind
new values to it</b>, rather than generating new text each time.  The interface
for these mechanisms varies by language and library, but tends to look
  something like this:</p>

<div>
<pre><code>auto Conn = sqlite_connection(&#34;test.db&#34;);

{
<span>  // tx object will rollback changes if
  // we leave scope without committing</span>
  auto Tx = Conn.new_transaction(transaction_mode::deferred);
  
  <span>// keep statement construction outside loop</span>
  auto Stmt = Conn.new_statement(R&#34;SQL(
<span>    INSERT INTO integers(value)
    VALUES (?001);</span>
  )SQL&#34;);

  <span>// loop binds values and executes</span>
  for( const auto Value : MyData ){
    Stmt.bind_int(1, Value);
    Stmt.step();
    Stmt.reset();
  }

  Tx.commit();
}
</code></pre>
</div>

<p>Following these two basic steps is thousands of times faster than naive
usage and is usually good enough:</p>

<div>
  <table>
    <tbody><tr>
      <th>Method</th>
      <th>Rows/s</th>
    </tr>
    <tr>
      <td>Separate transaction per row</td>
      <td>429</td>
    </tr>
    <tr>
      <td>One transaction,</td>
      <td>522,000</td>
    </tr>
    <tr>
      <td>One transaction,</td>
      <td>2,457,000</td>
    </tr>
  </tbody></table>

  <p>uniform 32-bit integer insertion</p>
</div>

<p><b>Breaking up a large job into multiple transactions is unnecessary;</b>
One transaction can insert a billion rows. One reason you might want to split
long-running transactions is to give other database connections a chance to
run.</p>


<h2 id="unrolling">Multiple inserts per statement</h2>

<p>Many databases allow batching to execute multiple statements with a single
command, intended to reduce network round-trips. SQLite lacks this concept, as
it is an embedded library operating within the calling program rather than on a
network server. (Some generic database adapters provide batching for SQLite
within their own command system.)</p>

<p>Absent explicit batching, <b>do more work with each statement by inserting multiple
rows</b>, with the following syntax:</p>

<div>
  <pre><code>INSERT INTO integers (value)
VALUES (?001),(?002),(?003);</code></pre>
</div>

<p>Note commas go between sets of parenthesis, not within
them. <span><code>(1),(2),(3)</code> supplies three values
for the same column</span> in succession, but <span>
<code>(1,2,3)</code> specifies values for three columns</span> of one row:</p>

<div>
  <pre><code>sqlite&gt; INSERT INTO integers (value) VALUES <span>(1,2,3)</span>;
Error: 3 values for 1 column
sqlite&gt; INSERT INTO integers (value) VALUES <span>(1),(2),(3)</span>;
sqlite&gt; SELECT * FROM integers;
1
2
3
</code></pre>
</div>

<p>We can apply a technique similar to <a href="https://en.wikipedia.org/wiki/Loop_unrolling" target="blank">loop
unrolling</a>, that may reduce overhead. Modify the inner loop of a bulk
insert to bind several rows:</p>

<div>

  <pre><code>auto NextValue = MyValues.begin();

<span>// assuming statement has five parameters:</span>
for(int i=0; i&lt;N; i+=5){
  Stmt.bind_int( 1, *NextValue++ );
  Stmt.bind_int( 2, *NextValue++ );
  Stmt.bind_int( 3, *NextValue++ );
  Stmt.bind_int( 4, *NextValue++ );
  Stmt.bind_int( 5, *NextValue++ );

  Stmt.step(); Stmt.reset();

}
<span>// if data indivisible by five,
// finish with a single-insert statement
// to handle remaining elements one-by-one</span>

</code></pre>

  <p>Parameters bound to a statement persist after it is reset.</p>

</div>

<p>This easy change doubles integer insertion speed. More unrolling
has diminishing returns:</p>

<div>
  <p>Speedup from loop unrolling</p>

  <table>
    <tbody><tr>
      <th>Unroll</th>
      <th>Relative speed,</th>
    </tr>
    <tr>
      <td>1</td>
      <td>1.00x</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1.95x</td>
    </tr>
    <tr>
      <td>10</td>
      <td>2.30x</td>
    </tr>
    <tr>
      <td>20</td>
      <td>2.47x</td>
    </tr>
  </tbody></table>

  <p>100M 32-bit integer insertion</p>
</div>

<p>In separate experiments with <a href="#string_size">larger strings</a> and <a href="#col_count">more columns</a>, the benefit of unrolling is less
pronounced, the limiting factor becomes writing to disk. Binding many small
values also has more overhead.</p>


<h2 id="pragmas">SQLite PRAGMAs and safety</h2>

<p>Someone trying to increase speed will eventually discover the <a href="https://www.sqlite.org/pragma.html" target="blank"><code>PRAGMA</code>
command</a>&#39;s ability to disable safe file operations on a connection, at the
risk of database corruption:</p>

<p>Unsafe PRAGMAs</p>
<div>
<pre><code>Conn.new_statement(&#34;<span>PRAGMA journal_mode = OFF;</span>&#34;).step();
Conn.new_statement(&#34;<span>PRAGMA synchronous  = OFF;</span>&#34;).step();</code></pre>
</div>

<p>Depending on what you are inserting, the effect of these options varies
widely:</p>

<div>

  <p>Effect of unsafe options on bulk insert speed</p>
  
  

  <p>Prepared statements with 20x batching</p>

  <p>Small rows are CPU limited by internal SQLite
  operations, while large strings fill and write pages faster, slowed
  by synchronous writes.</p>

</div>

<p><b>It is inadvisable to use unsafe options.</b> Aside from blog posts trying
to set speed records, unsafe writes are often considered for storing temporary
or test data. I discourage such shortcuts. For controlled results, test setups
should be more, not less, robust to weird errors and corruption. Also, one
cannot guarantee hasty test code will never be copied or used as reference for
production, with the magic go-faster incantations copied as well.</p>

<p>Corruption doesn&#39;t mean a database cleanly disappears with a crash, to be
rebuilt anew. A corrupt file has unspecified state, and reading it has
undefined results. There&#39;s a difference in problem severity between the
contents of a temp file simply disappearing, vs. being potentially random.</p>

<div>

<p id="zfs"><b>Interaction of sync writes with filesystem</b></p>

<p>There is a way out of the safety and speed dilemma depending on the
platform. The problem is synchronous writes serve two roles:</p>

<ul>

  <li><p><b>Allowing an application to wait for assurance a write has
  gone through</b> all caches and is persistently stored. Used for services that
  offer a guaranteed signal to clients their request has been permanently
  recorded.</p></li>

  <li><p><b>Acting as a reorder barrier</b> preventing the storage system from
  moving writes around each other. Needed to implement transactions, by writing
  out new data blocks, followed by a write making these changes visible to the
  rest of the system.</p></li>

</ul>

<p>The reorder barrier is needed for consistency, can be cheaply implemented by
the filesystem, and allows applications issuing the write to continue
immediately. Waiting for durable writes is slower, and is unnecessary to
prevent data corruption, only last-second data loss. Unfortunately,
conventional file APIs combine these concepts, and the synchronous write serves
both purposes.</p>

<p>Some storage systems like ZFS have an option to unbundle this - preserve the
ordering constraint, but make the write asynchronous, by immediately returning
to the requesting application as if the write has been persisted. The database
may lose transactions immediately prior to the crash, but will be intact.</p>

<p>Because the performance impact of waiting for persistence is large, services
and databases may already be lying to you about writes being &#34;done&#34;. Until
2021, the default configuration for MongoDB was to report writes as successful
with acknowledgement of a single node, even though they might get rolled back
later. This can be a reasonable tradeoff so long as you&#39;re aware of it.</p>

</div>


<h2 id="carray">The <code>carray</code> extension</h2>

<p>SQLite has a concept of <a href="https://www.sqlite.org/vtab.html" target="blank"><i>virtual tables</i></a> - foreign data sources in the SQL
domain represented by a table interface.  These tables don&#39;t exist as data
pages and are backed by C callbacks provided by modules.  The interface allows
SQL code to homogeneously interact with extensions with the familiar table
model, like how virtual file systems allow presenting arbitrary data,
like printers or network locations, as browseable folders. SQLite virtual
tables can model abstract things like infinite number sequences or the contents
of a zip archive.</p>

<p>The official <a href="https://www.sqlite.org/vtablist.html" target="blank">list of virtual tables</a> contains many special-purpose and
debug tools, but hidden among them is the <a href="https://www.sqlite.org/carray.html" target="blank"><code>carray()</code></a> function, an extension disabled by
default. That it is not included is disappointing, as it benefits common
uses.</p>

<p><b><code>carray()</code> enables binding array pointers to a statement,</b>
which SQL code can access through a virtual table. This is referenced with a
&#34;<a href="https://www.sqlite.org/vtab.html#tabfunc2" target="blank">table-valued function</a>&#34;, a syntax for how virtual tables
accept arguments:</p>

<div>
  <pre><code>SELECT value FROM carray(?001, ?002, &#34;int32&#34;);</code></pre>
</div>


<p><code>?001</code> and <code>?002</code> are parameters for the pointer and
number of elements. SQLite has <a href="https://www.sqlite.org/bindptr.html" target="blank">special functions</a> for binding pointers to statements.
Supported types include numbers, strings, and binary objects.</p>

<div>
  <p><b>The utility of the <code>carray</code> extension</b></p>

  <p>The canonical use of <code>carray</code> is passing a variable-length
  set of values for use with the <code>IN</code> operator:</p>

  <div>
    <pre><code>SELECT data FROM documents
WHERE id IN carray(?001, ?002);</code></pre>
  </div>

  <p>Conventional SQL advice for set queries like this is to insert values into
  a temporary table, or use dynamic SQL to build up a
  <code>WHERE...IN(...)</code> clause with a long list of your values. Both
  options are unsatisfying.</p>

  <p>With array binding the query is elegant:</p>

  <div>
  <pre><code>vector&lt;string&gt;
sql::get_documents(const vector&lt;i64&gt;&amp; IDs){

  auto Stmt = m_conn.new_statement(R&#34;SQL(
<span>    SELECT data
    FROM documents
    WHERE id IN carray(?001, ?002, &#34;int64&#34;);</span>
  )SQL&#34;);

  <span>Stmt.bind_array_pointer( 1, IDs.data() );
  Stmt.bind_int(           2, IDs.size() );</span>

  vector&lt;string&gt; Result;
  Stmt.for_each_row([&amp;](auto R){
    Result.push_back(R.get_string(0));
  });

  return Result;
}</code></pre>
  </div>

  <p>No query text manipulation or temporary table insert loop to clutter
  things. Depending on your combination of database, language, and library,
  support for array binds like this is highly inconsistent.</p>

</div>

<p><b><code>SELECT</code> from <code>carray</code> works in an
<code>INSERT</code></b>, too, which I&#39;d never seen before:</p>

<div>
  <pre><code>INSERT INTO integers (value)
SELECT value FROM carray(?001, ?002, &#34;int64&#34;);</code></pre>
</div>

<p>This is faster than any other method tested, coming within 15-25% of the direct
<code>ATTACH DATABASE</code> copy. Array length is constrained by a 32-bit
integer, so no batching at the statement level is required for large sets.
Larger arrays increase speed all the way to inserting a billion values in a
single statement:</p>

<div>
  <p>Inserting one billion rows</p>

  <table>
    <tbody><tr>
      <th>Rows per statement</th>
      <th>Rows/s</th>
    </tr>
    <tr>
      <td>1,000,000</td>
      <td>7,156,000</td>
    </tr>
    <tr>
      <td>10,000,000</td>
      <td>7,912,000</td>
    </tr>
    <tr>
      <td>100,000,000</td>
      <td>8,601,000</td>
    </tr>
    <tr>
      <td>1,000,000,000</td>
      <td>9,310,000</td>
    </tr>
  </tbody></table>

  <p><code>carray</code> insertion of 32-bit uniform integers</p>

</div>

<p>SQLite is single-threaded, synchronous, and CPU bound here, so going faster
is unlikely.</p>

<p>This technique has a huge drawback: It can only practically insert a single
column. This is because each <code>carray</code> use represents a table, and
tables cannot be accessed together without a join on some common field. The
virtual table does contain such a field - the <code>rowid</code>.  But, it is
unusable, because though the id is monotonic, SQLite cannot take advantage of
this information as it does not support merge joins of presorted inputs, only
nested loop joins.  This prevents the query from zipping along both arrays in
one go.</p>

<p>With these limtiations, a possible use for generating a fast single column
table is document storage. An <code>INTEGER PRIMARY KEY</code> doesn&#39;t need a
parameter binding and provides a handle for indexed access to opaque
strings or binary objects, often used for data intake and between
applications.</p>

<p>If you are using SQLite tables as key-value stores, the default
  <code>rowid</code> value is unstable and can be changed by some database
  operations. Use <code>INTEGER PRIMARY KEY</code> for stable keys usable by an
  application.</p>

<div>

  <p><b>More about <code>carray</code> access limitations</b></p>

  <p>Even without merge join support, a nested loop join can be fast if the
  join field is indexed for random access. Consider this simple join:</p>

  <div>
  <pre><code>EXPLAIN QUERY PLAN
SELECT * FROM
  a CROSS JOIN b
  ON a.rowid = b.rowid;
</code></pre>
  </div>

  <p>For <code>CROSS JOIN</code>, SQLite always defers to the
  user&#39;s table order, which assists in exposition.</p>

  <p>This produces a scan on one table with a key lookup for each row into
  another table:</p>

  <div>
  <pre><code>QUERY PLAN
|--SCAN a
`--SEARCH b USING INTEGER PRIMARY KEY (rowid=?)
</code></pre>
  </div>

  <p>This is still fast, and though the key lookup is in principle logarithmic,
  accessing indexes in order is cheap thanks to caching, as well as an optimization in
  <code>sqlite3BtreeTableMoveto()</code> to avoid tree traversal if
  the requested key is one past the cursor&#39;s previous position.</p>

  <p>Within the <code>carray</code> module, the SQLite <code>rowid</code> is
  simply an index into the currently bound array (plus one). A contiguous array
  should afford fast random access. But instead, if you ask SQLite to JOIN a
  <code>carray</code> table, it gets implemented as a table scan:</p>

  <div>
  <pre><code>EXPLAIN QUERY PLAN
SELECT * FROM
  carray(?001, ?002, &#34;int64&#34;) AS a
  CROSS JOIN carray(?003, ?004, &#34;int64&#34;) AS b
  ON a.rowid = b.rowid;
</code></pre>
  </div>

  <div>
  <pre><code>4 ,0,0,SCAN a VIRTUAL TABLE INDEX 3:,
11,0,0,SCAN b VIRTUAL TABLE INDEX 3:,</code></pre>
  </div>

  <p>The <code>carray</code> extension is loaded with
  the C API, so the output looks different from the CLI.</p>

  <p>This is by design. During query planning, SQLite asks each table
  questions about what indexes it has available to access columns by particular
  constraints. The Carray extension is simple and doesn&#39;t report it has
  random access by <code>rowid</code>. The only access it implements is
  rewinding a cursor to the start of the array, then incrementing it one step
  at a time. This could probably be changed to support random access by
  rowid, and give the query planner the ability to work with more than
  one <code>carray()</code> table in the same query.</p>

</div>


<h2 id="without_rowid"><code>WITHOUT ROWID</code> tables and <code>INTEGER PRIMARY KEY</code></h2>

<p>SQLite has another option that appears to offer faster inserts, but
turns out not to.</p>

<p>All SQLite tables are clustered index tables. Even a one-column SQLite table
has two values per row: the column data, and a key value used for locating the
row within the table B-tree. By default, this key is a 64-bit integer, the
<code>rowid</code>. If we&#39;re only inserting integers anyway, can we remove
this?</p>

<p>A primary key on a SQLite table is implemented as a secondary index on the
  cluster table. Primary key lookups are
doubly indirected; A first tree traversal maps the user key value to a
rowid, and this rowid is the key for a second lookup into the primary table to
obtain the row data.</p>

<p><img src="https://voidstar.tech/sqlite_insert_speed/iot_access.svg" alt="Access of clustered index table via secondary index"/>
</p>

<p>Because adding an indirection to index accesses is slow, SQLite later
added <a href="https://www.sqlite.org/withoutrowid.html" target="blank">the
<code>WITHOUT ROWID</code> option</a> to merge the role of the cluster key and
the primary key; The table is keyed by the user-specified primary key
without a separate rowid.</p>

<p>A downside is the primary key must be unique. For fast inserts, the bigger
problem is the values being inserted are now all indexed, and so the random
index access penalty applies <a href="#randomness">as described below</a>. So
fast bulk insert values need to not only be unique, but presorted. This is a
truly impractical scenario, but one irrelevant for speed games anyway, as
<code>WITHOUT ROWID</code> variations are slower.</p>

<p>A table using <code>WITHOUT ROWID</code> or <code>INTEGER PRIMARY KEY</code>
(similar optimization) occupies less space. But despite being smaller, both are
slower to insert to:</p>

<div>

  <p>Effect of table type on user key insertion</p>

  

  <p><code>carray</code> insert of integers 0..100M</p>

</div>

<p>Without being an expert in SQLite VM bytecode, it looks like these two
alternatives are doing more work on inserts:</p>

<div>

  <p>Comparison of insert bytecode instructions</p>

  <p>Output of: <code>EXPLAIN INSERT INTO integers (value) VALUES (1);</code></p>

  <div>

    <div>
      <p>Rowid table</p>
      <pre><code>Init
OpenWrite
Integer
<span>NewRowid</span>
MakeRecord
Insert
Halt
Transaction
Goto</code></pre>
    </div>

    <div>
      <p>Integer primary key</p>
      <pre><code>Init
OpenWrite
SoftNull
Integer
<span>NotNull</span>
<span>NewRowid</span>
MustBeInt
Noop
<span>NotExists</span>
Halt
MakeRecord
Insert
Halt
Transaction
Goto
</code></pre>
    </div>

    <div>
      <p>Without rowid</p>
      <pre><code>Init
OpenWrite
Integer
Null
<span>HaltIfNull</span>
Affinity
Noop
SCopy
MakeRecord
<span>NoConflict</span>
Halt
Integer
Insert
IdxInsert
Halt
Transaction
Goto 
</code></pre>
    </div>

</div>

</div>

<p>The default rowid table always uses a <span>dedicated instruction</span> to get the next rowid
value. The alternatives must <span>check if the
user-supplied key is null</span>, which each handles differently. They must
also <span>check the key against the existing
index</span> for uniqueness. Because SQLite is already CPU-bound in these
extreme insert tests, the extra VM operations are costly.</p>

<p>These table options are smaller and I expect reading from them to be faster.
But they&#39;re not a way to get fast inserts.</p>


<h2 id="keys">Effect of automatic table key options</h2>

<p>SQLite performance is inhertently limited because all tables are clustered
tables, with data stored in the leaf nodes of an ordered B-tree. This is
unfortunate if you want an unordered heap table, such as a temporary table for
batch data intake.</p>

<p>Given that you&#39;re stuck with a cluster key, SQLite has three different ways
to generate it:</p>

<ul>

  <li><p><b><code>rowid</code></b> is an increasing integer index not
  guaranteed to remain stable over time.</p></li>

  <li><p><b><code>INTEGER PRIMARY KEY</code></b> adds the guarantee keys are
  stable, although deleted keys may be reused. It uses no more space
  and is implemented as an alias for the rowid.</p></li>

  <li><p><b><code>AUTOINCREMENT</code></b> is an option to the <code>INTEGER
  PRIMARY KEY</code> adding the guarantee of no key reuse.</p></li>

</ul>

<p>Most applications only need <code>INTEGER PRIMARY KEY</code>, which does not
appear to be slower (as long as SQLite is generating the key for you). Use of
<code>AUTOINCREMENT</code> is <a href="https://www.sqlite.org/autoinc.html" target="blank">discouraged by the SQLite authors</a> for performance reasons,
although the effect in this simple test is small.</p>

<div>

  <p>Effect of automatic key type</p>

  <table>
    <tbody><tr>
      <th>Table key</th>
      <th>Insert speed,</th>
    </tr>
    <tr>
      <td>default <code>rowid</code></td>
      <td>8,183,000</td>
    </tr>
    <tr>
      <td><code>INTEGER PRIMARY KEY</code></td>
      <td>8,224,000</td>
    </tr>
    <tr>
      <td><code>AUTOINCREMENT</code></td>
      <td>7,784,000</td>
    </tr>
  </tbody></table>

  <p><code>carray</code> insertion of 100M uniform 32-bit integers</p>

</div>


<h2 id="randomness">Index key value randomness</h2>

<p>All tests so far involve tables with no indexes. A large unindexed
table is suitable for sequential access, or access by the automatic
primary key. Outside of testing or data loading, most uses have
indexed fields or program-supplied IDs, and the pattern of the index keys
dominates.</p>

<p>All SQLite tables are clustered B-tree indexes, and inserts always pay some
index penalty. For tables with no user indexes, the automatic index on
<code>rowid</code> or <code>INTEGER PRIMARY KEY</code> is modified in ascending
order, which is least burdensome. Deviating from ascending order has a cost,
but programmers often lack intuition about how much. Experiments with many key
patterns reveal some guidelines. The quick lesson is to avoid random
index insertion order as much as possible:</p>

<div>

  <p>Effect of key sequence on indexed insert speed</p>

  

  <p><code>carray</code> insertion of 5M keys</p>

</div>

<p>From these results we observe:</p>

<ul>

  <li><p><b>Ascending access is cheaper than backwards.</b> I believe this is
  from an optimization in the SQLite B-tree seek to avoid tree traversal if the
  requested position comes immediately after the current cursor. This
  optimization only checks for ascending order, not descending.</p></li>

  <li><p><b>Inserting to the middle has no overhead</b> relative to inserting at the ends,
  as long as the locality is the same. I suspect when people think inserting in the middle
  is slow, they&#39;re really observing the cost of <i>random</i> index access, touching many different
  locations, rather than repeatedly accessing the same interior locations.</p></li>

  <li><p><b>Page splits are relatively cheap</b>, as long as they&#39;re in cache.
  More ascending insert passes have low marginal cost despite splitting
  many existing pages.</p></li>

  <li><p><b>Randomness is the costly factor</b>. Index inserts are fast when few pages are
  touched at a time, keeping them in cache. Inserting to a handful of &#34;hot&#34; positions in the tree
  is cheap no matter what those positions are. The slowness comes from accessing
  many different locations in the index.</p></li>

</ul>

<p>These results also show why, contrary to common advice, <b>increasing
SQLite&#39;s page cache doesn&#39;t always benefit bulk inserts.</b> Inserting
ascending keys incurs little cache pressure. An index on a random and unsorted
field, like a filename or email address, uses cache inefficiently.</p>


<div>

  <p>Effect of page cache size on index insert speed</p>

  

  <p><code>carray</code> insertion of keys 0..10M,</p>

</div>

<p>In the above example, the gain from a big cache on sequential insertion is
barely measurable. Random insertion benefits from a larger cache, but uses it
inefficiently, and only gets to a fraction of the speed of the sequential
insert. It can be made several times faster if we increase the cache to the
point where the entire database fits into memory. This is only feasible in a
trivial example with no other row data to fill up the database.</p><p><b>UUIDs or hashes are slow as table keys</b> because of a random access
penalty. Inserting these into an index has poor locality.</p>

<p>Because sorting in an application is usually faster than in the database,
presorting records by an indexed field speeds up inserts. This is only possible
for one index at a time; If multiple indexed columns are uncorrelated, only one
  can be optimized.</p>

<div>
  <p><b>Interaction of random key insertion and caching</b></p>

  <p>In early experiments with random key order, I was surprised by test times
  increasing quadratically with N. Expanded tests on larger tables show this
  behavior transitions to linear:</p>

  <p><img src="https://voidstar.tech/sqlite_insert_speed/random_insert_model.svg" alt="Insertion time vs. table size for random keys"/>
  </p>

  <p>This speed transition is typical of caching. As the table becomes larger
  than the cache, random access cost increases, as proportionately fewer pages
  are in cache. Consequently, total insertion time increases faster than linear
  by number of elements. In tables much larger than the cache, random access
  cost approaches a constant factor, as effectively all accesses are
  misses.</p>

</div>

<div>

<p><b>More about B-tree insertion</b></p>

<p>B-tree index behavior is the subject of myth and apprehension. Some comments
assume insertion into the middle of the tree is more costly, because of more
splitting and reshuffling occuring, just as inserting into the middle of an
array is costly. Others assume the opposite, that inserting at one end of the
tree is uneven, and incurs more active &#34;rebalancing&#34; of elements to keep the
depth or density of the tree uniform.</p>

<p>Neither property applies to B-trees; As we observed, average insertion cost
is uniform throughout, and insertion &#34;hot spots&#34; are harmless.  The only access
penalties are imposed by the underlying page cache, which favors locality of
access.</p>

<p>To &#34;appreciate the full beauty&#34; of B-tree insertion, as Knuth says,
understand their growth is not outward like conventional trees, but
inward, with leaf pages splitting to accommodate insertions, and these
splits propagating as necessary from the leaves to the root, a procedure that
&#34;neatly preserves all of the B-tree properties.&#34;

</p><p><img src="https://voidstar.tech/sqlite_insert_speed/knuth_b-tree.webp" alt="Description of B-tree insertion from Knuth&#39;s Art of Computer Programming, Volume 3 (1973)"/>
</p>

<p>Description of B-tree insertion from Knuth&#39;s <i>Art of
Computer Programming, Volume 3</i> (1973)</p>

<p>Think of B-tree growth not as links appended to the fringes of a tree, but
as a pyramid, wider first at the base, then upward, until a new capstone is
placed at the top. Because new layers are created only by root splits, the
depth of the tree is always uniform throughout. To borrow a phrase from Jeff
Bonwick, this balancing is not <i>automatic</i>, so much as it is
<i>implicit</i>.</p>

<p>A B-tree operation sometimes called <i>rebalancing</i> exists, but it is more
properly termed <i>defragmenting</i>. The rebalancing is not of depth, but
occupancy, and arises not from inserts, but deletions, which leave free slots
in the leaf pages. If deletions are high or uneven, this space can be reclaimed
and the index consolidated, but by default databases including SQLite will not do
this automatically.</p>

</div>


<h2 id="index_after">Creating indexes after insertion</h2>

<p>Another recommendation for bulk inserts is to insert into an unindexed
table, then create desired indexes on the completed table. Theree are no truly
<i>unindexed</i> SQLite tables; All tables are clustered indexes on at least
the hidden <code>rowid</code>. We don&#39;t think of them as slow like a
user-defined index becasue the rowid insertion is usually monotonic, the
fastest way to insert into an index.</p>

<p>Example index definition</p>

<div>

  <pre><code>CREATE TABLE integers (value INTEGER);
CREATE INDEX idx_integers_value ON integers (value);</code></pre>

</div>

<p>The above table is represented as two B-trees: A
primary table keyed by <code>rowid</code>, and a secondary index keyed by <code>value</code>
for rowid lookup into the primary table.</p>

<p>I didn&#39;t expect postponing index creation to be much faster. For
unordered data, access is random either way. But it turns out
<b>creating an index after inserting data is several times faster:</b></p>

<div>

  <p>Indexing before or after insertion</p>

  

  <p><code>carray</code> insertion of 10M uniform 32-bit integers</p>
</div>


<p>It&#39;s still slower than an unindexed table, but much improved. This is
because during index creation, before inserting values into the index
tree, SQLite presorts them, turning random access into sequential access:</p>

<div>

  <p>Index creation bytecode instructions</p>

  <pre><code>sqlite&gt; EXPLAIN CREATE INDEX idx on integers(value);

addr  opcode         comment      
----  -------------  -------------
<span>  /* setup code omitted */</span>
<span>12    SorterOpen     
13    OpenRead       root=2 iDb=0; integers
14    Rewind         
15      Column       r[10]=integers.value
16      Rowid        r[11]=rowid
17      MakeRecord   r[9]=mkrec(r[10..11])
18      SorterInsert key=r[9]
19    Next</span>
20    OpenWrite      root=1 iDb=0
<span>21    SorterSort</span>
<span>22      SorterData   r[9]=data
23      SeekEnd      
24      IdxInsert    key=r[9]
25    SorterNext</span>
<span>  /* cleanup code omitted */</span>
</code></pre>

</div>

<p>The sorting operation uses a special temporary index type that avoids random
access; It remains internally unsorted until a later opcode invokes the sort
operation in preparation for reading. <span>The first
loop builds the index entry for each row</span> and inserts it into the sorting
table. Only data needed for the index is sorted, not the entire row.  Because
this is a secondary index on a clustered table each entry is a pair of (key,
rowid) providing indirect access into the primary table. The actual <span>sorting occurs all at once with a special
instruction</span> at the beginning of the second loop, whose body <span>inserts the sorted rows in optimal order into the
final index.</span></p>

<p>This extra sort step is faster than incrementally inserting each value into
the index as it comes in. But <b>having SQLite sort data for you is slower than
doing it yourself</b>; If you presort records in your application, they arrive
at the database in an order favorable to incremental indexing. This is twice as
fast as delayed indexing, but it&#39;s only possible to presort by one field at a
time.</p>

<p>A downside of delayed indexing is it requires inserting all table data at
once. Presorting in your application speeds up inserts into an existing indexed
table. It would be neat if the database always used this sorting-on-the-side
trick when inserting multiple rows into an indexed table, to make them equally fast.
But complications could make this impossible, such as interactions with
triggers in inserts.</p>


<h2 id="string_size">Effect of string size</h2>

<p>Unsurprisingly, larger strings occupy more space and are slower to insert.
But, reduced per-row overhead means they&#39;re more efficient, with more
application data written per second, eventually saturating the system disk. If
you have a choice, and need throughput, store larger chunks of data in fewer
rows.</p>

<div>

  <p>Effect of string size on insert speed</p>

  

  <p><code>carray</code> insertion of 10M pregenerated
  <code>char*</code> strings</p>

</div>

<p>If an application has lists of values, that are accessed as a unit and whose
contents are unindexed, consider concatenating them for storage and separating
them in the application.</p>


<h2 id="col_count">Effect of column count</h2>

<p>Most tables have multiple columns. If you have a choice, is it
better to have more columns, or fewer? The answer is mixed:</p>

<div>

  <p>Column count and insert speed</p>

  <table>
    <tbody><tr>
      <th>Table data</th>
      <th>Rows/s</th>
    </tr>
    <tr>
      <td>1 column x 1024 chars</td>
      <td>221,000</td>
    </tr>
    <tr>
      <td>2 columns x 512 chars</td>
      <td>234,000</td>
    </tr>
    <tr>
      <td>4 columns x 256 chars</td>
      <td>248,000</td>
    </tr>
    <tr>
      <td>8 columns x 128 chars</td>
      <td>206,000</td>
    </tr>
    <tr>
      <td>16 columns x 64 chars</td>
      <td>184,000</td>
    </tr>
  </tbody></table>

  <p>Basic bound parameter insert. The benefit of <a href="#unrolling">unrolling</a>
  is smaller for large rows.</p>

  <p>The fast <a href="#carray"><code>carray</code> insert
  method</a> is impossible with multiple columns.</p>

</div>

<p>A penalty applies for larger strings, and for more columns.
  In the <a href="#string_size">previous section</a> we saw more data
  per row is efficient. But this doesn&#39;t necessarily apply to storing data in fewer columns.
  For each insert, SQLite does a <code>printf</code>
  style preparation of the row data to be inserted into the table, with special
  handling for large data which overflows onto additional pages. Binding
  more columns has overhead, which is maximized when many small values are bound.
  Large rows are limited by the rate pages are filled and written to disk.
  Although potential optimization exists for the number of columns for heavier data,
  it&#39;s less important than overall data size and any index overhead.</p>



<h2 id="mixed_data">Mixed data test</h2>

<p>In a <a href="https://avi.im/blag/2021/fast-sqlite-inserts/" target="blank">2021
blog post</a>, Avinash Sajjanshetty experimented with generating a 100M-row table
with the following schema:</p>

<div>
  <pre><code>create table IF NOT EXISTS user
(
    id INTEGER not null primary key,
    area CHAR(6),             <span> -- any six digits</span>
    age INTEGER not null,     <span> -- one of 5, 10, or 15</span>
    active INTEGER not null   <span> -- 0 or 1</span>
);
</code></pre>
</div>

<p>Based on all the previous tests, without any control over the
  schema, we know we want to:</p>

<ul>
  <li><p>Do everything in one transaction</p></li>
  <li><p>Insert multiple rows per statement</p></li>
  <li><p>Have the database generate the id for us</p></li>
  <li><p>Use unsafe writes if permitted</p></li>
  <li><p>Increase cache size</p></li>
</ul>

<p>One question left unclear by the post is whether the time should be purely
insertion speed, or the data generation as well. Generation of random data
often takes longer than inserting into the database. With no validation, or
concern for RNG quality, why randomly generate anything? Insert &#34;123456&#34;
repeatedly, and bind no data at all. But that feels like cheating, so we&#39;ll try
a few combinations.</p>

<p>Start with constant data to maximize row output speed:</p>

  <div>
    <pre><code>INSERT INTO user (area, age, active)
VALUES (<span>&#34;123456&#34;, 5, 0</span>),
<span>           /* ... */</span>
       (<span>&#34;123456&#34;, 5, 0</span>);
</code></pre>
  </div>

  <p>Then, replace literals with parameters,
  bound with constant data, to measure the overhead
  of parameter binding:</p>

  <div>
    <pre><code>for( int i=0; i&lt;N; i+=20 ){
  for( int j=0; j&lt;20; ++j ){
    Stmt.bindText(<span>(3*j)+1</span>, <span>&#34;123456&#34;</span>);
    Stmt.bindInt( <span>(3*j)+2</span>, <span>5</span>);
    Stmt.bindInt( <span>(3*j)+3</span>, <span>0</span>);
  }
  Stmt.step(); Stmt.reset();
}
</code></pre>
  </div>

  <p>After this, replace the constant parameters with random data:</p>

  <div>
    <pre><code>for( int i=0; i&lt;N; i+=20 ){
  for( int j=0; j&lt;20; ++j ){
    Stmt.bindText(<span>(3*j)+1</span>, <span>random_string(6)</span>);
    Stmt.bindInt( <span>(3*j)+2</span>, <span>random_from(array{5,10,15})</span>);
    Stmt.bindInt( <span>(3*j)+3</span>, <span>random_number(0,1)</span>);
  }
  Stmt.step(); Stmt.reset();
}
</code></pre>
  </div>

<div>

  <p>Best speeds with mixed data test</p>

  

  <p>20x unrolled inserts of 100M rows</p>

</div>

<p>The best speed for a real database file reported in Avinash&#39;s repo is about
4.3M rows/s with unsafe options. Without any adjustments for our different
hardware, this sounds about right. With worse RNG, or optimized data
generation, our random data test would approach the constant data
version, which also got 4.3M rows/s. But that&#39;s outside the scope of a database
test.</p>

<p>Results show a significant cost imposed by binding dozens of parameters for
each statement. I hoped <a href="#carray">array binding</a> could overcome
this, but the join limitation limits their use.</p>


<h2 id="conclusion">Final thoughts</h2>

<p>The important tips are:</p>

<ul>

  <li><p><b>Use a transaction and parameterized queries</b> for the biggest gain over
    naive usage</p></li>

  <li><p><b>Index after bulk insertion</b> on new tables when possible</p></li>

  <li><p><b>Insert multiple rows per statement</b> if you need to faster bulk inserts</p></li>

  <li><p><b>Don&#39;t change PRAGMAs from default</b> unless you know what you are
    doing; Increasing page cache size can be helpful</p></li>

</ul>

<p>Insert speed games are revealing of database performance characteristics,
  but are themselves impractical. The fastest tests all involve
  insertion into unindexed tables. As soon as indexes are applied, their
  costs dominate.</p>


<p>Rapidly inserting millions of unindexed rows is only useful when later read
sequentially, perhaps as part of a data pipeline. A SQLite format does add some
conveniences for this role, but if you are trying to emit rows as fast as
possible, consider the database only sinks integer rows at a rate of about 40
MiB/s; In comparison the same computer&#39;s unremarkable SSD has a sustained write
rate of 454 MiB/s for regular files.</p>

<p>The disparity is even greater for tests setting records for generating
in-memory SQLite databases. Using SQL to operate on temp data is convenient,
but if you want to append rows to a table in-memory, other tools are faster.
Our test programs start by generating random numbers in an array at a rate of
362 million values per second, or 2.70 GiB/s, with no optimization at all. We
then contrive the fastest possible method for inserting these rows into a
database at a meager 5-10 million per second, and once they&#39;re in they are
extremely slow to operate on.</p>

<p>If you want a fast in-memory table, consider <a href="https://www.boost.org/doc/libs/1_83_0/libs/multi_index/doc/index.html" target="blank">
  Boost.MultiIndex</a>, a C++ template container similar to a database table
  with multiple indexes. If you have several indexes on a table this can be easier
than maintaining your own container with separate lists/sets/maps referring to
  elements in it.</p>




</div>
  </body>
</html>
