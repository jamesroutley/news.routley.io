<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.zettalane.com/blog/openzfs-summit-2025-mayanas-objbacker.html">Original</a>
    <h1>Native ZFS VDEV for Object Storage (OpenZFS Summit)</h1>
    
    <div id="readability-page-1" class="page"><div>

          <p>
            We presented MayaNAS and MayaScale at OpenZFS Developer Summit 2025 in Portland, Oregon. The centerpiece of our presentation: <strong>objbacker.io</strong>—a native ZFS VDEV implementation for object storage that bypasses FUSE entirely, achieving 3.7 GB/s read throughput directly from S3, GCS, and Azure Blob Storage.
          </p>

          <h2 id="introduction">Presenting at OpenZFS Summit</h2>
          <p>
            The OpenZFS Developer Summit brings together the core developers and engineers who build and maintain ZFS across platforms. It was the ideal venue to present our approach to cloud-native storage: using ZFS&#39;s architectural flexibility to create a hybrid storage system that combines the performance of local NVMe with the economics of object storage.
          </p>

          <p>
            Our 50-minute presentation covered the complete Zettalane storage platform—MayaNAS for file storage and MayaScale for block storage—with a deep technical dive into the objbacker.io implementation that makes ZFS on object storage practical for production workloads.
          </p>

          <h2 id="challenge">The Cloud NAS Challenge</h2>
          <p>
            Cloud storage economics present a fundamental problem for NAS deployments:
          </p>

          <div>
            <div>
              <div>
                <h3>$96K/year</h3>
                <p>100TB on EBS (gp3)</p>
              </div>
            </div>
            <div>
              <div>
                <h3>$360K/year</h3>
                <p>100TB on AWS EFS</p>
              </div>
            </div>
          </div>

          <p>
            The insight that drives MayaNAS: not all data needs the same performance tier. Metadata operations require low latency and high IOPS. Large sequential data needs throughput, not IOPS. ZFS&#39;s special device architecture lets us place each workload on the appropriate storage tier.
          </p>

          <p><strong>ZFS Special Device Architecture:</strong> Metadata and small blocks (&lt;128KB) on local NVMe SSD. Large blocks (1MB+) streamed from object storage. One filesystem, two performance tiers, optimal cost.
          </p>

          <h2 id="objbacker">objbacker.io: Native ZFS VDEV for Object Storage</h2>
          <p>
            The traditional approach to ZFS on object storage uses FUSE-based filesystems like s3fs or goofys to mount buckets, then creates ZFS pools on top. This works, but FUSE adds overhead: every I/O crosses the kernel-userspace boundary twice.
          </p>

          <p>
            <strong>objbacker.io takes a different approach.</strong> We implemented a native ZFS VDEV type (<code>VDEV_OBJBACKER</code>) that communicates directly with a userspace daemon via a character device (<code>/dev/zfs_objbacker</code>). The daemon uses native cloud SDKs (AWS SDK, Google Cloud SDK, Azure SDK) for direct object storage access.
          </p>

          <h3>Architecture Comparison</h3>
          <table>
            <thead>
              <tr>
                <th>Approach</th>
                <th>I/O Path</th>
                <th>Overhead</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>FUSE-based (s3fs)</strong></td>
                <td>ZFS → VFS → FUSE → userspace → FUSE → VFS → s3fs → S3</td>
                <td>High (multiple context switches)</td>
              </tr>
              <tr>
                <td><strong>objbacker.io</strong></td>
                <td>ZFS → /dev/zfs_objbacker → objbacker.io → S3 SDK</td>
                <td>Minimal (direct path)</td>
              </tr>
            </tbody>
          </table>

          <h2 id="how-it-works">How objbacker.io Works</h2>
          <p>
            objbacker.io is a Golang program with two interfaces:
          </p>

          <ul>
            <li><strong>Frontend:</strong> ZFS VDEV interface via <code>/dev/zfs_objbacker</code> character device</li>
            <li><strong>Backend:</strong> Native cloud SDK integration for GCS, AWS S3, and Azure Blob Storage</li>
          </ul>

          <h3>ZIO to Object Storage Mapping</h3>
          <table>
            <thead>
              <tr>
                <th>ZFS VDEV I/O</th>
                <th>/dev/objbacker</th>
                <th>Object Storage</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>ZIO_TYPE_WRITE</td>
                <td>WRITE</td>
                <td>PUT object</td>
              </tr>
              <tr>
                <td>ZIO_TYPE_READ</td>
                <td>READ</td>
                <td>GET object</td>
              </tr>
              <tr>
                <td>ZIO_TYPE_TRIM</td>
                <td>UTRIM</td>
                <td>DELETE object</td>
              </tr>
              <tr>
                <td>ZIO_TYPE_IOCTL (sync)</td>
                <td>USYNC</td>
                <td>Flush pending writes</td>
              </tr>
            </tbody>
          </table>

          <h3>Data Alignment</h3>
          <p>
            With ZFS recordsize set to 1MB, each ZFS block maps directly to a single object. Aligned writes go directly as PUT requests without caching. This alignment is critical for performance—object storage performs best with large, aligned operations.
          </p>

          <p><strong>Object Naming:</strong> S3backer-compatible layout. A 5MB file creates 5 objects at offsets 0, 1MB, 2MB, 3MB, 4MB. Object names: <code>bucket/00001</code>, <code>bucket/00002</code>, etc.
          </p>

          <h2 id="performance">Validated Performance Results</h2>
          <p>
            We presented benchmark results from AWS c5n.9xlarge instances (36 vCPUs, 96 GB RAM, 50 Gbps network):
          </p>

          <div>
            <div>
              <div>
                <h3>3.7 GB/s</h3>
                <p>Sequential Read from S3</p>
              </div>
            </div>
            <div>
              <div>
                <h3>2.5 GB/s</h3>
                <p>Sequential Write to S3</p>
              </div>
            </div>
          </div>

          <p>
            The key to this throughput: parallel bucket I/O. With 6 S3 buckets configured as a striped pool, ZFS parallelizes reads and writes across multiple object storage endpoints, saturating the available network bandwidth.
          </p>

          <h3>FIO Test Configuration</h3>
          <table>
            <tbody>
              <tr>
                <td><strong>ZFS Recordsize</strong></td>
                <td>1MB (aligned with object size)</td>
              </tr>
              <tr>
                <td><strong>Block Size</strong></td>
                <td>1MB</td>
              </tr>
              <tr>
                <td><strong>Parallel Jobs</strong></td>
                <td>10 concurrent FIO jobs</td>
              </tr>
              <tr>
                <td><strong>File Size</strong></td>
                <td>10 GB per job (100 GB total)</td>
              </tr>
              <tr>
                <td><strong>I/O Engine</strong></td>
                <td>sync (POSIX synchronous I/O)</td>
              </tr>
            </tbody>
          </table>

          <h2 id="mayascale">MayaScale: High-Performance Block Storage</h2>
          <p>
            We also presented MayaScale, our NVMe-oF block storage solution for workloads requiring sub-millisecond latency. MayaScale uses local NVMe SSDs with Active-Active HA clustering.
          </p>

          <h3>MayaScale Performance Tiers (GCP)</h3>
          <table>
            <thead>
              <tr>
                <th>Tier</th>
                <th>Write IOPS (&lt;1ms)</th>
                <th>Read IOPS (&lt;1ms)</th>
                <th>Best Latency</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Ultra</strong></td>
                <td>585K</td>
                <td>1.1M</td>
                <td>280 us</td>
              </tr>
              <tr>
                <td><strong>High</strong></td>
                <td>290K</td>
                <td>1.02M</td>
                <td>268 us</td>
              </tr>
              <tr>
                <td><strong>Medium</strong></td>
                <td>175K</td>
                <td>650K</td>
                <td>211 us</td>
              </tr>
              <tr>
                <td><strong>Standard</strong></td>
                <td>110K</td>
                <td>340K</td>
                <td>244 us</td>
              </tr>
              <tr>
                <td><strong>Basic</strong></td>
                <td>60K</td>
                <td>120K</td>
                <td>218 us</td>
              </tr>
            </tbody>
          </table>

          <h2 id="multi-cloud">Multi-Cloud Architecture</h2>
          <p>
            Both MayaNAS and MayaScale deploy consistently across AWS, Azure, and GCP. Same Terraform modules, same ZFS configuration, same management interface. Only the cloud-specific networking and storage APIs differ.
          </p>

          <table>
            <thead>
              <tr>
                <th>Component</th>
                <th>AWS</th>
                <th>Azure</th>
                <th>GCP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Instance</strong></td>
                <td>c5.xlarge</td>
                <td>D4s_v4</td>
                <td>n2-standard-4</td>
              </tr>
              <tr>
                <td><strong>Block Storage</strong></td>
                <td>EBS gp3</td>
                <td>Premium SSD</td>
                <td>pd-ssd</td>
              </tr>
              <tr>
                <td><strong>Object Storage</strong></td>
                <td>S3</td>
                <td>Blob Storage</td>
                <td>GCS</td>
              </tr>
              <tr>
                <td><strong>VIP Migration</strong></td>
                <td>ENI attach</td>
                <td>LB health probe</td>
                <td>IP alias</td>
              </tr>
              <tr>
                <td><strong>Deployment</strong></td>
                <td>CloudFormation</td>
                <td>ARM Template</td>
                <td>Terraform</td>
              </tr>
            </tbody>
          </table>

          <h2 id="video">Watch the Full Presentation</h2>
          <p>
            The complete 50-minute presentation is available on the OpenZFS YouTube channel:
          </p>

          <p>
            <iframe src="https://www.youtube.com/embed/8htVeyHkQSM" title="MayaNAS at OpenZFS Developer Summit 2025" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          </p>

          <p>
            <small><strong>Note:</strong> Video will be available once published by OpenZFS. Check the <a href="https://www.youtube.com/@OpenZFS" target="_blank">OpenZFS YouTube channel</a> for the recording.</small>
          </p>

          <h3>Presentation Highlights</h3>
          <ul>
            <li><strong>0:00</strong> - Introduction and Zettalane overview</li>
            <li><strong>5:00</strong> - Zettalane ZFS port architecture (illumos-gate based)</li>
            <li><strong>12:00</strong> - The cloud NAS cost challenge</li>
            <li><strong>18:00</strong> - MayaNAS hybrid architecture with ZFS special devices</li>
            <li><strong>25:00</strong> - objbacker.io deep dive: native VDEV implementation</li>
            <li><strong>35:00</strong> - Performance benchmarks on AWS</li>
            <li><strong>42:00</strong> - MayaScale NVMe-oF block storage</li>
            <li><strong>48:00</strong> - Q&amp;A and future directions</li>
          </ul>

          <h2 id="deployment">Getting Started</h2>
          <p>
            Deploy MayaNAS or MayaScale on your preferred cloud platform:
          </p>

          

          <h2 id="conclusion">Conclusion</h2>
          <p>
            Presenting at OpenZFS Developer Summit 2025 gave us the opportunity to share our approach with the community that makes ZFS possible. The key technical contribution: objbacker.io demonstrates that native ZFS VDEV integration with object storage is practical and performant, achieving 3.7 GB/s throughput without FUSE overhead.
          </p>

          <p>
            MayaNAS with objbacker.io delivers enterprise-grade NAS on object storage with 70%+ cost savings versus traditional cloud block storage. MayaScale provides sub-millisecond block storage with Active-Active HA for latency-sensitive workloads. Together, they cover 90% of enterprise storage needs on any major cloud platform.
          </p>

          <p>
            Special thanks to the OpenZFS community for the foundation that makes this possible.
          </p>

          <p>
            <strong>Ready to deploy cloud-native storage?</strong></p>

        </div></div>
  </body>
</html>
