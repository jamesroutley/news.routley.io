<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/openai/evals">Original</a>
    <h1>Evals: a framework for evaluating OpenAI models and a registry of benchmarks</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.</p>
<p dir="auto">You can use Evals to create and run evaluations that:</p>
<ul dir="auto">
<li>use datasets to generate prompts,</li>
<li>measure the quality of completions provided by an OpenAI model, and</li>
<li>compare performance across different datasets and models.</li>
</ul>
<p dir="auto">With Evals, we aim to make it as simple as possible to build an eval while writing as little code as possible. To get started, we recommend that you follow these steps <strong>in order</strong>:</p>
<ol dir="auto">
<li>Read through this doc and follow the <a href="https://blog.plover.com/openai/evals/blob/main/README.md#Setup">setup instructions below</a>.</li>
<li>Learn how to run existing evals: <a href="https://blog.plover.com/openai/evals/blob/main/docs/run-evals.md">run-evals.md</a>.</li>
<li>Familiarize yourself with the existing eval templates: <a href="https://blog.plover.com/openai/evals/blob/main/docs/eval-templates.md">eval-templates.md</a>.</li>
<li>Walk through the process for building an eval: <a href="https://blog.plover.com/openai/evals/blob/main/docs/build-eval.md">build-eval.md</a></li>
<li>See an example of implementing custom eval logic: <a href="https://blog.plover.com/openai/evals/blob/main/docs/custom-eval.md">custom-eval.md</a>.</li>
</ol>
<p dir="auto">If you think you have an interesting eval, please open a PR with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.</p>
<hr/>
<p dir="auto"><g-emoji alias="rotating_light" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a8.png">üö®</g-emoji> For a limited time, we will be granting GPT-4 access to those who contribute high quality evals. Please follow the instructions mentioned above and note that spam or low quality submissions will be ignored<g-emoji alias="exclamation" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2757.png">‚ùóÔ∏è</g-emoji></p>
<p dir="auto">Access will be granted to the email address associated with an accepted Eval. Due to high volume, we are unable to grant access to any email other than the one used for the pull request.</p>
<hr/>
<h2 tabindex="-1" dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup</h2>
<p dir="auto">To run evals, you will need to set up and specify your OpenAI API key. You can generate one at <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a>. After you obtain an API key, specify it using the <code>OPENAI_API_KEY</code> environment variable. <strong>Please be aware of the <a href="https://openai.com/pricing" rel="nofollow">costs</a> associated with using the API when running evals.</strong></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-downloading-evals" aria-hidden="true" href="#downloading-evals"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Downloading evals</h3>
<p dir="auto">Our Evals registry is stored using <a href="https://git-lfs.com/" rel="nofollow">Git-LFS</a>. Once you have downloaded and installed LFS, you can fetch the evals with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git lfs fetch --all
git lfs pull"><pre>git lfs fetch --all
git lfs pull</pre></div>
<p dir="auto">You may just want to fetch data for a select eval. You can achieve this via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git lfs fetch --include=evals/registry/data/${your eval}
git lfs pull"><pre>git lfs fetch --include=evals/registry/data/<span>${your eval}</span>
git lfs pull</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-making-evals" aria-hidden="true" href="#making-evals"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Making evals</h3>
<p dir="auto">If you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -e ."><pre>pip install -e <span>.</span></pre></div>
<p dir="auto">Using <code>-e</code>, changes you make to your eval will be reflected immediately without having to reinstall.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-running-evals" aria-hidden="true" href="#running-evals"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Running evals</h3>
<p dir="auto">If you don&#39;t want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install evals"><pre>pip install evals</pre></div>
<p dir="auto">We provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the <code>SNOWFLAKE_ACCOUNT</code>, <code>SNOWFLAKE_DATABASE</code>, <code>SNOWFLAKE_USERNAME</code>, and <code>SNOWFLAKE_PASSWORD</code> environment variables.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-faq" aria-hidden="true" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQ</h2>
<p dir="auto">Do you have any examples of how to build an eval from start to finish?</p>
<ul dir="auto">
<li>Yes! These are in the <code>examples</code> folder. We recommend that you also read through <a href="https://blog.plover.com/openai/evals/blob/main/docs/build-eval.md">build-eval.md</a> in order to gain a deeper understanding of what is happening in these examples.</li>
</ul>
<p dir="auto">Do you have any examples of evals implemented in multiple different ways?</p>
<ul dir="auto">
<li>Yes! In particular, see <code>evals/registry/evals/coqa.yaml</code>. We have implemented small subsets of the <a href="https://stanfordnlp.github.io/coqa/" rel="nofollow">CoQA</a> dataset for various eval templates to help illustrate the differences.</li>
</ul>
<p dir="auto">I changed my data but this isn&#39;t reflected when running my eval, what&#39;s going on?</p>
<ul dir="auto">
<li>Your data may have been cached to <code>/tmp/filecache</code>. Try removing this cache and rerunning your eval.</li>
</ul>
<p dir="auto">There&#39;s a lot of code, and I just want to spin up a quick eval. Help? OR,</p>
<p dir="auto">I am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?</p>
<ul dir="auto">
<li>If you follow an existing <a href="https://blog.plover.com/openai/evals/blob/main/docs/eval-templates.md">eval template</a> to build a basic or model-graded eval, you don&#39;t need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. <a href="https://blog.plover.com/openai/evals/blob/main/docs/build-eval.md">build-eval.md</a> walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the <code>examples</code> folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-disclaimer" aria-hidden="true" href="#disclaimer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Disclaimer</h2>
<p dir="auto">By contributing to Evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an Eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI Evals will be subject to our usual Usage Policies: <a href="https://platform.openai.com/docs/usage-policies" rel="nofollow">https://platform.openai.com/docs/usage-policies</a>.</p>
</article>
          </div></div>
  </body>
</html>
