<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/stephansturges/WALDO">Original</a>
    <h1>WALDO: Whereabouts Ascertainment for Low-Lying Detectable Objects</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">W.A.L.D.O.
Whereabouts Ascertainment for Low-lying Detectable Objects</h2><a id="user-content-waldowhereabouts-ascertainment-for-low-lying-detectable-objects" aria-label="Permalink: W.A.L.D.O.
Whereabouts Ascertainment for Low-lying Detectable Objects" href="#waldowhereabouts-ascertainment-for-low-lying-detectable-objects"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://www.youtube.com/watch?v=1caA0ZVPqhA" rel="nofollow"><img src="https://camo.githubusercontent.com/0874c110d856f31f4fb03ec230c65fabb3a26a773bd28fcf7d155256abe36369/68747470733a2f2f692e696d6775722e636f6d2f36597a6d5a35502e706e67" alt="WALDO 2.4 preview vid" data-canonical-src="https://i.imgur.com/6YzmZ5P.png"/></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Welcome to the WALDO v2.5 FINAL release! ü•≥ü•≥ü•≥ü•≥</h2><a id="user-content-welcome-to-the-waldo-v25-final-release-" aria-label="Permalink: Welcome to the WALDO v2.5 FINAL release! ü•≥ü•≥ü•≥ü•≥" href="#welcome-to-the-waldo-v25-final-release-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Thanks to all participants in the beta! I had over 3000 sign-ups for the
beta release and iterated really fast... I hope you&#39;ll like the result!</p>
<p dir="auto">I am assuming you have  some experience with deployment of AI systems,
but if you have any trouble using this release you can contact me at
stephan.sturges at gmail</p>
<hr/>
<p dir="auto">WHAT IS WALDO?</p>
<p dir="auto">WALDO is a detection AI model, based on a large YOLO-v7 backbone and my own
synthetic data pipeline. The basic model shared here, which is the only
one published as FOSS at the moment, is capable of detecting these classes
of items in overhead images ranging in altitude from about 30 feet to
satellite imagery with a resolution of 50cm per pixel or better.</p>
<p dir="auto">Well trained classes:</p>
<ol dir="auto">
<li>&#39;car&#39;  --&gt; all kinds of civilan cars, including pickup trucks</li>
<li>&#39;van&#39; --&gt; all kinds of civilian vans, gets confused with &#34;car&#34; a lot. You might want to fuse them! üöó</li>
<li>&#39;truck&#39; --&gt; all kinds of box-trucks, flatbeds or articulated trucks, NOT small pickup trucks üöö</li>
<li>&#39;building&#39; --&gt; buildings of all kinds üè£</li>
<li>&#39;human&#39; --&gt; people! üßç</li>
<li>&#39;gastank&#39;--&gt; cylindrical tanks such as butane tanks and gas expansion tanks, or grain silos ü´ô</li>
<li>&#39;digger&#39; --&gt; all kinds of construction vehicles, including tractors and construction gear üöú</li>
<li>&#39;container&#39; --&gt; shipping containers, including on the back of an articulated truck</li>
<li>&#39;bus&#39; --&gt; a bus üöå</li>
<li>&#39;u_pole&#39; --&gt; utility poles, power poles, anything thin and sticking up that you should avoid with a plane üéè</li>
<li>&#39;boat&#39; --&gt; boats üö¢</li>
<li>&#39;bike&#39; --&gt; bikes, mopeds, motorbikes, all things with 2 wheels üö≤</li>
<li>&#39;smoke&#39; --&gt; smoke and fire üî•üî•üî•</li>
<li>&#39;solarpanels&#39; --&gt; solar panels</li>
<li>&#39;arm/mil&#39; --&gt; this class detects certain types of armored vehicles (very unreliable for now, don&#39;t use it yet)</li>
<li>&#39;plane&#39; --&gt; planes (very unreliable for now, probably not worth using yet)</li>
</ol>
<hr/>
<p dir="auto">WHERE IS WALDO?</p>
<p dir="auto">Due to the size of the model files and the constraints of github LFS the files
are no longer stored directly on Github, please download the latest package
using the link below:</p>
<p dir="auto"><a href="https://bit.ly/3P7UdZ6" rel="nofollow">https://bit.ly/3P7UdZ6</a></p>
<hr/>
<p dir="auto">FOR AI NERDS !</p>
<p dir="auto">It&#39;s a big set of YOLOv7 model, trained on my own datasets of synthetic and &#34;augmented&#34; / semi-synthetic data.
I&#39;m not going to release the dataset for the time being.</p>
<p dir="auto">The ONNX models are exported for onnx-runtime with a batch-size of 1 and a max input size corresponding to the
the network dimensions. They are also set up to export only the top 200 highest-confidence objects in most cases.</p>
<p dir="auto">I&#39;m planning to set up a way for people to get the .pt files and the ONNX models with unlimited outputs
for people who support further development of the project on Ko-Fi (<a href="https://ko-fi.com/stephansturges" rel="nofollow">https://ko-fi.com/stephansturges</a>), the goal
being to offset some of the cost of training these networks (over 60K USD spent on AWS to date! üòÖ)</p>
<hr/>
<p dir="auto">HOW CAN I START WITH WALDO?</p>
<p dir="auto">Setup the environment with python3:</p>
<ol dir="auto">
<li>(optional) create a virtual python env for the project</li>
<li>install dependencies using the <code>setup.py</code> file: <code>python3 setup.py install</code></li>
</ol>
<p dir="auto">You may need to install a couple of other bits and pieces depending on your python3 env...
If you find anything really blocking send me an email and I&#39;ll update this readme.</p>
<hr/>
<p dir="auto">RUN THE MODELS USING THE BOILERPLATE CODE IN /playground:</p>
<ol dir="auto">
<li>To run on video:
put one or multiple .mp4 files in the ./input_vids subfolder, and then run:</li>
</ol>
<p dir="auto">python3 run_local_onnx_on_videos.py</p>
<p dir="auto">This will run the detection network in default settings and save an annotated video to
the ./output_vids/ subfolder.</p>
<p dir="auto">You can also use the following command-line arguments:</p>
<p dir="auto">python3 run_local_onnx_on_videos.py --frame_limit 3000 --frame_skip 8</p>
<p dir="auto">&#34;frame limit&#34; defines where to stop processing the video, if you only want to test it
on the first 1000 frame then use --frame_limit 1000 for example</p>
<p dir="auto">&#34;frame skip&#34; allows you to skip frames to keep processing quicker for testing, so
if your video is 30 fps and you only one 1 frame per second to be AI-annotated
then you can use --frame_skip 30 for instance</p>
<ol start="2" dir="auto">
<li>To run on a single image of any size:</li>
</ol>
<p dir="auto">Put some images in ./images_in/ and run:</p>
<p dir="auto">python3 run_local_onnx_on_images.py</p>
<p dir="auto">This will run detection on all images in the input folder and save the annotated
output images in the output folder, along with the txt files of the detections
in YOLO format.</p>
<p dir="auto">If the image is LARGER than 960x960px format it will be tiled into squares of 960px with
a litte overlap for analysis and then merged back together, so you can process
huge satellite images for example without needing to split them first.</p>
<p dir="auto">If you want to run the network on a single image that should be processed at native resolution
you can use the &#34;--resize&#34; flag like this:</p>
<p dir="auto">python3 run_local_onnx_on_images.py --resize</p>
<p dir="auto">The output can be found in ./images_out/, you&#39;ll get images with pretty overlays and .txt files
with the actual detections</p>
<hr/>
<p dir="auto">WHAT IS INCLUDED?</p>
<p dir="auto">In the FOSS package there are a bunch of networks in ONNX format prepared for ONNXruntime, as
well as a few examples of networks in other export formats. Only the &#34;V7-base/square/416px&#34;
network is included in all formats as part of this release, meaning you get a selection of
ONNX exported models including some quantized and prepared for Nvidia TensorRT, and you
also have the raw .pt files for the training run so that you can export your own.
I also added the base .pt files for the 512px V7 model.
These files also exist for each other network (or can be exported), but I&#39;m thinking about
how to make those available for people who support the future development of WALDO in order
to support the cost of AI model training (which is over 50K $ already up to this point).
Reach out to me via email if you want a model / export that isn&#39;t in here!</p>
<p dir="auto">/!\ Some tips for use:</p>
<ul dir="auto">
<li>In real-world use cases you may want to merge classes 1 &amp; 2 since there this still
a lot of confusion between those classes</li>
<li>The models are exported with non-maximum-suppression, so if you are using the
AI system in cases where objects are occluded by one another you will only get
the &#34;most valid&#34; object in most cases.</li>
</ul>
<p dir="auto">Some of the network that is in this repo is very large, and is meant to be run on
an inference server, and some are made for embedding on tiny edge devices... take
a look around and find one that works for you!</p>
<hr/>
<p dir="auto">GOING DEEPER</p>
<p dir="auto">Of course if you know your way around deploying AI models there is a lot more you do
with this release, inclusing:</p>
<ol dir="auto">
<li>There are certain models already released in CoreML format for iOS, give those a try</li>
<li>There are some models that are exported for TensorRT, including some cool quantization!</li>
<li>For a couple of models the .pt files are included in this release, play with making
your own exports or running thos directly using YOLOv7 from <a href="https://github.com/WongKinYiu/yolov7">https://github.com/WongKinYiu/yolov7</a></li>
<li>Get yourself a cool, cheap, little AI camera from Luxonis and run one of the OpenVino blobs
that are currently exported for the V7-base/416px network and the V7-tiny/512px network. These
are super cool and do excellent AI detections directly on 15g hardware that costs &lt;200$... crazy stuff.</li>
<li>Build your own commercial application!</li>
</ol>
<p dir="auto">Enjoy!</p>
<hr/>
<p dir="auto">PREVIOUS VERSIONS</p>
<p dir="auto">You can find the repo with WALDO v1.0 here:
<a href="https://github.com/stephansturges/WALDO">https://github.com/stephansturges/WALDO</a></p>
<hr/>
<p dir="auto">CAN YOU HELP ME WITH X?</p>
<p dir="auto">Sure, email me at <a href="mailto:stephan.sturges@gmail.com">stephan.sturges@gmail.com</a></p>
<hr/>
<p dir="auto">DETECTION OF X ISN&#39;T WORKING AS EXPECTED:</p>
<p dir="auto">I&#39;d love to see example images, videos, sample data, etc at:
<a href="mailto:stephan.sturges@gmail.com">stephan.sturges@gmail.com</a></p>
<p dir="auto">HOW DOES AIRCORTEX MAKE MONEY?</p>
<p dir="auto">Aircortex&#39; mission statement is to make the SOTA in ground-risk AI and sensing,
and to make the basic models free and easy to use for both hobbyists and
professionals in the UAV / AAM industry, to acclerate safe access to the skies
in the 21st century.</p>
<p dir="auto">Aircortex is an &#34;open-core&#34; AI company: the basic model is completely
free and open-source for anyone to use including in commercial products.</p>
<p dir="auto">I make money by charging for:</p>
<ol dir="auto">
<li>help with training additional detection classes,</li>
<li>retraining for your specific hardware,</li>
<li>building the software stack to support specific deployment cases,</li>
<li>helping companies set up the right hardware architecture for AI integration,</li>
<li>custom hardware setups for specific environments</li>
<li>more &#34;feature-complete&#34; versions of my FOSS products such as integrating 3D perception
etc...</li>
</ol>
<p dir="auto">Contact me at <a href="mailto:stephan.sturges@gmail.com">stephan.sturges@gmail.com</a> to find out more.</p>
<hr/>
<p dir="auto">SUPPORT WALDO!</p>
<p dir="auto">Training this base model took about 3 months of work and ~20K$ in cloud compute.
If you find value in it, please support development of the next version on:
<a href="https://ko-fi.com/stephansturges" rel="nofollow">https://ko-fi.com/stephansturges</a></p>
<p dir="auto">You can also sign-up there to be a sponsor of WALDO for 500$ / month and get
early access to future models.</p>
<hr/>
<p dir="auto">/_<em><strong>//_</strong></em>//_<em><strong>//_</strong></em>//_<em><strong>//_</strong></em>//_<em><strong>//_</strong></em>//_<em><strong>//_</strong>
/</em><em><strong>//</strong></em><em>//</em><em><strong>//</strong></em><em>//</em><em><strong>//</strong></em><em>//</em><em><strong>//</strong></em><em>//</em><em><strong>//</strong></em></p>

<div dir="auto"><h2 tabindex="-1" dir="auto">Unless otherwise specified all code in this release is published with the
licence conditions below.</h2><a id="user-content-unless-otherwise-specified-all-code-in-this-release-is-published-with-thelicence-conditions-below" aria-label="Permalink: Unless otherwise specified all code in this release is published with the
licence conditions below." href="#unless-otherwise-specified-all-code-in-this-release-is-published-with-thelicence-conditions-below"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">MIT License</p>
<p dir="auto">Copyright (c) 2023 Stephan Sturges / Aircortex.com</p>
<p dir="auto">Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &#34;Software&#34;), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:</p>
<p dir="auto">The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.</p>
<p dir="auto">THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.</p>
</article></div></div>
  </body>
</html>
