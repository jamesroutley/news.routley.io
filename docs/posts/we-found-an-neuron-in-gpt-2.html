<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://clementneo.com/posts/2023/02/11/we-found-an-neuron">Original</a>
    <h1>We Found an Neuron in GPT-2</h1>
    
    <div id="readability-page-1" class="page"><section><blockquote><p>Written in collaboration with Joseph Miller. See the discussion of this post over on <a href="https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2">LessWrong</a>.</p></blockquote><p>We started out with the question: How does GPT-2 know when to use the word <code>an</code> over <code>a</code>? The choice depends on whether the word that comes after starts with a vowel or not, but GPT-2 is only capable of predicting one word at a time.</p><p>We still don’t have a full answer, but we did find a single MLP neuron in GPT-2 Large that is crucial for predicting the token “ an”. And we also found that the weights of this neuron correspond with the embedding of the “ an” token, which led us to find other neurons that predict a specific token.</p><h2 id="discovering-the-neuron">Discovering the neuron</h2><h3 id="choosing-the-prompt">Choosing the prompt</h3><p>It was surprisingly hard to think of a prompt where GPT-2 would output the token <code>“ an”</code> (the leading space is part of the token) as the top prediction. In fact, we gave up with <code>GPT-2_small</code> and switched to GPT-2_large. As we’ll see later, even <code>GPT-2_large</code> systematically under-predicts <code>“ an”</code> in favor of <code>“ a”</code>. This may be because smaller language models lean on the higher frequency of a to make a best guess. The prompt we finally found that gave a high (64%) probability for “ an” was:</p><blockquote><p><em>“I climbed up the pear tree and picked a pear. I climbed up the apple tree and picked”</em></p></blockquote><p>The first sentence was necessary to push the model towards an indefinite article — without it the model would make other predictions such as <em>“[picked] up”</em>.</p><p>Before we proceed, here’s a quick overview on the <a href="https://transformer-circuits.pub/2021/framework/index.html">transformer architecture</a>. Each attention block and MLP takes inputs and adds outputs to the residual stream.</p><p><img src="https://leourbina.substack.com/assets/images/anneuron/transformer-architecture.png"/></p><h3 id="logit-lens">Logit Lens</h3><p>Using a technique known as <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">logit lens</a>, we took the logits from the residual stream between each layer and plotted the difference between<code> logit(‘ an’)</code> and <code>logit(‘ a’)</code>. We found a big spike after Layer 31’s MLP.</p><meta charset="utf-8"/><h3 id="activation-patching-by-the-layer">Activation Patching by the Layer</h3><p>Activation patching is a technique introduced by <a href="https://arxiv.org/abs/2202.05262">Meng et. al. (2022)</a> to analyze the significance of a single layer in a transformer. First, we saved the activation of each layer when running the original prompt through the model — the “clean activation”.</p><p>We then ran a <strong>corrupted</strong> prompt through the model: <em>“I climbed up the pear tree and picked a pear. I climbed up the <strong>lemon</strong> tree and picked”</em>. By replacing the word ‘apple’ with ‘lemon’, we induce the model to predict the token ‘ a’ instead of ‘ an’.</p><p>With the model predicting <code>&#34; a&#34;</code> over <code>&#34; an&#34;</code>, we can replace a layer’s corrupted activation with its clean activation to see how much the model shifts towards the <code>&#34; an&#34;</code> token, which indicates that layer’s significance to predicting <code>&#34; an&#34;</code>. We repeat this process over all the layers of the model.</p><meta charset="utf-8"/><meta charset="utf-8"/><p>We’re mostly going to ignore attention for the rest of this post, but these results indicate that Layer 26 is where <code>&#34; picked&#34;</code> starts thinking a lot about <code>&#34; apple&#34;</code>, which is obviously required to predict <code>&#34; an&#34;</code>.</p><meta charset="utf-8"/><p>The two MLP layers that stand out are Layer 0 and Layer 31. We already know that Layer 0’s MLP is generally important for GPT-2 to function<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup> (although we’re not sure why attention in Layer 0 is important). The effect of Layer 31 is more interesting. Our results suggests that Layer 31’s MLP plays a significant role in predicting the ‘ an’ token. (See <a href="https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2?commentId=FLpxtfnwnMjZwXv3B#comments">this comment</a> if you’re confused how this result fits with the logit lens above.)</p><h2 id="finding-1-we-can-discover-predictive-neurons-by-activation-patching-individual-neurons">Finding 1: We can discover predictive neurons by activation patching individual neurons</h2><p>Activation patching has been used to investigate transformers by the layer, but can we push this technique further and apply it to individual neurons? Since each MLP in a transformer only has one hidden layer, each neuron’s activation does not affect any other neuron in the MLP. So we should be able to patch individual neurons, because they are independent from each other in the same sense that the attention heads in a single layer are independent from each other.</p><p>We run neuron-wise activation patching for Layer 31’s MLP in a similar fashion to the layer-wise patching above. We reintroduce the clean activation of each neuron in the MLP when running the corrupted prompt through the model, and look at how much restoring each neuron contributes to the logit difference between <code>&#34; a&#34;</code> and <code>&#34; an&#34;</code>.</p><meta charset="utf-8"/><p>We see that patching Neuron 892 recovers 50% of the clean prompt’s logit difference, while patching whole layer actually does worse at 49%.</p><h2 id="finding-2-the-activation-of-the-an-neuron-correlates-with-the--an-token-being-predicted">Finding 2: The activation of the “an-neuron” correlates with the “ an” token being predicted.</h2><h3 id="neuroscope-layer-31-neuron-892-maximum-activating-examples">Neuroscope <a href="https://neuroscope.io/gpt2-large/31/892.html">Layer 31 Neuron 892 Maximum Activating Examples</a></h3><p><img src="https://leourbina.substack.com/assets/images/anneuron/neuroscope_an.png" alt="Neuroscope&#39;s An"/> Neuroscope is an online tool that shows the top activating examples in a large dataset for each neuron in GPT-2. When we look at Layer 31 Neuron 892, we see that the neuron maximally activates on tokens where the subsequent token is <code>&#34; an&#34;</code>.</p><p>But Neuroscope only shows us the top 20 most activating examples. Would there be a trend for a wider range of activations?</p><h3 id="testing-the-neuron-on-a-larger-dataset">Testing the neuron on a larger dataset</h3><p>To check for a trend, we ran the <a href="https://huggingface.co/datasets/NeelNanda/pile-10k/tree/main/data">pile-10k</a> dataset through the model. This is a diverse set of about 10 million tokens taken from <a href="https://pile.eleuther.ai/">The Pile</a>, split into prompts of 1,024 tokens. We plotted the proportion of <code>&#34; an&#34;</code> predictions across the range of neuron activations:</p><meta charset="utf-8"/><p>We see that the proportion of <code>&#34; an&#34;</code> predictions increases as the neuron’s activation increases, to the point where <code>&#34; an&#34;</code> is always the top prediction. The trend is somewhat noisy, which suggests that there might be other mechanisms in the model that also contribute towards the ‘ an’ prediction. Or maybe when the <code>&#34; an&#34;</code> logit increases, other logits increase at the time.</p><p>Note that the model only predicted “ an” 1,500 times, even though it actually occurred 12,000 times in the dataset. No wonder it was so hard to find a good prompt!</p><h3 id="the-neurons-output-weights-have-a-high-dot-product-with-the--an-token">The neuron’s output weights have a high dot-product with the “ an” token</h3><p>How does the neuron influence the model’s output? Well, the neuron’s output weights have a high dot product with the embedding for the token “ an”. We call this the <strong>congruence</strong> of the neuron with the token. Compared to other random tokens like <code>&#34; any&#34;</code> and <code>&#34; had&#34;</code>, the neuron’s congruence with “ an” is very high:</p><p><img src="https://leourbina.substack.com/assets/images/anneuron/congruence_illustration.png" alt="Congruence Illustration"/></p><p>In fact, when we calculate the neuron’s congruence with all of the tokens, there are a few clear outliers:</p><meta charset="utf-8"/><p>It seems like the neuron basically adds the embedding of <code>“ an”</code> to the residual stream, which increases the output probability for “ an” since the unembedding step consists of taking the dot product of the final residual with each token.</p><p>Are there other neurons that are also congruent to <code>“ an”</code>? To find out, we can calculate the congruence of all neurons with the <code>“ an”</code> token:</p><meta charset="utf-8"/><p>Our neuron is way above the rest, but there are other neurons with a fairly high congruence with the <code>&#34; an&#34;</code> token. These other neurons could be part of the reason why the correlation between the an-neuron’s activation and the prediction of the <code>&#34; an&#34;</code> token isn’t perfect: there may be prompts where <code>&#34; an&#34;</code> is predicted, but the model uses these other neurons to do it.</p><p>If this is the case, could we use congruence to find a neuron that is perfectly correlated with a single token prediction?</p><h2 id="finding-3-we-can-use-the-neurons-output-congruence-to-find-specific-neurons-that-predict-a-token">Finding 3: We can use the neurons’ output congruence to find specific neurons that predict a token</h2><h3 id="finding-a-token-associated-neuron">Finding a token-associated neuron</h3><p>We can try to find a neuron that is associated with a specific token by running the following search:</p><ol><li>For each token, find the neuron with the highest output congruence.</li><li>For each of these congruent neurons, find how much more congruent they are as compared to the next most congruent neuron for the same token.</li><li>Take the neuron(s) that are the most exclusively congruent.</li></ol><p>With this search, we wanted to find neurons that were uniquely responsible for a token. Our conjecture was that with a neuron that was mostly responsible for a token, its activation would be more correlated with the token’s prediction, since any prediction of that token would “rely” on that neuron.</p><p>Let’s run the search and plot the graph of the most congruent neurons for each token:</p><meta charset="utf-8"/><p>With this search, we see that for tokens like “ off” and “ though”, there are neurons that stand out in their congruence. Let’s try running the “ though” neuron — Layer 28 Neuron 1921 — through the dataset and see whether we get a cleaner graph!</p><meta charset="utf-8"/><p>Woah, that is much messier than the graph for the an-neuron. What is going on?</p><p>Looking at <a href="https://neuroscope.io/gpt2-large/28/1921.html">Neuroscope</a>’s data for the neuron reveals that the max activating neuron predicts both the tokens <code>“ though”</code> and <code>“ however”</code>. This complicates things — it seems that this neuron is correlated with a group of semantically similar tokens (<a href="https://en.wikipedia.org/wiki/Conjunctive_adverb">conjunctive adverbs</a>)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">3</a></sup></p><p><img src="https://leourbina.substack.com/assets/images/anneuron/neuroscope_though.png" alt="Though Neuroscope"/></p><p>When we calculate the neuron’s congruence for all tokens, we find that the same tokens pop up as outliers:</p><meta charset="utf-8"/><p>In our large dataset correlation graph above, instances where the neuron activates and <code>&#34; however&#34;</code> is predicted over <code>&#34; though&#34;</code> would be counted as negative examples, since “ though” was not the top prediction. This could also explain some of the noise in the <code>&#34; an&#34;</code> correlation, where the neuron is also congruent with <code>&#34;An&#34;</code>, <code>&#34; An&#34;</code> and <code>&#34;an&#34;</code><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">4</a></sup>.</p><p>Can we find a better neuron to look at — preferably a neuron that only predicts for one token?</p><h3 id="finding-a-cleanly-associated-neuron">Finding a cleanly associated neuron</h3><p>For a neuron to be ‘cleanly associated’ with a token, their congruence with each other should be <em>mutually exclusive</em>, meaning:</p><ol><li>The neuron is much more congruent with the token than any other neuron.</li><li>The neuron is much more congruent with the token than any other token.</li></ol><p>(Remember, congruence is just the dot product.)</p><p>Both criteria help to simplify the relationship between the neuron and its token. If a neuron’s congruence with a token is a representation of how much it contributes to that token’s prediction, the first criteria can be seen as making sure that <strong>only this neuron</strong> is responsible for predicting that token, while the second criteria can be seen as making sure that this neuron is responsible for predicting <strong>only that token</strong>.</p><p>Our search then is as follows:</p><ol><li>For each token, find the most congruent neuron.</li><li>For each neuron, find the most congruent token<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">5</a></sup>.</li><li>Find the token-neuron pairs that are on both lists — that is, the pairs where the neuron’s most congruent token is a token which is most congruent with that neuron!</li><li>Calculate how distinct they are by multiplying their top 2 token congruence difference with their top 2 neuron congruence difference.</li><li>Find the pairs with the highest mutual exclusive congruence.</li></ol><meta charset="utf-8"/><p>For <code>GPT-2_large</code>, Layer 33 Neuron 4142 paired with <code>&#34;i&#34;</code> scores the highest on this metric. Looking at Neuroscope<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">6</a></sup> confirms the connection:</p><p><img src="https://leourbina.substack.com/assets/images/anneuron/neuroscope_i.png" alt="Neuroscope i"/></p><p>And when we plot the graph of top prediction proportion over activation for the top 5 highest scorers<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">7</a></sup>:</p><meta charset="utf-8"/><p>We see that we do indeed get a smooth correlations for each pair!</p><h2 id="what-does-this-all-mean">What Does This All Mean?</h2><p>Does the congruence of a neuron with a token actually measure the extent to which the neuron predicts that token? We don’t know. There could be several reasons why even token-neuron pairs with high mutually exclusive congruence may not always correlate:</p><ul><li>The token could be also predicted by a combination of less congruent neurons</li><li>The token could be predicted by attention heads</li><li>Even if a neuron’s activation has a high correlation with a token’s logit, it may also indirectly correlate with other token’s logits, such that the neuron activation does not correlate with the token’s probability.</li><li>There may be later layers which add the opposite direction to the residual stream, cancelling the effect of this neuron.</li></ul><p>However, we’ve found that the token neuron pair with the highest mutually exclusive congruence (the “i” and the “i-neuron”) does in fact have a strong correlation. We haven’t tested any others pairs yet but we expect that many others pairs that score high on this metric will also correlate.</p><h2 id="tldr">TL;DR</h2><ol><li>We used activation patching on a neuron level to find a neuron that’s important for predicting the token <code>&#34; an&#34;</code> in a specific prompt.</li><li>The “an-neuron” activation actually correlates with <code>&#34; an&#34;</code> being predicted in general.</li><li>This may be because the neuron’s output weights have a high dot product with the <code>&#34; an&#34;</code> token (the neuron is highly <em>congruent</em> with the token). Moreover this neuron has a higher dot product with this token than any other token. <strong>And</strong> this neuron has a higher dot product with this token than the token has with any other neuron (they have high mutual exclusive congruence).</li><li>The congruence between a neuron and a token is cool. We find the “i” neuron-token pair which has the highest <em>mutual exclusive congruence</em> of any token-neuron pair. The activation of this neuron is strongly correlated with the <code>&#34;i&#34;</code> token being predicted.</li></ol><p>The code to reproduce our results can be found <a href="https://github.com/UFO-101/an-neuron">here</a>.</p><p>This is a write-up and extension of our winning submission to <a href="https://apartresearch.com/">Apart Research</a>’s <a href="https://itch.io/jam/mechint">Mechanistic Interpretability Hackathon</a>. Thanks to the London EA Hub for letting us use their co-working space, <a href="https://fbarez.github.io/">Fazl Barez</a> for his comments and <a href="https://www.neelnanda.io/">Neel Nanda</a> for his feedback and for creating <a href="https://neuroscope.io/">Neuroscope</a>, the <a href="https://huggingface.co/datasets/NeelNanda/pile-10k">pile-10k</a> dataset and <a href="https://github.com/neelnanda-io/TransformerLens">TransformerLens</a>.</p><span><time datetime="2023-02-11T00:00:00+00:00">February 11, 2023</time> <!--<span class="meta"><time datetime="2023-02-11T00:00:00+00:00">February 11, 2023</time> · </span> --></span></section></div>
  </body>
</html>
