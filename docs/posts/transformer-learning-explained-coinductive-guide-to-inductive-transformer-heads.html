<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2302.01834">Original</a>
    <h1>Transformer learning explained: Coinductive guide to inductive transformer heads</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      
      <div id="content">
        <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2302.01834"
        dc:identifier="/abs/2302.01834"
        dc:title="Coinductive guide to inductive transformer heads"
        trackback:ping="/trackback/2302.01834" />
    </rdf:RDF>
-->
<div id="abs-outer">
  

  <div>
    

    <p><strong>arXiv:2302.01834</strong> (cs)
    </p>
    



<div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2302.01834">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  We argue that all building blocks of transformer models can be expressed with
a single concept: combinatorial Hopf algebra. Transformer learning emerges as a
result of the subtle interplay between the algebraic and coalgebraic operations
of the combinatorial Hopf algebra. Viewed through this lens, the transformer
model becomes a linear time-invariant system where the attention mechanism
computes a generalized convolution transform and the residual stream serves as
a unit impulse. Attention-only transformers then learn by enforcing an
invariant between these two paths. We call this invariant Hopf coherence. Due
to this, with a degree of poetic license, one could call combinatorial Hopf
algebras &#34;tensors with a built-in loss function gradient&#34;. This loss function
gradient occurs within the single layers and no backward pass is needed. This
is in contrast to automatic differentiation which happens across the whole
graph and needs a explicit backward pass. This property is the result of the
fact that combinatorial Hopf algebras have the surprising property of
calculating eigenvalues by repeated squaring.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div>
    <div>
      <h2>Submission history</h2><p> From: Adam Nemecek [<a href="https://arxiv.org/show-email/bee97476/2302.01834">view email</a>]
      </p></div>
  </div>
  <!--end leftcolumn-->
  
  <!--end extra-services-->

  <!-- LABS AREA -->
<div id="labstabs">
  <div><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      
      <div>
        
        <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
      </div>
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p>


      <p>
      <label for="tabfour">Related Papers</label></p><div>
        
        <div>
          
          
          <div>
            <p><label>
                
                <span></span>
                <span>IArxiv recommender toggle</span>
              </label>
            </p>
            
          </div>
          </div>
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
        <div>
          <div>
            
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv&#39;s community? <a href="https://labs.arxiv.org/"><strong>Learn more about arXivLabs</strong></a> and <a href="https://arxiv.org/about/people/developers"><strong>how to get involved</strong></a>.</p>
          </div>
          
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->

  
  
</div>

      </div>
    </div></div>
  </body>
</html>
