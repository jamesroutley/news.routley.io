<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.norvig.com/spell-correct.html">Original</a>
    <h1>How to Write a Spelling Corrector (2007)</h1>
    
    <div id="readability-page-1" class="page">
<p><i>Feb 2007</i></p>



One week in 2007, two friends (Dean and Bill) independently told me
they were amazed at Google&#39;s  spelling correction.  Type in a search like <a href="http://www.google.com/search?q=speling">[speling]</a> and Google
instantly comes back with <b>Showing results for:
<i><a href="http://www.google.com/search?q=spelling">spelling</a></i></b>.
I thought Dean and Bill, being highly
accomplished engineers and mathematicians, would have good intuitions
about how this process works.  But they didn&#39;t, and come to think of it, why should they
know about something so far outisde their specialty?

<p>
I figured they, and others, could benefit from an explanation.  The
full details of an industrial-strength spell corrector are quite complex (you
can read a little about it <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/36180.pdf">here</a> or <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=52A3B869596656C9DA285DCE83A0339F?doi=10.1.1.146.4390&amp;rep=rep1&amp;type=pdf">here</a>).
But I figured that in the course of a transcontinental plane ride I could write and explain a toy
spelling corrector that achieves 80 or 90% accuracy at a processing
speed of at least 10 words per second in about half a page of code.


</p><p>And here it is (or see <a href="http://www.norvig.com/spell.py">spell.py</a>):

</p><pre>import re
from collections import Counter

def words(text): return re.findall(r&#39;\w+&#39;, text.lower())

WORDS = Counter(words(open(&#39;big.txt&#39;).read()))

def P(word, N=sum(WORDS.values())): 
    &#34;Probability of `word`.&#34;
    return WORDS[word] / N

def correction(word): 
    &#34;Most probable spelling correction for word.&#34;
    return max(candidates(word), key=P)

def candidates(word): 
    &#34;Generate possible spelling corrections for word.&#34;
    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])

def known(words): 
    &#34;The subset of `words` that appear in the dictionary of WORDS.&#34;
    return set(w for w in words if w in WORDS)

def edits1(word):
    &#34;All edits that are one edit away from `word`.&#34;
    letters    = &#39;abcdefghijklmnopqrstuvwxyz&#39;
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    deletes    = [L + R[1:]               for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1]
    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
    inserts    = [L + c + R               for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)

def edits2(word): 
    &#34;All edits that are two edits away from `word`.&#34;
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))
</pre>
<p>
The  function <tt>correction(word)</tt>  returns
a likely spelling correction:
</p><pre>&gt;&gt;&gt; correction(&#39;speling&#39;)
&#39;spelling&#39;

&gt;&gt;&gt; correction(&#39;korrectud&#39;)
&#39;corrected&#39;
</pre>


<h2>How It Works: Some Probability Theory</h2>

<p>The call <tt>correction(w)</tt>
tries to choose the most likely spelling correction for <tt>w</tt>. There is no way to
know for sure (for example, should &#34;lates&#34; be corrected to &#34;late&#34; or
&#34;latest&#34; or &#34;lattes&#34; or ...?), which suggests we use probabilities.  We 
are trying to find the correction <i>c</i>, out of all possible candidate
corrections, that maximizes the probability that <i>c</i> is the intended correction, given the
original word <i>w</i>:
</p><blockquote>
argmax<sub><i>c âˆˆ candidates</i></sub> P(<i>c</i>|<i>w</i>)
</blockquote>
By <a href="http://en.wikipedia.org/wiki/Bayes&#39;_theorem">Bayes&#39; Theorem</a> this is equivalent 
to:
<blockquote>
argmax<sub><i>c âˆˆ candidates</i></sub> P(<i>c</i>) P(<i>w</i>|<i>c</i>)  / P(<i>w</i>)
</blockquote>
Since P(<i>w</i>) is the same for every possible candidate <i>c</i>, we can factor it out, giving:
<blockquote>
argmax<sub><i>c âˆˆ candidates</i></sub> P(<i>c</i>) P(<i>w</i>|<i>c</i>) 
</blockquote>

The four parts of this expression are:

<ol>
  <li><b>Selection Mechanism</b>: argmax </li><li> <b>Candidate Model</b>: <i>c âˆˆ candidates</i></li><li> <b>Language Model</b>: P(<i>c</i>)
  </li><li> <b>Error Model</b>: P(<i>w</i>|<i>c</i>)</li></ol>

<p>One obvious question is: why take a simple expression like P(<i>c</i>|<i>w</i>) and replace
  it with a more complex expression involving two models rather than one? The answer is that
  P(<i>c</i>|<i>w</i>) is <i>already</i> conflating two factors, and it is
  easier to separate the two out and deal with them explicitly. Consider the misspelled word
  <i>w</i>=&#34;thew&#34; and the two candidate corrections <i>c</i>=&#34;the&#34; and <i>c</i>=&#34;thaw&#34;.
  Which has a higher P(<i>c</i>|<i>w</i>)?  Well, &#34;thaw&#34; seems good because the only change
  is &#34;a&#34; to &#34;e&#34;, which is a small change.  On the other hand, &#34;the&#34; seems good because &#34;the&#34; is a very
  common word, and while adding a &#34;w&#34; seems like a larger, less probable change, perhaps the typist&#39;s finger slipped off the &#34;e&#34;.  The point is that to
  estimate P(<i>c</i>|<i>w</i>) we have to consider both the probability of <i>c</i> and the
  probability of the change from <i>c</i> to <i>w</i> anyway, so it is cleaner to formally separate the
  two factors.

</p><h2>How It Works: Some Python</h2>

The four parts of the program are:

<ol>
<li><b>Selection Mechanism</b>: In Python, <tt>max</tt> with a <tt>key</tt> argument does &#39;argmax&#39;.

</li><li> <b>Candidate Model</b>: 
First a new concept: a <b>simple edit</b> to a word is a deletion (remove one letter), a transposition (swap two adjacent letters),
a replacement (change one letter to another) or an insertion (add a letter).  The function
<tt>edits1</tt> returns a set of all the edited strings (whether words or not) that can be made with one simple edit:

<pre>def edits1(word):
    &#34;All edits that are one edit away from `word`.&#34;
    letters    = &#39;abcdefghijklmnopqrstuvwxyz&#39;
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    deletes    = [L + R[1:]               for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)&gt;1]
    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
    inserts    = [L + c + R               for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)
</pre>

<p>This can be a big set.  For a word of length <i>n</i>, there will
be <i>n</i> deletions, <i>n</i>-1 transpositions, 26<i>n</i>
alterations, and 26(<i>n</i>+1) insertions, for a total of
54<i>n</i>+25 (of which a few are typically duplicates).  For example,

</p><pre>&gt;&gt;&gt; len(edits1(&#39;somthing&#39;))
442
</pre>

However, if we restrict ourselves to words that are <i>known</i>â€”that is, 
in the dictionaryâ€” then the set is much smaller:

<pre>def known(words): return set(w for w in words if w in WORDS)

&gt;&gt;&gt; known(edits1(&#39;somthing&#39;))
{&#39;something&#39;, &#39;soothing&#39;}
</pre>

We&#39;ll also consider corrections that require <i>two</i> simple edits. This generates a much bigger set of
possibilities, but usually only a few of them are known words:

<pre>def edits2(word): return (e2 for e1 in edits1(word) for e2 in edits1(e1))

&gt;&gt;&gt; len(set(edits2(&#39;something&#39;))
90902

&gt;&gt;&gt; known(edits2(&#39;something&#39;))
{&#39;seething&#39;, &#39;smoothing&#39;, &#39;something&#39;, &#39;soothing&#39;}

&gt;&gt;&gt; known(edits2(&#39;somthing&#39;))
{&#39;loathing&#39;, &#39;nothing&#39;, &#39;scathing&#39;, &#39;seething&#39;, &#39;smoothing&#39;, &#39;something&#39;, &#39;soothing&#39;, &#39;sorting&#39;}
</pre>

We say that the results of <tt>edits2(w)</tt> have an <b>edit distance</b> of 2 from <tt>w</tt>.

</li><li><b>Language Model</b>: 
We  can estimate the probability of a word, <tt>P(word)</tt>, by counting
the number of times each word appears in a text file of about a million words, <a href="http://www.norvig.com/big.txt"><tt>big.txt</tt></a>.
  It is a concatenation of public domain book excerpts from <a href="http://www.gutenberg.org/wiki/Main_Page">Project Gutenberg</a>
  and lists of most frequent words from <a href="http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists">Wiktionary</a>
  and the <a href="http://www.kilgarriff.co.uk/bnc-readme.html">British
  National Corpus</a>. The function <tt>words</tt> breaks text into words, then the
  variable <tt>WORDS</tt> holds a Counter of how often each word appears, and <tt>P</tt>
  estimates the probability of each word, based on this Counter:


<pre>def words(text): return re.findall(r&#39;\w+&#39;, text.lower())

WORDS = Counter(words(open(&#39;big.txt&#39;).read()))

def P(word, N=sum(WORDS.values())): return WORDS[word] / N
</pre>

We can see that there are 32,192 distinct words, which together appear 1,115,504 times, with &#39;the&#39; being the most common word, appearing 79,808 times
(or a probability of about 7%) and other words being less probable:

<pre>&gt;&gt;&gt; len(WORDS)
32192

&gt;&gt;&gt; sum(WORDS.values())
1115504

&gt;&gt;&gt; WORDS.most_common(10)
[(&#39;the&#39;, 79808),
 (&#39;of&#39;, 40024),
 (&#39;and&#39;, 38311),
 (&#39;to&#39;, 28765),
 (&#39;in&#39;, 22020),
 (&#39;a&#39;, 21124),
 (&#39;that&#39;, 12512),
 (&#39;he&#39;, 12401),
 (&#39;was&#39;, 11410),
 (&#39;it&#39;, 10681),
 (&#39;his&#39;, 10034),
 (&#39;is&#39;, 9773),
 (&#39;with&#39;, 9739),
 (&#39;as&#39;, 8064),
 (&#39;i&#39;, 7679),
 (&#39;had&#39;, 7383),
 (&#39;for&#39;, 6938),
 (&#39;at&#39;, 6789),
 (&#39;by&#39;, 6735),
 (&#39;on&#39;, 6639)]

&gt;&gt;&gt; max(WORDS, key=P)
&#39;the&#39;

&gt;&gt;&gt; P(&#39;the&#39;)
0.07154434228832886

&gt;&gt;&gt; P(&#39;outrivaled&#39;)
8.9645577245801e-07

&gt;&gt;&gt; P(&#39;unmentioned&#39;)
0.0
</pre>




</li><li><b>Error Model</b>:
When I started
to write this program, sitting on
a plane in 2007, I had no data on spelling errors, and no internet connection (I know
that may be hard to imagine today). Without data I couldn&#39;t build a good spelling error model, so I
took a shortcut: I defined a trivial, flawed error model that says all known words
of edit distance 1 are infinitely more probable than known words of
edit distance 2, and infinitely less probable than a known word of
edit distance 0. So we can make <tt>candidates(word)</tt> produce the first non-empty list of candidates 
in order of priority:
<ol>
<li> The original word, if it is known; otherwise
</li><li> The list of known words at edit distance one away, if there are any; otherwise
</li><li> The list of known words at edit distance two away, if there are any; otherwise
</li><li> The original word, even though it is not known.
</li></ol>
Then we don&#39;t need to multiply by a P(<i>w</i>|<i>c</i>) factor, because every candidate
at the chosen priority will have the same probability (according to our flawed model). That gives us:

<pre>def correction(word): return max(candidates(word), key=P)

def candidates(word): 
    return known([word]) or known(edits1(word)) or known(edits2(word)) or [word]
</pre>

</li></ol>



<h2>Evaluation</h2>

Now it is time to evaluate how well this program does. After my plane landed, I
downloaded Roger Mitton&#39;s <a href="http://ota.ahds.ac.uk/texts/0643.html">Birkbeck spelling error
corpus</a> from the Oxford Text Archive. From that I extracted two
test sets of corrections. The first is for development, meaning I get
to look at it while I&#39;m developing the program. The second is a final
test set, meaning I&#39;m not allowed to look at it, nor change my program
after evaluating on it. This practice of having two sets is good
hygiene; it keeps me from fooling myself into thinking I&#39;m doing
better than I am by tuning the program to one specific set of
tests. I also wrote some unit tests:

<pre>def unit_tests():
    assert correction(&#39;speling&#39;) == &#39;spelling&#39;              # insert
    assert correction(&#39;korrectud&#39;) == &#39;corrected&#39;           # replace 2
    assert correction(&#39;bycycle&#39;) == &#39;bicycle&#39;               # replace
    assert correction(&#39;inconvient&#39;) == &#39;inconvenient&#39;       # insert 2
    assert correction(&#39;arrainged&#39;) == &#39;arranged&#39;            # delete
    assert correction(&#39;peotry&#39;) ==&#39;poetry&#39;                  # transpose
    assert correction(&#39;peotryy&#39;) ==&#39;poetry&#39;                 # transpose + delete
    assert correction(&#39;word&#39;) == &#39;word&#39;                     # known
    assert correction(&#39;quintessential&#39;) == &#39;quintessential&#39; # unknown
    assert words(&#39;This is a TEST.&#39;) == [&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;]
    assert Counter(words(&#39;This is a test. 123; A TEST this is.&#39;)) == (
           Counter({&#39;123&#39;: 1, &#39;a&#39;: 2, &#39;is&#39;: 2, &#39;test&#39;: 2, &#39;this&#39;: 2}))
    assert len(WORDS) == 32192
    assert sum(WORDS.values()) == 1115504
    assert WORDS.most_common(10) == [
     (&#39;the&#39;, 79808),
     (&#39;of&#39;, 40024),
     (&#39;and&#39;, 38311),
     (&#39;to&#39;, 28765),
     (&#39;in&#39;, 22020),
     (&#39;a&#39;, 21124),
     (&#39;that&#39;, 12512),
     (&#39;he&#39;, 12401),
     (&#39;was&#39;, 11410),
     (&#39;it&#39;, 10681)]
    assert WORDS[&#39;the&#39;] == 79808
    assert P(&#39;quintessential&#39;) == 0
    assert 0.07 &lt; P(&#39;the&#39;) &lt; 0.08
    return &#39;unit_tests pass&#39;

def spelltest(tests, verbose=False):
    &#34;Run correction(wrong) on all (right, wrong) pairs; report results.&#34;
    import time
    start = time.clock()
    good, unknown = 0, 0
    n = len(tests)
    for right, wrong in tests:
        w = correction(wrong)
        good += (w == right)
        if w != right:
            unknown += (right not in WORDS)
            if verbose:
                print(&#39;correction({}) =&gt; {} ({}); expected {} ({})&#39;
                      .format(wrong, w, WORDS[w], right, WORDS[right]))
    dt = time.clock() - start
    print(&#39;{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second &#39;
          .format(good / n, n, unknown / n, n / dt))
    
def Testset(lines):
    &#34;Parse &#39;right: wrong1 wrong2&#39; lines into [(&#39;right&#39;, &#39;wrong1&#39;), (&#39;right&#39;, &#39;wrong2&#39;)] pairs.&#34;
    return [(right, wrong)
            for (right, wrongs) in (line.split(&#39;:&#39;) for line in lines)
            for wrong in wrongs.split()]

print(unit_tests())
spelltest(Testset(open(&#39;<a href="http://www.norvig.com/spell-testset1.txt">spell-testset1.txt</a>&#39;))) # Development set
spelltest(Testset(open(&#39;<a href="http://www.norvig.com/spell-testset1.txt">spell-testset2.txt</a>&#39;))) # Final test set
</pre>

<p>This gives the output:

</p><pre>unit_tests pass
75% of 270 correct at 41 words per second
68% of 400 correct at 35 words per second
None</pre>

<p>So on the development set we get 75% correct (processing words at a rate of 41 words/second), and on the final test set we get 68%
correct (at 35 words/second).  In conclusion, I met my goals for brevity, development time, and runtime speed, but not for accuracy.
Perhaps my test set was extra tough, or perhaps my simple model is just not good enough to get to 80% or 90% accuracy.

</p><h2>Future Work</h2>

Let&#39;s think about how we
could do better. (I&#39;ve developed the ideas some more in a <a href="http://norvig.com/ngrams/">separate chapter</a> for a book
and in a <a href="http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb">Jupyter notebook</a>.)<ol>

<li>P(<i>c</i>), the language model.  We can distinguish two sources
of error in the language model.  The more serious is unknown words. In
the development set, there are 15 unknown words, or 5%, and in the
final test set, 43 unknown words or 11%. Here are some examples
of the output of <tt>spelltest</tt> with <tt>verbose=True)</tt>:



<pre>correction(&#39;transportibility&#39;) =&gt; &#39;transportibility&#39; (0); expected &#39;transportability&#39; (0)
correction(&#39;addresable&#39;) =&gt; &#39;addresable&#39; (0); expected &#39;addressable&#39; (0)
correction(&#39;auxillary&#39;) =&gt; &#39;axillary&#39; (31); expected &#39;auxiliary&#39; (0)
</pre>

<p>In this output we show the call to <tt>correction</tt> and the actual and expected results
(with the <tt>WORDS</tt> counts in parentheses).  
Counts of (0) mean the target word was not in the dictionary, so we have no chance of getting it right.
We could create a better language model by collecting more data, and perhaps by
using a little English morphology (such as adding &#34;ility&#34; or &#34;able&#34; to the end of a word).


</p><p>Another way to deal with unknown words is to allow the result of
<tt>correction</tt> to be a word we have not seen. For example, if the
input is &#34;electroencephalographicallz&#34;, a good correction would be to
change the final &#34;z&#34; to an &#34;y&#34;, even though
&#34;electroencephalographically&#34; is not in our dictionary.  We could
achieve this with a language model based on components of words:
perhaps on syllables or suffixes, but it is
easier to base it on sequences of characters: common 2-, 3- and 4-letter
sequences.

</p></li><li>P(<i>w</i>|<i>c</i>), the error model. So far, the error model
has been trivial: the smaller the edit distance, the smaller the
error.  This causes some problems, as the examples below show.  First,
some cases where <tt>correction</tt> returns a word at edit distance 1
when it should return one at edit distance 2:

<pre>correction(&#39;reciet&#39;) =&gt; &#39;recite&#39; (5); expected &#39;receipt&#39; (14)
correction(&#39;adres&#39;) =&gt; &#39;acres&#39; (37); expected &#39;address&#39; (77)
correction(&#39;rember&#39;) =&gt; &#39;member&#39; (51); expected &#39;remember&#39; (162)
correction(&#39;juse&#39;) =&gt; &#39;just&#39; (768); expected &#39;juice&#39; (6)
correction(&#39;accesing&#39;) =&gt; &#39;acceding&#39; (2); expected &#39;assessing&#39; (1)
</pre>

<p>Why should &#34;adres&#34; be corrected to &#34;address&#34; rather than &#34;acres&#34;?
The intuition is that the two edits from &#34;d&#34; to &#34;dd&#34; and &#34;s&#34; to &#34;ss&#34;
should both be fairly common, and have high probability, while the
single edit from &#34;d&#34; to &#34;c&#34; should have low probability.


</p><p>Clearly we could use a better model of the cost of edits.  We could
use our intuition to assign lower costs for doubling letters and
changing a vowel to another vowel (as compared to an arbitrary letter
change), but it seems better to gather data: to get a corpus of
spelling errors, and count how likely it is to make each insertion,
deletion, or alteration, given the surrounding characters.  We need a
lot of data to do this well.  If we want to look at the change of one
character for another, given a window of two characters on each side,
that&#39;s 26<sup>6</sup>, which is over 300 million characters.  You&#39;d
want several examples of each, on average, so we need at least a
billion characters of correction data; probably safer with at least 10
billion.

</p><p>Note there is a connection between the language model and the error model.
The current program has such a simple error model (all edit distance 1 words
before any edit distance 2 words) that it handicaps the language model: we are
afraid to add obscure words to the model, because if one of those obscure words
happens to be edit distance 1 from an input word, then it will be chosen, even if
there is a very common word at edit distance 2.  With a better error model we
can be more aggressive about adding obscure words to the dictionary.  Here are some
examples where the presence of obscure words in the dictionary hurts us:

</p><pre>correction(&#39;wonted&#39;) =&gt; &#39;wonted&#39; (2); expected &#39;wanted&#39; (214)
correction(&#39;planed&#39;) =&gt; &#39;planed&#39; (2); expected &#39;planned&#39; (16)
correction(&#39;forth&#39;) =&gt; &#39;forth&#39; (83); expected &#39;fourth&#39; (79)
correction(&#39;et&#39;) =&gt; &#39;et&#39; (20); expected &#39;set&#39; (325)
</pre>

</li><li>The enumeration of possible
corrections, argmax<sub><i>c</i></sub>.  Our program enumerates all corrections within
edit distance 2.  In the development set, only 3 words out of 270 are
beyond edit distance 2, but in the final test set, there were 23 out
of 400.  Here they are:

<blockquote><pre>purple perpul
curtains courtens
minutes muinets

successful sucssuful
hierarchy heiarky
profession preffeson
weighted wagted
inefficient ineffiect
availability avaiblity
thermawear thermawhere
nature natior
dissension desention
unnecessarily unessasarily
disappointing dissapoiting
acquaintances aquantences
thoughts thorts
criticism citisum
immediately imidatly
necessary necasery
necessary nessasary
necessary nessisary
unnecessary unessessay
night nite
minutes muiuets
assessing accesing
necessitates nessisitates
</pre></blockquote>

<p>We could consider extending the model by allowing a limited set of
edits at edit distance 3. For example, allowing only the insertion of
a vowel next to another vowel, or the replacement of a vowel for
another vowel, or replacing close consonants like &#34;c&#34; to &#34;s&#34; would
handle almost all these cases.

</p></li><li>There&#39;s actually a fourth (and best) way to improve: change the
interface to <tt>correction</tt> to look at more context. So far,
<tt>correction</tt> only looks at one word at a time.  It turns out that
in many cases it is difficult to make a decision based only on a
single word.  This is most obvious when there is a word that appears
in the dictionary, but the test set says it should be corrected to
another word anyway:

<pre>correction(&#39;where&#39;) =&gt; &#39;where&#39; (123); expected &#39;were&#39; (452)
correction(&#39;latter&#39;) =&gt; &#39;latter&#39; (11); expected &#39;later&#39; (116)
correction(&#39;advice&#39;) =&gt; &#39;advice&#39; (64); expected &#39;advise&#39; (20)
</pre>

<p>We can&#39;t possibly know that <tt>correction(&#39;where&#39;)</tt> should be
&#39;were&#39; in at least one case, but should remain &#39;where&#39; in other cases.
But if the query had been <tt>correction(&#39;They where going&#39;)</tt> then it
seems likely that &#34;where&#34; should be corrected to &#34;were&#34;.
</p><p>
The context of the surrounding words can help when there are obvious errors,
but two or more good candidate corrections.  Consider:

</p><pre>correction(&#39;hown&#39;) =&gt; &#39;how&#39; (1316); expected &#39;shown&#39; (114)
correction(&#39;ther&#39;) =&gt; &#39;the&#39; (81031); expected &#39;their&#39; (3956)
correction(&#39;quies&#39;) =&gt; &#39;quiet&#39; (119); expected &#39;queries&#39; (1)
correction(&#39;natior&#39;) =&gt; &#39;nation&#39; (170); expected &#39;nature&#39; (171)
correction(&#39;thear&#39;) =&gt; &#39;their&#39; (3956); expected &#39;there&#39; (4973)
correction(&#39;carrers&#39;) =&gt; &#39;carriers&#39; (7); expected &#39;careers&#39; (2)
</pre>

<p>Why should &#39;thear&#39; be corrected as &#39;there&#39; rather than &#39;their&#39;?  It is
difficult to tell by the single word alone, but if the query were
<tt>correction(&#39;There&#39;s no there thear&#39;)</tt> it would be clear.



</p><p>
To build a model that looks at multiple words at a time, we will need a lot of data.
Fortunately, Google has released
a <a href="http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html">database
of word counts</a> for all sequences up to five words long,
gathered from a corpus of a <i>trillion</i> words.

</p><p>I believe that a spelling corrector that scores 90% accuracy will
<i>need</i> to use the context of the surrounding words to make a
choice.  But we&#39;ll leave that for another day...

</p><p>We could also decide what dialect we are trying to train for.  The
following three errors are due to confusion about American versus
British spelling (our training data contains both):

</p><pre>correction(&#39;humor&#39;) =&gt; &#39;humor&#39; (17); expected &#39;humour&#39; (5)
correction(&#39;oranisation&#39;) =&gt; &#39;organisation&#39; (8); expected &#39;organization&#39; (43)
correction(&#39;oranised&#39;) =&gt; &#39;organised&#39; (11); expected &#39;organized&#39; (70)
</pre>


</li><li>Finally, we could improve the implementation by making it much
faster, without changing the results.  We could re-implement in a
compiled language rather than an interpreted one.  We could cache the results of computations so
that we don&#39;t have to repeat them multiple times.  One word of advice:
before attempting any speed optimizations, profile carefully to see
where the time is actually going.
</li></ol>



<h2>Further Reading</h2>

<ul>
<li>Roger Mitton has a <a href="http://www.dcs.bbk.ac.uk/~roger/spellchecking.html">survey article</a>
on spell checking.

</li><li>Jurafsky and Martin cover spelling correction well in their text 
<i><a href="http://www.cs.colorado.edu/~martin/slp.html">Speech and Language Processing</a></i>.
</li><li>Manning and Schutze
cover statistical language models very well in their text
<i><a href="http://nlp.stanford.edu/fsnlp/">Foundations of Statistical Natural Language Processing</a></i>,
but they don&#39;t seem to cover spelling (at least it is not in the index).
</li><li> The <a href="http://aspell.net">aspell</a> project has a lot of interesting material,
including some <a href="http://aspell.net/test/">test data</a> that seems better than what I used.
</li><li> The <a href="http://alias-i.com/lingpipe">LingPipe</a> project has a <a href="http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html">spelling tutorial</a>.
</li></ul>

<h2>Acknowledgments</h2>

Ivan Peev, Jay Liang, Dmitriy Ryaboy and Darius Bacon pointed out problems in <a href="https://web.archive.org/web/*/http://norvig.com/spell-correct.html">earlier versions</a>
of this document.



<h2>Other Computer Languages</h2>

After I posted this article, various people wrote versions in
different programming languages.  These
may be interesting for those who like comparing
languages, or for those who want to borrow an implementation in their
desired target language:

<p>
<table>
<tbody><tr><th>Language</th><th>Lines</th><th>Author</th></tr><tr><td>Awk</td><td>15</td><td><a href="http://pacman.blog.br/wiki/index.php?title=Um_Corretor_Ortogr%C3%A1fico_em_GAWK">Tiago &#34;PacMan&#34; Peczenyj</a>
</td></tr><tr><td>Awk</td><td>28</td><td><a href="http://feedback.exalead.com/feedbacks/191466-spell-checking">Gregory Grefenstette</a>
</td></tr><tr><td>C</td><td>184</td><td><a href="http://blog.marcelotoledo.org/2007/08/10/how-to-write-a-spelling-corrector/">Marcelo Toledo</a>
</td></tr><tr><td>C++</td><td>98</td><td><a href="http://scarvenger.wordpress.com/2007/12/11/how-to-write-a-spelling-corrector/">Felipe Farinon</a>
</td></tr><tr><td>C#</td><td>43</td><td><a href="https://github.com/lorenzo-stoakes/spell-correct">Lorenzo Stoakes</a>
</td></tr><tr><td>C#</td><td>69</td><td><a href="http://frederictorres.blogspot.com/2011/04/how-to-write-spelling-corrector-from.html">Frederic Torres</a>
</td></tr><tr><td>C#</td><td>160</td><td><a href="http://www.anotherchris.net/csharp/how-to-write-a-spelling-corrector-in-csharp/">Chris Small</a>
</td></tr><tr><td>C#</td><td>---</td><td><a href="https://github.com/joaocarvalhoopen/USB_SpellChecker_GUI_in_C_Sharp/blob/master/SpellChecker_GUI/SpellChecker_GUI/TweakedPeterNorvigSpellChecker.cs">João Nuno Carvalho</a>
</td></tr><tr><td>Clojure</td><td>18</td><td><a href="http://en.wikibooks.org/wiki/Clojure_Programming/Examples#Norvig.27s_Spelling_Corrector">Rich Hickey</a>
</td></tr><tr><td>Coffeescript</td><td>21</td><td><a href="https://metaphysicaldeveloper.wordpress.com/2011/03/31/354/">Daniel Ribeiro</a>
</td></tr><tr><td>D</td><td>23</td><td><a href="http://leonardo-m.livejournal.com/59589.html">Leonardo M</a>
</td></tr><tr><td>Erlang</td><td>87</td><td><a href="http://www.pixzone.com/blog/223/spell-corrector-aka-google-suggest-in-erlang-first-part/">Federico Feroldi</a>
</td></tr><tr><td>F#</td><td>16</td><td><a href="http://www.jelovic.com/weblog/?p=201">Dejan Jelovic</a>
</td></tr><tr><td>F#</td><td>34</td><td><a href="http://cs.hubfs.net/forums/thread/3085.aspx">Sebastian G</a>
</td></tr><tr><td>Go</td><td>57</td><td><a href="http://cxwangyi.wordpress.com/2012/02/15/peter-norvigs-spelling-corrector-in-go/">Yi Wang</a>
</td></tr><tr><td>Groovy</td><td>22</td><td><a href="http://raelcunha.com/spell-correct.php#groovy">Rael Cunha</a>
</td></tr><tr><td>Haskell</td><td>24</td><td><a href="http://pithekos.net/brainwave/">Grzegorz</a>
</td></tr><tr><td>Java 8</td><td>23</td><td><a href="https://github.com/unrelatedlabs/SpellingCorrector-Java8">Peter Kuhar</a>
</td></tr><tr><td>Java</td><td>35</td><td><a href="http://raelcunha.com/spell-correct.php">Rael Cunha</a>
</td></tr><tr><td>Java</td><td>372</td><td><a href="http://developer.gauner.org/jspellcorrect/">Dominik Schulz</a>
</td></tr><tr><td>Javascript</td><td>92</td><td><a href="http://stoi.wordpress.com/2012/12/31/jspell/">Shine Xavier</a>
</td></tr><tr><td>Javascript</td><td>53</td><td><a href="http://astithas.blogspot.com/2009/08/spell-checking-in-javascript.html">Panagiotis Astithas</a>
</td></tr><tr><td>Lisp</td><td>26</td><td> <a href="https://github.com/mikaelj/snippets/blob/master/lisp/spellcheck/spellcheck.lisp">Mikael Jansson</a>
</td></tr><tr><td>OCaml</td><td>148</td><td><a href="http://spacifico.org/programming/norvig-spell-corrector-ocaml">Stefano Pacifico</a>
</td></tr><tr><td>Perl</td><td>63</td><td><a href="http://www.riffraff.info/2007/5/20/a-spell-corrector-in-perl6-part-3">riffraff</a>
</td></tr><tr><td>PHP</td><td>68</td><td><a href="http://www.phpclasses.org/browse/package/4859.html">Felipe Ribeiro</a>
</td></tr><tr><td>PHP</td><td>103</td><td><a href="http://soundofemotion.com/spellcorrect.txt">Joe Sanders</a>
</td></tr><tr><td>R</td><td>2</td><td><a href="http://www.sumsar.net/blog/2014/12/peter-norvigs-spell-checker-in-two-lines-of-r/">Rasmus Bååth</a>
</td></tr><tr><td>Rebol</td><td>133</td><td><a href="http://www.rebol.cz/~cyphre/spell.r">Cyphre</a>
</td></tr><tr><td>Ruby</td><td>34</td><td><a href="http://lojic.com/blog/2008/09/04/how-to-write-a-spelling-corrector-in-ruby/">Brian Adkins</a>
</td></tr><tr><td>Scala</td><td>20</td><td><a href="https://gist.github.com/pathikrit/d5b26fe1c166a97e2162">Pathikrit Bhowmick</a>
</td></tr><tr><td>Scala</td><td>23</td><td><a href="http://theyougen.blogspot.com/2009/12/peter-norvigs-spelling-corrector-in.html">Thomas Jung</a>
</td></tr><tr><td>Scheme</td><td>45</td><td><a href="http://practical-scheme.net/wiliki/wiliki.cgi?Gauche%3aSpellingCorrection&amp;l=en">Shiro</a> 
</td></tr><tr><td>Scheme</td><td>89</td><td><a href="http://scheme.dk/blog/2007/04/writing-spelling-corrector-in-plt.html">Jens Axel</a>
</td></tr><tr><td>Swift</td><td>108</td><td><a href="http://airspeedvelocity.net/2015/05/02/spelling/"> Airspeed Velocity<!---->
</a></td></tr></tbody></table>

</p><h2>Other Natural Languages</h2>

This essay has been translated into:

<ul>
<li> <a href="http://blog.youxu.info/spell-correct.html">Simplified Chinese</a>
by Eric You XU
</li><li> <a href="http://www.aoky.net/articles/peter_norvig/spell-correct.htm">Japanese</a> by Yasushi Aoki
</li><li> <a href="http://theyearlyprophet.com/spell-correct.html">Korean</a> by JongMan Koo
</li><li> <a href="http://gmdidro.googlepages.com/Ru_HowtoWriteaSpellingCorrector.html">Russian</a> by Petrov Alexander

</li></ul>
<p>
Thanks to all the authors for creating these implementations and translations.
</p><hr/>
<address><a href="http://norvig.com"><i>Peter Norvig</i></a></address>


 

</div>
  </body>
</html>
