<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fablestudio.github.io/showrunner-agents/">Original</a>
    <h1>AI-Generated South Park</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    
    
  
</section>

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            In this work we present our approach to generating high-quality episodic content for IP&#39;s (Intellectual Property) using large language models (LLMs), custom state-of-the art diffusion models and our multi-agent simulation for contextualization, story progression and behavioral control.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Youtube video -->
<section>
  <div>
    <div>
      <!-- Paper video. -->
     
      <div>
        <div>
          <h2>Video</h2>
          <div>
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
           
            <p><iframe src="https://player.vimeo.com/video/830748401?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="To Infinity and Beyond: Showrunner Systems in Multi-Agent Simulations Example Generated Episode"></iframe></p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->



<section>
  <div>
    <div>

      <div>
        <div>
          <p>
            <i>Note: This web-version of the paper does not include footnotes. Please refer to the pdf-version above for all citations and references.</i>
          </p>

          <h2>Creative limitations of existing generative AI Systems</h2>
          <p>
              Current generative AI systems such as Stable Diffusion (Image Generator) and ChatGPT (Large Language Model) excel at short-term general tasks through prompt engineering. However, they do not provide contextual guidance or intentionality to either a user or an automated generative story system (showrunner) as part of a long-term creative process which is often essential to producing high-quality creative works, especially in the context of existing IP&#39;s.
             </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section>
  <div>
    <div>
      <h3>Living with uncertainty</h3>
      <div>
        <p>
          By using a multi-agent simulation as part of the process we can make use of data points such as a character&#39;s history, their goals and emotions, simulation events and localities to generate scenes and image assets more coherently and consistently aligned with the IP story world. The IP-based simulation also provides a clear, well known context to the user which allows them to judge the generated story more easily.
          Moreover, by allowing them to exert behavioral control over agents, observe their actions and engage in interactive conversations, the user&#39;s expectations and intentions are formed which we then funnel into a simple prompt to kick off the generation process.
          </p>
          <p>
       Our simulation is sufficiently complex and non-deterministic to favor a positive disconfirmation. Amplification effects can help mitigate what we consider an undesired &#34;slot machine&#34; effect which we&#39;ll briefly touch on later. We are used to watching episodes passively and the timespan between input and &#34;end of scene/episode&#34; discourages immediate judgment by the user and as a result reduces their desire to &#34;retry&#34;. This disproportionality of the user&#39;s minimal input prompt and the resulting high-quality long-form output in the form of a full episode is a key factor for positive disconfirmation.
      </p>
      <p>
        While using and prompting a large language model as part of the process can introduce <i>&#34;several challenges&#34;</i>. Some of them, like hallucinations, which introduce uncertainty or in more creative terms &#34;unexpectedness&#34;, can be regarded as creative side-effects to influence the expected story outcome in positive ways. As long as the randomness introduced by hallucination does not lead to implausible plot or agent behavior and the system can recover, they act as happy-accidents, a term often used during the creative process, further enhancing the user experience.

      </p>
  </div>
</div></div></section>


<section>
  <div>
    <div>

      <div>
        <div>
          <h2>Large Language Models</h2>
          <div>
            <p>
              LLMs represent the forefront of natural language processing and machine learning research, demonstrating exceptional capabilities in understanding and generating human-like text. They are typically built on Transformer-based architectures, a class of models that rely on self-attention mechanisms. Transformers allow for efficient use of computational resources, enabling the training of significantly larger language models. GPT-4, for instance, comprises billions of parameters that are trained on extensive datasets, effectively encoding a substantial quantity of worldly knowledge in their weights.
            <br/>
            </p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/The-Transformer-model-architecture%20%281%29.png" alt=""/>
            </p>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section>
  <div>
    <div>
      <h2>Episode Generation</h2>
      <div>
        <p>
          We define an episode as a sequence of dialogue scenes in specific locations which add up to a total runtime of a regular 22 min south park episode.
<br/>

</p><p>
  In order to generate a full south park episode, we prompt the story system with a high level idea, usually in the form of a title, synopsis and major events we want to see happen over the course of 1 week in simulation time (=roughly 3 hours of play time).

</p>
<p>
  From this, the story system automatically extrapolates up to 14 scenes by making use of simulation data as part of a prompt chain. The showrunner system takes care of casting the characters for each scene and how the story should progress through a plot pattern. Each scene is associated with a plot letter (e.g. A, B, C) which is then used by the showrunner to alternate between different character groups and follow their individual storylines over the course of an episode to keep the user engaged.

</p>
<p>
  In the end, each scene simply defines the location, cast and dialogue for each cast member.
The scene is played back according to the plot pattern (e.g. ABABC) after the staging system and AI camera system went through initial setup. The voice of each character has been cloned in advance and voice clips are generated on the fly for every new dialogue line.

</p>
        

        <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/showrunnergraph.png" alt="Comparison of Response Speed: gpt-3.5-turbo, gpt-4"/>
          </p>
  </div>

</div>

</div></section>



<!-- Youtube video -->
<section>
  <div>
    <div>
      <!-- Paper video. -->
      
      <div>
        <div>
          <h2>Video</h2>
          <div>
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <p><iframe src="https://player.vimeo.com/video/834880928?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="To Infinity and Beyond Showrunner Systems in Multi-Agent Simulations Example Episode with Prompt"></iframe></p>
       
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<section>
  <div>
  <div>
    <h3>Reducing Latency</h3>
    <div>
      <p>
        In our experiments, generating a single scene can take a significant amount of time of up to one minute. Below is a response time comparison between GPT-3.5-turbo and GPT-4. 
        Speed will increase in the short-term as models and service infrastructure get improved and other factors like artificial throttling due to high user demand will get removed.
      </p>
      <p>
        Since we generate the episodes during gameplay, we have ways to hide most of the generation time in moments when the user is still interacting with the simulation or other user interfaces. Another way to reduce the time needed to generate a scene or episode is to use faster models such as GPT-3.5-turbo for specific prompts in the chain where the highest quality and accuracy is not so important.

      </p>        


<p><img src="https://fablestudio.github.io/showrunner-agents/static/images/Gpt3_gpt4_comparison.png" alt="Comparison of Response Speed: gpt-3.5-turbo, gpt-4"/>
</p>

<p>
  During scene playback, we avoid any unwanted pauses between dialogue lines related to audio generation by using a simple buffering system which generates at least one voice clip in advance. This means while one character is delivering their voice clip, we already make the web request for the next voice clip, wait for it to generate, download the file and then wait for the current speaker to finish his dialogue before playback (delay). In this way the next dialogue line&#39;s voice clip is always delivered without any delay. Text generation and voice cloning services become increasingly fast and allow for highly adaptive and near-real time voice conversations.

</p>
  <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/AudioLatency_00000.png" alt="Timeline showing audio clips"/>
  </p>


<h3>Simulate creative thinking</h3>

<p>
  As stated earlier, the data produced by the simulation acts as creative fuel to both, the user who is writing the initial prompt and the generative story system which is interacting with the LLM via prompt-chaining.
Prompt-chaining is a technique, which involves supplying the language model with a sequence of related prompts to simulate a continuous thought process. Sometimes it can take on different roles in each step to act as the discriminator against the previous prompt and generated result.
</p><p>  


In our case we try to mimic that of a discontinuous creative thought process.
</p><p>

  For example, the creation of 14 distinct South Park scenes could be managed by initially providing a broad prompt to outline the general narrative, followed by specific prompts detailing and evaluating each scene&#39;s cast, location, and key plot points. This mimics the process of human brainstorming, where ideas are built upon and refined in multiple often discontinuous steps. By leveraging the generative capabilities of LLMs in conjunction with the iterative refinement offered by prompt-chaining, we can effectively construct a dynamic, detailed, and engaging narrative. 
</p>

<p>
  In addition, we explore new concepts like plot patterns and dramatic operators (DrOps) to enhance the episode structure overall but also the connective tissue between each scene. Stylistic devices like reversals, foreshadowing, cliffhangers are difficult to evaluate as part of a prompt chain. A user without a writing background would have equal difficulty in judging these stylistic devices for their effectiveness and proper placement. We propose a procedural approach, injecting these show specific patterns and stylistic devices into the prompt chain programmatically as plot patterns and DrOps which can operate on the level of act structures, scene structures and individual dialogue lines. We are investigating future opportunities to extract what we call a dramatic fingerprint which is specific to each IP and format and train our custom SHOW-1 model with these data points. This dataset combined with overall human feedback could further align tone, style and entertainment value between the user and the specified IP while offering a highly adaptive and interactive story system as part of the on-going simulation.

</p>
</div>

<br/>
</div>

</div></section>


<section>
  <div>

<div>
 
<h3>Blank page problem
</h3>

<p>
  As mentioned above, one of the advantages of the simulation is that it avoids the blank page problem for both a user and a large language model by providing creative fuel. Even experienced writers can sometimes feel overwhelmed when asked to come up with a title or story idea without any prior incubation of related material. The same could be said for LLMs. The simulation provides context and data points before starting the creative prompt chain.
    </p>
   

    <h3>Who is driving the story?
    </h3>
    
    <p>
      The story generation process in our approach is a shared responsibility between the simulation, the user, and GPT-4. Each has strengths and weaknesses and a unique role to play depending on how much we want to involve them in the overall creative process. Their contributions can have different weights. While the simulation usually provides the foundational IP-based context, character histories, emotions, events, and localities that seed the initial creative process. The user introduces their intentionality, exerts behavioral control over the agents and provides the initial prompts that kick off the generative process. The user also serves as the final discriminator, evaluating the generated story content at the end of the process. GPT-4, on the other hand, serves as the main generative engine, creating and extrapolating the scenes and dialogue based on the prompts it receives from both the user and the simulation. It&#39;s a symbiotic process where the strengths of each participant contribute to a coherent, engaging story. Importantly, our multi-step approach in the form of a prompt-chain also provides checks and balances, mitigating the potential for unwanted randomness and allowing for more consistent alignment with the IP story world.
      </p>
       

        <h3>SHOW-1 and Intentionality
        </h3>
        
        <p>
          The formular (creative characteristics) and format (technical characteristics) of a show are often a function of real-world limitations and production processes. They usually don&#39;t change, even over the course of many seasons (South Park currently has 26 seasons and 325 episodes) 

        </p>
        <p>
          A single dramatic fingerprint of a show, which is used to train the proposed SHOW-1 model, can be regarded as a highly variable template or &#34;formula&#34;  for a procedural generator that produces South Park-like episodes.
        </p>
        <p>
          To train a model such as SHOW-1 we need to gather a sufficient amount of data points in relation to each other that characterize a show. A TV show does not just come into existence and is made up of the final dialogue lines and set descriptions as seen by the audience. Existing datasets on which current LLM&#39;s are trained on only consist of the final screenplay which has the cast, dialogue lines and sometimes a short scene header. A lot of information is missing, such as timing, emotional states, themes, contexts discussed in the writer&#39;s room and detailed directorial notes to give a few examples. The development and refinement of characters is also part of this on-going process. Fictional characters have personalities, backstories and daily routines which help authors to sculpt not only scenes but the arcs of whole seasons. Even during a show characters keep evolving based on audience feedback or changes in creative direction. With the Simulation, we can gather data continuously from both the user&#39;s input and the simulated agents. Over time, as episodes are created, refined and rated by the user we can start to train a show specific model and deploy it as a checkpoint which allows the user to continue to refine and iterate on either their own original show or alternatively push an already existing show such as south park into directions previously not conceived by the original show runners and IP holders. To illustrate this, we imagine a user generating multiple south park episodes in which Cartman, one of the main characters and known for his hot headedness, slowly changes to be shy and naive while the life of other characters such as Butters could be tuned to follow a much more dominant and aggressive path. Over time, this feedback loop of interacting with and fine-tuning the SHOW-1 model can lead to new interpretations of existing shows but more excitingly to new original shows based on the user&#39;s intention. One of the challenges in order to make this feedback loop engaging and satisfying is the frequency at which a model can be trained. A model which is fed by real-time simulation data and user input should not feel static or require expensive resources to adapt. Otherwise the output it generates can feel static and unresponsive as well.
        </p>
         <p>
          When a generative system is not limited in its ability to swiftly produce high amounts of content and there is no limit for the user to consume such content immediately and potentially simultaneously, the 10,000 Bowls of Oatmeal problem can become an issue. Everything starts to look and feel the same or even worse, the user starts to recognize a pattern which in turn reduces their engagement as they expect newly generated episodes to be like the ones before it, without any surprises.

         </p>
         <p>
          This is quite different from a predictable plot which in combination with the above mentioned &#34;positive hallucinations&#34; or happy accidents of a complex generative system can be a good thing. Surprising the user by balancing and changing the phases of certainty vs. uncertainty helps increase their overall engagement. If they would not expect or predict anything, they could also not get pleasantly surprised.
         </p>

         <p>
          With our work we aim for perceptual uniqueness. The OatMeal problem of procedural generators is mitigated by making use of an on-going simulation (a hidden generator) and the long-form content of 22 min episodes which are only generated every 3h. This way the user generally does not consume a high quantity of content simultaneously or in a very short amount of time. This artificial scarcity, natural game play limits and simulation time help.


         </p>
         <p>
          Another factor that keeps audiences engaged while watching a show and what makes episodes unique is intentionality from the authors. A satirical moral premise, twisted social commentary, recent world events or cameos by celebrities are major elements for South Park. Other show types, for example sitcoms, usually progress mainly through changes in relationship (some of which are never fulfilled), keeping the audience hooked despite following the same format and formula. 
         </p>

         <p>
          Intentionality from the user to generate a high-quality episode is another area of internal research. Even users without a background in dramatic writing should be able to come up with stories, themes or major dramatic questions they want to see played out within the simulation.
          </p>

         <p>
          One of the remaining unanswered questions in the context of intentionality is how much entertainment value (or overall creative value) is directly attributed to the creative personas of living authors and directors. Big names usually drive ticket sales but the creative credit the audience gives to the work while consuming it seems different. 
Watching a Disney movie certainly carries with it a sense of creative quality, regardless of famous voice actors, as a result of brand attachment and its history.

         </p>

         <p>
          AI generated content is generally perceived as lower quality and the fact that it can get generated in abundance further decreases its value. How much this perception would change if Disney were to openly pride themselves on having produced a fully AI generated movie is hard to say. What if Steven Spielberg, single handedly generated an AI movie? Our assumption is that the perceived value of AI generated content would certainly increase.

         </p>
         <p>
          A new interesting approach to replicate this could be the embodiment of creative AI models such as SHOW-1 to allow them to build a persona outside their simulated world and build relationships via social media or real world events with their audience. As long as an AI model is perceived as a black box and does not share their creative process and reasoning in a human and accessible way, as is the case for living writers and directors, it&#39;s unlikely to get credit with real creative values. However, for now this is a more philosophical question in the context of AGI.
         </p>
            
           
</div>

</div></section>




<section>
  <div>
    <div>

      <div>
        <div>
          <h2>Conclusion</h2>
          <div>
            <p>
              Our approach of using multi-agent simulation and large language models for generating high-quality episodic content provides a novel and effective solution to many of the limitations of current AI systems in creative storytelling. By integrating the strengths of the simulation, the user, and the AI model, we provide a rich, interactive, and engaging storytelling experience that is consistently aligned with the IP story world. Our method also mitigates issues such as the &#39;slot machine effect&#39;, &#39;the oatmeal problem&#39; and &#39;blank page problem&#39; that plague conventional generative AI systems. As we continue to refine this approach, we are confident that we can further enhance the quality of the generated content, the user experience, and the creative potential of generative AI systems in storytelling.
            </p><p>
            We are grateful to Lewis Hackett for his help and expertise in training the custom Stable Diffusion Models.
            
           
          </p></div>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>
        @article{fable2023showrunner,
          author    = {Maas, Carey, Wheeler, Saatchi, Billington, Shamash},
          title     = {To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations},
          journal   = {arXiv preprint},
          year      = {2023}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/SouthPark_AI_Main_rc_originalIntro_watermarks_230518_v7.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Westland Chronicles - AI generated South Park episode.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->



<!-- Image carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here 
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here 
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->







<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here 
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here 
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->




  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
