<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cerebras.ai/blog/100x-defect-tolerance-how-cerebras-solved-the-yield-problem">Original</a>
    <h1>100x defect tolerance: How we solved the yield problem</h1>
    
    <div id="readability-page-1" class="page"><div data-parent="true" id="row-unique-1"><div><div><div><div><div><div><div><div><p>Conventional wisdom in semiconductor manufacturing has long held that bigger chips mean worse yields. Yet at Cerebras, we’ve successfully built and commercialized a chip 50x larger than the largest computer chips – and achieved comparable yields. This seeming paradox is one of our most frequently asked questions: how do we achieve a usable yield with a wafer-scale processor?</p>
<p>The answer lies in rethinking the relationship between chip size and fault tolerance. This article will provide a detailed, apples-to-apples comparison of manufacturing yields between the Cerebras Wafer Scale Engine and an H100-sized chip, both manufactured at 5nm. By examining the interplay between defect rates, core size, and fault tolerance, we’ll show how we achieve wafer scale integration with equal or better yields vs. reticle limited GPUs.</p>
<h3>What determines yield</h3>
<p>Like any manufacturing process, computer chips are prone to defects. Larger chips are more likely to encounter defects, thus as chips grow in size, yields fall exponentially with increasing die area. Even though larger chips generally run faster, early microprocessors were built to a modest size to maintain acceptable manufacturing yields and profit margins. In the early 2000s, this started to change. As transistor budgets grew to over 100 million, it became the norm to build processors with multiple independent cores per chip. Since all the cores were identical and independent, chip designers built-in core-level fault tolerance so that if one core suffered a defect, the remaining cores could still operate. For example in 2006 Intel released the Intel Core Duo – a chip with two CPU cores. If one core was faulty, it was disabled and the product was sold as an Intel Core Solo. Nvidia, AMD, and others all embraced this core-level redundancy in the coming years.</p>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-scaled.jpg" width="2560" height="919" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-scaled.jpg 2560w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-300x108.jpg 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-1024x368.jpg 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-768x276.jpg 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-1536x551.jpg 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-2048x735.jpg 2048w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-01-350x126.jpg 350w" sizes="(max-width: 2560px) 100vw, 2560px"/></p>
					</div>
				</div></div></div></div></div><div><p>Today, fault tolerance is widely used by high performance processors and it’s perfectly normal to sell chips with some cores disabled. AMD and Intel CPUs generally ship a flagship version with all cores enabled and a lower end version with a portion of cores disabled. Nvidia’s data center GPUs are substantially larger than CPU dies and as a result even its flagship models have some portion of cores disabled.</p>
<p>Take the Nvidia H100 – a massive GPU weighing in at 814mm<sup>2</sup>. Traditionally this chip would be very difficult to yield economically. But since its cores (SMs) are fault tolerant, a manufacturing defect does not knock out the entire product. The chip physically has 144 SMs but the commercialized product only has 132 SMs active. This means the chip could suffer numerous defects across 12 SMs and still be sold as a flagship part.</p>
<h3>Defect tolerance is the key to yield</h3>
<p>Traditionally, chip size directly dictated chip yields. In the modern era, yield is a function of both chip size and defect tolerance. 800mm<sup>2</sup> chips were once thought infeasible to commercialize due to yield, but by using defect tolerant design, they are now mainstream products.</p>
<p>The degree of defect tolerance can be measured by the amount of chip area that is lost when a defect occurs. For multi-core chips, this means the smaller the core, the greater the defect tolerance. If individual cores are small enough, it may be possible to build a very large chip.</p>
<h3>Wafer Scale Engine Cores</h3>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01.jpg" width="1640" height="862" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01.jpg 1640w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01-300x158.jpg 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01-1024x538.jpg 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01-768x404.jpg 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01-1536x807.jpg 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-wafer-scale-01-350x184.jpg 350w" sizes="(max-width: 1640px) 100vw, 1640px"/></p>
					</div>
				</div></div></div></div></div><div><p>At Cerebras, before committing to build a wafer-scale chip, we first designed a very small core. Each AI core in the Wafer Scale Engine 3 is approximately 0.05mm<sup>2</sup> or about 1% the size of an H100 SM core. Both core designs are fault tolerant. This means a defect in a WSE core would disable 0.05mm<sup>2</sup> of silicon while the same defect in an H100 disables ~6mm<sup>2</sup>. To a first order of approximation, the wafer scale engine is ~100x more fault tolerant than a GPU when considering the silicon area affected by each defect.</p>
<h3>The Routing Architecture</h3>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-scaled.jpg" width="2560" height="2050" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-scaled.jpg 2560w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-300x240.jpg 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-1024x820.jpg 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-768x615.jpg 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-1536x1230.jpg 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-2048x1640.jpg 2048w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-defects-01-350x280.jpg 350w" sizes="(max-width: 2560px) 100vw, 2560px"/></p>
					</div>
				</div></div></div></div></div><div><p>But small cores alone aren’t enough. We developed a sophisticated routing architecture that allows us to dynamically reconfigure connections between cores. When a defect is detected, the system can automatically route around it using redundant communication pathways, preserving the chip’s overall computational capabilities by leveraging nearby cores.</p>
<p>This routing system works in concert with a small reserve of spare cores that can be used to replace defective units. Unlike previous approaches that required massive redundancy overhead, our architecture achieves high yield with minimal spare capacity through intelligent routing.</p>
<h3>A wafer scale walkthrough</h3>
<p>Defect tolerance at a chip level is fairly clear. Let’s now compare how a traditional GPU and a wafer-scale chip would yield using TSMC 5nm’s 300mm wafer:</p>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01.png" width="2146" height="1144" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01.png 2146w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-300x160.png 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-1024x546.png 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-768x409.png 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-1536x819.png 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-2048x1092.png 2048w, https://cerebras.ai/wp-content/uploads/2025/01/100x-GPUs-per-wafer-01-350x187.png 350w" sizes="(max-width: 2146px) 100vw, 2146px"/></p>
					</div>
				</div></div></div></div></div><div><p>On the left is a H100-like GPU: it is 814mm<sup>2</sup>, it has 144 fault tolerant cores, and a single 300mm wafer yields 72 full die chips. On the right we have the Cerebras Wafer Scale Engine 3. It’s one giant square measuring 46,225mm<sup>2</sup>. It has 970,000 fault tolerant cores. One wafer yields one chip.</p>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-scaled.jpg" width="2560" height="1071" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-scaled.jpg 2560w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-300x125.jpg 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-1024x428.jpg 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-768x321.jpg 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-1536x642.jpg 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-2048x856.jpg 2048w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-02-350x146.jpg 350w" sizes="(max-width: 2560px) 100vw, 2560px"/></p>
					</div>
				</div></div></div></div></div><div><p>At the current TSMC 5nm node, TSMC’s process <a href="https://www.anandtech.com/show/16028/better-yield-on-5nm-than-7nm-tsmc-update-on-defect-rates-for-n5"><span>reportedly</span></a> has ~0.001 defect per mm<sup>2</sup>. 72 GPU dies have total die area of 58,608mm<sup>2</sup>. Applying this defect density, this area would see a total of 59 defects. For simplicity, let’s assume each defect lands on a separate core. At 6.2mm<sup>2</sup> per core, this means 361mm<sup>2</sup> of die space would be lost of defects.</p>
<p>On the Cerebras side, the effective die size is a bit smaller at 46,225mm<sup>2</sup>. Applying the same defect rate, the WSE-3 would see 46 defects. Each core is 0.05mm<sup>2</sup>. This means 2.2mm<sup>2</sup> in total would be lost to defects.</p>
<p>Measuring the total area lost, the GPU in this case loses 164x more silicon area than the Wafer Scale Engine on an apple-to-apples basis on the same manufacturing node and defect rate.</p>
<p>The above makes a high-level point but simplifies a few details. First, not all areas of the chip are taken by the compute cores. Caches, memory controllers, and on-chip fabric take up a substantial amount of die size, perhaps up to 50%. However, these components can be designed to be fault tolerant in their own way. An H100 SM is likely smaller than 6.2mm<sup>2</sup> due to these components, though not by an order of magnitude. Second, a cluster of defects could overwhelm fault tolerant areas and disable the whole chip. In practice, even fault tolerant chips will not yield close to 100%. These caveats aside, the general rule that smaller cores make for greater fault tolerance still holds.</p>
<h3>Putting Cerebras in the Table</h3>
</div><div><div><div><div><div><div><p><img decoding="async" src="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-scaled.jpg" width="2560" height="1029" alt="" srcset="https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-scaled.jpg 2560w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-300x121.jpg 300w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-1024x412.jpg 1024w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-768x309.jpg 768w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-1536x617.jpg 1536w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-2048x823.jpg 2048w, https://cerebras.ai/wp-content/uploads/2025/01/100x-tolerance-table-03-350x141.jpg 350w" sizes="(max-width: 2560px) 100vw, 2560px"/></p>
					</div>
				</div></div></div></div></div><div><p>Let’s revisit the first table, now with the Cerebras Wafer Scale Engine added. Like Nvidia’s data center GPUs, the WSE-3 is designed to be fault-tolerant and disables a portion of its cores to manage yield. Because our cores are so tiny, the number of cores is so much larger – 970,000 physical cores with 900,000 active on our current shipping product. This provides tremendous, fine grained, defect tolerance. Despite having built the world’s largest chip, we enable 93% of our silicon area, which is higher than the leading GPU today.</p>
<p>To summarize, Cerebras resolved the wafer scale manufacturing challenge by designing a small fault-tolerant core in combination with a fault tolerant on-chip fabric. While total chip area increased by ~50x compared to conventional GPUs, we reduced individual core size by ~100x. As a result, defects are far less damaging to the WSE than conventional multi-core processors. The third generation WSE engine achieves 93% silicon utilization – the highest among leading AI accelerators – demonstrating that wafer-scale computing is not just possible, but commercially viable at scale.</p>
</div></div></div></div></div></div></div></div></div></div>
  </body>
</html>
