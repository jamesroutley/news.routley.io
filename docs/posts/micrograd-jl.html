<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://liorsinai.github.io/machine-learning/2024/07/27/micrograd-1-chainrules.html">Original</a>
    <h1>Micrograd.jl</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p><em>A series on automatic differentiation in Julia. Part 1 provides an overview and defines explicit chain rules.</em></p>

<p>This is part of a series. The other articles are:</p>
<ul>
  <li><a href="https://liorsinai.github.io/machine-learning/2024/08/03/micrograd-2-expr">Part 2: Automation with expressions</a>.</li>
  <li><a href="https://liorsinai.github.io/machine-learning/2024/08/10/micrograd-3-ir">Part 3: Automation with IR</a>.</li>
  <li><a href="https://liorsinai.github.io/machine-learning/2024/08/17/micrograd-4-ext">Part 4: Extensions</a>.</li>
  <li><a href="https://liorsinai.github.io/machine-learning/2024/08/19/micrograd-5-mlp">Part 5: MLP</a>.</li>
</ul>

<p>All source code can be found at <a href="https://github.com/LiorSinai/MicroGrad.jl">MicroGrad.jl</a>.</p>

<h3 id="table-of-contents">Table of Contents</h3>

<nav id="toc"></nav>


<h2 id="introduction">1 Introduction</h2>

<p>A major convenience of modern machine learning frameworks is automatic differentiation (AD).
Training a machine learning model typically consist of two steps, a forward pass and a backwards pass.
The forward pass takes an input sample and calculates the result. 
Examples include a label in a classifier model or a word or image in a generative model.
In the backward pass, the result is compared to a ground truth sample and the error is backpropagated throughout the model, from the final layers through to the start.
Backpropagation is driven by gradients which are calculated with the differentiation rules of Calculus.</p>

<p>With modern machine learning frameworks, such as <a href="https://pytorch.org/">PyTorch</a> or <a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a>, only the forward pass needs to be defined and they will automatically generate the backward pass. This (1) makes them easier to use and (2) enforces consistency between the forward pass and backward pass.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/micrograd/moons_decision_boundary.png" alt="Decision boundary"/>
<figcaption>The probability boundaries of a multi-layer perceptron trained on the moons dataset with MicroGrad.jl.</figcaption>
</figure>

<p>Andrej Kaparthy made an excellent <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">video</a> where he built a minimal automatic differentiation module called <a href="https://github.com/karpathy/micrograd">Micrograd</a> in Python.
This is the first video in his <a href="https://karpathy.ai/zero-to-hero.html">Zero to Hero</a> series.
He later uses it to train a multi-layer perceptron model.
I highly recommend it for anyone who wants to understand backpropagation.</p>

<p>The aim of this series is to create a minimal automatic differentiation package in Julia.
It is based on <a href="https://fluxml.ai/Zygote.jl/stable/">Zygote.jl</a> and works very differently to the Python AD packages.
The latter are based on objects with their own custom implementations of mathematical operations that calculate both the forward and backward passes.
All operations are only done with these objects.<sup id="fnref:micrograd" role="doc-noteref"><a href="#fn:micrograd" rel="footnote">1</a></sup>
Zygote.jl is instead based on the principle that Julia is a functional programming language. 
It utilises Julia’s multiple dispatch feature and its comprehensive metaprogramming abilities to generate new code for the backward pass.
Barring some limitations, it can be used to differentiate all existing functions as well as any custom code.</p>

<p>Zygote’s approach is complex and pushes the boundaries of Julia’s metaprogramming. It can sometimes be <a href="https://discourse.julialang.org/t/state-of-machine-learning-in-julia/74385/4#post_4">buggy</a>.
However its promise is true automatic differentiation of any forward pass code without further work on the coder’s part.</p>

<p>For the final code, see my <a href="https://github.com/LiorSinai/MicroGrad.jl">MicroGrad.jl</a> repository.
It is very versatile but has several limitations, including less code coverage than Zygote.jl and it is unable to handle control flow or keyword arguments.</p>

<p>There are almost no comprehensive tutorials on AD in Julia and so this series aims to cover that gap.
A good understanding of Julia and of Calculus is required.</p>



<p>The Julia automatic differentiation ecosystem is centered around three packages: Flux.jl, ChainRules.jl and Zygote.jl.</p>
<ul>
  <li><a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a> is a machine learning framework. It uses either ChainRules.jl or Zygote.jl to differentiate code.</li>
  <li><a href="https://fluxml.ai/Zygote.jl/stable/">Zygote.jl</a> implements automatic differentiation through metaprogramming.
    <ul>
      <li>The core functionality is defined in the minimal <a href="https://github.com/FluxML/ZygoteRules.jl">ZygoteRules.jl</a> package.</li>
      <li>The main functions it exposes are <code>gradient</code>, <code>withgradient</code> and <code>pullback</code>. The <code>pullback</code> function is a light wrapper around <code>_pullback</code> which does most of the heavy lifting.</li>
      <li>The goal of <code>_pullback</code> is to dispatch a function, its arguments and its keyword arguments to a <code>ChainRule.rrule</code>. If it cannot, it will inspect the code, decompose it into smaller steps, and follow the rules of differentiation to dispatch  each of those to <code>_pullback</code> to recursively find an <code>rrule</code>. If this recursive process does not find a valid rule it will raise an error.</li>
    </ul>
  </li>
  <li><a href="https://juliadiff.org/ChainRulesCore.jl/stable/">ChainRules.jl</a> defines forward rules and reverse rules.
    <ul>
      <li>The core functionality is defined in the minimal <a href="https://github.com/JuliaDiff/ChainRulesCore.jl">ChainRulesCore.jl</a> package.</li>
      <li>The main functions it exposes are <code>frule</code> and <code>rrule</code>. This series deals only with backpropagation, so it will only concentrate on <code>rrule</code>.</li>
    </ul>
  </li>
</ul>

<p>Also important is <a href="https://fluxml.ai/IRTools.jl/latest/">IRTools.jl</a>, an extended metaprogramming package for working with an intermediate representation (IR) between raw Julia code and lowered code.
MicroGrad.jl in particular is based on the example code at <a href="https://github.com/FluxML/IRTools.jl/blob/master/examples/reverse.jl">IRTools.jl</a> with alignment with Zyogte.jl functions and names.</p>

<p>As an example, consider the function $f(x) = \sin(\cos(x))$. Using the chain rule of Calculus, it is differentiated as:</p><p>

\[\begin{align}
\frac{df}{dx} &amp;= \frac{df}{dh}\frac{dh}{dx} \quad ; h(x)=cos(x)\\
              &amp;= \frac{d}{dh}\sin(h)\frac{d}{dx}\cos(x) \\
              &amp;= \cos(h)(-\sin(x)) \\
              &amp;= -\cos(\cos(x))\sin(x)
\end{align}\]

</p><p><code>Zygote.withgradient</code>, exposed as <code>Flux.withgradient</code>, can be used to calculate this:</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>Flux</span>
<span>f</span><span>(</span><span>x</span><span>)</span> <span>=</span> <span>sin</span><span>(</span><span>cos</span><span>(</span><span>x</span><span>))</span>
<span>y</span><span>,</span> <span>grad</span> <span>=</span> <span>Flux</span><span>.</span><span>withgradient</span><span>(</span><span>f</span><span>,</span> <span>0.9</span><span>)</span> <span># 0.5823, (-0.6368,)</span>
<span>grad</span><span>[</span><span>1</span><span>]</span> <span>==</span> <span>-</span><span>cos</span><span>(</span><span>cos</span><span>(</span><span>0.9</span><span>))</span><span>*</span><span>sin</span><span>(</span><span>0.9</span><span>)</span> <span># true</span></code></pre></figure>

<p>More commonly we differentiate with respect to the model, not the data:</p>

<figure><pre><code data-lang="julia"><span>y</span><span>,</span> <span>grad</span> <span>=</span> <span>Flux</span><span>.</span><span>withgradient</span><span>(</span><span>m</span><span>-&gt;</span><span>m</span><span>(</span><span>0.9</span><span>),</span> <span>f</span><span>)</span> <span># 0.5823, (nothing,)</span></code></pre></figure>

<p>This is more useful for a model with parameters. For example a dense, fully connected layer:</p>

<figure><pre><code data-lang="julia"><span>model</span> <span>=</span> <span>Dense</span><span>(</span><span>3</span><span>=&gt;</span><span>1</span><span>)</span>
<span>x</span> <span>=</span> <span>rand</span><span>(</span><span>Float32</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>)</span>
<span>y</span><span>,</span> <span>grad</span> <span>=</span> <span>Flux</span><span>.</span><span>withgradient</span><span>(</span><span>m</span><span>-&gt;</span><span>sum</span><span>(</span><span>m</span><span>(</span><span>x</span><span>)),</span> <span>model</span><span>)</span> <span># 1.5056f0, ((weight=[4.9142 6.235 5.3379],bias=Fill(10.0f0,1),σ=nothing),)</span></code></pre></figure>

<p>The aim of the rest of the series is to recreate this functionality.
This first part will focus solely on ChainRules.jl and recreating the <code>rrule</code> function.
Part 2 will focus on recreating the <code>Zygote._pullback</code> function.
Part 3 repeats part 2 in a more robust manner.
Part 4 extends part 3’s solution to handle maps, anonymous functions and structs.
Finally part 5 shows how this AD code can be used by a machine learning framework.</p>

<h2 id="chainrules">3 ChainRules</h2>
<h3 id="chainrules-definition">3.1 Definition</h3>

<p>ChainRules.jl’s <code>rrule</code> returns the output of the forward pass $y(x)$ and a function $\mathcal{B}$ which calculates the backward pass.
$\mathcal{B}$ takes as input $\Delta = \frac{\partial l}{\partial y}$, the gradient of some scalar $l$ with regards to the output variable $y$, and returns a tuple of $\left(\frac{\partial l}{\partial \text{self}}, \frac{\partial l}{\partial x_1}, …, \frac{\partial l}{\partial x_n}\right)$, the gradient of $l$ with regards to each of the input variables $x_i$.
(The extra gradient $\frac{\partial l}{\partial \text{self}}$ is needed for internal fields and closures.
See the <code>Dense</code> layer example above.)
According to the chain rule of Calculus, each gradient is calculated as:</p><p>

\[\mathcal{B_i}\left(\frac{\partial l}{\partial y}\right) = \frac{\partial l}{\partial x_i} = \frac{\partial l}{\partial y} \frac{\partial y}{\partial x_i}\]

</p><p>As a starting point $\frac{\partial l}{\partial y}=1$ is used to evaluate only $\frac{\partial y}{\partial x}$.</p>

<p>If $x$ and $y$ are vectors, then the gradient $J=\frac{\partial y}{\partial x}$ is a <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>:</p><p>

\[J = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \dots &amp; \frac{\partial y_1}{\partial x_n}  \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1} &amp; \dots &amp; \frac{\partial y_m}{\partial x_n} 
\end{bmatrix}\]

</p><p>To maintain the correct order, we need to use the <a href="https://juliadiff.org/ChainRulesCore.jl/stable/maths/propagators.html">conjugate transpose (adjoint) of the Jacobian</a>. So each gradient is calculated as:</p><p>

\[\mathcal{B_i}(\Delta) = J_i^{\dagger} \Delta\]

</p><p>Note the Jacobian does not need to be explicitly calculated; only the product needs to be. 
This is can be useful when coding the <code>rrule</code> for matrix functions.
See the section on the chain rule for <a href="#chainrules-matrix-multiplication">matrix multiplication</a> later.</p>

<p>To start, define a default fallback for <code>rrule</code> that returns <code>nothing</code> for any function with any number of arguments (<a href="https://github.com/JuliaDiff/ChainRulesCore.jl/blob/a95c181c662ead23aaf9904b8a560bebeb9022a3/src/rules.jl#L131">source</a>):</p>

<figure><pre><code data-lang="julia"><span>rrule</span><span>(</span><span>::</span><span>Any</span><span>,</span> <span>::</span><span>Vararg</span><span>{</span><span>Any</span><span>})</span> <span>=</span> <span>nothing</span></code></pre></figure>

<p>An <code>rrule</code> can now be defined for any function.
For it to be really useful <code>rrule</code> must cover a large set of functions.
Thankfully ChainRules.jl provides us with that.
However in this post I’ll only work through a limited set of examples.</p>

<h3 id="chainrules-arithmetic">3.2 Arithmetic</h3>

<p>The derivatives of adding two variables is:</p><p>

\[\frac{\partial}{\partial x}(x+y) = 1 + 0; \frac{\partial}{\partial y}(x+y) = 0 + 1\]

</p><p>
  <a data-toggle="collapse" href="#proof-derivative-addition" role="button" aria-expanded="false" aria-controls="collapse-derivative-addition">
    Proof ⇩
  </a>
</p>
<div id="proof-derivative-addition">
  <p>
    $$
    \begin{align}
    \Delta f_x &amp;= (x+\Delta x+ y) - (x+y) \\
    \therefore \lim_{\Delta x \to 0}\frac{\Delta f_x}{\Delta x} &amp;=\frac{\partial f}{\partial x}= 1 \\
    \therefore \lim_{\Delta y \to 0}\frac{\Delta f_y}{\Delta y} &amp;=\frac{\partial f}{\partial y}= 1
    \end{align}
    $$
    </p>
</div>

<p>There are no internal fields so $\frac{\partial l}{\partial \text{self}}$ is <code>nothing</code>.
$\mathcal{B}$ can be returned as an anonymous function, but giving it the name <code>add_back</code>  helps with debugging (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/fastmath_able.jl#L167">source</a>).</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>+</span><span>),</span> <span>x</span><span>::</span><span>Real</span><span>,</span> <span>y</span><span>::</span><span>Real</span><span>)</span>
    <span>add_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>true</span> <span>*</span> <span>Δ</span><span>,</span> <span>true</span> <span>*</span> <span>Δ</span><span>)</span> <span># ∂self, ∂x, ∂y</span>
    <span>x</span> <span>+</span> <span>y</span><span>,</span> <span>add_back</span> <span># also (Δ) -&gt; (nothing, true * Δ, true * Δ)</span>
<span>end</span></code></pre></figure>

<p>Usage:</p>

<figure><pre><code data-lang="julia"><span>z</span><span>,</span> <span>back</span> <span>=</span> <span>rrule</span><span>(</span><span>+</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>)</span> <span># (3, var&#34;#add_back#&#34;())</span>
<span>back</span><span>(</span><span>1.2</span><span>)</span> <span># (nothing, 1.2, 1.2)</span></code></pre></figure>

<p>Subtraction is almost identical:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>-</span><span>),</span> <span>x</span><span>::</span><span>Real</span><span>,</span> <span>y</span><span>::</span><span>Real</span><span>)</span>
    <span>minus_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>true</span> <span>*</span> <span>Δ</span><span>,</span> <span>-</span><span>1</span> <span>*</span> <span>Δ</span><span>)</span> <span># ∂f, ∂x, ∂y</span>
    <span>x</span> <span>-</span> <span>y</span><span>,</span> <span>minus_back</span>
<span>end</span></code></pre></figure>

<p>With multiplication, the incoming gradient is multiplied by the other variable:</p><p>

\[\frac{\partial}{\partial x}(xy) = y; \frac{\partial}{\partial y}(xy) = x\]

</p><p>
  <a data-toggle="collapse" href="#proof-derivative-multiplication" role="button" aria-expanded="false" aria-controls="collapse-derivative-multiplication">
    Proof ⇩
  </a>
</p>
<div id="proof-derivative-multiplication">
  <p>
    $$
    \begin{align}
    \Delta f_x &amp;= (x+\Delta x)y - xy \\
    \therefore \lim_{\Delta x \to 0}\frac{\Delta f_x}{\Delta x} &amp;=\frac{\partial f}{\partial x}= y \\
    \therefore \lim_{\Delta y \to 0}\frac{\Delta f_y}{\Delta y} &amp;=\frac{\partial f}{\partial y}= x
    \end{align}
    $$
    </p>
</div>

<p>In code (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/fastmath_able.jl#L254">source</a>):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>*</span><span>),</span> <span>x</span><span>::</span><span>Real</span><span>,</span> <span>y</span><span>::</span><span>Real</span><span>)</span>
    <span>times_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>y</span> <span>*</span> <span>Δ</span><span>,</span> <span>x</span> <span>*</span> <span>Δ</span><span>)</span> <span># ∂self, ∂x, ∂y</span>
    <span>x</span> <span>*</span> <span>y</span><span>,</span> <span>times_back</span>
<span>end</span></code></pre></figure>

<p>Note that Julia will create a <em>closure</em> around the incoming <code>x</code> and <code>y</code> variables for <code>times_back</code>.
A closure is when the function stores the values of variables from its parents scope (it closes over the variables).
In other words, <code>x</code> and <code>y</code> will become constants in the <code>times_back</code> scope.
In this way, the <code>times_back</code> function will always “remember” what values it was called with:</p>

<p>Example:</p>

<figure><pre><code data-lang="julia"><span>z</span><span>,</span> <span>back</span> <span>=</span> <span>rrule</span><span>(</span><span>*</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span> <span># (6, var&#34;#times_back#4&#34;{Int64, Int64}(2, 3))</span>
<span>back</span><span>.</span><span>x</span> <span># 2</span>
<span>back</span><span>.</span><span>y</span> <span># 3</span>
<span>back</span><span>(</span><span>1.2</span><span>)</span> <span># (nothing, 3.6, 2.4)</span></code></pre></figure>

<p>Every call to <code>rrule</code> with <code>*</code> will return a different <code>back</code> instance based on the input arguments.</p>

<p>Division is slightly different in that the derivatives look different for $x$ and $y$:</p><p>

\[\frac{\partial}{\partial x}\frac{x}{y} = \frac{1}{y}; \frac{\partial}{\partial y}\frac{x}{y}= -\frac{x}{y^2}\]

</p><p>
  <a data-toggle="collapse" href="#proof-derivative-division" role="button" aria-expanded="false" aria-controls="collapse-derivative-division">
    Proof ⇩
  </a>
</p>
<div id="proof-derivative-division">
  <p>
    $$
    \begin{align}
    \Delta f_x &amp;= \frac{x+\Delta x}{y} - \frac{x}{y} \\
    \therefore \lim_{\Delta x \to 0}\frac{\Delta f_x}{\Delta x} &amp;=\frac{\partial f}{\partial x}= \frac{1}{y} \\
    \Delta f_y &amp;= \frac{x}{y+\Delta y} - \frac{x}{y} \\
             &amp;= \frac{xy}{y(y+\Delta y)} - \frac{x(y+\Delta y)}{y(y+\Delta y)} \\
             &amp;= -\frac{x \Delta y}{y(y+\Delta y)} \\
   \therefore \lim_{\Delta y \to 0}\frac{\Delta f_y}{\Delta y} &amp;=\frac{\partial f}{\partial y} = -\frac{x}{y^2}         
    \end{align}
    $$
    </p>
</div>

<p>Here we can calculate an internal variable <code>Ω</code> to close over, and use it for the $\frac{\partial}{\partial y}$ derivative (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/fastmath_able.jl#L169">source</a>):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>/</span><span>),</span> <span>x</span><span>::</span><span>Real</span><span>,</span> <span>y</span><span>::</span><span>Real</span><span>)</span>
    <span>Ω</span> <span>=</span> <span>x</span> <span>/</span> <span>y</span>
    <span>divide_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>1</span> <span>/</span> <span>y</span> <span>*</span> <span>Δ</span><span>,</span> <span>-</span><span>Ω</span><span>/</span><span>y</span> <span>*</span> <span>Δ</span><span>)</span> <span># ∂self, ∂x, ∂y</span>
    <span>Ω</span><span>,</span> <span>divide_back</span>
<span>end</span></code></pre></figure>

<p>Example:</p>

<figure><pre><code data-lang="julia"><span>z</span><span>,</span> <span>back</span> <span>=</span> <span>rrule</span><span>(</span><span>/</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>)</span> <span># (0.6667, var&#34;#divide_back#5&#34;{Int64, Float64}(3, 0.6667))</span>
<span>back</span><span>.</span><span>Ω</span> <span># 0.6667</span>
<span>back</span><span>.</span><span>y</span> <span># 3</span>
<span>back</span><span>.</span><span>x</span> <span># ERROR</span>
<span>back</span><span>(</span><span>1.2</span><span>)</span> <span># (nothing, 0.4, -0.2667)</span></code></pre></figure>

<h3 id="chainrules-trigonometry">3.3 Trigonometry</h3>

<p>The derivatives of $\sin$ and $\cos$ are:</p><p>

\[\begin{align}
  \frac{\partial}{\partial x} \sin(x) &amp;= \cos(x) \\
  \frac{\partial}{\partial x} \cos(x) &amp;= -\sin(x)
\end{align}\]

</p><p>Because both use $\sin$ and $\cos$, we can use <code>sincos</code> to calculate both simultaneously and more efficiently than calculating each on its own. This shows the advantage of calculating the forward pass and backward pass at the same time (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/fastmath_able.jl#L12">source</a>):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>sin</span><span>),</span> <span>x</span><span>::</span><span>Number</span><span>)</span>
    <span>s</span><span>,</span> <span>c</span> <span>=</span> <span>sincos</span><span>(</span><span>x</span><span>)</span>
    <span>sin_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>Δ</span> <span>*</span> <span>c</span><span>)</span> <span># ∂self, ∂x</span>
    <span>s</span><span>,</span> <span>sin_back</span>
<span>end</span>

<span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>cos</span><span>),</span> <span>x</span><span>::</span><span>Number</span><span>)</span>
    <span>s</span><span>,</span> <span>c</span> <span>=</span> <span>sincos</span><span>(</span><span>x</span><span>)</span>
    <span>cos_back</span><span>(</span><span>Δ</span><span>)</span> <span>=</span> <span>(</span><span>nothing</span><span>,</span> <span>-</span><span>Δ</span> <span>*</span> <span>s</span><span>)</span> <span># ∂self, ∂x</span>
    <span>c</span><span>,</span> <span>cos_back</span>
<span>end</span></code></pre></figure>

<p>Let’s now revisit the example from earlier, $f(x) = \sin(\cos(x))$.
We have the forward pass:</p><p>

\[\begin{align}
y_1 &amp;= \cos(x) \\
y_2 &amp;= \sin(y_1)\\
\end{align}\]

</p><p>And the backwards pass:</p><p>

\[\begin{align}
\frac{\partial y_2}{\partial y_1} &amp;= (1.0)  \frac{\partial}{\partial y_1} \sin(y_1) \\
            &amp;= \cos(y_1) \\
\frac{\partial y_2}{\partial x} &amp;= \frac{\partial y_2}{\partial y_1} \frac{\partial}{\partial x} \cos(x) \\
         &amp;= -\Delta_2 \sin(x)
\end{align}\]

</p><p>In code:</p>

<figure><pre><code data-lang="julia"><span>x</span> <span>=</span> <span>0.9</span>
<span>y1</span><span>,</span> <span>back1</span> <span>=</span> <span>rrule</span><span>(</span><span>cos</span><span>,</span> <span>x</span><span>)</span> <span># (0.6216, cos_back)</span>
<span>y2</span><span>,</span> <span>back2</span> <span>=</span> <span>rrule</span><span>(</span><span>sin</span><span>,</span> <span>y1</span><span>)</span> <span># (0.5823, sin_back)</span>
<span>grad_sin</span><span>,</span> <span>grad_y1</span> <span>=</span> <span>back2</span><span>(</span><span>1.0</span><span>)</span> <span># (nothing, 0 .8129)</span>
<span>grad_cos</span><span>,</span> <span>grad_x</span> <span>=</span> <span>back1</span><span>(</span><span>grad_y1</span><span>)</span> <span># (nothing, -0.6368)</span>
<span>grad_x</span> <span>==</span> <span>-</span><span>cos</span><span>(</span><span>cos</span><span>(</span><span>x</span><span>))</span><span>*</span><span>sin</span><span>(</span><span>x</span><span>)</span> <span># true</span></code></pre></figure>

<h3 id="chainrules-polynomial">3.4 Polynomials</h3>

<p>The next section will showcase an example of polynomial curve fitting.
This requires an <code>rrule</code> for the <code>evalpoly</code> function.</p>

<p>For a general polynomial:</p><p>

\[y = a_0 + a_1x + a_2x^2 + ... + a_n x^n\]

</p><p>The derivatives are:</p><p>

\[\begin{align}
\frac{\partial y}{\partial x} &amp;= 0 + a_1 + 2a_2x^1 + ... + n a_n x^{n-1} \\
\frac{\partial y}{\partial a_i} &amp;= 0 + ... + x^{i} + ... + 0
\end{align}\]

</p><p>For the most efficient implementation, the powers of $x$ can be calculated for both the forward and backwards pass at the same time.
For simplicity, I’m not going to do that (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/evalpoly.jl">source</a>):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>evalpoly</span><span>),</span> <span>x</span><span>,</span> <span>coeffs</span><span>::</span><span>AbstractVector</span><span>)</span>
    <span>y</span> <span>=</span> <span>evalpoly</span><span>(</span><span>x</span><span>,</span> <span>coeffs</span><span>)</span>
    <span>function</span><span> evalpoly_back</span><span>(</span><span>Δ</span><span>)</span>
        <span>xpow</span> <span>=</span> <span>one</span><span>(</span><span>x</span><span>)</span>
        <span>dp</span> <span>=</span> <span>similar</span><span>(</span><span>coeffs</span><span>,</span> <span>typeof</span><span>(</span><span>xpow</span> <span>*</span> <span>Δ</span><span>))</span>
        <span>dx</span> <span>=</span> <span>zero</span><span>(</span><span>x</span><span>)</span>
        <span>for</span> <span>i</span> <span>in</span> <span>eachindex</span><span>(</span><span>coeffs</span><span>)</span>
            <span>dp</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>Δ</span> <span>*</span> <span>xpow</span>
            <span>dx</span> <span>+=</span> <span>(</span><span>i</span><span>-</span><span>1</span><span>)</span> <span>*</span> <span>coeffs</span><span>[</span><span>i</span><span>]</span> <span>*</span> <span>xpow</span> <span>/</span> <span>x</span> <span>*</span> <span>Δ</span>
            <span>xpow</span> <span>*=</span> <span>x</span>
        <span>end</span>
        <span>return</span> <span>nothing</span><span>,</span> <span>dx</span><span>,</span> <span>dp</span>
    <span>end</span>
    <span>y</span><span>,</span> <span>evalpoly_back</span>
<span>end</span></code></pre></figure>

<p>Usage:</p>

<figure><pre><code data-lang="julia"><span>y</span><span>,</span> <span>back</span> <span>=</span> <span>rrule</span><span>(</span><span>evalpoly</span><span>,</span> <span>1.2</span><span>,</span> <span>[</span><span>2.0</span><span>,</span> <span>0.0</span><span>,</span> <span>3.0</span><span>,</span> <span>4.0</span><span>])</span> <span># 13.232, evalpoly_back</span>
<span>back</span><span>(</span><span>1.0</span><span>)</span> <span># (nothing, 24.48, [1.0, 1.2, 1.44, 1.728]) </span></code></pre></figure>

<h3 id="chainrules-matrix-multiplication">3.5 Matrix multiplication</h3>

<p>For some scaler loss function $l$, we can calculate a derivative $\Delta=\frac{\partial l}{\partial Y}$
against some matrix $Y$. Then for $Y=AB$, the partial derivatives are:</p><p>

\[\begin{align}
\frac{\partial l}{\partial A} &amp;= \frac{\partial Y}{\partial A} \frac{\partial L}{\partial Y} \\
                              &amp;= \Delta B^T \\
\frac{\partial l}{\partial B} &amp;= \frac{\partial Y}{\partial B} \frac{\partial L}{\partial Y} \\
                              &amp;= A^T \Delta
\end{align}\]

</p><p>Note that the Jacobians $\frac{\partial Y}{\partial A}$ and $\frac{\partial Y}{\partial B}$ are not explicitly calculated here; only the product is. (These Jacobians would have many zeros because each output element depends only on a small subset of the input elements.)</p>

<div>
	
  <div>
    <p>
      The most common use case in machine learning is $Y=WX$, where $W$ is a set of weights and $X$ is the data.
      Machine learning algorithms only alter the weights, not the data. Hence only $\frac{\partial l}{\partial W}$ is required.
      This means computation is wasted on $\frac{\partial l}{\partial X}$.
      For large matrices, this can be significant.
      To avoid this ChainRules.jl uses the <code>ChainRulesCore.@thunk</code> macro to wrap code in a <code>ChainRulesCore.Thunk</code> struct. This struct defers computation until it is used. 
      If it is not used, the computation is not run.
    </p>
  </div>
</div>

<p>In code (<a href="https://github.com/JuliaDiff/ChainRules.jl/blob/dba6cb57d73ba837c5ab6fd1f968f3a5d301ca9c/src/rulesets/Base/arraymath.jl#L27">source</a>):</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>*</span><span>),</span> <span>A</span><span>::</span><span>AbstractVecOrMat</span><span>{</span><span>&lt;:</span><span>Real</span><span>},</span> <span>B</span><span>::</span><span>AbstractVecOrMat</span><span>{</span><span>&lt;:</span><span>Real</span><span>})</span>
    <span>function</span><span> times_back</span><span>(</span><span>Δ</span><span>)</span>
        <span>dA</span> <span>=</span> <span>Δ</span> <span>*</span> <span>B</span><span>&#39;</span>
        <span>dB</span> <span>=</span> <span>A</span><span>&#39;</span> <span>*</span> <span>Δ</span>
        <span>return</span> <span>(</span><span>nothing</span><span>,</span> <span>dA</span><span>,</span> <span>dB</span><span>)</span>
    <span>end</span>
    <span>A</span> <span>*</span> <span>B</span><span>,</span> <span>times_back</span>
<span>end</span></code></pre></figure>

<p>Test:</p>

<figure><pre><code data-lang="julia"><span>A</span><span>,</span> <span>B</span> <span>=</span> <span>rand</span><span>(</span><span>2</span><span>,</span> <span>4</span><span>),</span> <span>rand</span><span>(</span><span>4</span><span>,</span> <span>3</span><span>)</span>
<span>C</span><span>,</span> <span>back</span> <span>=</span> <span>rrule</span><span>(</span><span>*</span><span>,</span> <span>A</span><span>,</span> <span>B</span><span>)</span> <span># (2×3 Matrix{Float64}, times_back)</span>
<span>back</span><span>(</span><span>ones</span><span>(</span><span>2</span><span>,</span> <span>3</span><span>))</span> <span># (nothing, 2×4 Matrix, 4×3 Matrix)</span></code></pre></figure>

<h3 id="chainrules-mse">3.6 MSE</h3>

<p>The mean square error (MSE) is a common loss function in machine learning.
It will be used shortly for polynomial curve fitting.
It is:</p><p>

\[MSE(\hat{y}, y) = \frac{1}{n}\sum^n_{i=1} (\hat{y}_i - y_i)^2\]

</p><p>with derivatives:</p><p>

\[\begin{align}
  \frac{\partial MSE}{\partial \hat{y}_i} &amp;= \frac{1}{n}(0 + ... + 2(\hat{y}_i - y_i) + ... + 0) \\
        &amp;= \frac{2(\hat{y}_i - y_i)}{n} \\
  \frac{\partial MSE}{\partial y_i} &amp;= \frac{1}{n}(0 + ... - 2(\hat{y}_i - y_i) + ... + 0) \\
       &amp;= -\frac{2(\hat{y}_i - y_i)}{n}
\end{align}\]

</p><p>In code it is:</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>StatsBase</span>
<span>mse</span><span>(</span><span>ŷ</span><span>::</span><span>AbstractVecOrMat</span><span>,</span> <span>y</span><span>::</span><span>AbstractVecOrMat</span><span>)</span> <span>=</span> <span>mean</span><span>(</span><span>abs2</span><span>.</span><span>(</span><span>ŷ</span> <span>-</span> <span>y</span><span>))</span></code></pre></figure>

<p>Flux.jl does not define an <code>rrule</code> for its <code>mse</code> because it can be decomposed into functions which already have an <code>rrule</code> (<code>-</code>, <code>broadcast</code>, <code>abs2</code> and <code>mean</code>). 
However since we don’t have <code>rrule</code>s for these parts and have not yet automated decomposition, it is simplest to create an <code>rrule</code> for the entire function:</p>

<figure><pre><code data-lang="julia"><span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>mse</span><span>),</span> <span>ŷ</span><span>::</span><span>AbstractVecOrMat</span><span>,</span> <span>y</span><span>::</span><span>AbstractVecOrMat</span><span>)</span>
    <span>Ω</span> <span>=</span> <span>mse</span><span>(</span><span>ŷ</span><span>,</span> <span>y</span><span>)</span>
    <span>function</span><span> mse_back</span><span>(</span><span>Δ</span><span>)</span>
        <span>c</span> <span>=</span> <span>2</span> <span>*</span> <span>(</span><span>ŷ</span> <span>-</span> <span>y</span><span>)</span> <span>/</span> <span>length</span><span>(</span><span>y</span><span>)</span> <span>*</span> <span>Δ</span>
        <span>return</span> <span>nothing</span><span>,</span> <span>c</span><span>,</span> <span>-</span><span>c</span> <span># ∂self, ∂ŷ, ∂y</span>
    <span>end</span>
    <span>Ω</span><span>,</span> <span>mse_back</span>
<span>end</span></code></pre></figure>

<p>The <code>mse</code> can also be applied per individual data point and summed up separately.
This form is not common but will be useful for explanatory purposes in the polynomial curve fitting section:</p>

<figure><pre><code data-lang="julia"><span>mse</span><span>(</span><span>ŷ</span><span>::</span><span>Number</span><span>,</span> <span>y</span><span>::</span><span>Number</span><span>,</span> <span>n</span><span>::</span><span>Int</span><span>)</span> <span>=</span> <span>abs2</span><span>(</span><span>ŷ</span> <span>-</span> <span>y</span><span>)</span><span>/</span><span>n</span>
<span>function</span><span> rrule</span><span>(</span><span>::</span><span>typeof</span><span>(</span><span>mse</span><span>),</span> <span>ŷ</span><span>::</span><span>Number</span><span>,</span> <span>y</span><span>::</span><span>Number</span><span>,</span> <span>n</span><span>::</span><span>Int</span><span>)</span>
    <span>Ω</span> <span>=</span> <span>mse</span><span>(</span><span>ŷ</span><span>,</span> <span>y</span><span>,</span> <span>n</span><span>)</span>
    <span>function</span><span> mse_back</span><span>(</span><span>Δ</span><span>)</span>
        <span>c</span> <span>=</span> <span>2</span> <span>*</span> <span>(</span><span>ŷ</span> <span>-</span> <span>y</span><span>)</span> <span>/</span> <span>n</span> <span>*</span> <span>Δ</span>
        <span>return</span> <span>nothing</span><span>,</span> <span>c</span><span>,</span> <span>-</span><span>c</span><span>,</span> <span>-</span><span>Ω</span><span>/</span><span>n</span> <span># ∂self, ∂ŷ, ∂y, ∂n</span>
    <span>end</span>
    <span>Ω</span><span>,</span> <span>mse_back</span>
<span>end</span></code></pre></figure>

<h2 id="gradient-descent">4 Gradient Descent</h2>
<h3 id="polynomial-curve-fitting">4.1 Polynomial curve fitting</h3>

<p>Gradient descent is a great algorithm to illustrate the usefulness of the code developed so far.
The toy example of fitting a polynomial to data will be used.
This is a useful example because (1) we can start with a target curve and so have ground truth values to compare and (2) this problem can be solved analytically without gradients.</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/micrograd/polyfit_data.png" alt="Polynomial with noise"/>
<figcaption></figcaption>
</figure>

<p>Here is code to create the above data:</p>

<figure><pre><code data-lang="julia"><span>using</span> <span>StatsBase</span>
<span>target_weights</span> <span>=</span> <span>[</span><span>15.0</span><span>,</span> <span>-</span><span>2.1</span><span>,</span> <span>13.9</span><span>,</span> <span>1.5</span><span>]</span>
<span>noise_factor</span> <span>=</span> <span>0.2</span>
<span>xs</span> <span>=</span> <span>(</span><span>rand</span><span>(</span><span>100</span><span>)</span> <span>.-</span> <span>0.5</span><span>)</span> <span>.*</span> <span>10</span>
<span>ys</span> <span>=</span> <span>map</span><span>(</span><span>x</span> <span>-&gt;</span> <span>evalpoly</span><span>(</span><span>x</span><span>,</span> <span>target_weights</span><span>),</span> <span>xs</span><span>)</span>
<span>scale_factor</span> <span>=</span> <span>mean</span><span>(</span><span>abs</span><span>.</span><span>(</span><span>ys</span><span>))</span>
<span>ys</span> <span>.+=</span> <span>randn</span><span>(</span><span>length</span><span>(</span><span>ys</span><span>))</span> <span>*</span> <span>scale_factor</span> <span>*</span> <span>noise_factor</span></code></pre></figure>

<p>
  <a data-toggle="collapse" href="#poly-fit-analytical" role="button" aria-expanded="false" aria-controls="collapseExample">
    Analytical least squares fitting of polynomials ⇩
  </a>
</p>
<div id="poly-fit-analytical">
  <div>
		<p> For a polynomial of order $p$, if there are exactly $n=p+1$ training samples (including for the constant $a_0$) than there exactly $n$ equations for $n$ unknowns ($a_0$,...,$a_p$) and this can be solved as an ordinary linear system:
        $$
            \begin{align}
            &amp;a_0 + a_1 x_1 + a_2x_1^2 + ... + a_p x_1^p = y_1 \\
            &amp;\vdots \\
            &amp;a_0 + a_1 x_n + a_2x_n^2 + ... + a_p x_n^p = y_n \\
            &amp;\Rightarrow \begin{bmatrix}
            1 &amp; x_1 &amp; x_1^2 &amp; \cdots &amp; x_1^p \\
            \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            1 &amp; x_n &amp; x_n^2 &amp; \cdots &amp; x_n^p
            \end{bmatrix}
            \begin{bmatrix}
            a_0 \\
            \vdots \\
            a_n
            \end{bmatrix}
            =
            \begin{bmatrix}
            y_1 \\
            \vdots \\
            y_n
            \end{bmatrix} \\
            &amp;\Rightarrow XA=Y \\
            &amp;\Rightarrow A = X^{-1}Y
            \end{align}
        $$
        Where $X^{-1}$ usually exists because $X$ is a square matrix.
        </p>
        <p>
        However usually $n &gt; p + 1$ and thus $X^{-1}$ will not exist. In that case the pseudoinverse $X^+$, also called the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse</a>, can be used instead:
        $$
            \begin{align}
            X^{+} &amp;= (X^T X)^{-1} X^T \\
            \Rightarrow A &amp;= X^{+}Y
            \end{align}
        $$
        It can be proven that this solution for $A$ minimises the least squared error.  
        </p>
        <p>
        Here is this solution in code:

</p><figure><pre><code data-lang="julia"><span>using</span> <span>LinearAlgebra</span>
<span>function</span><span> solve_poly_linear</span><span>(</span><span>order</span><span>::</span><span>Int</span><span>,</span> <span>xs</span><span>::</span><span>AbstractVector</span><span>,</span> <span>ys</span><span>::</span><span>AbstractVector</span><span>)</span>
    <span>n</span> <span>=</span> <span>length</span><span>(</span><span>xs</span><span>)</span>
    <span>X</span> <span>=</span> <span>zeros</span><span>(</span><span>n</span><span>,</span> <span>order</span> <span>+</span> <span>1</span><span>)</span>
    <span>for</span> <span>(</span><span>i</span><span>,</span> <span>x</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>xs</span><span>)</span>
        <span>xpow</span> <span>=</span> <span>1</span>
        <span>for</span> <span>j</span> <span>in</span> <span>1</span><span>:</span><span>(</span><span>size</span><span>(</span><span>X</span><span>,</span> <span>2</span><span>))</span>
            <span>X</span><span>[</span><span>i</span><span>,</span> <span>j</span><span>]</span> <span>=</span> <span>xpow</span>
            <span>xpow</span> <span>*=</span> <span>x</span>
        <span>end</span>
    <span>end</span>
    <span>pinv</span><span>(</span><span>X</span><span>)</span> <span>*</span> <span>ys</span>
<span>end</span></code></pre></figure>

        
  </div>
</div>

<p>Here is a simple version of gradient descent:</p>
<blockquote>
<u><b>Gradient descent</b></u> </blockquote>

<p>where $m_\theta$ is the model with parameters $\theta$ and $L$ is the loss function.</p>

<p>This is a Julia implementation for specifically applying the algorithm to polynomials.
The stopping condition is a maximum number of iterations, so the <code>while</code> loop has been replaced with a <code>for</code> loop.
The code also saves the loss so that the training progress can be analysed.</p>

<figure><pre><code data-lang="julia"><span>function</span><span> gradient_descent_poly!</span><span>(</span>
    <span>coeffs</span><span>::</span><span>AbstractVector</span><span>,</span>
    <span>xs</span><span>::</span><span>AbstractVector</span><span>,</span>
    <span>ys</span><span>::</span><span>AbstractVector</span>
    <span>;</span> <span>learning_rate</span><span>::</span><span>AbstractFloat</span><span>=</span><span>0.1</span><span>,</span>
    <span>max_iters</span><span>::</span><span>Integer</span><span>=</span><span>100</span>
    <span>)</span>
    <span>history</span> <span>=</span> <span>Float64</span><span>[]</span>
    <span>n</span> <span>=</span> <span>length</span><span>(</span><span>xs</span><span>)</span>
    <span>p</span> <span>=</span> <span>length</span><span>(</span><span>coeffs</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>1</span><span>:</span><span>max_iters</span>
        <span>loss_iter</span> <span>=</span> <span>0.0</span>
        <span>Δcoeffs</span> <span>=</span> <span>zeros</span><span>(</span><span>p</span><span>)</span>
        <span>for</span> <span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span> <span>in</span> <span>zip</span><span>(</span><span>xs</span><span>,</span> <span>ys</span><span>)</span>
            <span># forward</span>
            <span>ŷ</span><span>,</span> <span>back_poly</span> <span>=</span> <span>rrule</span><span>(</span><span>evalpoly</span><span>,</span> <span>x</span><span>,</span> <span>coeffs</span><span>)</span>
            <span>loss_x</span><span>,</span> <span>back_loss</span> <span>=</span> <span>rrule</span><span>(</span><span>mse</span><span>,</span> <span>ŷ</span><span>,</span> <span>y</span><span>,</span> <span>n</span><span>)</span>
            <span># reverse</span>
            <span>Δloss</span><span>,</span> <span>Δŷ</span><span>,</span> <span>Δy</span><span>,</span> <span>Δn</span> <span>=</span> <span>back_loss</span><span>(</span><span>1.0</span><span>)</span>    
            <span>Δevalpoly</span><span>,</span> <span>Δx</span><span>,</span> <span>Δcoeffs_x</span> <span>=</span> <span>back_poly</span><span>(</span><span>Δŷ</span><span>)</span>
            <span># accumulate</span>
            <span>loss_iter</span> <span>+=</span> <span>loss_x</span>
            <span>Δcoeffs</span> <span>+=</span> <span>Δcoeffs_x</span>
        <span>end</span>
        <span># update</span>
        <span>coeffs</span> <span>.-=</span> <span>learning_rate</span> <span>.*</span> <span>Δcoeffs</span>
        <span># history</span>
        <span>push!</span><span>(</span><span>history</span><span>,</span> <span>loss_iter</span><span>)</span>
    <span>end</span>
    <span>history</span>
<span>end</span></code></pre></figure>

<p>Calling the code:</p>

<figure><pre><code data-lang="julia"><span>coeffs</span> <span>=</span> <span>rand</span><span>(</span><span>4</span><span>)</span>
<span>history</span> <span>=</span> <span>gradient_descent_poly!</span><span>(</span><span>coeffs</span><span>,</span> <span>xs</span><span>,</span> <span>ys</span><span>;</span> <span>learning_rate</span><span>=</span><span>1e-5</span><span>,</span> <span>max_iters</span><span>=</span><span>2000</span><span>)</span></code></pre></figure>

<p>Plotting the history:</p>

<figure>
<img src="https://liorsinai.github.io/assets/posts/micrograd/polyfit_training.png" alt="Gradient descent training history"/>
<figcaption></figcaption>
</figure>

<p>Comparing losses on the train set:</p>

<figure><pre><code data-lang="julia"><span>ys_est</span> <span>=</span>  <span>map</span><span>(</span><span>x</span> <span>-&gt;</span> <span>evalpoly</span><span>(</span><span>x</span><span>,</span> <span>coeffs</span><span>),</span> <span>xs</span><span>)</span>
<span>mse</span><span>(</span><span>ys_est</span><span>,</span> <span>ys</span><span>)</span></code></pre></figure>

<table><thead>
  <tr>
    <th>Method</th>
    <th>Loss</th>
    <th>Coefficients</th>
  </tr></thead>
<tbody>
  <tr>
    <td>Target</td>
    <td>416.62</td>
    <td>(15.0, -2.1, 13.9, 13.9, 1.5)</td>
  </tr>
  <tr>
    <td>Analytical</td>
    <td>391.64</td>
    <td>(15.34, -3.24, 13.84, 1.46)</td>
  </tr>
  <tr>
    <td>Gradient Descent</td>
    <td>498.50</td>
    <td>(1.37, 0.54, 14.51, 1.26)</td>
  </tr>
</tbody>
</table>

<p>And finally, comparing the curves:</p>

<figure><pre><code data-lang="julia"><span>x_model</span> <span>=</span> <span>-</span><span>5</span><span>:</span><span>0.01</span><span>:</span><span>5</span>
<span>ys_model</span> <span>=</span>  <span>map</span><span>(</span><span>x</span> <span>-&gt;</span> <span>evalpoly</span><span>(</span><span>x</span><span>,</span> <span>coeffs</span><span>),</span> <span>x_model</span><span>)</span></code></pre></figure>

<figure>
<img src="https://liorsinai.github.io/assets/posts/micrograd/polyfit.png" alt="Fitted polynomial curves"/>
<figcaption></figcaption>
</figure>

<h3 id="gradient-descent-map">4.2 Revisited with map</h3>

<p>It is possible to replace the inner loop over the training data with <code>map</code>.</p>

<figure><pre><code data-lang="julia"><span>function</span><span> gradient_descent_poly!</span><span>(</span>
    <span>coeffs</span><span>::</span><span>AbstractVector</span><span>,</span>
    <span>xs</span><span>::</span><span>AbstractVector</span><span>,</span>
    <span>ys</span><span>::</span><span>AbstractVector</span>
    <span>;</span> <span>learning_rate</span><span>::</span><span>AbstractFloat</span><span>=</span><span>0.1</span><span>,</span>
    <span>max_iters</span><span>::</span><span>Integer</span><span>=</span><span>100</span>
    <span>)</span>
    <span>history</span> <span>=</span> <span>Float64</span><span>[]</span>
    <span>for</span> <span>i</span> <span>in</span> <span>1</span><span>:</span><span>max_iters</span>
        <span># forward</span>
        <span>ys_and_backs</span> <span>=</span> <span>map</span><span>(</span><span>x</span><span>-&gt;</span><span>rrule</span><span>(</span><span>evalpoly</span><span>,</span> <span>x</span><span>,</span> <span>coeffs</span><span>),</span> <span>xs</span><span>)</span>
        <span>ŷ</span> <span>=</span> <span>map</span><span>(</span><span>first</span><span>,</span> <span>ys_and_backs</span><span>)</span>
        <span>loss_iter</span><span>,</span> <span>back_loss</span> <span>=</span> <span>rrule</span><span>(</span><span>mse</span><span>,</span> <span>ŷ</span><span>,</span> <span>ys</span><span>)</span>
        <span># reverse</span>
        <span>Δmse</span><span>,</span> <span>Δŷ</span><span>,</span> <span>Δy</span> <span>=</span> <span>back_loss</span><span>(</span><span>1.0</span><span>)</span>
        <span>∂f_and_∂x_zipped</span> <span>=</span> <span>map</span><span>(((</span><span>_</span><span>,</span> <span>pb</span><span>),</span> <span>δ</span><span>)</span> <span>-&gt;</span> <span>pb</span><span>(</span><span>δ</span><span>),</span> <span>ys_and_backs</span><span>,</span> <span>Δŷ</span><span>)</span>
        <span>Δcoeffs_unzipped</span> <span>=</span> <span>map</span><span>(</span><span>Δ</span><span>-&gt;</span><span>Δ</span><span>[</span><span>3</span><span>],</span> <span>∂f_and_∂x_zipped</span><span>)</span> <span># Δ[i] = (Δevalpoly, Δx, Δcoeffs)</span>
        <span>Δcoeffs</span> <span>=</span> <span>reduce</span><span>(</span><span>+</span><span>,</span> <span>Δcoeffs_unzipped</span><span>)</span>
        <span># update</span>
        <span>coeffs</span> <span>.-=</span> <span>learning_rate</span> <span>.*</span> <span>Δcoeffs</span>
        <span># history</span>
        <span>push!</span><span>(</span><span>history</span><span>,</span> <span>loss_iter</span><span>)</span>
    <span>end</span>
    <span>history</span>
<span>end</span></code></pre></figure>

<p>This is code is slightly more complex than the previous version.
The behaviour and performance is practically identical.
However, it is one step closer to being more generic.</p>

<p>In machine learning, models usually execute on multiple inputs at once.
We could make a polynomial model that does that:</p>

<figure><pre><code data-lang="julia"><span>struct</span><span> Polynomial</span><span>{</span><span>V</span><span>&lt;:</span><span>AbstractVector</span><span>}</span>
    <span>weights</span><span>::</span><span>V</span>
<span>end</span>
<span>(</span><span>m</span><span>::</span><span>Polynomial</span><span>)(</span><span>x</span><span>)</span> <span>=</span> <span>evalpoly</span><span>(</span><span>x</span><span>,</span> <span>m</span><span>.</span><span>weights</span><span>)</span>
<span>(</span><span>m</span><span>::</span><span>Polynomial</span><span>)(</span><span>x</span><span>::</span><span>AbstractVector</span><span>)</span> <span>=</span> <span>map</span><span>(</span><span>m</span><span>,</span> <span>x</span><span>)</span></code></pre></figure>

<p>The goal then is to get gradients for the model’s weights directly:</p>

<figure><pre><code data-lang="julia"><span>model</span> <span>=</span> <span>Polynomial</span><span>(</span><span>coeffs</span><span>)</span>
<span>zs</span><span>,</span> <span>back</span> <span>=</span> <span>pullback</span><span>(</span><span>m</span> <span>-&gt;</span> <span>m</span><span>(</span><span>xs</span><span>),</span> <span>model</span><span>)</span></code></pre></figure>

<p>In the next sections we will write code that will inspect the model function call, recognise that it calls <code>map</code>, and call a <code>pullback</code> for map.<sup id="fnref:pullback_vs_rrule" role="doc-noteref"><a href="#fn:pullback_vs_rrule" rel="footnote">2</a></sup>
This in turn will call the <code>pullback</code> for <code>evalpoly</code>, which will pass the arguments to the <code>rrule</code> defined above.</p>

<h2 id="conclusion">5 Conclusion</h2>

<p>The next two sections will develop the <code>pullback</code> function.
It will inspect and decompose code with the goal of passing arguments to <code>rrule</code> and accumulating gradients via the chain rule.</p>

<p><a href="https://liorsinai.github.io/machine-learning/2024/08/03/micrograd-2-expr">Part 2</a> will introduce metaprogamming Julia and generate expressions for the backpropagation code. 
However the code is unstable and prone to errors - it is recursive metaprogramming - so <a href="https://liorsinai.github.io/machine-learning/2024/08/10/micrograd-3-ir">part 3</a> will introduce more robust code making use of the <a href="https://fluxml.ai/IRTools.jl/latest/">IRTools.jl</a> package. 
This code really pushes Julia’s metaprogramming to its limits.</p>

<p>It is possible to jump straight to <a href="https://liorsinai.github.io/machine-learning/2024/08/10/micrograd-3-ir">part 3</a> if desired.</p>

<hr/>



        <hr/>
        
      </div></div>
  </body>
</html>
