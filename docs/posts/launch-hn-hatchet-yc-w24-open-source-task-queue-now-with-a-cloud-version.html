<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=40810986">Original</a>
    <h1>Launch HN: Hatchet (YC W24) – Open-source task queue, now with a cloud version</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN - this is Alexander and Gabe from Hatchet (<a href="https://hatchet.run">https://hatchet.run</a>). We’re building a modern task queue as an alternative to tools like Celery for Python and BullMQ for Node. Our open-source repo is at <a href="https://github.com/hatchet-dev/hatchet">https://github.com/hatchet-dev/hatchet</a> and is 100% MIT licensed.</p><p>When we did a Show HN a few months ago (<a href="https://news.ycombinator.com/item?id=39643136">https://news.ycombinator.com/item?id=39643136</a>), our cloud version was invite-only and we were focused on our open-source offering.</p><p>Today we’re launching our self-serve cloud so that anyone can get started creating tasks on our platform - you can get started at <a href="https://cloud.onhatchet.run" rel="nofollow">https://cloud.onhatchet.run</a>, or you can use these credentials to access a demo (should be prefilled):</p><pre><code>  URL: https://demo.hatchet-tools.com 
  Email: hacker@news.ycombinator.com
  Password: HatchetDemo123!
</code></pre><p>
People are currently using Hatchet for a bunch of use-cases: orchestrating RAG pipelines, queueing up user notifications, building agentic LLM workflows, or scheduling image generation tasks on GPUs.</p><p>We built this out of frustration with existing tools and a conviction that PostgreSQL is the right choice for a task queue. Beyond the fact that many developers are already using Postgres in their stack, which makes it easier to self-host Hatchet, it’s also easier to model higher-order concepts in Postgres, like chains of tasks (which we call workflows). In our system, the acknowledgement of the task, the task result, and the updates to higher-order models are done as part of the same Postgres transaction, which significantly reduces the risk of data loss/race conditions when compared with other task queues (which usually pass acknowledgements through a broker, storing the task results elsewhere, and only then figuring out the next task in the chain).</p><p>We also became increasingly frustrated with tools like Celery and the challenges it introduces when using a modern Python stack (&gt; 3.5). We wrote up a list of these frustrations here: <a href="https://docs.hatchet.run/blog/problems-with-celery">https://docs.hatchet.run/blog/problems-with-celery</a>.</p><p>Since our Show HN, we’ve (partially or completely) addressed the most common pieces of feedback from the post, which we’ll outline here:</p><p>1. The most common ask was built-in support for fanout workflows — one task which triggers an arbitrary number of child tasks to run in parallel. We previously only had support for DAG executions. We generalized this concept and launched child workflows (<a href="https://docs.hatchet.run/home/features/child-workflows">https://docs.hatchet.run/home/features/child-workflows</a>). This is the first step towards a developer-friendly model of durable execution.</p><p>2. Support for HTTP-based triggers — we’ve built out support for webhook workers (<a href="https://docs.hatchet.run/home/features/webhooks">https://docs.hatchet.run/home/features/webhooks</a>), which allow you to trigger any workflow over an HTTP webhook. This is particularly useful for apps on Vercel, who are dealing with timeout limits of 60s, 300s, or 900s (depending on your tier).</p><p>3. Our RabbitMQ dependency — while we haven’t gotten rid of this completely, we’ve recently launched hatchet-lite (<a href="https://docs.hatchet.run/self-hosting/hatchet-lite">https://docs.hatchet.run/self-hosting/hatchet-lite</a>), which allows you to run the various Hatchet components in a single Docker image that bundles RabbitMQ along with a migration process, admin CLI, our REST API, and our gRPC engine. Hopefully the lite was a giveaway, but this is meant for local development and low-volume processing, on the order of hundreds per minute.</p><p>We’ve also launched more features, like support for global rate limiting, steps which only run on workflow failure, and custom event streaming.</p><p>We’ll be here the whole day for questions and feedback, and look forward to hearing your thoughts!</p></div></td></div></div>
  </body>
</html>
