<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.rfleury.com/p/multi-core-by-default">Original</a>
    <h1>Multi-Core by Default</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><p><span>Learning to program a single CPU core is difficult. There is an enormous number of techniques, amount of information, and number of hours to spend in order to learn to do it effectively. Learning to program </span><em>multiple CPU cores</em><span> to do work in parallel, all while these cores cooperate in accomplishing some overarching task, to me seemed like the anvil that broke the camel’s back—so to speak—there is already so much to wrangle when doing single-core programming, that for me, it was much more convenient to ignore multi-core programming for a long time.</span></p><p><span>But in the modern computer hardware era, there emerges an elephant in the room. With modern CPU core counts far exceeding 1—and instead reaching numbers like 8, 16, 32, 64—programmers leave an </span><em>enormous</em><span> amount of performance on the table by ignoring the fundamentally multi-core reality of their machines.</span></p><p><span>I’m not a “performance programmer”. </span><a href="https://www.youtube.com/watch?v=apREl0KmTdQ" rel="">Like Casey Muratori</a><span> (which is partly what made me follow him to begin with), I have always wanted </span><em>reasonable</em><span> performance (though this might </span><em>appear</em><span> like “performance programming” to a concerning proportion of the software industry), but historically I’ve worked in domains where I control the data involved, like my own games and engines, where I am either doing the art, design, and levels myself, or heavily involved in the process. Thus, I’ve often been able to use my own </span><em>programming constraints</em><span> to inform </span><em>artistic constraints</em><span>.</span></p><p><span>All of that went out the window over the past few years, when in my </span><a href="https://github.com/EpicGamesExt/raddebugger" rel="">work</a><span> on debuggers, I’ve needed to work with data which is not only </span><em>not under my control</em><span>, but is almost </span><em>exactly identical to the opposite of what I’d want—</em><span>it’s dramatically bigger, unfathomably poorly structured, extraordinarily complicated, and not to mention unpredictable and highly variable. This is because, as I’ve </span><a href="https://www.rfleury.com/p/demystifying-debuggers-part-1-a-busy" rel="">written about</a><span>, debuggers are at a “busy intersection”. They deal with unknowns from the external computing world on almost all fronts. And if one wanted a debugger to be useful for—for instance—extraordinarily large codebases that highly successful companies use to ship real things, those unknowns include unfortunate details about those codebases too.</span></p><p>As such, in my work, making more effective use of the hardware has been far more important than it ever has been for me in the past. As such, I was forced to address the “elephant in the room” that is CPU core counts, and actually doing multi-core programming.</p><p><span>I’ve learned a lot about the multi-core aspect of programming in the past few years, and I’ve written about lessons I’ve learned during that time, like those on </span><a href="https://www.rfleury.com/p/a-taxonomy-of-computation-shapes" rel="">basic mental building blocks I used to plan for multithreaded architecture</a><span>, and </span><a href="https://www.rfleury.com/p/multi-threading-and-mutation" rel="">carefully organizing mutations such that multiple threads require little-to-no synchronization</a><span>.</span></p><p><span>I still find those ideas useful, and my past writing still captures my thoughts on the </span><em>first principles</em><span> of multi-core programming. But recently, thanks to some lessons I learned after a few discussions with </span><a href="https://computerenhance.com" rel="">Casey</a><span>, my abilities in </span><em>concretely applying</em><span> those first principles have “leveled up”. I’m writing this post now to capture and share those lessons.</span></p><p>Because every programmer learns single-core programming first, it’s common—after one first learns multi-core programming techniques—to apply those techniques conservatively within otherwise single-core code.</p><p>To make this more concrete, consider the following simple example:</p><pre><code>S64 *values = ...;
S64 values_count = ...;
S64 sum = 0;
for(S64 idx = 0; idx &lt; values_count; idx += 1)
{
  sum += values[idx];
}</code></pre><p><span>In this example, we compute a sum of all elements in the </span><code>values</code><span> array. Let’s now consider a few properties of sums:</span></p><ul><li><p><code>a + b + c + d = (a + b) + (c + d)</code></p></li><li><p><code>a + b + c + d = d + c + b + a</code></p></li><li><p><code>(a + b) + (c + d) = (c + d) + (a + b)</code></p></li></ul><p>Because we can reconsider a sum of elements as a sum of sums of groups of those elements, and because the order in which we sum elements does not impact the final computation, the original code can be rewritten like:</p><pre><code>S64 *values = ...;
S64 values_count = ...;

S64 sum0 = 0;
for(S64 idx = 0; idx &lt; values_count/4; idx += 1)
{
  sum0 += values[idx];
}

S64 sum1 = 0;
for(S64 idx = values_count/4; idx &lt; (2*values_count)/4; idx += 1)
{
  sum1 += values[idx];
}

S64 sum2 = 0;
for(S64 idx = (2*values_count)/4; idx &lt; (3*values_count)/4; idx += 1)
{
  sum2 += values[idx];
}

S64 sum3 = 0;
for(S64 idx = (3*values_count)/4; idx &lt; (4*values_count)/4 &amp;&amp; idx &lt; values_count; idx += 1)
{
  sum3 += values[idx];
}

S64 sum = sum0 + sum1 + sum2 + sum3;</code></pre><p>That obviously doesn’t win us anything—but what this means is that we can obtain the same result by subdividing the computation into several, smaller, independent computations.</p><p><span>Because several independent computations do not require writing to the same memory, they fit nicely with multi-core programming—each core does not need to synchronize at all with any other. This not only greatly simplifies the multi-core programming, but improves its performance—or, more precisely, it doesn’t </span><em>eat away </em><span>from the natural performance obtained by executing in parallel.</span></p><p><span>For cases like this, we can implement what’s known as a “</span><strong>parallel </strong><code>for”</code><span>. The idea is that we’d like to specify our original </span><code>for</code><span> loop…</span></p><pre><code>for(S64 idx = 0; idx &lt; values_count; idx += 1)
{
  sum += values[idx];
}</code></pre><p>…but we’d like to also express that the loop can be subdivided into independent computations (the results of which we can join into a single result later).</p><p>In other words, we begin with normal, single-core code. But, for some computation, we want to “go wide”, and compute something in parallel. Then, we want to “join” this wide, parallel work, and go back to more single-core code, which can use the results of the work done in parallel.</p><p><span>This is a widely known and used concept. In many real codebases written in modern programming languages which offer many tools for abstraction building, you’ll find a number of </span><a href="https://learn.microsoft.com/en-us/cpp/parallel/concrt/reference/concurrency-namespace-functions?view=msvc-170#parallel_for" rel="">impressive gymnastics</a><span> to succinctly express this.</span></p><p>One of the reasons I prefer working in a simpler language is that, if what my code ultimately generates to facilitate some abstraction is complicated, that being reflected directly in the source code helps keep me honest about how “clean” some construct actually is.</p><p>If, on the other hand, some higher level utility can be provided by a simple and straightforward concrete implementation, that is a sign of a superior design—one that does not compromise on its implementation, but also does not compromise on its higher level utility.</p><p>Many people behave as though this is impossible—that higher level utility necessarily incurs substantial tradeoffs at the low level, or vice versa, that low level properties like performance necessitate undesirable high level design. This is simply not universally true. By hunting for tradeoffs, many programmers train themselves to ignore cases when they can both have, and eat, their cake.</p><p><span>So, if we consider our options for implementing a “parallel </span><code>for</code><span>” without a lot of modern language machinery, we might start with something like this:</span></p><pre><code>struct SumParams
{
  S64 *values;
  S64 count;
  S64 sum;
};

void SumTask(SumParams *p)
{
  for(S64 idx = 0; idx &lt; p-&gt;count; idx += 1)
  {
    p-&gt;sum += p-&gt;values[idx];
  }
}

S64 ComputeSum(S64 *values, S64 count)
{
  S64 count_per_core = count / NUMBER_OF_CORES;
  SumParams params[NUMBER_OF_CORES] = {0};
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &lt; NUMBER_OF_CORES; core_idx += 1)
  {
    params[core_idx].values = values + core_idx*count_per_core;
    params[core_idx].count = count_per_core;
    S64 overkill = ((core_idx+1)*count_per_core - count);
    if(overkill &gt; 0)
    {
      params[core_idx].count -= overkill;
    }
    threads[core_idx] = LaunchThread(SumTask, &amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &lt; NUMBER_OF_CORES; core_idx += 1)
  {
    JoinThread(threads[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}</code></pre><p>There are a number of unfortunate realities about this mechanism:</p><ol><li><p><span>In something like </span><code>LaunchThread</code><span> and </span><code>JoinThread</code><span>, we interact with the kernel to create and destroy kernel resources (threads) every time we perform a sum.</span></p></li><li><p><span>The actual case-specific code we needed (for the sum, in this case), and the number of particular details we had to specify and get right—like the work subdivision—has exploded. What used to be a simple </span><code>for</code><span> loop has been spread around to different, more intricate parts, all implementing different details of the mechanism we wanted—the work preparation, the work kickoff, and the joining and combination of work results. All parts must be maintained and changed together, every time we want a parallel </span><code>for</code><span>.</span></p></li><li><p><span>The solution’s control flow has been scattered across threads, CPU cores, and time. We can no longer trivially step through the sum in a debugger. If we encounter a bug in some iterations in a parallel </span><code>for</code><span>, we need to correlate the </span><em>launching</em><span> of that particular work, and that actual work. For example, if we stop the program in the debugger and find ourselves within a thread performing some iterations of the parallel </span><code>for</code><span>, we have lost context about </span><em>who</em><span> launched that work (in single-core code, this information is universally provided with call stacks).</span></p></li></ol><p><span>The first problem can be partly addressed with a new underlying layer which our code uses instead of the underlying kernel primitives. In many codebases, this layer is called a “job system”, or a “worker thread pool”. In those cases, the program prepares a set of threads </span><em>once</em><span>, and these threads simply wait for work, and execute it once they receive it:</span></p><pre><code>void JobThread(void *p)
{
  for(;;)
  {
    Job job = GetNextJob(...);
    job.Function(job.params);
  }
}

void SumJob(SumParams *p)
{
  ...
}

S64 ComputeSum(S64 *values, S64 count)
{
  Job jobs[NUMBER_OF_CORES] = {0};
  for(S64 core_idx = 0; core_idx &lt; NUMBER_OF_CORES; core_idx += 1)
  {
    ...
    jobs[core_idx] = LaunchJob(SumJob, &amp;params[core_idx]);
  }

  S64 sum = 0;
  for(S64 core_idx = 0; core_idx &lt; NUMBER_OF_CORES; core_idx += 1)
  {
    WaitForJob(jobs[core_idx]);
    sum += params[core_idx].sum;
  }

  return sum;
}</code></pre><p>In this case, there is still some overhead incurred by sending to and receiving information from the job threads, but it is significantly lighter than interacting with the kernel.</p><p><span>But it hasn’t improved the higher level code very much at all—we’ve simply replaced “threads” with “jobs”. The latter two problems hold. We still need to perform an entire dance in order to set up a “wide loop”—a “parallel </span><code>for</code><span>”, which scatters control flow for a problem across both source code, and coherent contexts (CPU cores, call stacks) at runtime.</span></p><p><span>In this concrete case—computing a sum in parallel—this is not a huge concern. Will it compute a sum in parallel? Yes. Does it have very few shared data writes? Yes. Can you parallelize all similarly parallelizable problems this way? Yes. But, we pay the costs of these problems every time we use this mechanism. If we have to pay that cost </span><em>very frequently</em><span> throughout a problem, it can become onerous to write, debug, and maintain all of this machinery.</span></p><p><span>One desirable property of the parallel </span><code>for</code><span> is that all jobs—which execute at roughly the same time, across some number of cores—are identical in their “shape”. Each job thread participating in the problem is executing exactly the same </span><em>code</em><span>—we simply parameterize each job slightly differently, to distribute different subproblems to different cores. This makes understanding, predicting, profiling, and debugging such code much simpler.</span></p><p><span>Furthermore, within a parallel </span><code>for</code><span>, each job’s lifetime is scoped by the originating single-core code’s lifetime. Each job begins and ends within some scope—the scope responsible for launching, then joining, all of the jobs. This means no substantial lifetime management complexity occurs—allocations for a parallel </span><code>for</code><span> are as simple as for normal single-core code.</span></p><p><span>But in practice, the mechanism often used to implement parallel </span><code>for</code><span>s—the </span><em>job system</em><span>—is rarely </span><em>only</em><span> used in this way, which is understandable, given its highly generic structure. For example, it’s also often used to launch a number of </span><em>heterogeneous</em><span> jobs. In these cases, it becomes even more difficult to understand the context of a particular job—who launched it, and in what context? It also becomes more difficult to comprehensively understand a system—because there is such a large number of possible configurations of thread states, it can be difficult to ensure a threaded system is robust in all cases.</span></p><p>These jobs are also often not bounded by their launcher scope—as such, more engineering must be spent on managing resources, like memory allocations, whose lifetimes are now defined by what happens across multiple threads in multiple contexts.</p><p><span>And this is, really, the tip of the iceberg. In more sophisticated systems, one might observe that there are </span><em>dependencies</em><span> between jobs, and jobs ought to be implicitly launched when their dependency jobs complete, creating an even longer (and more difficult to inspect) chain of context related to some independent through line of work.</span></p><p><span>Ultimately, this presents recurring writing, reading, debugging, and maintenance costs that don’t exist in normal single-core code. All of the costs incurred by this job system design—whether used in a parallel </span><code>for</code><span> or otherwise—are paid </span><em>any time</em><span> new parallel work is introduced, or any time parallel work is maintained.</span></p><p><span>Now, </span><em>if</em><span> we have few parts of our code that </span><em>can</em><span> be parallelized in this way, then this is not a significant cost.</span></p><p><span>…But that </span><em>if</em><span> is doing a lot of heavy lifting.</span></p><p>In practice, I’ve found that an enormous number of systems are riddled with opportunities for parallelization, because of a lack of serial dependence between many of their parts. But, if taking advantage of every instance of serial independence requires significantly more engineering than just accepting single-core performance, then in many cases, programmers will opt for the latter.</p><p><span>Again—does this mean that a job system </span><em>cannot </em><span>be used to do such parallelization in these systems? No. But, it </span><em>also</em><span> means that we pay the costs of using this job system—the more moving parts; the extra code and concepts to write, read, and debug—much more frequently, if we’d like to take advantage of this widespread serial independence, or if we’d like any algorithm in particular to scale its performance with the number of cores.</span></p><p><span>The critical insight I learned from speaking with </span><a href="https://computerenhance.com" rel="">Casey</a><span> on this topic was that a significant reason why these costs arise is because of the careful organization a system needs in order to </span><em>switch</em><span> from single-core to multi-core code. Mechanisms like job systems and their special case usage in parallel </span><code>for</code><span>s represent, in some sense, the most conservative application of multi-core code. The vast majority of code is written as single-core, and a few carveouts are made when multi-core is critically important. In other words, code remains </span><em>single-core by default</em><span>, and in a few special cases, work is done to briefly hand work off to a multi-core system.</span></p><p><span>Because the context of code execution changes across time—because work is </span><em>handed off</em><span> from one system to another—it necessarily requires more code to set up, and it is more difficult to debug and understand the full context at any point in time.</span></p><p><span>But is this the best approach? Perhaps, instead of writing </span><em>single-core code</em><span> (which sometimes goes </span><em>wide</em><span>) by default, we can write </span><em>multi-core code</em><span> (which sometimes goes </span><em>narrow</em><span>) by default.</span></p><p>What does this look like in practice?</p><p>There’s a good chance that you’ve already experienced this style in other areas of programming—notably, in GPU shader programming.</p><p><span>GPU shaders—like vertex or pixel shaders, used in a traditional GPU rendering pipeline—are written such that they are </span><em>multi-core by default</em><span>. You author a single function (the entry point of the shader), but this function is executed on </span><em>many cores, always,</em><span> implicitly. The language constructs and rules are arranged in such a way that data reads and writes are always scoped by whatever core happens to be executing the code. A single execution of a </span><em>vertex</em><span> shader is scoped to a </span><em>vertex</em><span>—a </span><em>pixel </em><span>shader to a </span><em>pixel</em><span>—and so on.</span></p><p><span>Because the fundamental, underlying architecture is always </span><em>multi-core by default</em><span>, and because there is little involvement of each specific shader in how the multi-core parallelism is achieved, GPU programming enjoys enormous performance benefits, and yet as the shader programmer, it feels that there are </span><em>few costs</em><span> to pay for it. So few, in fact, that it feels more like artistic scripting, to the degree that someone can build an entire website—</span><a href="https://www.shadertoy.com/" rel="">Shadertoy</a><span>—built around rapid-iteration, high-performance, visual GPU scripting.</span></p><p>Wait a minute… “high performance”, “rapid-iteration scripting”? It seems like many believe that these are mutually exclusive!</p><p>Why does CPU programming feel so different?</p><p><span>Contrast the GPU programming model to the usual CPU programming model—you author a single function (the entry point of your program), which is scheduled onto a </span><em>single core only</em><span>, normally by a kernel scheduler, using a </span><em>single</em><span> thread state. This model is, in contrast, </span><em>single-core by default</em><span>.</span></p><p>Long story short: it doesn’t have to be!</p><p><span>Let’s begin by exactly inverting the approach. Instead of having a single thread which kicks off work to many threads, let’s just have many threads, all running the same code, by default. In a sense, let’s have </span><em>just one big parallel </em><code>for</code><span>:</span></p><pre><code>void BootstrapEntryPoint(void)
{
  Thread threads[NUMBER_OF_CORES] = {0};
  for(S64 thread_idx = 0; thread_idx &lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    threads[thread_idx] = LaunchThread(EntryPoint, (void *)thread_idx);
  }
  for(S64 thread_idx = 0; thread_idx &lt; NUMBER_OF_CORES; thread_idx += 1)
  {
    JoinThread(threads[thread_idx]);
  }
}

void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;
  // program&#39;s actual work occurs here!
}</code></pre><p><span>To click into an architecture which assumes a single-threaded entry point, we start with a </span><code>BootstrapEntryPoint</code><span>. But the only work this function actually does is launch all of the threads executing the </span><em>actual</em><span> entry point, </span><code>EntryPoint</code><span>.</span></p><p><span>Let’s consider the earlier summation example. First, let’s just take the original single-threaded code, and put it into </span><code>EntryPoint</code><span>, and see how we can continue from there.</span></p><pre><code>void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // compute the sum
  S64 sum = 0;
  for(S64 idx = 0; idx &lt; values_count; idx += 1)
  {
    sum += values[idx];
  }
}</code></pre><p><span>What is actually happening? Well, we’re “computing the sum across many cores”. That is… </span><em>technically </em><span>true! Ship it!</span></p><p>There’s just one little problem… This is just as fast as the single-core version, except it also uses enormously more energy, and steals time from other tasks the CPU could be doing, because it is simply duplicating all work on each core.</p><p>But, if we were to measure this, and consider the real costs, and profile the actual code, the profile would look something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!FMe9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!FMe9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 424w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 848w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 1272w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!FMe9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png" width="515" height="356.81626928471246" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:988,&#34;width&#34;:1426,&#34;resizeWidth&#34;:515,&#34;bytes&#34;:620805,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!FMe9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 424w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 848w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 1272w, https://substackcdn.com/image/fetch/$s_!FMe9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa15a04d3-a7c7-4a12-8ba9-862b932dd95d_1426x988.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>Duplication itself is not, in principle, a problem, and it is sometimes not to be avoided, because </span><em>deduplication</em><span> can sometimes be more expensive than </span><em>duplication</em><span>. For instance, communicating the result of a single </span><code>add</code><span> instruction across many threads—to deduplicate the work of that </span><code>add</code><span>—would be vastly more expensive than simply duplicating the </span><code>add</code><span> itself. We </span><em>do</em><span> want deduplication, but only when necessary, or when it actually helps.</span></p><p><span>So, where does it help? Unsurprisingly in this case, the dominating cost—the reason we are using multiple cores </span><em>at all</em><span>—is the sum across all elements in </span><code>values</code><span>. We want to distribute different parts of the sum across cores. To start, instead of computing </span><em>the full sum</em><span>, we can instead compute a </span><em>per-thread</em><span> </span><em>sum</em><span>. After each </span><em>per-thread sum</em><span> is computed, we can then combine them:</span></p><pre><code><code>void EntryPoint(void *params)
{
  S64 thread_idx = (S64)params;

  // we obtain these somehow:
  S64 *values = ...;
  S64 values_count = ...;

  // decide this thread&#39;s subset of the sum
  S64 thread_first_value_idx = ???;
  S64 thread_opl_value_idx = ???; // one past last

  // compute the thread sum
  S64 thread_sum = 0;
  for(S64 idx = thread_first_value_idx;
      idx &lt; thread_opl_value_idx;
      idx += 1)
  {
    thread_sum += values[idx];
  }

  // combine the thread sums
  S64 sum = ???;
}</code></code></pre><p>We have two blanks to fill in:</p><ol><li><p>How do we decide each thread’s subset of work?</p></li><li><p>How do we combine all thread sums?</p></li></ol><p>Let’s tackle each.</p><p><span>Currently, the only input I’ve provided each thread is its </span><em>index</em><span>, which would be in </span><em>[0, N)</em><span>, where </span><em>N</em><span> is the number of threads. This is stored in the local variable </span><code>thread_idx</code><span>, which will have a different value in </span><em>[0, N)</em><span> for each thread. This is an easy example, because a good way to distribute the sum work across all threads is to uniformly distribute the number of values to sum amongst the threads. This means we are simply mapping </span><em>[0, M) </em><span>to </span><em>[0, N)</em><span>, where </span><em>M</em><span> is the number of values—</span><code>values_count</code><span>—and </span><em>N</em><span> is the number of threads.</span></p><p><span>We can </span><em>almost</em><span> compute this as follows:</span></p><pre><code>S64 values_count = ...;
S64 thread_idx = ...;
S64 thread_count = NUMBER_OF_CORES;

S64 values_per_thread = values_count / thread_count;
S64 thread_first_value_idx = values_per_thread * thread_idx;
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;</code></pre><p><span>This is almost right, but only almost, because we also need to account for the case where </span><code>values_count</code><span> is not cleanly subdivided by </span><code>thread_count</code><span>. Because our </span><code>values_per_thread</code><span> will truncate to the next lowest integer, this current distribution will </span><em>underestimate</em><span> the number of values we need to compute per thread, by anywhere from 0 (if it divides cleanly) to </span><code>thread_count-1</code><span> values—or in other words, the remainder of the division.</span></p><p>Thus, the number of values this division underestimates by—the “leftovers”—can be computed as follows:</p><pre><code>S64 leftover_values_count = values_count % thread_count;</code></pre><p><span>We can then distribute these leftovers amongst the first </span><code>leftover_values_count</code><span> threads:</span></p><pre><code>// compute the values-per-thread, &amp; number of leftovers
S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;

// determine if the current thread gets a leftover
// (we distribute them amongst the first threads in the group)
B32 thread_has_leftover = (thread_idx &lt; leftover_values_count);

// decide on how many leftovers have been distributed before this
// thread&#39;s range (just the thread index, clamped by the number of
// leftovers)
S64 leftovers_before_this_thread_idx = 0;
if(thread_has_leftover)
{
  leftovers_before_this_thread_idx = thread_idx;
}
else
{
  leftovers_before_this_thread_idx = leftover_values_count;
}

// decide on the [first, opl) range:
// we shift `first` by the number of leftovers we&#39;ve placed earlier,
// and we shift `opl` by 1 if we have a leftover.
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = thread_first_value_idx + values_per_thread;
if(thread_has_leftover)
{
  thread_opl_value_idx += 1;
}</code></pre><p>Or more succinctly:</p><pre><code>S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);</code></pre><p><span>Now, using this </span><code>[first, opl)</code><span> calculation, we can arrange each thread to only loop over its associated range, thus not duplicating all sum work done by other threads.</span></p><p><span>Now, how might we combine each thread’s sum to form the total sum? There are two simple options available: </span><strong>(a)</strong><span> we can define a global sum counter to which each thread atomically adds (using atomic add intrinsics) its per-thread sum, or </span><strong>(b) </strong><span>we can define global storage which stores </span><em>all</em><span> thread sums, and each thread can duplicate the work of computing the total sum.</span></p><p><span>For </span><strong>(a)</strong><span>, we just need to define </span><code>sum</code><span> as </span><code>static</code><span>, and atomically add each </span><code>thread_sum</code><span> to it:</span></p><pre><code>static S64 sum = 0;

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;sum, thread_sum);
}</code></pre><p><em><strong>Note:</strong></em><span> </span><em>This has a downside in that only one thread group can be executing this codepath at once. This is sometimes not a practical concern, since if we are going wide at all, we are often using all available cores to do so, and it is likely not beneficial to also have some other thread group executing the same codepath for a different purpose. That said, it’s now a new hidden restriction of this code, and it can be a critical problem. There are some techniques we can use to solve this problem, which I will cover later—for now, the important concept is that the data is shared across participating threads.</em></p><p><span>For </span><strong>(b)</strong><span>, we’d instead have a global table, and duplicate the work of summing across all thread sums. But we can only do that </span><em>after</em><span> we know that each thread has completed its summation work—otherwise we’d potentially add some other thread’s sum before it was actually computed!</span></p><pre><code>static S64 thread_sums[NUMBER_OF_CORES] = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  // ??? need to wait here for all threads to finish!

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
}</code></pre><p><span>That extra waiting requirement might seem like an argument in favor of </span><strong>(a)</strong><span>, but we’d actually need the same mechanism if we did </span><strong>(a)</strong><span> once we wanted to actually </span><em>use</em><span> the sum—we’d need to wait for all threads to reach some point, so that we’d know that they’d all atomically updated </span><code>sum</code><span>.</span></p><p><span>We can use a </span><em><a href="https://en.wikipedia.org/wiki/Barrier_(computer_science)" rel="">barrier</a></em><span> to do this. In </span><strong>(a)</strong><span>:</span></p><pre><code>static S64 sum = 0;
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  AtomicAddEval64(&amp;sum, thread_sum);
  BarrierSync(barrier);
  // `sum` is now fully computed!
}</code></pre><p><span>And in </span><strong>(b)</strong><span>:</span></p><pre><code>static S64 thread_sums[NUMBER_OF_CORES] = {0};
static Barrier barrier = {0};

void EntryPoint(void *params)
{
  // ...
  // compute `thread_sum`
  // ...
  thread_sums[thread_idx] = thread_sum;

  BarrierSync(barrier);

  S64 sum = 0;
  for(S64 t_idx = 0; t_idx &lt; NUMBER_OF_CORES; t_idx += 1)
  {
    sum += thread_sums[t_idx];
  }
  // `sum` is now fully computed!
}</code></pre><p><span>At this point, we have everything we need for both </span><strong>(a)</strong><span> and </span><strong>(b)</strong><span>. Both are simple, and likely negligibly different. </span><strong>(a)</strong><span> requires atomic summation across all the threads, which implies hardware-level synchronization, whereas </span><strong>(b)</strong><span> duplicates the sum of all per-thread sums—these likely subtly differ in there costs, but not by much when compared to the actual </span><code>values</code><span> summation.</span></p><p><span>Now, while I hope this summation example has been a useful introduction, I know it’s a bit contrived, and incomplete. Specifically, it’s missing two key parts of any program: </span><em>inputs</em><span> and </span><em>outputs</em><span>. What are we </span><em>doing with this sum</em><span>, and how do we use that in producing some form of output, and how do obtain the inputs, and store them in </span><code>values</code><span> and </span><code>values_count</code><span>?</span></p><p><span>Let’s barely extend the summation example with stories for the inputs and outputs. For the inputs, let’s say that we read </span><code>values</code><span> out of a binary file, which just contains the whole array stored as it will be in memory. For the outputs, let’s say that we just print the sum to </span><code>stdout</code><span> with </span><code>printf</code><span>.</span></p><p>Printing out the sum will be the easiest part, so let’s begin with that.</p><p><span>In single-core code, after computing the sum, we’d simply call </span><code>printf</code><span>:</span></p><pre><code>S64 sum = ...;
// ...
printf(&#34;Sum: %I64d&#34;, sum);</code></pre><p>We can start by just doing the same in our “multi-core by default” code. What we’ll find is that our output looks something like this:</p><pre><code>Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678
Sum: 12345678</code></pre><p><span>And obviously, we only want our many cores to be involved with the majority of the computation, but we only need one thread to do the actual </span><code>printf</code><span>. In other words, we need to </span><em>go narrow</em><span>. Luckily, going </span><em>narrow</em><span> from </span><em>wide code</em><span> is much simpler than going </span><em>wide</em><span> from </span><em>narrow code</em><span>:</span></p><pre><code>S64 sum = ...;
// ...
if(thread_idx == 0)
{
  printf(&#34;Sum: %I64d&#34;, sum);
}</code></pre><p>We simply need to mask away the work from all threads except one.</p><p><span>Now, let’s consider the </span><em>input</em><span> problem. We need to compute </span><code>values_count</code><span> based on the size of some input file, allocate storage for </span><code>values</code><span>, and then fill </span><code>values</code><span> by reading all data from the file.</span></p><p>Single-threaded code to do that might look something like this:</p><pre><code>char *input_path = ...;
File file = FileOpen(input_path);
S64 size = SizeFromFile(file);
S64 values_count = (size / sizeof(S64));
S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
FileRead(file, 0, values_count * sizeof(values[0]), values);
FileClose(file);</code></pre><p><span>So, naturally, one option is to simply do this </span><em>narrow</em><span>:</span></p><pre><code><code>if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  S64 values_count = (size / sizeof(S64));
  S64 *values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier); // `values` and `values_count` ready after this point</code></code></pre><p><span>This will work, but we somehow need to broadcast the computed values of </span><code>values</code><span> and </span><code>values_count</code><span> across all threads. One easy way to do this is simply to pull them out as </span><code>static</code><span>, like we did for shared data earlier:</span></p><pre><code>static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  char *input_path = ...;
  File file = FileOpen(input_path);
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
  FileRead(file, 0, values_count * sizeof(values[0]), values);
  FileClose(file);
}
BarrierSync(barrier);</code></pre><p><span>But consider that we </span><em>might not </em><span>want to do this </span><em>completely </em><span>single-core. It might be the case that it’s more efficient to issue </span><code>FileRead</code><span>s from many threads, rather than just one. In practice, this is partly true (although, depending on the full stack—the kernel, the storage drive hardware, and so on—it may not be beneficial past some number of threads, and for certain read sizes).</span></p><p><span>So let’s say we’d like to do the </span><code>FileRead</code><span>s wide now also. We need to still allocate </span><code>values</code><span> on a single thread, but once that is done, we can distribute the rest of the work trivially:</span></p><pre><code>// we can open the file on all threads (though for some reasons
// we may want to deduplicate this too - for simplicity I am
// keeping it on all threads)
File file = FileOpen(input_path);

// calculate number of values and allocate (only single thread)
static S64 values_count = 0;
static S64 *values = 0;
if(thread_idx == 0)
{
  S64 size = SizeFromFile(file);
  values_count = (size / sizeof(S64));
  values = (S64 *)Allocate(values_count * sizeof(values[0]));
}
BarrierSync(barrier);

// compute thread&#39;s range of values (same calculation as before)
S64 thread_first_value_idx = ...;
S64 thread_opl_value_idx = ...;

// do read of this thread&#39;s portion
S64 num_values_this_thread = (thread_opl_value_idx - thread_first_value_idx);
FileRead(file,
         thread_first_value_idx*sizeof(values[0]),
         num_values_this_thread*sizeof(values[0]),
         values + thread_first_value_idx);

// close file on all threads
FileClose(file);</code></pre><p><span>It’s </span><em>much simpler</em><span>, now—compared to, say, the original parallel </span><code>for</code><span> case—to simply take another part of the problem like this, and to also distribute it amongst threads, simply because </span><em>wide</em><span> is the default shape of the program.</span></p><p><span>Instead of spending most programming time acting like we’re on a single-core machine, we simply assume our actual circumstances, which is that we have several cores, and </span><em>sometimes</em><span> we need to tie it all together with a few serial dependencies.</span></p><p><span>Let’s take a look at our earlier calculations to distribute portions of the </span><code>values</code><span> array:</span></p><pre><code><code>S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);</code></code></pre><p><span>This was an easy case, because uniformly dividing portions of </span><code>values</code><span> produces nearly uniform </span><em>work</em><span> across all cores.</span></p><p><span>If, in a different scenario, we </span><em>don’t</em><span> produce nearly uniform work across all cores, we have a problem: some cores will finish their work in some section long before others, and they’ll be stuck at the next barrier synchronization point while the other cores finish. This diminishes the returns we obtain from going wide in the first place.</span></p><p><span>Thus, it’s always important to uniformly distribute </span><em>work</em><span> whenever it’s possible. The exact strategy for doing so will vary by problem. But I’ve noticed three common strategies:</span></p><ol><li><p>Uniformly distributing inputs produces uniformly distributed work (the case with the sum). So, we can decide the work distribution upfront.</p></li><li><p>Each portion of an input requires a variable amount of per-core work. The work is relatively bounded, and there are many portions of input (larger than the core count). So, we can dynamically grab work on each core, so cores which complete smaller work first receive more, whereas cores that are stuck on longer work leave more units of work for other cores.</p></li><li><p>Each portion of an input requires a variable amount of per-core work, but there is a small number (lower than the core count) of potentially very long sequences of work. We can attempt to redesign this algorithm such that it can be distributed more uniformly instead.</p></li></ol><p>We’ve already covered the first strategy with the sum example—let’s look at the latter two.</p><p>Let’s consider a case where we have many units of work—“tasks”—and we’d like to distribute these tasks across cores. We may start by distributing the tasks in the same way that we distributed values to sum in the earlier example:</p><pre><code>Task *tasks = ...;
S64 tasks_count = ...;
S64 thread_first_task_idx = ...;
S64 thread_opl_task_idx = ...;
for(S64 task_idx = thread_first_task_idx;
    task_idx &lt; thread_last_task_idx;
    task_idx += 1)
{
  // do task
}</code></pre><p>If each task requires a variable amount of work, then a profile of the program might look something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9BhX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9BhX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 424w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 848w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 1272w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!9BhX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png" width="565" height="578.5817307692307" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1491,&#34;width&#34;:1456,&#34;resizeWidth&#34;:565,&#34;bytes&#34;:726260,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!9BhX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 424w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 848w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 1272w, https://substackcdn.com/image/fetch/$s_!9BhX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad2b7ba9-03fa-47d0-a520-1052a8739002_1767x1809.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>Instead of deciding the task division upfront, we can dynamically assign tasks, such that the threads which are occupied (performing larger tasks) are not assigned more tasks until they’re done, and threads which complete shorter tasks earlier are quickly assigned more tasks, if available.</p><p>We can do that simply with a shared atomic counter, which each thread increments:</p><pre><code>Task *tasks = ...;
S64 tasks_count = ...;

// set up the counter
static S64 task_take_counter = 0;
task_take_counter = 0;
BarrierSync(barrirer);

// loop on all threads - take tasks as long as we can
for(;;)
{
  S64 task_idx = AtomicIncEval64(&amp;task_take_counter) - 1;
  if(task_idx &gt;= tasks_count)
  {
    break;
  }
  // do task
}</code></pre><p>This will dynamically distribute tasks across the cores, so that a profile of the program will look more like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!J770!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!J770!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 424w, https://substackcdn.com/image/fetch/$s_!J770!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 848w, https://substackcdn.com/image/fetch/$s_!J770!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 1272w, https://substackcdn.com/image/fetch/$s_!J770!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!J770!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png" width="544" height="590.3296703296703" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1580,&#34;width&#34;:1456,&#34;resizeWidth&#34;:544,&#34;bytes&#34;:679123,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!J770!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 424w, https://substackcdn.com/image/fetch/$s_!J770!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 848w, https://substackcdn.com/image/fetch/$s_!J770!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 1272w, https://substackcdn.com/image/fetch/$s_!J770!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F110d5755-36bf-42a6-bd91-bab886abce0b_1554x1686.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>Dynamically assigning tasks to cores will help in many cases, but it gets less effective if tasks are highly variable, to the point of sometimes being exceedingly long (e.g. many times more expensive than smaller tasks), or if there are fewer tasks than the number of cores.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1raa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1raa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 424w, https://substackcdn.com/image/fetch/$s_!1raa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 848w, https://substackcdn.com/image/fetch/$s_!1raa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 1272w, https://substackcdn.com/image/fetch/$s_!1raa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!1raa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png" width="517" height="512.0288461538462" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1442,&#34;width&#34;:1456,&#34;resizeWidth&#34;:517,&#34;bytes&#34;:463349,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!1raa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 424w, https://substackcdn.com/image/fetch/$s_!1raa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 848w, https://substackcdn.com/image/fetch/$s_!1raa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 1272w, https://substackcdn.com/image/fetch/$s_!1raa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F045126c2-acfe-4a60-b75d-fec1171e05f5_1524x1509.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>In these cases, it can often be helpful to reconsider the serial independencies </span><em>within</em><span> a single task, or whether the same </span><em>effect</em><span> as a highly serially-dependent algorithm can be provided by an alternative highly serially-</span><em>independent</em><span> algorithm. Can a single task be subdivided further? Can it be performed in a different way? Can serially-dependent work be untangled from heavier work which can be done in a serially-independent way?</span></p><p><span>The answers to such questions are highly problem-specific, so it’s impossible to offer substantially more useful advice while staying similarly generic. But to illustrate that it’s sometimes possible—even when counterintuitive—I have an example problem from my recent work, in which finding more uniform work distribution required switching from a single-threaded </span><a href="https://en.wikipedia.org/wiki/Comparison_sort" rel="">comparison sort</a><span> to a highly parallelizable </span><a href="https://en.wikipedia.org/wiki/Radix_sort" rel="">radix sort</a><span>.</span></p><p><span>In this problem, I had a small </span><em>number</em><span> of arrays that needed to be sorted, but these arrays were potentially very </span><em>large</em><span>, thus requiring a fairly expensive sorting pass.</span></p><p><span>My first approach was to simply distribute the comparison sort tasks themselves, so I would sort one array on a single core, while other cores would be sorting other arrays. But as I’ve said, there were a relatively small number of arrays, and the arrays were large, so sorting was fairly expensive—thus, </span><em>most</em><span> cores were doing nothing, and simply waiting for the small number of cores performing sorts to finish.</span></p><p><span>This approach would’ve worked fine if I had a larger number of smaller tasks. In fact, another part of the same program </span><em>does </em><span>distribute single-threaded comparison sort tasks in this way, because in that part of the problem, there </span><em>are</em><span> a larger number of smaller tasks.</span></p><p>In this case, I needed to sort array elements based on 64-bit integer keys. After sorting, the elements needed to be ordered such that their associated keys were ascending in value.</p><p><span>Conveniently, this can be done with a radix sort. I won’t cover the full details of the algorithm here (although I briefly covered it during a stream recently, which I recorded and uploaded </span><a href="https://www.rfleury.com/p/multithreaded-radix-sort-implementation" rel="">here</a><span>), but the important detail is that a radix sort requires a fixed number of </span><em>O(N)</em><span> passes over the array, and huge portions of work in each pass can be distributed uniformly across cores (in the same way that we distributed the sum work earlier).</span></p><p><span>Now, </span><em>all </em><span>cores participate in </span><em>every </em><span>larger sorting task, but they only perform a nearly uniform fraction of the work in each sort. This results in a much more uniform work distribution, and thus a much shorter total time spend sorting:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!L74N!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!L74N!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 424w, https://substackcdn.com/image/fetch/$s_!L74N!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 848w, https://substackcdn.com/image/fetch/$s_!L74N!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 1272w, https://substackcdn.com/image/fetch/$s_!L74N!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!L74N!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png" width="545" height="581.6826923076923" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1554,&#34;width&#34;:1456,&#34;resizeWidth&#34;:545,&#34;bytes&#34;:407108,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!L74N!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 424w, https://substackcdn.com/image/fetch/$s_!L74N!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 848w, https://substackcdn.com/image/fetch/$s_!L74N!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 1272w, https://substackcdn.com/image/fetch/$s_!L74N!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99d792c3-ceed-4aab-b63d-65fc065569f8_1557x1662.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>This is just one concrete example a larger pattern I’ve noticed: In many problems, upon close examination, some serial dependencies can either vanish, or they can be untangled from heavier work.</p><p><span>In some problems, serially-dependent parts of the algorithm can be isolated, such that they prepare data which allows the rest of the algorithm to be done in a serially-independent fashion. Imagine a program which walks a linked list early, on a single core, to compute a layout in a serially-dependent way. This layout can then allow subsequent work to execute </span><em>just</em><span> using the full layout, rather than forcing that subsequent work to also include the serially-dependent pointer chasing.</span></p><p>Code which is multi-core by default feels like normal single-threaded code, just with a few extra constructs that express the missing information needed to execute on multiple cores. This style has some useful and interesting properties, which make it preferable in many contexts to many of the popular styles of multi-core code found in the wild.</p><p><span>One interesting implication of code written in this way—to be multi-core by default—is that it offers a strict </span><em>superset</em><span> of functionality than code which is written to be single-core, because “multi-core” in this case includes “single-core”, as one possible case. We can use the </span><em>same code</em><span> to execute on only a single core, simply by instead executing our entry point on a single thread, and parameterizing that thread with </span><code>thread_idx = 0</code><span> and </span><code>thread_count = 1</code><span>.</span></p><p><span>In that case, one core necessarily receives all of the work. </span><code>BarrierSync</code><span>s turn into no-ops, since there is only one thread (there are no other threads to wait for). Thus, it is equivalent to single-core functionality.</span></p><p><span>This style of multi-core programming requires far less busywork and machinery in order to use multiple cores for some codepath. But one of the problems I mentioned with job systems and parallel </span><code>for</code><span>s earlier was not only that they require more busywork and machinery, but that they’re also more difficult to debug.</span></p><p>In this case, debugging is much simpler—in fact, it doesn’t look all that different from single-core debugging. At every point, you have access to a full call stack, and all contextual data which led to whatever point in time that you happen to be inspecting in a debugger.</p><p><span>Furthermore, because all threads involved are nearly homogeneous (rather than the generic job system, where all threads are heterogeneous at all times), debugging a </span><em>single thread</em><span> is a lot like debugging </span><em>all threads</em><span>. This is especially true because—between barrier synchronization points—the threads are all executing the same code. In other words, the context and state on one thread is likely to be highly informative of the context and state on </span><em>all</em><span> threads.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7Q9d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7Q9d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 424w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 848w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 1272w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!7Q9d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png" width="679" height="423" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/e06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:423,&#34;width&#34;:679,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:98317,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7Q9d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 424w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 848w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 1272w, https://substackcdn.com/image/fetch/$s_!7Q9d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe06911f6-c9b7-46d8-befd-425bd831f74c_679x423.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>Because the context for some through line of computation frequently changes in traditional job systems, extra machinery must be involved to pipe data from one context to another—across jobs and threads—and maintain any associated allocations and lifetimes. But in this style, resources and lifetimes are kept as simple as they are in single-threaded code.</p><p><span>The stack, containing all contextual state at any point, becomes a single bucket for useful thread-local storage. In a job system, the stack is useful multi-core thread-local storage, but </span><em>only</em><span> for the duration of the job. The job is equivalent to the inner body of a </span><code>for</code><span>—this is a tiny, fragmentary scope. With this style, the entire stack is available, at any point.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!m_bz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!m_bz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 424w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 848w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 1272w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!m_bz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png" width="545" height="318.54052197802196" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:851,&#34;width&#34;:1456,&#34;resizeWidth&#34;:545,&#34;bytes&#34;:328198,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.rfleury.com/i/172146732?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!m_bz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 424w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 848w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 1272w, https://substackcdn.com/image/fetch/$s_!m_bz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7910c19-974e-4a3a-a6b7-fbca4453e8a8_1893x1107.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>I’ve found some useful patterns which can be extracted and widely used in code which is multi-core by default. These patterns seem as widely applicable as </span><a href="https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator" rel="">arenas</a><span>—as such, they can be a useful addition to a </span><a href="https://github.com/EpicGamesExt/raddebugger/blob/master/src/base/base_thread_context.h" rel="">codebase’s base layer</a><span>.</span></p><p><span>The earlier example code frequently uses the </span><code>thread_idx</code><span>, </span><code>thread_count</code><span>, and </span><code>barrier</code><span> variables. Passing these to every codepath which might need them is redundant and cumbersome. As such, they are good candidates for thread-local storage.</span></p><p><span>In my code, I’ve bundled these into the base layer’s “thread context”, which is a thread-local structure which is universally accessible—it’s where, for example, </span><a href="https://www.rfleury.com/i/70173682/per-thread-scratch-arenas" rel="">thread-local scratch arenas</a><span> are stored.</span></p><p><span>This provides all code the ability to read its index within a thread group (</span><code>thread_idx</code><span>), or the number of threads in its group (</span><code>thread_count</code><span>), and to synchronize with other lanes (</span><code>BarrierSync</code><span>).</span></p><p><span>As I suggested earlier, any code’s </span><em>caller</em><span> can choose “how wide”—how many cores—they’d like to execute that code, by configuring this per-thread storage. In general, </span><em>shallow</em><span> parts of a call stack can decide how wide </span><em>deeper</em><span> parts of a call stack are executed. If some work is expected to be small (to the point where it doesn’t benefit from being executed on many cores), and other cores can be doing other useful work, then before doing that work, the calling code can simply set </span><code>thread_idx = 0</code><span>, </span><code>thread_count = 1</code><span>, and </span><code>barrier = {0}</code><span>.</span></p><p><span>This means that a single thread may participate in many </span><em>different</em><span> thread groups—in other words, </span><code>thread_idx</code><span> and </span><code>thread_count</code><span> are not static within the execution of a single thread. Therefore, I found it appropriate to introduce another disambiguating term: </span><em>lane</em><span>. A </span><em>lane</em><span> is distinct from a thread in that a lane is simply one thread </span><em>within</em><span> a potentially-temporary group of threads, all executing the same code.</span></p><p><span>As such, in my terminology, </span><code>thread_idx</code><span> is exposed as </span><code>LaneIdx()</code><span>, and </span><code>thread_count</code><span> is exposed as </span><code>LaneCount()</code><span>. To synchronize with other lanes, a helper </span><code>LaneSync()</code><span> is available, which just waits on the thread context’s currently selected barrier.</span></p><p>I’ve mentioned the following computation multiple times:</p><pre><code>S64 values_per_thread = values_count / thread_count;
S64 leftover_values_count = values_count % thread_count;
B32 thread_has_leftover = (thread_idx &lt; leftover_values_count);
S64 leftovers_before_this_thread_idx = (thread_has_leftover
                                        ? thread_idx
                                        : leftover_values_count);
S64 thread_first_value_idx = (values_per_thread * thread_idx +
                              leftovers_before_this_thread_idx);
S64 thread_opl_value_idx = (thread_first_value_idx + values_per_thread + 
                            !!thread_has_leftover);</code></pre><p><span>This is useful whenever a uniformly distributed range corresponds to uniformly distributed work amongst cores. As I mentioned, this is sometimes not desirable. But nevertheless, it’s an extremely common case. As such, I found it useful to expose this as </span><code>LaneRange(count)</code><span>:</span></p><pre><code>Rng1U64 range = LaneRange(count);
for(U64 idx = range.min; idx &lt; range.max; idx += 1)
{
  // ...
}</code></pre><p><span>Earlier, we saw that when a variable needs to be shared across lanes, it can simply be marked as </span><code>static</code><span>. I mentioned that this has the unfortunate downside that only a single group can be executing the code at one time, since one group of lanes could trample over the </span><code>static</code><span> variable while another group is still using it. As I mentioned, this is sometimes not a concern (since it’s desirable to only have a single lane group executing some code), but it invisibly makes code inapplicable for some cases.</span></p><p><span>For example, let’s suppose I have some code which is written to be multi-core by default. Depending on the inputs to this codepath, I may want this to be executed—on the same inputs—with all of my cores. But in other cases, I may want this to be executed with only a single core—I may still want to execute this codepath on other cores, but for different inputs. That requires many lane groups to be executing the code at the same time, thus disqualifying the use of </span><code>static</code><span> to share data amongst lanes within the same group.</span></p><p>To address this, I also created a simple mechanism to broadcast small amounts of data across lanes.</p><p>Each thread context also stores—in addition to a lane index, lane count, and lane group barrier—a pointer to a shared buffer, which is the same value for all lanes in the same group.</p><p>If one lane has a value which it needs to be broadcasted to other lanes—for instance, if it allocated a buffer that the other lanes are about to fill—then that value can be communicated in the following way:</p><pre><code>U64 broadcast_size = ...;         // the number of bytes to broadcast
U64 broadcast_src_lane_idx = ...; // the index of the broadcasting lane
void *lane_local_storage = ...;   // unique for each lane
void *lane_shared_storage = ...;  // same for all lanes

// copy from broadcaster -&gt; shared
if(LaneIdx() == broadcast_src_lane_idx)
{
  MemoryCopy(lane_shared_storage, lane_local_storage, broadcast_size);
}
LaneSync();

// copy from shared -&gt; broadcastees
if(LaneIdx() != broadcast_src_lane_idx)
{
  MemoryCopy(lane_local_storage, lane_shared_storage, broadcast_size);
}
LaneSync();</code></pre><p>I’ve found that this shared buffer just needs to be big enough to broadcast 8 bytes, given that most small data can be broadcasted with a small number of 8 byte broadcasts, and larger data can be broadcasted with a single pointer broadcast.</p><p>I expose this mechanism with the following API:</p><pre><code>U64 some_value = 0;
U64 src_lane_idx = 0;
LaneSyncU64(&amp;some_value, src_lane_idx);
// after this line, all lanes share the same value for `some_value`</code></pre><p>It might be used in the following way:</p><pre><code>// set `values_count`, allocate for `values`, on lane 0, then
// broadcast their values to all other lanes:
S64 values_count = 0;
S64 *values = 0;
if(LaneIdx() == 0)
{
  values_count = ...;
  values = Allocate(sizeof(values[0]) * values_count);
}
LaneSyncU64(&amp;values_count, 0);
LaneSyncU64(&amp;values, 0);</code></pre><p>With the above mechanisms, we can program the original summation example with the following steps.</p><p>First, we load the values from the file:</p><pre><code><code>U64 values_count = 0;
S64 *values = 0;
{
  File file = FileOpen(input_path);
  values_count = SizeFromFile(file) / sizeof(values[0]);
  if(LaneIdx() == 0)
  {
    values = (S64 *)Allocate(values_count * sizeof(values[0]));
  }
  LaneSyncU64(&amp;values);
  Rng1U64 value_range = LaneRange(values_count);
  Rng1U64 byte_range = R1U64(value_range.min * sizeof(values[0]),
                             value_range.max * sizeof(values[0]));
  FileRead(file, byte_range, values + value_range.min);
  FileClose(file);
}
LaneSync();</code></code></pre><p>Then, we perform the sum across all lanes:</p><pre><code><code>// grab the shared counter
S64 sum = 0;
S64 *sum_ptr = &amp;sum;
LaneSyncU64(&amp;sum_ptr, 0);

// calculate lane&#39;s sum
S64 lane_sum = 0;
Rng1U64 range = LaneRange(values_count);
for(U64 idx = range.min; idx &lt; range.max; idx += 1)
{
  lane_sum += values[idx];
}

// contribute this lane&#39;s sum to the total sum
AtomicAddEval64(sum_ptr, lane_sum);
LaneSync();
LaneSyncU64(&amp;sum, 0);</code></code></pre><p>And finally, we output the sum value:</p><pre><code><code>if(LaneIdx() == 0)
{
  printf(”Sum: %I64d\n”);
}</code></code></pre><p><span>The concepts I’ve shared in this post represent what I feel is a fundamental shift in how CPU code can be expressed, compared to the normal single-core code all programmers are familiar with. Through small, additional annotations to code—basic concepts like </span><code>LaneIdx()</code><span>, </span><code>LaneCount()</code><span>, and </span><code>LaneSync()</code><span>—all code can contain the information necessary to be executed wide, using multiple cores to better take advantage of serial independence.</span></p><p><span>The same exact code can </span><em>also</em><span> be executed on a single core, meaning through these extra annotations, that code becomes strictly more flexible—at the low level—than its single-core equivalent which does </span><em>not</em><span> have these annotations.</span></p><p><span>Note that this is still not a comprehensive family of multithreading techniques, because it is strictly zooming in on one unique </span><em>timeline</em><span> of work, and how a single timeline can be accelerated using the fundamental multi-core reality of modern machines. But consider that programs often require </span><em>multiple</em><span> heterogeneous timelines of work, where one lane group is not in lockstep with others, and thus </span><em>should not</em><span> prohibit others from making progress.</span></p><p><span>But what I appreciate about the ideas in this post is that they do not </span><em>unnecessarily</em><span> introduce extra timelines. Communication between two heterogeneous timelines has intrinsic, relativity-related complexity. Those will always be necessary. But why pay that complexity cost </span><em>everywhere</em><span>, to accomplish simple multi-core execution?</span></p><p><span>I’m aware that, for many, these ideas are old news—indeed, everyone learns different things at different times. But in my own past programming, and when I look at the programming of many others, it seems that there is an awful lot of overengineering to do what seems trivial, and indeed what </span><em>is </em><span>trivial in other domains (like shader programming). So, for at least many people, these concepts do not </span><em>seem</em><span> well-known or old (even if they are in some circles and domains).</span></p><p>In any case, the concepts I’ve shared in this post have been dramatically helpful in improving my ability to structure multi-core code without overcomplication, and it seemed like an important-enough shift to carefully document it here.</p><p>I hope it was similarly helpful to you, if you didn’t know the concepts, or if you did, I hope it was nonetheless interesting.</p><p>If you enjoyed this post, please consider subscribing. Thanks for reading.</p><p>-Ryan</p></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
