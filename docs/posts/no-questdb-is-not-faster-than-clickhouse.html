<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://telegra.ph/No-QuestDB-is-not-Faster-than-ClickHouse-06-15">Original</a>
    <h1>No, QuestDB is not Faster than ClickHouse</h1>
    
    <div id="readability-page-1" class="page"><article id="_tl_editor"><address>Geoff Genz, Principal Engineer, ClickHouse Inc.<br/></address><p>QuestDB created a minor stir in the small but active world of analytics database benchmarking with this blog article:   <a href="https://questdb.io/blog/2022/05/26/query-benchmark-questdb-versus-clickhouse-timescale" target="_blank">https://questdb.io/blog/2022/05/26/query-benchmark-questdb-versus-clickhouse-timescale</a>.  This is the money quote:  “QuestDB is an order of magnitude faster than both TimescaleDB and ClickHouse in this specific query”</p><p>We at ClickHouse were . . . skeptical and intrigued.  We believe that when results are unexpected and surprising there is always an opportunity to learn. So, we set out to learn.</p><p>It turns out that the benchmark suite that QuestDB uses for the comparison is constructed in such a way that the ClickHouse query is doing a complete table scan on 1.2 billion rows while QuestDB utilizes its full indexing strategy to read just a tiny fraction of the actual data.  After adding just a few key indexes to the ClickHouse tables, ClickHouse dramatically outperforms QuestDB.</p><p>We’ll use the same specifications in AWS and (starting out) load the data the same way into QuestDB and ClickHouse.  For the timings below, “cold” means with no data preloaded in the Linux file system cache, “hot” means run right after a “cold” query has completed.  Queries are run three times.  Times for QuestDb come from the Web UI (excluding network).  For ClickHouse we use clickhouse-client.</p><p>Here&#39;s the headline query:</p><p><strong><em>SELECT * FROM readings WHERE velocity &gt; 90.0 AND latitude &gt;= 7.75 AND latitude &lt;= 7.80 AND longitude &gt;= 14.90 AND longitude &lt;= 14.95</em></strong></p><p>And the results from both QuestDB and ClickHouse:</p><p><em>QuestDB (Cold):  46.64s  46.62s   46.63s   – 46.63s average</em></p><p><em>QuestDB (Hot):   436.05ms  366.51ms  399.10ms  – 400.55ms average</em></p><p>(Hot times are a little higher than in the blog post, but close enough.)</p><p><em>ClickHouse (Cold):  69.233s  69.227s  69.222s – 69.227s average</em></p><p><em>ClickHouse (Hot):  4.776s  4.662s 4.657s – 4.698s average</em></p><p>So as reported in the blog post, an order of magnitude advantage for QuestDB on “hot” queries, and even some advantage on cold queries.  </p><p>(A quick aside – what is the storage cost of that extra performance?  The QuestDB “readings” directory on disk is 118.685 GB.  The ClickHouse “readings” directory on disk is 39.074 GB.  That&#39;s almost 3x, before tuning ClickHouse compression.)</p><p>What&#39;s going on here?  Why is ClickHouse struggling a bit with this query?  The culprit is the table definition for our ClickHouse table as generated by the TimescaleDb benchmark suite:</p><p><strong><em>CREATE TABLE benchmark.readings (</em></strong></p><p><strong><em>    `created_date` Date DEFAULT today(),</em></strong></p><p><strong><em>    `created_at` DateTime DEFAULT now(),</em></strong></p><p><strong><em>    `time` String,</em></strong></p><p><strong><em>    `tags_id` UInt32,</em></strong></p><p><strong><em>    `latitude` Nullable(Float64),</em></strong></p><p><strong><em>    `longitude` Nullable(Float64),</em></strong></p><p><strong><em>    `elevation` Nullable(Float64),</em></strong></p><p><strong><em>    `velocity` Nullable(Float64),</em></strong></p><p><strong><em>    `heading` Nullable(Float64),</em></strong></p><p><strong><em>    `grade` Nullable(Float64),</em></strong></p><p><strong><em>    `fuel_consumption` Nullable(Float64),</em></strong></p><p><strong><em>    `additional_tags` String DEFAULT &#39;&#39;</em></strong></p><p><strong><em>)</em></strong></p><p><strong><em>ENGINE = MergeTree(created_date, (tags_id, created_at), 8192)</em></strong></p><p>The partition key is “created_date” – that means we’re creating a partition for every day.  For this amount of data we at ClickHouse would probably recommend partitioning by month, but that’s not the end of the world.</p><p>The primary key is the tuple “tags_id, created_at”.  What?  That’s a really strange choice.  Why are we sorting by tags_id?  Maybe it makes sense with some other set of queries, but let’s tweak how our ClickHouse table is sorted in a more realistic way.  We’ll partition by month, sort by day, and add latitude to the primary/sorting key.</p><p><strong><em>CREATE TABLE benchmark.readings_lat</em></strong></p><p><strong><em>(</em></strong></p><p><strong><em>    `created_at` DateTime,</em></strong></p><p><strong><em>    `latitude` Nullable(Float64),</em></strong></p><p><strong><em>    `longitude` Nullable(Float64),</em></strong></p><p><strong><em>    `elevation` Nullable(Float64),</em></strong></p><p><strong><em>    `velocity` Nullable(Float64),</em></strong></p><p><strong><em>    `heading` Nullable(Float64),</em></strong></p><p><strong><em>    `grade` Nullable(Float64),</em></strong></p><p><strong><em>    `tags_id` UInt32,</em></strong></p><p><strong><em>    `additional_tags` String</em></strong></p><p><strong><em>)</em></strong></p><p><strong><em>ENGINE = MergeTree</em></strong></p><p><strong><em>PARTITION BY toYYYYMM(created_at)</em></strong></p><p><strong><em>ORDER BY (toDate(created_at), latitude, created_at)</em></strong></p><p><strong><em>SETTINGS index_granularity = 8192</em></strong></p><p>(Another quick aside -- this table took only 103.788 seconds to load nearly 1.2 billion rows from the other ClickHouse readings table.  ClickHouse ingest speed is really fast. At some point maybe we’ll do an ingest comparison of ClickHouse vs QuestDb and some other competitors.)</p><p>Now that we’ve assumed ‘latitude’ (that is, the location of these artificial data points) is important enough to query on that we include it in our primary key, let’s look at the performance:</p><p><em>ClickHouse (cold):  1.689s  1.705s  1.699s  – 1.698s average</em></p><p>That’s 27x faster than QuestDB assuming your users aren’t always querying exactly the same cached data on your servers.  But if they are, let’s look at the “hot” times:</p><p><em>ClickHouse (hot):  0.100s  0.107s  0.109s – 0.105s average</em></p><p>Almost 4x faster than QuestDB with the only difference being adding “latitude” to the ClickHouse sorting/primary key.</p><p>To recap – QuestDB found one edge case in an obsolete benchmark suite where a narrowly tailored query outperforms ClickHouse – <em>if</em> ClickHouse is handicapped by having no utilized indexes.  By simply adding an appropriate key/index, ClickHouse dominates QuestDB.  This performance advantage exists in almost any real world scenario.</p><p>Whenever a benchmark is posted that is a surprise, or unexpected, we encourage you to try and reproduce it.  While we are able to reproduce portions of the results, the testing methodology itself can lead to varying outcomes.  Substantially varying outcomes.  Happy benchmarking!</p></article></div>
  </body>
</html>
