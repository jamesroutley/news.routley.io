<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arstechnica.com/gadgets/2023/12/2023-was-the-year-that-gpus-stood-still/">Original</a>
    <h1>2023 was the year that GPUs stood still</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/IMG_1072-800x533.jpeg" alt="2023 was the year that GPUs stood still"/>
      <figcaption><p>Andrew Cunningham</p></figcaption>  </figure>

  




<!-- cache hit 359:single/related:8e3b3346f5d3d88cd2ba442c8d5fbfe4 --><!-- empty -->
<p>In many ways, 2023 was a long-awaited return to normalcy for people who build their own gaming and/or workstation PCs. For the entire year, most mainstream components have been available at or a little under their official retail prices, making it possible to build all kinds of PCs at relatively reasonable prices without worrying about restocks or waiting for discounts. It was a welcome continuation of <a href="https://arstechnica.com/gaming/2022/12/2022-in-gpus-the-shortage-ends-but-higher-prices-seem-here-to-stay/">some GPU trends</a> that started in 2022. Nvidia, AMD, and Intel could release a new GPU, and you could consistently buy that GPU for roughly what it was supposed to cost.</p>
<p>That&#39;s where we get into how frustrating 2023 was for GPU buyers, though. Cards like the GeForce RTX 4090 and Radeon RX 7900 series launched in late 2022 and boosted performance beyond what any last-generation cards could achieve. But 2023&#39;s midrange GPU launches were less ambitious. Not only did they offer the performance of a last-generation GPU, but most of them did it for around the same price as the last-gen GPUs whose performance they matched.</p>
<h2>The midrange runs in place</h2>
<p>Not every midrange GPU launch will get us a <a href="https://arstechnica.com/gadgets/2016/07/nvidia-gtx-1060-review/">GTX 1060</a>—a card roughly 50 percent faster than its immediate predecessor and beat the previous-generation GTX 980 despite costing just a bit over half as much money. But even if your expectations were low, this year&#39;s midrange GPU launches have been underwhelming.</p>
<p>The worst was probably the GeForce RTX 4060 Ti, which sometimes struggled to beat the card it replaced at around the same price. The 16GB version of the card was particularly maligned since it was $100 more expensive but was only faster than the 8GB version in a handful of games.</p>
<p>The regular RTX 4060 was slightly better news, thanks partly to a $30 price drop from where the RTX 3060 started. The performance gains were small, and a drop from 12GB to 8GB of RAM isn&#39;t the direction we prefer to see things move, but it was still a slightly faster and more efficient card at around the same price. AMD&#39;s <a href="https://arstechnica.com/gadgets/2023/05/review-amds-269-rx-7600-is-a-good-1080p-card-but-the-rtx-4060-looms/">Radeon RX 7600</a>, <a href="https://arstechnica.com/gadgets/2023/09/amd-rx-7700-xt-and-7800-xt-review-closing-out-the-fine-i-guess-gpu-generation/">RX 7700 XT, and RX 7800 XT</a> all belong in this same broad category—some improvements, but generally similar performance to previous-generation parts at similar or slightly lower prices. Not an exciting leap for people with aging GPUs who waited out the GPU shortage to get an upgrade.</p>                                            
                                                        
<p>The best midrange card of the generation—and at $600, we&#39;re definitely stretching the definition of &#34;midrange&#34;—might be the GeForce RTX 4070, which can generally match or slightly beat the RTX 3080 while using much less power and costing $100 less than the RTX 3080&#39;s suggested retail price. That seems like a solid deal once you consider that the RTX 3080 was essentially unavailable at its suggested retail price for most of its life span. But $600 is still a $100 increase from the 2070 and a $220 increase from the 1070, making it tougher to swallow.</p>
<p>In all, 2023 wasn&#39;t the worst time to buy a $300 GPU; that dubious honor belongs to the depths of 2021, when you&#39;d be lucky to snag a GTX 1650 for that price. But &#34;consistently available, basically competent GPUs&#34; are harder to be thankful for the further we get from the GPU shortage.</p>
<h2>Marketing gets more misleading</h2>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/GeForce-RTX-4060-Series-Press-Deck-12-2.jpeg" data-height="1080" data-width="1920" alt="1.7 times faster than the last-gen GPU? Sure, under exactly the right conditions in specific games."><img alt="1.7 times faster than the last-gen GPU? Sure, under exactly the right conditions in specific games." src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/GeForce-RTX-4060-Series-Press-Deck-12-2-980x551.jpeg" width="980" height="551"/></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/GeForce-RTX-4060-Series-Press-Deck-12-2.jpeg" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> 1.7 times faster than the last-gen GPU? Sure, under exactly the right conditions in specific games.</p><p>Nvidia</p></figcaption></figure>
<p>If you just looked at <a href="https://arstechnica.com/gadgets/2023/05/nvidias-399-4060-ti-comes-with-no-price-hike-but-mild-performance-improvements/">Nvidia&#39;s early performance claims</a> for each of these GPUs, you might think that the RTX 40-series was an exciting jump forward.</p>
<p>But these numbers were only possible in games that supported these GPUs&#39; newest software gimmick, DLSS Frame Generation (FG). The original DLSS and DLSS 2 improve performance by upsampling the images generated by your GPU, generating interpolated pixels that make lower-res image into higher-res ones without the blurriness and loss of image quality you&#39;d get from simple upscaling. DLSS FG generates entire frames in between the ones being rendered by your GPU, theoretically providing big frame rate boosts without requiring a powerful GPU.</p>

<p>The technology is impressive when it works, and it&#39;s been successful enough to spawn hardware-agnostic imitators like <a href="https://arstechnica.com/gaming/2023/08/amds-fps-doubling-fsr-3-is-coming-soon-and-not-just-to-radeon-graphics-cards/">the AMD-backed FSR 3</a> and <a href="https://www.tomshardware.com/pc-components/gpus/intel-details-game-boosting-frame-generation-tech-that-applies-a-different-technique-extrass-uses-extrapolation-instead-of-amd-and-nvidias-approach-that-uses-interpolation">an alternate implementation from Intel</a> that&#39;s still in early stages. But it has notable limitations—mainly, it needs a reasonably high base frame rate to have enough data to generate convincing extra frames, something that these midrange cards may struggle to do. Even when performance is good, it can introduce weird visual artifacts or lose fine detail. The technology isn&#39;t available in all games. And DLSS FG also adds a bit of latency, though this can be offset with latency-reducing technologies like <a href="https://www.nvidia.com/en-us/geforce/technologies/reflex/">Nvidia Reflex</a>.</p>
<p>As another tool in the performance-enhancing toolbox, DLSS FG is nice to have. But to put it front-and-center in comparisons with previous-generation graphics cards is, at best, painting an overly rosy picture of what upgraders can actually expect.</p>

                                                </div></div>
  </body>
</html>
