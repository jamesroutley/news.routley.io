<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.latent.space/p/o3-pro">Original</a>
    <h1>First thoughts on o3 pro</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><em><span>As “</span><a href="https://discord.com/channels/822583790773862470/1075282825051385876/1381889553777954888" rel="">leaked</a><span>”, OpenAI </span><a href="https://x.com/sama/status/1932434606558462459" rel="">cut o3 pricing by 80%</a><span> today (from $10/$40 per mtok to $2/$8 - matching </span><a href="https://www.latent.space/p/quasar" rel="">GPT 4.1</a><span> pricing!!) to set the stage of the launch of o3-pro ($20/$80, supporting an unverified community theory that the -pro variants are 10x base model calls with majority voting as referenced in </span><a href="https://openai.com/index/browsecomp/" rel="">their papers</a><span> and in </span><a href="https://www.latent.space/p/chai" rel="">our Chai episode</a><span>). o3-pro reports a </span><a href="https://help.openai.com/en/articles/9624314-model-release-notes" rel="">64% win rate</a><span> vs o3 on human testers and does marginally better on </span><a href="https://x.com/OpenAI/status/1932530418936655923" rel="">4/4 reliability benchmarks</a><span>, but as </span><a href="https://x.com/sama/status/1932533208366608568" rel="">sama noticed</a><span>, the actual experience expands when you test it DIFFERENTLY…</span></em></p><p><strong>I’ve had early access to o3 pro for the past week. below are my (early) thoughts:</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png" width="1042" height="896" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:896,&#34;width&#34;:1042,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:189366,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/165643712?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa958ca07-2119-4dd1-a0c0-617528fd5d49_1042x896.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><p>We’re in the era of task-specific models. On one hand, we have “normal” models like 3.5 Sonnet and 4o—the ones we talk to like friends, who help us with our writing, and answer our day-to-day queries. On the other, we have gigantic, slow, expensive, IQ-maxxing reasoning models that we go to for deep analysis (they’re great at criticism), one-shotting complex problems, and pushing the edge of pure intelligence.</p><p><span>If you follow me on </span><a href="https://x.com/benhylak/status/1878237490194366744" rel="">Twitter</a><span>, you know I&#39;ve had a journey with the o-reasoning models. My first impression of o1/o1-pro was quite negative. But as I gritted my teeth through the first weeks, propelled by other people&#39;s raving reviews, I realized that I was, in fact, using it wrong. I wrote up all my thoughts, got ratio’ed by @sama, and quote-tweeted by @</span><a href="https://x.com/gdb/status/1878489681702310392" rel="">gdb</a><span>.</span></p><div data-component-name="DigestPostEmbed"><div><a href="https://www.latent.space/p/o1-skill-issue" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e97fcc3-a572-4a87-b2c2-6fc016edd5dd_1276x598.png"/><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e97fcc3-a572-4a87-b2c2-6fc016edd5dd_1276x598.png" sizes="100vw" alt="o1 isn’t a chat model (and that’s the point)" width="140" height="140"/></picture></div></a></div></div><p><span>The key, I discovered, was to </span><em>not</em><span> chat with it. Instead, </span><strong>treat it like a </strong><em><strong><span>report generator</span><span>.</span></strong></em><span> Give it context, give it a goal, and let it rip. And that&#39;s exactly how I use o3 today.</span></p><p>But therein lines the problem with evaluating o3 pro.</p><p><span>It’s smarter. </span><em>much smarter.</em></p><p><strong>But in order to see that, you need to give it </strong><em><strong>a lot</strong></em><strong> more context. and I’m running out of context.</strong></p><p>There was no simple test or question i could ask it that blew me away.</p><p><span>But then I took a different approach. My co-founder Alexis and I took the the time to assemble a history of all of our past planning meetings at </span><strong><a href="http://raindrop.ai" rel="">Raindrop</a></strong><span>, all of our goals, even record voice memos: and then asked o3-pro to come up with a plan.</span></p><p>We were blown away; it spit out the exact kind of concrete plan and analysis I’ve always wanted an LLM to create — complete with target metrics, timelines, what to prioritize, and strict instructions on what to absolutely cut.</p><p><span>The plan o3 gave us was plausible, reasonable; but the plan o3 Pro gave us was specific and rooted enough that </span><em><strong>it actually changed how we are thinking about our future.</strong></em></p><p>This is hard to capture in an eval.</p><p><span>Trying out o3 Pro made me realize that models today are so </span><em>good</em><span> in isolation, we’re running out of simple tests. The real challenge is integrating them into society. It&#39;s almost like a really high IQ 12-year-old going to college. </span><strong>They might be smart, but they’re not a useful employee if they can’t integrate.</strong></p><p><span>Today, this integration primarily comes down to tool calls: how well the model collaborates with humans, external data, and other AIs. It’s a great thinker, but it’s gotta grow into being a great do-er. o3 Pro makes real jumps here. It’s noticeably better at discerning what it’s </span><strong>environment</strong><span> is; accurately </span><strong>communicating what tools it has access to</strong><span>, when to </span><strong>ask questions about the outside world</strong><span> (rather than pretending it has the information/access), and </span><strong>choosing the right tool</strong><span> for the job.</span></p><p><strong>o3 pro (left) vs o3 (right):</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png" width="1456" height="858" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:858,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:1582790,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/165643712?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F70e6cdb1-9691-4f10-b495-d4479c207bf7_2684x1582.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png" width="1456" height="1185" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1185,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:1272076,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/165643712?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31b789a0-a417-4b2b-9984-624a1d03c772_1942x1580.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>o3 pro (left) clearly understanding the confines of it’s environment way better.</p><p>One thing I noticed from early access: if you don’t give it enough context, it does tend to overthink. It’s insanely good at analyzing, amazing at using tools to do things, not so good at doing things directly itself. I think it would be a fantastic orchestrator. But, for example, there were some ClickHouse SQL questions that o3 did better with. YMMV!</p><p>To illustrate:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg" width="1456" height="835" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:835,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:907963,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/jpeg&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://www.latent.space/i/165643712?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e7c5aa6-5ffd-40cb-941b-f25631d8fa5a_6863x3935.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>o3 Pro feels very, very different from Opus and Gemini 2.5 Pro. Where Claude Opus </span><em>feels</em><span> big but never truly showed me a clear sign of its “bigness,” o3 Pro’s takes are just... better. It feels like a completely different playing field.</span></p><p>OpenAI’s really driving down this vertical RL path (Deep Research, Codex)—not just teaching models how to use tools, but how to reason about when to use them.</p><p><span>The best ways to prompt reasoning models haven’t changed. </span><a href="https://www.latent.space/p/o1-skill-issue" rel="">My guide on how to prompt o1 still stands</a><span>. Context is everything, its like feeding cookies to the cookie monster. It’s a way of bootstrapping LLM memory but actually targeted so it works well. And the system prompt really matters. Models have actually become really malleable so LLM “harnesses” that teach a model about it’s environment and its goals have outsize impact. It’s this “harness” - a combination of model, tools, memory, and other methods - that makes AI products actually good (what makes things like Cursor just work most of the time).</span></p><p>Other assorted tasks:</p><ul><li><p>The system prompt wildly shaped model behavior (in a good way!) It felt way more pronounced than even o3.</p></li><li><p><span>Leaps and bounds different than anthropic and gemini. Where Claude Opus </span><em>feels</em><span> big (but has really never shown me a true sign of its “bigness”), these takes are just.. better. Feels like a completely different playing field.</span></p></li><li><p>OAI’s really driving down this vertical RL path (Deep Research, Codex). E.g. teaching models not just how to use tools, but to reason about when to use them.</p></li></ul></div></div></div>
  </body>
</html>
