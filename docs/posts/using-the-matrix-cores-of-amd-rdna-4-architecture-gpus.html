<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gpuopen.com/learn/using_matrix_core_amd_rdna4/">Original</a>
    <h1>Using the Matrix Cores of AMD RDNA 4 architecture GPUs</h1>
    
    <div id="readability-page-1" class="page"><div> <h2 id="amd-rdna-4-architecture-gpus">AMD RDNA 4 architecture GPUs</h2>
<p>AMD RDNA™ 4 architecture GPUs, which have 3rd-generation Matrix Cores, improved the performance of Generalized Matrix Multiplication (GEMM) operations. The table below compares theoretical FLOPS/clock/CU (floating point operations per clock, per compute unit) to previous generations. However, we changed the VGPR layout for the arguments of Wave Matrix Multiply Accumulate (WMMA) operations compared to the previous RDNA 3 generation [1]. Therefore, it does not have backward compatibility. In order to accelerate GEMM operations on RDNA 4 GPUs, we need to use the new intrinsics added for GPUs of this generation. In this post, we explain how to use matrix cores on RDNA 4 GPUs from a HIP kernel.</p>

<h2 id="wave-matrix-multiply-accumulate-wmma-on-amd-rdna-4">Wave Matrix Multiply Accumulate (WMMA) on AMD RDNA 4</h2>
<p>Before going into the detail of the AMD RDNA 4 architecture implementation, let’s do a refresher on the matrix operation. A GEMM operation can be written as follows: </p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>D</mi><mo>=</mo><mi>A</mi><mi>B</mi><mo>+</mo><mi>C</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">D=AB+C \tag{1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span>B</span><span></span><span>+</span><span></span></span><span><span></span><span>C</span></span><span><span></span><span><span>(</span><span><span>1</span></span><span>)</span></span></span></span></span></span></p><p>WMMA operates on matrices of 16x16 dimension only. Thus, if the matrix we are working on is smaller than that, the matrices need to be padded. If the matrix is larger than that, we can still use WMMA intrinsics by decomposing the larger matrix into GEMM operations of 16x16 matrices. </p>
<p>This GEMM operation can be implemented easily in a kernel. A single lane of a wavefront simply allocates multiple 16x16 matrices in VGPR, loads them into the VGPRs, and then just executes the operation. If this operation is executed in all the lanes in a wavefront, it’s obviously very inefficient because they all load the same matrices. WMMA changes it and simplifies the operation. The WMMA intrinsic is different from other operations we do in a HIP kernel. Instead of writing code for each lane in a wavefront, it is required to write code to use the entire wavefront. </p>
<p>First, it changes the VGPR allocation. Instead of allocating the entire matrix in a single lane, each lane allocates smaller VGPRs to store part of the matrix. This reduces VGPR pressure. The other point is that once the matrices are loaded into VGPRs, a WMMA intrinsic is called from all the lanes in a wavefront, which triggers the execution of the GEMM operation in the matrix core. </p>
<p>There are a few WMMA intrinsics, but here in this blog, we focus on the WMMA intrinsic taking A, B as 16-bit floating-point numbers, and D, C as 32-bit floating-point numbers, which is <code dir="auto">__builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12</code>.</p>
<h2 id="using-wmma-intrinsics">Using WMMA intrinsics</h2>
<p>There are a few WMMA intrinsics for AMD RDNA 4 architecture GPUs. They have postfix <code dir="auto">_gfx12</code>, which did not exist for the WMMA intrinsics for RDNA 3 or gfx11 generation. It expects the same format for A, B matrices and C, D matrices. These formats are included in the name of the intrinsic like this:</p>
<p><code dir="auto">__builtin_amdgcn_wmma_&lt;C, D format&gt;_16x16x16_&lt;A, B format&gt;_w32_gfx12</code></p>
<p>Therefore, the intrinsic above uses 32-bit float for C, D, while 16-bit float is used for A and B. How 16x16 matrix is mapped to VGPRs in each lane in a wavefront is illustrated below,</p>
<p><img width="2058" height="1244" loading="lazy" decoding="async" src="https://gpuopen.com/_astro/matrix_amd_rdna4.CRTwkazN_Z1Nq1fr.png"/></p>
<p>In the AMD RDNA 3 architecture, we needed to duplicate some elements for A and B, but it is removed for RDNA 4, which makes the layout simpler in VGPRs. Each lane only needs to load (or store) 8 elements of a matrix. Here a wavefront consists of 32 lanes, single wavefront loads matrix exactly once <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>32</mn><mo>=</mo><mn>256</mn><mo>=</mo><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">8\times 32=256=16\times 16</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>8</span><span></span><span>×</span><span></span></span><span><span></span><span>32</span><span></span><span>=</span><span></span></span><span><span></span><span>256</span><span></span><span>=</span><span></span></span><span><span></span><span>16</span><span></span><span>×</span><span></span></span><span><span></span><span>16</span></span></span></span>. One thing to note is that B, C, and D matrices are row major while A is transposed thus column major as shown in the figure above. Now let’s look at the example code below.</p>
<div><figure><pre data-language="cpp" dir="ltr"><code><div><p><span>extern</span><span> </span><span>&#34;C&#34;</span><span> __global__ </span><span>void</span><span> </span><span>wmma_matmul</span><span>( </span><span>__fp16</span><span>*</span><span> a, </span><span>__fp16</span><span>*</span><span> b, </span><span>__fp16</span><span>*</span><span> c )</span></p></div><div><p><span>{</span></p></div><div><p><span><span>  </span></span><span>frag_type a_frag;</span></p></div><div><p><span><span>  </span></span><span>frag_type b_frag;</span></p></div><div><p><span><span>  </span></span><span>frag_type_c c_frag </span><span>=</span><span> {};</span></p></div><div></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> WMMA_DATA_WIDTH </span><span>=</span><span> </span><span>8</span><span>;</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> laneWrapped </span><span>=</span><span> threadIdx.x </span><span>%</span><span> </span><span>16</span><span>;</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> laneGroup </span><span>=</span><span> threadIdx.x </span><span>/</span><span> </span><span>16</span><span>;</span></p></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span><span>    </span></span><span>b_frag[ele] </span><span>=</span><span> b[</span><span>16</span><span> </span><span>*</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped];</span></p></div><div><p><span><span>    </span></span><span>a_frag[ele] </span><span>=</span><span> a[</span><span>16</span><span> </span><span>*</span><span> laneWrapped </span><span>+</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH )];</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div></div><div><p><span><span>  </span></span><span>c_frag </span><span>=</span><span> </span><span>__builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12</span><span>( a_frag, b_frag, c_frag );</span></p></div><div></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span><span>    </span></span><span>c[</span><span>16</span><span> </span><span>*</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped] </span><span>=</span><span> c_frag[ele];</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div>
<p>With this instruction, A and B matrices are 16-bit float. There is an intrinsic we can use to convert and pack two 32-bit floats into a single 32-bit VGPR, which helps the compiler compared to converting one by one for 32-bit float inputs. This is suitable for converting 32-bit float matrix to 16-bit float matrix, especially in contexts like matrix multiplication chains.</p>
<div><figure><pre data-language="cpp" dir="ltr"><code><div><p><span>__device__ half_2 </span><span>packFp32s</span><span>( </span><span>float</span><span> a, </span><span>float</span><span> b ) { </span><span>return</span><span> </span><span>__builtin_amdgcn_cvt_pkrtz</span><span>( a, b ); }</span></p></div></code></pre></figure></div>
<p>The load of A, B matrices can be written as shown in below with this function .</p>
<div><figure><pre data-language="cpp" dir="ltr"><code><div><p><span><span>  </span></span><span>half_2</span><span>*</span><span> a_ptr </span><span>=</span><span> </span><span>reinterpret_cast&lt;</span><span>half_2</span><span>*&gt;</span><span>( </span><span>&amp;</span><span>a_frag );</span></p></div><div><p><span><span>  </span></span><span>half_2</span><span>*</span><span> b_ptr </span><span>=</span><span> </span><span>reinterpret_cast&lt;</span><span>half_2</span><span>*&gt;</span><span>( </span><span>&amp;</span><span>b_frag );</span></p></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH </span><span>/</span><span> </span><span>2</span><span>; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span>    </span><span>const</span><span> </span><span>int</span><span> e0 </span><span>=</span><span> ele </span><span>*</span><span> </span><span>2</span><span> </span><span>+</span><span> </span><span>0</span><span>, e1 </span><span>=</span><span> ele </span><span>*</span><span> </span><span>2</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span></p></div><div><p><span><span>    </span></span><span>b_ptr[ele] </span><span>=</span><span> </span><span>packFp32s</span><span>( b[</span><span>16</span><span> </span><span>*</span><span> ( e0 </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped], b[</span><span>16</span><span> </span><span>*</span><span> ( e1 </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped] );</span></p></div><div><p><span><span>    </span></span><span>a_ptr[ele] </span><span>=</span><span> </span><span>packFp32s</span><span>( a[</span><span>16</span><span> </span><span>*</span><span> laneWrapped </span><span>+</span><span> ( e0 </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH )], a[</span><span>16</span><span> </span><span>*</span><span> laneWrapped </span><span>+</span><span> ( e1 </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH )] );</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div></code></pre></figure></div>
<p>Full source code including host program using <a href="https://gpuopen.com/orochi/">Orochi</a> can be found <a href="https://github.com/GPUOpen-LibrariesAndSDKs/Orochi/tree/main/Test/WMMA">here</a>.</p>
<h2 id="transitioning-from-amd-rdna-3-wmma-to-rdna-4-wmma">Transitioning from AMD RDNA 3 WMMA to RDNA 4 WMMA</h2>
<p>As briefly mentioned above, the VGPR format for WMMA has been changed since the AMD RDNA 3 architecture. The VGPR format of RDNA 4 is much simpler than the one of RDNA 3 which is illustrated below. We can see that C and D matrices are split into even and odd for lower 16 lanes and upper 16 lanes. Therefore, in order to convert D matrix to B matrix to chain WMMA operations as we do for MLP we will explain shortly, data needs to be shuffled among lanes in RDNA 3. This is not needed for RDNA 4.</p>
<p><img width="2058" height="1124" loading="lazy" decoding="async" src="https://gpuopen.com/_astro/matrix_amd_rdna3.DRNQpxiV_1t2XEf.png"/></p>
<h2 id="implementing-a-simple-mlp">Implementing a simple MLP</h2>
<p>With WMMA operation, we can implement a fully connected neural network called MLP easily. Here we assume the input dimension is 16, the number of internal neurons is 16, and the output dimension is 16 for simplicity. The inference execution of an MLP can be written as
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><msub><mi>X</mi><mn>0</mn></msub><mo>+</mo><msub><mi>B</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_{1}=W_{0}X_{0}+B_{0}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span>0</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>X</span><span><span><span><span><span><span></span><span><span><span>0</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span><span>0</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>2</mn></msub><mo>=</mo><msub><mi>W</mi><mn>1</mn></msub><msub><mi>X</mi><mn>1</mn></msub><mo>+</mo><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">X_{2}=W_{1}X_{1}+B_{1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span><span>2</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>X</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span><span>1</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>
This can be mapped into WMMA intrinsic <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mi>A</mi><mi>B</mi><mo>+</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">D=AB+C</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span><span></span><span>=</span><span></span></span><span><span></span><span>A</span><span>B</span><span></span><span>+</span><span></span></span><span><span></span><span>C</span></span></span></span> twice. For the first equation, we need to load all the values for the weight matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">W_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, input data <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">X_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and bias matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">B_0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. For the second equation, we only need to load <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">W_1,B_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>W</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and use the output from the first equation as B in eqn. 1. Here we need to be careful as the format of D and B matrices in eqn. 1 are different. When <code dir="auto">__builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12</code> is used, the D matrix is 32-bit float while the C matrix is 16-bit float. Although we need downcasts, note that we do not need any data exchange among lanes, since matrix D and B have the same layout, except for the floating-point data type. The kernel below is an example of the MLP. Note that we dropped bias term (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">B_0,B_1</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>0</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>B</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>) for simplicity.</p>
<div><figure><pre data-language="cpp" dir="ltr"><code><div><p><span>extern</span><span> </span><span>&#34;C&#34;</span><span> __global__ </span><span>void</span><span> </span><span>minimumMlp</span><span>( </span><span>__fp16</span><span>*</span><span> w, </span><span>__fp16</span><span>*</span><span> x, </span><span>__fp16</span><span>*</span><span> cOut )</span></p></div><div><p><span>{</span></p></div><div><p><span><span>  </span></span><span>frag_type a_frag;</span></p></div><div><p><span><span>  </span></span><span>frag_type b_frag;</span></p></div><div><p><span><span>  </span></span><span>frag_type_c c_frag </span><span>=</span><span> {};</span></p></div><div></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> WMMA_DATA_WIDTH </span><span>=</span><span> </span><span>8</span><span>;</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> laneWrapped </span><span>=</span><span> threadIdx.x </span><span>%</span><span> </span><span>16</span><span>;</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> laneGroup </span><span>=</span><span> threadIdx.x </span><span>/</span><span> </span><span>16</span><span>;</span></p></div><div><p><span>  </span><span>const</span><span> </span><span>int</span><span> MAT_SIZE </span><span>=</span><span> </span><span>16</span><span>;</span></p></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span><span>    </span></span><span>// load B matrix</span></p></div><div><p><span><span>    </span></span><span>b_frag[ele] </span><span>=</span><span> x[</span><span>16</span><span> </span><span>*</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped];</span></p></div><div><p><span><span>    </span></span><span>// load A matrix</span></p></div><div><p><span><span>    </span></span><span>a_frag[ele] </span><span>=</span><span> w[</span><span>16</span><span> </span><span>*</span><span> laneWrapped </span><span>+</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH )];</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span><span>  </span></span><span>// matMul</span></p></div><div><p><span><span>  </span></span><span>c_frag </span><span>=</span><span> </span><span>__builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12</span><span>( a_frag, b_frag, c_frag );</span></p></div><div></div><div><p><span><span>  </span></span><span>// move pointer for the weight matrix</span></p></div><div><p><span><span>  </span></span><span>w </span><span>+=</span><span> MAT_SIZE</span><span>*</span><span>MAT_SIZE;</span></p></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span><span>    </span></span><span>// convert C matrix to B matrix</span></p></div><div><p><span><span>    </span></span><span>b_frag[ele] </span><span>=</span><span> c_frag[ele];</span></p></div><div><p><span><span>    </span></span><span>// load A matrix</span></p></div><div><p><span><span>    </span></span><span>a_frag[ele] </span><span>=</span><span> w[</span><span>16</span><span> </span><span>*</span><span> laneWrapped </span><span>+</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH )];</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div></div><div><p><span><span>  </span></span><span>// matMul</span></p></div><div><p><span><span>  </span></span><span>c_frag </span><span>=</span><span> </span><span>__builtin_amdgcn_wmma_f32_16x16x16_f16_w32_gfx12</span><span>( a_frag, b_frag, c_frag );</span></p></div><div><p><span><span>  </span></span><span>// store</span></p></div><div><p><span>  </span><span>for</span><span>( </span><span>int</span><span> ele </span><span>=</span><span> </span><span>0</span><span>; ele </span><span>&lt;</span><span> WMMA_DATA_WIDTH; </span><span>++</span><span>ele )</span></p></div><div><p><span><span>  </span></span><span>{</span></p></div><div><p><span><span>    </span></span><span>cOut[</span><span>16</span><span> </span><span>*</span><span> ( ele </span><span>+</span><span> laneGroup </span><span>*</span><span> WMMA_DATA_WIDTH ) </span><span>+</span><span> laneWrapped] </span><span>=</span><span> c_frag[ele];</span></p></div><div><p><span><span>  </span></span><span>}</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this article, we explained how to use WMMA intrinsics on AMD RDNA 4 architecture GPUs. We also showed how to implement an MLP inference using WMMA intrinsics. What we did not cover in this article is application to GEMM operations with larger matrices, which we leave as an exercise for the readers. Further information can be found in the AMD ISA guide [2].</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://gpuopen.com/learn/wmma_on_rdna3/">How to accelerate AI applications on AMD RDNA 3 using WMMA</a>, 2023</li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf">“AMD RDNA 4” Instruction Set Architecture Reference Guide</a>, 2025</li>
</ol> </div></div>
  </body>
</html>
