<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://varunshenoy.substack.com/p/why-open-source-ai-will-win">Original</a>
    <h1>Why open source AI will win</h1>
    
    <div id="readability-page-1" class="page"><div class=""><div><div dir="auto"><blockquote><p>Linux is subversive. Who would have thought even five years ago (1991) that a world-class operating system could coalesce as if by magic out of part-time hacking by several thousand developers scattered all over the planet, connected only by the tenuous strands of the Internet? </p><p>Certainly not I.</p><p><em>opening remarks in The Cathedral and the Bazaar by Eric Raymond.</em></p></blockquote><p>There’s a popular floating theory on the Internet that a combination of the existing foundation model companies will be the end game for AI. </p><p>In the near future, every company will rent a “brain” from a model provider, such as OpenAI/Anthropic, and build applications that build on top of its cognitive capabilities.</p><p>In other words, AI is shaping up to be an oligopoly of sorts, with only a small set of serious large language model (LLM) providers.</p><p>I don’t think this could be farther from the truth. I truly believe that open source will have more of an impact on the future of LLMs and image models than the broad public believes.</p><p>There are a few arguments against open source that I see time and time again.</p><ol><li><p><strong>Open source AI cannot compete with the resources at industry labs.</strong><span> Building foundation models is expensive, and non-AI companies looking to build AI features will outsource their intelligence layer to a company that specializes in it. Your average company cannot scale LLMs or produce novel results the same way a well capitalized team of talented researchers can. On the image generation side, Midjourney is miles ahead of anything else.</span></p></li><li><p><strong>Open source AI is not safe.</strong><span> Mad scientists cooking up intelligence on their </span><a href="https://nonint.com/2022/05/30/my-deep-learning-rig/" rel="">cinderblock-encased GPUs</a><span> will not align their models with general human interests</span></p><span>. </span></li><li><p><strong>Open source AI is incapable of reasoning.</strong><span> Not only do open source models perform more poorly than closed models on benchmarks, but they also lack </span><a href="https://arxiv.org/abs/2206.07682" rel="">emergent capabilities</a><span>, those that would enable agentic workflows, for example.</span></p></li></ol><p>While they seem reasonable, I think these arguments hold very little water.</p><p>Outsourcing a task is fine — when the task is not business critical. </p><p><a href="https://www.baseten.co/" rel="">Infrastructure products</a><span> save users from wasting money and energy on learning Kubernetes or hiring a team of DevOps engineers. No company should have to hand-roll their own HR/bill payments software. There are categories of products that enable companies to “focus on what makes their beer taste better”</span></p><p><span>. </span></p><p>LLMs, for the most part, do not belong in this category. There are some incumbents building AI features on existing products, where querying OpenAI saves them on hiring ML engineers. For them, leveraging closed AI makes sense. </p><p>However, there’s a whole new category of AI native businesses for whom this risk is too great. Do you really want to outsource your core business, one that relies on confidential data, to OpenAI or Anthropic? Do you want to spend the next few years of your life working on a “GPT wrapper”? </p><p><strong>Obviously not.</strong></p><p>If you’re building an AI native product, your primary goal is getting off of OpenAI as soon as you possibly can. Ideally, you can bootstrap your intelligence layer using a closed source provider, build a data flywheel from engaged users, and then fine-tune your own models to perform your tasks with higher accuracy, less latency, and more control.</p><p><span>Every business needs to own their core product, and for AI native startups, their core product is a model trained on proprietary data</span></p><p><span>. Using closed source model providers for the long haul exposes an AI native company to undue risk.</span></p><p><span>There is too much pressure pent up for open source LLMs to flop. The lives of many companies are at stake. Even Google has acknowledged that </span><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither" rel="">they have no moat</a><span> in this new world of open source AI.</span></p><p>The general capabilities of LLMs open them up to an exponential distribution of use cases. The most important tasks are fairly straightforward: summarization, explain like I’m 5, create a list (or some other structure) from a blob of text, etc. </p><p>Reasoning, the type you get from scaling these models to get larger, doesn’t matter for 85% of use cases. Researchers love sharing that their 200B param model can solve challenging math problems or build a website from a napkin sketch, but I don’t think most users (or developers) have a burning need for these capabilities.</p><p><span>The truth is that open source models are </span><em>incredibly</em><span> good at the most valuable tasks, and can be fine-tuned to cover likely up to 99% of use-cases when a product has collected enough labeled data.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" width="578" height="328.74010079193664" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:790,&#34;width&#34;:1389,&#34;resizeWidth&#34;:578,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;Llama 2 performance&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="Llama 2 performance" title="Llama 2 performance" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>Fine-tuned Llama 2 models vs. GPT-4 (from </span><a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="">Anyscale</a><span>)</span></figcaption></figure></div><p>Reasoning, the holy grail that researchers are chasing, probably doesn’t matter nearly as much as people think.</p><p>More important than reasoning is context length and truthfulness. </p><p>Let’s start with context length. The longer the context length for a language model, the longer the prompts and chat logs you can pass in. </p><p>The original Llama has a context length of 2k tokens. Llama 2 has a context length of 4k. </p><p><span>Earlier this year, </span><a href="https://kaiokendev.github.io/til" rel="">an indie AI hacker</a><span> discovered that a single line code change to the RoPE embeddings for Llama 2 would give you up to 8K of context length </span><em>for free with no additional training.</em><span> </span></p><p><span>Just last week another indie research project was released, </span><a href="https://github.com/jquesnelle/yarn" rel="">YaRN</a><span>, that extends Llama 2’s context length to 128k tokens. </span></p><p>I still don’t have access to GPT-4 32k. This is the speed of open source.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" width="466" height="410.95054945054943" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/e239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1284,&#34;width&#34;:1456,&#34;resizeWidth&#34;:466,&#34;bytes&#34;:574648,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>While contexts have scaled up, the hardware requirements to run massive models have also scaled down. You can now run state-of-the-art </span><a href="https://twitter.com/ggerganov/status/1699791226780975439?s=20" rel="">massive</a><span> language models from your Macbook thanks to projects like </span><a href="https://github.com/ggerganov/llama.cpp" rel="">Llama.cpp</a><span>. Being able to use these models locally is a huge plus for security and costs as well. In the limit, you can run your models on your users’ hardware. Models are continuing to scale down while retaining quality. Microsoft’s Phi-1.5 is only 1.3 billion parameters but meets Llama 2 7B </span><a href="https://x.com/Teknium1/status/1701422303643615571?s=20" rel="">on several benchmarks</a><span>. Open source LLM experimentation will continue to explode as consumer hardware and </span><a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini" rel="">the GPU poor</a><span> rise to the challenge.</span></p><p><span>On truthfulness: out-of-the-box open source models are less truthful than closed source models, and I think this is actually fine. In many cases, </span><a href="https://towardsdatascience.com/llm-hallucinations-ec831dcd7786" rel="">hallucination</a><span> can be a feature, not a bug, particularly when it comes to creative tasks like storytelling. </span></p><p><span>Closed AI models have a certain filter that make them sound </span><em>artificial</em><span> and less interesting. </span><a href="https://huggingface.co/Gryphe/MythoMax-L2-13b" rel="">MythoMax-L2</a><span> tells significantly better stories than Claude 2 or ChatGPT, at only 13B parameters. When it comes to honestly, the latest open source LLMs work well with </span><a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" rel="">retrieval augmented generation</a><span>, and they will only get better.</span></p><p>Let’s take a brief look at the image generation side. </p><p>I would argue that Stable Diffusion XL (SDXL), the best open source model, is nearly on-par with Midjourney. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" width="1456" height="507" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:507,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:2871704,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Stable Diffusion XL generations for the prompt “an astronaut playing a guitar on Mars with a llama”. These images were generated on the first try, no cherry-picking needed.</figcaption></figure></div><p><span>In exchange for the slightly worse ergonomics, Stable Diffusion users have access to hundreds of community crafted LoRAs</span></p><p><span>, fine-tunes, and textual embeddings. Users quickly discovered hands were a sore for SDXL, and within weeks a LoRA </span><a href="https://minimaxir.com/2023/08/stable-diffusion-xl-wrong/" rel="">that fixes hands appeared online</a><span>. </span></p><p><span>Other open source projects like </span><a href="https://huggingface.co/docs/diffusers/training/controlnet" rel="">ControlNet</a><span> give Stable Diffusion users significantly more power when it comes to structuring their outputs, where Midjourney falls flat.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" width="512" height="208.21333333333334" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:305,&#34;width&#34;:750,&#34;resizeWidth&#34;:512,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>A flowchart of how Stable Diffusion + ControlNet works. Clipped from </span><a href="https://stable-diffusion-art.com/controlnet/" rel="">here</a><span>.</span></figcaption></figure></div><p>Moreover, Midjourney doesn’t have an API, so if you want to build a product with an image diffusion feature, you would have to use Stable Diffusion in some form. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" width="432" height="432" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:960,&#34;width&#34;:960,&#34;resizeWidth&#34;:432,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;r/StableDiffusion - Spiral Town - different approach to qr monster&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="r/StableDiffusion - Spiral Town - different approach to qr monster" title="r/StableDiffusion - Spiral Town - different approach to qr monster" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>This image went viral on </span><a href="https://x.com/MrUgleh/status/1702041188482658758?s=20" rel="">Twitter</a><span> and </span><a href="https://www.reddit.com/r/StableDiffusion/comments/16ew9fz/spiral_town_different_approach_to_qr_monster/" rel="">Reddit</a><span> this week. It uses Stable Diffusion with ControlNet. Currently, you can’t create images like this on Midjourney.</span></figcaption></figure></div><p>There are similar controllable features and optimizations that open source LLMs enable.</p><p>An LLM’s logits, the token-wise probability mass function at each iteration, can be used to generate structured output. In other words, you can guarantee the generation of JSON without entering a potentially expensive “validate-retry” loop, which is what you would need to do if you were using OpenAI. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" width="818" height="218" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:218,&#34;width&#34;:818,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" title="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>An example of logits from </span><a href="https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/" rel="">NVIDIA</a><span>.</span></figcaption></figure></div><p><span>Open source models are smaller and run on your own dedicated instance, leading to lower end-to-end </span><a href="https://twitter.com/abacaj/status/1699602420882378932?s=20" rel="">latencies</a><span>. You can improve throughput by batching queries and using inference servers like </span><a href="https://vllm.ai/" rel="">vLLM</a><span>. </span></p><p><span>There are many more tricks (see: </span><a href="https://arxiv.org/abs/2302.01318" rel="">speculative</a><span> </span><a href="https://twitter.com/ggerganov/status/1697262700165013689?s=20" rel="">sampling</a><span>, </span><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#concurrent-model-execution" rel="">concurrent model execution</a><span>, </span><a href="https://github.com/ggerganov/llama.cpp/issues/64" rel="">KV caching</a><span>) that you can apply to improve on the axes of latency and throughput. The latency you see on the OpenAI endpoint is the best you can do with closed models, rendering it useless for many latency-sensitive products and too costly for large consumer products.</span></p><p><span>On top of all this, you can also fine-tune or train your own LoRAs on top of open source models with maximal control. Frameworks like </span><a href="https://github.com/OpenAccess-AI-Collective/axolotl#axolotl" rel="">Axolotl</a><span> and </span><a href="https://huggingface.co/docs/trl/sft_trainer" rel="">TRL</a><span> have made this process simple</span></p><p><span>. While closed source model providers also have their own fine-tuning endpoints, you wouldn’t get the same level of control or visibility than if you did it yourself. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" width="468" height="382.7168674698795" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/bf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1086,&#34;width&#34;:1328,&#34;resizeWidth&#34;:468,&#34;bytes&#34;:384308,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><span>Falcon 180B, the largest open source model to date, was </span><a href="https://huggingface.co/blog/falcon-180b" rel="">released last week</a><span>. Within hours, Discords filled with mostly anonymous developers began exploring how they could recreate GPT-4 using this new model as a base layer.</span></figcaption></figure></div><p>Open source also provides guarantees on privacy and security.</p><p>You control the inflow and outflow of data in open models. The option to self-host is a necessity for many users, especially those working in regulated fields like healthcare. Many applications will also need to run on proprietary data, on both the training and inference side.</p><p><span>Security is best explained by </span><a href="https://en.wikipedia.org/wiki/Linus%27s_law" rel="">Linus’s Law</a><span>:</span></p><blockquote><p>Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone. </p><p>Or, less formally, ‘‘Given enough eyeballs, all bugs are shallow.’’</p></blockquote><p><span>Linux succeeded because it was built in the open. Users knew </span><em>exactly</em><span> what they were getting and had the opportunity to file bugs or even attempt to fix them on their own with community support. </span></p><p><span>The same is true for open source models. Even </span><a href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="">software 2.0</a><span> needs to be audited. Otherwise, things can change under the hood, leading to regressions in your application. This is unacceptable for most business use cases.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" width="526" height="377.15934065934067" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1044,&#34;width&#34;:1456,&#34;resizeWidth&#34;:526,&#34;bytes&#34;:275898,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption><a href="https://arxiv.org/pdf/2307.09009.pdf" rel="">This paper</a><span> recently showed that OpenAI’s endpoints drift over time. You cannot be confident that a prompt that works flawlessly will perform the same a month from now.</span></figcaption></figure></div><p><span>Adopting an open source approach for AI technology can create a wide-reaching network of checks and balances. Scientists and developers globally can peer-review, critique, study, and understand the underlying mechanisms, leading to improved safety, reliability, interpretability, and trust. Furthermore, widespread knowledge helps advance the technology responsibly while mitigating the risk of its misuse. </span><strong>Hugging Face is the new RedHat.</strong><span> </span></p><p>You can only trust models that you own and control. The same can’t be said for black box APIs. This is also why the AI safety argument against open source makes zero sense. History suggests, open source AI is, in fact, safer. </p><p><span>Why do people currently prefer closed source? Two reasons: </span><em>ease-of-use</em><span> and </span><em>mindshare</em><span>.</span></p><p>Open source is much harder to use than closed source models. It seems like you need to hire a team of machine learning engineers to build on top of open source as opposed to using the OpenAI API. This is ok, and will be true in the short-term. This is the cost of control and the rapid pace of innovation. People who are willing to spend time at the frontier will be treated by being able to build much better products. The ergonomics will get better.</p><p><span>The more unfortunate issue is </span><strong>mindshare</strong><span>. </span></p><p>Closed source model providers have captured the collective mindshare of this AI hype cycle. People don’t have time to mess around with open source nor do they have the awareness of what open source is capable of. But they do know about OpenAI, Pinecone, and LangChain. </p><p>Using the right tool is often conflated with using the best known tool. The current hype cycle has put closed source AI in the spotlight. As open source offerings mature and become more user-friendly and customizable, they will emerge as the superior choice for many applications. </p><p>Rather than getting swept up in the hype, forward-thinking organizations will use this period to deeply understand their needs and lay the groundwork to take full advantage of open source AI. They will build defensible and differentiated AI experiences on open technology. This measured approach enables a sustainable competitive advantage in the long run. </p><p>The future remains bright for pragmatic adopters who see past the hype and keep their eyes on the true prize: truly open AI.</p></div></div></div></div>
  </body>
</html>
