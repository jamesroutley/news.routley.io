<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/google/langextract">Original</a>
    <h1>LangExtract: Python library for extracting structured data from language models</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://github.com/google/langextract">
    <img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg" alt="LangExtract Logo" width="128"/>
  </a>
</p>

<p dir="auto"><a href="https://pypi.org/project/langextract/" rel="nofollow"><img src="https://camo.githubusercontent.com/8ab672bf1b3dfe57df2f86b2b67a08b0493c72aebae76878c7fb9be309d3cd28/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c616e67657874726163742e737667" alt="PyPI version" data-canonical-src="https://img.shields.io/pypi/v/langextract.svg"/></a>
<a href="https://github.com/google/langextract"><img src="https://camo.githubusercontent.com/ad8cdf4ad306f0ddcee22ddaa1229d6c997e35263d123a4ff64d3d56aef7bfcf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6c616e67657874726163742e7376673f7374796c653d736f6369616c266c6162656c3d53746172" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star"/></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg"><img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg" alt="Tests"/></a></p>

<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#why-langextract">Why LangExtract?</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#api-key-setup-for-cloud-models">API Key Setup for Cloud Models</a></li>
<li><a href="#more-examples">More Examples</a>
<ul dir="auto">
<li><a href="#romeo-and-juliet-full-text-extraction"><em>Romeo and Juliet</em> Full Text Extraction</a></li>
<li><a href="#medication-extraction">Medication Extraction</a></li>
<li><a href="#radiology-report-structuring-radextract">Radiology Report Structuring: RadExtract</a></li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#testing">Testing</a></li>
<li><a href="#disclaimer">Disclaimer</a></li>
</ul>

<p dir="auto">LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.</p>

<ol dir="auto">
<li><strong>Precise Source Grounding:</strong> Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.</li>
<li><strong>Reliable Structured Outputs:</strong> Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.</li>
<li><strong>Optimized for Long Documents:</strong> Overcomes the &#34;needle-in-a-haystack&#34; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.</li>
<li><strong>Interactive Visualization:</strong> Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.</li>
<li><strong>Flexible LLM Support:</strong> Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.</li>
<li><strong>Adaptable to Any Domain:</strong> Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.</li>
<li><strong>Leverages LLM World Knowledge:</strong> Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.</li>
</ol>

<blockquote>
<p dir="auto"><strong>Note:</strong> Using cloud-hosted models like Gemini requires an API key. See the <a href="#api-key-setup-for-cloud-models">API Key Setup</a> section for instructions on how to get and configure your key.</p>
</blockquote>
<p dir="auto">Extract structured information with just a few lines of code.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">1. Define Your Extraction Task</h3><a id="user-content-1-define-your-extraction-task" aria-label="Permalink: 1. Define Your Extraction Task" href="#1-define-your-extraction-task"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&#34;&#34;&#34;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&#34;&#34;&#34;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&#34;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&#34;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&#34;character&#34;,
                extraction_text=&#34;ROMEO&#34;,
                attributes={&#34;emotional_state&#34;: &#34;wonder&#34;}
            ),
            lx.data.Extraction(
                extraction_class=&#34;emotion&#34;,
                extraction_text=&#34;But soft!&#34;,
                attributes={&#34;feeling&#34;: &#34;gentle awe&#34;}
            ),
            lx.data.Extraction(
                extraction_class=&#34;relationship&#34;,
                extraction_text=&#34;Juliet is the sun&#34;,
                attributes={&#34;type&#34;: &#34;metaphor&#34;}
            ),
        ]
    )
]"><pre><span>import</span> <span>langextract</span> <span>as</span> <span>lx</span>
<span>import</span> <span>textwrap</span>

<span># 1. Define the prompt and extraction rules</span>
<span>prompt</span> <span>=</span> <span>textwrap</span>.<span>dedent</span>(<span>&#34;&#34;&#34;<span>\</span></span>
<span><span></span>    Extract characters, emotions, and relationships in order of appearance.</span>
<span>    Use exact text for extractions. Do not paraphrase or overlap entities.</span>
<span>    Provide meaningful attributes for each entity to add context.&#34;&#34;&#34;</span>)

<span># 2. Provide a high-quality example to guide the model</span>
<span>examples</span> <span>=</span> [
    <span>lx</span>.<span>data</span>.<span>ExampleData</span>(
        <span>text</span><span>=</span><span>&#34;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&#34;</span>,
        <span>extractions</span><span>=</span>[
            <span>lx</span>.<span>data</span>.<span>Extraction</span>(
                <span>extraction_class</span><span>=</span><span>&#34;character&#34;</span>,
                <span>extraction_text</span><span>=</span><span>&#34;ROMEO&#34;</span>,
                <span>attributes</span><span>=</span>{<span>&#34;emotional_state&#34;</span>: <span>&#34;wonder&#34;</span>}
            ),
            <span>lx</span>.<span>data</span>.<span>Extraction</span>(
                <span>extraction_class</span><span>=</span><span>&#34;emotion&#34;</span>,
                <span>extraction_text</span><span>=</span><span>&#34;But soft!&#34;</span>,
                <span>attributes</span><span>=</span>{<span>&#34;feeling&#34;</span>: <span>&#34;gentle awe&#34;</span>}
            ),
            <span>lx</span>.<span>data</span>.<span>Extraction</span>(
                <span>extraction_class</span><span>=</span><span>&#34;relationship&#34;</span>,
                <span>extraction_text</span><span>=</span><span>&#34;Juliet is the sun&#34;</span>,
                <span>attributes</span><span>=</span>{<span>&#34;type&#34;</span>: <span>&#34;metaphor&#34;</span>}
            ),
        ]
    )
]</pre></div>

<p dir="auto">Provide your input text and the prompt materials to the <code>lx.extract</code> function.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# The input text to be processed
input_text = &#34;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&#34;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&#34;gemini-2.5-flash&#34;,
)"><pre><span># The input text to be processed</span>
<span>input_text</span> <span>=</span> <span>&#34;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&#34;</span>

<span># Run the extraction</span>
<span>result</span> <span>=</span> <span>lx</span>.<span>extract</span>(
    <span>text_or_documents</span><span>=</span><span>input_text</span>,
    <span>prompt_description</span><span>=</span><span>prompt</span>,
    <span>examples</span><span>=</span><span>examples</span>,
    <span>model_id</span><span>=</span><span>&#34;gemini-2.5-flash&#34;</span>,
)</pre></div>
<blockquote>
<p dir="auto"><strong>Model Selection</strong>: <code>gemini-2.5-flash</code> is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, <code>gemini-2.5-pro</code> may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the <a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2" rel="nofollow">rate-limit documentation</a> for details.</p>
<p dir="auto"><strong>Model Lifecycle</strong>: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions" rel="nofollow">official model version documentation</a> to stay informed about the latest stable and legacy versions.</p>
</blockquote>

<p dir="auto">The extractions can be saved to a <code>.jsonl</code> file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&#34;extraction_results.jsonl&#34;, output_dir=&#34;.&#34;)

# Generate the visualization from the file
html_content = lx.visualize(&#34;extraction_results.jsonl&#34;)
with open(&#34;visualization.html&#34;, &#34;w&#34;) as f:
    f.write(html_content)"><pre><span># Save the results to a JSONL file</span>
<span>lx</span>.<span>io</span>.<span>save_annotated_documents</span>([<span>result</span>], <span>output_name</span><span>=</span><span>&#34;extraction_results.jsonl&#34;</span>, <span>output_dir</span><span>=</span><span>&#34;.&#34;</span>)

<span># Generate the visualization from the file</span>
<span>html_content</span> <span>=</span> <span>lx</span>.<span>visualize</span>(<span>&#34;extraction_results.jsonl&#34;</span>)
<span>with</span> <span>open</span>(<span>&#34;visualization.html&#34;</span>, <span>&#34;w&#34;</span>) <span>as</span> <span>f</span>:
    <span>f</span>.<span>write</span>(<span>html_content</span>)</pre></div>
<p dir="auto">This creates an animated and interactive HTML file:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif"><img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " data-animated-image=""/></a></p>
<blockquote>
<p dir="auto"><strong>Note on LLM Knowledge Utilization:</strong> This example demonstrates extractions that stay close to the text evidence - extracting &#34;longing&#34; for Lady Juliet&#39;s emotional state and identifying &#34;yearning&#34; from &#34;gazed longingly at the stars.&#34; The task could be modified to generate attributes that draw more heavily from the LLM&#39;s world knowledge (e.g., adding <code>&#34;identity&#34;: &#34;Capulet family daughter&#34;</code> or <code>&#34;literary_context&#34;: &#34;tragic heroine&#34;</code>). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.</p>
</blockquote>
<div dir="auto"><h3 tabindex="-1" dir="auto">Scaling to Longer Documents</h3><a id="user-content-scaling-to-longer-documents" aria-label="Permalink: Scaling to Longer Documents" href="#scaling-to-longer-documents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&#34;https://www.gutenberg.org/files/1513/1513-0.txt&#34;,
    prompt_description=prompt,
    examples=examples,
    model_id=&#34;gemini-2.5-flash&#34;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)"><pre><span># Process Romeo &amp; Juliet directly from Project Gutenberg</span>
<span>result</span> <span>=</span> <span>lx</span>.<span>extract</span>(
    <span>text_or_documents</span><span>=</span><span>&#34;https://www.gutenberg.org/files/1513/1513-0.txt&#34;</span>,
    <span>prompt_description</span><span>=</span><span>prompt</span>,
    <span>examples</span><span>=</span><span>examples</span>,
    <span>model_id</span><span>=</span><span>&#34;gemini-2.5-flash&#34;</span>,
    <span>extraction_passes</span><span>=</span><span>3</span>,    <span># Improves recall through multiple passes</span>
    <span>max_workers</span><span>=</span><span>20</span>,         <span># Parallel processing for speed</span>
    <span>max_char_buffer</span><span>=</span><span>1000</span>    <span># Smaller contexts for better accuracy</span>
)</pre></div>
<p dir="auto">This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. <strong><a href="https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md">See the full <em>Romeo and Juliet</em> extraction example →</a></strong> for detailed results and performance insights.</p>



<p dir="auto"><em>Recommended for most users. For isolated environments, consider using a virtual environment:</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract"><pre>python -m venv langextract_env
<span>source</span> langextract_env/bin/activate  <span><span>#</span> On Windows: langextract_env\Scripts\activate</span>
pip install langextract</pre></div>

<p dir="auto">LangExtract uses modern Python packaging with <code>pyproject.toml</code> for dependency management:</p>
<p dir="auto"><em>Installing with <code>-e</code> puts the package in development mode, allowing you to modify the code without reinstalling.</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &#34;.[dev]&#34;

# For testing (includes pytest):
pip install -e &#34;.[test]&#34;"><pre>git clone https://github.com/google/langextract.git
<span>cd</span> langextract

<span><span>#</span> For basic installation:</span>
pip install -e <span>.</span>

<span><span>#</span> For development (includes linting tools):</span>
pip install -e <span><span>&#34;</span>.[dev]<span>&#34;</span></span>

<span><span>#</span> For testing (includes pytest):</span>
pip install -e <span><span>&#34;</span>.[test]<span>&#34;</span></span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&#34;your-api-key&#34; langextract python your_script.py"><pre>docker build -t langextract <span>.</span>
docker run --rm -e LANGEXTRACT_API_KEY=<span><span>&#34;</span>your-api-key<span>&#34;</span></span> langextract python your_script.py</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">API Key Setup for Cloud Models</h2><a id="user-content-api-key-setup-for-cloud-models" aria-label="Permalink: API Key Setup for Cloud Models" href="#api-key-setup-for-cloud-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#39;ll need to
set up an API key. On-device models don&#39;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.</p>

<p dir="auto">Get API keys from:</p>
<ul dir="auto">
<li><a href="https://aistudio.google.com/app/apikey" rel="nofollow">AI Studio</a> for Gemini models</li>
<li><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview" rel="nofollow">Vertex AI</a> for enterprise use</li>
<li><a href="https://platform.openai.com/api-keys" rel="nofollow">OpenAI Platform</a> for OpenAI models</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Setting up API key in your environment</h3><a id="user-content-setting-up-api-key-in-your-environment" aria-label="Permalink: Setting up API key in your environment" href="#setting-up-api-key-in-your-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Option 1: Environment Variable</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="export LANGEXTRACT_API_KEY=&#34;your-api-key-here&#34;"><pre><span>export</span> LANGEXTRACT_API_KEY=<span><span>&#34;</span>your-api-key-here<span>&#34;</span></span></pre></div>
<p dir="auto"><strong>Option 2: .env File (Recommended)</strong></p>
<p dir="auto">Add your API key to a <code>.env</code> file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#39;EOF&#39;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#39;.env&#39; &gt;&gt; .gitignore"><pre><span><span>#</span> Add API key to .env file</span>
cat <span>&gt;&gt;</span> .env <span><span>&lt;&lt;</span> &#39;<span>EOF</span>&#39;</span>
<span>LANGEXTRACT_API_KEY=your-api-key-here</span>
<span><span>EOF</span></span>

<span><span>#</span> Keep your API key secure</span>
<span>echo</span> <span><span>&#39;</span>.env<span>&#39;</span></span> <span>&gt;&gt;</span> .gitignore</pre></div>
<p dir="auto">In your Python code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&#34;Extract information...&#34;,
    examples=[...],
    model_id=&#34;gemini-2.5-flash&#34;
)"><pre><span>import</span> <span>langextract</span> <span>as</span> <span>lx</span>

<span>result</span> <span>=</span> <span>lx</span>.<span>extract</span>(
    <span>text_or_documents</span><span>=</span><span>input_text</span>,
    <span>prompt_description</span><span>=</span><span>&#34;Extract information...&#34;</span>,
    <span>examples</span><span>=</span>[...],
    <span>model_id</span><span>=</span><span>&#34;gemini-2.5-flash&#34;</span>
)</pre></div>
<p dir="auto"><strong>Option 3: Direct API Key (Not Recommended for Production)</strong></p>
<p dir="auto">You can also provide the API key directly in your code, though this is not recommended for production use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&#34;Extract information...&#34;,
    examples=[...],
    model_id=&#34;gemini-2.5-flash&#34;,
    api_key=&#34;your-api-key-here&#34;  # Only use this for testing/development
)"><pre><span>result</span> <span>=</span> <span>lx</span>.<span>extract</span>(
    <span>text_or_documents</span><span>=</span><span>input_text</span>,
    <span>prompt_description</span><span>=</span><span>&#34;Extract information...&#34;</span>,
    <span>examples</span><span>=</span>[...],
    <span>model_id</span><span>=</span><span>&#34;gemini-2.5-flash&#34;</span>,
    <span>api_key</span><span>=</span><span>&#34;your-api-key-here&#34;</span>  <span># Only use this for testing/development</span>
)</pre></div>

<p dir="auto">LangExtract also supports OpenAI models. Example OpenAI configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langextract.inference import OpenAILanguageModel

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    language_model_type=OpenAILanguageModel,
    model_id=&#34;gpt-4o&#34;,
    api_key=os.environ.get(&#39;OPENAI_API_KEY&#39;),
    fence_output=True,
    use_schema_constraints=False
)"><pre><span>from</span> <span>langextract</span>.<span>inference</span> <span>import</span> <span>OpenAILanguageModel</span>

<span>result</span> <span>=</span> <span>lx</span>.<span>extract</span>(
    <span>text_or_documents</span><span>=</span><span>input_text</span>,
    <span>prompt_description</span><span>=</span><span>prompt</span>,
    <span>examples</span><span>=</span><span>examples</span>,
    <span>language_model_type</span><span>=</span><span>OpenAILanguageModel</span>,
    <span>model_id</span><span>=</span><span>&#34;gpt-4o&#34;</span>,
    <span>api_key</span><span>=</span><span>os</span>.<span>environ</span>.<span>get</span>(<span>&#39;OPENAI_API_KEY&#39;</span>),
    <span>fence_output</span><span>=</span><span>True</span>,
    <span>use_schema_constraints</span><span>=</span><span>False</span>
)</pre></div>
<p dir="auto">Note: OpenAI models require <code>fence_output=True</code> and <code>use_schema_constraints=False</code> because LangExtract doesn&#39;t implement schema constraints for OpenAI yet.</p>

<p dir="auto">Additional examples of LangExtract in action:</p>
<div dir="auto"><h3 tabindex="-1" dir="auto"><em>Romeo and Juliet</em> Full Text Extraction</h3><a id="user-content-romeo-and-juliet-full-text-extraction" aria-label="Permalink: Romeo and Juliet Full Text Extraction" href="#romeo-and-juliet-full-text-extraction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of <em>Romeo and Juliet</em> from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.</p>
<p dir="auto"><strong><a href="https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md">View <em>Romeo and Juliet</em> Full Text Example →</a></strong></p>

<blockquote>
<p dir="auto"><strong>Disclaimer:</strong> This demonstration is for illustrative purposes of LangExtract&#39;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.</p>
</blockquote>
<p dir="auto">LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#39;s effectiveness for healthcare applications.</p>
<p dir="auto"><strong><a href="https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md">View Medication Examples →</a></strong></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Radiology Report Structuring: RadExtract</h3><a id="user-content-radiology-report-structuring-radextract" aria-label="Permalink: Radiology Report Structuring: RadExtract" href="#radiology-report-structuring-radextract"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.</p>
<p dir="auto"><strong><a href="https://huggingface.co/spaces/google/radextract" rel="nofollow">View RadExtract Demo →</a></strong></p>

<p dir="auto">Contributions are welcome! See <a href="https://github.com/google/langextract/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> to get started
with development, testing, and pull requests. You must sign a
<a href="https://cla.developers.google.com/about" rel="nofollow">Contributor License Agreement</a>
before submitting patches.</p>

<p dir="auto">To run tests locally from the source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &#34;.[test]&#34;

# Run all tests
pytest tests"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/google/langextract.git
<span>cd</span> langextract

<span><span>#</span> Install with test dependencies</span>
pip install -e <span><span>&#34;</span>.[test]<span>&#34;</span></span>

<span><span>#</span> Run all tests</span>
pytest tests</pre></div>
<p dir="auto">Or reproduce the full CI matrix locally with tox:</p>
<div dir="auto" data-snippet-clipboard-copy-content="tox  # runs pylint + pytest on Python 3.10 and 3.11"><pre>tox  <span><span>#</span> runs pylint + pytest on Python 3.10 and 3.11</span></pre></div>


<p dir="auto">This project uses automated formatting tools to maintain consistent code style:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml"><pre><span><span>#</span> Auto-format all code</span>
./autoformat.sh

<span><span>#</span> Or run formatters separately</span>
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml</pre></div>

<p dir="auto">For automatic formatting checks:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run"><pre>pre-commit install  <span><span>#</span> One-time setup</span>
pre-commit run --all-files  <span><span>#</span> Manual run</span></pre></div>

<p dir="auto">Run linting before submitting PRs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pylint --rcfile=.pylintrc langextract tests"><pre>pylint --rcfile=.pylintrc langextract tests</pre></div>
<p dir="auto">See <a href="https://github.com/google/langextract/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for full development guidelines.</p>

<p dir="auto">This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the <a href="https://github.com/google/langextract/blob/main/LICENSE">Apache 2.0 License</a>.
For health-related applications, use of LangExtract is also subject to the
<a href="https://developers.google.com/health-ai-developer-foundations/terms" rel="nofollow">Health AI Developer Foundations Terms of Use</a>.</p>
<hr/>
<p dir="auto"><strong>Happy Extracting!</strong></p>
</article></div></div>
  </body>
</html>
