<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://behind.pretix.eu/2025/05/23/captchas-are-over/">Original</a>
    <h1>CAPTCHAs are over (in ticketing)</h1>
    
    <div id="readability-page-1" class="page"><div role="main">

    <article>

        

<!--         <header class="post-header">
            <a id="blog-logo" href="https://behind.pretix.eu">
                
                    <span class="blog-title">pretix – behind the scenes</span>
                
            </a>
        </header> -->

        <!-- <span class="post-meta">
            <time datetime="2025-05-23">23 May 2025</time>
            
                on Technology, Web, and Accessibility
            
        </span> -->

        <!-- <h1 class="post-title">CAPTCHAs are over (in ticketing)</h1> -->

        <section>
            <p>One of the issues in ticketing is that many events have much more demand for tickets than they can supply.
Obviously, this is a good problem to have (better than empty halls), but it attracts certain types of bad actors trying to get as many tickets as possible in order to resell them (“ticket scalping”).
While this is possible of course just by buying tickets like a regular customer, many of them use computer programs (“bots”) to enhance either their chance of claiming a ticket or their scale of operation by buying more tickets.</p>

<p>The naive economic solution to the problem would be raising ticket prices step by step until it is no longer attractive for scalpers to resell your ticket, because the original price of the ticket is already the maximum that people are willing to pay, creating an <a href="https://en.wikipedia.org/wiki/Economic_equilibrium">economic equilibrium</a>.
Most organizers, including for-profit organizations, do not want to choose this option due to ethical concerns or concerns about community building.</p>

<p>Therefore, whenever this topic comes up, someone quickly suggests a technical solution – usually a <a href="https://en.wikipedia.org/wiki/CAPTCHA">CAPTCHA</a>.
I believe that CAPTCHAs no longer provide any meaningful protection in this scenario.
Here’s why:</p>

<h3 id="we-have-run-out-of-problems">We have run out of problems</h3>

<p>CAPTCHAs were invented over 20 years ago to distinguish between human and non-human users of a website.
The basic idea is this:
Ask the user to solve a problem that is easy to solve for a human but hard to solve for a computer.
The most popular early problem used in CAPTCHAs was <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">text recognition</a>, often under artificially bad conditions.
This often looked like this:</p>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-v1.jpg" alt="reCAPTCHA v1 using two distorted words"/></p>

<p>These days, a state-of-the-art machine learning model is able to <em>easily</em> solve this problem and recognize the words.
Any attempt to make the text even more unreadable will make it too hard for the average human user as well, making it unsuitable.</p>

<p>Around ten years ago, Google and others therefore switched to one of the problems that remained hard at the time: Image recognition.
I don’t need to explain this to you, you have all clicked on way to many motorcycles yourselves:</p>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-img.png" alt="reCAPTCHA v2 asking to select all squares with motorcycles"/></p>

<p>However, in 2025, this is also no longer a challenge to computers.
A modern image detection model will be able to point out the motorcycles in this screenshot faster than any human.</p>

<p>To make it harder, a good CAPTCHA always needs a second type of problem since we need to provide accessibility for humans who cannot see.
This has always been the right thing to do, but in Europe it’s now also becoming <a href="https://en.wikipedia.org/wiki/European_Accessibility_Act">the law</a>.
A typical choice is an audio recognition task:</p>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/recaptcha-audio.png" alt="reCAPTCHA v2 asking to type the content of an audio snippet"/></p>

<p>Have you seen what modern speech recognition models can do?
They can certainly understand audio better than any non-native speaker of most languages.</p>

<p>Unfortunately, I don’t see any new approaches of this type replacing the current ones.
We are running out of suitable problems that humans can solve better than computers.
Of course, there are still a lot of things that humans can do better than computers – but not things that <em>all</em> humans can do better than computers and that can be tested over the internet with a few simple clicks in a few seconds.</p>

<h3 id="if-ai-is-the-problem-surely-it-is-also-the-solution">If AI is the problem, surely it is also the solution</h3>

<p>What I’m saying above is not new information.
That’s why people working on bot protection have moved to solutions other than having the user solve puzzles many years ago.
One of these approaches is behavior analysis using machine learning.</p>

<p>In 2014, Google presented the <a href="https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html">one-click CAPTCHA</a> and later, with reCAPTCHA v3, the “invisible CAPTCHA”.
Similar services are offered by other companies like Cloudflare, Akamai, and others.</p>

<p>They rely on analyzing everything they know about you and feeding it into a large machine-learning model to compare you with behavior they know from humans and bots.
That’s a clever idea, but it comes with two significant drawbacks:</p>

<p>First, it’s a privacy nightmare.
The entire idea only works if you’re collecting a lot of data.
Given the types of companies who offer this service, they most likely not only collect data about how people behave on <em>your</em> website, but how people behave on other websites as well.
This requires the creation of extensive centralized profiles about the people visiting your site.</p>

<p>Second, at least in ticketing, there’s not really a good way to handle false positives.
Say the model makes a wrong decision and puts an entire class of valid human users into the “bot” bucket, for example, everyone who has never visited one of the sites protected by the company before, or everyone who uses a specific assistive technology.</p>

<p>In some scenarios, e.g. when protecting an online forum against spam, you can place submissions with a high bot score into manual review.
However, in high-demand ticketing, you need to make a binary choice:
Either you sell the user a ticket, or you don’t.
Risking to exclude an entire class of users is not only unethical, but also a possible legal risk.</p>

<p>So what do these solutions usually do?
If your bot score is too high, they show you a classical CAPTCHA again.
This however, as we’ve seen above, is no longer a real hurdle to bots.</p>

<h3 id="many-signals-are-not-accessibility-safe">Many signals are not accessibility-safe</h3>

<p>Say we want to build a solution that is built on behavior analysis only of the user’s behavior on our site, not across half the internet.
What information do we have to work with?</p>

<p>Fifteen years ago, bots used to be scripts that used HTTP libraries to interact with web pages.
So we could look at JavaScript execution behavior, HTTP header fingerprinting, timing differences in loading resources, and similar signals.
These days, bots are using <a href="https://pptr.dev/">real browsers</a> that load web pages just like regular users.
The bots use the APIs provided by browsers to analyze the page and control what the browser does.</p>

<p>Now, I’ve already mentioned that we need to build solutions in a way that are accessible to all users.
This includes users who make use of <a href="https://en.wikipedia.org/wiki/Assistive_technology">assistive technology</a> such as screen readers.
And what does a screen reader do?
It interacts with the browser through the APIs the browser provides to analyze the page and control what the browser does.</p>

<p>So all the differences we might expect between an “average user” and a bot, such as the lack of mouse movement before a button is clicked, we will also see when someone is using assistive technology (or, in this example, just a touch screen).</p>

<h3 id="proof-of-work-doesnt-work-for-ticketing">Proof of Work doesn’t work for ticketing</h3>

<p>The current trend in bot prevention is using a <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof of work</a> scheme, popularized by solutions like <a href="https://friendlycaptcha.com/">Friendly Captcha</a> or <a href="https://anubis.techaro.lol/">Anubis</a>.</p>

<p>The idea between proof of work for spam protection goes back to 1997 and is quite different than the CAPTCHAs presented above.
We’re no longer looking for problems that humans can solve better than computers.
We’re now looking for problems that are <strong>costly</strong> for computers to solve.
The typical example is brute-forcing a hash function.
Any computer can do that, but – depending on the capability of the computer – it will take the computer a certain amount of time.
Computing time costs power, and power costs money.</p>

<p>The idea is to make spamming too expensive for the spammers.
If you need to spend 0.0001 € in power to access a website, a human user will probably not care.
For someone who is trying to spam millions of messages or scrape the entire internet, however, it will add up to a significant total cost and the spamming or scraping might not be worth it any more.</p>

<p>The ethically questionable idea of burning power on useless computations (even if just a little) aside, I don’t consider this economic argument plausible for ticketing.
If a spammer needs to spend 0.0001 € in power to access the site only to gain a marginal profit of 0.00005 €, they are losing money with every site access.
However, if a ticket scalper needs to spend 0.0001 € in power to buy a ticket that they will later sell at a 200 € profit, this will not stop them.</p>

<h3 id="the-economics-problem-affects-traditional-captchas-as-well">The economics problem affects traditional CAPTCHAs as well</h3>

<p>This economic argument also applies to all the traditional CAPTCHAs we’ve talked about.
While modern machine learning can easily recognize characters, spoken words, or images of motorcycles, every run of the machine learning model comes at a cost.
Google’s cloud vision API, for example, costs 0.0015 $ per picture for text detection and 0.00225 € for object localization.</p>

<p>Even if we did find new problems that are still too hard for computers, there are companies who employ gig workers in low-price countries to solve CAPTCHAs for you.
It’s not hard to find CAPTCHA bypass providers that use a combination of AI and low-paid workers to solve CAPTCHAs for you at speed and very low cost:</p>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/captcha-solving-services.png" alt="reCAPTCHA v2 asking to type the content of an audio snippet"/></p>

<h3 id="so-whats-left">So what’s left?</h3>

<p>I believe that the possibility to reliably tell a bot from a human through something simple like a CAPTCHA is a thing of the past in an industry like ticketing where the financial motivation for circumvention is very high.</p>

<p>One tactic that can still be quite effective is limiting resale possibilities by strongly personalizing tickets, including ID card verification of at least a relevant sample of sold tickets at the event entrance.
This still allows scalpers to sell something like “bot as a service” in advance of the ticket sale but limits any type of “buy-and-resell-later” schemes.
Of course it also harms real buyers who want to go to a concert with a +1 but do not yet know who they will bring.</p>

<p>A related option is to strongly bind purchase limits to other resources that are not easy to acquire quickly in large amounts, such as allowing only X tickets per (verified) phone number or – as you will need to collect payment information for the ticket anyway – per verified credit card or verified bank account.
People are able to easily get multiple phone numbers and bank accounts, but getting dozens or hundreds of them becomes a lot more effort.
A sufficiently motivated bad actor will still be able to acquire many of these, but it will increase the cost involved significantly and also many possible circumvention techniques (like buying stolen credit card numbers) are significantly more illegal than solving CAPTCHAs with AI and might be beyond what at least some of the actors are ready to do.</p>

<h3 id="the-bap-theorem">The BAP theorem</h3>

<p>In computer science, there is the “<a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a>”.
It states that when designing a database system, you can choose between the three properties “consistency”, “availability”, and “partition tolerance”.
And while it is possible to combine two of them, it is impossible to combine all three – you always need to give one up.</p>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/cap.svg" alt="CAP theorem shown as a Venn Diagram, based on CC-BY-SA diagram of Moond on Wikimedia"/></p>

<p>I’m proposing a similar theorem for bot protection, the “BAP theorem”, stating that you can only combine two of the following three properties:</p>

<ul>
  <li><strong>B</strong>ot-resistance</li>
  <li><strong>A</strong>ccessibility</li>
  <li><strong>P</strong>rivacy-friendliness</li>
</ul>

<p><img src="https://behind.pretix.eu/assets/images/2025-05-captchas/bap.svg" alt="Variation of the Venn Diagram with B = bot-resistant, A = accessible, P = privacy-friendly"/></p>

<p>The possible combinations of these properties would be:</p>

<ul>
  <li>
    <p><strong>BA</strong>: Bot-resistant and accessible, but not privacy-friendly. In this category we find all approaches based on heavily personalized tickets, possibly with strong identification methods. If there is a good accessibility fallback, then large-scale behaviour analysis also falls under this category.</p>
  </li>
  <li>
    <p><strong>BP</strong>: Bot-resistant and privacy-friendly, but not accessible. This category contains all approaches trying to use the remaining bits and pieces that are hard for AI and easy for humans, such as heavily relying on mouse movements, any puzzles involving complex user interaction, or any tricks playing with the limitations of the browser APIs.</p>
  </li>
  <li>
    <p><strong>AP</strong>: Accessible and privacy-friendly, but not bot-resistant. This category is easy and includes many things down to the trivial case of no protection at all, or any attempt to fight off at least very simple bots without harming the other goals.</p>
  </li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>Since accessibility is <a href="https://en.wikipedia.org/wiki/European_Accessibility_Act">required by law</a>, we’re basically left with the first or last option.
As much as I hope that I’ve missed something significant, I feel the conclusion is inevitable:</p>

<p><strong>Events will need to decide whether they want to protect against bots, or preserve high privacy standards. You will not be able to do both.</strong></p>

<p>It remains, unfortunately, very hard to solve social problems with technology.
Looking at social solutions, some have tried <a href="https://en.wikipedia.org/wiki/Ticket_resale#International_responses">making ticket scalping illegal</a> – with varying success.
If your country is not on that list, lobbying for it should be part of your solution strategy – but will most likely will be a very long road to a solution.</p>


        </section>

        

        

    </article>

</div></div>
  </body>
</html>
