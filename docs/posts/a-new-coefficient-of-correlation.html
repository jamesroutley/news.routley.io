<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/1909.10140">Original</a>
    <h1>A New Coefficient of Correlation</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/1909.10140">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Is it possible to define a coefficient of correlation which is (a) as simple
as the classical coefficients like Pearson&#39;s correlation or Spearman&#39;s
correlation, and yet (b) consistently estimates some simple and interpretable
measure of the degree of dependence between the variables, which is 0 if and
only if the variables are independent and 1 if and only if one is a measurable
function of the other, and (c) has a simple asymptotic theory under the
hypothesis of independence, like the classical coefficients? This article
answers this question in the affirmative, by producing such a coefficient. No
assumptions are needed on the distributions of the variables. There are several
coefficients in the literature that converge to 0 if and only if the variables
are independent, but none that satisfy any of the other properties mentioned
above.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Sourav Chatterjee [<a href="https://arxiv.org/show-email/c763ebed/1909.10140">view email</a>]
      </p></div></div>
  </body>
</html>
