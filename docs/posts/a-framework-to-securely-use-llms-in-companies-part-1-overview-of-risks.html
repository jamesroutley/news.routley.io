<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boringappsec.substack.com/p/edition-21-a-framework-to-securely">Original</a>
    <h1>A framework to securely use LLMs in companies – Part 1: Overview of Risks</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png" width="1024" height="1024" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1024,&#34;width&#34;:1024,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:1794460,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a><figcaption>This image was generated by the Dall-E as a response to the prompt: “Confused AI bot, pencil sketch”</figcaption></figure></div><p>The potential of using AI in enterprises has been on the rise for the last few years. The release of ChatGPT made it apparent to enterprises that AI could supercharge their existing applications. However, like with any new technology, the usage of LLMs also brings with it some risks. Depending on how the LLMs are deployed (training an in-house LLM v/s 3rd party LLMs) and how the LLMs are used (by individuals to supercharge their work v/s integrating with LLM APIs in applications), the risks LLMs pose will change. This post outlines key risks and helps prioritize them based on your organization’s use case. </p><p>Most posts in this newsletter are based on my personal AppSec experience. Like most people in Appsec, I do not have any significant experience building, managing, or securing LLM usage at scale.  In that sense, this post is a departure from the norm. However, I am still publishing this as a submission to the growing body of work on this subject. In the coming months, I hope we will have some consensus on how to address this important topic.</p><p>Here are a few caveats you should keep in mind before you read this document: </p><ol><li><p>There is a reasonable chance that some of this information will be outdated a few weeks from now. That is just the nature of fast-changing technology. Most posts in Boring AppSec are “timeless topics”. This one is timely and fluid. </p></li><li><p>Most of my research for this post is secondary. This means, not all the information provided here is based on my experience securing LLMs. Much of it is from other authors who have published excellent, public work (e.g.: OWASP Top 10 for LLMs). For instance, if an excellent description of a vulnerability already exists, it has been used here. All the material which has influenced my thinking is linked in the references section. </p><ol><li><p>Side note: The reference section has some amazing resources. Please feel free to enter rabbit holes :) </p></li></ol></li></ol><p>There are many ways to slice and dice LLM use cases, but from a Security perspective, it may help to categorize them as follows: </p><ol><li><p><strong>Use-cases</strong></p><ol><li><p>Employees use LLM tools to improve productivity or help with their work. (e.g.: Github Copilot, ChatGPT, and Google BARD). </p></li><li><p>Integrating applications with LLM APIs </p><ol><li><p><strong>Internal applications</strong><span> leveraging LLMs to improve their efficacy or inform internal decision-making. </span></p></li><li><p><strong>Customer-facing applications</strong><span>, where customer input informs part (or all) of the prompts sent to the LLM, and the response is consumed in some form by the customer.</span></p><ol><li><p>Note: I am using “customer” as a proxy for “someone external”. In your context, this could be a non-paying user, a business that uses your product, and so on.  </p></li></ol></li></ol></li></ol></li><li><p><strong>Deployment type.</strong><span> Companies can choose from 2 broad paths:</span></p><ol><li><p><strong>3rd party LLMs: </strong><span>Integrating applications with 3rd party LLMs such as OpenAI. </span></p></li><li><p><strong>Self-hosted LLMs: </strong><span>Deploying an open-source LLM in-house and using proprietary data to train the LLM.</span></p></li></ol></li></ol><blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png" width="1200" height="486.2669245647969" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png&#34;,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:419,&#34;width&#34;:1034,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Common LLM applications categorized by use-case and deployment type.</figcaption></figure></div></blockquote><p>A few points on the trade-off between self-hosted and 3rd party LLMs:</p><ol><li><p>Unless significant investments are made in building a cross-functional team involving ML engineers, security engineers, and privacy professionals, self-hosting an open-source LLM and training brings more security risk than leveraging a trusted 3rd party.</p></li><li><p> On the flip side, 3rd party hosted LLMs pose more privacy and data security risks. Over time, 3rd party hosted LLMs can also get really expensive.  </p></li><li><p>If using an LLM has to be a business differentiator for your organization, deploying and training a model in-house is the right way to go</p></li><li><p>Irrespective of which path your organization chooses, it is critical to understand the risks it poses and find ways to manage them. </p></li></ol><p>There are many excellent lists of risks to using LLMs (see references for a solid list). In this post, I am focusing on seven patterns that can pose a high level of risk for your organization. While it&#39;s clear that new risk categories will emerge over time, this list should give us a broad overview of the risks out there and more importantly, help us understand what risks are applicable to our organization. </p><p>While it’s tempting to solve all risks, it is important to prioritize.  Companies should evaluate which use-case and deployment type is most prevalent in their organization and focus on mitigating High-risk items first. The below table provides a summary of risk levels for the use cases and deployment models mentioned above. In the next section, we will dive deeper into each of the highlighted “high-risk” scenarios. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png" width="1200" height="457.3875802997859" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png&#34;,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:534,&#34;width&#34;:1401,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:303578,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Risk ranking of the most common LLM risks, categorized by use-case and deployment type</figcaption></figure></div><ol><li><p><strong>Prompt injection: </strong><em><span>“Prompt Injection Vulnerabilities in LLMs involve crafty inputs leading to undetected manipulations. The impact ranges from data exposure to unauthorized actions, serving attacker goals.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Customer-facing applications: </strong><span>A malicious customer can take advantage of the concatenation of user input to a pre-written prompt string to override the guardrails put in place for user prompts. Much like other forms of injection, user input is used as instructions to control the outcome of the application. </span></p></li><li><p><strong>Self-hosted LLMs:</strong><span> An attacker can take advantage of the concatenation of user input to a pre-written prompt string to override the guardrails put in place for user prompts. Much like other forms of injection, user input is used as instructions to control the outcome of the application. Prompt injection is a broad attack vector and can take many forms, including </span></p><ol><li><p>Trusting data from plugins or other third parties </p></li><li><p>Exploiting LLM hallucination. If an attacker can predict that LLMs recommend made-up package names, the attacker can create and publish such packages with malicious software in them </p></li></ol></li></ol></li><li><p><strong>Data leakage:</strong><span> </span><em><span>“Data leakage in LLMs can expose sensitive information or proprietary details, leading to privacy and security breaches. Proper data sanitization and clear terms of use are crucial for prevention.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Employees using online tools:</strong><span> Given there is a free version of these tools available, there is a risk of leaking sensitive data to the LLM tool. This risk is higher with tools like ChatGPT, where the prompts entered are used to train the underlying model (e.g.: Samsung code data). Note that other tools (such as Bard from Google) do not use prompts to train their model. However, the risk of leaking data to the tool owner still exists.</span></p></li><li><p><strong>Customer-facing applications:  </strong><span>When used without sufficient guardrails, customer-facing applications leveraging LLMs can leak sensitive (PHI, PII) and proprietary information to the customer.</span></p><ol><li><p><em>Note: Even when guardrails are in place, Prompt Injection can be used to bypass the guardrails, leading to data leakage</em></p></li></ol></li><li><p><strong>3rd party LLMs:</strong><span> Insecure usage can lead to sensitive information (PII, PHI) or proprietary details being leaked to 3rd parties leading to privacy and security breaches.  </span></p></li></ol></li><li><p><strong>Training data poisoning: </strong><em><strong>- </strong><span>“LLMs learn from diverse text but risk training data poisoning, leading to user misinformation. Overreliance on AI is a concern. Key data sources include Common Crawl, WebText, OpenWebText, and books” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Self-hosted LLMs: </strong><span>Training data poisoning is a significant risk for self-hosted LLMs. Insecure data training can lead to bias and hallucination. We need to break down the problem into various stages of the machine learning pipeline. For instance: Bias can be introduced in the training data (e.g.: all the training data came from a specific neighborhood), in the classifier algorithm, or in the prediction engine. </span></p></li></ol></li><li><p><strong>Denial of service: </strong><span>“</span><em><span>An attacker interacts with an LLM in a way that is particularly resource-consuming, causing the quality of service to degrade for them and other users, or for high resource costs to be incurred.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span> </span></p><ol><li><p><strong>Self-hosted LLMs:</strong><span>  LLMs are resource intensive to train and maintain. Carefully crafted prompts or malicious training data can lead to the LLM consuming a lot of infrastructure resources which can lead to resource exhaustion and hence, denial of service. </span></p></li></ol></li><li><p><strong>Money loss: </strong><span>Much like cloud computing resources, 3rd party LLMs charge based on consumption (OpenAI uses “tokens”, others use a similar mechanism). Unvalidated usage of these APIs can lead to a massive, unplanned cost escalation. </span></p><ol><li><p><strong>3rd party LLMs: </strong><span>Most 3rd party LLMs charge by usage. Higher the usage, the higher the cost. Attackers can carefully craft prompts to lead to massive charges on LLM usage. If the usage is capped to limit money loss, similar attacks can lead to DoS (denial of service).   </span></p></li></ol></li><li><p><strong>Insecure supply chain: </strong><em>“LLM supply chains risk integrity due to vulnerabilities leading to biases, security breaches, or system failures. Issues arise from pre-trained models, crowdsourced data, and plugin extensions.” - </em><span> </span><em><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span> </span></p><ol><li><p><strong>3rd party LLMs: </strong><span> Most popular 3rd party LLMs allow developers to build plugins on top of their platform (e.g.: OpenAI). While we may have a trusted relationship with the LLM (through an NDA and a contract), managing risk from plugins is harder. While the risk is significant, this is not very different from the risk of using other platforms (e.g.: using Github actions and importing 3rd party actions). </span></p></li><li><p><strong>Self-hosted LLMs:  </strong><span>An in-house LLM relies heavily on various kinds of third-party components. From malicious training data (which we may not have complete control of) to outsourcing suppliers who will train the data, there are many supply chain attack vectors to worry about</span></p></li></ol></li><li><p><strong>Overreliance on LLM-generated content: </strong><em>“Overreliance on LLMs can lead to misinformation or inappropriate content due to &#34;hallucinations.&#34; Without proper oversight, this can result in legal issues and reputational damage” </em><span> -</span><em> </em><span> </span><em><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span> </span></p><ol><li><p><strong>Employees using online tools: </strong><span>A common use case is to use such tools to generate source code. While the tool can help reduce development time, it can lead to using insecure (e.g.: code generated is susceptible to SSRF) or unlicensed code (e.g.: the code generated is from an open source repo. Even outside engineering, using LLM responses to make important decisions at work can lead to unpredictable outcomes. </span></p></li><li><p><strong>Internal applications:</strong><span>  Given its propensity to hallucinate and introduce undesired bias, relying solely on LLM output can lead to undesirable outcomes. This can especially lead to systemic, long-term issues if the training data itself is poisoned.</span><strong> </strong></p></li><li><p><strong>Customer-facing applications: </strong><span>When systems excessively depend on LLMs for decision-making or content generation without adequate oversight, validation mechanisms, or risk communication. LLMs are also susceptible to &#34;hallucinations,&#34; producing content that is factually incorrect, nonsensical, or inappropriate. These hallucinations can lead to misinformation, miscommunication, potential legal issues, and damage to an organization&#39;s reputation if unchecked.</span></p></li></ol></li></ol><p>There is a possibility that there is no clarity on which of these use cases and deployment models are prevalent in your organization. In that scenario, your first job as a Security team would be to understand how LLMs are currently used and understand the plans for future usage. On the data gathered, you can apply the above framework to narrow down the risks that matter most to your organization.</p><p><span>That’s it for today! Are there significant risks that are missed in this post? What other aspects of leveraging LLMs worry you? Is there value in having yet another author talking about securing LLMs?  Tell me more! You can drop me a message on </span><a href="https://twitter.com/JubbaOnJeans" rel="">Twitter</a><span>, </span><a href="https://www.linkedin.com/in/anandsandesh/" rel="">LinkedIn</a><span>, or </span><a href="mailto:anand.sandesh@gmail.com" rel="">email</a><span>. If you find this newsletter useful, share it with a friend, or colleague, or on your social media feed. </span></p><p data-attrs="{&#34;url&#34;:&#34;https://boringappsec.substack.com/p/edition-21-a-framework-to-securely?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;,&#34;action&#34;:null,&#34;class&#34;:null}" data-component-name="ButtonCreateButton"><a href="https://boringappsec.substack.com/p/edition-21-a-framework-to-securely?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><ol><li><p>Playgrounds</p><ol><li><p><span>Aviary Explorer: A way to compare results from open source LLMs: </span><a href="https://aviary.anyscale.com/" rel="">Aviary Explorer (anyscale.com)</a></p></li><li><p><span>A playground for prompt injection. Basically tricking LLMs in revealing secrets </span><a href="https://gandalf.lakera.ai/" rel="">https://gandalf.lakera.ai/</a><span> </span></p></li><li><p><span>Holistic evaluation of LLMs (HELM) from Stanford: </span><a href="https://crfm.stanford.edu/helm/latest/" rel="">https://crfm.stanford.edu/helm/latest/</a></p></li></ol></li><li><p>Security </p><ol><li><p><span>LLM OWASP Top 10: Very useful, but some of them are a stretch. Currently at v0.5  </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf</a></p></li><li><p><span>Prompt injection methods: </span><a href="https://github.com/greshake/llm-security" rel="">GitHub - greshake/llm-security: New ways of breaking app-integrated LLMs</a><span> </span></p></li><li><p><span>Skyflow data privacy for GPT: </span><a href="https://www.skyflow.com/post/generative-ai-data-privacy-skyflow-gpt-privacy-vault" rel="">https://www.skyflow.com/post/generative-ai-data-privacy-skyflow-gpt-privacy-vault</a></p></li><li><p><span>Lakera is an AI security company. They have specific products to protect against Prompt injection: </span><a href="https://www.lakera.ai/llms" rel="">Lakera Guard | Unlock LLMs for Production | Lakera – Protecting AI teams that disrupt the world.</a><span> </span></p></li><li><p><span>Daniel Miessler on AI Attack Surface Map: </span><a href="https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/" rel="">https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/</a><span> </span></p></li><li><p><span> </span><a href="https://www.salesforce.com/news/stories/generative-ai-guidelines/" rel="">Generative AI: 5 Guidelines for Responsible Development - Salesforce News</a></p></li><li><p><span>Nvidia’s AI red team framework: :</span><a href="https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/" rel="">https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/</a></p></li><li><p><span>IBM AI fairness 360 tools to detect bias: </span><a href="https://www.ibm.com/opensource/open/projects/ai-fairness-360/" rel="">https://www.ibm.com/opensource/open/projects/ai-fairness-360/</a></p></li><li><p><span>tldrsec on a similar topic: </span><a href="https://tldrsec.com/p/securely-build-product-ai-machine-learning" rel="">How to securely build product features using AI APIs (tldrsec.com)</a></p></li></ol></li><li><p>Enterprise related things</p><ol><li><p><a href="https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/" rel="">Enterprise architectures for LLMs</a><span> (a16z)</span></p></li><li><p><span>Should you buy or build: </span><a href="https://techcrunch.com/2023/01/25/when-it-comes-to-large-language-models-should-you-build-or-buy/" rel="">When it comes to large language models, should you build or buy? | TechCrunch</a><span> </span></p></li><li><p><span>Companies blocking ChatGPT and other publicly trained chatbots: </span><a href="https://fortune.com/2023/05/19/chatgpt-banned-workplace-apple-goldman-risk-privacy/" rel="">Employees are banned from using ChatGPT at these companies | Fortune</a></p></li><li><p><span> Google thinks open source LLMs will be as good as OpenAI soon: </span><a href="https://www.bigtechwire.com/2023/05/04/googles-leaked-document-reveals-open-source-threat-a-new-era-in-language-models/" rel="">Google’s Leaked Document Reveals Open Source Threat: A New Era in Language Models | BigTechWire</a><span> </span></p></li><li><p><span>Triveto language model whitepaper: </span><a href="https://www.truveta.com/wp-content/uploads/2023/04/Truveta-Language-Model.pdf" rel="">https://www.truveta.com/wp-content/uploads/2023/04/Truveta-Language-Model.pdf</a></p></li></ol></li></ol></div></div></div></article></div></div></div>
  </body>
</html>
