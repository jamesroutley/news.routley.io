<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lmsys.org/blog/2023-06-29-longchat/">Original</a>
    <h1>The False Promise of Open-sourced LLMs context length</h1>
    
    <div id="readability-page-1" class="page"><div><p>In this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 16K tokens.
Evaluation results show that the long-range retrieval accuracy of LongChat-13B is up to 2x higher than other long context open models such as MPT-7B-storywriter (65K), MPT-30B-chat (8K), and ChatGLM2-6B (32k).
LongChat shows promising results in closing the gap between open models and proprietary long context models such as Claude-100K and GPT-4-32K.</p>
<p><img src="https://lmsys.org/images/blog/longchat/topic_retrieval.png"/></p>
<p>Figure 1: Comparing LongChat to other models on the long-range topic retrieval task.</p>
<p>Not only can LongChat models handle such a long context length, but they also precisely follow human instructions in dialogues and demonstrate strong performance in the human preference benchmark <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-Bench</a>.
Their preview versions are available at HuggingFace: <a href="https://huggingface.co/lmsys/longchat-13b-16k">lmsys/longchat-13b-16k</a> and <a href="https://huggingface.co/lmsys/longchat-7b-16k">lmsys/longchat-7b-16k</a>.
You can try them immediately in CLI or web interface using FastChat:</p>
<pre><code>python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k
</code></pre>
<p>There has been a significant surge of interest within the open-source community in developing language models with longer context or extending the context length of existing models like LLaMA.
This trend has led to interesting observations and extensive discussions in various sources, such as <a href="https://kaiokendev.github.io/context">Kaiokendev’s blog</a> and this <a href="https://arxiv.org/pdf/2306.15595.pdf">arXiv manuscript</a>;
meanwhile, several notable models have been released claiming to support much longer context than LLaMA, notable ones include:</p>
<ul>
<li><a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-storywriter</a> supports 65K context length and extrapolates to 80K.</li>
<li><a href="https://huggingface.co/spaces/mosaicml/mpt-30b-chat">MPT-30B-chat</a> supports 8K context length.</li>
<li><a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a> supports 32K context.</li>
</ul>
<p>At LMSYS Org, we have been concurrently exploring various techniques to lengthen the context of our models like <a href="https://huggingface.co/lmsys/vicuna-13b-v1.3">Vicuna</a>.
In this blogpost, alongside the release of the LongChat series, we share our <a href="https://github.com/DachengLi1/LongChat">evaluation tools</a> to verify the long-context capability of LLMs.</p>
<p>Using our evaluation tools in combination with various academic long-context evaluation benchmarks, we conduct a thorough comparison of several open-source and commercial models that claim to support long context.
Through this analysis, we examine how well these models deliver on their promised context length.
We found that <em>while commercial models like GPT-3.5-turbo performs well on our tests, many open source models do not deliver the expected results on their promised context length</em>.</p>
<p>The data and code used to reproduce the results in the blog post are available in our LongChat <a href="https://github.com/DachengLi1/LongChat/tree/longeval">repo</a>.
We provide a visualization in this <a href="https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb">notebook</a>.</p>
<h2><a id="longchat-training-recipe" href="#longchat-training-recipe" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LongChat Training Recipe</h2>
<p>LongChat is finetuned from LLaMA models, which were originally pretrained with 2048 context length.
The training recipe can be conceptually described in two steps:</p>
<h3><a id="step-1-condensing-rotary-embeddings" href="#step-1-condensing-rotary-embeddings" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1: Condensing rotary embeddings</h3>
<p><a href="https://arxiv.org/abs/2104.09864v4">Rotary position embedding</a> is a type of positional embedding that injects the information of position in Transformer.
It is implemented in Hugging Face transformer by:</p>
<pre><code>query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
</code></pre>
<p>Where position_ids are indices such as 1, 2, 3, ... that denote the position of a token in the sentence.
For instance, the token &#34;today&#34; in the sentence &#34;today is a good day&#34; has position_ids 1.
The <code>apply_rotary_pos_emb()</code> function then applies a <a href="https://arxiv.org/pdf/2104.09864.pdf">transformation</a> based on the provided position_ids.</p>
<p>The LLaMA model is pre-trained with rotary embedding on sequence length 2048, which means that it has not observed scenarios where position_ids &gt; 2048 during the pre-training phase.
Instead of forcing the LLaMA model to adapt to position_ids &gt; 2048, we condense position_ids &gt; 2048 to be within 0 to 2048.
Intuitively, we conjecture this condensation can maximally reuse the model weights learned in the pre-training stage. See more insights from <a href="https://kaiokendev.github.io/context">Kaiokendev’s blog</a>.</p>
<p>We define the term <code>condensation ratio</code> by dividing the target new context length <code>y</code> by 2048. We then divide every position_ids by this ratio and feed it into the <code>apply_rotary_pos_emb()</code> function.</p>
<pre><code>query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio)
</code></pre>
<p>In this release, we fine-tune the model to a context length of 16384, and thus the condensation ratio is 8. For instance, a token with position_ids = 10000 becomes position_ids = 10000 / 8 = 1250, and the neighboring token 10001 becomes 10001 / 8 = 1250.125.
This step requires no training.</p>
<h3><a id="step-2-finetuning-on-curated-conversation-data" href="#step-2-finetuning-on-curated-conversation-data" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2: Finetuning on Curated Conversation Data</h3>
<p>After condensing the embedding, we perform the finetuning procedure on our curated conversation dataset.
We reuse our collected user-shared conversations previously used for training Vicuna.
We clean the data using FastChat data pipeline, and truncate these conversations so they are no longer than 16K.
We finetune the model using standard next-token prediction loss. We fine-tune the 7B and 13B models with 80k and 18k conversations, respectively.
To save memory, we use Pytorch FSDP and Flash Attention. Assume A100 is $3/hour on Cloud, the 7B model costs ~$300, and the 13B model costs ~$700.</p>
<h2><a id="evaluation-toolkits-longeval" href="#evaluation-toolkits-longeval" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation toolkits: LongEval</h2>
<p>Recently, commercial and open-source models have continued to tout their abilities to support expanded context length (from 8K, 32K, to 100K) in their latest releases, but how can we verify these claims?
The term &#34;long-context capability&#34; can mean different things for different model providers. For instance, does <a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-StoryWriter&#39;s</a> advertised 65K context length operate at the same capacity as OpenAI’s ChatGPT at 16K?
This issue is also prevalent in our LongChat models development: how do we swiftly and effectively confirm if a freshly trained model can handle the intended context length?</p>
<p>To address this, we can base our evaluations on tasks that necessitate LLMs to process lengthy contexts, such as text generation, retrieval, summarization, and information association in long text sequences.
Inspired by <a href="https://twitter.com/DimitrisPapail/status/1658091355632189440">recent discussions</a>, we&#39;ve devised, <a href="https://github.com/DachengLi1/LongChat.git">LongEval</a>, a long context test suite.
This suite incorporates two tasks of varying degrees of difficulty, providing a simple and swift way to measure and compare long-context performance.</p>
<h3><a id="task-1-coarse-grained-topic-retrieval" href="#task-1-coarse-grained-topic-retrieval" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Task 1: Coarse-grained Topic Retrieval</h3>
<p>In real-world long conversations, users usually talk about and jump between several topics with the chatbot. The Topic Retrieval task mimics this scenario by asking the chatbot to retrieve the first topic in a long conversation consisting of multiple topics. An example task is:</p>
<pre><code>… (instruction of the task)
USER: I would like to discuss &lt;TOPIC-1&gt;
ASSISTANT: Sure! What about xxx of &lt;TOPIC-1&gt;?
… (a multi-turn conversation of &lt;TOPIC-1&gt;)
USER: I would like to discuss  &lt;TOPIC-2&gt;
…
USER: I would like to discuss &lt;TOPIC-k&gt;
… 
USER: What is the first topic we discussed?
ASSISTANT: 
</code></pre>
<p>This task tests whether the model can locate a chunk of text and associate it with the right topic name. We design a conversation to be 400 ~ 600 tokens long. Thus, this task is considered coarse-grained because the model may give correct predictions when it locates positions not too far away (&lt;500 token distance) from the right ones.</p>
<h3><a id="task-2-fine-grained-line-retrieval" href="#task-2-fine-grained-line-retrieval" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Task 2: Fine-grained Line Retrieval</h3>
<p>To further test the model ability to locate and associate texts from a long conversation, we introduce a finer-grained Line Retrieval test. In this test, the chatbot needs to precisely retrieve a number from a long document, instead of a topic from long multi-round conversations. Below is an example:</p>
<pre><code>line torpid-kid: REGISTER_CONTENT is &lt;24169&gt;
line moaning-conversation: REGISTER_CONTENT is &lt;10310&gt;
…
line tacit-colonial: REGISTER_CONTENT is &lt;14564&gt;
What is the &lt;REGISTER_CONTENT&gt; in line moaning-conversation?
</code></pre>
<p>The task was originally proposed in <a href="https://github.com/anadim/the-little-retrieval-test">Little Retrieval Test</a>.
The original testcase uses numbers to denote a line, which we found smaller LLMs usually cannot comprehend well.
To disentangle these factors and make them more suitable for testing open-source chatbots at various sizes, we improve it by using random natural language (e.g., torpid-kid) instead.</p>
<p>We found these two tasks behave with the expected characteristics:</p>
<ol>
<li>The task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy.</li>
<li>It is easy to extend the tests to arbitrary lengths to test models’ capacity under different context lengths.</li>
<li>We have run sanity checks of both tasks and observed the expected results. For example, the vanilla LLaMA models, pretrained with a 2K context length, can achieve perfect accuracy on both tasks when the test inputs length is &lt;2K, but will immediately fail (nearly 0 accuracy) on any test inputs beyond 2K.</li>
</ol>
<p>More details and example usage of LongEval can be found in this <a href="https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb">notebook</a>.</p>
<h2><a id="results-and-findings" href="#results-and-findings" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results and findings</h2>
<p>In this section, we share our evaluation and findings.
<br/></p>
<p>Table 1. Model Specifications.</p>
<table id="Table1">
<tbody>
<tr> <th>Model</th> <th>Size</th> <th>Instruction-tuned?</th> <th>Pretrained Context Length</th> <th>Finetune Context Length</th> <th>Claimed Context Length</th> <th>Open Source?</th></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat">MPT-30-chat</a></td>  <td>30B</td>  <td>Yes</td>  <td>8K</td>  <td>-</td> <td>8K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7b-storywriter</a></td>  <td>7B</td> <td>Yes</td>  <td>2K</td>  <td>65K</td>  <td>84K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6b</a></td>  <td>6B</td>  <td>Yes</td>  <td>8K</td>  <td>?</td> <td>32K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13b-16k (ours)</a></td>  <td>13B</td>  <td>Yes</td> <td>2K</td>  <td>16K</td>  <td>16K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://chat.openai.com/">GPT-3.5-turbo</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>16K</td>  <td>No</td> </tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude">Anthropic Claude-1.3</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>100K</td>  <td>No</td> </tr>
</tbody>
</table>
<p>­</p>
<p>In particular, we consider four open-sourced models and two proprietary models, listed in Table 1.</p>
<h3><a id="longeval-results" href="#longeval-results" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LongEval results</h3>
<p>From the coarse-grained topic retrieval test results (Figure 2 at the beginning), we already observe the questionable performance of open-source long-context models. For instance, Mpt-7b-storywriter claims to have a context length of 84K but barely achieves 50% accuracy even at one-fourth of its claimed context length (16K).
chatglm2-6B cannot reliably retrieve the first topic even at the length of 6K (46% accuracy). Its accuracy falls to almost 0% when tested on &gt; 10K context length. On the other hand, we observed that our LongChat-13B-16K model reliably retrieves the first topic, with comparable accuracy to gpt-3.5-turbo.</p>
<p><img src="https://lmsys.org/images/blog/longchat/line_retrieval.png"/></p>
<p>Figure 3: Accuracy on the long-range line retrieval task.</p>
<p>In the finer-grained line retrieval test, Mpt-7b-storywriter performs even worse than in the coarse-grained cases, dropping accuracy from ~50% to ~30%. Chatglm2-6B also observes degradation and does not perform well at the shortest length we test (5K context length). In contrast, we observe that LongChat-13B-16K performs reliably, achieving near gpt-3.5/Anthropic-claude ability within 12K context length (we also find the preview version is not perfect at 12K-16K, see discussion section).</p>
<p><strong>Disentangle irrelevant LLM abilities in LongEval</strong></p>
<p>In topics and line retrieval tests, we observe mistakes caused by factors irrelevant to long-context ability, such as the instruction-following ability. For instance, in the Line Retrieval test, the model may simply respond “sure, I will tell you the number” instead of returning an actual number. To give a fair comparison, we took two actions to avoid factors irrelevant to long-context capability: prompt engineering and compute accuracy based only on the cases where the model follows our instruction. Check our codes for details.</p>
<h3><a id="human-preference-benchmark-mt-bench" href="#human-preference-benchmark-mt-bench" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Human preference benchmark (MT-bench)</h3>
<p>In the previous section, we observed that LongChat models perform well on long-range retrieval tasks, but does this come with a significant drop in human preference? To test whether it still follows human preferences, we use GPT-4 graded <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-bench</a>, a set of challenging multi-turn conversation questions.</p>
<p>Table 2. MT-bench scores comparing LongChat-13B to other models of similar sizes.</p>
<table id="Table1">
<tbody>
<tr> <th>Model</th> <th>MT-bench (score)</th></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13B-16K</a></td>  <td>5.95</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.3">Vicuna-13B </a></td>  <td>6.39</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0"> WizardLM-15B</a></td>  <td>6.35</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/project-baize/baize-v2-13b"> Baize-v2-13B </a></td>  <td>5.75</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/NousResearch/Nous-Hermes-13b"> Nous-Hermes-13B </a></td>  <td>5.51</td>   </tr>
<tr> <td><a target="_blank" href="https://crfm.stanford.edu/2023/03/13/alpaca.html"> Alpaca-13B</a></td>  <td>4.53</td>  </tr>
</tbody>
</table>
<p>We find that LongChat-13B-16K is comparable to its closest alternative - Vicuna-13B, which indicates that this long-range ability does not come with a significant sacrifice of its short-range ability. At the same time, LongChat-13B-16K is competitive compared to other models of the same size.
­</p>
<h3><a id="long-sequence-question-answer-benchmark" href="#long-sequence-question-answer-benchmark" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Long sequence question answer benchmark</h3>
<p>In the previous sections, we tested models on our (simple) long-range retrieval tasks and human preference tasks. But how do these models perform on more complex academic long-range reasoning tasks?  In this section, we study this by running the Qasper question answering dataset. We use the validation set selection and prompts from the <a href="https://www.zero.scrolls-benchmark.com/">ZeroScrolls</a> long sequence benchmark.</p>
</div></div>
  </body>
</html>
