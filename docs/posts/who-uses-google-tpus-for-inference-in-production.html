<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=39670121">Original</a>
    <h1>Who uses Google TPUs for inference in production?</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>I am really puzzled by TPUs. I&#39;ve been reading everywhere that TPUs are powerful and a great alternative to NVIDIA.</p><p>I have been playing with TPUs for a couple of months now, and to be honest I don&#39;t understand how can people use them in production for inference:</p><p>- almost no resources online showing how to run modern generative models like Mistral, Yi 34B, etc. on TPUs
- poor compatibility between JAX and Pytorch
- very hard to understand the memory consumption of the TPU chips (no nvidia-smi equivalent)
- rotating IP addresses on TPU VMs
- almost impossible to get my hands on a TPU v5</p><p>Is it only me? Or did I miss something?</p><p>I totally understand that TPUs can be useful for training though.</p></div></div></div>
  </body>
</html>
