<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers">Original</a>
    <h1>Eagle 7B: Soaring past Transformers</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png" width="1200" height="967.5824175824176" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:1174,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:7661615,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bbd31a7-21b4-4ff6-b43f-8735d1decf25_2048x1652.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a><figcaption>An eagle, flying past a transformer-looking robot</figcaption></figure></div><p>Eagle 7B is a 7.52B parameter model that:</p><ul><li><p><span>Built on the </span><a href="https://wiki.rwkv.com" rel="">RWKV-v5 architecture</a></p></li><li><p><a href="https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs" rel="">Ranks as the world’s greenest 7B model (per token)</a></p></li><li><p>Trained on 1.1 Trillion Tokens across 100+ languages</p></li><li><p>Outperforms all 7B class models in multi-lingual benchmarks</p></li><li><p>Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (&gt;2T?) level of performance in English evals</p></li><li><p>Trade blows with MPT-7B (1T) in English evals</p></li><li><p><a href="https://www.isattentionallyouneed.com/" rel="">All while being an “Attention-Free Transformer”</a><br/></p></li></ul><p><span>We are releasing RWKV-v5 Eagle 7B, </span><a href="https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as" rel="">licensed as Apache 2.0 license, under the Linux Foundation</a><span>, and can be used personally or commercially without restrictions</span></p><ul><li><p><a href="https://huggingface.co/RWKV/v5-Eagle/blob/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth" rel="">Download from Huggingface</a><span>, and use it anywhere (even locally)</span></p></li><li><p><span>Use our reference </span><a href="https://pypi.org/project/rwkv/" rel="">pip inference package</a><span>, or any other community inference options (</span><a href="https://github.com/josStorer/RWKV-Runner" rel="">Desktop App</a><span>, </span><a href="https://github.com/saharNooby/rwkv.cpp" rel="">RWKV.cpp</a><span>, </span><a href="https://wiki.rwkv.com/basic/play.html" rel="">etc</a><span>)</span></p></li><li><p><a href="https://github.com/RWKV/RWKV-infctx-trainer" rel="">Fine-tune using our Infctx trainer</a></p></li><li><p><a href="https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2" rel="">Try it online on Huggingface</a></p></li><li><p><a href="https://github.com/huggingface/transformers/pull/26963" rel="">[Pending PR] Get it merged into Huggingface transformers!</a></p></li></ul><p><span>We performed multi-lingual performance across the following benchmarks: </span><a href="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#advanced-usage-tips" rel="">xLAMBDA</a><span>, </span><a href="https://huggingface.co/datasets/Muennighoff/xstory_cloze" rel="">xStoryCloze</a><span>, </span><a href="https://huggingface.co/datasets/Muennighoff/xwinograd" rel="">xWinograd</a><span>, </span><a href="https://huggingface.co/datasets/xcopa" rel="">xCopa</a></p><p>Across a total of 23 languages</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png" width="1200" height="362.6373626373626" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:440,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:185118,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F432a507a-d2a1-4741-b196-d34a411aa960_1462x442.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png" width="1200" height="358.5164835164835" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:435,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:129293,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51648027-6abf-4b05-88c0-06175e3b9cb5_1739x519.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Most of these benchmarks cover common sense reasoning, in their respective languages. And show a huge overall jump in multi-lingual performance for RWKV v4-to-v5 architecture. And the v2 world dataset.</p><p>It should also be noted, that there is a lack of multi-lingual benchmarks, as the above covers approximately the top 23 languages.</p><p>This makes it hard to evaluate model language performance directly over the remaining 75+ languages, over the total 100+ trained languages. A shortcoming we hope to improve in future models.</p><p>English performance was measured across 12 separate benchmarks, across commonsense reasoning, and world knowledge</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png" width="1200" height="206.86813186813185" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:251,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:349737,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ea68f59-8f76-48ea-b4c6-15f292c8a46c_2559x441.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Once again we see a huge overall jump from RWKV v4-to-v5 architecture. And the v2 world dataset.</p><p>Where v4 previously lost out to MPT-7b, the top model in the 1T token tier. </p><p>v5 begins trading blows in benchmarks, in some cases even coming on top in certain benchmarks ( LAMBADA, StoryCloze16, WinoGrande, HeadQA_en, Sciq ) over Falcon, or even llama2.</p><p>In addition, v5 performance starts to fall in line with the expected transformer performance level, with its given approximate token training count.</p><p>With Mistral-7B maintaining its lead with its rumored 2~7 Trillion token training.</p><p>We expect to narrow the gap, as we train an additional 1T token, to cross the llama2 line and hopefully reach the mistral line.</p><p>Alternatively, as a base model, which is lightly tuned (really small instruct set mixed in), we are eager to see how the various community and instruct-tuned variants</p><p>A notable observation was that our checkpoints near the 300 Billion token point, show similar performance to pythia-6.9b</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png" width="988" height="268" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:268,&#34;width&#34;:988,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:85110,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97b32c55-9a16-4a70-9e1e-8d2370721660_988x268.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>This is consistent with previous pile-based experiments on our RWKV-v4 architecture, that linear transformers like RWKV scale similarly in performance levels to transformers, with the same token count training.</p><p>If so, it does repeat the question. If the exact architecture, matter less than the data for the model eval performance?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png" width="616" height="463" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f819aa3e-d559-41a9-879a-2a157c323787_616x463.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:463,&#34;width&#34;:616,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:61048,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff819aa3e-d559-41a9-879a-2a157c323787_616x463.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>CUDA computational cost, for RWKV-based architecture vs transformer models - that quadratic-vs-linear really scales!</figcaption></figure></div><p><span>If true, perhaps we should seek more efficient and scalable architecture, to increase accessibility, drive the cost of AI downwards for everyone, and </span><a href="https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs" rel="">lessen the impact on our environment.</a></p><p>A common feedback we receive for the RWKV multi-lingual approach is</p><ul><li><p>it hurts our English evaluation scores and slows the growth of linear transformers </p></li><li><p>that it is not fair to compare the multi-lingual performance of a multi-lingual model -vs- a purely English model</p></li></ul><p>And for most parts, we agree on both points.</p><p>But we have no plans on changing this, as we are building AI for the world - which is not just an English world.</p><p><a href="https://preply.com/en/blog/english-language-statistics/#:~:text=Current%20research%20suggests%20that%20the,widely%20spoken%20language%20in%202022%3F" rel="">In 2023, only 17% of the world&#39;s population speaks English</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png" width="1280" height="615" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:615,&#34;width&#34;:1280,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:394289,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f204b8b-69d2-47d7-b7b0-cd284ac7892b_1280x615.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>World Map showing the distribution of regions and people who are fluent in English (source: Wikipedia)</figcaption></figure></div><p><span>However, by ensuring support for the top 25 languages in the world and beyond, we can cover approximately </span><a href="https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers#Top_languages_by_population" rel="">4 billion people, or 50% of the world</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png" width="780" height="533" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/f8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:533,&#34;width&#34;:780,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:139698,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8f5d7d9-604f-439e-a4fe-8c4869003cca_780x533.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Incomplete map, of regions where the Eagle Language model will support entirely or partially</figcaption></figure></div><p>This aligns well with the team’s common goal, of getting AI to support everyone, not just by allowing it to run cheaply and affordably even on lower-end hardware. But by supporting their language.</p><p><span>A major example of this in our community is the </span><a href="https://discord.gg/dy9YWXjV" rel="">Indonesian-NLP discord group</a><span>, which finetunes an Indonesian language model from the RWKV line of base models.</span></p><p>Allowing them to build strong language-specific models - on a cheap affordable basis (ie. single node), without needing to do half a million dollars of pre-training.</p><p>This release marks the release of the strongest linear transformer (in terms of eval benchmarks) to date. </p><p>While it may not have succeeded in passing LLaMA2 and Mistral. It provides strong evidence of the following</p><ul><li><p>The RWKV-v5 model architecture scales similarly to transformer performance with a similar token count</p></li><li><p>You can achieve a near LLaMA2-like level of performance, with a substantially lower inference cost</p></li><li><p><span>While supporting multi-lingual levels of performance</span><br/></p></li></ul><p>We plan to follow by pushing further ahead with</p><ul><li><p>[Feb 2024] An updated RWKV v5: Eagle paper, where we will go deeper in-depth on the architecture changes since v4, and the model benchmarks and evals</p></li><li><p>[Feb 2024] A further 1T token in training (2T total), for direct comparisons with the LLaMA2 7B model</p></li><li><p>[Mar 2024] An MoE model based on the v5 Eagle 2T model</p></li><li><p>[Mar 2024] RWKV-v6: “Finch” 1.5B, 3B world models</p></li></ul><blockquote><p>Disclaimer: All dates are approximate, and is heavily subjected to compute avaliability from our sponsors/provider</p></blockquote><p>We are grateful and would like to thank the following key groups:</p><ul><li><p><a href="https://stability.ai/" rel="">StabilityAI</a><span> for the bulk of the computing provided to train this foundation model</span></p></li><li><p><a href="https://www.eleuther.ai/" rel="">EleutherAI</a><span> for their support, especially in the ongoing paper-writing process</span></p></li><li><p><a href="https://lfaidata.foundation/" rel="">Linux Foundation AI &amp; Data group</a><span> for supporting and hosting the RWKV project</span></p></li></ul><p><span>Along with the various developers, working on the growing collection of </span><a href="https://wiki.rwkv.com" rel="">RWKV-related projects</a><span>.</span></p></div></div></div></article></div></div></div>
  </body>
</html>
