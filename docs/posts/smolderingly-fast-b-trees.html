<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.scattered-thoughts.net/writing/smolderingly-fast-btrees/">Original</a>
    <h1>Smolderingly fast b-trees</h1>
    
    <div id="readability-page-1" class="page"><article>
  
<p>(<em>This is part of a series on the design of a language. See the list of posts <a href="https://www.scattered-thoughts.net/#zest">here</a></em>.)</p>
<p>Many &#39;scripting&#39; languages use a hashmap for their default associative data-structure (javascript objects, python dicts, etc). Hashtables have a lot of annoying properties:</p>
<ul>
<li>Vulnerable to <a href="https://en.wikipedia.org/wiki/Collision_attack#Hash_flooding">hash flooding</a>. </li>
<li>If protected against hash flooding by random seeds, the iteration order becomes non-deterministic which is annoying for snapshot testing, reproducible builds, etc.</li>
<li>May have to rehash on insert, which produces terrible worst-case latencies for large hashmaps.</li>
<li>Repeatedly growing large allocations without fragmentation is difficult on wasm targets, because virtual memory tricks are not available and pages can&#39;t be unmapped.</li>
<li>Vector instructions on wasm are limited and there are no <a href="https://en.wikipedia.org/wiki/AES_instruction_set">AES instructions</a>. This makes many hash functions much slower.</li>
</ul>
<p>Ordered data-structures like b-trees don&#39;t have any of these disadvantages. They are typically slower than hashmaps, but I was surprised to find fairly wide variation in people&#39;s expectations of how much slower. So let&#39;s compare:</p>
<ul>
<li>rust&#39;s <a href="https://doc.rust-lang.org/std/collections/struct.HashMap.html">std::collections::HashMap</a> with siphash</li>
<li>rust&#39;s <a href="https://doc.rust-lang.org/std/collections/struct.BTreeMap.html">std::collections::BTreeMap</a></li>
<li>zig&#39;s <a href="https://github.com/ziglang/zig/blob/master/lib/std/hash_map.zig">std.HashMap</a> with siphash</li>
<li>zig&#39;s <a href="https://github.com/ziglang/zig/blob/master/lib/std/hash_map.zig">std.HashMap</a> with wyhash</li>
<li>My own <a href="https://github.com/jamii/maps/blob/main/bptree.zig">bptree</a> with various compile-time options for different experiments.</li>
</ul>
<h2 id="microbenchmarks-are-hard">microbenchmarks are hard</h2>
<p>Let&#39;s start with the dumbest possible benchmark - fill a map with uniformly distributed random integers, measure how many cycles takes to lookup all those integers, and take the mean over many iterations.</p>
<pre data-lang="zig"><code data-lang="zig"><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>for </span><span>(keys) </span><span>|</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> value_found </span><span>=</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>    </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>        </span><span>panic</span><span>(</span><span>&#34;Value not found&#34;</span><span>,</span><span> .{})</span><span>;
</span><span>    }
</span><span>}
</span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>record</span><span>(</span><span>&#34;lookup_hit_all&#34;</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> map.</span><span>count</span><span>()))</span><span>;
</span></code></pre>
<div><p>lookup_hit_all / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>rust btree</td><td>46</td><td>26</td><td>19</td><td>54</td><td>78</td><td>94</td><td>106</td><td>133</td><td>152</td><td>166</td><td>193</td><td>215</td><td>236</td><td>262</td><td>290</td><td>316</td><td>367</td></tr>
<tr><td>rust hashmap siphash</td><td>102</td><td>67</td><td>49</td><td>40</td><td>37</td><td>34</td><td>33</td><td>32</td><td>32</td><td>32</td><td>32</td><td>33</td><td>33</td><td>34</td><td>37</td><td>43</td><td>51</td></tr>
<tr><td>zig b+tree</td><td>46</td><td>26</td><td>37</td><td>56</td><td>64</td><td>87</td><td>100</td><td>110</td><td>123</td><td>143</td><td>165</td><td>180</td><td>197</td><td>220</td><td>235</td><td>269</td><td>294</td></tr>
<tr><td>zig hashmap siphash</td><td>99</td><td>84</td><td>64</td><td>69</td><td>70</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>68</td><td>69</td><td>68</td><td>69</td><td>73</td><td>77</td></tr>
<tr><td>zig hashmap wyhash</td><td>80</td><td>35</td><td>29</td><td>35</td><td>34</td><td>35</td><td>34</td><td>33</td><td>34</td><td>32</td><td>33</td><td>31</td><td>31</td><td>32</td><td>32</td><td>33</td><td>34</td></tr>
</tbody></table>
<p>That makes btrees look pretty bad. Much worse, in fact, than the <a href="https://github.com/Rufflewind/bench-maps/">other benchmark</a> that several people pointed me at, where at similar sizes btree lookups are only ~2x slower than hashmaps. But don&#39;t worry, we can reproduce that too:</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(keys) </span><span>|</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>const</span><span> value_found </span><span>=</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>record</span><span>(</span><span>&#34;lookup_hit_one&#34;</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>,</span><span> after </span><span>-</span><span> before)</span><span>;
</span><span>    </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>        </span><span>panic</span><span>(</span><span>&#34;Value not found&#34;</span><span>,</span><span> .{})</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<div><p>lookup_hit_one / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>rust btree</td><td>47</td><td>48</td><td>50</td><td>82</td><td>107</td><td>123</td><td>135</td><td>163</td><td>182</td><td>200</td><td>227</td><td>256</td><td>279</td><td>309</td><td>328</td><td>358</td><td>405</td></tr>
<tr><td>rust hashmap siphash</td><td>101</td><td>101</td><td>101</td><td>100</td><td>105</td><td>103</td><td>102</td><td>103</td><td>103</td><td>103</td><td>103</td><td>108</td><td>112</td><td>116</td><td>124</td><td>140</td><td>166</td></tr>
<tr><td>zig b+tree</td><td>45</td><td>45</td><td>59</td><td>72</td><td>85</td><td>107</td><td>126</td><td>135</td><td>148</td><td>170</td><td>188</td><td>203</td><td>223</td><td>246</td><td>264</td><td>292</td><td>319</td></tr>
<tr><td>zig hashmap siphash</td><td>106</td><td>108</td><td>116</td><td>117</td><td>120</td><td>121</td><td>123</td><td>124</td><td>124</td><td>124</td><td>123</td><td>124</td><td>127</td><td>130</td><td>132</td><td>137</td><td>147</td></tr>
<tr><td>zig hashmap wyhash</td><td>53</td><td>53</td><td>57</td><td>63</td><td>68</td><td>69</td><td>72</td><td>72</td><td>73</td><td>71</td><td>72</td><td>69</td><td>76</td><td>81</td><td>78</td><td>87</td><td>92</td></tr>
</tbody></table>
<p>All we did differently was average over one lookup at a time instead of over many, but somehow that made the hashmaps 2-3x slower!</p>
<p>Both of these benchmarks are pretty bad, so let&#39;s make better versions of both before trying to explain the differences. I only did this for the zig data-structures, because I am lazy.</p>
<p>First, rather than looking the keys up in the same order we inserted them, we&#39;ll precompute a random sample (with replacement):</p>
<pre data-lang="zig"><code data-lang="zig"><span>const</span><span> keys_hitting </span><span>= </span><span>try</span><span> map.allocator.</span><span>alloc</span><span>(Key</span><span>, </span><span>@max</span><span>(batch_size</span><span>,</span><span> count))</span><span>;
</span><span>var</span><span> permutation </span><span>= </span><span>XorShift64</span><span>{}</span><span>;
</span><span>for </span><span>(keys_hitting) </span><span>|*</span><span>key</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> i </span><span>=</span><span> permutation.</span><span>next</span><span>() </span><span>%</span><span> count</span><span>;
</span><span>    key</span><span>.* =</span><span> keys[i]</span><span>;
</span><span>}
</span></code></pre>
<p>Then rather than measuring a single lookup at a time, we&#39;ll measure a batch of 256 lookups to amortize out the overhead of the rdtscp instruction and pull the panics out of the measurement:</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(0</span><span>..</span><span>@divTrunc</span><span>(keys_hitting.len</span><span>,</span><span> batch_size)) </span><span>|</span><span>batch</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> keys_batch </span><span>=</span><span> keys_hitting[batch </span><span>*</span><span> batch_size </span><span>..</span><span>][0</span><span>..</span><span>batch_size]</span><span>;
</span><span>    </span><span>var </span><span>values_found</span><span>:</span><span> [batch_size]</span><span>?</span><span>Key </span><span>= </span><span>undefined</span><span>;
</span><span>
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>var </span><span>key</span><span>: </span><span>Key </span><span>=</span><span> keys_batch[</span><span>0</span><span>]</span><span>;
</span><span>    </span><span>for </span><span>(</span><span>&amp;</span><span>values_found</span><span>,</span><span> 0</span><span>..</span><span>) </span><span>|*</span><span>value</span><span>,</span><span> i</span><span>| </span><span>{
</span><span>        value</span><span>.* =</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>        key </span><span>=</span><span> keys_batch[(i </span><span>+ </span><span>1</span><span>) </span><span>%</span><span> keys_batch.len]</span><span>;
</span><span>    }
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>report</span><span>(</span><span>&#34;lookup_hit_batch&#34;</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> batch_size))</span><span>;
</span><span>
</span><span>    </span><span>for </span><span>(values_found) </span><span>|</span><span>value_found</span><span>| </span><span>{
</span><span>        </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>            </span><span>panic</span><span>(</span><span>&#34;Value not found&#34;</span><span>,</span><span> .{})</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The results look similar to the results for <code>lookup_hit_all</code>:</p>
<div><p>lookup_hit_batch / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>6</td><td>9</td><td>31</td><td>47</td><td>60</td><td>82</td><td>102</td><td>112</td><td>126</td><td>147</td><td>174</td><td>186</td><td>204</td><td>230</td><td>245</td><td>276</td><td>299</td></tr>
<tr><td>zig hashmap siphash</td><td>52</td><td>53</td><td>61</td><td>68</td><td>71</td><td>72</td><td>75</td><td>76</td><td>76</td><td>75</td><td>75</td><td>76</td><td>78</td><td>80</td><td>81</td><td>88</td><td>95</td></tr>
<tr><td>zig hashmap wyhash</td><td>29</td><td>29</td><td>31</td><td>35</td><td>38</td><td>38</td><td>40</td><td>41</td><td>42</td><td>40</td><td>40</td><td>42</td><td>41</td><td>39</td><td>42</td><td>46</td><td>43</td></tr>
</tbody></table>
<p>Now we make one tiny tweak. Instead of iterating through the batch in order, we&#39;ll use the value we just looked up to pick the next key.</p>
<pre data-lang="zig"><code data-lang="zig"><span>for </span><span>(0</span><span>..</span><span>@divTrunc</span><span>(keys_hitting.len</span><span>,</span><span> batch_size)) </span><span>|</span><span>batch</span><span>| </span><span>{
</span><span>    </span><span>const</span><span> keys_batch </span><span>=</span><span> keys_hitting[batch </span><span>*</span><span> batch_size </span><span>..</span><span>][0</span><span>..</span><span>batch_size]</span><span>;
</span><span>    </span><span>var </span><span>values_found</span><span>:</span><span> [batch_size]</span><span>?</span><span>Key </span><span>= </span><span>undefined</span><span>;
</span><span>
</span><span>    </span><span>const</span><span> before </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>var </span><span>key</span><span>: </span><span>Key </span><span>=</span><span> keys_batch[</span><span>0</span><span>]</span><span>;
</span><span>    </span><span>for </span><span>(</span><span>&amp;</span><span>values_found</span><span>,</span><span> 0</span><span>..</span><span>) </span><span>|*</span><span>value</span><span>,</span><span> i</span><span>| </span><span>{
</span><span>        value</span><span>.* =</span><span> map.</span><span>get</span><span>(key)</span><span>;
</span><span>        key </span><span>=</span><span> keys_batch[(i </span><span>+</span><span> value</span><span>.*.?</span><span>) </span><span>%</span><span> keys_batch.len]</span><span>; </span><span>// &lt;-- we changed this line
</span><span>    }
</span><span>    </span><span>const</span><span> after </span><span>= </span><span>rdtscp</span><span>()</span><span>;
</span><span>    </span><span>report</span><span>(</span><span>&#34;lookup_hit_chain&#34;</span><span>,</span><span> map.</span><span>count</span><span>()</span><span>, </span><span>@divTrunc</span><span>(after </span><span>-</span><span> before</span><span>,</span><span> batch_size))</span><span>;
</span><span>
</span><span>    </span><span>for </span><span>(values_found) </span><span>|</span><span>value_found</span><span>| </span><span>{
</span><span>        </span><span>if </span><span>(value_found </span><span>== </span><span>null</span><span>) {
</span><span>            </span><span>panic</span><span>(</span><span>&#34;Value not found&#34;</span><span>,</span><span> .{})</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we have two benchmarks with the exact same batch size and the exact same instructions to execute. The only difference is that in <code>lookup_hit_chain</code> we&#39;ve introduced a data dependency between each iteration of the loop - we need to know <code>value</code> before we know what to lookup next. This prevents successful speculative execution of the next loop iteration.</p>
<div><p>lookup_hit_chain / uniform u64</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>7</td><td>25</td><td>37</td><td>48</td><td>59</td><td>82</td><td>105</td><td>115</td><td>126</td><td>148</td><td>172</td><td>186</td><td>198</td><td>219</td><td>240</td><td>273</td><td>300</td></tr>
<tr><td>zig hashmap siphash</td><td>77</td><td>79</td><td>88</td><td>88</td><td>90</td><td>91</td><td>93</td><td>94</td><td>94</td><td>93</td><td>95</td><td>99</td><td>102</td><td>103</td><td>106</td><td>115</td><td>130</td></tr>
<tr><td>zig hashmap wyhash</td><td>35</td><td>37</td><td>44</td><td>48</td><td>52</td><td>52</td><td>55</td><td>57</td><td>55</td><td>54</td><td>57</td><td>63</td><td>64</td><td>63</td><td>70</td><td>76</td><td>88</td></tr>
</tbody></table>
<p>The wyhash hashmap halves it&#39;s lookup time in <code>lookup_hit_batch</code> vs <code>lookup_hit_chain</code> but the btree doesn&#39;t benefit at all. I don&#39;t understand the limits to speculative execution at all, but let&#39;s make some mildly informed guesses.</p>
<p>At 2^16 keys the whole dataset is about 1mb, which fits comfortably in L2 but is much bigger than L1. Each hashmap lookup costs 1 or maybe 2 L2 cache lookups, each of which have ~20 cycle latency. The wyhash <a href="https://github.com/ziglang/zig/blob/3b465ebec59ee942b6c490ada2f81902ec047d7f//lib/std/hash/wyhash.zig#L110-L113">fast path for u64</a> is only a handful of instructions with no branches:</p>
<pre><code><span>bench[0x1014710] &lt;+0&gt;:  rorxq  $0x20, %rdi, %rdx
</span><span>bench[0x1014716] &lt;+6&gt;:  movabsq $-0x18fc812e5f4bd725, %rax ; imm = 0xE7037ED1A0B428DB
</span><span>bench[0x1014720] &lt;+16&gt;: xorq   %rax, %rdx
</span><span>bench[0x1014723] &lt;+19&gt;: movabsq $0x1ff5c2923a788d2c, %rcx ; imm = 0x1FF5C2923A788D2C
</span><span>bench[0x101472d] &lt;+29&gt;: xorq   %rdi, %rcx
</span><span>bench[0x1014730] &lt;+32&gt;: mulxq  %rcx, %rcx, %rdx
</span><span>bench[0x1014735] &lt;+37&gt;: movabsq $-0x5f89e29b87429bd9, %rsi ; imm = 0xA0761D6478BD6427
</span><span>bench[0x101473f] &lt;+47&gt;: xorq   %rcx, %rsi
</span><span>bench[0x1014742] &lt;+50&gt;: xorq   %rax, %rdx
</span><span>bench[0x1014745] &lt;+53&gt;: mulxq  %rsi, %rcx, %rax
</span><span>bench[0x101474a] &lt;+58&gt;: xorq   %rcx, %rax
</span><span>bench[0x101474d] &lt;+61&gt;: retq
</span></code></pre>
<p>So while we&#39;re waiting for one cache lookup we can start hashing the next key (if we can predict what it is) and maybe even get started on the next cache lookup.</p>
<p>The btree at 2^16 keys has 4 levels. There are typically 15-16 keys per node when inserting random keys, so we&#39;ll expect to do around 32 comparisons on average before finding our key. The body of that search loop looks like:</p>
<pre><code><span>bench[0x10121b0] &lt;+16&gt;: cmpq   %rdx, (%rdi,%rax,8)
</span><span>bench[0x10121b4] &lt;+20&gt;: jae    0x10121c2                 ; &lt;+34&gt; at bptree.zig
</span><span>bench[0x10121b6] &lt;+22&gt;: addq   $0x1, %rax
</span><span>bench[0x10121ba] &lt;+26&gt;: cmpq   %rax, %rsi
</span><span>bench[0x10121bd] &lt;+29&gt;: jne    0x10121b0                 ; &lt;+16&gt; [inlined] bench.less_than_u64 + 9 at bench.zig:159:5
</span></code></pre>
<p>So 5 instructions, including two branches, per comparison. At least 160 instructions and 64 branches for the whole lookup.</p>
<p>The 16 keys take up 2 cachelines, so we&#39;ll average 1.5 cache lookups for each linear key search, plus 1 cache lookup to hit the child/values array. 10 cache lookups in total for the whole btree lookup. Each of those cache lookups depends on the previous lookup so it&#39;ll be hard to speculate correctly, but prefetching might help within a single node. If every cache lookup hit L2 in strict series we&#39;d expect ~200 cycles, but probably some of the earlier nodes are in L1.</p>
<p>Anyway, let&#39;s leave that rabbit hole alone for now. It&#39;s enough to notice that hashmaps benefit from speculative execution between multiple lookups and btrees don&#39;t.</p>
<p>We can roughly think of <code>lookup_hit_batch</code> as measuring throughput and <code>lookup_hit_chain</code> as measuring latency. All the other benchmarks I&#39;ve seen only measure one or the other, which starts to explain the disagreement over btree vs hashmap performance.</p>
<h2 id="string-keys">string keys</h2>
<p>Btrees do a lot of key comparisons. If the keys are integers then they are stored in the btree node, so it doesn&#39;t matter too much. But for strings keys we potentially pay for a cache lookup for every comparison.</p>
<div><p>lookup_hit_chain / random strings</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>101</td><td>124</td><td>144</td><td>160</td><td>184</td><td>219</td><td>255</td><td>279</td><td>310</td><td>366</td><td>427</td><td>470</td><td>512</td><td>577</td><td>700</td><td>826</td><td>965</td></tr>
<tr><td>zig hashmap siphash</td><td>145</td><td>147</td><td>154</td><td>159</td><td>162</td><td>163</td><td>165</td><td>167</td><td>168</td><td>173</td><td>181</td><td>186</td><td>196</td><td>211</td><td>247</td><td>287</td><td>312</td></tr>
<tr><td>zig hashmap wyhash</td><td>49</td><td>50</td><td>59</td><td>65</td><td>68</td><td>69</td><td>72</td><td>74</td><td>75</td><td>81</td><td>88</td><td>93</td><td>103</td><td>119</td><td>154</td><td>188</td><td>219</td></tr>
</tbody></table>
<p>Completely random strings is actually pretty unlikely and unfairly benefits the btree, which typically only has to compare the first character of each string to find that they are not equal. But real datasets (eg urls in a log) often have large common prefixes. We can see the difference if we make the first 16 characters constant:</p>
<div><p>lookup_hit_chain / random-ish strings</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>102</td><td>150</td><td>221</td><td>338</td><td>546</td><td>618</td><td>807</td><td>911</td><td>1077</td><td>1393</td><td>1507</td><td>1641</td><td>1812</td><td>2095</td><td>2628</td><td>2848</td><td>3302</td></tr>
<tr><td>zig hashmap siphash</td><td>145</td><td>147</td><td>153</td><td>159</td><td>162</td><td>163</td><td>165</td><td>167</td><td>168</td><td>174</td><td>182</td><td>187</td><td>197</td><td>210</td><td>245</td><td>282</td><td>313</td></tr>
<tr><td>zig hashmap wyhash</td><td>49</td><td>50</td><td>59</td><td>66</td><td>69</td><td>69</td><td>72</td><td>74</td><td>74</td><td>81</td><td>88</td><td>93</td><td>102</td><td>117</td><td>154</td><td>188</td><td>214</td></tr>
</tbody></table>
<p>Hashmaps don&#39;t care, but the btree is really hurting.</p>
<p>If we&#39;re just supporting strings we can optimize for this case by storing the length of the common prefix within each node. But it&#39;s hard to imagine how to generalize that to arbitrary types. What if the key is a tuple of strings? Or a small set?</p>
<h2 id="wasm-hashes">wasm hashes</h2>
<p>Let&#39;s try the same in wasm, where hash functions have less access to fast vector instructions. Wasm also doesn&#39;t have rdtscp so times here are in nanoseconds rather than cycles.</p>
<div><p>lookup_hit_chain / uniform u64 / wasm</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>5</td><td>14</td><td>19</td><td>22</td><td>27</td><td>38</td><td>45</td><td>49</td><td>53</td><td>59</td><td>70</td><td>75</td><td>80</td><td>87</td><td>98</td><td>111</td><td>120</td></tr>
<tr><td>zig hashmap siphash</td><td>33</td><td>34</td><td>37</td><td>38</td><td>40</td><td>40</td><td>41</td><td>42</td><td>41</td><td>41</td><td>42</td><td>43</td><td>44</td><td>45</td><td>46</td><td>52</td><td>58</td></tr>
<tr><td>zig hashmap wyhash</td><td>29</td><td>30</td><td>33</td><td>35</td><td>36</td><td>36</td><td>37</td><td>38</td><td>38</td><td>37</td><td>38</td><td>40</td><td>40</td><td>42</td><td>43</td><td>47</td><td>51</td></tr>
</tbody></table>
<div><p>lookup_hit_chain / random strings / wasm</p></div>
<table><thead><tr><th>log2(#keys)</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th></tr></thead><tbody>
<tr><td>zig b+tree</td><td>64</td><td>77</td><td>88</td><td>97</td><td>117</td><td>141</td><td>154</td><td>167</td><td>192</td><td>216</td><td>269</td><td>287</td><td>294</td><td>308</td><td>369</td><td>436</td><td>500</td></tr>
<tr><td>zig hashmap siphash</td><td>65</td><td>64</td><td>68</td><td>70</td><td>73</td><td>71</td><td>72</td><td>73</td><td>73</td><td>75</td><td>78</td><td>81</td><td>84</td><td>88</td><td>102</td><td>118</td><td>135</td></tr>
<tr><td>zig hashmap whyhash</td><td>45</td><td>45</td><td>48</td><td>50</td><td>52</td><td>52</td><td>53</td><td>54</td><td>54</td><td>56</td><td>59</td><td>62</td><td>65</td><td>69</td><td>79</td><td>93</td><td>105</td></tr>
</tbody></table>
<p>The overall ratios are fairly similar, although wyhash seems to have been penalized a little relative to siphash.</p>
<p>Neither the hash functions nor the table scan generates vector instructions at all, even when comparing strings. And, uh, now that I think to look, they don&#39;t generate vector instructions on x86 either. So I guess that&#39;s a non-issue.</p>
<h2 id="btree-tuning">btree tuning</h2>
<p>I implemented both btrees and b+trees. I didn&#39;t see much difference between them for insert/lookup, so I preferred the b+tree for the easier/faster implementation of scans and range queries.</p>
<p>The rust btreemap fixes the max key count per node to 11. For all the workloads I&#39;ve tried the sweet spot seems to be to fix the node size to 512 bytes, which is 31 keys for u64 and 20 keys for strings. </p>
<p>Allowing leaves and branches to have different sizes didn&#39;t help.</p>
<p>I gained a small speedup by manually laying out the node like <code>key_count, keys, values/children</code>. Zig by default prefers to put <code>key_count</code> at the end of the struct to avoid padding, but we always read the key_count first so it&#39;s nice to get some keys on the same cacheline. Maintaining this optimization across different architectures was annoying though, so I rolled it back and it&#39;s not reflected in the tests above.</p>
<p>The rust btreemap <a href="https://github.com/rust-lang/rust/blob/55a22d2a63334e0faff0202b72a31ce832b56125/library/alloc/src/collections/btree/search.rs#L223">switches on Ordering</a>. I got a small boost from instead using <code>less_than</code> during the search and calling <code>equal</code> afterwards. For a lookup at 2^16 keys we&#39;ll expect to call <code>less_than</code> 32 times and <code>equal</code> once, so it&#39;s worth paying for the extra call to <code>equal</code> in exchange for tightening up the inner search loop.</p>
<p>I use tried various different binary searches. The best was this &#39;branchless&#39; variant:</p>
<pre data-lang="zig"><code data-lang="zig"><span>fn </span><span>searchBinary</span><span>(</span><span>keys</span><span>:</span><span> []</span><span>Key</span><span>, </span><span>search_key</span><span>: </span><span>Key</span><span>) </span><span>usize </span><span>{
</span><span>    </span><span>if </span><span>(keys.len </span><span>== </span><span>0</span><span>) </span><span>return </span><span>0</span><span>;
</span><span>    </span><span>var </span><span>offset</span><span>: </span><span>usize </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>var </span><span>length</span><span>: </span><span>usize </span><span>=</span><span> keys.len</span><span>;
</span><span>    </span><span>while </span><span>(length </span><span>&gt; </span><span>1</span><span>) {
</span><span>        </span><span>const</span><span> half </span><span>=</span><span> length </span><span>/ </span><span>2</span><span>;
</span><span>        </span><span>const</span><span> mid </span><span>=</span><span> offset </span><span>+</span><span> half</span><span>;
</span><span>        </span><span>if </span><span>(</span><span>less_than</span><span>(keys[mid]</span><span>,</span><span> search_key)) {
</span><span>            </span><span>@branchHint</span><span>(.unpredictable)</span><span>;
</span><span>            offset </span><span>=</span><span> mid</span><span>;
</span><span>        }
</span><span>        length </span><span>-=</span><span> half</span><span>;
</span><span>    }
</span><span>    offset </span><span>+= </span><span>@intFromBool</span><span>(</span><span>less_than</span><span>(keys[offset]</span><span>,</span><span> search_key))</span><span>;
</span><span>    </span><span>return</span><span> offset</span><span>;
</span><span>}
</span></code></pre>
<p><code>while (length &gt; 1)</code> requires a branch but it&#39;s easily predictable. <code>branchHint(.unpredictable)</code> causes llvm to emit a conditional move for <code>offset = mid</code>. </p>
<pre><code><span>bench[0x101f5e0] &lt;+0&gt;:  movq   %rsi, %rax
</span><span>bench[0x101f5e3] &lt;+3&gt;:  testq  %rsi, %rsi
</span><span>bench[0x101f5e6] &lt;+6&gt;:  je     0x101f616                 ; &lt;+54&gt; at bptree.zig
</span><span>bench[0x101f5e8] &lt;+8&gt;:  xorl   %ecx, %ecx
</span><span>bench[0x101f5ea] &lt;+10&gt;: cmpq   $0x1, %rax
</span><span>bench[0x101f5ee] &lt;+14&gt;: je     0x101f60b                 ; &lt;+43&gt; [inlined] bench.less_than_u64 at bench.zig:185:5
</span><span>bench[0x101f5f0] &lt;+16&gt;: movq   %rax, %rsi
</span><span>bench[0x101f5f3] &lt;+19&gt;: shrq   %rsi
</span><span>bench[0x101f5f6] &lt;+22&gt;: leaq   (%rcx,%rsi), %r8
</span><span>bench[0x101f5fa] &lt;+26&gt;: cmpq   %rdx, (%rdi,%r8,8)
</span><span>bench[0x101f5fe] &lt;+30&gt;: cmovbq %r8, %rcx
</span><span>bench[0x101f602] &lt;+34&gt;: subq   %rsi, %rax
</span><span>bench[0x101f605] &lt;+37&gt;: cmpq   $0x1, %rax
</span><span>bench[0x101f609] &lt;+41&gt;: ja     0x101f5f0                 ; &lt;+16&gt; at bptree.zig:334:37
</span><span>bench[0x101f60b] &lt;+43&gt;: cmpq   %rdx, (%rdi,%rcx,8)
</span><span>bench[0x101f60f] &lt;+47&gt;: adcq   $0x0, %rcx
</span><span>bench[0x101f613] &lt;+51&gt;: movq   %rcx, %rax
</span><span>bench[0x101f616] &lt;+54&gt;: retq
</span></code></pre>
<p>Linear search was still faster in most benchmarks though, even with ludicrously large node sizes.</p>
<p>I also tried a btree variant I saw in some paper where leaves are left unsorted and keys are always inserted at the end of the leaf. This saves some memcopying in the common case, but having to sort the leaf before splitting negates the benefit.</p>
<h2 id="outcome">outcome</h2>
<p>All the benchmarks above are basically best case scenarios, where we&#39;re doing a lot of lookups in a row. If we were just doing one lookup in the middle of some other work then the btree might not be in cache at all and each of those 2.5 cache lookups per level are going all the way to main memory. That&#39;s catastrophic. Whereas an open-addressed hashmap will typically only hit 1 or 2 cachelines per lookup regardless of size.</p>
<p>And while btrees avoid many of the performance edge cases of hashmaps, they also have some cliffs of their own to fall off as comparisons get more expensive and touch more memory, as we saw with the random-ish strings above.</p>
<p>I haven&#39;t measure space usage yet, but we can expect it to be worse for btrees. For random keys the typical node occupancy is 50%, minus per-node overhead like key_count, whereas I&#39;ve been running the zig hashmaps at 80%. So we can guesstimate the btrees will use &gt;60% more memory. EDIT Duh, hashmaps have low occupancy after doubling, so their average occupancy is actually more like 57%. Whereas the btree with steady churn over time will approach 67%.</p>
<p>Space usage is really bad for small maps too. I&#39;d need to add some extra tweaks to allow the btree root node to start small and grow, rather than paying the full 512 bytes for a map with only 1 key.</p>
<p>Overall, I&#39;m unconvinced that it&#39;s worth exploring btrees further. I&#39;ll stick to hashmaps and I&#39;ll either iterate in insertion order or I&#39;ll require sorting entries before iterating. But if anyone else likes the look of this rabbit hole I left some other ideas untouched:</p>
<ul>
<li>Consider the hash function part of the stable api for the compiler. Use an open-addressed hashtable that preserves hash order. Solve hash flooding without seeding the hash eg by falling back to a tree.</li>
<li><a href="https://en.wikipedia.org/wiki/Hash_array_mapped_trie">Hash-array mapped tries</a> have similar O(log(n)) cacheline behaviour to btrees. But if we used open-addressed hashtables at the leaves we could keep the nesting depth pretty low.</li>
<li>In-memory LSMs with incremental merging work pretty well in differential dataflow, but still have this O(log(n)) cacheline behaviour. But maybe with a hashmap as secondary index lookups might be reasonable, and we can consider slow inserts the price to pay for not rehashing.</li>
</ul>
<h2 id="miscellanea">miscellanea</h2>
<p>All the experiments here are using:</p>
<ul>
<li>rustc 1.77.2</li>
<li>zig 0.14.0-dev.1694+3b465ebec</li>
<li>wasmtime-cli 20.0.2</li>
</ul>
<p>Running on an intel i7-1260P with:</p>
<ul>
<li>efficiency cores disabled</li>
<li>scaling governer set to performance</li>
<li>turbo mode disabled</li>
<li>aslr disabled</li>
</ul>
<p>Usually I would also use cset to pin experiments to a shielded core, but it seems that recent systemd versions have broken that workflow and I haven&#39;t figured out a replacement yet.</p>
<p>The rest of the benchmark setup can be found in <a href="https://github.com/jamii/maps">jamii/maps</a>. I measured more than just lookups, but btrees have similar performance for every operation due to having to find a key first.</p>
<p>Cache latency numbers came from <a href="https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html">mlc</a> and roughly match claims I found online.</p>
<p><em>Thanks to Dan Luu for help thinking through cache behaviour.</em></p>

</article></div>
  </body>
</html>
