<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bellard.org/ts_server/">Original</a>
    <h1>TextSynth Server</h1>
    
    <div id="readability-page-1" class="page">


<b>ts_server</b> is a web server proposing a REST API to large
language models. They can be used for example for text completion,
question answering, classification, chat, translation, image
generation, ...
<p>It has the following characteristics:
</p><ul>
  <li>Supports many Transformer variants (<a href="https://github.com/kingoflolz/mesh-transformer-jax">GPT-J</a>, <a href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a>, <a href="https://github.com/EleutherAI/gpt-neo">GPT-Neo</a>, <a href="https://github.com/facebookresearch/metaseq">OPT</a>, <a href="https://github.com/pytorch/fairseq/tree/main/examples/moe_lm">Fairseq GPT</a>, <a href="https://arxiv.org/abs/2010.11125">M2M100</a>, <a href="https://github.com/salesforce/CodeGen">CodeGen</a>, <a href="https://github.com/openai/gpt-2">GPT2</a>, <a href="https://arxiv.org/abs/2210.11416">T5</a>, <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a>, <a href="https://github.com/facebookresearch/llama">LLAMA</a>) and <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a>.</li>
  <li>Integrated REST JSON API for text completion, translation and image generation. It is used by <a href="https://textsynth.com/documentation.html">textsynth.com</a>.</li>
  <li>Very high performance for small and large batches on CPU and GPU.</li>
  <li>Efficient custom 8 bit and 4 bit quantization.</li>
  <li>Larger models work optimally on lower cost GPUs (e.g. RTX 3090, RTX A6000) thanks to efficient quantization.</li>
  <li>All is included in a single binary. Very few external dependencies (Python is not needed) so installation is easy on most Linux distributions.</li>
  <li>Uses the <a href="https://bellard.org/libnc/">LibNC</a> library for simple tensor manipulation using the
  C language.</li>
  <li>Simple command line tools (<b>ts_test</b>, <b>ts_sd</b>) are provided to test the various models.</li>
</ul>

The CPU version is released as binary code under the MIT license. The GPU version is commercial software. Please contact <a href="http://bellard.org">
</a> for the exact terms.

<h2>Download</h2>
               
<ul>
  <li>Linux version <a href="https://bellard.org/ts_server/ts_server-2023-03-15.tar.gz">ts_server-2023-03-15.tar.gz</a> (<a href="https://bellard.org/ts_server/Changelog">Changelog</a>).</li>
</ul>

<h2><a href="https://bellard.org/ts_server/ts_server.html">Documentation</a></h2>

<h2>Benchmarks</h2>

<ul>
  <li>CPU: the speed is measured on an AMD Epyc 7313 CPU using 8 threads (<tt>ts_test -T 8</tt>). 100 tokens are generated.</li>
  <li>GPU: the speed is measured on a RTX A6000 GPU. 100 tokens
    are generated.</li>
</ul>

<table>
  <thead>
    <tr>
      <th aria-sort="ascending">Model<sup>(3)</sup><br/>
      </th><th>CPU Speed</th><th>GPU Speed</th></tr>
  </thead>
  <tbody>
    <tr><td>gptj_6B_q8</td><td>12.6</td><td>84.2</td></tr>
    <tr><td>gptneox_20B_q4</td><td>4.3</td><td>40.8</td></tr>
    <tr><td>gptneox_20B_q8</td><td>3.5</td><td>27.4</td></tr>
    <tr><td>llama_65B_q4</td><td>1.4</td><td>13.8</td></tr>
  </tbody>
</table>

<h2>Available Models</h2>

We provide here the model files that can be used with the TextSynth
Server. Each model was evaluated with
the <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>
with the TextSynth server on a RTX A6000 GPU.
<p>Language Models:

<table>
  <thead>
    <tr>
      <th aria-sort="ascending">
      </th><th>
      </th><th>
      </th><th>
      </th><th>
      </th><th>
      </th><th>
      </th><th>
      </th><th>
    </th></tr>
  </thead>
  <tbody>
<tr><td><a href="https://www2.bellard.org/models/bloom_560M.bin">bloom_560M</a>
</td><td>    2</td><td>     29.176</td><td>      36.8%</td><td>      35.8%</td><td>      51.4%</td><td>      63.7%</td><td>      36.0%</td><td>      44.7%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/codegen_6B_mono_q4.bin">codegen_6B_mono_q4</a>
</td><td>    5</td><td>     69.409</td><td>      28.0%</td><td>      35.7%</td><td>      51.1%</td><td>      60.2%</td><td>      38.0%</td><td>      42.6%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/codegen_6B_mono_q8.bin">codegen_6B_mono_q8</a>
</td><td>    8</td><td>     67.262</td><td>      28.1%</td><td>      35.8%</td><td>      50.8%</td><td>      60.1%</td><td>      39.1%</td><td>      42.8%</td></tr>
<tr><td>fairseq_gpt_13B       </td><td>   27</td><td>      3.567</td><td>      71.9%</td><td>      72.7%</td><td>      67.5%</td><td>      77.6%</td><td>      70.1%</td><td>      71.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/fairseq_gpt_13B_bf4.bin">fairseq_gpt_13B_bf4</a>
</td><td>    9</td><td>      3.646</td><td>      71.2%</td><td>      72.5%</td><td>      67.6%</td><td>      77.4%</td><td>      70.6%</td><td>      71.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/fairseq_gpt_13B_bf8.bin">fairseq_gpt_13B_bf8</a>
</td><td>   15</td><td>      3.565</td><td>      71.8%</td><td>      72.7%</td><td>      67.2%</td><td>      77.7%</td><td>      70.0%</td><td>      71.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_base.bin">flan_t5_base</a>
</td><td>    1</td><td>     12.891</td><td>      54.2%</td><td>      36.5%</td><td>      54.7%</td><td>      65.8%</td><td>      62.1%</td><td>      54.7%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_base_q8.bin">flan_t5_base_q8</a>
</td><td>    1</td><td>     13.098</td><td>      54.2%</td><td>      36.4%</td><td>      54.2%</td><td>      65.7%</td><td>      61.8%</td><td>      54.5%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_small.bin">flan_t5_small</a>
</td><td>    1</td><td>     23.343</td><td>      46.7%</td><td>      29.2%</td><td>      50.0%</td><td>      62.4%</td><td>      47.9%</td><td>      47.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_small_q8.bin">flan_t5_small_q8</a>
</td><td>    1</td><td>     23.449</td><td>      46.7%</td><td>      29.2%</td><td>      49.7%</td><td>      62.4%</td><td>      48.2%</td><td>      47.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_xxl_q4.bin">flan_t5_xxl_q4</a>
</td><td>    7</td><td>      3.010</td><td>      77.7%</td><td>      71.5%</td><td>      73.4%</td><td>      77.6%</td><td>      71.8%</td><td>      74.4%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/flan_t5_xxl_q8.bin">flan_t5_xxl_q8</a>
</td><td>   13</td><td>      3.049</td><td>      77.8%</td><td>      72.1%</td><td>      75.1%</td><td>      77.8%</td><td>      73.1%</td><td>      75.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_117M.bin">gpt2_117M</a>
</td><td>    1</td><td>     40.110</td><td>      32.9%</td><td>      31.1%</td><td>      52.1%</td><td>      62.9%</td><td>      27.3%</td><td>      41.3%</td></tr>
<tr><td>gpt2_1558M            </td><td>    4</td><td>     10.637</td><td>      51.3%</td><td>      50.8%</td><td>      58.4%</td><td>      70.8%</td><td>      53.2%</td><td>      56.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_1558M_q8.bin">gpt2_1558M_q8</a>
</td><td>    2</td><td>     10.655</td><td>      51.2%</td><td>      50.8%</td><td>      58.6%</td><td>      70.8%</td><td>      53.2%</td><td>      56.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_345M.bin">gpt2_345M</a>
</td><td>    1</td><td>     18.272</td><td>      43.5%</td><td>      39.4%</td><td>      53.3%</td><td>      67.7%</td><td>      43.1%</td><td>      49.4%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_345M_q8.bin">gpt2_345M_q8</a>
</td><td>    1</td><td>     18.452</td><td>      43.1%</td><td>      39.4%</td><td>      53.1%</td><td>      67.5%</td><td>      41.9%</td><td>      49.0%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_774M.bin">gpt2_774M</a>
</td><td>    2</td><td>     12.966</td><td>      47.8%</td><td>      45.4%</td><td>      55.6%</td><td>      70.4%</td><td>      48.5%</td><td>      53.5%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gpt2_774M_q8.bin">gpt2_774M_q8</a>
</td><td>    1</td><td>     12.928</td><td>      47.9%</td><td>      45.4%</td><td>      55.3%</td><td>      70.3%</td><td>      48.2%</td><td>      53.4%</td></tr>
<tr><td>gptj_6B               </td><td>   13</td><td>      4.124</td><td>      69.0%</td><td>      66.2%</td><td>      64.8%</td><td>      75.5%</td><td>      66.9%</td><td>      68.5%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gptj_6B_q4.bin">gptj_6B_q4</a>
</td><td>    4</td><td>      4.153</td><td>      68.9%</td><td>      65.7%</td><td>      63.9%</td><td>      74.4%</td><td>      67.0%</td><td>      68.0%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gptj_6B_q8.bin">gptj_6B_q8</a>
</td><td>    7</td><td>      4.122</td><td>      69.1%</td><td>      66.2%</td><td>      64.4%</td><td>      75.4%</td><td>      66.4%</td><td>      68.3%</td></tr>
<tr><td>gptneox_20B           </td><td>   43</td><td>      3.657</td><td>      72.6%</td><td>      71.4%</td><td>      65.5%</td><td>      77.5%</td><td>      73.3%</td><td>      72.0%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gptneox_20B_q4.bin">gptneox_20B_q4</a>
</td><td>   13</td><td>      3.711</td><td>      72.0%</td><td>      69.3%</td><td>      64.8%</td><td>      76.7%</td><td>      70.8%</td><td>      70.7%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/gptneox_20B_q8.bin">gptneox_20B_q8</a>
</td><td>   23</td><td>      3.659</td><td>      72.6%</td><td>      71.3%</td><td>      65.8%</td><td>      77.3%</td><td>      72.9%</td><td>      72.0%</td></tr>
<tr><td>llama_13B_q4          </td><td>    8</td><td>      3.130</td><td>      77.1%</td><td>      78.6%</td><td>      72.2%</td><td>      78.3%</td><td>      77.8%</td><td>      76.8%</td></tr>
<tr><td>llama_13B_q8          </td><td>   15</td><td>      3.178</td><td>      76.5%</td><td>      79.1%</td><td>      73.2%</td><td>      79.1%</td><td>      77.1%</td><td>      77.0%</td></tr>
<tr><td>llama_30B_q4          </td><td>   20</td><td>      2.877</td><td>      77.5%</td><td>      82.4%</td><td>      75.7%</td><td>      80.2%</td><td>      80.2%</td><td>      79.2%</td></tr>
<tr><td>llama_30B_q8          </td><td>   36</td><td>      2.853</td><td>      77.7%</td><td>      82.7%</td><td>      76.3%</td><td>      80.3%</td><td>      80.4%</td><td>      79.5%</td></tr>
<tr><td>llama_65B_q4          </td><td>   39</td><td>      2.760</td><td>      78.5%</td><td>      83.9%</td><td>      76.6%</td><td>      81.4%</td><td>      83.2%</td><td>      80.7%</td></tr>
<tr><td>llama_7B              </td><td>   14</td><td>      3.463</td><td>      73.6%</td><td>      76.2%</td><td>      70.4%</td><td>      78.1%</td><td>      75.4%</td><td>      74.7%</td></tr>
<tr><td>llama_7B_q4           </td><td>    5</td><td>      3.549</td><td>      73.2%</td><td>      75.5%</td><td>      70.4%</td><td>      78.0%</td><td>      74.7%</td><td>      74.4%</td></tr>
<tr><td>llama_7B_q8           </td><td>    8</td><td>      3.453</td><td>      73.7%</td><td>      76.1%</td><td>      70.2%</td><td>      78.0%</td><td>      75.5%</td><td>      74.7%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/opt_125M.bin">opt_125M</a>
</td><td>    1</td><td>     26.028</td><td>      37.9%</td><td>      31.3%</td><td>      50.2%</td><td>      63.2%</td><td>      23.4%</td><td>      41.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/opt_30B_q4.bin">opt_30B_q4</a>
</td><td>   19</td><td>      3.656</td><td>      71.5%</td><td>      72.1%</td><td>      68.0%</td><td>      77.4%</td><td>      69.9%</td><td>      71.8%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/opt_30B_q8.bin">opt_30B_q8</a>
</td><td>   34</td><td>      3.628</td><td>      71.6%</td><td>      72.3%</td><td>      68.2%</td><td>      77.7%</td><td>      71.4%</td><td>      72.3%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/opt_66B_q4.bin">opt_66B_q4</a>
</td><td>   40</td><td>      3.308</td><td>      73.4%</td><td>      74.4%</td><td>      68.4%</td><td>      78.5%</td><td>      75.0%</td><td>      73.9%</td></tr>
<tr><td>pythia_deduped_1.4B   </td><td>    3</td><td>      6.546</td><td>      63.1%</td><td>      52.2%</td><td>      57.1%</td><td>      72.7%</td><td>      52.6%</td><td>      59.5%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_1.4B_q8.bin">pythia_deduped_1.4B_q8</a>
</td><td>    2</td><td>      6.577</td><td>      63.3%</td><td>      52.1%</td><td>      55.7%</td><td>      73.1%</td><td>      53.0%</td><td>      59.4%</td></tr>
<tr><td>pythia_deduped_12B    </td><td>   25</td><td>      3.854</td><td>      70.9%</td><td>      69.2%</td><td>      63.9%</td><td>      76.3%</td><td>      70.8%</td><td>      70.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_12B_q4.bin">pythia_deduped_12B_q4</a>
</td><td>    8</td><td>      4.187</td><td>      69.2%</td><td>      68.5%</td><td>      63.1%</td><td>      76.4%</td><td>      69.6%</td><td>      69.4%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_12B_q8.bin">pythia_deduped_12B_q8</a>
</td><td>   14</td><td>      3.857</td><td>      70.9%</td><td>      69.2%</td><td>      64.2%</td><td>      76.1%</td><td>      70.9%</td><td>      70.3%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_160M.bin">pythia_deduped_160M</a>
</td><td>    1</td><td>     26.380</td><td>      36.9%</td><td>      32.3%</td><td>      51.4%</td><td>      63.8%</td><td>      23.2%</td><td>      41.5%</td></tr>
<tr><td>pythia_deduped_1B     </td><td>    3</td><td>      7.273</td><td>      58.5%</td><td>      49.0%</td><td>      54.5%</td><td>      71.0%</td><td>      49.9%</td><td>      56.6%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_1B_q8.bin">pythia_deduped_1B_q8</a>
</td><td>    2</td><td>      7.286</td><td>      58.4%</td><td>      49.0%</td><td>      54.9%</td><td>      70.9%</td><td>      49.0%</td><td>      56.5%</td></tr>
<tr><td>pythia_deduped_2.8B   </td><td>    6</td><td>      4.787</td><td>      67.1%</td><td>      61.6%</td><td>      60.9%</td><td>      74.4%</td><td>      65.5%</td><td>      65.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_2.8B_q8.bin">pythia_deduped_2.8B_q8</a>
</td><td>    4</td><td>      4.778</td><td>      66.9%</td><td>      61.5%</td><td>      61.2%</td><td>      74.5%</td><td>      65.6%</td><td>      66.0%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_410M.bin">pythia_deduped_410M</a>
</td><td>    1</td><td>     10.827</td><td>      51.7%</td><td>      40.8%</td><td>      54.0%</td><td>      67.2%</td><td>      43.0%</td><td>      51.4%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_410M_q8.bin">pythia_deduped_410M_q8</a>
</td><td>    1</td><td>     10.729</td><td>      51.8%</td><td>      40.7%</td><td>      53.8%</td><td>      67.1%</td><td>      42.7%</td><td>      51.2%</td></tr>
<tr><td>pythia_deduped_6.9B   </td><td>   15</td><td>      4.195</td><td>      69.1%</td><td>      65.7%</td><td>      63.9%</td><td>      75.1%</td><td>      66.1%</td><td>      68.0%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_6.9B_q4.bin">pythia_deduped_6.9B_q4</a>
</td><td>    5</td><td>      4.344</td><td>      68.3%</td><td>      65.0%</td><td>      62.5%</td><td>      75.3%</td><td>      66.3%</td><td>      67.5%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_6.9B_q8.bin">pythia_deduped_6.9B_q8</a>
</td><td>    8</td><td>      4.187</td><td>      69.4%</td><td>      65.7%</td><td>      63.6%</td><td>      75.5%</td><td>      66.8%</td><td>      68.2%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/pythia_deduped_70M.bin">pythia_deduped_70M</a>
</td><td>    1</td><td>     96.126</td><td>      25.6%</td><td>      28.3%</td><td>      54.4%</td><td>      60.4%</td><td>      13.1%</td><td>      36.3%</td></tr>
<tr><td>rwkv_14B              </td><td>   29</td><td>      3.819</td><td>      71.6%</td><td>      70.2%</td><td>      63.1%</td><td>      77.5%</td><td>      47.2%</td><td>      65.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/rwkv_14B_q4.bin">rwkv_14B_q4</a>
</td><td>    9</td><td>      4.076</td><td>      68.3%</td><td>      69.8%</td><td>      63.1%</td><td>      77.1%</td><td>      45.0%</td><td>      64.7%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/rwkv_14B_q8.bin">rwkv_14B_q8</a>
</td><td>   16</td><td>      3.806</td><td>      71.9%</td><td>      70.2%</td><td>      63.0%</td><td>      77.5%</td><td>      47.1%</td><td>      65.9%</td></tr>
<tr><td>rwkv_7B               </td><td>   16</td><td>      4.396</td><td>      67.5%</td><td>      65.6%</td><td>      61.9%</td><td>      75.6%</td><td>      39.7%</td><td>      62.1%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/rwkv_7B_q4.bin">rwkv_7B_q4</a>
</td><td>    5</td><td>      4.939</td><td>      64.7%</td><td>      64.8%</td><td>      61.2%</td><td>      75.4%</td><td>      38.4%</td><td>      60.9%</td></tr>
<tr><td><a href="https://www2.bellard.org/models/rwkv_7B_q8.bin">rwkv_7B_q8</a>
</td><td>    9</td><td>      4.395</td><td>      67.5%</td><td>      65.6%</td><td>      61.6%</td><td>      75.9%</td><td>      40.2%</td><td>      62.2%</td></tr>
  </tbody>
</table>
</p>

<p>
Additional Models:

<table>
  <thead>
    <tr>
      <th aria-sort="ascending">
      </th><th>
      </th><th>Description
    </th></tr>
  </thead>
  <tbody>
    <tr><td><a href="https://www2.bellard.org/models/m2m100_1_2B_q8.bin">m2m100_1_2B_q8</a></td><td>2</td><td>Translation between 100 languages</td></tr>
    <tr><td><a href="https://www2.bellard.org/models/sd-v1-4.bin">sd-v1-4</a></td><td>3</td><td>Stable Diffusion text-to-image version 1.4</td></tr>
  </tbody>
</table>
</p>
<p>
  SHA256 of all the models: <a href="https://www2.bellard.org/models/sha256.txt">sha256.txt</a>.
</p>
<p>
  Notes:
  </p><ol>
    <li>Some models have restrictive licenses. In particular, OPT
    and LLAMA cannot be used commercially. BLOOM and Stable Diffusion
      can be used commercially but have use limitations.</li>
    <li>For the larger models we don&#39;t provide the unquantized version when it is too large for consumer GPUs or when the quantized version gives the same performance as the unquantized version.</li>
    <li>The <b>q8</b> suffix indicates that the model was 8 bit
    quantized. The <b>q4</b> suffix indicates that the model was 4 bit
    quantized. Unquantized models use either float16 or bfloat16
    parameters.</li>
    <li>Approximate amount of CPU or GPU RAM needed to run the
      model. It is also the approximate size of the model file.</li>
    <li>lambada perplexity (ppl) are comparable only for models
      using the same tokenizer. So the lambada accuracy (acc) should be used
      when comparing all models.</li>
  </ol>


<hr/>
Fabrice Bellard - <a href="https://bellard.org">https://bellard.org/</a>


</div>
  </body>
</html>
